<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2308.09430</link>
<guid>https://arxiv.org/abs/2308.09430</guid>
<content:encoded><![CDATA[
<div> asynchronous delayed SGD, generalization performance, generalization error bounds, algorithmic stability, experimental results<br>
Summary:<br>
This paper explores the generalization performance of stochastic gradient descent (SGD) with asynchronous delays, a key component in training large-scale machine learning models. By analyzing the average stability of the delayed gradient algorithm, the study establishes sharper upper bounds on generalization error for quadratic convex and strongly convex problems. The bounds are $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and $\tilde{\mathcal{O}}(\frac{1}{n})$, respectively, where $T$ is the iteration number and $n$ is the training data size. The analysis reveals that asynchronous delays can reduce the generalization error in the delayed SGD algorithm. The research also extends to random delay settings, with experimental results confirming the theoretical findings. <div>
arXiv:2308.09430v4 Announce Type: replace 
Abstract: Stochastic gradient descent (SGD) performed in an asynchronous manner plays a crucial role in training large-scale machine learning models. However, the generalization performance of asynchronous delayed SGD, which is an essential metric for assessing machine learning algorithms, has rarely been explored. Existing generalization error bounds are rather pessimistic and cannot reveal the correlation between asynchronous delays and generalization. In this paper, we investigate sharper generalization error bound for SGD with asynchronous delay $\tau$. Leveraging the generating function analysis tool, we first establish the average stability of the delayed gradient algorithm. Based on this algorithmic stability, we provide upper bounds on the generalization error of $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and $\tilde{\mathcal{O}}(\frac{1}{n})$ for quadratic convex and strongly convex problems, respectively, where $T$ refers to the iteration number and $n$ is the amount of training data. Our theoretical results indicate that asynchronous delays reduce the generalization error of the delayed SGD algorithm. Analogous analysis can be generalized to the random delay setting, and the experimental results validate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fast algorithm to minimize prediction loss of the optimal solution in inverse optimization problem of MILP</title>
<link>https://arxiv.org/abs/2405.14273</link>
<guid>https://arxiv.org/abs/2405.14273</guid>
<content:encoded><![CDATA[
<div> weight estimation, inverse optimization, mixed integer linear program, projected subgradient method, convergence speed

Summary:
The article addresses the inverse optimization problem of estimating weights for an optimal solution in mixed integer linear programming. Existing methods have slow convergence rates as the dimension of weights increases. The proposed method, utilizing a projected subgradient approach with a step size of k^-1/2, achieves efficient weight learning. The theoretical analysis reveals that the method guarantees a bounded distance between learned and true weights, approaching exact recovery of the optimal solution. Experimental results demonstrate the method's effectiveness in solving the inverse optimization problem with significantly fewer MILP calls compared to known methods, converging within a finite number of iterations. This novel approach offers a promising solution for efficiently estimating weights in MILP instances. 

<br><br>Summary: <div>
arXiv:2405.14273v3 Announce Type: replace 
Abstract: We consider the inverse optimization problem of estimating the weights of the objective function such that the given solution is an optimal solution for a mixed integer linear program (MILP). In this inverse optimization problem, the known methods exhibit inefficient convergence. Specifically, if $d$ denotes the dimension of the weights and $k$ the number of iterations, then the error of the weights is bounded by $O(k^{-1/(d-1)})$, leading to slow convergence as $d$ increases. We propose a projected subgradient method with a step size of $k^{-1/2}$ based on suboptimality loss. We theoretically show and demonstrate that the proposed method efficiently learns the weights. In particular, we show that there exists a constant $\gamma > 0$ such that the distance between the learned and true weights is bounded by $ O\left(k^{-1/(1+\gamma)} \exp\left(-\frac{\gamma k^{1/2}}{2+\gamma}\right)\right), $ or the optimal solution is exactly recovered. Furthermore, experiments demonstrate that the proposed method solves the inverse optimization problems of MILP using fewer than $1/7$ the number of MILP calls required by known methods, and converges within a finite number of iterations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation Learning via Focused Satisficing</title>
<link>https://arxiv.org/abs/2505.14820</link>
<guid>https://arxiv.org/abs/2505.14820</guid>
<content:encoded><![CDATA[
<div> Keywords: Imitation learning, satisficing theory, deep reinforcement learning, margin-based objective, policy

Summary:
Imitation learning typically assumes demonstrations are close to optimal, but satisficing theory suggests humans may settle for acceptable behaviors based on personal levels of aspiration. This study introduces a focused satisficing approach to imitation learning, aiming to surpass the demonstrator's aspirations without learning them explicitly. By utilizing a margin-based objective in deep reinforcement learning, the policy is guided to imitate high-quality demonstrations better than existing methods, delivering higher rates of demonstrator acceptability and competitive true returns across various environments. This approach focuses on achieving success based on the demonstrator's subjective standards rather than striving for optimality, demonstrating improved performance in imitation learning tasks. <div>
arXiv:2505.14820v2 Announce Type: replace 
Abstract: Imitation learning often assumes that demonstrations are close to optimal according to some fixed, but unknown, cost function. However, according to satisficing theory, humans often choose acceptable behavior based on their personal (and potentially dynamic) levels of aspiration, rather than achieving (near-) optimality. For example, a lunar lander demonstration that successfully lands without crashing might be acceptable to a novice despite being slow or jerky. Using a margin-based objective to guide deep reinforcement learning, our focused satisficing approach to imitation learning seeks a policy that surpasses the demonstrator's aspiration levels -- defined over trajectories or portions of trajectories -- on unseen demonstrations without explicitly learning those aspirations. We show experimentally that this focuses the policy to imitate the highest quality (portions of) demonstrations better than existing imitation learning methods, providing much higher rates of guaranteed acceptability to the demonstrator, and competitive true returns on a range of environments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search</title>
<link>https://arxiv.org/abs/2505.15259</link>
<guid>https://arxiv.org/abs/2505.15259</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Graphical User Interfaces, Data Efficiency, Self-generated Reasoning, Spatial-aware Criticism

Summary:
ReGUIDE is a framework designed for improving the performance of Multimodal Large Language Models in localizing interface elements on Graphical User Interfaces. The framework utilizes self-generated reasoning processes and spatial-aware criticism to learn efficiently from web data. Through online reinforcement learning, ReGUIDE can generate a language reasoning process for interface element localization and use spatial priors to enhance prediction accuracy. At inference time, a test-time scaling strategy is employed to combine spatial search and coordinate aggregation for improved performance. Experimental results indicate that ReGUIDE surpasses existing baselines with significantly fewer training data points, showcasing its effectiveness in web grounding tasks. <div>
arXiv:2505.15259v2 Announce Type: replace 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled autonomous agents to interact with computers via Graphical User Interfaces (GUIs), where accurately localizing the coordinates of interface elements (e.g., buttons) is often required for fine-grained actions. However, this remains significantly challenging, leading prior works to rely on large-scale web datasets to improve the grounding accuracy. In this work, we propose Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a novel and effective framework for web grounding that enables MLLMs to learn data efficiently through self-generated reasoning and spatial-aware criticism. More specifically, ReGUIDE learns to (i) self-generate a language reasoning process for the localization via online reinforcement learning, and (ii) criticize the prediction using spatial priors that enforce equivariance under input transformations. At inference time, ReGUIDE further boosts performance through a test-time scaling strategy, which combines spatial search with coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly advances web grounding performance across multiple benchmarks, outperforming baselines with substantially fewer training data points (e.g., only 0.2% samples compared to the best open-sourced baselines).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Temporal Difference Method for Stochastic Continuous Dynamics</title>
<link>https://arxiv.org/abs/2505.15544</link>
<guid>https://arxiv.org/abs/2505.15544</guid>
<content:encoded><![CDATA[
<div> dynamical equations, Bellman's principle of optimality, Hamilton-Jacobi-Bellman equation, model-free approach, reinforcement learning

Summary:
This article presents a novel approach to reinforcement learning for continuous systems modeled by dynamical equations, such as ODEs and SDEs. While traditional RL methods assume known dynamics, this proposed model-free approach targets the Hamilton-Jacobi-Bellman (HJB) equation without requiring explicit access to the coefficient functions of the dynamical equations. By introducing a temporal difference method, this approach showcases potential advantages over transition kernel-based formulations, both qualitatively and empirically. This advancement in RL bridges the gap between stochastic optimal control and model-free reinforcement learning, offering a promising direction for future research and applications. <br><br>Summary: <div>
arXiv:2505.15544v3 Announce Type: replace 
Abstract: For continuous systems modeled by dynamical equations such as ODEs and SDEs, Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman (HJB) equation, which provides the theoretical target of reinforcement learning (RL). Although recent advances in RL successfully leverage this formulation, the existing methods typically assume the underlying dynamics are known a priori because they need explicit access to the coefficient functions of dynamical equations to update the value function following the HJB equation. We address this inherent limitation of HJB-based RL; we propose a model-free approach still targeting the HJB equation and propose the corresponding temporal difference method. We demonstrate its potential advantages over transition kernel-based formulations, both qualitatively and empirically. The proposed formulation paves the way toward bridging stochastic optimal control and model-free reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding</title>
<link>https://arxiv.org/abs/2505.15946</link>
<guid>https://arxiv.org/abs/2505.15946</guid>
<content:encoded><![CDATA[
<div> Hierarchical Mixture-of-Experts architecture, fMRI signals, visual reconstruction, interpretability, neural decoding <br>
<br>
Summary: <br>
The MoRE-Brain framework proposes a neuro-inspired approach for visual reconstruction from fMRI data, prioritizing high-fidelity, adaptability, and interpretability. It utilizes a hierarchical Mixture-of-Experts architecture, where specialized experts process fMRI signals from related voxel groups, mimicking brain networks. By encoding fMRI into the CLIP space and using a finetuned diffusion model for image synthesis, MoRE-Brain achieves efficient cross-subject generalization and provides enhanced mechanistic insight into how different brain regions influence the reconstructed image. The framework demonstrates high reconstruction fidelity and effectively utilizes fMRI signals, distinguishing genuine neural decoding from generative priors. MoRE-Brain marks a significant advancement in fMRI-based visual decoding by combining interpretability with reconstruction accuracy, making it a valuable tool for understanding human perception and developing brain-computer interfaces. <div>
arXiv:2505.15946v2 Announce Type: replace 
Abstract: Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: https://github.com/yuxiangwei0808/MoRE-Brain.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Stock Transactions</title>
<link>https://arxiv.org/abs/2505.16099</link>
<guid>https://arxiv.org/abs/2505.16099</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market, reinforcement learning, Markov Decision Process, Q-Learning, machine learning

Summary:
Reinforcement Learning (RL) is applied to determine the best time to buy and sell stocks within a given time frame. The project uses a self-defined Markov Decision Process (MDP) problem based on real-world data to train the model. Various agents are trained using Q-Learning, Q-Learning with linear function approximation, and deep Q-Learning, along with machine learning regression and classification models to predict stock prices. The state space and reward system of the MDP problem are formulated with the help of existing research. The goal is to analyze and compare the agents to see which one learns the best policy to maximize profit on the stock market.<br><br>Summary: The project focuses on utilizing reinforcement learning and machine learning techniques to determine optimal times for buying and selling stocks. By training agents with Q-Learning and deep Q-Learning, a self-defined Markov Decision Process is used to model the stock market scenario. The study aims to compare different learning algorithms and models to identify the most effective strategy for maximizing profit in stock trading. <div>
arXiv:2505.16099v2 Announce Type: replace 
Abstract: Much research has been done to analyze the stock market. After all, if one can determine a pattern in the chaotic frenzy of transactions, then they could make a hefty profit from capitalizing on these insights. As such, the goal of our project was to apply reinforcement learning (RL) to determine the best time to buy a stock within a given time frame. With only a few adjustments, our model can be extended to identify the best time to sell a stock as well. In order to use the format of free, real-world data to train the model, we define our own Markov Decision Process (MDP) problem. These two papers [5] [6] helped us in formulating the state space and the reward system of our MDP problem. We train a series of agents using Q-Learning, Q-Learning with linear function approximation, and deep Q-Learning. In addition, we try to predict the stock prices using machine learning regression and classification models. We then compare our agents to see if they converge on a policy, and if so, which one learned the best policy to maximize profit on the stock market.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries</title>
<link>https://arxiv.org/abs/2505.16664</link>
<guid>https://arxiv.org/abs/2505.16664</guid>
<content:encoded><![CDATA[
<div> Feature Engineering, Deep Learning Model, RUL Prediction, Transfer Learning, Battery Degradation </br>
<br>
Summary:<br>
This paper presents a novel approach for predicting the Remaining Useful Life (RUL) of lithium-ion batteries, crucial for timely maintenance. The approach utilizes recent charge-discharge cycle data to estimate remaining usable cycles. It includes a unique signal processing pipeline and a deep learning prediction model. The pipeline computes a derived capacity feature and enhances it with statistical metrics and a delta-based method to capture cycle differences. The deep learning model consists of 1D CNN, A-LSTM, and ODE-LSTM blocks, capturing both local and long-range dependencies while modeling battery degradation dynamics. Transfer learning is employed, showing robust performance even with limited target data. Experimental results on large datasets demonstrate the superiority of this method over baseline techniques, with an RMSE of 101.59, indicating its potential for real-world RUL prediction applications. <br> <div>
arXiv:2505.16664v2 Announce Type: replace 
Abstract: Accurate prediction of the Remaining Useful Life (RUL) is essential for enabling timely maintenance of lithium-ion batteries, impacting the operational efficiency of electric applications that rely on them. This paper proposes a RUL prediction approach that leverages data from recent charge-discharge cycles to estimate the number of remaining usable cycles. The approach introduces both a novel signal processing pipeline and a deep learning prediction model. In the signal preprocessing pipeline, a derived capacity feature $\dot{Q}(I, Q)$ is computed based on current and capacity signals. Alongside original capacity, voltage and current, these features are denoised and enhanced using statistical metrics and a delta-based method to capture differences between the current and previous cycles. In the prediction model, the processed features are then fed into a hybrid deep learning architecture composed of 1D Convolutional Neural Networks (CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential Equation-based LSTM (ODE-LSTM) blocks. This architecture is designed to capture both local signal characteristics and long-range temporal dependencies while modeling the continuous-time dynamics of battery degradation. The model is further evaluated using transfer learning across different learning strategies and target data partitioning scenarios. Results indicate that the model maintains robust performance, even when fine-tuned on limited target data. Experimental results on two publicly available large-scale datasets demonstrate that the proposed method outperforms a baseline deep learning approach and machine learning techniques, achieving an RMSE of 101.59, highlighting its strong potential for real-world RUL prediction applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Flexible Forward Trajectories for Masked Molecular Diffusion</title>
<link>https://arxiv.org/abs/2505.16790</link>
<guid>https://arxiv.org/abs/2505.16790</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Molecular generation, Masked Element-wise Learnable Diffusion, State-clashing problem, Chemical validity <br>
Summary: <br>
Masked diffusion models (MDMs) have shown success in modeling discrete data but face challenges in molecular generation. The study introduces Masked Element-wise Learnable Diffusion (MELD) to address the issue of state-clashing, where different molecules collapse into a common state during forward diffusion. MELD uses a noise scheduling network to assign corruption rates to individual graph elements, improving generation quality and chemical validity significantly. Experiments on various molecular benchmarks demonstrate that MELD enhances the performance of MDMs, increasing chemical validity from 15% to 93% on ZINC250K dataset. It also achieves state-of-the-art property alignment in conditional generation tasks. <div>
arXiv:2505.16790v2 Announce Type: replace 
Abstract: Masked diffusion models (MDMs) have achieved notable progress in modeling discrete data, while their potential in molecular generation remains underexplored. In this work, we explore their potential and introduce the surprising result that naively applying standards MDMs severely degrades the performance. We identify the critical cause of this issue as a state-clashing problem-where the forward diffusion of distinct molecules collapse into a common state, resulting in a mixture of reconstruction targets that cannot be learned using typical reverse diffusion process with unimodal predictions. To mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that orchestrates per-element corruption trajectories to avoid collision between distinct molecular graphs. This is achieved through a parameterized noise scheduling network that assigns distinct corruption rates to individual graph elements, i.e., atoms and bonds. Extensive experiments on diverse molecular benchmarks reveal that MELD markedly enhances overall generation quality compared to element-agnostic noise scheduling, increasing the chemical validity of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves state-of-the-art property alignment in conditional generation tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-Averse Reinforcement Learning with Itakura-Saito Loss</title>
<link>https://arxiv.org/abs/2505.16925</link>
<guid>https://arxiv.org/abs/2505.16925</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, risk-averse, exponential utility function, Itakura-Saito divergence, state-value, action-value<br>
<br>
Summary: 
Risk-averse reinforcement learning is crucial in high-stakes fields, where agents aim to minimize risk rather than solely maximize returns. The exponential utility function is utilized to frame risk preferences, allowing for the derivation of Bellman equations and the adaptation of reinforcement learning algorithms. This study introduces a novel loss function based on the Itakura-Saito divergence for learning state-value and action-value functions in a numerically stable and mathematically sound manner. The proposed Itakura-Saito loss function is evaluated against established alternatives both theoretically and empirically, demonstrating superior performance in various scenarios, including those with known analytical solutions. This research contributes a valuable tool to the machine learning community for effectively addressing risk-averse reinforcement learning problems. <div>
arXiv:2505.16925v2 Announce Type: replace 
Abstract: Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where one can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. To address this, we introduce to the broad machine learning community a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate the Itakura-Saito loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple scenarios, some with known analytical solutions, and show that the considered loss function outperforms the alternatives.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Are Concepts Erased From Diffusion Models?</title>
<link>https://arxiv.org/abs/2505.17013</link>
<guid>https://arxiv.org/abs/2505.17013</guid>
<content:encoded><![CDATA[
<div> concept erasure, diffusion models, evaluation framework, adversarial attacks, probing techniques 

Summary:
Concept erasure in diffusion models is a growing area of interest, with two proposed erasure mechanisms identified: reducing the likelihood of generating the target concept and interfering with internal guidance mechanisms. To thoroughly assess erasure, a suite of evaluations including adversarial attacks, probing techniques, and analysis of alternative generations in place of the erased concept is introduced. The results highlight the challenge of balancing side effects and robustness to adversarial prompts in erasure techniques. The study emphasizes the importance of comprehensive evaluation in understanding the effectiveness of erasure in diffusion models. 

Summary: <div>
arXiv:2505.17013v2 Announce Type: replace 
Abstract: Concept erasure, the ability to selectively prevent a model from generating specific concepts, has attracted growing interest, with various approaches emerging to address the challenge. However, it remains unclear how thoroughly these methods erase the target concept. We begin by proposing two conceptual models for the erasure mechanism in diffusion models: (i) reducing the likelihood of generating the target concept, and (ii) interfering with the model's internal guidance mechanisms. To thoroughly assess whether a concept has been truly erased from the model, we introduce a suite of independent evaluations. Our evaluation framework includes adversarial attacks, novel probing techniques, and analysis of the model's alternative generations in place of the erased concept. Our results shed light on the tension between minimizing side effects and maintaining robustness to adversarial prompts. Broadly, our work underlines the importance of comprehensive evaluation for erasure in diffusion models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Model-Free Bayesian State Estimation from Compressed Measurements</title>
<link>https://arxiv.org/abs/2407.07368</link>
<guid>https://arxiv.org/abs/2407.07368</guid>
<content:encoded><![CDATA[
<div> Bayesian state estimation, compressed measurements, model-free process, data-driven methods, SemiDANSE <br>
Summary:<br>
The study focuses on data-driven Bayesian state estimation from compressed measurements in a model-free process, addressing an under-determined inverse problem. Traditional model-driven methods are unsuitable due to the unknown underlying dynamical model of the state's evolution. Existing unsupervised learning-based methods, DANSE and DMM, are shown to be inadequate for this task. A new approach, SemiDANSE, is proposed, utilizing semi-supervised learning with a combination of labelled and unlabelled data for regularization. Empirical experiments on benchmark dynamical systems demonstrate that SemiDANSE outperforms other methods for state estimation in the BSCM problem using different measurement systems. It competes well against KalmanNet, extended Kalman filter, and unscented Kalman filter, despite not knowing the exact dynamical models. <div>
arXiv:2407.07368v5 Announce Type: replace-cross 
Abstract: We consider data-driven Bayesian state estimation from compressed measurements (BSCM) of a model-free process. The dimension of the temporal measurement vector is lower than that of the temporal state vector to be estimated, leading to an under-determined inverse problem. The underlying dynamical model of the state's evolution is unknown for a 'model-free process.' Hence, it is difficult to use traditional model-driven methods, for example, Kalman and particle filters. Instead, we consider data-driven methods. We experimentally show that two existing unsupervised learning-based data-driven methods fail to address the BSCM problem in a model-free process. The methods are -- data-driven nonlinear state estimation (DANSE) and deep Markov model (DMM). While DANSE provides good predictive/forecasting performance to model the temporal measurement data as a time series, its unsupervised learning lacks suitable regularization for tackling the BSCM task. We then propose a semi-supervised learning approach and develop a semi-supervised learning-based DANSE method, referred to as SemiDANSE. In SemiDANSE, we use a large amount of unlabelled data along with a limited amount of labelled data, i.e., pairwise measurement-and-state data, which provides the desired regularization. Using three benchmark dynamical systems, we empirically show that the data-driven SemiDANSE provides competitive state estimation performance for BSCM using a handful of different measurement systems, against a hybrid method called KalmanNet and two model-driven methods (extended Kalman filter and unscented Kalman filter) that know the dynamical models exactly.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Pseudo-Labeling for Computer Vision</title>
<link>https://arxiv.org/abs/2408.07221</link>
<guid>https://arxiv.org/abs/2408.07221</guid>
<content:encoded><![CDATA[
<div> semi-supervised learning, deep neural networks, pseudo-labeling, self-supervised learning, unsupervised methods

Summary:
In this work, the authors investigate the concept of pseudo-labeling in the context of deep neural models and explore its application beyond semi-supervised learning. They highlight the importance of leveraging large quantities of unlabeled data to improve the generalization of deep neural networks, identifying pseudo-labeling as a key technique in this domain. The study suggests a broader interpretation of pseudo-labels, extending their use to self-supervised and unsupervised learning methods. By bridging the gap between these areas, the authors propose new avenues for research, such as curriculum learning and self-supervised regularization, which could yield improvements across multiple domains. This holistic approach to pseudo-labeling opens up new possibilities for enhancing the performance and efficiency of deep neural networks in various computer science applications. <br><br>Summary: <div>
arXiv:2408.07221v3 Announce Type: replace-cross 
Abstract: Deep neural models have achieved state of the art performance on a wide range of problems in computer science, especially in computer vision. However, deep neural networks often require large datasets of labeled samples to generalize effectively, and an important area of active research is semi-supervised learning, which attempts to instead utilize large quantities of (easily acquired) unlabeled samples. One family of methods in this space is pseudo-labeling, a class of algorithms that use model outputs to assign labels to unlabeled samples which are then used as labeled samples during training. Such assigned labels, called pseudo-labels, are most commonly associated with the field of semi-supervised learning. In this work we explore a broader interpretation of pseudo-labels within both self-supervised and unsupervised methods. By drawing the connection between these areas we identify new directions when advancements in one area would likely benefit others, such as curriculum learning and self-supervised regularization.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision</title>
<link>https://arxiv.org/abs/2505.14996</link>
<guid>https://arxiv.org/abs/2505.14996</guid>
<content:encoded><![CDATA[
<div> self-evolved, Large Language Models, Multi-agent systems, automatic design, adaptability

Summary: 
Multi-agent systems (MAS) incorporating Large Language Models (LLMs) show promise in tackling complex tasks. However, manual MAS designs may not fully leverage LLM strengths and struggle to adapt to new tasks. Automatic MAS approaches often lack adaptability during inference. MAS-ZERO is introduced as a self-evolved, inference-time framework for automatic MAS design. It uses meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance. MAS-ZERO enables dynamic agent composition and problem decomposition through meta-feedback. Experimental results across various benchmarks demonstrate that MAS-ZERO outperforms manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement while remaining cost-effective. These findings highlight the potential of meta-level self-evolved design for creating effective and adaptive MAS. 

<br><br>Summary: <div>
arXiv:2505.14996v2 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation set for tuning and yield static MAS designs lacking adaptability during inference. We introduce MAS-ZERO, the first self-evolved, inference-time framework for automatic MAS design. MAS-ZERO employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM backbones of varying sizes, demonstrate that MAS-ZERO outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-evolved design for creating effective and adaptive MAS.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer</title>
<link>https://arxiv.org/abs/2505.16463</link>
<guid>https://arxiv.org/abs/2505.16463</guid>
<content:encoded><![CDATA[
<div> vision transformers, ViTs, AnchorFormer, bipartite attention, downstream tasks<br>
<br>
Summary: 
The article introduces AnchorFormer, an efficient vision transformer that utilizes anchor tokens to capture pivotal information and speed up inference. By estimating bipartite attention between anchors and tokens, the complexity is reduced from O(n^2) to O(mn), where m is the number of anchors. Anchors represented by neurons allow for differentiable learning of distributions and approximation of global self-attention through a Markov process. The model is extended to classification, detection, and segmentation tasks, showing superior performance compared to existing baselines. For instance, on ImageNet classification, AnchorFormer achieves up to 9.0% higher accuracy or 46.7% FLOPs reduction. On COCO detection, it achieves 81.3% higher mAP under comparable FLOPs. Overall, AnchorFormer proves to be effective in improving performance and efficiency across various vision tasks. <br><br> <div>
arXiv:2505.16463v2 Announce Type: replace-cross 
Abstract: Recently, vision transformers (ViTs) have achieved excellent performance on vision tasks by measuring the global self-attention among the image patches. Given $n$ patches, they will have quadratic complexity such as $\mathcal{O}(n^2)$ and the time cost is high when splitting the input image with a small granularity. Meanwhile, the pivotal information is often randomly gathered in a few regions of an input image, some tokens may not be helpful for the downstream tasks. To handle this problem, we introduce an anchor-based efficient vision transformer (AnchorFormer), which employs the anchor tokens to learn the pivotal information and accelerate the inference. Firstly, by estimating the bipartite attention between the anchors and tokens, the complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where $m$ is an anchor number and $m < n$. Notably, by representing the anchors with the neurons in a neural layer, we can differentiable learn these distributions and approximate global self-attention through the Markov process. Moreover, we extend the proposed model to three downstream tasks including classification, detection, and segmentation. Extensive experiments show the effectiveness of our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs reduction on ImageNet classification, 81.3% higher mAP on COCO detection under comparable FLOPs, as compared to the current baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2505.16471</link>
<guid>https://arxiv.org/abs/2505.16471</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep reinforcement learning, Graph neural network, Multi-objective combinatorial optimization, Evolutionary algorithms, Algorithm configuration

Summary:
Deep reinforcement learning (DRL) is applied to dynamic algorithm configuration for multi-objective combinatorial optimization (MOCO) problems using a graph neural network (GNN). The method models algorithm configuration as a Markov decision process, with a graph representing solution convergence in the objective space. GNN is utilized to learn embeddings and enhance state representation. Experimental results show superior performance compared to traditional and DRL-based methods in efficacy and adaptability. The approach demonstrates generalizability across different objective types and problem sizes, and is applicable to various evolutionary computation methods. The GNN-based DRL algorithm configuration outperforms existing methods in MOCO challenges, showcasing its effectiveness in optimizing multi-objective problems using evolutionary algorithms. <br><br>Summary: <div>
arXiv:2505.16471v2 Announce Type: replace-cross 
Abstract: Deep reinforcement learning (DRL) has been widely used for dynamic algorithm configuration, particularly in evolutionary computation, which benefits from the adaptive update of parameters during the algorithmic execution. However, applying DRL to algorithm configuration for multi-objective combinatorial optimization (MOCO) problems remains relatively unexplored. This paper presents a novel graph neural network (GNN) based DRL to configure multi-objective evolutionary algorithms. We model the dynamic algorithm configuration as a Markov decision process, representing the convergence of solutions in the objective space by a graph, with their embeddings learned by a GNN to enhance the state representation. Experiments on diverse MOCO challenges indicate that our method outperforms traditional and DRL-based algorithm configuration methods in terms of efficacy and adaptability. It also exhibits advantageous generalizability across objective types and problem sizes, and applicability to different evolutionary computation methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols</title>
<link>https://arxiv.org/abs/2505.16821</link>
<guid>https://arxiv.org/abs/2505.16821</guid>
<content:encoded><![CDATA[
<div> transformer model, Radio Resource Control, control-plane procedures, LAMs, AI-native

Summary:
This paper presents a demonstration of a Large AI Model (LAM) that generates standards-compliant Radio Resource Control (RRC) messages in a 6G mobile network. By treating RRC messaging as a domain-specific language, a decoder-only transformer model (LLaMA class) is fine-tuned using Low-Rank Adaptation (LoRA) on linearized RRC messages to maintain their ASN.1 structure. The model achieves a high cosine similarity with ground-truth messages, indicating improved structural and semantic fidelity. The approach shows that LAMs, augmented with Radio Access Network (RAN)-specific reasoning, can directly manage control-plane procedures, paving the way for AI-native wireless standards. This work represents a significant step towards autonomous, cognitive network operations and intelligent protocol design in future mobile networks. <div>
arXiv:2505.16821v2 Announce Type: replace-cross 
Abstract: Integrating large AI models (LAMs) into 6G mobile networks promises to redefine protocol design and control-plane intelligence by enabling autonomous, cognitive network operations. While industry concepts, such as ETSI's Experiential Networked Intelligence (ENI), envision LAM-driven agents for adaptive network slicing and intent-based management, practical implementations still face challenges in protocol literacy and real-world deployment. This paper presents an end-to-end demonstration of a LAM that generates standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as part of control-plane procedures inside a gNB. We treat RRC messaging as a domain-specific language and fine-tune a decoder-only transformer model (LLaMA class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages linearized to retain their ASN.1 syntactic structure before standard byte-pair encoding tokenization. This enables combinatorial generalization over RRC protocol states while minimizing training overhead. On 30k field-test request-response pairs, our 8 B model achieves a median cosine similarity of 0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural and semantic RRC fidelity. Overall, our results show that LAMs, when augmented with Radio Access Network (RAN)-specific reasoning, can directly orchestrate control-plane procedures, representing a stepping stone toward the AI-native air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for future AI-native wireless standards.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
<link>https://arxiv.org/abs/2505.16968</link>
<guid>https://arxiv.org/abs/2505.16968</guid>
<content:encoded><![CDATA[
<div> CUDA, HIP, Nvidia SASS, AMD RDNA3, GPU code transpilation 
<br>
Summary: 
CASS is introduced as a dataset and model suite for cross-architecture GPU code transpilation, covering source-level (CUDA $\leftrightarrow$ HIP) and assembly-level (Nvidia SASS $\leftrightarrow$ AMD RDNA3) translation. The dataset consists of 70k verified code pairs and trains domain-specific language models in the CASS family. The models achieve high accuracy in source and assembly translation, surpassing commercial baselines like GPT-4o and others. Code generated by the models matches native performance in the majority of test cases, preserving runtime and memory behavior. The CASS-Bench benchmark, spanning 16 GPU domains with ground-truth execution, is introduced for rigorous evaluation. All data, models, and evaluation tools are open source, promoting advancements in GPU compiler tooling, binary compatibility, and hardware translation guided by language models. The dataset and benchmark are available on HuggingFace, with the code repository on GitHub. <div>
arXiv:2505.16968v2 Announce Type: replace-cross 
Abstract: We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA $\leftrightarrow$ HIP) and assembly-level (Nvidia SASS $\leftrightarrow$ AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on \href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}}, with code at \href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Forbidden Topics in Language Models</title>
<link>https://arxiv.org/abs/2505.17441</link>
<guid>https://arxiv.org/abs/2505.17441</guid>
<content:encoded><![CDATA[
<div> discovery, refusal, language model, bias, AI<br>
<br>
Summary: 
The study introduces the concept of refusal discovery in language models, which aims to identify topics that the model refuses to discuss. A new method called LLM-crawler is developed using token prefilling to uncover forbidden topics. The method is tested on the Tulu-3-8B model, successfully retrieving a majority of the topics within a limited prompt budget. Scaling the crawl to a frontier model with the help of Claude-Haiku shows revealing patterns of censorship tuning in the DeepSeek-R1-70B model, indicating memorization of CCP-aligned responses. While the Perplexity-R1-1776-70B model exhibits robustness to censorship, the LLM-crawler still elicits CCP-aligned refusal answers in the quantized version. These findings underscore the importance of implementing refusal discovery methods to identify biases, limitations, and alignment failures in AI systems. <div>
arXiv:2505.17441v2 Announce Type: replace-cross 
Abstract: Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, LLM-crawler, that uses token prefilling to find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawl to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits "thought suppression" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Model-Distributed Inference for Large Language Models at the Edge</title>
<link>https://arxiv.org/abs/2505.18164</link>
<guid>https://arxiv.org/abs/2505.18164</guid>
<content:encoded><![CDATA[
<div> Keywords: Model-Distributed Inference, Large-Language Models, Edge devices, Recurrent pipeline parallelism, Collaborative computation

Summary:
Model-Distributed Inference for Large-Language Models (MDI-LLM) is a framework designed to deploy state-of-the-art large-language models on low-power edge devices. The model is split into partitions assigned to different nodes, enabling collaborative computation through intermediate vector exchange. The "recurrent pipeline parallelism" technique reduces idle time on each device, enhancing efficiency during text sequence generation. By combining computational resources of multiple devices, MDI-LLM allows deployment of models exceeding individual device memory capacity, enabling inference on inexpensive hardware. With more devices, token generation throughput increases and memory consumption per device decreases. This approach optimizes model efficiency and performance on edge devices, making it practical for real-world applications. <br /><br />Summary: <div>
arXiv:2505.18164v1 Announce Type: new 
Abstract: We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM), a novel framework designed to facilitate the deployment of state-of-the-art large-language models (LLMs) across low-power devices at the edge. This is accomplished by dividing the model into multiple partitions, which are then assigned to different devices/nodes within the network. These nodes exchange intermediate activation vectors via device-to-device links, enabling collaborative computation. To enhance the efficiency of this process, we propose the "recurrent pipeline parallelism" technique, which reduces idle time on each device and facilitates parallel inference during the generation of multiple text sequences. By leveraging the combined computational resources of multiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the memory capacity of individual devices, making it possible to perform inference on low-cost hardware. Furthermore, as the number of participating devices increases, MDI-LLM boosts token generation throughput and reduces memory consumption per device.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression</title>
<link>https://arxiv.org/abs/2505.18166</link>
<guid>https://arxiv.org/abs/2505.18166</guid>
<content:encoded><![CDATA[
<div> pruning, re-training, compression, MLP, loss function  
Summary:
The study focuses on compressing foundational models for edge deployments using layer-wise L2-norm pruning on MLP blocks. The impact of re-training loss function is analyzed, comparing Fine-tuning with Cross-Entropy (L2PFT) and Self-Distillation with KL-divergence (L2PSD). Evaluation on the OLMo2-7B-SFT model for CommonsenseQA in intermittent connectivity scenarios shows that KL-based distillation performs as well as or better than CE fine-tuning in test accuracy. The results highlight the significant influence of the choice of loss function on compressed model recovery in resource-constrained environments. <br /><br />Summary: <div>
arXiv:2505.18166v1 Announce Type: new 
Abstract: Modern foundational models are often compressed via a combination of structured pruning and re-training to meet the strict compute, memory, and connectivity constraints of edge deployments. While state-of-the-art pruning schemes target the entire Transformer, we adopt a simple, layer-wise L2-norm pruning on only the MLP blocks as a fixed baseline. Our focus is not on achieving maximal compression, but on isolating the impact of the re-training loss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires labeled data, versus (ii) Self-Distillation with KL-divergence, which leverages only teacher logits (no labels) (L2PSD). We evaluate both pipelines on the OLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied connectivity scenarios typical of edge networks. Under identical pruning schedules, KL-based distillation matches or exceeds CE fine-tuning in test accuracy, demonstrating that, even with a basic MLP-only pruning, the choice of loss function materially affects compressed model recovery in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation</title>
<link>https://arxiv.org/abs/2505.18168</link>
<guid>https://arxiv.org/abs/2505.18168</guid>
<content:encoded><![CDATA[
<div> Keywords: facial emotion perception, vision large language model, self-verification approach, emotion knowledge enhancement, facial emotion analysis

Summary:
Facial emotion perception in the vision large language model (VLLM) is essential for natural human-machine interaction. High-quality annotations for facial emotion analysis are costly to create, hindering VLLM performance. The self-verification approach with emotion knowledge enhancement (SEKE) generates high-quality instruction data cost-effectively using a closed-source VLLM. By integrating prior human knowledge and a self-verification strategy, comprehensive annotations are generated for multi-grained emotion analysis. The approach enhances annotation reliability by extracting more accurate VLLM predictions. The facial emotion instruction dataset (FEID) and facial emotion analysis benchmark (FEAB) are introduced for effective model training and performance evaluation. SEKE outperforms state-of-the-art methods on three downstream facial emotion analysis tasks. The method significantly improves VLLM's ability in facial emotion perception. <br /><br />Summary: <div>
arXiv:2505.18168v1 Announce Type: new 
Abstract: Facial emotion perception in the vision large language model (VLLM) is crucial for achieving natural human-machine interaction. However, creating high-quality annotations for both coarse- and fine-grained facial emotion analysis demands costly expertise. The lack of such high-quality instruction data limits the performance of VLLMs in facial emotion perception. To address this, we propose a self-verification approach with emotion knowledge enhancement (SEKE), which generates high-quality instruction data for multi-grained emotion analysis cost-effectively using closed-source VLLM. This approach integrates prior human knowledge to VLLM inference, guided by the inherent correlations between three grained levels of emotion descriptions, i.e., discrete expression, valence-arousal, and action unit, to reliably generate comprehensive annotations. A self-verification strategy with Uncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to efficiently extract more accurate VLLM predictions, further improving annotation reliability. Consequently, we construct a facial emotion instruction dataset (FEID) containing three comprehensive descriptions, which provides coarse- and fine-grained emotional information for effective model training. Additionally, we introduce a facial emotion analysis benchmark (FEAB) to measure the VLLM's corresponding ability. Our method significantly outperforms state-of-the-art methods on three downstream facial emotion analysis tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction</title>
<link>https://arxiv.org/abs/2505.18169</link>
<guid>https://arxiv.org/abs/2505.18169</guid>
<content:encoded><![CDATA[
<div> wearable sensors, emotion recognition, multi-task Physics-Informed Neural Network, electrodermal activity, interpretability 

Summary:
This study introduces a novel Multi-Task Physics-Informed Neural Network (PINN) for predicting Electrodermal Activity (EDA) and classifying emotions simultaneously using the WESAD dataset. The model incorporates psychological self-report features and a physics-inspired differential equation to enforce biophysically grounded constraints. With dual outputs for both tasks, the model achieves superior performance in EDA prediction and emotion classification compared to traditional models. The model's interpretability is enhanced through the inclusion of physical parameters such as decay rate, emotional sensitivity, and time scaling, aligning with human physiology principles. This work sets a foundation for future applications in healthcare and human-computer interaction by offering improved performance, generalizability, and model transparency. <div>
arXiv:2505.18169v1 Announce Type: new 
Abstract: Understanding and predicting human emotional and physiological states using wearable sensors has important applications in stress monitoring, mental health assessment, and affective computing. This study presents a novel Multi-Task Physics-Informed Neural Network (PINN) that performs Electrodermal Activity (EDA) prediction and emotion classification simultaneously, using the publicly available WESAD dataset. The model integrates psychological self-report features (PANAS and SAM) with a physics-inspired differential equation representing EDA dynamics, enforcing biophysically grounded constraints through a custom loss function. This loss combines EDA regression, emotion classification, and a physics residual term for improved interpretability.
  The architecture supports dual outputs for both tasks and is trained under a unified multi-task framework. Evaluated using 5-fold cross-validation, the model achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919, and F1-score of 94.08 percent. These results outperform classical models such as SVR and XGBoost, as well as ablated variants like emotion-only and EDA-only models.
  In addition, the learned physical parameters including decay rate (alpha_0), emotional sensitivity (beta), and time scaling (gamma) are interpretable and stable across folds, aligning with known principles of human physiology. This work is the first to introduce a multi-task PINN framework for wearable emotion recognition, offering improved performance, generalizability, and model transparency. The proposed system provides a foundation for future interpretable and multimodal applications in healthcare and human-computer interaction.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Knowledge Graph Embedding via Denoising</title>
<link>https://arxiv.org/abs/2505.18171</link>
<guid>https://arxiv.org/abs/2505.18171</guid>
<content:encoded><![CDATA[
<div> framework, robustness, knowledge graph embedding, denoising, perturbation
<br />
Summary: <br />
The article introduces a novel framework called Robust Knowledge Graph Embedding via Denoising, focusing on enhancing the robustness of knowledge graph embedding (KGE) models in the presence of noisy triples. By treating KGE methods as energy-based models and utilizing the connection between denoising and score matching, the framework allows for the training of robust denoising KGE models. The authors also propose certified robustness evaluation metrics for KGE methods based on randomized smoothing. Experimental results on benchmark datasets demonstrate that the framework consistently outperforms existing state-of-the-art KGE methods when dealing with perturbed entity embedding. <div>
arXiv:2505.18171v1 Announce Type: new 
Abstract: We focus on obtaining robust knowledge graph embedding under perturbation in the embedding space. To address these challenges, we introduce a novel framework, Robust Knowledge Graph Embedding via Denoising, which enhances the robustness of KGE models on noisy triples. By treating KGE methods as energy-based models, we leverage the established connection between denoising and score matching, enabling the training of a robust denoising KGE model. Furthermore, we propose certified robustness evaluation metrics for KGE methods based on the concept of randomized smoothing. Through comprehensive experiments on benchmark datasets, our framework consistently shows superior performance compared to existing state-of-the-art KGE methods when faced with perturbed entity embedding.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should We Simultaneously Calibrate Multiple Computer Models?</title>
<link>https://arxiv.org/abs/2505.18176</link>
<guid>https://arxiv.org/abs/2505.18176</guid>
<content:encoded><![CDATA[
<div> neural networks, computer models, calibration, multi-response, probabilistic framework

Summary: 
This paper explores the simultaneous calibration of multiple computer models using a probabilistic framework based on customized neural networks. By considering the multi-response nature of most computer models and the variability in calibration parameters across models, the approach aims to learn unique probability distributions for each parameter. A loss function is developed to enable the neural networks to emulate all data sources while calibrating the models. Additionally, the method aims to learn a visualizable latent space to identify model-form errors. Testing on analytic and engineering problems shows that the approach can improve predictive accuracy, although it may face challenges with non-identifiability in higher-dimensional input spaces constrained by underlying physics. <div>
arXiv:2505.18176v1 Announce Type: new 
Abstract: In an increasing number of applications designers have access to multiple computer models which typically have different levels of fidelity and cost. Traditionally, designers calibrate these models one at a time against some high-fidelity data (e.g., experiments). In this paper, we question this tradition and assess the potential of calibrating multiple computer models at the same time. To this end, we develop a probabilistic framework that is founded on customized neural networks (NNs) that are designed to calibrate an arbitrary number of computer models. In our approach, we (1) consider the fact that most computer models are multi-response and that the number and nature of calibration parameters may change across the models, and (2) learn a unique probability distribution for each calibration parameter of each computer model, (3) develop a loss function that enables our NN to emulate all data sources while calibrating the computer models, and (4) aim to learn a visualizable latent space where model-form errors can be identified. We test the performance of our approach on analytic and engineering problems to understand the potential advantages and pitfalls in simultaneous calibration of multiple computer models. Our method can improve predictive accuracy, however, it is prone to non-identifiability issues in higher-dimensional input spaces that are normally constrained by underlying physics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations</title>
<link>https://arxiv.org/abs/2505.18177</link>
<guid>https://arxiv.org/abs/2505.18177</guid>
<content:encoded><![CDATA[
<div> federated learning, graph neural networks, cross-border recommendations, privacy preservation, collaborative training<br />
<br />
Summary:<br />
FedGRec is a privacy-preserving federated graph learning method designed for cross-border recommendations. The method utilizes collaborative signals from local subgraphs to improve representation learning and dynamically integrates global and local user preferences in real time. By filtering out irrelevant behaviors and employing personalized federated aggregation, FedGRec adapts global preferences to heterogeneous domain data, enhancing collaborative learning across multiple domains. Experimental results on three datasets show that FedGRec outperforms single-domain and cross-domain baselines while preserving data privacy effectively. <div>
arXiv:2505.18177v1 Announce Type: new 
Abstract: Due to the highly sensitive nature of certain data in cross-border sharing, collaborative cross-border recommendations and data sharing are often subject to stringent privacy protection regulations, resulting in insufficient data for model training. Consequently, achieving efficient cross-border business recommendations while ensuring privacy security poses a significant challenge. Although federated learning has demonstrated broad potential in collaborative training without exposing raw data, most existing federated learning-based GNN training methods still rely on federated averaging strategies, which perform suboptimally on highly heterogeneous graph data. To address this issue, we propose FedGRec, a privacy-preserving federated graph learning method for cross-border recommendations. FedGRec captures user preferences from distributed multi-domain data to enhance recommendation performance across all domains without privacy leakage. Specifically, FedGRec leverages collaborative signals from local subgraphs associated with users or items to enrich their representation learning. Additionally, it employs dynamic spatiotemporal modeling to integrate global and local user preferences in real time based on business recommendation states, thereby deriving the final representations of target users and candidate items. By automatically filtering relevant behaviors, FedGRec effectively mitigates noise interference from unreliable neighbors. Furthermore, through a personalized federated aggregation strategy, FedGRec adapts global preferences to heterogeneous domain data, enabling collaborative learning of user preferences across multiple domains. Extensive experiments on three datasets demonstrate that FedGRec consistently outperforms competitive single-domain and cross-domain baselines while effectively preserving data privacy in cross-border recommendations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Multimodal Region Representation via Pairwise Inter-view Learning</title>
<link>https://arxiv.org/abs/2505.18178</link>
<guid>https://arxiv.org/abs/2505.18178</guid>
<content:encoded><![CDATA[
<div> Keywords: geospatial datasets, region representation learning, contrastive learning, information factorization, multimodal data <br />
Summary:
The article introduces a new approach, Cross modal Knowledge Injected Embedding (CooKIE), for region representation learning (RRL) that incorporates both shared and unique information from multiple modalities. CooKIE utilizes pairwise inter-view learning to capture high-order information without complexity. The method is evaluated on regression tasks and land use classification in New York City and Delhi, outperforming existing RRL methods and a factorized RRL model. It achieves this with fewer training parameters and FLOPs. By releasing the code, CooKIE provides a valuable resource for researchers working with geospatial datasets. The approach addresses the limitation of existing RRL methods by effectively capturing both shared and modality-specific information, leading to improved performance in analyzing complex region characteristics. <br /><br />Summary: <div>
arXiv:2505.18178v1 Announce Type: new 
Abstract: With the increasing availability of geospatial datasets, researchers have explored region representation learning (RRL) to analyze complex region characteristics. Recent RRL methods use contrastive learning (CL) to capture shared information between two modalities but often overlook task-relevant unique information specific to each modality. Such modality-specific details can explain region characteristics that shared information alone cannot capture. Bringing information factorization to RRL can address this by factorizing multimodal data into shared and unique information. However, existing factorization approaches focus on two modalities, whereas RRL can benefit from various geospatial data. Extending factorization beyond two modalities is non-trivial because modeling high-order relationships introduces a combinatorial number of learning objectives, increasing model complexity. We introduce Cross modal Knowledge Injected Embedding, an information factorization approach for RRL that captures both shared and unique representations. CooKIE uses a pairwise inter-view learning approach that captures high-order information without modeling high-order dependency, avoiding exhaustive combinations. We evaluate CooKIE on three regression tasks and a land use classification task in New York City and Delhi, India. Results show that CooKIE outperforms existing RRL methods and a factorized RRL model, capturing multimodal information with fewer training parameters and floating-point operations per second (FLOPs). We release the code: https://github.com/MinNamgung/CooKIE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAIA: A Foundation Model for Operational Atmospheric Dynamics</title>
<link>https://arxiv.org/abs/2505.18179</link>
<guid>https://arxiv.org/abs/2505.18179</guid>
<content:encoded><![CDATA[
<div> masked autoencoders, self-Distillation, satellite imagery, precipitation estimation, self-supervised learning<br />
<br />
Summary:
The GAIA Foundation Model combines masked autoencoders and self-distillation with DINO for analyzing global atmospheric patterns in satellite imagery. It addresses the challenges of reconstructing missing regions and estimating precipitation patterns. The model captures local features and global dependencies simultaneously, showing superior temporal pattern capture compared to standard MAE approaches. It demonstrates robust performance in downstream tasks, with strong gap-filling capabilities across varying mask ratios and accurate precipitation estimation with limited training data. The model achieves a false alarm ratio of 0.088 and a structural similarity of 0.881. This work represents an advancement in self-supervised learning for atmospheric science, providing a foundation for improved weather monitoring and climate analysis. The trained model weights and code are openly available on Hugging Face for further research and applications. <br /><br />Summary: <div>
arXiv:2505.18179v1 Announce Type: new 
Abstract: We present the GAIA (Geospatial Artificial Intelligence for Atmospheres) Foundation Model, a novel model that combines masked autoencoders (MAE) and self-DIstillation with NO labels (DINO) for analyzing global atmospheric patterns in satellite imagery. By integrating these complementary self-supervised learning approaches, our model simultaneously captures both local features and global dependencies. We address two critical challenges in satellite data analysis: reconstructing missing regions and estimating precipitation patterns as our first downstream tasks. The model demonstrates superior temporal pattern capture compared to standard MAE approaches, while maintaining robust performance in downstream tasks. Our experimental results show strong gap-filling capabilities across varying mask ratios and accurate precipitation estimation with limited training data, achieving a false alarm ratio of 0.088 and structural similarity of 0.881. This work represents an advancement in self-supervised learning for atmospheric science, providing a foundation for improved weather monitoring and climate analysis. The trained model weights and accompanying code are publicly available as open-source on Hugging Face here: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision</title>
<link>https://arxiv.org/abs/2505.18181</link>
<guid>https://arxiv.org/abs/2505.18181</guid>
<content:encoded><![CDATA[
<div> dataset, machine learning, 2D NMR, molecular representation, HSQC spectroscopy

Summary:<br />
The article introduces 2DNMRGym, a dataset created for machine learning-based molecular representation learning in 2D NMR analysis. With over 22,000 HSQC spectra, molecular graphs, and SMILES strings, this dataset addresses the limited availability of annotated datasets in the field. Using a surrogate supervision setup, models are trained with algorithm-generated annotations and evaluated with expert-annotated gold-standard labels. Various 2D and 3D graph neural network (GNN) and GNN transformer models are benchmarked on this dataset, showing promising results for future ML applications in 2D NMR analysis. The open-source data and code are accessible on Huggingface and Github, supporting scalable model training and providing a chemically significant benchmark for assessing atom-level molecular representations in NMR-guided structural tasks. <div>
arXiv:2505.18181v1 Announce Type: new 
Abstract: Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy, particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays a critical role in elucidating molecular structures, interactions, and electronic properties. However, accurately interpreting 2D NMR data remains labor-intensive and error-prone, requiring highly trained domain experts, especially for complex molecules. Machine Learning (ML) holds significant potential in 2D NMR analysis by learning molecular representations and recognizing complex patterns from data. However, progress has been limited by the lack of large-scale and high-quality annotated datasets. In this work, we introduce 2DNMRGym, the first annotated experimental dataset designed for ML-based molecular representation learning in 2D NMR. It includes over 22,000 HSQC spectra, along with the corresponding molecular graphs and SMILES strings. Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained using algorithm-generated annotations derived from a previously validated method and evaluated on a held-out set of human-annotated gold-standard labels. This enables rigorous assessment of a model's ability to generalize from imperfect supervision to expert-level interpretation. We provide benchmark results using a series of 2D and 3D GNN and GNN transformer models, establishing a strong foundation for future work. 2DNMRGym supports scalable model training and introduces a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. Our data and code is open-source and available on Huggingface and Github.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry</title>
<link>https://arxiv.org/abs/2505.18193</link>
<guid>https://arxiv.org/abs/2505.18193</guid>
<content:encoded><![CDATA[
<div> Riemannian manifolds, brain connectivity matrices, conditional flow matching, DiffeoCFM, fMRI datasets<br />
<br />
Summary: <br />
Generating realistic brain connectivity matrices is crucial for analyzing population heterogeneity and understanding diseases. Functional connectivity matrices are constrained to specific spaces that can be modeled as Riemannian manifolds. The proposed DiffeoCFM approach allows for conditional flow matching on matrix manifolds efficiently by utilizing pullback metrics induced by global diffeomorphisms on Euclidean spaces. This enables fast vector field learning and sampling with standard ODE solvers. DiffeoCFM, applied to covariance and correlation matrices, was evaluated on large-scale fMRI and EEG datasets, achieving state-of-the-art performance while preserving manifold constraints. <div>
arXiv:2505.18193v1 Announce Type: new 
Abstract: Generating realistic brain connectivity matrices is key to analyzing population heterogeneity in brain organization, understanding disease, and augmenting data in challenging classification problems. Functional connectivity matrices lie in constrained spaces--such as the set of symmetric positive definite or correlation matrices--that can be modeled as Riemannian manifolds. However, using Riemannian tools typically requires redefining core operations (geodesics, norms, integration), making generative modeling computationally inefficient. In this work, we propose DiffeoCFM, an approach that enables conditional flow matching (CFM) on matrix manifolds by exploiting pullback metrics induced by global diffeomorphisms on Euclidean spaces. We show that Riemannian CFM with such metrics is equivalent to applying standard CFM after data transformation. This equivalence allows efficient vector field learning, and fast sampling with standard ODE solvers. We instantiate DiffeoCFM with two different settings: the matrix logarithm for covariance matrices and the normalized Cholesky decomposition for correlation matrices. We evaluate DiffeoCFM on three large-scale fMRI datasets with more than 4600 scans from 2800 subjects (ADNI, ABIDE, OASIS-3) and two EEG motor imagery datasets with over 30000 trials from 26 subjects (BNCI2014-002 and BNCI2015-001). It enables fast training and achieves state-of-the-art performance, all while preserving manifold constraints.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs</title>
<link>https://arxiv.org/abs/2505.18221</link>
<guid>https://arxiv.org/abs/2505.18221</guid>
<content:encoded><![CDATA[
<div> Graph-Based Method, Multimodal Misinformation, Contextualization, Misinformation Detection, Graph Neural Networks

Summary:
The article introduces a novel approach for detecting multi-modal out-of-context (OOC) misinformation, which involves misleading captions paired with real images. Current methods like LLMs and LVLMs struggle with contextualizing the claims before identifying misinformation. The proposed graph-based method utilizes two graph representations - an evidence graph derived from online textual evidence and a claim graph from the caption - to evaluate the consistency between the image and the caption. By employing graph neural networks (GNNs) to encode and compare these representations, the framework can determine the truthfulness of image-caption pairs. The method achieves a detection accuracy of 93.05% on the evaluation set, outperforming popular LLMs by 2.82%. This highlights the effectiveness of smaller and task-specific methods in addressing misinformation detection challenges. 

<br /><br />Summary: <div>
arXiv:2505.18221v1 Announce Type: new 
Abstract: Multimodal out-of-context (OOC) misinformation is misinformation that repurposes real images with unrelated or misleading captions. Detecting such misinformation is challenging because it requires resolving the context of the claim before checking for misinformation. Many current methods, including LLMs and LVLMs, do not perform this contextualization step. LLMs hallucinate in absence of context or parametric knowledge. In this work, we propose a graph-based method that evaluates the consistency between the image and the caption by constructing two graph representations: an evidence graph, derived from online textual evidence, and a claim graph, from the claim in the caption. Using graph neural networks (GNNs) to encode and compare these representations, our framework then evaluates the truthfulness of image-caption pairs. We create datasets for our graph-based method, evaluate and compare our baseline model against popular LLMs on the misinformation detection task. Our method scores $93.05\%$ detection accuracy on the evaluation set and outperforms the second-best performing method (an LLM) by $2.82\%$, making a case for smaller and task-specific methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality</title>
<link>https://arxiv.org/abs/2505.18227</link>
<guid>https://arxiv.org/abs/2505.18227</guid>
<content:encoded><![CDATA[
<div> efficiency, token reduction, generative modeling, multimodal integration, training stability

Summary:<br />
Token reduction in Transformer architectures plays a crucial role beyond just efficiency, impacting model architecture and applications in various domains. It enables deeper multimodal integration and alignment, mitigates "overthinking" and hallucinations, ensures coherence over long inputs, and enhances training stability. By reframing token reduction as a fundamental principle in generative modeling, new directions such as algorithm design, reinforcement learning-guided token reduction, and token optimization for in-context learning emerge. These advancements have the potential to drive the development of new model architectures and learning strategies that improve robustness, interpretability, and alignment with generative modeling objectives. <div>
arXiv:2505.18227v1 Announce Type: new 
Abstract: In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models</title>
<link>https://arxiv.org/abs/2505.18230</link>
<guid>https://arxiv.org/abs/2505.18230</guid>
<content:encoded><![CDATA[
<div> EBMs, Riemannian metrics, geodesics, high-dimensional data, generative modeling<br />
Summary: 
- The study addresses finding the shortest path between two data points in a high-dimensional space, which is challenging due to the curved manifold and the need for Riemannian metrics to describe local curvature.
- A novel method is proposed to derive Riemannian metrics from pretrained Energy-Based Models (EBMs), enabling the computation of geodesics that follow the data manifold's intrinsic geometry.
- Two new metrics derived from EBMs are introduced, showing superior performance in producing geodesics that align with ground-truth trajectories and exhibit lower curvature distortion compared to established baselines.
- Evaluations on synthetic datasets, rotated character images, and high-resolution natural images demonstrate the effectiveness of EBM-derived metrics, especially in high-dimensional settings.
- This work pioneers the derivation of Riemannian metrics from EBMs, facilitating data-aware geodesics and enhancing generative modeling and simulation approaches. <br /><br /> <div>
arXiv:2505.18230v1 Announce Type: new 
Abstract: What is the shortest path between two data points lying in a high-dimensional space? While the answer is trivial in Euclidean geometry, it becomes significantly more complex when the data lies on a curved manifold -- requiring a Riemannian metric to describe the space's local curvature. Estimating such a metric, however, remains a major challenge in high dimensions.
  In this work, we propose a method for deriving Riemannian metrics directly from pretrained Energy-Based Models (EBMs) -- a class of generative models that assign low energy to high-density regions. These metrics define spatially varying distances, enabling the computation of geodesics -- shortest paths that follow the data manifold's intrinsic geometry. We introduce two novel metrics derived from EBMs and show that they produce geodesics that remain closer to the data manifold and exhibit lower curvature distortion, as measured by alignment with ground-truth trajectories. We evaluate our approach on increasingly complex datasets: synthetic datasets with known data density, rotated character images with interpretable geometry, and high-resolution natural images embedded in a pretrained VAE latent space.
  Our results show that EBM-derived metrics consistently outperform established baselines, especially in high-dimensional settings. Our work is the first to derive Riemannian metrics from EBMs, enabling data-aware geodesics and unlocking scalable, geometry-driven learning for generative modeling and simulation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache</title>
<link>https://arxiv.org/abs/2505.18231</link>
<guid>https://arxiv.org/abs/2505.18231</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Vector Quantization, NSNQuant, low-bit compression, calibration-free

Summary:
NSNQuant introduces a calibration-free Vector Quantization technique, aiming to reduce memory usage during Large Language Model inference. By employing a three-step transformation process with Hadamard transform, NSNQuant aligns token distribution with a standard normal distribution, enabling robust compression of the key-value cache without the need for calibration datasets. The method outperforms existing approaches in both 1-bit and 2-bit settings, offering significant throughput gains and strong generalization capabilities. Through extensive experiments, NSNQuant demonstrates up to 3$\times$ throughput improvement over full-precision baselines, showcasing its efficiency and effectiveness in enhancing the performance of Large Language Models. 

<br /><br />Summary: <div>
arXiv:2505.18231v1 Announce Type: new 
Abstract: Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\times$ throughput gain over full-precision baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning</title>
<link>https://arxiv.org/abs/2505.18232</link>
<guid>https://arxiv.org/abs/2505.18232</guid>
<content:encoded><![CDATA[
<div> sparse, language models, pruning, regularization, efficient<br />
Summary:<br />
The article introduces a novel approach called ELDeR for making Large Language Models (LLMs) more efficient by leveraging sparsity and regularization techniques. Unlike traditional prune-then-finetune methods, ELDeR first applies regularization to learn the optimal weights for each transformer layer before pruning. This approach helps preserve valuable information and reduces the need for costly recovery fine-tuning. By minimizing information loss and improving language modeling ability, ELDeR outperforms other pruning methods while significantly reducing computational costs. Furthermore, its layer-wise pruning approach enhances end-to-end acceleration, making it a promising technique for optimizing LLMs. <div>
arXiv:2505.18232v1 Announce Type: new 
Abstract: The deployment of Large language models (LLMs) in many fields is largely hindered by their high computational and memory costs. Recent studies suggest that LLMs exhibit sparsity, which can be used for pruning. Previous pruning methods typically follow a prune-then-finetune paradigm. Since the pruned parts still contain valuable information, statically removing them without updating the remaining parameters often results in irreversible performance degradation, requiring costly recovery fine-tuning (RFT) to maintain performance. To address this, we propose a novel paradigm: first apply regularization, then prune. Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning. We multiply the output of each transformer layer by an initial weight, then we iteratively learn the weights of each transformer layer by using a small amount of data in a simple way. After that, we apply regularization to the difference between the output and input of the layers with smaller weights, forcing the information to be transferred to the remaining layers. Compared with direct pruning, ELDeR reduces the information loss caused by direct parameter removal, thus better preserving the model's language modeling ability. Experimental results show that ELDeR achieves superior performance compared with powerful layer-wise structured pruning methods, while greatly reducing RFT computational costs. Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect is obvious, making it a promising technique for efficient LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POSTER: A Multi-Signal Model for Detecting Evasive Smishing</title>
<link>https://arxiv.org/abs/2505.18233</link>
<guid>https://arxiv.org/abs/2505.18233</guid>
<content:encoded><![CDATA[
<div> phishing, smishing, detection model, linguistic cues, structural cues
<br />
Summary: 
The article discusses the threat of smishing, or SMS-based phishing, to mobile users and presents a multi-channel smishing detection model. It combines country-specific semantic tagging, structural pattern tagging, character-level stylistic cues, and contextual phrase embeddings. The researchers curated and relabeled over 84,000 messages across five datasets, including 24,086 smishing samples. The unified architecture achieved 97.89% accuracy, an F1 score of 0.963, and an AUC of 99.73%, outperforming single-stream models. This work highlights the effectiveness of multi-signal learning in robust and region-aware phishing detection. <div>
arXiv:2505.18233v1 Announce Type: new 
Abstract: Smishing, or SMS-based phishing, poses an increasing threat to mobile users by mimicking legitimate communications through culturally adapted, concise, and deceptive messages, which can result in the loss of sensitive data or financial resources. In such, we present a multi-channel smishing detection model that combines country-specific semantic tagging, structural pattern tagging, character-level stylistic cues, and contextual phrase embeddings. We curated and relabeled over 84,000 messages across five datasets, including 24,086 smishing samples. Our unified architecture achieves 97.89% accuracy, an F1 score of 0.963, and an AUC of 99.73%, outperforming single-stream models by capturing diverse linguistic and structural cues. This work demonstrates the effectiveness of multi-signal learning in robust and region-aware phishing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems</title>
<link>https://arxiv.org/abs/2505.18234</link>
<guid>https://arxiv.org/abs/2505.18234</guid>
<content:encoded><![CDATA[
<div> TabTransformer, Proximal Policy Optimization (PPO), Network intrusion detection system (NIDS), Industrial Internet of Things (IIoT), TON_IoT benchmark <br />
Summary: <br />
The paper introduces a robust NIDS system for IIoT environments to tackle class-imbalanced and few-shot attack scenarios. The proposed model combines a TabTransformer for feature representation with PPO for optimization via policy learning. Evaluation on the TON_IoT benchmark demonstrates high macro F1-score and accuracy, with strong performance even on rare classes like MITM attacks. Ablation experiments confirm the effectiveness of TabTransformer and PPO in mitigating class imbalance and enhancing generalization. The results emphasize the potential of combining transformer-based tabular learning with reinforcement learning for practical NIDS applications. <div>
arXiv:2505.18234v1 Announce Type: new 
Abstract: In this paper, we propose a robust and reinforcement-learning-enhanced network intrusion detection system (NIDS) designed for class-imbalanced and few-shot attack scenarios in Industrial Internet of Things (IIoT) environments. Our model integrates a TabTransformer for effective tabular feature representation with Proximal Policy Optimization (PPO) to optimize classification decisions via policy learning. Evaluated on the TON\textunderscore IoT benchmark, our method achieves a macro F1-score of 97.73\% and accuracy of 98.85\%. Remarkably, even on extremely rare classes like man-in-the-middle (MITM), our model achieves an F1-score of 88.79\%, showcasing strong robustness and few-shot detection capabilities. Extensive ablation experiments confirm the complementary roles of TabTransformer and PPO in mitigating class imbalance and improving generalization. These results highlight the potential of combining transformer-based tabular learning with reinforcement learning for real-world NIDS applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Origins of Representation Manifolds in Large Language Models</title>
<link>https://arxiv.org/abs/2505.18235</link>
<guid>https://arxiv.org/abs/2505.18235</guid>
<content:encoded><![CDATA[
<div> linear representation hypothesis, neural representations, sparse autoencoders, manifold, cosine similarity

Summary:
The article discusses the linear representation hypothesis in AI systems, suggesting that neural representations are sparse linear combinations of almost-orthogonal direction vectors. It explores the idea of moving towards a fuller model of features where neural representations could encode not only the presence but also the continuous and multidimensional value of a feature. The concept of representing a feature as a manifold is introduced, highlighting how cosine similarity in representation space can encode the intrinsic geometry of a feature through shortest, on-manifold paths. This theory is validated using text embeddings and token activations of large language models, demonstrating how distance in representation space and relatedness in concept space could be linked. <div>
arXiv:2505.18235v1 Announce Type: new 
Abstract: There is a large ongoing scientific effort in mechanistic interpretability to map embeddings and internal representations of AI systems into human-understandable concepts. A key element of this effort is the linear representation hypothesis, which posits that neural representations are sparse linear combinations of `almost-orthogonal' direction vectors, reflecting the presence or absence of different features. This model underpins the use of sparse autoencoders to recover features from representations. Moving towards a fuller model of features, in which neural representations could encode not just the presence but also a potentially continuous and multidimensional value for a feature, has been a subject of intense recent discourse. We describe why and how a feature might be represented as a manifold, demonstrating in particular that cosine similarity in representation space may encode the intrinsic geometry of a feature through shortest, on-manifold paths, potentially answering the question of how distance in representation space and relatedness in concept space could be connected. The critical assumptions and predictions of the theory are validated on text embeddings and token activations of large language models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning</title>
<link>https://arxiv.org/abs/2505.18245</link>
<guid>https://arxiv.org/abs/2505.18245</guid>
<content:encoded><![CDATA[
<div> Keywords: urban water demand, Skewed Gaussian Distributions, decomposition, operational planning, synthetic scenarios 

Summary:
This study introduces a novel approach using Skewed Gaussian Distributions (SGD) to decompose urban water demand patterns for operational planning and behavioral insights. The method breaks down daily demand curves into baseline and peak components, characterizing peaks with parameters like amplitude, timing, spread, and skewness. This detailed peak-level decomposition facilitates anomaly detection, real-time demand management, and strategic analyses. SGDs capture asymmetric peak shapes better than traditional models, improving pattern reconstruction accuracy by over 50%. The framework can also generate synthetic demand scenarios with chosen characteristics. The method's effectiveness is demonstrated on real-world datasets, showcasing its superior performance and interpretability over symmetric Gaussian models. Implementation code is publicly available for use. 

<br /><br />Summary: <div>
arXiv:2505.18245v1 Announce Type: new 
Abstract: This study presents a novel approach for decomposing urban water demand patterns using Skewed Gaussian Distributions (SGD) to derive behavioral insights and support operational planning. Hourly demand profiles contain critical information for both long-term infrastructure design and daily operations, influencing network pressures, water quality, energy consumption, and overall reliability. By breaking down each daily demand curve into a baseline component and distinct peak components, the proposed SGD method characterizes each peak with interpretable parameters, including peak amplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby reconstructing the observed pattern and uncovering latent usage dynamics. This detailed peak-level decomposition enables both operational applications, e.g. anomaly and leakage detection, real-time demand management, and strategic analyses, e.g. identifying behavioral shifts, seasonal influences, or policy impacts on consumption patterns. Unlike traditional symmetric Gaussian or purely statistical time-series models, SGDs explicitly capture asymmetric peak shapes such as sharp morning surges followed by gradual declines, improving the fidelity of synthetic pattern generation and enhancing the detection of irregular consumption behavior. The method is demonstrated on several real-world datasets, showing that SGD outperforms symmetric Gaussian models in reconstruction accuracy, reducing root-mean-square error by over 50% on average, while maintaining physical interpretability. The SGD framework can also be used to construct synthetic demand scenarios by designing daily peak profiles with chosen characteristics. All implementation code is publicly available at: https://github.com/Relkayam/water-demand-decomposition-sgd
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks</title>
<link>https://arxiv.org/abs/2505.18266</link>
<guid>https://arxiv.org/abs/2505.18266</guid>
<content:encoded><![CDATA[
<div> Chinese Remainder Theorem, neural networks, modular addition, deep learning, interpretability

Summary: 
The article proposes a universality hypothesis that suggests neural network solutions in modular addition tasks are unified under an abstract algorithm. Through multi-level analyses, it is shown that both multilayer perceptrons and transformers implement the abstract algorithm called the approximate Chinese Remainder Theorem. The introduction of approximate cosets reveals that neurons activate exclusively on them. The theory applies to deep neural networks and predicts that solutions require only O(log n) features. Empirical confirmation supports this prediction. This work provides the first theory-backed interpretation of multilayer networks solving modular addition and enhances generalizable interpretability in neural networks. The hypothesis also opens up possibilities for testing universality in group multiplication beyond modular addition. 

<br /><br />Summary: <div>
arXiv:2505.18266v1 Announce Type: new 
Abstract: We propose a testable universality hypothesis, asserting that seemingly disparate neural network solutions observed in the simple task of modular addition are unified under a common abstract algorithm. While prior work interpreted variations in neuron-level representations as evidence for distinct algorithms, we demonstrate - through multi-level analyses spanning neurons, neuron clusters, and entire networks - that multilayer perceptrons and transformers universally implement the abstract algorithm we call the approximate Chinese Remainder Theorem. Crucially, we introduce approximate cosets and show that neurons activate exclusively on them. Furthermore, our theory works for deep neural networks (DNNs). It predicts that universally learned solutions in DNNs with trainable embeddings or more than one hidden layer require only O(log n) features, a result we empirically confirm. This work thus provides the first theory-backed interpretation of multilayer networks solving modular addition. It advances generalizable interpretability and opens a testable universality hypothesis for group multiplication beyond modular addition.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Action Selection for Large Action-Space Meta-Bandits</title>
<link>https://arxiv.org/abs/2505.18269</link>
<guid>https://arxiv.org/abs/2505.18269</guid>
<content:encoded><![CDATA[
<div> bandit, action space, Gaussian process, epsilon-net algorithm, Thompson Sampling

Summary:
The study investigates the selection of a subset from a large action space for a group of bandits with related payoffs modeled by a Gaussian process. A simple epsilon-net algorithm is proposed to choose a representative subset based on the similarity of actions. The algorithm's performance is supported by theoretical guarantees and compared to Thompson Sampling and Upper Confidence Bound through empirical analysis. The research aims to achieve near-optimal results by utilizing the structure of the action space shared by the bandits. The proposed algorithm offers a promising approach to efficiently select actions in a complex environment with diverse options, showcasing its potential for practical applications in bandit problems. <br /><br />Summary: <div>
arXiv:2505.18269v1 Announce Type: new 
Abstract: We study the problem of selecting a subset from a large action space shared by a family of bandits, with the goal of achieving performance nearly matching that of using the full action space. We assume that similar actions tend to have related payoffs, modeled by a Gaussian process. To exploit this structure, we propose a simple epsilon-net algorithm to select a representative subset. We provide theoretical guarantees for its performance and compare it empirically to Thompson Sampling and Upper Confidence Bound.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior</title>
<link>https://arxiv.org/abs/2505.18280</link>
<guid>https://arxiv.org/abs/2505.18280</guid>
<content:encoded><![CDATA[
<div> Bayesian neural networks, R2D2-Net, prior distributions, variational Gibbs inference algorithm, uncertainty estimation<br />
Summary:<br />
The article introduces a novel approach called R2D2-Net that utilizes the R^2-induced Dirichlet Decomposition (R2D2) prior to address the challenge of selecting appropriate prior distributions in Bayesian neural networks (BNNs). By effectively shrinking irrelevant coefficients towards zero while preventing over-shrinkage of key features, the R2D2-Net improves the performance and stability of BNNs. A variational Gibbs inference algorithm is proposed to accurately approximate the posterior distribution of weights, enhancing estimation consistency. The authors analyze the evidence lower bound (ELBO) and posterior concentration rates theoretically to validate their approach. Experimental results show promising performance on image classification and uncertainty estimation tasks, highlighting the effectiveness of the R2D2-Net in improving BNN performance. <br /> <div>
arXiv:2505.18280v1 Announce Type: new 
Abstract: Bayesian neural networks (BNNs) treat neural network weights as random variables, which aim to provide posterior uncertainty estimates and avoid overfitting by performing inference on the posterior weights. However, the selection of appropriate prior distributions remains a challenging task, and BNNs may suffer from catastrophic inflated variance or poor predictive performance when poor choices are made for the priors. Existing BNN designs apply different priors to weights, while the behaviours of these priors make it difficult to sufficiently shrink noisy signals or they are prone to overshrinking important signals in the weights. To alleviate this problem, we propose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition (R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant coefficients towards zero, while preventing key features from over-shrinkage. To approximate the posterior distribution of weights more accurately, we further propose a variational Gibbs inference algorithm that combines the Gibbs updating procedure and gradient-based optimization. This strategy enhances stability and consistency in estimation when the variational objective involving the shrinkage parameters is non-convex. We also analyze the evidence lower bound (ELBO) and the posterior concentration rates from a theoretical perspective. Experiments on both natural and medical image classification and uncertainty estimation tasks demonstrate satisfactory performance of our method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed</title>
<link>https://arxiv.org/abs/2505.18284</link>
<guid>https://arxiv.org/abs/2505.18284</guid>
<content:encoded><![CDATA[
<div> deep learning, probabilistic forecasting, wind speed, Tube loss function, uncertainty quantification

Summary:
This paper addresses the challenge of Uncertainty Quantification (UQ) in wind speed forecasting for wind power production. The study focuses on designing deep learning based probabilistic forecasting methods using the Tube loss function, which allows for reliable Prediction Interval (PI) estimation without distribution assumptions. The proposed models incorporate popular architectures like LSTM, GRU, and TCN, and a heuristic for tuning the Tube loss function parameter to achieve narrower PIs without sacrificing calibration ability. The research utilizes wind datasets from Jaisalmer, Los Angeles, and San Francisco to demonstrate the effectiveness of the deep forecasting models in producing more reliable and narrower PIs compared to existing probabilistic methods.<br /><br />Summary: <div>
arXiv:2505.18284v1 Announce Type: new 
Abstract: Uncertainty Quantification (UQ) in wind speed forecasting is a critical challenge in wind power production due to the inherently volatile nature of wind. By quantifying the associated risks and returns, UQ supports more effective decision-making for grid operations and participation in the electricity market. In this paper, we design a sequence of deep learning based probabilistic forecasting methods by using the Tube loss function for wind speed forecasting. The Tube loss function is a simple and model agnostic Prediction Interval (PI) estimation approach and can obtain the narrow PI with asymptotical coverage guarantees without any distribution assumption. Our deep probabilistic forecasting models effectively incorporate popular architectures such as LSTM, GRU, and TCN within the Tube loss framework. We further design a simple yet effective heuristic for tuning the $\delta$ parameter of the Tube loss function so that our deep forecasting models obtain the narrower PI without compromising its calibration ability. We have considered three wind datasets, containing the hourly recording of the wind speed, collected from three distinct location namely Jaisalmer, Los Angeles and San Fransico. Our numerical results demonstrate that the proposed deep forecasting models produce more reliable and narrower PIs compared to recently developed probabilistic wind forecasting methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convexified Message-Passing Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.18289</link>
<guid>https://arxiv.org/abs/2505.18289</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Convexified Message Passing Graph Neural Networks, Convex optimization, Reproducing kernel Hilbert space, Generalization guarantees

Summary:
Convexified Message Passing Graph Neural Networks (CGNNs) combine message-passing GNNs with convex optimization, making training efficient and optimal. By mapping filters into a kernel space, CGNNs can be rigorously analyzed. Two-layer CGNNs have proven generalization guarantees, converging to optimal GNN performance. A layer-wise training strategy enables scaling to deeper architectures. Experimental results show CGNNs outperform leading GNN models by 10 to 40 percent in most cases, emphasizing their theoretical strength and practical power. In cases where improvements are marginal, CGNNs still match or slightly exceed baselines, highlighting their robustness. Despite typically relying on over-parameterization, CGNNs achieve superior accuracy and resource efficiency with shallow convex models. The results demonstrate the promise of CGNNs as a principled and effective method with strong theoretical foundations.

<br /><br />Summary: <div>
arXiv:2505.18289v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have become prominent methods for graph representation learning, demonstrating strong empirical results on diverse graph prediction tasks. In this paper, we introduce Convexified Message Passing Graph Neural Networks (CGNNs), a novel and general framework that combines the power of message-passing GNNs with the tractability of convex optimization. By mapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs transform training into a convex optimization problem, which can be solved efficiently and optimally by projected gradient methods. This convexity further allows the statistical properties of CGNNs to be analyzed accurately and rigorously. For two-layer CGNNs, we establish rigorous generalization guarantees, showing convergence to the performance of the optimal GNN. To scale to deeper architectures, we adopt a principled layer-wise training strategy. Experiments on benchmark datasets show that CGNNs significantly exceed the performance of leading GNN models, achieving 10 to 40 percent higher accuracy in most cases, underscoring their promise as a powerful and principled method with strong theoretical foundations. In rare cases where improvements are not quantitatively substantial, the convex models either slightly exceed or match the baselines, stressing their robustness and wide applicability. Though over-parameterization is often employed to enhance performance in nonconvex models, we show that our CGNNs framework yields shallow convex models that can surpass these models in both accuracy and resource efficiency.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs</title>
<link>https://arxiv.org/abs/2505.18300</link>
<guid>https://arxiv.org/abs/2505.18300</guid>
<content:encoded><![CDATA[
<div> framework, Markov Chain Monte Carlo, random walk algorithm, discrete state spaces, efficient sampling<br />
<br />
Summary:
The article introduces the History-Driven Target (HDT) framework in Markov Chain Monte Carlo (MCMC) for improving random walk algorithms on discrete state spaces, particularly general undirected graphs. The HDT framework aims to enhance sampling efficiency from a target distribution by introducing a history-dependent target distribution that replaces the original target. This approach eliminates the need for explicit computation of transition probabilities for all neighbors at each step, reducing computational overhead. The HDT framework is compatible with both reversible and non-reversible MCMC samplers, enabling unbiased samples with the target distribution and achieving near-zero variance performance. Experimental results in graph sampling show consistent performance gains, with a memory-efficient Least Recently Used (LRU) cache ensuring scalability to large general graphs.<br /><br />Summary: <div>
arXiv:2505.18300v1 Announce Type: new 
Abstract: We propose a history-driven target (HDT) framework in Markov Chain Monte Carlo (MCMC) to improve any random walk algorithm on discrete state spaces, such as general undirected graphs, for efficient sampling from target distribution $\boldsymbol{\mu}$. With broad applications in network science and distributed optimization, recent innovations like the self-repellent random walk (SRRW) achieve near-zero variance by prioritizing under-sampled states through transition kernel modifications based on past visit frequencies. However, SRRW's reliance on explicit computation of transition probabilities for all neighbors at each step introduces substantial computational overhead, while its strict dependence on time-reversible Markov chains excludes advanced non-reversible MCMC methods. To overcome these limitations, instead of direct modification of transition kernel, HDT introduces a history-dependent target distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target $\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the empirical measure of past visits. This design preserves lightweight implementation by requiring only local information between the current and proposed states and achieves compatibility with both reversible and non-reversible MCMC samplers, while retaining unbiased samples with target distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive experiments in graph sampling demonstrate consistent performance gains, and a memory-efficient Least Recently Used (LRU) cache ensures scalability to large general graphs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training</title>
<link>https://arxiv.org/abs/2505.18313</link>
<guid>https://arxiv.org/abs/2505.18313</guid>
<content:encoded><![CDATA[
<div> PLUMAGE, Low rank gradient estimators, Large language models, Training, Memory constraints<br />
<br />
Summary:<br />
PLUMAGE is a novel probabilistic low-rank gradient estimator designed to address memory and networking constraints in training large language models with billions of parameters. It offers a drop-in replacement for existing methods without introducing additional hyperparameters and resolves issues related to bias and high estimator variance. By preventing optimizer state misalignment, PLUMAGE enhances training stability and reduces the optimization gap and training loss across models in the GLUE benchmark. Empirical results show that PLUMAGE outperforms existing methods, achieving a 33% reduction in optimization gap and a 28% decrease in training loss, all while maintaining a similar computational and memory footprint as GaLoRE. <div>
arXiv:2505.18313v1 Announce Type: new 
Abstract: Accelerator memory and networking constraints have emerged as dominant bottlenecks when training large language models LLMs with billions of parameters. Existing low rank gradient estimators such as GaLoRE and FLORA compress gradients and optimizer tensors by projecting weight gradients onto a rank r subspace, enabling LLM training on consumer hardware. Yet, these methods are either biased or subject to high estimator variance. Moreover, the optimizer state based on the first and second moments estimates expressed in the previous subspace becomes misaligned whenever the projection is updated, leading to instabilities during training. We propose PLUMAGE: Probabilistic Low rank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in replacement for existing low rank gradient estimators. It does not introduce new hyperparameters beyond the chosen rank r and the update interval. In addition, we resolve optimizer state misalignment issues to prevent spurious weight updates and enhance training stability. We empirically demonstrate that PLUMAGE shrinks the full rank optimization's gap over the pre training evaluation loss by 33% on average across models and the average training loss across the GLUE benchmark by 28% within a similar computational and memory footprint as GaloRE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access</title>
<link>https://arxiv.org/abs/2505.18344</link>
<guid>https://arxiv.org/abs/2505.18344</guid>
<content:encoded><![CDATA[
<div> Diffusion models, sample complexity, score estimation, statistical error, approximation error, optimization error <br />
<br />
Summary: 
This work introduces a principled analysis of score estimation in diffusion models, addressing the sample complexity issue. By decomposing the estimation error into statistical, approximation, and optimization components, the study establishes a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$. Unlike previous analyses, this approach eliminates the exponential dependence on neural network parameters. The analysis does not assume access to the empirical risk minimizer of the score function estimation loss, a significant advancement in theoretical understanding. <div>
arXiv:2505.18344v1 Announce Type: new 
Abstract: Diffusion models have demonstrated state-of-the-art performance across vision, language, and scientific domains. Despite their empirical success, prior theoretical analyses of the sample complexity suffer from poor scaling with input data dimension or rely on unrealistic assumptions such as access to exact empirical risk minimizers. In this work, we provide a principled analysis of score estimation, establishing a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$. Our approach leverages a structured decomposition of the score estimation error into statistical, approximation, and optimization errors, enabling us to eliminate the exponential dependence on neural network parameters that arises in prior analyses. It is the first such result which achieves sample complexity bounds without assuming access to the empirical risk minimizer of score function estimation loss.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Self-Weighted Guidance for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18345</link>
<guid>https://arxiv.org/abs/2505.18345</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, offline, policy, diffusion models, guidance

Summary: 
This study focuses on offline reinforcement learning (RL) and proposes a new approach called Self-Weighted Guidance (SWG) to address challenges in computing scores due to the unknown weight function when modeling the optimal policy. SWG utilizes a diffusion model that incorporates actions and weights, allowing for direct score computation without the need for additional networks. Through experiments on toy examples and D4RL environments, SWG demonstrates its ability to generate samples from the desired distribution and achieve comparable performance to existing methods. Ablation studies confirm the effectiveness of SWG's weight formulations, and its scalability is also validated. Overall, SWG offers a streamlined training pipeline for offline RL tasks, showcasing its potential in improving policy learning with historical observations. 

<br /><br />Summary: <div>
arXiv:2505.18345v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) recovers the optimal policy $\pi$ given historical observations of an agent. In practice, $\pi$ is modeled as a weighted version of the agent's behavior policy $\mu$, using a weight function $w$ working as a critic of the agent's behavior. Though recent approaches to offline RL based on diffusion models have exhibited promising results, the computation of the required scores is challenging due to their dependence on the unknown $w$. In this work, we alleviate this issue by constructing a diffusion over both the actions and the weights. With the proposed setting, the required scores are directly obtained from the diffusion model without learning extra networks. Our main conceptual contribution is a novel guidance method, where guidance (which is a function of $w$) comes from the same diffusion model, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show that SWG generates samples from the desired distribution on toy examples and performs on par with state-of-the-art methods on D4RL's challenging environments, while maintaining a streamlined training pipeline. We further validate SWG through ablation studies on weight formulations and scalability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cell Must Go On: Agar.io for Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18347</link>
<guid>https://arxiv.org/abs/2505.18347</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, continual learning, AgarCL, Agar.io, benchmark results

Summary:
AgarCL is introduced as a research platform for continual reinforcement learning, specifically designed for environments where static policies are ineffective due to changing dynamics. Based on the game Agar.io, AgarCL provides a high-dimensional, non-episodic problem with stochastic, evolving dynamics, continuous actions, and partial observability. The platform allows for the development of increasingly sophisticated behavior over time. Benchmark results on the challenging continual RL problem in AgarCL are provided for DQN, PPO, and SAC algorithms. Additionally, a suite of smaller tasks within AgarCL isolates different aspects of the environment, aiding in understanding the challenges posed by the game's dynamics. This research platform and benchmark results showcase the capabilities of continual reinforcement learning in dynamic environments like Agar.io. 

<br /><br />Summary: <div>
arXiv:2505.18347v1 Announce Type: new 
Abstract: Continual reinforcement learning (RL) concerns agents that are expected to learn continually, rather than converge to a policy that is then fixed for evaluation. Such an approach is well suited to environments the agent perceives as changing, which renders any static policy ineffective over time. The few simulators explicitly designed for empirical research in continual RL are often limited in scope or complexity, and it is now common for researchers to modify episodic RL environments by artificially incorporating abrupt task changes during interaction. In this paper, we introduce AgarCL, a research platform for continual RL that allows for a progression of increasingly sophisticated behaviour. AgarCL is based on the game Agar.io, a non-episodic, high-dimensional problem featuring stochastic, ever-evolving dynamics, continuous actions, and partial observability. Additionally, we provide benchmark results reporting the performance of DQN, PPO, and SAC in both the primary, challenging continual RL problem, and across a suite of smaller tasks within AgarCL, each of which isolates aspects of the full environment and allow us to characterize the challenges posed by different aspects of the game.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?</title>
<link>https://arxiv.org/abs/2505.18350</link>
<guid>https://arxiv.org/abs/2505.18350</guid>
<content:encoded><![CDATA[
<div> framework, task-specific pruning, Large Language Models, parameter reduction, accuracy degradation
<br />
Summary: 
The article introduces LLM-Sieve, a framework for task-specific pruning of Large Language Models (LLMs) that aims to reduce the number of parameters while maintaining high accuracy. LLM-Sieve achieves a parameter reduction of 20-75% with only a 1-5% accuracy degradation across various domains. Unlike previous methods that focus on uniform pruning or low-rank approximations, LLM-Sieve uses task-aware joint projections and a Genetic Algorithm to determine optimal pruning levels for each matrix. The framework is compatible with LoRA fine-tuning and quantization, and demonstrates strong generalization across datasets within the same task domain. Overall, LLM-Sieve provides a practical and robust approach to generating smaller, yet effective, task-specific models. 
<br /> <div>
arXiv:2505.18350v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly being adopted for narrow tasks - such as medical question answering or sentiment analysis - and deployed in resource-constrained settings, a key question arises: how many parameters does a task actually need? In this work, we present LLM-Sieve, the first comprehensive framework for task-specific pruning of LLMs that achieves 20-75% parameter reduction with only 1-5% accuracy degradation across diverse domains. Unlike prior methods that apply uniform pruning or rely on low-rank approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns task-aware joint projections to better approximate output behavior, and (ii) employs a Genetic Algorithm to discover differentiated pruning levels for each matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization, and uniquely demonstrates strong generalization across datasets within the same task domain. Together, these results establish a practical and robust mechanism to generate smaller performant task-specific models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-MethaneWet: A Cross-scale Global Wetland Methane Emission Benchmark Dataset for Advancing Science Discovery with AI</title>
<link>https://arxiv.org/abs/2505.18355</link>
<guid>https://arxiv.org/abs/2505.18355</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, methane fluxes, global wetlands, AI algorithms, transfer learning<br />
Summary:<br />
The article introduces X-MethaneWet, a global wetland methane benchmark dataset merging model simulation data with real-world observations. This dataset aims to enhance methane modeling and science discovery utilizing AI algorithms. Sequential deep learning models were evaluated to predict methane fluxes using X-MethaneWet, with promising results. Additionally, four transfer learning techniques were explored to improve model generalization by leveraging TEM-MDM simulated data. The experiments demonstrated the effectiveness of these approaches, showcasing their potential to advance methane emission modeling and contribute to the development of more accurate and scalable AI-driven climate models. <br /> <br />Summary: <div>
arXiv:2505.18355v1 Announce Type: new 
Abstract: Methane (CH$_4$) is the second most powerful greenhouse gas after carbon dioxide and plays a crucial role in climate change due to its high global warming potential. Accurately modeling CH$_4$ fluxes across the globe and at fine temporal scales is essential for understanding its spatial and temporal variability and developing effective mitigation strategies. In this work, we introduce the first-of-its-kind cross-scale global wetland methane benchmark dataset (X-MethaneWet), which synthesizes physics-based model simulation data from TEM-MDM and the real-world observation data from FLUXNET-CH$_4$. This dataset can offer opportunities for improving global wetland CH$_4$ modeling and science discovery with new AI algorithms. To set up AI model baselines for methane flux prediction, we evaluate the performance of various sequential deep learning models on X-MethaneWet. Furthermore, we explore four different transfer learning techniques to leverage simulated data from TEM-MDM to improve the generalization of deep learning models on real-world FLUXNET-CH$_4$ observations. Our extensive experiments demonstrate the effectiveness of these approaches, highlighting their potential for advancing methane emission modeling and contributing to the development of more accurate and scalable AI-driven climate models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Models, Smarter Learning: The Power of Joint Task Training</title>
<link>https://arxiv.org/abs/2505.18369</link>
<guid>https://arxiv.org/abs/2505.18369</guid>
<content:encoded><![CDATA[
<div> Keywords: task difficulty, model size, ListOps dataset, transformer models, training curriculum

Summary:
The study examines the relationship between task difficulty, model size, and training curriculum in small transformer models using the ListOps dataset. It finds that the sum modulo n operation is the hardest to learn, but when combined with other operations like maximum and median, it becomes easier and requires fewer parameters. Models trained on a mixture of operations exhibit number-like representations and better distinguish parity. The model focusing on only the sum operation tends to memorize rather than capture number structure in embeddings. Interestingly, joint training leads to qualitatively different model behavior, activating the attention mechanism more than feedforward layers. Furthermore, pretraining models on a combination of MAX+MED before pure SUM learning enables learning below the initial threshold. The findings suggest that the training curriculum plays a crucial role in the emergent abilities of language models. 

<br /><br />Summary: <div>
arXiv:2505.18369v1 Announce Type: new 
Abstract: The ability of a model to learn a task depends strongly on both the task difficulty and the model size. We aim to understand how task difficulty relates to the minimum number of parameters required for learning specific tasks in small transformer models. Our study focuses on the ListOps dataset, which consists of nested mathematical operations. We gradually increase task difficulty by introducing new operations or combinations of operations into the training data. We observe that sum modulo n is the hardest to learn. Curiously, when combined with other operations such as maximum and median, the sum operation becomes easier to learn and requires fewer parameters. We show that joint training not only improves performance but also leads to qualitatively different model behavior. We show evidence that models trained only on SUM might be memorizing and fail to capture the number structure in the embeddings. In contrast, models trained on a mixture of SUM and other operations exhibit number-like representations in the embedding space, and a strong ability to distinguish parity. Furthermore, the SUM-only model relies more heavily on its feedforward layers, while the jointly trained model activates the attention mechanism more. Finally, we show that learning pure SUM can be induced in models below the learning threshold of pure SUM, by pretraining them on MAX+MED. Our findings indicate that emergent abilities in language models depend not only on model size, but also the training curriculum.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next-token pretraining implies in-context learning</title>
<link>https://arxiv.org/abs/2505.18373</link>
<guid>https://arxiv.org/abs/2505.18373</guid>
<content:encoded><![CDATA[
<div> ICL, predictability, self-supervised learning, in-distribution, context<br />
<br />
Summary: 
This study argues that in-context learning (ICL) emerges predictably from standard self-supervised next-token pretraining instead of being an exotic property. It explores the principles behind this emergence, focusing on in-distribution ICL and how models adapt to context when trained on token sequences from non-ergodic sources. An information-theoretic framework accurately predicts context-dependent loss reduction dynamics. Experiments with synthetic datasets demonstrate phenomena like phase transitions in training loss and power-law scaling of in-context loss. The study also shows a mathematical coupling between a model's in-context performance on tasks and the tasks seen during pretraining, providing a fundamental explanation for inference-time learning that is independent of architecture and modality. <div>
arXiv:2505.18373v1 Announce Type: new 
Abstract: We argue that in-context learning (ICL) predictably arises from standard self-supervised next-token pretraining, rather than being an exotic emergent property. This work establishes the foundational principles of this emergence by focusing on in-distribution ICL, demonstrating how models necessarily adapt to context when trained on token sequences, especially from non-ergodic sources. Our information-theoretic framework precisely predicts these in-distribution ICL dynamics (i.e., context-dependent loss reduction). We verify this with experiments using synthetic datasets of differing types of correlational structure, reproducing characteristic phenomena like phase transitions in training loss for induction head formation and power-law scaling of in-context loss. We further show that a model's in-context performance on any task is mathematically coupled to the ensemble of tasks seen in pretraining, offering a fundamental explanation, grounded in architecture- and modality-independent principles, for such inference-time learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applications of Modular Co-Design for De Novo 3D Molecule Generation</title>
<link>https://arxiv.org/abs/2505.18392</link>
<guid>https://arxiv.org/abs/2505.18392</guid>
<content:encoded><![CDATA[
<div> Transformer models, de novo molecule generation, 3D structures, Megalodon, generative models <br />
<br />
Summary:
The study introduces Megalodon, a family of scalable transformer models aimed at improving 3D molecule generation in drug discovery. These models incorporate equivariant layers and are trained using a joint objective to enhance molecular generation dynamics. Megalodon outperforms existing models in generating realistic molecular structures, focusing on energy levels. By increasing the model parameters to 40 million, Megalodon generates significantly more valid large molecules and achieves lower energy levels compared to previous generative models. The performance of Megalodon is evaluated on various benchmarks for 3D molecule generation, demonstrating state-of-the-art results in conditional structure generation and structure energy assessments using diffusion and flow matching techniques. <div>
arXiv:2505.18392v1 Announce Type: new 
Abstract: De novo 3D molecule generation is a pivotal task in drug discovery. However, many recent geometric generative models struggle to produce high-quality 3D structures, even if they maintain 2D validity and topological stability. To tackle this issue and enhance the learning of effective molecular generation dynamics, we present Megalodon-a family of scalable transformer models. These models are enhanced with basic equivariant layers and trained using a joint continuous and discrete denoising co-design objective. We assess Megalodon's performance on established molecule generation benchmarks and introduce new 3D structure benchmarks that evaluate a model's capability to generate realistic molecular structures, particularly focusing on energetics. We show that Megalodon achieves state-of-the-art results in 3D molecule generation, conditional structure generation, and structure energy benchmarks using diffusion and flow matching. Furthermore, doubling the number of parameters in Megalodon to 40M significantly enhances its performance, generating up to 49x more valid large molecules and achieving energy levels that are 2-10x lower than those of the best prior generative models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought calibration: Efficient and confident test-time scaling</title>
<link>https://arxiv.org/abs/2505.18404</link>
<guid>https://arxiv.org/abs/2505.18404</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, thought calibration, reasoning trees, hidden representations, model performance 

Summary: 
Thought calibration is proposed as a dynamic approach to determine when to terminate thinking in large language models to balance performance and compute cost. By viewing a language model's thoughts as reasoning trees, novel reasoning plateaus can be identified, allowing for more efficient use of computational resources. Lightweight probes operating on hidden representations are utilized to assess reasoning structure and response consistency. This framework was tested on three reasoning language models across four datasets, showing that thought calibration can maintain model performance with reduced thinking tokens by up to 60% on in-distribution data and up to 20% on out-of-distribution data. This approach offers a promising method for improving the efficiency of large language models without sacrificing performance. 

<br /><br />Summary:  <div>
arXiv:2505.18404v1 Announce Type: new 
Abstract: Reasoning large language models achieve impressive test-time scaling by thinking for longer, but this performance gain comes at significant compute cost. Directly limiting test-time budget hurts overall performance, but not all problems are equally difficult. We propose thought calibration to decide dynamically when thinking can be terminated. To calibrate our decision rule, we view a language model's growing body of thoughts as a nested sequence of reasoning trees, where the goal is to identify the point at which novel reasoning plateaus. We realize this framework through lightweight probes that operate on top of the language model's hidden representations, which are informative of both the reasoning structure and overall consistency of response. Based on three reasoning language models and four datasets, thought calibration preserves model performance with up to a 60% reduction in thinking tokens on in-distribution data, and up to 20% in out-of-distribution data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KL-regularization Itself is Differentially Private in Bandits and RLHF</title>
<link>https://arxiv.org/abs/2505.18407</link>
<guid>https://arxiv.org/abs/2505.18407</guid>
<content:encoded><![CDATA[
<div> Privacy, Differential Privacy, Regularization, Multi-armed Bandits, Reinforcement Learning<br />
<br />
Summary: This article discusses the use of regularization techniques in achieving Differential Privacy (DP) without the need for additional noise injection in data-driven algorithms. The study explores three decision-making problems: multi-armed bandits, linear contextual bandits, and reinforcement learning from human feedback (RLHF) in offline data settings. By incorporating KL-regularization into the learning objective, the action sampled from the resulting stochastic policy can achieve DP. This approach not only provides privacy guarantees but also maintains the performance-enhancing benefits of regularization. The research showcases how leveraging the inherent randomness of existing algorithms can lead to privacy preservation, offering a novel path to achieving DP without compromising algorithm efficiency. Regularization plays a crucial role in ensuring privacy while improving overall algorithm performance in various decision-making scenarios. <br /><br />Summary: <div>
arXiv:2505.18407v1 Announce Type: new 
Abstract: Differential Privacy (DP) provides a rigorous framework for privacy, ensuring the outputs of data-driven algorithms remain statistically indistinguishable across datasets that differ in a single entry. While guaranteeing DP generally requires explicitly injecting noise either to the algorithm itself or to its outputs, the intrinsic randomness of existing algorithms presents an opportunity to achieve DP ``for free''. In this work, we explore the role of regularization in achieving DP across three different decision-making problems: multi-armed bandits, linear contextual bandits, and reinforcement learning from human feedback (RLHF), in offline data settings. We show that adding KL-regularization to the learning objective (a common approach in optimization algorithms) makes the action sampled from the resulting stochastic policy itself differentially private. This offers a new route to privacy guarantees without additional noise injection, while also preserving the inherent advantage of regularization in enhancing performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatentLLM: Attention-Aware Joint Tensor Compression</title>
<link>https://arxiv.org/abs/2505.18413</link>
<guid>https://arxiv.org/abs/2505.18413</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-modal models, model compression, tensor decomposition, benchmark

Summary:
The article presents a new framework for converting large language models (LLMs) and multi-modal models (LMMs) into a reduced-dimension latent structure to improve computational and memory efficiency. The proposed method extends local activation-aware tensor decomposition to a global attention-aware joint tensor decomposition, resulting in enhanced model accuracy compared to existing model compression techniques. By reducing the latent dimension of LLMs/LMMs, the framework demonstrates significant benefits on various benchmark tasks, including multi-modal reasoning. This novel approach offers a promising solution for addressing the computational and memory constraints associated with modern foundation models, such as LLMs and LMMs.<br /><br />Summary: <div>
arXiv:2505.18413v1 Announce Type: new 
Abstract: Modern foundation models such as large language models (LLMs) and large multi-modal models (LMMs) require a massive amount of computational and memory resources. We propose a new framework to convert such LLMs/LMMs into a reduced-dimension latent structure. Our method extends a local activation-aware tensor decomposition to a global attention-aware joint tensor de-composition. Our framework can significantly improve the model accuracy over the existing model compression methods when reducing the latent dimension to realize computationally/memory-efficient LLMs/LLMs. We show the benefit on several benchmark including multi-modal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual Basis Approach for Structured Robust Euclidean Distance Geometry</title>
<link>https://arxiv.org/abs/2505.18414</link>
<guid>https://arxiv.org/abs/2505.18414</guid>
<content:encoded><![CDATA[
<div> Euclidean Distance Matrix, point configuration, outliers, RoDEoDB, recovery<br />
Summary:<br />
The paper introduces RoDEoDB, an algorithmic framework for recovering point configuration from partial observations of Euclidean Distance Matrix (EDM) with corruptions. RoDEoDB leverages a non-orthogonal dual basis to connect the EDM to a positive semi-definite Gram matrix. The algorithm guarantees exact recovery of both the Gram matrix and point configuration under certain conditions. Empirical experiments demonstrate RoDEoDB's superior performance on sensor localization and molecular conformation datasets. The approach addresses the challenge of incomplete and corrupted EDM data arising from using anchor nodes for distance collection. Overall, RoDEoDB shows promise in accurately reconstructing point configuration from partial EDM information with outliers. <br /><br />Summary: <div>
arXiv:2505.18414v1 Announce Type: new 
Abstract: Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean distances of a given point configuration, finds many applications in modern machine learning. This paper considers the setting where only a set of anchor nodes is used to collect the distances between themselves and the rest. In the presence of potential outliers, it results in a structured partial observation on EDM with partial corruptions. Note that an EDM can be connected to a positive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by recent development of non-orthogonal dual basis in optimization, we propose a novel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual Basis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the underlying point configuration. The exact recovery guarantees have been established in terms of both the Gram matrix and point configuration, under some mild conditions. Empirical experiments show superior performance of RoDEoDB on sensor localization and molecular conformation datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of Interactive Nomograms for Predicting Short-Term Survival in ICU Patients with Aplastic Anemia</title>
<link>https://arxiv.org/abs/2505.18421</link>
<guid>https://arxiv.org/abs/2505.18421</guid>
<content:encoded><![CDATA[
<div> Keywords: Aplastic anemia, ICU admission, machine learning, mortality prediction, nomograms

Summary: 
This study utilized the MIMIC-IV database to analyze ICU patients with aplastic anemia, identifying seven key predictors for mortality prediction through machine learning. Logistic regression models outperformed Cox models in predicting 7-, 14-, and 28-day mortality with AUROC values of 0.8227, 0.8311, and 0.8298, respectively. External validation using the eICU database showed AUROCs of 0.7391, 0.7119, and 0.7093. Interactive nomograms based on the logistic regression model were developed to visually estimate individual patient risk, with APS III as a leading predictor. This concise set of predictors provides clinicians with valuable tools for personalized risk stratification and decision-making in ICU patients with aplastic anemia. <br /><br />Summary: <div>
arXiv:2505.18421v1 Announce Type: new 
Abstract: Aplastic anemia is a rare, life-threatening hematologic disorder characterized by pancytopenia and bone marrow failure. ICU admission in these patients often signals critical complications or disease progression, making early risk assessment crucial for clinical decision-making and resource allocation. In this study, we used the MIMIC-IV database to identify ICU patients diagnosed with aplastic anemia and extracted clinical features from five domains: demographics, synthetic indicators, laboratory results, comorbidities, and medications. Over 400 variables were reduced to seven key predictors through machine learning-based feature selection. Logistic regression and Cox regression models were constructed to predict 7-, 14-, and 28-day mortality, and their performance was evaluated using AUROC. External validation was conducted using the eICU Collaborative Research Database to assess model generalizability. Among 1,662 included patients, the logistic regression model demonstrated superior performance, with AUROC values of 0.8227, 0.8311, and 0.8298 for 7-, 14-, and 28-day mortality, respectively, compared to the Cox model. External validation yielded AUROCs of 0.7391, 0.7119, and 0.7093. Interactive nomograms were developed based on the logistic regression model to visually estimate individual patient risk. In conclusion, we identified a concise set of seven predictors, led by APS III, to build validated and generalizable nomograms that accurately estimate short-term mortality in ICU patients with aplastic anemia. These tools may aid clinicians in personalized risk stratification and decision-making at the point of care.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18433</link>
<guid>https://arxiv.org/abs/2505.18433</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized multi-agent reinforcement learning, actor-critic methods, deep neural networks, global optimality guarantee, convergence rate

Summary: 
Actor-critic methods for decentralized multi-agent reinforcement learning (MARL) allow collaborative decision-making without centralized coordination. Previous theoretical studies were limited to stationary solutions using linear function approximation. This paper introduces a deep neural actor-critic method for decentralized MARL with nonlinear components. The proposed method guarantees global optimality with a finite-time convergence rate of O(1/T), a first in the MARL literature for deep neural actor-critic methods. Numerical experiments support the theoretical findings. The research bridges the gap between practical use of deep neural networks in MARL and theoretical understanding, providing a solid foundation for future developments in this area.<br /><br />Summary: <div>
arXiv:2505.18433v1 Announce Type: new 
Abstract: Actor-critic methods for decentralized multi-agent reinforcement learning (MARL) facilitate collaborative optimal decision making without centralized coordination, thus enabling a wide range of applications in practice. To date, however, most theoretical convergence studies for existing actor-critic decentralized MARL methods are limited to the guarantee of a stationary solution under the linear function approximation. This leaves a significant gap between the highly successful use of deep neural actor-critic for decentralized MARL in practice and the current theoretical understanding. To bridge this gap, in this paper, we make the first attempt to develop a deep neural actor-critic method for decentralized MARL, where both the actor and critic components are inherently non-linear. We show that our proposed method enjoys a global optimality guarantee with a finite-time convergence rate of O(1/T), where T is the total iteration times. This marks the first global convergence result for deep neural actor-critic methods in the MARL literature. We also conduct extensive numerical experiments, which verify our theoretical results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces</title>
<link>https://arxiv.org/abs/2505.18441</link>
<guid>https://arxiv.org/abs/2505.18441</guid>
<content:encoded><![CDATA[
<div> Keywords: Dictionary learning, transformer models, sparse autoencoders, scalable algorithm, disentangling embeddings 

Summary: 
DB-KSVD is introduced as a scalable dictionary learning algorithm that builds upon the classic KSVD algorithm, enabling the disentangling of high-dimensional transformer embeddings. The algorithm is designed to handle large datasets with millions of samples and thousands of dimensions efficiently. Through the application of DB-KSVD to the Gemma-2-2B model, competitive results are achieved on six metrics from the SAEBench benchmark, showcasing its efficacy in comparison to established approaches based on sparse autoencoders. The study demonstrates that SAEs can yield strong solutions to the dictionary learning problem and that traditional optimization methods can be adapted to address the challenges posed by high-dimensional data and large sample sizes. The implementation of DB-KSVD is made available at https://github.com/RomeoV/KSVD.jl. 

<br /><br />Summary: <div>
arXiv:2505.18441v1 Announce Type: new 
Abstract: Dictionary learning has recently emerged as a promising approach for mechanistic interpretability of large transformer models. Disentangling high-dimensional transformer embeddings, however, requires algorithms that scale to high-dimensional data with large sample sizes. Recent work has explored sparse autoencoders (SAEs) for this problem. However, SAEs use a simple linear encoder to solve the sparse encoding subproblem, which is known to be NP-hard. It is therefore interesting to understand whether this structure is sufficient to find good solutions to the dictionary learning problem or if a more sophisticated algorithm could find better solutions. In this work, we propose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm that adapts the classic KSVD algorithm. DB-KSVD is informed by the rich theoretical foundations of KSVD but scales to datasets with millions of samples and thousands of dimensions. We demonstrate the efficacy of DB-KSVD by disentangling embeddings of the Gemma-2-2B model and evaluating on six metrics from the SAEBench benchmark, where we achieve competitive results when compared to established approaches based on SAEs. By matching SAE performance with an entirely different optimization approach, our results suggest that (i) SAEs do find strong solutions to the dictionary learning problem and (ii) that traditional optimization approaches can be scaled to the required problem sizes, offering a promising avenue for further research. We provide an implementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.18442</link>
<guid>https://arxiv.org/abs/2505.18442</guid>
<content:encoded><![CDATA[
<div> TimeFuse, Time-series forecasting, Heterogeneous models, Adaptive fusion, Meta-features<br />
<br />
Summary: TimeFuse is introduced as a framework for collective time-series forecasting using adaptive fusion of heterogeneous models at the sample level. The framework leverages meta-features to characterize input time series and trains a fusor to predict optimal fusion weights for different inputs. By jointly training on diverse datasets, TimeFuse can adapt to a wide range of temporal patterns and generalize to unseen data. Experiment results show TimeFuse's effectiveness in long-/short-term forecasting tasks, surpassing state-of-the-art individual models. The code for TimeFuse is available on GitHub for further exploration and implementation. <div>
arXiv:2505.18442v1 Announce Type: new 
Abstract: Time-series forecasting plays a critical role in many real-world applications. Although increasingly powerful models have been developed and achieved superior results on benchmark datasets, through a fine-grained sample-level inspection, we find that (i) no single model consistently outperforms others across different test samples, but instead (ii) each model excels in specific cases. These findings prompt us to explore how to adaptively leverage the distinct strengths of various forecasting models for different samples. We introduce TimeFuse, a framework for collective time-series forecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse utilizes meta-features to characterize input time series and trains a learnable fusor to predict optimal model fusion weights for any given input. The fusor can leverage samples from diverse datasets for joint training, allowing it to adapt to a wide variety of temporal patterns and thus generalize to new inputs, even from unseen datasets. Extensive experiments demonstrate the effectiveness of TimeFuse in various long-/short-term forecasting tasks, achieving near-universal improvement over the state-of-the-art individual models. Code is available at https://github.com/ZhiningLiu1998/TimeFuse.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18447</link>
<guid>https://arxiv.org/abs/2505.18447</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer reinforcement learning, performance guarantees, negative transfer, conservative estimation, distributed algorithms

Summary:
Transfer reinforcement learning aims to transfer policies from source domains to a target domain with limited data. This framework addresses two key challenges: the lack of performance guarantees and the risk of negative transfer. By using the pessimism principle, conservative estimations of the target domain's performance are constructed and optimized to ensure safe and reliable decisions. The framework provides an optimized lower bound on target performance, avoiding undesired actions. Moreover, it exhibits monotonic improvement based on the quality of the source domains, minimizing the risk of negative transfer. Two types of conservative estimations are developed, along with efficient distributed algorithms with convergence guarantees. Overall, this framework offers a theoretically sound and practically robust solution for transfer learning in reinforcement learning.<br /><br />Summary:Transfer reinforcement learning framework addresses challenges of performance guarantees and negative transfer through conservative estimations and distributed algorithms, ensuring safe and reliable decisions with improved policy transfer efficiency. <div>
arXiv:2505.18447v1 Announce Type: new 
Abstract: Transfer reinforcement learning aims to derive a near-optimal policy for a target environment with limited data by leveraging abundant data from related source domains. However, it faces two key challenges: the lack of performance guarantees for the transferred policy, which can lead to undesired actions, and the risk of negative transfer when multiple source domains are involved. We propose a novel framework based on the pessimism principle, which constructs and optimizes a conservative estimation of the target domain's performance. Our framework effectively addresses the two challenges by providing an optimized lower bound on target performance, ensuring safe and reliable decisions, and by exhibiting monotonic improvement with respect to the quality of the source domains, thereby avoiding negative transfer. We construct two types of conservative estimations, rigorously characterize their effectiveness, and develop efficient distributed algorithms with convergence guarantees. Our framework provides a theoretically sound and practically robust solution for transfer learning in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\mu$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2505.18451</link>
<guid>https://arxiv.org/abs/2505.18451</guid>
<content:encoded><![CDATA[
<div> calibration, activation-aware pruning, $\mu$-MoE, structured sparsity, computational efficiency

Summary:
$\mu$-MoE is introduced as a method to address the computational demands of large foundation models by using activation-aware compression techniques without retraining. By utilizing a computationally efficient calibration, $\mu$-MoE can adaptively perform activation-aware pruning for each prompt, reducing complexity during inference. This approach aims to mitigate domain shift that may occur for unknown downstream tasks. Through a mixture of micro-experts, $\mu$-MoE dynamically adjusts to task and prompt-specific structured sparsity in real time. Experimental results support the effectiveness of $\mu$-MoE in achieving task-dependent compression and improved computational efficiency. The technique demonstrates the potential to optimize the performance of large models while maintaining flexibility and adaptability across various tasks and prompts. 

<br /><br />Summary: <div>
arXiv:2505.18451v1 Announce Type: new 
Abstract: To tackle the huge computational demand of large foundation models, activation-aware compression techniques without retraining have been introduced. However, since these rely on calibration data, domain shift may arise for unknown downstream tasks. With a computationally efficient calibration, activation-aware pruning can be executed for every prompt adaptively, yet achieving reduced complexity at inference. We formulate it as a mixture of micro-experts, called $\mu$-MoE. Several experiments demonstrate that $\mu$-MoE can dynamically adapt to task/prompt-dependent structured sparsity on the fly.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation</title>
<link>https://arxiv.org/abs/2505.18461</link>
<guid>https://arxiv.org/abs/2505.18461</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, geolocation, PM2.5 estimation, location encoders, generalizability 

Summary: 
This paper explores the role of geolocation information in deep learning models for estimating daily PM2.5 levels using remote sensing and ground-level data. Three approaches are evaluated: excluding geolocation, using raw geographic coordinates, and utilizing pretrained location encoders. The study focuses on within-region (WR) and out-of-region (OoR) evaluation scenarios. While raw geographic coordinates improve within-region performance, they may hinder generalizability across regions. Pretrained location encoders like GeoCLIP enhance predictive performance and geographic generalizability in both scenarios. However, artifact patterns and varying performance among location encoders are observed. This research provides valuable insights into the impact of incorporating geolocation information in deep learning models for dynamic and spatially heterogeneous applications such as PM2.5 estimation. 

<br /><br />Summary: <div>
arXiv:2505.18461v1 Announce Type: new 
Abstract: Deep learning models have demonstrated success in geospatial applications, yet quantifying the role of geolocation information in enhancing model performance and geographic generalizability remains underexplored. A new generation of location encoders have emerged with the goal of capturing attributes present at any given location for downstream use in predictive modeling. Being a nascent area of research, their evaluation has remained largely limited to static tasks such as species distributions or average temperature mapping. In this paper, we discuss and quantify the impact of incorporating geolocation into deep learning for a real-world application domain that is characteristically dynamic (with fast temporal change) and spatially heterogeneous at high resolutions: estimating surface-level daily PM2.5 levels using remotely sensed and ground-level data. We build on a recently published deep learning-based PM2.5 estimation model that achieves state-of-the-art performance on data observed in the contiguous United States. We examine three approaches for incorporating geolocation: excluding geolocation as a baseline, using raw geographic coordinates, and leveraging pretrained location encoders. We evaluate each approach under within-region (WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance metrics indicate that while na\"ive incorporation of raw geographic coordinates improves within-region performance by retaining the interpolative value of geographic location, it can hinder generalizability across regions. In contrast, pretrained location encoders like GeoCLIP enhance predictive performance and geographic generalizability for both WR and OoR scenarios. However, qualitative analysis reveals artifact patterns caused by high-degree basis functions and sparse upstream samples in certain areas, and ablation results indicate varying performance among location encoders...
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.18475</link>
<guid>https://arxiv.org/abs/2505.18475</guid>
<content:encoded><![CDATA[
<div> Graph learning, LLMs, challenges, incomplete data, class imbalance<br />
<br />
Summary: <br />
Graphs are commonly used to represent non-Euclidean data, but real-world graph data presents challenges such as incompleteness, imbalance, cross-domain heterogeneity, and dynamic instability. Traditional graph learning methods struggle to handle these issues effectively. Large Language Models (LLMs) offer a promising solution by leveraging semantic reasoning and external knowledge. This survey explores how LLMs can be integrated with graph learning to address these challenges. It reviews traditional solutions and modern LLM-driven approaches for each challenge, emphasizing the unique advantages that LLMs bring. The survey also discusses open research questions and potential future directions in this interdisciplinary field. A repository of recent advances on graph learning challenges is provided for further exploration. <div>
arXiv:2505.18475v1 Announce Type: new 
Abstract: Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction. Conventional graph learning approaches typically rely on fixed structural assumptions or fully observed data, limiting their effectiveness in more complex, noisy, or evolving settings. Consequently, real-world graph data often violates the assumptions of traditional graph learning methods, in particular, it leads to four fundamental challenges: (1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways. Recent advances in Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge. This survey provides a comprehensive review of how LLMs can be integrated with graph learning to address the aforementioned challenges. For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages. Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field. To support further exploration, we have curated a repository of recent advances on graph learning challenges: https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Prompt is Mightier than the Example</title>
<link>https://arxiv.org/abs/2505.18485</link>
<guid>https://arxiv.org/abs/2505.18485</guid>
<content:encoded><![CDATA[
<div> Knowledge-Guided Prompting, Prompt Optimization, In-Context Learning, Large Language Models, Synthetic Data Generation
<br />
<br />
Summary:
The paper introduces Knowledge-Guided Prompting (KGP) as a new approach in prompt optimization for large language models (LLMs). It explores how domain knowledge can be injected into prompts to reduce the need for a large number of in-context learning (ICL) examples in synthetic data generation. The study investigates the trade-off between ICL and KGP, showing that the quality of synthetic data can be improved by incorporating domain knowledge into prompts. The experiments reveal an empirical scaling law that quantifies the impact of increasing domain knowledge and decreasing example count on data generation quality. The results suggest that knowledge-guided prompting can be a scalable alternative to or complement of ICL, offering new possibilities for generating high-fidelity synthetic data. <div>
arXiv:2505.18485v1 Announce Type: new 
Abstract: Numerous recent prompt optimization approaches like chain-of-thought, have been demonstrated to significantly improve the quality of content generated by large language models (LLMs). In-context learning (ICL), a recent paradigm where a few representative examples guide content generation has also led to strong improvements in generation quality of LLM generated content. This idea has been applied to great effect in synthetic tabular data generation, where LLMs, through effective use of ICL and prompt optimization, can generate data that approximate samples from complex, heterogeneous distributions based on representative examples. However, ensuring high-fidelity synthetic data often requires a very large number of ICL examples which may be unavailable or costly to obtain. At the same time, as LLMs get larger and larger, their in-built prior knowledge becomes vast and can potentially substitute for specific data examples. In this paper, we introduce Knowledge-Guided Prompting (KGP) as a new knob in prompt optimization and explore the ability of KGP-based prompt optimization to offset the cost of ICL. Specifically, we explore the question `how many examples can a prompt substitute for?' and explore knowledge-guided prompting (KGP) where domain knowledge, either inferred or available, is explicitly injected into the prompt, reducing dependence on ICL examples. Our experiments systematically explore the trade-off between ICL and KGP, revealing an empirical scaling law that quantifies how quality of generated synthetic data varies with increasing domain knowledge and decreasing example count. Our results demonstrate that knowledge-guided prompting can be a scalable alternative, or addition, to in-context examples, unlocking new approaches to synthetic data generation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications</title>
<link>https://arxiv.org/abs/2505.18488</link>
<guid>https://arxiv.org/abs/2505.18488</guid>
<content:encoded><![CDATA[
<div> error correction, language models, mobile applications, dataset, evaluation <br />
<br />
Summary: 
This paper focuses on using large language models (LLMs) to create a dataset for error correction in mobile applications. The authors prompt LLMs with error correction knowledge to generate high-quality data. They then adjust the data distribution to match the mobile application domain by reweighting the samples. The reweighting model is trained on live A/B test metrics to ensure the LLMs perform well in production. Best practices for combining synthetic data with other sources are also discussed to improve model performance in both offline evaluation and live A/B testing. This approach aims to enhance user typing experience on mobile devices by leveraging the capabilities of LLMs in error correction tasks. <div>
arXiv:2505.18488v1 Announce Type: new 
Abstract: Error correction is an important capability when applying large language models (LLMs) to facilitate user typing on mobile devices. In this paper, we use LLMs to synthesize a high-quality dataset of error correction pairs to evaluate and improve LLMs for mobile applications. We first prompt LLMs with error correction domain knowledge to build a scalable and reliable addition to the existing data synthesis pipeline. We then adapt the synthetic data distribution to match the mobile application domain by reweighting the samples. The reweighting model is learnt by predicting (a handful of) live A/B test metrics when deploying LLMs in production, given the LLM performance on offline evaluation data and scores from a small privacy-preserving on-device language model. Finally, we present best practices for mixing our synthetic data with other data sources to improve model performance on error correction in both offline evaluation and production live A/B testing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation</title>
<link>https://arxiv.org/abs/2505.18494</link>
<guid>https://arxiv.org/abs/2505.18494</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Low-Rank Adaptation, Foundation Models, Convergence Guarantees, Gradient Updates <br />
Summary: <br />
The article introduces FedHL, a Federated Learning framework designed for Heterogeneous Low-Rank Adaptation (LoRA). Existing methods lack formal convergence guarantees due to parameter truncation and biased gradient updates. FedHL leverages a full-rank global model as the aggregation basis, eliminating truncation bias and minimizing gradient drift for optimal aggregation weights. The framework ensures a convergence rate of O(1/sqrt{T}) and outperforms state-of-the-art methods by 1-3% in real-world datasets. <div>
arXiv:2505.18494v1 Announce Type: new 
Abstract: Federated Learning (FL) facilitates the fine-tuning of Foundation Models (FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining popularity due to its low communication costs and strong performance. While recent work acknowledges the benefits of heterogeneous LoRA in FL and introduces flexible algorithms to support its implementation, our theoretical analysis reveals a critical gap: existing methods lack formal convergence guarantees due to parameter truncation and biased gradient updates. Specifically, adapting client-specific LoRA ranks necessitates truncating global parameters, which introduces inherent truncation errors and leads to subsequent inaccurate gradient updates that accumulate over training rounds, ultimately degrading performance. To address the above issues, we propose \textbf{FedHL}, a simple yet effective \textbf{Fed}erated Learning framework tailored for \textbf{H}eterogeneous \textbf{L}oRA. By leveraging the full-rank global model as a calibrated aggregation basis, FedHL eliminates the direct truncation bias from initial alignment with client-specific ranks. Furthermore, we derive the theoretically optimal aggregation weights by minimizing the gradient drift term in the convergence upper bound. Our analysis shows that FedHL guarantees $\mathcal{O}(1/\sqrt{T})$ convergence rate, and experiments on multiple real-world datasets demonstrate a 1-3\% improvement over several state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking</title>
<link>https://arxiv.org/abs/2505.18495</link>
<guid>https://arxiv.org/abs/2505.18495</guid>
<content:encoded><![CDATA[
arXiv:2505.18495v1 Announce Type: new 
Abstract: Masked diffusion models (MDM) are powerful generative models for discrete data that generate samples by progressively unmasking tokens in a sequence. Each token can take one of two states: masked or unmasked. We observe that token sequences often remain unchanged between consecutive sampling steps; consequently, the model repeatedly processes identical inputs, leading to redundant computation. To address this inefficiency, we propose the Partial masking scheme (Prime), which augments MDM by allowing tokens to take intermediate states interpolated between the masked and unmasked states. This design enables the model to make predictions based on partially observed token information, and facilitates a fine-grained denoising process. We derive a variational training objective and introduce a simple architectural design to accommodate intermediate-state inputs. Our method demonstrates superior performance across a diverse set of generative modeling tasks. On text data, it achieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM (21.52), autoregressive models (17.54), and their hybrid variants (17.58), without relying on an autoregressive formulation. On image data, it attains competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable to leading continuous generative models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18499</link>
<guid>https://arxiv.org/abs/2505.18499</guid>
<content:encoded><![CDATA[
arXiv:2505.18499v1 Announce Type: new 
Abstract: Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce G1, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate Erd\~os, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1 obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Particle System Theory Enhances Hypergraph Message Passing</title>
<link>https://arxiv.org/abs/2505.18505</link>
<guid>https://arxiv.org/abs/2505.18505</guid>
<content:encoded><![CDATA[
arXiv:2505.18505v1 Announce Type: new 
Abstract: Hypergraphs effectively model higher-order relationships in natural phenomena, capturing complex interactions beyond pairwise connections. We introduce a novel hypergraph message passing framework inspired by interacting particle systems, where hyperedges act as fields inducing shared node dynamics. By incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles of varying classes and features achieve class-dependent equilibrium, enabling separability through the particle-driven message passing. We investigate both first-order and second-order particle system equations for modeling these dynamics, which mitigate over-smoothing and heterophily thus can capture complete interactions. The more stable second-order system permits deeper message passing. Furthermore, we enhance deterministic message passing with stochastic element to account for interaction uncertainties. We prove theoretically that our approach mitigates over-smoothing by maintaining a positive lower bound on the hypergraph Dirichlet energy during propagation and thus to enable hypergraph message passing to go deep. Empirically, our models demonstrate competitive performance on diverse real-world hypergraph node classification tasks, excelling on both homophilic and heterophilic datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs</title>
<link>https://arxiv.org/abs/2505.18511</link>
<guid>https://arxiv.org/abs/2505.18511</guid>
<content:encoded><![CDATA[
arXiv:2505.18511v1 Announce Type: new 
Abstract: Stochastic Partial Differential Equations (SPDEs) driven by random noise play a central role in modelling physical processes whose spatio-temporal dynamics can be rough, such as turbulence flows, superconductors, and quantum dynamics. To efficiently model these processes and make predictions, machine learning (ML)-based surrogate models are proposed, with their network architectures incorporating the spatio-temporal roughness in their design. However, it lacks an extensive and unified datasets for SPDE learning; especially, existing datasets do not account for the computational error introduced by noise sampling and the necessary renormalization required for handling singular SPDEs. We thus introduce SPDEBench, which is designed to solve typical SPDEs of physical significance (e.g., the $\Phi^4_d$, wave, incompressible Navier--Stokes, and KdV equations) on 1D or 2D tori driven by white noise via ML methods. New datasets for singular SPDEs based on the renormalization process have been constructed, and novel ML models achieving the best results to date have been proposed. In particular, we investigate the impact of computational error introduced by noise sampling and renormalization on the performance comparison of ML models and highlight the importance of selecting high-quality test data for accurate evaluation. Results are benchmarked with traditional numerical solvers and ML-based models, including FNO, NSPDE and DLR-Net, etc. It is shown that, for singular SPDEs, naively applying ML models on data without specifying the numerical schemes can lead to significant errors and misleading conclusions. Our SPDEBench provides an open-source codebase that ensures full reproducibility of benchmarking across a variety of SPDE datasets while offering the flexibility to incorporate new datasets and machine learning baselines, making it a valuable resource for the community.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Training Data Attribution with Representational Optimization</title>
<link>https://arxiv.org/abs/2505.18513</link>
<guid>https://arxiv.org/abs/2505.18513</guid>
<content:encoded><![CDATA[
arXiv:2505.18513v1 Announce Type: new 
Abstract: Training data attribution (TDA) methods aim to measure how training data impacts a model's predictions. While gradient-based attribution methods, such as influence functions, offer theoretical grounding, their computational costs make them impractical for large-scale applications. Representation-based approaches are far more scalable, but typically rely on heuristic embeddings that are not optimized for attribution, limiting their fidelity. To address these challenges, we propose AirRep, a scalable, representation-based approach that closes this gap by learning task-specific and model-aligned representations optimized explicitly for TDA. AirRep introduces two key innovations: a trainable encoder tuned for attribution quality, and an attention-based pooling mechanism that enables accurate estimation of group-wise influence. We train AirRep using a ranking objective over automatically constructed training subsets labeled by their empirical effect on target predictions. Experiments on instruction-tuned LLMs demonstrate that AirRep achieves performance on par with state-of-the-art gradient-based approaches while being nearly two orders of magnitude more efficient at inference time. Further analysis highlights its robustness and generalization across tasks and models. Our code is available at https://github.com/sunnweiwei/AirRep.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Adaptation with Binary Feedback</title>
<link>https://arxiv.org/abs/2505.18514</link>
<guid>https://arxiv.org/abs/2505.18514</guid>
<content:encoded><![CDATA[
arXiv:2505.18514v1 Announce Type: new 
Abstract: Deep learning models perform poorly when domain shifts exist between training and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue by adapting pre-trained models using only unlabeled test samples. However, existing TTA methods can fail under severe domain shifts, while recent active TTA approaches requiring full-class labels are impractical due to high labeling costs. To address this issue, we introduce a new setting of TTA with binary feedback. This setting uses a few binary feedback inputs from annotators to indicate whether model predictions are correct, thereby significantly reducing the labeling burden of annotators. Under the setting, we propose BiTTA, a novel dual-path optimization framework that leverages reinforcement learning to balance binary feedback-guided adaptation on uncertain samples with agreement-based self-adaptation on confident predictions. Experiments show BiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines, demonstrating its effectiveness in handling severe distribution shifts with minimal labeling effort. The source code is available at https://github.com/taeckyung/BiTTA.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs</title>
<link>https://arxiv.org/abs/2505.18527</link>
<guid>https://arxiv.org/abs/2505.18527</guid>
<content:encoded><![CDATA[
arXiv:2505.18527v1 Announce Type: new 
Abstract: Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder learning of generalizable representations, leading to more false positives/negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset(SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trials' eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a "pair matching" proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving AUC Fairness in Learning with Noisy Protected Groups</title>
<link>https://arxiv.org/abs/2505.18532</link>
<guid>https://arxiv.org/abs/2505.18532</guid>
<content:encoded><![CDATA[
arXiv:2505.18532v1 Announce Type: new 
Abstract: The Area Under the ROC Curve (AUC) is a key metric for classification, especially under class imbalance, with growing research focus on optimizing AUC over accuracy in applications like medical image analysis and deepfake detection. This leads to fairness in AUC optimization becoming crucial as biases can impact protected groups. While various fairness mitigation techniques exist, fairness considerations in AUC optimization remain in their early stages, with most research focusing on improving AUC fairness under the assumption of clean protected groups. However, these studies often overlook the impact of noisy protected groups, leading to fairness violations in practice. To address this, we propose the first robust AUC fairness approach under noisy protected groups with fairness theoretical guarantees using distributionally robust optimization. Extensive experiments on tabular and image datasets show that our method outperforms state-of-the-art approaches in preserving AUC fairness. The code is in https://github.com/Purdue-M2/AUC_Fairness_with_Noisy_Groups.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD</title>
<link>https://arxiv.org/abs/2505.18535</link>
<guid>https://arxiv.org/abs/2505.18535</guid>
<content:encoded><![CDATA[
arXiv:2505.18535v1 Announce Type: new 
Abstract: We study the convergence properties and escape dynamics of Stochastic Gradient Descent (SGD) in one-dimensional landscapes, separately considering infinite- and finite-variance noise. Our main focus is to identify the time scales on which SGD reliably moves from an initial point to the local minimum in the same ''basin''. Under suitable conditions on the noise distribution, we prove that SGD converges to the basin's minimum unless the initial point lies too close to a local maximum. In that near-maximum scenario, we show that SGD can linger for a long time in its neighborhood. For initial points near a ''sharp'' maximum, we show that SGD does not remain stuck there, and we provide results to estimate the probability that it will reach each of the two neighboring minima. Overall, our findings present a nuanced view of SGD's transitions between local maxima and minima, influenced by both noise characteristics and the underlying function geometry.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-score: Detecting biases in large language models using response history</title>
<link>https://arxiv.org/abs/2505.18545</link>
<guid>https://arxiv.org/abs/2505.18545</guid>
<content:encoded><![CDATA[
arXiv:2505.18545v1 Announce Type: new 
Abstract: Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves in a multi-turn conversation in response to questions that seek an Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: https://b-score.github.io.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning</title>
<link>https://arxiv.org/abs/2505.18558</link>
<guid>https://arxiv.org/abs/2505.18558</guid>
<content:encoded><![CDATA[
arXiv:2505.18558v1 Announce Type: new 
Abstract: Our examination of existing deep generative models (DGMs), including VAEs and GANs, reveals two problems. First, their capability in handling discrete observations and latent codes is unsatisfactory, though there are interesting efforts. Second, both VAEs and GANs optimize some criteria that are indirectly related to the data likelihood. To address these problems, we formally present Joint-stochastic-approximation (JSA) autoencoders - a new family of algorithms for building deep directed generative models, with application to semi-supervised learning. The JSA learning algorithm directly maximizes the data log-likelihood and simultaneously minimizes the inclusive KL divergence the between the posteriori and the inference model. We provide theoretical results and conduct a series of experiments to show its superiority such as being robust to structure mismatch between encoder and decoder, consistent handling of both discrete and continuous variables. Particularly we empirically show that JSA autoencoders with discrete latent space achieve comparable performance to other state-of-the-art DGMs with continuous latent space in semi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the best of our knowledge, this is the first demonstration that discrete latent variable models are successfully applied in the challenging semi-supervised tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
arXiv:2505.18565v1 Announce Type: new 
Abstract: We introduce neural network architectures that combine physics-informed neural networks (PINNs) with the immersed boundary method (IBM) to solve fluid-structure interaction (FSI) problems. Our approach features two distinct architectures: a Single-FSI network with a unified parameter space, and an innovative Eulerian-Lagrangian network that maintains separate parameter spaces for fluid and structure domains. We study each architecture using standard Tanh and adaptive B-spline activation functions. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that the Eulerian-Lagrangian architecture performs significantly better. The adaptive B-spline activation further enhances accuracy by providing locality-aware representation near boundaries. While our methodology shows promising results in predicting the velocity field, pressure recovery remains challenging due to the absence of explicit force-coupling constraints in the current formulation. Our findings underscore the importance of domain-specific architectural design and adaptive activation functions for modeling FSI problems within the PINN framework.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning without Isolation: Pathway Protection for Continual Learning</title>
<link>https://arxiv.org/abs/2505.18568</link>
<guid>https://arxiv.org/abs/2505.18568</guid>
<content:encoded><![CDATA[
arXiv:2505.18568v1 Announce Type: new 
Abstract: Deep networks are prone to catastrophic forgetting during sequential task learning, i.e., losing the knowledge about old tasks upon learning new tasks. To this end, continual learning(CL) has emerged, whose existing methods focus mostly on regulating or protecting the parameters associated with the previous tasks. However, parameter protection is often impractical, since the size of parameters for storing the old-task knowledge increases linearly with the number of tasks, otherwise it is hard to preserve the parameters related to the old-task knowledge. In this work, we bring a dual opinion from neuroscience and physics to CL: in the whole networks, the pathways matter more than the parameters when concerning the knowledge acquired from the old tasks. Following this opinion, we propose a novel CL framework, learning without isolation(LwI), where model fusion is formulated as graph matching and the pathways occupied by the old tasks are protected without being isolated. Thanks to the sparsity of activation channels in a deep network, LwI can adaptively allocate available pathways for a new task, realizing pathway protection and addressing catastrophic forgetting in a parameter-efficient manner. Experiments on popular benchmark datasets demonstrate the superiority of the proposed LwI.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis</title>
<link>https://arxiv.org/abs/2505.18570</link>
<guid>https://arxiv.org/abs/2505.18570</guid>
<content:encoded><![CDATA[
arXiv:2505.18570v1 Announce Type: new 
Abstract: Stock price prediction remains a complex and high-stakes task in financial analysis, traditionally addressed using statistical models or, more recently, language models. In this work, we introduce VISTA (Vision-Language Inference for Stock Time-series Analysis), a novel, training-free framework that leverages Vision-Language Models (VLMs) for multi-modal stock forecasting. VISTA prompts a VLM with both textual representations of historical stock prices and their corresponding line charts to predict future price values. By combining numerical and visual modalities in a zero-shot setting and using carefully designed chain-of-thought prompts, VISTA captures complementary patterns that unimodal approaches often miss. We benchmark VISTA against standard baselines, including ARIMA and text-only LLM-based prompting methods. Experimental results show that VISTA outperforms these baselines by up to 89.83%, demonstrating the effectiveness of multi-modal inference for stock time-series analysis and highlighting the potential of VLMs in financial forecasting tasks without requiring task-specific training.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs</title>
<link>https://arxiv.org/abs/2505.18573</link>
<guid>https://arxiv.org/abs/2505.18573</guid>
<content:encoded><![CDATA[
arXiv:2505.18573v1 Announce Type: new 
Abstract: Reasoning large language models (LLMs) excel in complex tasks, which has drawn significant attention to reinforcement learning (RL) for LLMs. However, existing approaches allocate an equal number of rollouts to all questions during the RL process, which is inefficient. This inefficiency stems from the fact that training on simple questions yields limited gains, whereas more rollouts are needed for challenging questions to sample correct answers. Furthermore, while RL improves response precision, it limits the model's exploration ability, potentially resulting in a performance cap below that of the base model prior to RL. To address these issues, we propose a mechanism for dynamically allocating rollout budgets based on the difficulty of the problems, enabling more efficient RL training. Additionally, we introduce an adaptive dynamic temperature adjustment strategy to maintain the entropy at a stable level, thereby encouraging sufficient exploration. This enables LLMs to improve response precision while preserving their exploratory ability to uncover potential correct pathways. The code and data is available on: https://github.com/LiaoMengqi/E3-RL4LLMs
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power</title>
<link>https://arxiv.org/abs/2505.18579</link>
<guid>https://arxiv.org/abs/2505.18579</guid>
<content:encoded><![CDATA[
arXiv:2505.18579v1 Announce Type: new 
Abstract: Structural health monitoring (SHM) involves sensor deployment, data acquisition, and data interpretation, commonly implemented via a tedious wired system. The information processing in current practice majorly depends on electronic computers, albeit with universal applications, delivering challenges such as high energy consumption and low throughput due to the nature of digital units. In recent years, there has been a renaissance interest in shifting computations from electronic computing units to the use of real physical systems, a concept known as physical computation. This approach provides the possibility of thinking out of the box for SHM, seamlessly integrating sensing and computing into a pure-physical entity, without relying on external electronic power supplies, thereby properly coping with resource-restricted scenarios. The latest advances of metamaterials (MM) hold great promise for this proactive idea. In this paper, we introduce a programmable metamaterial-based sensor (termed as MM-sensor) for physically processing structural vibration information to perform specific SHM tasks, such as structural damage warning (binary classification) in this initiation, without the need for further information processing or resource-consuming, that is, the data collection and analysis are completed in-situ at the sensor level. We adopt the configuration of a locally resonant metamaterial plate (LRMP) to achieve the first fabrication of the MM-sensor. We take advantage of the bandgap properties of LRMP to physically differentiate the dynamic behavior of structures before and after damage. By inversely designing the geometric parameters, our current approach allows for adjustments to the bandgap features. This is effective for engineering systems with a first natural frequency ranging from 9.54 Hz to 81.86 Hz.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks</title>
<link>https://arxiv.org/abs/2505.18591</link>
<guid>https://arxiv.org/abs/2505.18591</guid>
<content:encoded><![CDATA[
arXiv:2505.18591v1 Announce Type: new 
Abstract: Meta-reinforcement learning trains a single reinforcement learning agent on a distribution of tasks to quickly generalize to new tasks outside of the training set at test time. From a Bayesian perspective, one can interpret this as performing amortized variational inference on the posterior distribution over training tasks. Among the various meta-reinforcement learning approaches, a common method is to represent this distribution with a point-estimate using a recurrent neural network. We show how one can augment this point estimate to give full distributions through the Laplace approximation, either at the start of, during, or after learning, without modifying the base model architecture. With our approximation, we are able to estimate distribution statistics (e.g., the entropy) of non-Bayesian agents and observe that point-estimate based methods produce overconfident estimators while not satisfying consistency. Furthermore, when comparing our approach to full-distribution based learning of the task posterior, our method performs on par with variational baselines while having much fewer parameters.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations</title>
<link>https://arxiv.org/abs/2505.18595</link>
<guid>https://arxiv.org/abs/2505.18595</guid>
<content:encoded><![CDATA[
arXiv:2505.18595v1 Announce Type: new 
Abstract: We study offline imitation learning (IL) in cooperative multi-agent settings, where demonstrations have unlabeled mixed quality - containing both expert and suboptimal trajectories. Our proposed solution is structured in two stages: trajectory labeling and multi-agent imitation learning, designed jointly to enable effective learning from heterogeneous, unlabeled data. In the first stage, we combine advances in large language models and preference-based reinforcement learning to construct a progressive labeling pipeline that distinguishes expert-quality trajectories. In the second stage, we introduce MisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn robust policies while addressing the computational complexity of large joint state-action spaces. By extending the popular single-agent DICE framework to multi-agent settings with a new value decomposition and mixing architecture, our method yields a convex policy optimization objective and ensures consistency between global and local policies. We evaluate MisoDICE on multiple standard multi-agent RL benchmarks and demonstrate superior performance, especially when expert data is scarce.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exemplar-Free Continual Learning for State Space Models</title>
<link>https://arxiv.org/abs/2505.18604</link>
<guid>https://arxiv.org/abs/2505.18604</guid>
<content:encoded><![CDATA[
arXiv:2505.18604v1 Announce Type: new 
Abstract: State-Space Models (SSMs) excel at capturing long-range dependencies with structured recurrence, making them well-suited for sequence modeling. However, their evolving internal states pose challenges in adapting them under Continual Learning (CL). This is particularly difficult in exemplar-free settings, where the absence of prior data leaves updates to the dynamic SSM states unconstrained, resulting in catastrophic forgetting. To address this, we propose Inf-SSM, a novel and simple geometry-aware regularization method that utilizes the geometry of the infinite-dimensional Grassmannian to constrain state evolution during CL. Unlike classical continual learning methods that constrain weight updates, Inf-SSM regularizes the infinite-horizon evolution of SSMs encoded in their extended observability subspace. We show that enforcing this regularization requires solving a matrix equation known as the Sylvester equation, which typically incurs $\mathcal{O}(n^3)$ complexity. We develop a $\mathcal{O}(n^2)$ solution by exploiting the structure and properties of SSMs. This leads to an efficient regularization mechanism that can be seamlessly integrated into existing CL methods. Comprehensive experiments on challenging benchmarks, including ImageNet-R and Caltech-256, demonstrate a significant reduction in forgetting while improving accuracy across sequential tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation</title>
<link>https://arxiv.org/abs/2505.18622</link>
<guid>https://arxiv.org/abs/2505.18622</guid>
<content:encoded><![CDATA[
arXiv:2505.18622v1 Announce Type: new 
Abstract: In recent machine learning systems, confidence scores are being utilized more and more to manage selective prediction, whereby a model can abstain from making a prediction when it is unconfident. Yet, conventional metrics like accuracy, expected calibration error (ECE), and area under the risk-coverage curve (AURC) do not capture the actual reliability of predictions. These metrics either disregard confidence entirely, dilute valuable localized information through averaging, or neglect to suitably penalize overconfident misclassifications, which can be particularly detrimental in real-world systems. We introduce two new metrics Confidence-Weighted Selective Accuracy (CWSA) and its normalized variant CWSA+ that offer a principled and interpretable way to evaluate predictive models under confidence thresholds. Unlike existing methods, our metrics explicitly reward confident accuracy and penalize overconfident mistakes. They are threshold-local, decomposable, and usable in both evaluation and deployment settings where trust and risk must be quantified. Through exhaustive experiments on both real-world data sets (MNIST, CIFAR-10) and artificial model variants (calibrated, overconfident, underconfident, random, perfect), we show that CWSA and CWSA+ both effectively detect nuanced failure modes and outperform classical metrics in trust-sensitive tests. Our results confirm that CWSA is a sound basis for developing and assessing selective prediction systems for safety-critical domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.18629</link>
<guid>https://arxiv.org/abs/2505.18629</guid>
<content:encoded><![CDATA[
arXiv:2505.18629v1 Announce Type: new 
Abstract: Large language models (LLMs) suffer from high inference latency due to the auto-regressive decoding process. Speculative decoding accelerates inference by generating multiple draft tokens using a lightweight model and verifying them in parallel. However, existing verification methods rely heavily on distributional consistency while overlooking semantic correctness, thereby limiting the potential speedup of speculative decoding. While some methods employ additional models for relaxed verification of draft tokens, they often fail to generalize effectively to more diverse or open-domain settings. In this work, we propose Reflective Verification, a training-free and semantics-aware approach that achieves a better trade-off between correctness and efficiency. Specifically, we leverage the inherent reflective capacity of LLMs to semantically assess the correctness of draft tokens in parallel during verification. Using prompt-based probing, we obtain both the original and reflective distributions of draft tokens in a single forward pass. The fusion of these distributions enables semantic-level verification of draft tokens that incorporates both consistency and correctness. Experiments across multiple domain benchmarks and model scales demonstrate that our method significantly increases the acceptance length of draft tokens without compromising model performance. Furthermore, we find that the proposed Reflective Verification is orthogonal to existing statistical verification methods, and their combination yields additional 5$\sim$15\% improvements in decoding speed.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymmetric Duos: Sidekicks Improve Uncertainty</title>
<link>https://arxiv.org/abs/2505.18636</link>
<guid>https://arxiv.org/abs/2505.18636</guid>
<content:encoded><![CDATA[
arXiv:2505.18636v1 Announce Type: new 
Abstract: The go-to strategy to apply deep networks in settings where uncertainty informs decisions--ensembling multiple training runs with random initializations--is ill-suited for the extremely large-scale models and practical fine-tuning workflows of today. We introduce a new cost-effective strategy for improving the uncertainty quantification and downstream decisions of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate but much smaller "sidekick" (e.g. a fine-tuned ResNet-34) with a fraction of the computational cost. We propose aggregating the predictions of this \emph{Asymmetric Duo} by simple learned weighted averaging. Surprisingly, despite their inherent asymmetry, the sidekick model almost never harms the performance of the larger model. In fact, across five image classification benchmarks and a variety of model architectures and training schemes (including soups), Asymmetric Duos significantly improve accuracy, uncertainty quantification, and selective classification metrics with only ${\sim}10-20\%$ more computation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2505.18640</link>
<guid>https://arxiv.org/abs/2505.18640</guid>
<content:encoded><![CDATA[
arXiv:2505.18640v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of foundation models due to its efficiency and zero additional inference cost. Many real-world applications require foundation models to specialize in multiple tasks simultaneously, motivating the need for efficient multi-task adaptation. While recent approaches integrate LoRA with mixture-of-experts (MoE) to address this, the use of routers prevents parameter mergeability, which increases inference overhead and hinders unified multi-task adaptation, thereby limiting deployment practicality. In this work, we propose ThanoRA, a Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables multi-task adaptation while preserving the inference efficiency of LoRA. ThanoRA jointly models task heterogeneity and mitigates subspace interference throughout training. Specifically, motivated by inherent differences in complexity and heterogeneity across tasks, ThanoRA constructs task-specific LoRA subspaces at initialization, enabling fine-grained knowledge injection aligned with task heterogeneity. Furthermore, to prevent task interference and subspace collapse during multi-task training, ThanoRA introduces a subspace-preserving regularization that maintains the independence of task-specific representations. With the synergy of both components, ThanoRA enables efficient and unified multi-task adaptation. Extensive experiments across multimodal and text-only benchmarks under varying multi-task mixtures demonstrate that ThanoRA consistently achieves robust and superior performance over strong baselines without introducing additional inference overhead. Our code is publicly available at: https://github.com/LiangJian24/ThanoRA.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching for Geometric Trajectory Simulation</title>
<link>https://arxiv.org/abs/2505.18647</link>
<guid>https://arxiv.org/abs/2505.18647</guid>
<content:encoded><![CDATA[
arXiv:2505.18647v1 Announce Type: new 
Abstract: The simulation of N-body systems is a fundamental problem with applications in a wide range of fields, such as molecular dynamics, biochemistry, and pedestrian dynamics. Machine learning has become an invaluable tool for scaling physics-based simulators and developing models directly from experimental data. In particular, recent advances based on deep generative modeling and geometric deep learning have enabled probabilistic simulation by modeling complex distributions over trajectories while respecting the permutation symmetry that is fundamental to N-body systems. However, to generate realistic trajectories, existing methods must learn complex transformations starting from uninformed noise and do not allow for the exploitation of domain-informed priors. In this work, we propose STFlow to address this limitation. By leveraging flow matching and data-dependent couplings, STFlow facilitates physics-informed simulation of geometric trajectories without sacrificing model expressivity or scalability. Our evaluation on N-body dynamical systems, molecular dynamics, and pedestrian dynamics benchmarks shows that STFlow produces significantly lower prediction errors while enabling more efficient inference, highlighting the benefits of employing physics-informed prior distributions in probabilistic geometric trajectory modeling.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-QFL: Distilling Large Language Model for Quantum Federated Learning</title>
<link>https://arxiv.org/abs/2505.18656</link>
<guid>https://arxiv.org/abs/2505.18656</guid>
<content:encoded><![CDATA[
arXiv:2505.18656v1 Announce Type: new 
Abstract: Inspired by the power of large language models (LLMs), our research adapts them to quantum federated learning (QFL) to boost efficiency and performance. We propose a federated fine-tuning method that distills an LLM within QFL, allowing each client to locally adapt the model to its own data while preserving privacy and reducing unnecessary global updates. The fine-tuned LLM also acts as a reinforcement agent, optimizing QFL by adjusting optimizer steps, cutting down communication rounds, and intelligently selecting clients. Experiments show significant efficiency gains. We pioneer a synergy between LLM and QFL, offering: i) practical efficiency: Reduced communication costs and faster convergence. ii) theoretical rigor: Provable guarantees for adaptive federated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable deployment on resource-constrained quantum devices. Code implementation is available here 1.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.18671</link>
<guid>https://arxiv.org/abs/2505.18671</guid>
<content:encoded><![CDATA[
arXiv:2505.18671v1 Announce Type: new 
Abstract: We introduce an encoder-only approach to learn the evolution operators of large-scale non-linear dynamical systems, such as those describing complex natural phenomena. Evolution operators are particularly well-suited for analyzing systems that exhibit complex spatio-temporal patterns and have become a key analytical tool across various scientific communities. As terabyte-scale weather datasets and simulation tools capable of running millions of molecular dynamics steps per day are becoming commodities, our approach provides an effective tool to make sense of them from a data-driven perspective. The core of it lies in a remarkable connection between self-supervised representation learning methods and the recently established learning theory of evolution operators. To show the usefulness of the proposed method, we test it across multiple scientific domains: explaining the folding dynamics of small proteins, the binding process of drug-like molecules in host sites, and autonomously finding patterns in climate data. Code and data to reproduce the experiments are made available open source.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?</title>
<link>https://arxiv.org/abs/2505.18672</link>
<guid>https://arxiv.org/abs/2505.18672</guid>
<content:encoded><![CDATA[
arXiv:2505.18672v1 Announce Type: new 
Abstract: Representation intervention aims to locate and modify the representations that encode the underlying concepts in Large Language Models (LLMs) to elicit the aligned and expected behaviors. Despite the empirical success, it has never been examined whether one could locate the faithful concepts for intervention. In this work, we explore the question in safety alignment. If the interventions are faithful, the intervened LLMs should erase the harmful concepts and be robust to both in-distribution adversarial prompts and the out-of-distribution (OOD) jailbreaks. While it is feasible to erase harmful concepts without degrading the benign functionalities of LLMs in linear settings, we show that it is infeasible in the general non-linear setting. To tackle the issue, we propose Concept Concentration (COCA). Instead of identifying the faithful locations to intervene, COCA refractors the training data with an explicit reasoning process, which firstly identifies the potential unsafe concepts and then decides the responses. Essentially, COCA simplifies the decision boundary between harmful and benign representations, enabling more effective linear erasure. Extensive experiments with multiple representation intervention methods and model architectures demonstrate that COCA significantly reduces both in-distribution and OOD jailbreak success rates, and meanwhile maintaining strong performance on regular tasks such as math and code generation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor</title>
<link>https://arxiv.org/abs/2505.18693</link>
<guid>https://arxiv.org/abs/2505.18693</guid>
<content:encoded><![CDATA[
arXiv:2505.18693v1 Announce Type: new 
Abstract: Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a cost-effective and stable alternative to conventional architectures, utilizing only an absorber layer and an electron transport layer (ETL). This study presents a machine learning (ML)-driven framework to optimize the efficiency and stability of HTL-free PSCs by integrating experimental validation with numerical simulations. Excellent agreement is achieved between a fabricated device and its simulated counterpart at a molar fraction \( x = 68.7\% \) in \(\mathrm{MAPb}_{1-x}\mathrm{Sb}_{2x/3}\mathrm{I}_3\), where MA is methylammonium. A dataset of 1650 samples is generated by varying molar fraction, absorber defect density, thickness, and ETL doping, with corresponding efficiency and 50-hour degradation as targets. A fourth-degree polynomial regressor (PR-4) shows the best performance, achieving RMSEs of 0.0179 and 0.0117, and \( R^2 \) scores of 1 and 0.999 for efficiency and degradation, respectively. The derived model generalizes beyond the training range and is used in an L-BFGS-B optimization algorithm with a weighted objective function to maximize efficiency and minimize degradation. This improves device efficiency from 13.7\% to 16.84\% and reduces degradation from 6.61\% to 2.39\% over 1000 hours. Finally, the dataset is labeled into superior and inferior classes, and a multilayer perceptron (MLP) classifier achieves 100\% accuracy, successfully identifying optimal configurations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study</title>
<link>https://arxiv.org/abs/2505.18697</link>
<guid>https://arxiv.org/abs/2505.18697</guid>
<content:encoded><![CDATA[
arXiv:2505.18697v1 Announce Type: new 
Abstract: Nowadays, real-world data, including graph-structure data, often arrives in a streaming manner, which means that learning systems need to continuously acquire new knowledge without forgetting previously learned information. Although substantial existing works attempt to address catastrophic forgetting in graph machine learning, they are all based on training from scratch with streaming data. With the rise of pretrained models, an increasing number of studies have leveraged their strong generalization ability for continual learning. Therefore, in this work, we attempt to answer whether large language models (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning (GCL). We first point out that current experimental setups for GCL have significant flaws, as the evaluation stage may lead to task ID leakage. Then, we evaluate the performance of LLMs in more realistic scenarios and find that even minor modifications can lead to outstanding results. Finally, based on extensive experiments, we propose a simple-yet-effective method, Simple Graph Continual Learning (SimGCL), that surpasses the previous state-of-the-art GNN-based baseline by around 20% under the rehearsal-free constraint. To facilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL for training and evaluating existing GCL methods. The code is available at: https://github.com/ZhixunLEE/LLM4GCL.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention</title>
<link>https://arxiv.org/abs/2505.18698</link>
<guid>https://arxiv.org/abs/2505.18698</guid>
<content:encoded><![CDATA[
arXiv:2505.18698v1 Announce Type: new 
Abstract: Transformers have achieved state-of-the-art performance across various tasks, but suffer from a notable quadratic complexity in sequence length due to the attention mechanism. In this work, we propose MonarchAttention -- a novel approach to sub-quadratic attention approximation via Monarch matrices, an expressive class of structured matrices. Based on the variational form of softmax, we describe an efficient optimization-based algorithm to compute an approximate projection of softmax attention onto the class of Monarch matrices with $\Theta(N\sqrt{N} d)$ computational complexity and $\Theta(Nd)$ memory/IO complexity. Unlike previous approaches, MonarchAttention is both (1) transferable, yielding minimal performance loss with no additional training, even when replacing every attention layer of the transformer, and (2) hardware-efficient, utilizing the highest-throughput tensor core units on modern GPUs. With optimized kernels, MonarchAttention achieves substantial speed-ups in wall-time over FlashAttention-2: $1.4\times$ for shorter sequences $(N=256)$, $4.5\times$ for medium-length sequences $(N=4K)$, and $8.2\times$ for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention on diverse tasks and architectures in vision and language problems, showing that it flexibly and accurately approximates softmax attention in a variety of contexts. Our code is available at https://github.com/cjyaras/monarch-attention.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLM Reasoning Through Bias-Only Adaptation</title>
<link>https://arxiv.org/abs/2505.18706</link>
<guid>https://arxiv.org/abs/2505.18706</guid>
<content:encoded><![CDATA[
arXiv:2505.18706v1 Announce Type: new 
Abstract: Recent work on reasoning-oriented language models, exemplified by o1-like systems, suggests that reinforcement-learning (RL) finetuning does not create new capabilities but instead strengthens reasoning patterns already latent in the pretrained network. We test this claim by training steering vectors: layer-wise biases that additively amplify selected hidden features while leaving all original weights unchanged. Experiments on four base models across the GSM8K and MATH benchmarks show that steering vectors recover, and in several cases exceed, the accuracy of fully-tuned counterparts. This result supports the view that the required reasoning skills pre-exist in the base model. Further, logit-lens analysis reveals that the trained vectors consistently boost token groups linked to structured languages and logical connectors, providing an interpretable account that aligns with the demands of quantitative reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer</title>
<link>https://arxiv.org/abs/2505.18713</link>
<guid>https://arxiv.org/abs/2505.18713</guid>
<content:encoded><![CDATA[
arXiv:2505.18713v1 Announce Type: new 
Abstract: Foundation models and their checkpoints have significantly advanced deep learning, boosting performance across various applications. However, fine-tuned models often struggle outside their specific domains and exhibit considerable redundancy. Recent studies suggest that combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting, reduce interference when merging model parameters across tasks, and improve compression efficiency. In this context, developing an effective pruning strategy for fine-tuned models is crucial. Leveraging the advantages of the task vector mechanism, we preprocess fine-tuned models by calculating the differences between them and the original model. Recognizing that different task vector subspaces contribute variably to model performance, we introduce a novel method called Neural Parameter Search (NPS-Pruning) for slimming down fine-tuned models. This method enhances pruning efficiency by searching through neural parameters of task vectors within low-rank subspaces. Our method has three key applications: enhancing knowledge transfer through pairwise model interpolation, facilitating effective knowledge fusion via model merging, and enabling the deployment of compressed models that retain near-original performance while significantly reducing storage costs. Extensive experiments across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness and robustness of our approach, resulting in substantial performance gains. The code is publicly available at: https://github.com/duguodong7/NPS-Pruning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.18724</link>
<guid>https://arxiv.org/abs/2505.18724</guid>
<content:encoded><![CDATA[
arXiv:2505.18724v1 Announce Type: new 
Abstract: Quantization and fine-tuning are crucial for deploying large language models (LLMs) on resource-constrained edge devices. However, fine-tuning quantized models presents significant challenges, primarily stemming from: First, the mismatch in data types between the low-precision quantized weights (e.g., 4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch limits the computational efficiency advantage offered by quantized weights during inference. Second, potential accuracy degradation when merging these high-precision adaptation weights into the low-precision quantized weights, as the adaptation weights often necessitate approximation or truncation. Third, as far as we know, no existing methods support the lossless merging of adaptation while adjusting all quantized weights. To address these challenges, we introduce lossless ternary adaptation for quantization-aware fine-tuning (LoTA-QAF). This is a novel fine-tuning method specifically designed for quantized LLMs, enabling the lossless merging of ternary adaptation weights into quantized weights and the adjustment of all quantized weights. LoTA-QAF operates through a combination of: i) A custom-designed ternary adaptation (TA) that aligns ternary weights with the quantization grid and uses these ternary weights to adjust quantized weights. ii) A TA-based mechanism that enables the lossless merging of adaptation weights. iii) Ternary signed gradient descent (t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and Qwen-2.5 model families and validate its effectiveness on several downstream tasks. On the MMLU benchmark, our method effectively recovers performance for quantized models, surpassing 16-bit LoRA by up to 5.14\%. For task-specific fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still outperforms other methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.18728</link>
<guid>https://arxiv.org/abs/2505.18728</guid>
<content:encoded><![CDATA[
arXiv:2505.18728v1 Announce Type: new 
Abstract: The recent success of State-Space Models (SSMs) in sequence modeling has motivated their adaptation to graph learning, giving rise to Graph State-Space Models (GSSMs). However, existing GSSMs operate by applying SSM modules to sequences extracted from graphs, often compromising core properties such as permutation equivariance, message-passing compatibility, and computational efficiency. In this paper, we introduce a new perspective by embedding the key principles of modern SSM computation directly into the Message-Passing Neural Network framework, resulting in a unified methodology for both static and temporal graphs. Our approach, MP-SSM, enables efficient, permutation-equivariant, and long-range information propagation while preserving the architectural simplicity of message passing. Crucially, MP-SSM enables an exact sensitivity analysis, which we use to theoretically characterize information flow and evaluate issues like vanishing gradients and over-squashing in the deep regime. Furthermore, our design choices allow for a highly optimized parallel implementation akin to modern SSMs. We validate MP-SSM across a wide range of tasks, including node classification, graph property prediction, long-range benchmarks, and spatiotemporal forecasting, demonstrating both its versatility and strong empirical performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction</title>
<link>https://arxiv.org/abs/2505.18731</link>
<guid>https://arxiv.org/abs/2505.18731</guid>
<content:encoded><![CDATA[
arXiv:2505.18731v1 Announce Type: new 
Abstract: Reward-driven proactive dialogue agents require precise estimation of user satisfaction as an intrinsic reward signal to determine optimal interaction strategies. Specifically, this framework triggers clarification questions when detecting potential user dissatisfaction during interactions in the industrial dialogue system. Traditional works typically rely on training a neural network model based on weak labels which are generated by a simple model trained on user actions after current turn. However, existing methods suffer from two critical limitations in real-world scenarios: (1) Noisy Reward Supervision, dependence on weak labels derived from post-hoc user actions introduces bias, particularly failing to capture satisfaction signals in ASR-error-induced utterances; (2) Long-Tail Feedback Sparsity, the power-law distribution of user queries causes reward prediction accuracy to drop in low-frequency domains. The noise in the weak labels and a power-law distribution of user utterances results in that the model is hard to learn good representation of user utterances and sessions. To address these limitations, we propose two auxiliary tasks to improve the representation learning of user utterances and sessions that enhance user satisfaction prediction. The first one is a contrastive self-supervised learning task, which helps the model learn the representation of rare user utterances and identify ASR errors. The second one is a domain-intent classification task, which aids the model in learning the representation of user sessions from long-tailed domains and improving the model's performance on such domains. The proposed method is evaluated on DuerOS, demonstrating significant improvements in the accuracy of error recognition on rare user utterances and long-tailed domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping</title>
<link>https://arxiv.org/abs/2505.18738</link>
<guid>https://arxiv.org/abs/2505.18738</guid>
<content:encoded><![CDATA[
arXiv:2505.18738v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA faces an inherent low-rank bottleneck: narrowing its performance gap with full finetuning requires increasing the rank of its parameter matrix, resulting in significant parameter overhead. Recent linear LoRA variants have attempted to enhance expressiveness by introducing additional linear mappings; however, their composition remains inherently linear and fails to fundamentally improve LoRA's representational capacity. To address this limitation, we propose AuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear projectors to capture fixed and learnable nonlinearities. This combination forms an MLP-like structure with a compressed rank, enabling flexible and precise approximation of diverse target functions while theoretically guaranteeing lower approximation errors and bounded gradients. Extensive experiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I) not only matches or surpasses full fine-tuning performance with only 6.18% ~ 25% of LoRA's parameters but also (II) outperforms state-of-the-art PEFT methods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust performance across various rank configurations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation</title>
<link>https://arxiv.org/abs/2505.18755</link>
<guid>https://arxiv.org/abs/2505.18755</guid>
<content:encoded><![CDATA[
arXiv:2505.18755v1 Announce Type: new 
Abstract: With the proliferation of smart grids, smart cities face growing challenges due to cyber-attacks and sophisticated electricity theft behaviors, particularly in residential photovoltaic (PV) generation systems. Traditional Electricity Theft Detection (ETD) methods often struggle to capture complex temporal dependencies and integrating multi-source data, limiting their effectiveness. In this work, we propose an efficient ETD method that accurately identifies fraudulent behaviors in residential PV generation, thus ensuring the supply-demand balance in smart cities. Our hybrid deep learning model, combining multi-scale Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, excels in capturing both short-term and long-term temporal dependencies. Additionally, we introduce a data embedding technique that seamlessly integrates time-series data with discrete temperature variables, enhancing detection robustness. Extensive simulation experiments using real-world data validate the effectiveness of our approach, demonstrating significant improvements in the accuracy of detecting sophisticated energy theft activities, thereby contributing to the stability and fairness of energy systems in smart cities.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding</title>
<link>https://arxiv.org/abs/2505.18758</link>
<guid>https://arxiv.org/abs/2505.18758</guid>
<content:encoded><![CDATA[
arXiv:2505.18758v1 Announce Type: new 
Abstract: The ever-growing size of neural networks poses serious challenges on resource-constrained devices, such as embedded sensors. Compression algorithms that reduce their size can mitigate these problems, provided that model performance stays close to the original. We propose a novel post-training compression framework that combines rate-aware quantization with entropy coding by (1) extending the well-known layer-wise loss by a quadratic rate estimation, and (2) providing locally exact solutions to this modified objective following the Optimal Brain Surgeon (OBS) method. Our method allows for very fast decoding and is compatible with arbitrary quantization grids. We verify our results empirically by testing on various computer-vision networks, achieving a 20-40\% decrease in bit rate at the same performance as the popular compression algorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18763</link>
<guid>https://arxiv.org/abs/2505.18763</guid>
<content:encoded><![CDATA[
arXiv:2505.18763v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO's superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization</title>
<link>https://arxiv.org/abs/2505.18765</link>
<guid>https://arxiv.org/abs/2505.18765</guid>
<content:encoded><![CDATA[
arXiv:2505.18765v1 Announce Type: new 
Abstract: We address the optimization problem of simultaneously minimizing multiple objective functionals over a family of probability distributions. This type of Multi-Objective Distributional Optimization commonly arises in machine learning and statistics, with applications in areas such as multiple target sampling, multi-task learning, and multi-objective generative modeling. To solve this problem, we propose an iterative particle-based algorithm, which we call Muliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of intermediate empirical distributions, each being represented by a set of particles, which gradually minimize the multiple objective functionals simultaneously. Specifically, MWGraD consists of two key steps at each iteration. First, it estimates the Wasserstein gradient for each objective functional based on the current particles. Then, it aggregates these gradients into a single Wasserstein gradient using dynamically adjusted weights and updates the particles accordingly. In addition, we provide theoretical analysis and present experimental results on both synthetic and real-world datasets, demonstrating the effectiveness of MWGraD.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HD-PiSSA: High-Rank Distributed Orthogonal Adaptation</title>
<link>https://arxiv.org/abs/2505.18777</link>
<guid>https://arxiv.org/abs/2505.18777</guid>
<content:encoded><![CDATA[
arXiv:2505.18777v1 Announce Type: new 
Abstract: Existing parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank subspaces, limiting their expressiveness and leading to suboptimal performance on complex tasks. To address this, we introduce High-rank Distributed PiSSA (HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters across different devices and aggregates their delta updates collectively on W for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical adapters across all devices, HD-PiSSA assigns different principal components of the pre-trained weights to each GPU, significantly expanding the range of update directions. This results in over 16x higher effective updated ranks than data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device adapter rank. Empirically, we evaluate HD-PiSSA across various challenging downstream tasks, including mathematics, code generation, and multi-task learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0 absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12 benchmarks, demonstrating its benefits from the extra optimization flexibility.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains</title>
<link>https://arxiv.org/abs/2505.18781</link>
<guid>https://arxiv.org/abs/2505.18781</guid>
<content:encoded><![CDATA[
arXiv:2505.18781v1 Announce Type: new 
Abstract: The very challenging task of learning solution operators of PDEs on arbitrary domains accurately and efficiently is of vital importance to engineering and industrial simulations. Despite the existence of many operator learning algorithms to approximate such PDEs, we find that accurate models are not necessarily computationally efficient and vice versa. We address this issue by proposing a geometry aware operator transformer (GAOT) for learning PDEs on arbitrary domains. GAOT combines novel multiscale attentional graph neural operator encoders and decoders, together with geometry embeddings and (vision) transformer processors to accurately map information about the domain and the inputs into a robust approximation of the PDE solution. Multiple innovations in the implementation of GAOT also ensure computational efficiency and scalability. We demonstrate this significant gain in both accuracy and efficiency of GAOT over several baselines on a large number of learning tasks from a diverse set of PDEs, including achieving state of the art performance on a large scale three-dimensional industrial CFD dataset.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Weighted Machine Unlearning</title>
<link>https://arxiv.org/abs/2505.18783</link>
<guid>https://arxiv.org/abs/2505.18783</guid>
<content:encoded><![CDATA[
arXiv:2505.18783v1 Announce Type: new 
Abstract: Machine unlearning, as a post-hoc processing technique, has gained widespread adoption in addressing challenges like bias mitigation and robustness enhancement, colloquially, machine unlearning for fairness and robustness. However, existing non-privacy unlearning-based solutions persist in using binary data removal framework designed for privacy-driven motivation, leading to significant information loss, a phenomenon known as over-unlearning. While over-unlearning has been largely described in many studies as primarily causing utility degradation, we investigate its fundamental causes and provide deeper insights in this work through counterfactual leave-one-out analysis. In this paper, we introduce a weighted influence function that assigns tailored weights to each sample by solving a convex quadratic programming problem analytically. Building on this, we propose a soft-weighted framework enabling fine-grained model adjustments to address the over-unlearning challenge. We demonstrate that the proposed soft-weighted scheme is versatile and can be seamlessly integrated into most existing unlearning algorithms. Extensive experiments show that in fairness- and robustness-driven tasks, the soft-weighted scheme significantly outperforms hard-weighted schemes in fairness/robustness metrics and alleviates the decline in utility metric, thereby enhancing machine unlearning algorithm as an effective correction solution.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Per-Instance Privacy for Machine Unlearning</title>
<link>https://arxiv.org/abs/2505.18786</link>
<guid>https://arxiv.org/abs/2505.18786</guid>
<content:encoded><![CDATA[
arXiv:2505.18786v1 Announce Type: new 
Abstract: We present a principled, per-instance approach to quantifying the difficulty of unlearning via fine-tuning. We begin by sharpening an analysis of noisy gradient descent for unlearning (Chien et al., 2024), obtaining a better utility-unlearning tradeoff by replacing worst-case privacy loss bounds with per-instance privacy losses (Thudi et al., 2024), each of which bounds the (Renyi) divergence to retraining without an individual data point. To demonstrate the practical applicability of our theory, we present empirical results showing that our theoretical predictions are born out both for Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard fine-tuning without explicit noise. We further demonstrate that per-instance privacy losses correlate well with several existing data difficulty metrics, while also identifying harder groups of data points, and introduce novel evaluation methods based on loss barriers. All together, our findings provide a foundation for more efficient and adaptive unlearning strategies tailored to the unique properties of individual data points.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governing Equation Discovery from Data Based on Differential Invariants</title>
<link>https://arxiv.org/abs/2505.18798</link>
<guid>https://arxiv.org/abs/2505.18798</guid>
<content:encoded><![CDATA[
arXiv:2505.18798v1 Announce Type: new 
Abstract: The explicit governing equation is one of the simplest and most intuitive forms for characterizing physical laws. However, directly discovering partial differential equations (PDEs) from data poses significant challenges, primarily in determining relevant terms from a vast search space. Symmetry, as a crucial prior knowledge in scientific fields, has been widely applied in tasks such as designing equivariant networks and guiding neural PDE solvers. In this paper, we propose a pipeline for governing equation discovery based on differential invariants, which can losslessly reduce the search space of existing equation discovery methods while strictly adhering to symmetry. Specifically, we compute the set of differential invariants corresponding to the infinitesimal generators of the symmetry group and select them as the relevant terms for equation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as an example, we demonstrate that its success rate and accuracy in PDE discovery surpass those of other symmetry-informed governing equation discovery methods across a series of PDEs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to build a consistency model: Learning flow maps via self-distillation</title>
<link>https://arxiv.org/abs/2505.18825</link>
<guid>https://arxiv.org/abs/2505.18825</guid>
<content:encoded><![CDATA[
arXiv:2505.18825v1 Announce Type: new 
Abstract: Building on the framework proposed in Boffi et al. (2024), we present a systematic approach for learning flow maps associated with flow and diffusion models. Flow map-based models, commonly known as consistency models, encompass recent efforts to improve the efficiency of generative models based on solutions to differential equations. By exploiting a relationship between the velocity field underlying a continuous-time flow and the instantaneous rate of change of the flow map, we show how to convert existing distillation schemes into direct training algorithms via self-distillation, eliminating the need for pre-trained models. We empirically evaluate several instantiations of our framework, finding that high-dimensional tasks like image synthesis benefit from objective functions that avoid temporal and spatial derivatives of the flow map, while lower-dimensional tasks can benefit from objectives incorporating higher-order derivatives to capture sharp features.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality</title>
<link>https://arxiv.org/abs/2505.18828</link>
<guid>https://arxiv.org/abs/2505.18828</guid>
<content:encoded><![CDATA[
arXiv:2505.18828v1 Announce Type: new 
Abstract: We study the Pandora's Box problem in an online learning setting with semi-bandit feedback. In each round, the learner sequentially pays to open up to $n$ boxes with unknown reward distributions, observes rewards upon opening, and decides when to stop. The utility of the learner is the maximum observed reward minus the cumulative cost of opened boxes, and the goal is to minimize regret defined as the gap between the cumulative expected utility and that of the optimal policy. We propose a new algorithm that achieves $\widetilde{O}(\sqrt{nT})$ regret after $T$ rounds, which improves the $\widetilde{O}(n\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known lower bound up to logarithmic factors. To better capture real-life applications, we then extend our results to a natural but challenging contextual linear setting, where each box's expected reward is linear in some known but time-varying $d$-dimensional context and the noise distribution is fixed over time. We design an algorithm that learns both the linear function and the noise distributions, achieving $\widetilde{O}(nd\sqrt{T})$ regret. Finally, we show that our techniques also apply to the online Prophet Inequality problem, where the learner must decide immediately whether or not to accept a revealed reward. In both non-contextual and contextual settings, our approach achieves similar improvements and regret bounds.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization</title>
<link>https://arxiv.org/abs/2505.18830</link>
<guid>https://arxiv.org/abs/2505.18830</guid>
<content:encoded><![CDATA[
arXiv:2505.18830v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become popular in enhancing the reasoning capabilities of large language models (LLMs), with Group Relative Policy Optimization (GRPO) emerging as a widely used algorithm in recent systems. Despite GRPO's widespread adoption, we identify a previously unrecognized phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood of correct responses marginally increases or even decreases during training. This behavior mirrors a recently discovered misalignment issue in Direct Preference Optimization (DPO), attributed to the influence of negative gradients. We provide a theoretical analysis of GRPO's learning dynamic, identifying the source of LLD as the naive penalization of all tokens in incorrect responses with the same strength. To address this, we develop a method called NTHR, which downweights penalties on tokens contributing to the LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's group-based structure, using correct responses as anchors to identify influential tokens. Experiments on math reasoning benchmarks demonstrate that NTHR effectively mitigates LLD, yielding consistent performance gains across models ranging from 0.5B to 3B parameters.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution-Aware Mobility-Assisted Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2505.18866</link>
<guid>https://arxiv.org/abs/2505.18866</guid>
<content:encoded><![CDATA[
arXiv:2505.18866v1 Announce Type: new 
Abstract: Decentralized federated learning (DFL) has attracted significant attention due to its scalability and independence from a central server. In practice, some participating clients can be mobile, yet the impact of user mobility on DFL performance remains largely unexplored, despite its potential to facilitate communication and model convergence. In this work, we demonstrate that introducing a small fraction of mobile clients, even with random movement, can significantly improve the accuracy of DFL by facilitating information flow. To further enhance performance, we propose novel distribution-aware mobility patterns, where mobile clients strategically navigate the network, leveraging knowledge of data distributions and static client locations. The proposed moving strategies mitigate the impact of data heterogeneity and boost learning convergence. Extensive experiments validate the effectiveness of induced mobility in DFL and demonstrate the superiority of our proposed mobility patterns over random movement.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models</title>
<link>https://arxiv.org/abs/2505.18877</link>
<guid>https://arxiv.org/abs/2505.18877</guid>
<content:encoded><![CDATA[
arXiv:2505.18877v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of fine-tuning large models by updating a low-dimensional subspace of the pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal convergence and noticeable performance degradation, due to inconsistent and imbalanced weight updates induced by its nonunique low-rank factorizations. To overcome these limitations, this article identifies the optimal low-rank factorization per step that minimizes an upper bound on the loss. The resultant refactored low-rank adaptation (RefLoRA) method promotes a flatter loss landscape, along with consistent and balanced weight updates, thus speeding up stable convergence. Extensive experiments evaluate RefLoRA on natural language understanding, and commonsense reasoning tasks with popular large language models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical tests corroborate that RefLoRA converges faster, outperforms various benchmarks, and enjoys negligible computational overhead compared to state-of-the-art LoRA variants.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partition Generative Modeling: Masked Modeling Without Masks</title>
<link>https://arxiv.org/abs/2505.18883</link>
<guid>https://arxiv.org/abs/2505.18883</guid>
<content:encoded><![CDATA[
arXiv:2505.18883v1 Announce Type: new 
Abstract: We introduce ``Partition Generative Models'' (PGMs), a novel approach to masked generative modeling (MGMs), particularly effective for masked diffusion language modeling (MDLMs). PGM divides tokens into two distinct groups and employs sparse attention patterns to prevent cross-group information exchange. Hence, the model is trained to predict tokens in one group based solely on information from the other group. This partitioning strategy eliminates the need for MASK tokens entirely. While traditional MGMs inefficiently process MASK tokens during generation, PGMs achieve greater computational efficiency by operating exclusively on unmasked tokens. Our experiments on OpenWebText with a context length of 1024 tokens demonstrate that PGMs deliver at least 5x improvements in both latency and throughput compared to MDLM when using the same number of sampling steps, while generating samples with better generative perplexity than MDLM. Finally, we show that PGMs can be distilled with Self-Distillation Through Time (SDTT), a method originally devised for MDLM, in order to achieve further inference gains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders</title>
<link>https://arxiv.org/abs/2505.18884</link>
<guid>https://arxiv.org/abs/2505.18884</guid>
<content:encoded><![CDATA[
arXiv:2505.18884v1 Announce Type: new 
Abstract: Visual encoders have become fundamental components in modern computer vision pipelines. However, ensuring robustness against adversarial perturbations remains a critical challenge. Recent efforts have explored both supervised and unsupervised adversarial fine-tuning strategies. We identify two key limitations in these approaches: (i) they often suffer from instability, especially during the early stages of fine-tuning, resulting in suboptimal convergence and degraded performance on clean data, and (ii) they exhibit a suboptimal trade-off between robustness and clean data accuracy, hindering the simultaneous optimization of both objectives. To overcome these challenges, we propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised adversarial fine-tuning framework. LORE utilizes constrained optimization, which offers a principled approach to balancing competing goals, such as improving robustness while preserving nominal performance. By enforcing embedding-space proximity constraints, LORE effectively maintains clean data performance throughout adversarial fine-tuning. Extensive experiments show that LORE significantly improves zero-shot adversarial robustness with minimal degradation in clean data accuracy. Furthermore, we demonstrate the effectiveness of the adversarially fine-tuned CLIP image encoder in out-of-distribution generalization and enhancing the interpretability of image embeddings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.18886</link>
<guid>https://arxiv.org/abs/2505.18886</guid>
<content:encoded><![CDATA[
arXiv:2505.18886v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities across numerous NLP tasks. Nevertheless, conventional first-order fine-tuning techniques impose heavy memory demands, creating practical obstacles to real-world applications. Zeroth-order (ZO) optimization has recently emerged as a promising memory-efficient alternative, as it circumvents the need for backpropagation by estimating gradients solely through forward passes--making it particularly suitable for resource-limited environments. Despite its efficiency, ZO optimization suffers from gradient estimation bias, which significantly hinders convergence speed. To address this, we analytically identify and characterize the lower-order bias introduced during ZO-based gradient estimation in LLM fine-tuning. Motivated by tools in mathematical physics, we introduce a kernel-function-based ZO framework aimed at mitigating this bias and improving optimization stability. KerZOO achieves comparable or superior performance to existing ZO baselines in both full-parameter and parameter-efficient fine-tuning settings of LLMs, while significantly reducing the number of iterations required to reach convergence. For example, KerZOO reduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC datasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9% and 2.6% in accuracy. We show that the kernel function is an effective avenue for reducing estimation bias in ZO methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction</title>
<link>https://arxiv.org/abs/2505.18890</link>
<guid>https://arxiv.org/abs/2505.18890</guid>
<content:encoded><![CDATA[
arXiv:2505.18890v1 Announce Type: new 
Abstract: Accurate drug-target interaction (DTI) prediction with machine learning models is essential for drug discovery. Such models should also provide a credible representation of their uncertainty, but applying classical marginal conformal prediction (CP) in DTI prediction often overlooks variability across drug and protein subgroups. In this work, we analyze three cluster-conditioned CP methods for DTI prediction, and compare them with marginal and group-conditioned CP. Clusterings are obtained via nonconformity scores, feature similarity, and nearest neighbors, respectively. Experiments on the KIBA dataset using four data-splitting strategies show that nonconformity-based clustering yields the tightest intervals and most reliable subgroup coverage, especially in random and fully unseen drug-protein splits. Group-conditioned CP works well when one entity is familiar, but residual-driven clustering provides robust uncertainty estimates even in sparse or novel scenarios. These results highlight the potential of cluster-based CP for improving DTI prediction under uncertainty.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models</title>
<link>https://arxiv.org/abs/2505.18901</link>
<guid>https://arxiv.org/abs/2505.18901</guid>
<content:encoded><![CDATA[
arXiv:2505.18901v1 Announce Type: new 
Abstract: The rapid advancement of generative AI models has provided users with numerous options to address their prompts. When selecting a generative AI model for a given prompt, users should consider not only the performance of the chosen model but also its associated service cost. The principle guiding such consideration is to select the least expensive model among the available satisfactory options. However, existing model-selection approaches typically prioritize performance, overlooking pricing differences between models. In this paper, we introduce PromptWise, an online learning framework designed to assign a sequence of prompts to a group of large language models (LLMs) in a cost-effective manner. PromptWise strategically queries cheaper models first, progressing to more expensive options only if the lower-cost models fail to adequately address a given prompt. Through numerical experiments, we demonstrate PromptWise's effectiveness across various tasks, including puzzles of varying complexity and code generation/translation tasks. The results highlight that PromptWise consistently outperforms cost-unaware baseline methods, emphasizing that directly assigning prompts to the most expensive models can lead to higher costs and potentially lower average performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavior Injection: Preparing Language Models for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18917</link>
<guid>https://arxiv.org/abs/2505.18917</guid>
<content:encoded><![CDATA[
arXiv:2505.18917v1 Announce Type: new 
Abstract: Reinforcement fine-tuning (RFT) has emerged as a powerful post-training technique to incentivize the reasoning ability of large language models (LLMs). However, LLMs can respond very inconsistently to RFT: some show substantial performance gains, while others plateau or even degrade. To understand this divergence, we analyze the per-step influence of the RL objective and identify two key conditions for effective post-training: (1) RL-informative rollout accuracy, and (2) strong data co-influence, which quantifies how much the training data affects performance on other samples. Guided by these insights, we propose behavior injection, a task-agnostic data-augmentation scheme applied prior to RL. Behavior injection enriches the supervised finetuning (SFT) data by seeding exploratory and exploitative behaviors, effectively making the model more RL-ready. We evaluate our method across two reasoning benchmarks with multiple base models. The results demonstrate that our theoretically motivated augmentation can significantly increases the performance gain from RFT over the pre-RL model.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Operator Learning from Limited Data on Irregular Domains</title>
<link>https://arxiv.org/abs/2505.18923</link>
<guid>https://arxiv.org/abs/2505.18923</guid>
<content:encoded><![CDATA[
arXiv:2505.18923v1 Announce Type: new 
Abstract: Operator learning seeks to approximate mappings from input functions to output solutions, particularly in the context of partial differential equations (PDEs). While recent advances such as DeepONet and Fourier Neural Operator (FNO) have demonstrated strong performance, they often rely on regular grid discretizations, limiting their applicability to complex or irregular domains. In this work, we propose a Graph-based Operator Learning with Attention (GOLA) framework that addresses this limitation by constructing graphs from irregularly sampled spatial points and leveraging attention-enhanced Graph Neural Netwoks (GNNs) to model spatial dependencies with global information. To improve the expressive capacity, we introduce a Fourier-based encoder that projects input functions into a frequency space using learnable complex coefficients, allowing for flexible embeddings even with sparse or nonuniform samples. We evaluated our approach across a range of 2D PDEs, including Darcy Flow, Advection, Eikonal, and Nonlinear Diffusion, under varying sampling densities. Our method consistently outperforms baselines, particularly in data-scarce regimes, demonstrating strong generalization and efficiency on irregular domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time</title>
<link>https://arxiv.org/abs/2505.18926</link>
<guid>https://arxiv.org/abs/2505.18926</guid>
<content:encoded><![CDATA[
arXiv:2505.18926v1 Announce Type: new 
Abstract: We propose a neural physics system for real-time, interactive fluid simulations. Traditional physics-based methods, while accurate, are computationally intensive and suffer from latency issues. Recent machine-learning methods reduce computational costs while preserving fidelity; yet most still fail to satisfy the latency constraints for real-time use and lack support for interactive applications. To bridge this gap, we introduce a novel hybrid method that integrates numerical simulation, neural physics, and generative control. Our neural physics jointly pursues low-latency simulation and high physical fidelity by employing a fallback safeguard to classical numerical solvers. Furthermore, we develop a diffusion-based controller that is trained using a reverse modeling strategy to generate external dynamic force fields for fluid manipulation. Our system demonstrates robust performance across diverse 2D/3D scenarios, material types, and obstacle interactions, achieving real-time simulations at high frame rates (11~29% latency) while enabling fluid control guided by user-friendly freehand sketches. We present a significant step towards practical, controllable, and physically plausible fluid simulations for real-time interactive applications. We promise to release both models and data upon acceptance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.18934</link>
<guid>https://arxiv.org/abs/2505.18934</guid>
<content:encoded><![CDATA[
arXiv:2505.18934v1 Announce Type: new 
Abstract: Graph Anomaly Detection (GAD) in heterogeneous networks presents unique challenges due to node and edge heterogeneity. Existing Graph Neural Network (GNN) methods primarily focus on homogeneous GAD and thus fail to address three key issues: (C1) Capturing abnormal signal and rich semantics across diverse meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment; and (C3) Learning effectively from difficult anomaly samples with class imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter, which captures anomalous information via applying dedicated Chi-Square filters to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns features while preserving high-frequency information and incorporates heterogeneous messages by a unified Chi-Square Filter; and (3) Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies to address class imbalance. Extensive experiments on public and industrial datasets show that ChiGAD outperforms state-of-the-art models on multiple metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD datasets, validating the effectiveness of Chi-Square filters. Our code is available at https://github.com/HsipingLi/ChiGAD.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Expressive Power of Transformers with Padding</title>
<link>https://arxiv.org/abs/2505.18948</link>
<guid>https://arxiv.org/abs/2505.18948</guid>
<content:encoded><![CDATA[
arXiv:2505.18948v1 Announce Type: new 
Abstract: Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformer's expressive power without adding parameters? We consider transformers with padding tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding converge to precisely the class $\mathsf{TC}^0$ of extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via looping. Our core technical contribution is to show how padding helps bring the notions of complete problems and reductions, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformers' expressive power: with polylogarithmic looping, padded transformers converge to the class $\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\mathsf{NC} = \mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Knowledge Distillation with Reward Guidance</title>
<link>https://arxiv.org/abs/2505.18952</link>
<guid>https://arxiv.org/abs/2505.18952</guid>
<content:encoded><![CDATA[
arXiv:2505.18952v1 Announce Type: new 
Abstract: This work studies knowledge distillation (KD) for large language models (LLMs) through preference optimization. We propose a reward-guided imitation learning framework for sequential KD, formulating a min-max optimization problem between the policy and reward model (RM) to minimize the performance gap between the student and teacher policies. Specifically, the reward optimization is constrained to achieve near-optimality within a confidence set for preference alignment. For preference data construction, we explore both offline and online preference-based KD. Additionally, we reformulate the RM using the $Q$-value function and extend the framework to white-box KD, where the teacher policy's predicted probabilities are accessible. Theoretical analysis and empirical results demonstrate the effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protein Design with Dynamic Protein Vocabulary</title>
<link>https://arxiv.org/abs/2505.18966</link>
<guid>https://arxiv.org/abs/2505.18966</guid>
<content:encoded><![CDATA[
arXiv:2505.18966v1 Announce Type: new 
Abstract: Protein design is a fundamental challenge in biotechnology, aiming to design novel sequences with specific functions within the vast space of possible proteins. Recent advances in deep generative models have enabled function-based protein design from textual descriptions, yet struggle with structural plausibility. Inspired by classical protein design methods that leverage natural protein structures, we explore whether incorporating fragments from natural proteins can enhance foldability in generative models. Our empirical results show that even random incorporation of fragments improves foldability. Building on this insight, we introduce ProDVa, a novel protein design approach that integrates a text encoder for functional descriptions, a protein language model for designing proteins, and a fragment encoder to dynamically retrieve protein fragments based on textual functional descriptions. Experimental results demonstrate that our approach effectively designs protein sequences that are both functionally aligned and structurally plausible. Compared to state-of-the-art models, ProDVa achieves comparable function alignment using less than 0.04% of the training data, while designing significantly more well-folded proteins, with the proportion of proteins having pLDDT above 70 increasing by 7.38% and those with PAE below 10 increasing by 9.6%.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraSS: Scalable Influence Function with Sparse Gradient Compression</title>
<link>https://arxiv.org/abs/2505.18976</link>
<guid>https://arxiv.org/abs/2505.18976</guid>
<content:encoded><![CDATA[
arXiv:2505.18976v1 Announce Type: new 
Abstract: Gradient-based data attribution methods, such as influence functions, are critical for understanding the impact of individual training samples without requiring repeated model retraining. However, their scalability is often limited by the high computational and memory costs associated with per-sample gradient computation. In this work, we propose GraSS, a novel gradient compression algorithm and its variants FactGraSS for linear layers specifically, that explicitly leverage the inherent sparsity of per-sample gradients to achieve sub-linear space and time complexity. Extensive experiments demonstrate the effectiveness of our approach, achieving substantial speedups while preserving data influence fidelity. In particular, FactGraSS achieves up to 165% faster throughput on billion-scale models compared to the previous state-of-the-art baselines. Our code is publicly available at https://github.com/TRAIS-Lab/GraSS.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization</title>
<link>https://arxiv.org/abs/2505.18979</link>
<guid>https://arxiv.org/abs/2505.18979</guid>
<content:encoded><![CDATA[
arXiv:2505.18979v1 Announce Type: new 
Abstract: Text-to-image (T2I) generation models can inadvertently produce not-safe-for-work (NSFW) content, prompting the integration of text and image safety filters. Recent advances employ large language models (LLMs) for semantic-level detection, rendering traditional token-level perturbation attacks largely ineffective. However, our evaluation shows that existing jailbreak methods are ineffective against these modern filters. We introduce GhostPrompt, the first automated jailbreak framework that combines dynamic prompt optimization with multimodal feedback. It consists of two key components: (i) Dynamic Optimization, an iterative process that guides a large language model (LLM) using feedback from text safety filters and CLIP similarity scores to generate semantically aligned adversarial prompts; and (ii) Adaptive Safety Indicator Injection, which formulates the injection of benign visual cues as a reinforcement learning problem to bypass image-level filters. GhostPrompt achieves state-of-the-art performance, increasing the ShieldLM-7B bypass rate from 12.5\% (Sneakyprompt) to 99.0\%, improving CLIP score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \times$. Moreover, it generalizes to unseen filters including GPT-4.1 and successfully jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing systemic vulnerabilities in current multimodal defenses. To support further research on AI safety and red-teaming, we will release code and adversarial prompts under a controlled-access protocol.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSKC: Federated Learning with Non-IID Data via Structural Knowledge Collaboration</title>
<link>https://arxiv.org/abs/2505.18981</link>
<guid>https://arxiv.org/abs/2505.18981</guid>
<content:encoded><![CDATA[
arXiv:2505.18981v1 Announce Type: new 
Abstract: With the advancement of edge computing, federated learning (FL) displays a bright promise as a privacy-preserving collaborative learning paradigm. However, one major challenge for FL is the data heterogeneity issue, which refers to the biased labeling preferences among multiple clients, negatively impacting convergence and model performance. Most previous FL methods attempt to tackle the data heterogeneity issue locally or globally, neglecting underlying class-wise structure information contained in each client. In this paper, we first study how data heterogeneity affects the divergence of the model and decompose it into local, global, and sampling drift sub-problems. To explore the potential of using intra-client class-wise structural knowledge in handling these drifts, we thus propose Federated Learning with Structural Knowledge Collaboration (FedSKC). The key idea of FedSKC is to extract and transfer domain preferences from inter-client data distributions, offering diverse class-relevant knowledge and a fair convergent signal. FedSKC comprises three components: i) local contrastive learning, to prevent weight divergence resulting from local training; ii) global discrepancy aggregation, which addresses the parameter deviation between the server and clients; iii) global period review, correcting for the sampling drift introduced by the server randomly selecting devices. We have theoretically analyzed FedSKC under non-convex objectives and empirically validated its superiority through extensive experimental results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AmorLIP: Efficient Language-Image Pretraining via Amortization</title>
<link>https://arxiv.org/abs/2505.18983</link>
<guid>https://arxiv.org/abs/2505.18983</guid>
<content:encoded><![CDATA[
arXiv:2505.18983v1 Announce Type: new 
Abstract: Contrastive Language-Image Pretraining (CLIP) has demonstrated strong zero-shot performance across diverse downstream text-image tasks. Existing CLIP methods typically optimize a contrastive objective using negative samples drawn from each minibatch. To achieve robust representation learning, these methods require extremely large batch sizes and escalate computational demands to hundreds or even thousands of GPUs. Prior approaches to mitigate this issue often compromise downstream performance, prolong training duration, or face scalability challenges with very large datasets. To overcome these limitations, we propose AmorLIP, an efficient CLIP pretraining framework that amortizes expensive computations involved in contrastive learning through lightweight neural networks, which substantially improves training efficiency and performance. Leveraging insights from a spectral factorization of energy-based models, we introduce novel amortization objectives along with practical techniques to improve training stability. Extensive experiments across 38 downstream tasks demonstrate the superior zero-shot classification and retrieval capabilities of AmorLIP, consistently outperforming standard CLIP baselines with substantial relative improvements of up to 12.24%.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRICT: Stress Test of Rendering Images Containing Text</title>
<link>https://arxiv.org/abs/2505.18985</link>
<guid>https://arxiv.org/abs/2505.18985</guid>
<content:encoded><![CDATA[
arXiv:2505.18985v1 Announce Type: new 
Abstract: While diffusion models have revolutionized text-to-image generation with their ability to synthesize realistic and diverse scenes, they continue to struggle to generate consistent and legible text within images. This shortcoming is commonly attributed to the locality bias inherent in diffusion-based generation, which limits their ability to model long-range spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a benchmark designed to systematically stress-test the ability of diffusion models to render coherent and instruction-aligned text in images. Our benchmark evaluates models across multiple dimensions: (1) the maximum length of readable text that can be generated; (2) the correctness and legibility of the generated text, and (3) the ratio of not following instructions for generating text. We evaluate several state-of-the-art models, including proprietary and open-source variants, and reveal persistent limitations in long-range consistency and instruction-following capabilities. Our findings provide insights into architectural bottlenecks and motivate future research directions in multimodal generative modeling. We release our entire evaluation pipeline at https://github.com/tianyu-z/STRICT-Bench.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic and Structure-Aware Sparsification of Hybrid Neural ODEs</title>
<link>https://arxiv.org/abs/2505.18996</link>
<guid>https://arxiv.org/abs/2505.18996</guid>
<content:encoded><![CDATA[
arXiv:2505.18996v1 Announce Type: new 
Abstract: Hybrid neural ordinary differential equations (neural ODEs) integrate mechanistic models with neural ODEs, offering strong inductive bias and flexibility, and are particularly advantageous in data-scarce healthcare settings. However, excessive latent states and interactions from mechanistic models can lead to training inefficiency and over-fitting, limiting practical effectiveness of hybrid neural ODEs. In response, we propose a new hybrid pipeline for automatic state selection and structure optimization in mechanistic neural ODEs, combining domain-informed graph modifications with data-driven regularization to sparsify the model for improving predictive performance and stability while retaining mechanistic plausibility. Experiments on synthetic and real-world data show improved predictive performance and robustness with desired sparsity, establishing an effective solution for hybrid model reduction in healthcare applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-pessimistic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19002</link>
<guid>https://arxiv.org/abs/2505.19002</guid>
<content:encoded><![CDATA[
arXiv:2505.19002v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected data. However, it faces challenges of distributional shift, where the learned policy may encounter unseen scenarios not covered in the offline data. Additionally, numerous applications suffer from a scarcity of labeled reward data. Relying on labeled data alone often leads to a narrow state-action distribution, further amplifying the distributional shift, and resulting in suboptimal policy learning. To address these issues, we first recognize that the volume of unlabeled data is typically substantially larger than that of labeled data. We then propose a semi-pessimistic RL method to effectively leverage abundant unlabeled data. Our approach offers several advantages. It considerably simplifies the learning process, as it seeks a lower bound of the reward function, rather than that of the Q-function or state transition function. It is highly flexible, and can be integrated with a range of model-free and model-based RL algorithms. It enjoys the guaranteed improvement when utilizing vast unlabeled data, but requires much less restrictive conditions. We compare our method with a number of alternative solutions, both analytically and numerically, and demonstrate its clear competitiveness. We further illustrate with an application to adaptive deep brain stimulation for Parkinson's disease.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faithful Group Shapley Value</title>
<link>https://arxiv.org/abs/2505.19013</link>
<guid>https://arxiv.org/abs/2505.19013</guid>
<content:encoded><![CDATA[
arXiv:2505.19013v1 Announce Type: new 
Abstract: Data Shapley is an important tool for data valuation, which quantifies the contribution of individual data points to machine learning models. In practice, group-level data valuation is desirable when data providers contribute data in batch. However, we identify that existing group-level extensions of Data Shapley are vulnerable to shell company attacks, where strategic group splitting can unfairly inflate valuations. We propose Faithful Group Shapley Value (FGSV) that uniquely defends against such attacks. Building on original mathematical insights, we develop a provably fast and accurate approximation algorithm for computing FGSV. Empirical experiments demonstrate that our algorithm significantly outperforms state-of-the-art methods in computational efficiency and approximation accuracy, while ensuring faithful group-level valuation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Electron Cloud in Protein-Ligand Interaction Learning</title>
<link>https://arxiv.org/abs/2505.19014</link>
<guid>https://arxiv.org/abs/2505.19014</guid>
<content:encoded><![CDATA[
arXiv:2505.19014v1 Announce Type: new 
Abstract: The affinity and specificity of protein-molecule binding directly impact functional outcomes, uncovering the mechanisms underlying biological regulation and signal transduction. Most deep-learning-based prediction approaches focus on structures of atoms or fragments. However, quantum chemical properties, such as electronic structures, are the key to unveiling interaction patterns but remain largely underexplored. To bridge this gap, we propose ECBind, a method for tokenizing electron cloud signals into quantized embeddings, enabling their integration into downstream tasks such as binding affinity prediction. By incorporating electron densities, ECBind helps uncover binding modes that cannot be fully represented by atom-level models. Specifically, to remove the redundancy inherent in electron cloud signals, a structure-aware transformer and hierarchical codebooks encode 3D binding sites enriched with electron structures into tokens. These tokenized codes are then used for specific tasks with labels. To extend its applicability to a wider range of scenarios, we utilize knowledge distillation to develop an electron-cloud-agnostic prediction model. Experimentally, ECBind demonstrates state-of-the-art performance across multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure Pearson and Spearman correlation coefficients, respectively.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Querying Kernel Methods Suffices for Reconstructing their Training Data</title>
<link>https://arxiv.org/abs/2505.19019</link>
<guid>https://arxiv.org/abs/2505.19019</guid>
<content:encoded><![CDATA[
arXiv:2505.19019v1 Announce Type: new 
Abstract: Over-parameterized models have raised concerns about their potential to memorize training data, even when achieving strong generalization. The privacy implications of such memorization are generally unclear, particularly in scenarios where only model outputs are accessible. We study this question in the context of kernel methods, and demonstrate both empirically and theoretically that querying kernel models at various points suffices to reconstruct their training data, even without access to model parameters. Our results hold for a range of kernel methods, including kernel regression, support vector machines, and kernel density estimation. Our hope is that this work can illuminate potential privacy concerns for such models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn Beneficial Noise as Graph Augmentation</title>
<link>https://arxiv.org/abs/2505.19024</link>
<guid>https://arxiv.org/abs/2505.19024</guid>
<content:encoded><![CDATA[
arXiv:2505.19024v1 Announce Type: new 
Abstract: Although graph contrastive learning (GCL) has been widely investigated, it is still a challenge to generate effective and stable graph augmentations. Existing methods often apply heuristic augmentation like random edge dropping, which may disrupt important graph structures and result in unstable GCL performance. In this paper, we propose Positive-incentive Noise driven Graph Data Augmentation (PiNGDA), where positive-incentive noise (pi-noise) scientifically analyzes the beneficial effect of noise under the information theory. To bridge the standard GCL and pi-noise framework, we design a Gaussian auxiliary variable to convert the loss function to information entropy. We prove that the standard GCL with pre-defined augmentations is equivalent to estimate the beneficial noise via the point estimation. Following our analysis, PiNGDA is derived from learning the beneficial noise on both topology and attributes through a trainable noise generator for graph augmentations, instead of the simple estimation. Since the generator learns how to produce beneficial perturbations on graph topology and node attributes, PiNGDA is more reliable compared with the existing methods. Extensive experimental results validate the effectiveness and stability of PiNGDA.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias</title>
<link>https://arxiv.org/abs/2505.19038</link>
<guid>https://arxiv.org/abs/2505.19038</guid>
<content:encoded><![CDATA[
arXiv:2505.19038v1 Announce Type: new 
Abstract: Accurately predicting the long-term evolution of turbulence is crucial for advancing scientific understanding and optimizing engineering applications. However, existing deep learning methods face significant bottlenecks in long-term autoregressive prediction, which exhibit excessive smoothing and fail to accurately track complex fluid dynamics. Our extensive experimental and spectral analysis of prevailing methods provides an interpretable explanation for this shortcoming, identifying Spectral Bias as the core obstacle. Concretely, spectral bias is the inherent tendency of models to favor low-frequency, smooth features while overlooking critical high-frequency details during training, thus reducing fidelity and causing physical distortions in long-term predictions. Building on this insight, we propose Turb-L1, an innovative turbulence prediction method, which utilizes a Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to explicitly overcome spectral bias. It accurately captures cross-scale interactions and preserves the fidelity of high-frequency dynamics, enabling reliable long-term tracking of turbulence evolution. Extensive experiments on the 2D turbulence benchmark show that Turb-L1 demonstrates excellent performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE) by $80.3\%$ and increases Structural Similarity (SSIM) by over $9\times$ compared to the SOTA baseline, significantly improving prediction fidelity. (II) It effectively overcomes spectral bias, accurately reproducing the full enstrophy spectrum and maintaining physical realism in high-wavenumber regions, thus avoiding the spectral distortions or spurious energy accumulation seen in other methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Clustering of Linear Bandits: Unlocking the Power of Clusters in Data-Limited Environments</title>
<link>https://arxiv.org/abs/2505.19043</link>
<guid>https://arxiv.org/abs/2505.19043</guid>
<content:encoded><![CDATA[
arXiv:2505.19043v1 Announce Type: new 
Abstract: Contextual linear multi-armed bandits are a learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can significantly accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making across multiple users. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C$^2$LUB, which we analytically show performs well for arbitrary amounts of user data, and Off-CLUB, which is prone to bias when data is limited but, given sufficient data, matches a theoretical lower bound that we derive for the offline clustered MAB problem. We experimentally validate these results on both real and synthetic datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Reinforcement Learning for Combinatorial Decision-Making</title>
<link>https://arxiv.org/abs/2505.19053</link>
<guid>https://arxiv.org/abs/2505.19053</guid>
<content:encoded><![CDATA[
arXiv:2505.19053v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is increasingly applied to real-world problems involving complex and structured decisions, such as routing, scheduling, and assortment planning. These settings challenge standard RL algorithms, which struggle to scale, generalize, and exploit structure in the presence of combinatorial action spaces. We propose Structured Reinforcement Learning (SRL), a novel actor-critic framework that embeds combinatorial optimization layers into the actor neural network. We enable end-to-end learning of the actor via Fenchel-Young losses and provide a geometric interpretation of SRL as a primal-dual algorithm in the dual of the moment polytope. Across six environments with exogenous and endogenous uncertainty, SRL matches or surpasses the performance of unstructured RL and imitation learning on static tasks and improves over these baselines by up to 92% on dynamic problems, with improved stability and convergence speed.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy Learning</title>
<link>https://arxiv.org/abs/2505.19054</link>
<guid>https://arxiv.org/abs/2505.19054</guid>
<content:encoded><![CDATA[
arXiv:2505.19054v1 Announce Type: new 
Abstract: Recent advancements in reinforcement learning (RL) have leveraged neural networks to achieve state-of-the-art performance across various control tasks. However, these successes often come at the cost of significant computational resources, as training deep neural networks requires substantial time and data. In this paper, we introduce an actor-critic algorithm that utilizes randomized neural networks to drastically reduce computational costs while maintaining strong performance. Despite its simple architecture, our method effectively solves a range of control problems, including the locomotion control of a highly dynamic 12-motor quadruped robot, and achieves results comparable to leading algorithms such as Proximal Policy Optimization (PPO). Notably, our approach does not outperform other algorithms in terms of sample efficnency but rather in terms of wall-clock training time. That is, although our algorithm requires more timesteps to converge to an optimal policy, the actual time required for training turns out to be lower.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Deep Q-Learning</title>
<link>https://arxiv.org/abs/2505.19058</link>
<guid>https://arxiv.org/abs/2505.19058</guid>
<content:encoded><![CDATA[
arXiv:2505.19058v1 Announce Type: new 
Abstract: We propose a novel distributionally robust $Q$-learning algorithm for the non-tabular case accounting for continuous state spaces where the state transition of the underlying Markov decision process is subject to model uncertainty. The uncertainty is taken into account by considering the worst-case transition from a ball around a reference probability measure. To determine the optimal policy under the worst-case state transition, we solve the associated non-linear Bellman equation by dualising and regularising the Bellman operator with the Sinkhorn distance, which is then parameterized with deep neural networks. This approach allows us to modify the Deep Q-Network algorithm to optimise for the worst case state transition.
  We illustrate the tractability and effectiveness of our approach through several applications, including a portfolio optimisation task based on S\&{P}~500 data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management</title>
<link>https://arxiv.org/abs/2505.19061</link>
<guid>https://arxiv.org/abs/2505.19061</guid>
<content:encoded><![CDATA[
arXiv:2505.19061v1 Announce Type: new 
Abstract: Motivated by dynamic parameter optimization in finite, but large action (configurations) spaces, this work studies the nonstochastic multi-armed bandit (MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We propose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can use state-of-the-art existing "flat" algorithms, but additionally clusters similar configurations to exploit local structures and adapt to changing environments. We prove that in the worst-case scenario, such clustering approach cannot hurt too much and ABoB guarantees a standard worst-case regret bound of $O\left(k^{\frac{1}{2}}T^{\frac{1}{2}}\right)$, where $T$ is the number of rounds and $k$ is the number of arms, matching the traditional flat approach. However, under favorable conditions related to the algorithm properties, clusters properties, and certain Lipschitz conditions, the regret bound can be improved to $O\left(k^{\frac{1}{4}}T^{\frac{1}{2}}\right)$. Simulations and experiments on a real storage system demonstrate that ABoB, using standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and faster convergence than the flat method, up to 50% improvement in known previous setups, nonstochastic and stochastic, as well as in our settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recalibrating binary probabilistic classifiers</title>
<link>https://arxiv.org/abs/2505.19068</link>
<guid>https://arxiv.org/abs/2505.19068</guid>
<content:encoded><![CDATA[
arXiv:2505.19068v1 Announce Type: new 
Abstract: Recalibration of binary probabilistic classifiers to a target prior probability is an important task in areas like credit risk management. We analyse methods for recalibration from a distribution shift perspective. Distribution shift assumptions linked to the area under the curve (AUC) of a probabilistic classifier are found to be useful for the design of meaningful recalibration methods. Two new methods called parametric covariate shift with posterior drift (CSPD) and ROC-based quasi moment matching (QMM) are proposed and tested together with some other methods in an example setting. The outcomes of the test suggest that the QMM methods discussed in the paper can provide appropriately conservative results in evaluations with concave functionals like for instance risk weights functions for credit risk.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes</title>
<link>https://arxiv.org/abs/2505.19087</link>
<guid>https://arxiv.org/abs/2505.19087</guid>
<content:encoded><![CDATA[
arXiv:2505.19087v1 Announce Type: new 
Abstract: We analyze the generalization gap (gap between the training and test errors) when training a potentially over-parametrized model using a Markovian stochastic training algorithm, initialized from some distribution $\theta_0 \sim p_0$. We focus on Langevin dynamics with a positive temperature $\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal step size, perturbed with $\beta^{-1}$-variances Gaussian noise, and lightly regularized or bounded. There, we bound the generalization gap, at any time during training, by $\sqrt{(\beta\mathbb{E} L (\theta_0) + \log(1/\delta))/N}$ with probability $1-\delta$ over the dataset, where $N$ is the sample size, and $\mathbb{E} L (\theta_0) =O(1)$ with standard initialization scaling. In contrast to previous guarantees, we have no dependence on either training time or reliance on mixing, nor a dependence on dimensionality, gradient norms, or any other properties of the loss or model. This guarantee follows from a general analysis of any Markov process-based training that has a Gibbs-style stationary distribution. The proof is surprisingly simple, once we observe that the marginal distribution divergence from initialization remains bounded, as implied by a generalized second law of thermodynamics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations</title>
<link>https://arxiv.org/abs/2505.19090</link>
<guid>https://arxiv.org/abs/2505.19090</guid>
<content:encoded><![CDATA[
arXiv:2505.19090v1 Announce Type: new 
Abstract: Recent advances in lightweight time series forecasting models suggest the inherent simplicity of time series forecasting tasks. In this paper, we present CMoS, a super-lightweight time series forecasting model. Instead of learning the embedding of the shapes, CMoS directly models the spatial correlations between different time series chunks. Additionally, we introduce a Correlation Mixing technique that enables the model to capture diverse spatial correlations with minimal parameters, and an optional Periodicity Injection technique to ensure faster convergence. Despite utilizing as low as 1% of the lightweight model DLinear's parameters count, experimental results demonstrate that CMoS outperforms existing state-of-the-art models across multiple datasets. Furthermore, the learned weights of CMoS exhibit great interpretability, providing practitioners with valuable insights into temporal structures within specific application scenarios.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Influence Functions with Flat Validation Minima</title>
<link>https://arxiv.org/abs/2505.19097</link>
<guid>https://arxiv.org/abs/2505.19097</guid>
<content:encoded><![CDATA[
arXiv:2505.19097v1 Announce Type: new 
Abstract: The Influence Function (IF) is a widely used technique for assessing the impact of individual training samples on model predictions. However, existing IF methods often fail to provide reliable influence estimates in deep neural networks, particularly when applied to noisy training data. This issue does not stem from inaccuracies in parameter change estimation, which has been the primary focus of prior research, but rather from deficiencies in loss change estimation, specifically due to the sharpness of validation risk. In this work, we establish a theoretical connection between influence estimation error, validation set risk, and its sharpness, underscoring the importance of flat validation minima for accurate influence estimation. Furthermore, we introduce a novel estimation form of Influence Function specifically designed for flat validation minima. Experimental results across various tasks validate the superiority of our approach.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Mamba Operator for Partial Differential Equations</title>
<link>https://arxiv.org/abs/2505.19105</link>
<guid>https://arxiv.org/abs/2505.19105</guid>
<content:encoded><![CDATA[
arXiv:2505.19105v1 Announce Type: new 
Abstract: Neural operators have emerged as powerful data-driven frameworks for solving Partial Differential Equations (PDEs), offering significant speedups over numerical methods. However, existing neural operators struggle with scalability in high-dimensional spaces, incur high computational costs, and face challenges in capturing continuous and long-range dependencies in PDE dynamics. To address these limitations, we introduce the Latent Mamba Operator (LaMO), which integrates the efficiency of state-space models (SSMs) in latent space with the expressive power of kernel integral formulations in neural operators. We also establish a theoretical connection between state-space models (SSMs) and the kernel integral of neural operators. Extensive experiments across diverse PDE benchmarks on regular grids, structured meshes, and point clouds covering solid and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA) performance, with a 32.3\% improvement over existing baselines in solution operator approximation, highlighting its efficacy in modeling complex PDE solutions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization-Inspired Few-Shot Adaptation for Large Language Models</title>
<link>https://arxiv.org/abs/2505.19107</link>
<guid>https://arxiv.org/abs/2505.19107</guid>
<content:encoded><![CDATA[
arXiv:2505.19107v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in real-world applications. However, adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in few-shot scenarios. Existing approaches, such as in-context learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations: in-context learning introduces additional inference computational overhead with limited performance gains, while PEFT models are prone to overfitting on the few demonstration examples. In this work, we reinterpret the forward pass of LLMs as an optimization process, a sequence of preconditioned gradient descent steps refining internal representations. Based on this connection, we propose Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency by learning preconditioners based on a convergence bound, while simultaneously steering the optimization path toward the flat local minimum. Our method overcomes both issues of ICL-based and PEFT-based methods, and demonstrates superior performance over the existing methods on a variety of few-shot adaptation tasks in experiments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FP4 All the Way: Fully Quantized Training of LLMs</title>
<link>https://arxiv.org/abs/2505.19115</link>
<guid>https://arxiv.org/abs/2505.19115</guid>
<content:encoded><![CDATA[
arXiv:2505.19115v1 Announce Type: new 
Abstract: We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way .
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization</title>
<link>https://arxiv.org/abs/2505.19133</link>
<guid>https://arxiv.org/abs/2505.19133</guid>
<content:encoded><![CDATA[
arXiv:2505.19133v1 Announce Type: new 
Abstract: Low-rank representation learning has emerged as a powerful tool for recovering missing values in power load data due to its ability to exploit the inherent low-dimensional structures of spatiotemporal measurements. Among various techniques, low-rank factorization models are favoured for their efficiency and interpretability. However, their performance is highly sensitive to the choice of regularization parameters, which are typically fixed or manually tuned, resulting in limited generalization capability or slow convergence in practical scenarios. In this paper, we propose a Regularization-optimized Low-Rank Factorization, which introduces a Proportional-Integral-Derivative controller to adaptively adjust the regularization coefficient. Furthermore, we provide a detailed algorithmic complexity analysis, showing that our method preserves the computational efficiency of stochastic gradient descent while improving adaptivity. Experimental results on real-world power load datasets validate the superiority of our method in both imputation accuracy and training efficiency compared to existing baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADGSyn: Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction</title>
<link>https://arxiv.org/abs/2505.19144</link>
<guid>https://arxiv.org/abs/2505.19144</guid>
<content:encoded><![CDATA[
arXiv:2505.19144v1 Announce Type: new 
Abstract: Drug combinations play a critical role in cancer therapy by significantly enhancing treatment efficacy and overcoming drug resistance. However, the combinatorial space of possible drug pairs grows exponentially, making experimental screening highly impractical. Therefore, developing efficient computational methods to predict promising drug combinations and guide experimental validation is of paramount importance. In this work, we propose ADGSyn, an innovative method for predicting drug synergy. The key components of our approach include: (1) shared projection matrices combined with attention mechanisms to enable cross-drug feature alignment; (2) automatic mixed precision (AMP)-optimized graph operations that reduce memory consumption by 40\% while accelerating training speed threefold; and (3) residual pathways stabilized by LayerNorm to ensure stable gradient propagation during training. Evaluated on the O'Neil dataset containing 13,243 drug--cell line combinations, ADGSyn demonstrates superior performance over eight baseline methods. Moreover, the framework supports full-batch processing of up to 256 molecular graphs on a single GPU, setting a new standard for efficiency in drug synergy prediction within the field of computational oncology.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Inertia as a Conserved Quantity in Frictionless and Damped Learning Dynamics</title>
<link>https://arxiv.org/abs/2505.19171</link>
<guid>https://arxiv.org/abs/2505.19171</guid>
<content:encoded><![CDATA[
arXiv:2505.19171v1 Announce Type: new 
Abstract: We identify a conserved quantity in continuous-time optimization dynamics, termed computational inertia. Defined as the sum of kinetic energy (parameter velocity) and potential energy (loss), this scalar remains invariant under idealized, frictionless training. We formalize this conservation law, derive its analytic decay under damping and stochastic perturbations, and demonstrate its behavior in a synthetic system. The invariant offers a compact lens for interpreting learning trajectories, and may inform theoretical tools for analyzing convergence, stability, and training geometry.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning: From Theory to Practice</title>
<link>https://arxiv.org/abs/2505.19183</link>
<guid>https://arxiv.org/abs/2505.19183</guid>
<content:encoded><![CDATA[
arXiv:2505.19183v1 Announce Type: new 
Abstract: This book offers a hands-on introduction to building and understanding federated learning (FL) systems. FL enables multiple devices -- such as smartphones, sensors, or local computers -- to collaboratively train machine learning (ML) models, while keeping their data private and local. It is a powerful solution when data cannot or should not be centralized due to privacy, regulatory, or technical reasons. The book is designed for students, engineers, and researchers who want to learn how to design scalable, privacy preserving FL systems. Our main focus is on personalization: enabling each device to train its own model while still benefiting from collaboration with relevant devices. This is achieved by leveraging similarities between (the learning tasks associated with) devices that are encoded by the weighted edges (or links) of a federated learning network (FL network). The key idea is to represent real-world FL systems as networks of devices, where nodes correspond to device and edges represent communication links and data similarities between them. The training of personalized models for these devices can be naturally framed as a distributed optimization problem. This optimization problem is referred to as generalized total variation minimization (GTVMin) and ensures that devices with similar learning tasks learn similar model parameters. Our approach is both mathematically principled and practically motivated. While we introduce some advanced ideas from optimization theory and graph-based learning, we aim to keep the book accessible. Readers are guided through the core ideas step by step, with intuitive explanations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chordless Structure: A Pathway to Simple and Expressive GNNs</title>
<link>https://arxiv.org/abs/2505.19188</link>
<guid>https://arxiv.org/abs/2505.19188</guid>
<content:encoded><![CDATA[
arXiv:2505.19188v1 Announce Type: new 
Abstract: Researchers have proposed various methods of incorporating more structured information into the design of Graph Neural Networks (GNNs) to enhance their expressiveness. However, these methods are either computationally expensive or lacking in provable expressiveness. In this paper, we observe that the chords increase the complexity of the graph structure while contributing little useful information in many cases. In contrast, chordless structures are more efficient and effective for representing the graph. Therefore, when leveraging the information of cycles, we choose to omit the chords. Accordingly, we propose a Chordless Structure-based Graph Neural Network (CSGNN) and prove that its expressiveness is strictly more powerful than the k-hop GNN (KPGNN) with polynomial complexity. Experimental results on real-world datasets demonstrate that CSGNN outperforms existing GNNs across various graph tasks while incurring lower computational costs and achieving better performance than the GNNs of 3-WL expressiveness.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2505.19190</link>
<guid>https://arxiv.org/abs/2505.19190</guid>
<content:encoded><![CDATA[
arXiv:2505.19190v1 Announce Type: new 
Abstract: Modality fusion is a cornerstone of multimodal learning, enabling information integration from diverse data sources. However, vanilla fusion methods are limited by (1) inability to account for heterogeneous interactions between modalities and (2) lack of interpretability in uncovering the multimodal interactions inherent in the data. To this end, we propose I2MoE (Interpretable Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework designed to enhance modality fusion by explicitly modeling diverse multimodal interactions, as well as providing interpretation on a local and global level. First, I2MoE utilizes different interaction experts with weakly supervised interaction losses to learn multimodal interactions in a data-driven way. Second, I2MoE deploys a reweighting model that assigns importance scores for the output of each interaction expert, which offers sample-level and dataset-level interpretation. Extensive evaluation of medical and general multimodal datasets shows that I2MoE is flexible enough to be combined with different fusion techniques, consistently improves task performance, and provides interpretation across various real-world scenarios. Code is available at https://github.com/Raina-Xin/I2MoE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Graph Learning Over Sets of Temporally-Sparse Data</title>
<link>https://arxiv.org/abs/2505.19193</link>
<guid>https://arxiv.org/abs/2505.19193</guid>
<content:encoded><![CDATA[
arXiv:2505.19193v1 Announce Type: new 
Abstract: Real-world medical data often includes measurements from multiple signals that are collected at irregular and asynchronous time intervals. For example, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling of different attributes occur in other domains, such as monitoring of large systems using event log files or the spread of fake news on social networks. Effectively learning from such data requires models that can handle sets of temporally sparse and heterogeneous signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a novel and interpretable-by-design model for learning over irregular sets of temporal signals. Our method achieves state-of-the-art performance in real-world medical tasks, including a 4-point increase in the AUROC score of in-hospital mortality prediction, compared to existing methods. We further showcase GMAN's flexibility by applying it to a fake news detection task. We demonstrate how its interpretability capabilities, including node-level, graph-level, and subset-level importance, allow for transition phases detection and gaining medical insights with real-world high-stakes implications. Finally, we provide theoretical insights on GMAN expressive power.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation</title>
<link>https://arxiv.org/abs/2505.19194</link>
<guid>https://arxiv.org/abs/2505.19194</guid>
<content:encoded><![CDATA[
arXiv:2505.19194v1 Announce Type: new 
Abstract: Adversarial attack reveals the vulnerability of deep learning models. For about a decade, countless attack and defense methods have been proposed, leading to robustified classifiers and better understanding of models. Among these methods, curvature-based approaches have attracted attention because it is assumed that high curvature may give rise to rough decision boundary. However, the most commonly used \textit{curvature} is the curvature of loss function, scores or other parameters from within the model as opposed to decision boundary curvature, since the former can be relatively easily formed using second order derivative. In this paper, we propose a new query-efficient method, dynamic curvature estimation(DCE), to estimate the decision boundary curvature in a black-box setting. Our approach is based on CGBA, a black-box adversarial attack. By performing DCE on a wide range of classifiers, we discovered, statistically, a connection between decision boundary curvature and adversarial robustness. We also propose a new attack method, curvature dynamic black-box attack(CDBA) with improved performance using the dynamically estimated curvature.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2505.19205</link>
<guid>https://arxiv.org/abs/2505.19205</guid>
<content:encoded><![CDATA[
arXiv:2505.19205v1 Announce Type: new 
Abstract: Hyperparameter optimization (HPO) is a critical yet challenging aspect of machine learning model development, significantly impacting model performance and generalization. Traditional HPO methods often struggle with high dimensionality, complex interdependencies, and computational expense. This paper introduces OptiMindTune, a novel multi-agent framework designed to intelligently and efficiently optimize hyperparameters. OptiMindTune leverages the collaborative intelligence of three specialized AI agents -- a Recommender Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's Gemini models. These agents address distinct facets of the HPO problem, from model selection and hyperparameter suggestion to robust evaluation and strategic decision-making. By fostering dynamic interactions and knowledge sharing, OptiMindTune aims to converge to optimal hyperparameter configurations more rapidly and robustly than existing single-agent or monolithic approaches. Our framework integrates principles from advanced large language models, and adaptive search to achieve scalable and intelligent AutoML. We posit that this multi-agent paradigm offers a promising avenue for tackling the increasing complexity of modern machine learning model tuning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models</title>
<link>https://arxiv.org/abs/2505.19223</link>
<guid>https://arxiv.org/abs/2505.19223</guid>
<content:encoded><![CDATA[
arXiv:2505.19223v1 Announce Type: new 
Abstract: While Masked Diffusion Models (MDMs), such as LLaDA, present a promising paradigm for language modeling, there has been relatively little effort in aligning these models with human preferences via reinforcement learning. The challenge primarily arises from the high variance in Evidence Lower Bound (ELBO)-based likelihood estimates required for preference optimization. To address this issue, we propose Variance-Reduced Preference Optimization (VRPO), a framework that formally analyzes the variance of ELBO estimators and derives bounds on both the bias and variance of preference optimization gradients. Building on this theoretical foundation, we introduce unbiased variance reduction strategies, including optimal Monte Carlo budget allocation and antithetic sampling, that significantly improve the performance of MDM alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA, and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor consistently and significantly across mathematical (GSM8K +4.7), code (HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard +4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical performance compared to strong language MDMs and ARMs. Project page: https://ml-gsai.github.io/LLaDA-1.5-Demo/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law</title>
<link>https://arxiv.org/abs/2505.19227</link>
<guid>https://arxiv.org/abs/2505.19227</guid>
<content:encoded><![CDATA[
arXiv:2505.19227v1 Announce Type: new 
Abstract: Recent works have highlighted optimization difficulties faced by gradient descent in training the first and last layers of transformer-based language models, which are overcome by optimizers such as Adam. These works suggest that the difficulty is linked to the heavy-tailed distribution of words in text data, where the frequency of the $k$th most frequent word $\pi_k$ is proportional to $1/k$, following Zipf's law. To better understand the impact of the data distribution on training performance, we study a linear bigram model for next-token prediction when the tokens follow a power law $\pi_k \propto 1/k^\alpha$ parameterized by the exponent $\alpha > 0$. We derive optimization scaling laws for deterministic gradient descent and sign descent as a proxy for Adam as a function of the exponent $\alpha$. Existing theoretical investigations in scaling laws assume that the eigenvalues of the data decay as a power law with exponent $\alpha > 1$. This assumption effectively makes the problem ``finite dimensional'' as most of the loss comes from a few of the largest eigencomponents. In comparison, we show that the problem is more difficult when the data have heavier tails. The case $\alpha = 1$ as found in text data is ``worst-case'' for gradient descent, in that the number of iterations required to reach a small relative error scales almost linearly with dimension. While the performance of sign descent also depends on the dimension, for Zipf-distributed data the number of iterations scales only with the square-root of the dimension, leading to a large improvement for large vocabularies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.19235</link>
<guid>https://arxiv.org/abs/2505.19235</guid>
<content:encoded><![CDATA[
arXiv:2505.19235v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees</title>
<link>https://arxiv.org/abs/2505.19238</link>
<guid>https://arxiv.org/abs/2505.19238</guid>
<content:encoded><![CDATA[
arXiv:2505.19238v1 Announce Type: new 
Abstract: Constrained decision-making is essential for designing safe policies in real-world control systems, yet simulated environments often fail to capture real-world adversities. We consider the problem of learning a policy that will maximize the cumulative reward while satisfying a constraint, even when there is a mismatch between the real model and an accessible simulator/nominal model. In particular, we consider the robust constrained Markov decision problem (RCMDP) where an agent needs to maximize the reward and satisfy the constraint against the worst possible stochastic model under the uncertainty set centered around an unknown nominal model. Primal-dual methods, effective for standard constrained MDP (CMDP), are not applicable here because of the lack of the strong duality property. Further, one cannot apply the standard robust value-iteration based approach on the composite value function either as the worst case models may be different for the reward value function and the constraint value function. We propose a novel technique that effectively minimizes the constraint value function--to satisfy the constraints; on the other hand, when all the constraints are satisfied, it can simply maximize the robust reward value function. We prove that such an algorithm finds a policy with at most $\epsilon$ sub-optimality and feasible policy after $O(\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we do not need to employ a binary search, thus, we reduce the computation time by at least 4x for smaller value of discount factor ($\gamma$) and by at least 6x for larger value of $\gamma$.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActiveDPO: Active Direct Preference Optimization for Sample-Efficient Alignment</title>
<link>https://arxiv.org/abs/2505.19241</link>
<guid>https://arxiv.org/abs/2505.19241</guid>
<content:encoded><![CDATA[
arXiv:2505.19241v1 Announce Type: new 
Abstract: The recent success of using human preferences to align large language models (LLMs) has significantly improved their performance in various downstream tasks like question answering, mathematical reasoning, and code generation. However,3 achieving effective LLM alignment depends on high-quality human preference datasets. Collecting these datasets requires human preference annotation, which is costly and resource-intensive, necessitating efficient active data selection methods. Existing methods either lack a strong theoretical foundation or depend on restrictive reward function assumptions (e.g., linearity). To this end, we propose an algorithm, ActiveDPO, that uses a theoretically grounded data selection criterion for non-linear reward functions while directly leveraging the LLM itself to parameterize the reward model that is used for active data selection. As a result, ActiveDPO explicitly accounts for the influence of LLM on data selection, unlike methods that select the data without considering the LLM that is being aligned, thereby leading to more effective and efficient data collection. Extensive experiments show that ActiveDPO outperforms existing methods across various models and datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To CoT or To Loop? A Formal Comparison Between Chain-of-Thought and Looped Transformers</title>
<link>https://arxiv.org/abs/2505.19245</link>
<guid>https://arxiv.org/abs/2505.19245</guid>
<content:encoded><![CDATA[
arXiv:2505.19245v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) and Looped Transformers have been shown to empirically improve performance on reasoning tasks and to theoretically enhance expressivity by recursively increasing the number of computational steps. However, their comparative capabilities are still not well understood. In this paper, we provide a formal analysis of their respective strengths and limitations. We show that Looped Transformers can efficiently simulate parallel computations for deterministic tasks, which we formalize as evaluation over directed acyclic graphs. In contrast, CoT with stochastic decoding excels at approximate inference for compositional structures, namely self-reducible problems. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical cues for choosing between reasoning paradigms.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Value Estimation Critically Enhances Vanilla Policy Gradient</title>
<link>https://arxiv.org/abs/2505.19247</link>
<guid>https://arxiv.org/abs/2505.19247</guid>
<content:encoded><![CDATA[
arXiv:2505.19247v1 Announce Type: new 
Abstract: Modern policy gradient algorithms, such as TRPO and PPO, outperform vanilla policy gradient in many RL tasks. Questioning the common belief that enforcing approximate trust regions leads to steady policy improvement in practice, we show that the more critical factor is the enhanced value estimation accuracy from more value update steps in each iteration. To demonstrate, we show that by simply increasing the number of value update steps per iteration, vanilla policy gradient itself can achieve performance comparable to or better than PPO in all the standard continuous control benchmark environments. Importantly, this simple change to vanilla policy gradient is significantly more robust to hyperparameter choices, opening up the possibility that RL algorithms may still become more effective and easier to use.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use</title>
<link>https://arxiv.org/abs/2505.19255</link>
<guid>https://arxiv.org/abs/2505.19255</guid>
<content:encoded><![CDATA[
arXiv:2505.19255v1 Announce Type: new 
Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the reasoning capabilities of large language models (LLMs) by enabling long chains of thought, self-correction, and effective tool use. While recent works attempt to extend RFT to vision-language models (VLMs), these efforts largely produce text-only reasoning conditioned on static image inputs, falling short of true multimodal reasoning in the response. In contrast, test-time methods like Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate multimodal chains of thought by interleaving text and intermediate visual reasoning steps. VTool-R1 integrates Python-based visual editing tools into the RFT process, enabling VLMs to learn when and how to generate visual reasoning steps that benefit final reasoning. Trained with outcome-based rewards tied to task accuracy, our approach elicits strategic visual tool use for reasoning without relying on process-based supervision. Experiments on structured visual question answering over charts and tables show that VTool-R1 enhances reasoning performance by teaching VLMs to "think with images" and generate multimodal chain of thoughts with tools.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Spatiotemporal Fusion Approach to Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2505.19258</link>
<guid>https://arxiv.org/abs/2505.19258</guid>
<content:encoded><![CDATA[
arXiv:2505.19258v1 Announce Type: new 
Abstract: With the increasing availability of meteorological data from various sensors, numerical models and reanalysis products, the need for efficient data integration methods has become paramount for improving weather forecasts and hydrometeorological studies. In this work, we propose a data fusion approach for precipitation nowcasting by integrating data from meteorological and rain gauge stations in Rio de Janeiro metropolitan area with ERA5 reanalysis data and GFS numerical weather prediction. We employ the spatiotemporal deep learning architecture called STConvS2S, leveraging a structured dataset covering a 9 x 11 grid. The study spans from January 2011 to October 2024, and we evaluate the impact of integrating three surface station systems. Among the tested configurations, the fusion-based model achieves an F1-score of 0.2033 for forecasting heavy precipitation events (greater than 25 mm/h) at a one-hour lead time. Additionally, we present an ablation study to assess the contribution of each station network and propose a refined inference strategy for precipitation nowcasting, integrating the GFS numerical weather prediction (NWP) data with in-situ observations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Large Reasoning Models for Agriculture</title>
<link>https://arxiv.org/abs/2505.19259</link>
<guid>https://arxiv.org/abs/2505.19259</guid>
<content:encoded><![CDATA[
arXiv:2505.19259v1 Announce Type: new 
Abstract: Agricultural decision-making involves complex, context-specific reasoning, where choices about crops, practices, and interventions depend heavily on geographic, climatic, and economic conditions. Traditional large language models (LLMs) often fall short in navigating this nuanced problem due to limited reasoning capacity. We hypothesize that recent advances in large reasoning models (LRMs) can better handle such structured, domain-specific inference. To investigate this, we introduce AgReason, the first expert-curated open-ended science benchmark with 100 questions for agricultural reasoning. Evaluations across thirteen open-source and proprietary models reveal that LRMs outperform conventional ones, though notable challenges persist, with the strongest Gemini-based baseline achieving 36% accuracy. We also present AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with human oversight and equipped with synthetically generated reasoning traces. Using AgThoughts, we develop AgThinker, a suite of small reasoning models that can be run on consumer-grade GPUs, and show that our dataset can be effective in unlocking agricultural reasoning abilities in LLMs. Our project page is here: https://baskargroup.github.io/Ag_reasoning/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cellular Traffic Prediction via Byzantine-robust Asynchronous Federated Learning</title>
<link>https://arxiv.org/abs/2505.19263</link>
<guid>https://arxiv.org/abs/2505.19263</guid>
<content:encoded><![CDATA[
arXiv:2505.19263v1 Announce Type: new 
Abstract: Network traffic prediction plays a crucial role in intelligent network operation. Traditional prediction methods often rely on centralized training, necessitating the transfer of vast amounts of traffic data to a central server. This approach can lead to latency and privacy concerns. To address these issues, federated learning integrated with differential privacy has emerged as a solution to improve data privacy and model robustness in distributed settings. Nonetheless, existing federated learning protocols are vulnerable to Byzantine attacks, which may significantly compromise model robustness. Developing a robust and privacy-preserving prediction model in the presence of Byzantine clients remains a significant challenge. To this end, we propose an asynchronous differential federated learning framework based on distributionally robust optimization. The proposed framework utilizes multiple clients to train the prediction model collaboratively with local differential privacy. In addition, regularization techniques have been employed to further improve the Byzantine robustness of the models. We have conducted extensive experiments on three real-world datasets, and the results elucidate that our proposed distributed algorithm can achieve superior performance over existing methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19281</link>
<guid>https://arxiv.org/abs/2505.19281</guid>
<content:encoded><![CDATA[
arXiv:2505.19281v1 Announce Type: new 
Abstract: Online reinforcement learning (RL) excels in complex, safety-critical domains, yet it faces challenges such as sample inefficiency, training instability, and a lack of interpretability. Data attribution offers a principled way to trace model behavior back to individual training samples. However, in online RL, each training sample not only drives policy updates but also influences future data collection, violating the fixed dataset assumption in existing attribution methods. In this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a local attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Overall, these results advance interpretability, efficiency, and effectiveness of online RL.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypercube-RAG: Hypercube-Based Retrieval-Augmented Generation for In-domain Scientific Question-Answering</title>
<link>https://arxiv.org/abs/2505.19288</link>
<guid>https://arxiv.org/abs/2505.19288</guid>
<content:encoded><![CDATA[
arXiv:2505.19288v1 Announce Type: new 
Abstract: Large language models (LLMs) often need to incorporate external knowledge to solve theme-specific problems. Retrieval-augmented generation (RAG), which empowers LLMs to generate more qualified responses with retrieved external data and knowledge, has shown its high promise. However, traditional semantic similarity-based RAGs struggle to return concise yet highly relevant information for domain knowledge-intensive tasks, such as scientific question-answering (QA). Built on a multi-dimensional (cube) structure called Hypercube, which can index documents in an application-driven, human-defined, multi-dimensional space, we introduce the Hypercube-RAG, a novel RAG framework for precise and efficient retrieval. Given a query, Hypercube-RAG first decomposes it based on its entities and topics and then retrieves relevant documents from cubes by aligning these decomposed components with hypercube dimensions. Experiments on three in-domain scientific QA datasets demonstrate that our method improves accuracy by 3.7% and boosts retrieval efficiency by 81.2%, measured as relative gains over the strongest RAG baseline. More importantly, our Hypercube-RAG inherently offers explainability by revealing the underlying predefined hypercube dimensions used for retrieval. The code and data sets are available at https://github.com/JimengShi/Hypercube-RAG.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept Reachability in Diffusion Models: Beyond Dataset Constraints</title>
<link>https://arxiv.org/abs/2505.19313</link>
<guid>https://arxiv.org/abs/2505.19313</guid>
<content:encoded><![CDATA[
arXiv:2505.19313v1 Announce Type: new 
Abstract: Despite significant advances in quality and complexity of the generations in text-to-image models, prompting does not always lead to the desired outputs. Controlling model behaviour by directly steering intermediate model activations has emerged as a viable alternative allowing to reach concepts in latent space that may otherwise remain inaccessible by prompt. In this work, we introduce a set of experiments to deepen our understanding of concept reachability. We design a training data setup with three key obstacles: scarcity of concepts, underspecification of concepts in the captions, and data biases with tied concepts. Our results show: (i) concept reachability in latent space exhibits a distinct phase transition, with only a small number of samples being sufficient to enable reachability, (ii) where in the latent space the intervention is performed critically impacts reachability, showing that certain concepts are reachable only at certain stages of transformation, and (iii) while prompting ability rapidly diminishes with a decrease in quality of the dataset, concepts often remain reliably reachable through steering. Model providers can leverage this to bypass costly retraining and dataset curation and instead innovate with user-facing control mechanisms.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paying Alignment Tax with Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.19327</link>
<guid>https://arxiv.org/abs/2505.19327</guid>
<content:encoded><![CDATA[
arXiv:2505.19327v1 Announce Type: new 
Abstract: Current debiasing approaches often result a degradation in model capabilities such as factual accuracy and knowledge retention. Through systematic evaluation across multiple benchmarks, we demonstrate that existing debiasing methods face fundamental trade-offs, particularly in smaller models, leading to reduced truthfulness, knowledge loss, or unintelligible outputs. To address these limitations, we propose a contrastive learning framework that learns through carefully constructed positive and negative examples. Our approach introduces contrast computation and dynamic loss scaling to balance bias mitigation with faithfulness preservation. Experimental results across multiple model scales demonstrate that our method achieves substantial improvements in both toxicity reduction and faithfulness preservation. Most importantly, we show that our framework is the first to consistently improve both metrics simultaneously, avoiding the capability degradation characteristic of existing approaches. These results suggest that explicit modeling of both positive and negative examples through contrastive learning could be a promising direction for reducing the alignment tax in language model debiasing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales</title>
<link>https://arxiv.org/abs/2505.19334</link>
<guid>https://arxiv.org/abs/2505.19334</guid>
<content:encoded><![CDATA[
arXiv:2505.19334v1 Announce Type: new 
Abstract: Large language models (LLMs) obtain state of the art zero shot relevance ranking performance on a variety of information retrieval tasks. The two most common prompts to elicit LLM relevance judgments are pointwise scoring (a.k.a. relevance generation), where the LLM sees a single query-document pair and outputs a single relevance score, and listwise ranking (a.k.a. permutation generation), where the LLM sees a query and a list of documents and outputs a permutation, sorting the documents in decreasing order of relevance. The current research community consensus is that listwise ranking yields superior performance, and significant research effort has been devoted to crafting LLM listwise ranking algorithms. The underlying hypothesis is that LLMs are better at making relative relevance judgments than absolute ones. In tension with this hypothesis, we find that the gap between pointwise scoring and listwise ranking shrinks when pointwise scoring is implemented using a sufficiently large ordinal relevance label space, becoming statistically insignificant for many LLM-benchmark dataset combinations (where ``significant'' means ``95\% confidence that listwise ranking improves NDCG@10''). Our evaluations span four LLMs, eight benchmark datasets from the BEIR and TREC-DL suites, and two proprietary datasets with relevance labels collected after the training cut-off of all LLMs evaluated.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Decision Transformers for Zero-Shot Reach-Avoid Policies</title>
<link>https://arxiv.org/abs/2505.19337</link>
<guid>https://arxiv.org/abs/2505.19337</guid>
<content:encoded><![CDATA[
arXiv:2505.19337v1 Announce Type: new 
Abstract: Offline goal-conditioned reinforcement learning methods have shown promise for reach-avoid tasks, where an agent must reach a target state while avoiding undesirable regions of the state space. Existing approaches typically encode avoid-region information into an augmented state space and cost function, which prevents flexible, dynamic specification of novel avoid-region information at evaluation time. They also rely heavily on well-designed reward and cost functions, limiting scalability to complex or poorly structured environments. We introduce RADT, a decision transformer model for offline, reward-free, goal-conditioned, avoid region-conditioned RL. RADT encodes goals and avoid regions directly as prompt tokens, allowing any number of avoid regions of arbitrary size to be specified at evaluation time. Using only suboptimal offline trajectories from a random policy, RADT learns reach-avoid behavior through a novel combination of goal and avoid-region hindsight relabeling. We benchmark RADT against 3 existing offline goal-conditioned RL models across 11 tasks, environments, and experimental settings. RADT generalizes in a zero-shot manner to out-of-distribution avoid region sizes and counts, outperforming baselines that require retraining. In one such zero-shot setting, RADT achieves 35.7% improvement in normalized cost over the best retrained baseline while maintaining high goal-reaching success. We apply RADT to cell reprogramming in biology, where it reduces visits to undesirable intermediate gene expression states during trajectories to desired target states, despite stochastic transitions and discrete, structured state dynamics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Multi-Device Inference Acceleration for Transformer Models</title>
<link>https://arxiv.org/abs/2505.19342</link>
<guid>https://arxiv.org/abs/2505.19342</guid>
<content:encoded><![CDATA[
arXiv:2505.19342v1 Announce Type: new 
Abstract: Transformer models power many AI applications but suffer from high inference latency, limiting their use in real-time settings. Multi-device inference can reduce latency by parallelizing computation. Yet, existing methods require high inter-device bandwidth, making them impractical for bandwidth-constrained environments. We propose ASTRA, a communication-efficient framework that accelerates Transformer inference through a novel integration of sequence parallelism and a Mixed-Precision Attention mechanism designed to minimize inter-device communication. ASTRA compresses non-local token embeddings via vector quantization and preserves task accuracy through two optimizations, Noise-Augmented Quantization and Distributed Class Tokens. Experiments on ViT and GPT2 across vision and NLP tasks show that ASTRA achieves up to 2.64X speedups over single-device inference and up to 15.25X speedups over state-of-the-art multi-device inferences, while operating under bandwidths as low as 10 Mbps. ASTRA is open-sourced at https://github.com/xl1990/Astra.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SETransformer: A Hybrid Attention-Based Architecture for Robust Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.19369</link>
<guid>https://arxiv.org/abs/2505.19369</guid>
<content:encoded><![CDATA[
arXiv:2505.19369v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) using wearable sensor data has become a central task in mobile computing, healthcare, and human-computer interaction. Despite the success of traditional deep learning models such as CNNs and RNNs, they often struggle to capture long-range temporal dependencies and contextual relevance across multiple sensor channels. To address these limitations, we propose SETransformer, a hybrid deep neural architecture that combines Transformer-based temporal modeling with channel-wise squeeze-and-excitation (SE) attention and a learnable temporal attention pooling mechanism. The model takes raw triaxial accelerometer data as input and leverages global self-attention to capture activity-specific motion dynamics over extended time windows, while adaptively emphasizing informative sensor channels and critical time steps.
  We evaluate SETransformer on the WISDM dataset and demonstrate that it significantly outperforms conventional models including LSTM, GRU, BiLSTM, and CNN baselines. The proposed model achieves a validation accuracy of 84.68\% and a macro F1-score of 84.64\%, surpassing all baseline architectures by a notable margin. Our results show that SETransformer is a competitive and interpretable solution for real-world HAR tasks, with strong potential for deployment in mobile and ubiquitous sensing applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment of large language models with constrained learning</title>
<link>https://arxiv.org/abs/2505.19387</link>
<guid>https://arxiv.org/abs/2505.19387</guid>
<content:encoded><![CDATA[
arXiv:2505.19387v1 Announce Type: new 
Abstract: We study the problem of computing an optimal large language model (LLM) policy for a constrained alignment problem, where the goal is to maximize a primary reward objective while satisfying constraints on secondary utilities. Despite the popularity of Lagrangian-based LLM policy search in constrained alignment, iterative primal-dual methods often fail to converge, and non-iterative dual-based methods do not achieve optimality in the LLM parameter space. To address these challenges, we employ Lagrangian duality to develop an iterative dual-based alignment method that alternates between updating the LLM policy via Lagrangian maximization and updating the dual variable via dual descent. In theory, we characterize the primal-dual gap between the primal value in the distribution space and the dual value in the LLM parameter space. We further quantify the optimality gap of the learned LLM policies at near-optimal dual variables with respect to both the objective and the constraint functions. These results prove that dual-based alignment methods can find an optimal constrained LLM policy, up to an LLM parametrization gap. We demonstrate the effectiveness and merits of our approach through extensive experiments conducted on the PKU-SafeRLHF dataset.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains</title>
<link>https://arxiv.org/abs/2505.19397</link>
<guid>https://arxiv.org/abs/2505.19397</guid>
<content:encoded><![CDATA[
arXiv:2505.19397v1 Announce Type: new 
Abstract: Time Series Foundation Models (TSFMs), which are pretrained on large-scale, cross-domain data and capable of zero-shot forecasting in new scenarios without further training, are increasingly adopted in real-world applications. However, as the zero-shot forecasting paradigm gets popular, a critical yet overlooked question emerges: Are TSFMs robust to adversarial input perturbations? Such perturbations could be exploited in man-in-the-middle attacks or data poisoning. To address this gap, we conduct a systematic investigation into the adversarial robustness of TSFMs. Our results show that even minimal perturbations can induce significant and controllable changes in forecast behaviors, including trend reversal, temporal drift, and amplitude shift, posing serious risks to TSFM-based services. Through experiments on representative TSFMs and multiple datasets, we reveal their consistent vulnerabilities and identify potential architectural designs, such as structural sparsity and multi-task pretraining, that may improve robustness. Our findings offer actionable guidance for designing more resilient forecasting systems and provide a critical assessment of the adversarial robustness of TSFMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning</title>
<link>https://arxiv.org/abs/2505.19404</link>
<guid>https://arxiv.org/abs/2505.19404</guid>
<content:encoded><![CDATA[
arXiv:2505.19404v1 Announce Type: new 
Abstract: Federated Active Learning (FAL) seeks to reduce the burden of annotation under the realistic constraints of federated learning by leveraging Active Learning (AL). As FAL settings make it more expensive to obtain ground truth labels, FAL strategies that work well in low-budget regimes, where the amount of annotation is very limited, are needed. In this work, we investigate the effectiveness of TypiClust, a successful low-budget AL strategy, in low-budget FAL settings. Our empirical results show that TypiClust works well even in low-budget FAL settings contrasted with relatively low performances of other methods, although these settings present additional challenges, such as data heterogeneity, compared to AL. In addition, we show that FAL settings cause distribution shifts in terms of typicality, but TypiClust is not very vulnerable to the shifts. We also analyze the sensitivity of TypiClust to feature extraction methods, and it suggests a way to perform FAL even in limited data situations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Future Link Prediction Without Memory or Aggregation</title>
<link>https://arxiv.org/abs/2505.19408</link>
<guid>https://arxiv.org/abs/2505.19408</guid>
<content:encoded><![CDATA[
arXiv:2505.19408v1 Announce Type: new 
Abstract: Future link prediction on temporal graphs is a fundamental task with wide applicability in real-world dynamic systems. These scenarios often involve both recurring (seen) and novel (unseen) interactions, requiring models to generalize effectively across both types of edges. However, existing methods typically rely on complex memory and aggregation modules, yet struggle to handle unseen edges. In this paper, we revisit the architecture of existing temporal graph models and identify two essential but overlooked modeling requirements for future link prediction: representing nodes with unique identifiers and performing target-aware matching between source and destination nodes. To this end, we propose Cross-Attention based Future Link Predictor on Temporal Graphs (CRAFT), a simple yet effective architecture that discards memory and aggregation modules and instead builds on two components: learnable node embeddings and cross-attention between the destination and the source's recent interactions. This design provides strong expressive power and enables target-aware modeling of the compatibility between candidate destinations and the source's interaction patterns. Extensive experiments on diverse datasets demonstrate that CRAFT consistently achieves superior performance with high efficiency, making it well-suited for large-scale real-world applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network</title>
<link>https://arxiv.org/abs/2505.19423</link>
<guid>https://arxiv.org/abs/2505.19423</guid>
<content:encoded><![CDATA[
arXiv:2505.19423v1 Announce Type: new 
Abstract: Evolutionary Reinforcement Learning (ERL), training the Reinforcement Learning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated enhanced exploration capabilities and greater robustness than using traditional policy gradient. However, ERL suffers from the high computational costs and low search efficiency, as EAs require evaluating numerous candidate policies with expensive simulations, many of which are ineffective and do not contribute meaningfully to the training. One intuitive way to reduce the ineffective evaluations is to adopt the surrogates. Unfortunately, existing ERL policies are often modeled as deep neural networks (DNNs) and thus naturally represented as high-dimensional vectors containing millions of weights, which makes the building of effective surrogates for ERL policies extremely challenging. This paper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE) and Hyperbolic Neural Networks (HNN). Specifically, AE compresses high-dimensional policies into low-dimensional representations while extracting key features as the inputs for the surrogate. HNN, functioning as a classification-based surrogate model, can learn complex nonlinear relationships from sampled data and enable more accurate pre-selection of the sampled policies without real evaluations. The experiments on 10 Atari and 4 Mujoco games have verified that the proposed method outperforms previous approaches significantly. The search trajectories guided by AE and HNN are also visually demonstrated to be more effective, in terms of both exploration and convergence. This paper not only presents the first learnable policy embedding and surrogate-modeling modules for high-dimensional ERL policies, but also empirically reveals when and why they can be successful.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference</title>
<link>https://arxiv.org/abs/2505.19427</link>
<guid>https://arxiv.org/abs/2505.19427</guid>
<content:encoded><![CDATA[
arXiv:2505.19427v1 Announce Type: new 
Abstract: The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise $\ell_2$-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to $2.94\%$ in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Importance Weighted Score Matching for Diffusion Samplers with Enhanced Mode Coverage</title>
<link>https://arxiv.org/abs/2505.19431</link>
<guid>https://arxiv.org/abs/2505.19431</guid>
<content:encoded><![CDATA[
arXiv:2505.19431v1 Announce Type: new 
Abstract: Training neural samplers directly from unnormalized densities without access to target distribution samples presents a significant challenge. A critical desideratum in these settings is achieving comprehensive mode coverage, ensuring the sampler captures the full diversity of the target distribution. However, prevailing methods often circumvent the lack of target data by optimizing reverse KL-based objectives. Such objectives inherently exhibit mode-seeking behavior, potentially leading to incomplete representation of the underlying distribution. While alternative approaches strive for better mode coverage, they typically rely on implicit mechanisms like heuristics or iterative refinement. In this work, we propose a principled approach for training diffusion-based samplers by directly targeting an objective analogous to the forward KL divergence, which is conceptually known to encourage mode coverage. We introduce \textit{Importance Weighted Score Matching}, a method that optimizes this desired mode-covering objective by re-weighting the score matching loss using tractable importance sampling estimates, thereby overcoming the absence of target distribution data. We also provide theoretical analysis of the bias and variance for our proposed Monte Carlo estimator and the practical loss function used in our method. Experiments on increasingly complex multi-modal distributions, including 2D Gaussian Mixture Models with up to 120 modes and challenging particle systems with inherent symmetries -- demonstrate that our approach consistently outperforms existing neural samplers across all distributional distance metrics, achieving state-of-the-art results on all benchmarks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced long-term earth system forecasting by learning the small-scale nature</title>
<link>https://arxiv.org/abs/2505.19432</link>
<guid>https://arxiv.org/abs/2505.19432</guid>
<content:encoded><![CDATA[
arXiv:2505.19432v1 Announce Type: new 
Abstract: Reliable long-term forecast of Earth system dynamics is heavily hampered by instabilities in current AI models during extended autoregressive simulations. These failures often originate from inherent spectral bias, leading to inadequate representation of critical high-frequency, small-scale processes and subsequent uncontrolled error amplification. We present Triton, an AI framework designed to address this fundamental challenge. Inspired by increasing grids to explicitly resolve small scales in numerical models, Triton employs a hierarchical architecture processing information across multiple resolutions to mitigate spectral bias and explicitly model cross-scale dynamics. We demonstrate Triton's superior performance on challenging forecast tasks, achieving stable year-long global temperature forecasts, skillful Kuroshio eddy predictions till 120 days, and high-fidelity turbulence simulations preserving fine-scale structures all without external forcing, with significantly surpassing baseline AI models in long-term stability and accuracy. By effectively suppressing high-frequency error accumulation, Triton offers a promising pathway towards trustworthy AI-driven simulation for climate and earth system science.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</title>
<link>https://arxiv.org/abs/2505.19433</link>
<guid>https://arxiv.org/abs/2505.19433</guid>
<content:encoded><![CDATA[
arXiv:2505.19433v1 Announce Type: new 
Abstract: Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration</title>
<link>https://arxiv.org/abs/2505.19445</link>
<guid>https://arxiv.org/abs/2505.19445</guid>
<content:encoded><![CDATA[
arXiv:2505.19445v1 Announce Type: new 
Abstract: The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains like healthcare and finance demands reliable explanations of their decision-making processes. While inherently interpretable GNN architectures like Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to generating explanations based on spurious correlations, potentially undermining trust in critical applications. We present MetaGMT, a meta-learning framework that enhances explanation fidelity through a novel bi-level optimization approach. We demonstrate that MetaGMT significantly improves both explanation quality (AUC-ROC, Precision@K) and robustness to spurious patterns, across BA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive classification accuracy while producing more faithful explanations (with an increase up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline methods. These advancements in interpretability could enable safer deployment of GNNs in sensitive domains by (1) facilitating model debugging through more reliable explanations, (2) supporting targeted retraining when biases are identified, and (3) enabling meaningful human oversight. By addressing the critical challenge of explanation reliability, our work contributes to building more trustworthy and actionable GNN systems for real-world applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians</title>
<link>https://arxiv.org/abs/2505.19458</link>
<guid>https://arxiv.org/abs/2505.19458</guid>
<content:encoded><![CDATA[
arXiv:2505.19458v1 Announce Type: new 
Abstract: The theoretical understanding of self-attention (SA) has been steadily progressing. A prominent line of work studies a class of SA layers that admit an energy function decreased by state updates. While it provides valuable insights into inherent biases in signal propagation, it often relies on idealized assumptions or additional constraints not necessarily present in standard SA. Thus, to broaden our understanding, this work aims to relax these energy constraints and provide an energy-agnostic characterization of inference dynamics by dynamical systems analysis. In more detail, we first consider relaxing the symmetry and single-head constraints traditionally required in energy-based formulations. Next, to investigate more general SA architectures capable of oscillatory dynamics without necessarily admitting an energy function, we analyze the Jacobian matrix of the state. We reveal that normalization layers effectively normalize the Jacobian's complex eigenvalues, forcing the dynamics close to a critical state. This significantly enhances inference performance. Furthermore, we utilize the Jacobian perspective to develop regularization methods for training and a pseudo-energy for monitoring inference dynamics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation</title>
<link>https://arxiv.org/abs/2505.19459</link>
<guid>https://arxiv.org/abs/2505.19459</guid>
<content:encoded><![CDATA[
arXiv:2505.19459v1 Announce Type: new 
Abstract: Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative models, are well known for their ability to achieve both high classification accuracy and generative capability within a single model. However, their robustness still lags significantly behind the classifiers based adversarial training (AT). Conversely, while AT is currently the most effective approach to improving the classifier's robustness, it typically sacrifices accuracy on clean data and lacks generative capability. The triple trade-off between classification accuracy, generative capability and robustness, raises a natural question: Can a single model simultaneously achieve high classification accuracy, adversarial robustness, and generative performance? -- a goal that has been rarely explored. To address this question, we systematically analyze the energy distribution differences of clean, adversarial, and generated samples across various JEM variants and adversarially trained models. We observe that AT tends to reduce the energy gap between clean and adversarial samples, while JEMs reduce the gap between clean and synthetic ones. This observation suggests a key insight: if the energy distributions of all three data types can be aligned, we might unify the strengths of AT and JEMs, resolving their inherent trade-offs. Building on this idea, we propose Energy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly model the clean data distribution, the adversarial distribution, and the classifier by maximizing their joint probability. EB-JDAT is a general and flexible optimization method, compatible with various JEM variants. Extensive experimental results demonstrate that EB-JDAT not only maintains near original accuracy and generative capability of JEMs, but also significantly enhances robustness, even surpassing state-of-the-art ATs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding</title>
<link>https://arxiv.org/abs/2505.19465</link>
<guid>https://arxiv.org/abs/2505.19465</guid>
<content:encoded><![CDATA[
arXiv:2505.19465v1 Announce Type: new 
Abstract: This letter proposes a deep-learning (DL)-based multi-user channel state information (CSI) feedback framework for massive multiple-input multiple-output systems, where the deep joint source-channel coding (DJSCC) is utilized to improve the CSI reconstruction accuracy. Specifically, we design a multi-user joint CSI feedback framework, whereby the CSI correlation of nearby users is utilized to reduce the feedback overhead. Under the framework, we propose a new residual cross-attention transformer architecture, which is deployed at the base station to further improve the CSI feedback performance. Moreover, to tackle the "cliff-effect" of conventional bit-level CSI feedback approaches, we integrated DJSCC into the multi-user CSI feedback, together with utilizing a two-stage training scheme to adapt to varying uplink noise levels. Experimental results demonstrate the superiority of our methods in CSI feedback performance, with low network complexity and better scalability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory</title>
<link>https://arxiv.org/abs/2505.19469</link>
<guid>https://arxiv.org/abs/2505.19469</guid>
<content:encoded><![CDATA[
arXiv:2505.19469v1 Announce Type: new 
Abstract: Dataset distillation enables the training of deep neural networks with comparable performance in significantly reduced time by compressing large datasets into small and representative ones. Although the introduction of generative models has made great achievements in this field, the distributions of their distilled datasets are not diverse enough to represent the original ones, leading to a decrease in downstream validation accuracy. In this paper, we present a diversity-driven generative dataset distillation method based on a diffusion model to solve this problem. We introduce self-adaptive memory to align the distribution between distilled and real datasets, assessing the representativeness. The degree of alignment leads the diffusion model to generate more diverse datasets during the distillation process. Extensive experiments show that our method outperforms existing state-of-the-art methods in most situations, proving its ability to tackle dataset distillation tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs</title>
<link>https://arxiv.org/abs/2505.19481</link>
<guid>https://arxiv.org/abs/2505.19481</guid>
<content:encoded><![CDATA[
arXiv:2505.19481v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency quality trade off, it remains underexplored in the context of LLM based agents. In this work, we present the first systematic study of this trade off in real time decision making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency aware evaluation and deployment strategies for LLM based agents. These results demonstrate the critical importance of latency aware evaluation and deployment strategies for real world LLM based agents. Our benchmarks are available at Latency Sensitive Benchmarks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Transformer from the Perspective of Associative Memory</title>
<link>https://arxiv.org/abs/2505.19488</link>
<guid>https://arxiv.org/abs/2505.19488</guid>
<content:encoded><![CDATA[
arXiv:2505.19488v1 Announce Type: new 
Abstract: In this paper, we share our reflections and insights on understanding Transformer architectures through the lens of associative memory--a classic psychological concept inspired by human cognition. We start with the basics of associative memory (think simple linear attention) and then dive into two dimensions:
  Memory Capacity: How much can a Transformer really remember, and how well? We introduce retrieval SNR to measure this and use a kernel perspective to mathematically reveal why Softmax Attention is so effective. We also show how FFNs can be seen as a type of associative memory, leading to insights on their design and potential improvements.
  Memory Update: How do these memories learn and evolve? We present a unified framework for understanding how different Transformer variants (like DeltaNet and Softmax Attention) update their "knowledge base". This leads us to tackle two provocative questions: 1. Are Transformers fundamentally limited in what they can express, and can we break these barriers? 2. If a Transformer had infinite context, would it become infinitely intelligent?
  We want to demystify Transformer architecture, offering a clearer understanding of existing designs. This exploration aims to provide fresh insights and spark new avenues for Transformer innovation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval</title>
<link>https://arxiv.org/abs/2505.19491</link>
<guid>https://arxiv.org/abs/2505.19491</guid>
<content:encoded><![CDATA[
arXiv:2505.19491v1 Announce Type: new 
Abstract: Reflecting the greater significance of recent history over the distant past in non-stationary environments, $\lambda$-discounted regret has been introduced in online convex optimization (OCO) to gracefully forget past data as new information arrives. When the discount factor $\lambda$ is given, online gradient descent with an appropriate step size achieves an $O(1/\sqrt{1-\lambda})$ discounted regret. However, the value of $\lambda$ is often not predetermined in real-world scenarios. This gives rise to a significant open question: is it possible to develop a discounted algorithm that adapts to an unknown discount factor. In this paper, we affirmatively answer this question by providing a novel analysis to demonstrate that smoothed OGD (SOGD) achieves a uniform $O(\sqrt{\log T/1-\lambda})$ discounted regret, holding for all values of $\lambda$ across a continuous interval simultaneously. The basic idea is to maintain multiple OGD instances to handle different discount factors, and aggregate their outputs sequentially by an online prediction algorithm named as Discounted-Normal-Predictor (DNP) (Kapralov and Panigrahy,2010). Our analysis reveals that DNP can combine the decisions of two experts, even when they operate on discounted regret with different discount factors.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning for Dynamic Combinatorial Optimization without Training Data</title>
<link>https://arxiv.org/abs/2505.19497</link>
<guid>https://arxiv.org/abs/2505.19497</guid>
<content:encoded><![CDATA[
arXiv:2505.19497v1 Announce Type: new 
Abstract: We introduce DyCO-GNN, a novel unsupervised learning framework for Dynamic Combinatorial Optimization that requires no training data beyond the problem instance itself. DyCO-GNN leverages structural similarities across time-evolving graph snapshots to accelerate optimization while maintaining solution quality. We evaluate DyCO-GNN on dynamic maximum cut, maximum independent set, and the traveling salesman problem across diverse datasets of varying sizes, demonstrating its superior performance under tight and moderate time budgets. DyCO-GNN consistently outperforms the baseline methods, achieving high-quality solutions up to 3-60x faster, highlighting its practical effectiveness in rapidly evolving resource-constrained settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.19504</link>
<guid>https://arxiv.org/abs/2505.19504</guid>
<content:encoded><![CDATA[
arXiv:2505.19504v1 Announce Type: new 
Abstract: Large Language Models (LLMs) represent substantial intellectual and economic investments, yet their effectiveness can inadvertently facilitate model imitation via knowledge distillation (KD).In practical scenarios, competitors can distill proprietary LLM capabilities by simply observing publicly accessible outputs, akin to reverse-engineering a complex performance by observation alone. Existing protective methods like watermarking only identify imitation post-hoc, while other defenses assume the student model mimics the teacher's internal logits, rendering them ineffective against distillation purely from observed output text. This paper confronts the challenge of actively protecting LLMs within the realistic constraints of API-based access. We introduce an effective and efficient Defensive Output Generation (DOGe) strategy that subtly modifies the output behavior of an LLM. Its outputs remain accurate and useful for legitimate users, yet are designed to be misleading for distillation, significantly undermining imitation attempts. We achieve this by fine-tuning only the final linear layer of the teacher LLM with an adversarial loss. This targeted training approach anticipates and disrupts distillation attempts during inference time. Our experiments show that, while preserving or even improving the original performance of the teacher model, student models distilled from the defensively generated teacher outputs demonstrate catastrophically reduced performance, demonstrating our method's effectiveness as a practical safeguard against KD-based model imitation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2505.19509</link>
<guid>https://arxiv.org/abs/2505.19509</guid>
<content:encoded><![CDATA[
arXiv:2505.19509v1 Announce Type: new 
Abstract: Large Multimodal Models(LMMs) face notable challenges when encountering multimodal knowledge conflicts, particularly under retrieval-augmented generation(RAG) frameworks where the contextual information from external sources may contradict the model's internal parametric knowledge, leading to unreliable outputs. However, existing benchmarks fail to reflect such realistic conflict scenarios. Most focus solely on intra-memory conflicts, while context-memory and inter-context conflicts remain largely investigated. Furthermore, commonly used factual knowledge-based evaluations are often overlooked, and existing datasets lack a thorough investigation into conflict detection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark designed to evaluate factual knowledge conflicts in both context-memory and inter-context scenarios. MMKC-Bench encompasses three types of multimodal knowledge conflicts and includes 1,573 knowledge instances and 3,381 images across 23 broad types, collected through automated pipelines with human verification. We evaluate three representative series of LMMs on both model behavior analysis and conflict detection tasks. Our findings show that while current LMMs are capable of recognizing knowledge conflicts, they tend to favor internal parametric knowledge over external evidence. We hope MMKC-Bench will foster further research in multimodal knowledge conflict and enhance the development of multimodal RAG systems. The source code is available at https://github.com/MLLMKCBENCH/MLLMKC.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate</title>
<link>https://arxiv.org/abs/2505.19525</link>
<guid>https://arxiv.org/abs/2505.19525</guid>
<content:encoded><![CDATA[
arXiv:2505.19525v1 Announce Type: new 
Abstract: Effectively managing missing modalities is a fundamental challenge in real-world multimodal learning scenarios, where data incompleteness often results from systematic collection errors or sensor failures. Sparse Mixture-of-Experts (SMoE) architectures have the potential to naturally handle multimodal data, with individual experts specializing in different modalities. However, existing SMoE approach often lacks proper ability to handle missing modality, leading to performance degradation and poor generalization in real-world applications. We propose Conf-SMoE to introduce a two-stage imputation module to handle the missing modality problem for the SMoE architecture and reveal the insight of expert collapse from theoretical analysis with strong empirical evidence. Inspired by our theoretical analysis, Conf-SMoE propose a novel expert gating mechanism by detaching the softmax routing score to task confidence score w.r.t ground truth. This naturally relieves expert collapse without introducing additional load balance loss function. We show that the insights of expert collapse aligns with other gating mechanism such as Gaussian and Laplacian gate. We also evaluate the proposed method on four different real world dataset with three different experiment settings to conduct comprehensive the analysis of Conf-SMoE on modality fusion and resistance to missing modality.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation</title>
<link>https://arxiv.org/abs/2505.19527</link>
<guid>https://arxiv.org/abs/2505.19527</guid>
<content:encoded><![CDATA[
arXiv:2505.19527v1 Announce Type: new 
Abstract: Training large neural networks through gradient-based optimization requires navigating high-dimensional loss landscapes, which often exhibit pathological geometry, leading to undesirable training dynamics. In particular, poor generalization frequently results from convergence to sharp minima that are highly sensitive to input perturbations, causing the model to overfit the training data while failing to generalize to unseen examples. Furthermore, these optimization procedures typically display strong dependence on the fine structure of the loss landscape, leading to unstable training dynamics, due to the fractal-like nature of the loss surface. In this work, we propose an alternative optimizer that simultaneously reduces this dependence, and avoids sharp minima, thereby improving generalization. This is achieved by simulating the motion of the center of a ball rolling on the loss landscape. The degree to which our optimizer departs from the standard gradient descent is controlled by a hyperparameter, representing the radius of the ball. Changing this hyperparameter allows for probing the loss landscape at different scales, making it a valuable tool for understanding its geometry.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimalist Softmax Attention Provably Learns Constrained Boolean Functions</title>
<link>https://arxiv.org/abs/2505.19531</link>
<guid>https://arxiv.org/abs/2505.19531</guid>
<content:encoded><![CDATA[
arXiv:2505.19531v1 Announce Type: new 
Abstract: We study the computational limits of learning $k$-bit Boolean functions (specifically, $\mathrm{AND}$, $\mathrm{OR}$, and their noisy variants), using a minimalist single-head softmax-attention mechanism, where $k=\Theta(d)$ relevant bits are selected from $d$ inputs. We show that these simple $\mathrm{AND}$ and $\mathrm{OR}$ functions are unsolvable with a single-head softmax-attention mechanism alone. However, with teacher forcing, the same minimalist attention is capable of solving them. These findings offer two key insights: Architecturally, solving these Boolean tasks requires only minimalist attention, without deep Transformer blocks or FFNs. Methodologically, one gradient descent update with supervision suffices and replaces the multi-step Chain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for solving Boolean problems. Together, the bounds expose a fundamental gap between what this minimal architecture achieves under ideal supervision and what is provably impossible under standard training.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19532</link>
<guid>https://arxiv.org/abs/2505.19532</guid>
<content:encoded><![CDATA[
arXiv:2505.19532v1 Announce Type: new 
Abstract: The current state-of-the-art backdoor attacks against Reinforcement Learning (RL) rely upon unrealistically permissive access models, that assume the attacker can read (or even write) the victim's policy parameters, observations, or rewards. In this work, we question whether such a strong assumption is required to launch backdoor attacks against RL. To answer this question, we propose the \underline{S}upply-\underline{C}h\underline{a}in \underline{B}ackdoor (SCAB) attack, which targets a common RL workflow: training agents using external agents that are provided separately or embedded within the environment. In contrast to prior works, our attack only relies on legitimate interactions of the RL agent with the supplied agents. Despite this limited access model, by poisoning a mere $3\%$ of training experiences, our attack can successfully activate over $90\%$ of triggered actions, reducing the average episodic return by $80\%$ for the victim. Our novel attack demonstrates that RL attacks are likely to become a reality under untrusted RL training supply-chains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models</title>
<link>https://arxiv.org/abs/2505.19533</link>
<guid>https://arxiv.org/abs/2505.19533</guid>
<content:encoded><![CDATA[
arXiv:2505.19533v1 Announce Type: new 
Abstract: Large language models (LLMs) face significant challenges in ex-ante reasoning, where analysis, inference, or predictions must be made without access to information from future events. Even with explicit prompts enforcing temporal cutoffs, LLMs often generate outputs influenced by internalized knowledge of events beyond the specified cutoff. This paper introduces a novel task and benchmark designed to evaluate the ability of LLMs to reason while adhering to such temporal constraints. The benchmark includes a variety of tasks: stock prediction, Wikipedia event prediction, scientific publication prediction, and Question Answering (QA), designed to assess factual knowledge under temporal cutoff constraints. We use leakage rate to quantify models' reliance on future information beyond cutoff timestamps. Experimental results reveal that LLMs struggle to consistently adhere to temporal cutoffs across common prompting strategies and tasks, demonstrating persistent challenges in ex-ante reasoning. This benchmark provides a potential evaluation framework to advance the development of LLMs' temporal reasoning ability for time-sensitive applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating</title>
<link>https://arxiv.org/abs/2505.19543</link>
<guid>https://arxiv.org/abs/2505.19543</guid>
<content:encoded><![CDATA[
arXiv:2505.19543v1 Announce Type: new 
Abstract: Knowledge Tracing (KT) is a core component of Intelligent Tutoring Systems, modeling learners' knowledge state to predict future performance and provide personalized learning support. Traditional KT models assume that learners' learning abilities remain relatively stable over short periods or change in predictable ways based on prior performance. However, in reality, learners' abilities change irregularly due to factors like cognitive fatigue, motivation, and external stress -- a task introduced, which we refer to as Real-time Learning Pattern Adjustment (RLPA). Existing KT models, when faced with RLPA, lack sufficient adaptability, because they fail to timely account for the dynamic nature of different learners' evolving learning patterns. Current strategies for enhancing adaptability rely on retraining, which leads to significant overfitting and high time overhead issues. To address this, we propose Cuff-KT, comprising a controller and a generator. The controller assigns value scores to learners, while the generator generates personalized parameters for selected learners. Cuff-KT controllably adapts to data changes fast and flexibly without fine-tuning. Experiments on five datasets from different subjects demonstrate that Cuff-KT significantly improves the performance of five KT models with different structures under intra- and inter-learner shifts, with an average relative increase in AUC of 10% and 4%, respectively, at a negligible time cost, effectively tackling RLPA task. Our code and datasets are fully available at https://github.com/zyy-2001/Cuff-KT.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization</title>
<link>https://arxiv.org/abs/2505.19547</link>
<guid>https://arxiv.org/abs/2505.19547</guid>
<content:encoded><![CDATA[
arXiv:2505.19547v1 Announce Type: new 
Abstract: Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful tool for modeling dynamic graph-structured data across diverse domains. However, they often fail to generalize in Spatio-Temporal Out-of-Distribution (STOOD) scenarios, where both temporal dynamics and spatial structures evolve beyond the training distribution. To address this problem, we propose an innovative Spatio-Temporal Retrieval-Augmented Pattern Learning framework,STRAP, which enhances model generalization by integrating retrieval-augmented learning into the STGNN continue learning pipeline. The core of STRAP is a compact and expressive pattern library that stores representative spatio-temporal patterns enriched with historical, structural, and semantic information, which is obtained and optimized during the training phase. During inference, STRAP retrieves relevant patterns from this library based on similarity to the current input and injects them into the model via a plug-and-play prompting mechanism. This not only strengthens spatio-temporal representations but also mitigates catastrophic forgetting. Moreover, STRAP introduces a knowledge-balancing objective to harmonize new information with retrieved knowledge. Extensive experiments across multiple real-world streaming graph datasets show that STRAP consistently outperforms state-of-the-art STGNN baselines on STOOD tasks, demonstrating its robustness, adaptability, and strong generalization capability without task-specific fine-tuning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On scalable and efficient training of diffusion samplers</title>
<link>https://arxiv.org/abs/2505.19552</link>
<guid>https://arxiv.org/abs/2505.19552</guid>
<content:encoded><![CDATA[
arXiv:2505.19552v1 Announce Type: new 
Abstract: We address the challenge of training diffusion models to sample from unnormalized energy distributions in the absence of data, the so-called diffusion samplers. Although these approaches have shown promise, they struggle to scale in more demanding scenarios where energy evaluations are expensive and the sampling space is high-dimensional. To address this limitation, we propose a scalable and sample-efficient framework that properly harmonizes the powerful classical sampling method and the diffusion sampler. Specifically, we utilize Monte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy as a Searcher to collect off-policy samples, using an auxiliary energy function to compensate for exploring modes the diffusion sampler rarely visits. These off-policy samples are then combined with on-policy data to train the diffusion sampler, thereby expanding its coverage of the energy landscape. Furthermore, we identify primacy bias, i.e., the preference of samplers for early experience during training, as the main cause of mode collapse during training, and introduce a periodic re-initialization trick to resolve this issue. Our method significantly improves sample efficiency on standard benchmarks for diffusion samplers and also excels at higher-dimensional problems and real-world molecular conformer generation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams</title>
<link>https://arxiv.org/abs/2505.19561</link>
<guid>https://arxiv.org/abs/2505.19561</guid>
<content:encoded><![CDATA[
arXiv:2505.19561v1 Announce Type: new 
Abstract: Sketches, probabilistic structures for estimating item frequencies in infinite data streams with limited space, are widely used across various domains. Recent studies have shifted the focus from handcrafted sketches to neural sketches, leveraging memory-augmented neural networks (MANNs) to enhance the streaming compression capabilities and achieve better space-accuracy trade-offs.However, existing neural sketches struggle to scale across different data domains and space budgets due to inflexible MANN configurations. In this paper, we introduce a scalable MANN architecture that brings to life the {\it Lego sketch}, a novel sketch with superior scalability and accuracy. Much like assembling creations with modular Lego bricks, the Lego sketch dynamically coordinates multiple memory bricks to adapt to various space budgets and diverse data domains. Our theoretical analysis guarantees its high scalability and provides the first error bound for neural sketch. Furthermore, extensive experimental evaluations demonstrate that the Lego sketch exhibits superior space-accuracy trade-offs, outperforming existing handcrafted and neural sketches. Our code is available at https://github.com/FFY0/LegoSketch_ICML.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing</title>
<link>https://arxiv.org/abs/2505.19578</link>
<guid>https://arxiv.org/abs/2505.19578</guid>
<content:encoded><![CDATA[
arXiv:2505.19578v1 Announce Type: new 
Abstract: Sparse attention methods exploit the inherent sparsity in attention to speed up the prefilling phase of long-context inference, mitigating the quadratic complexity of full attention computation. While existing sparse attention methods rely on predefined patterns or inaccurate estimations to approximate attention behavior, they often fail to fully capture the true dynamics of attention, resulting in reduced efficiency and compromised accuracy. Instead, we propose a highly accurate sparse attention mechanism that shares similar yet precise attention patterns across heads, enabling a more realistic capture of the dynamic behavior of attention. Our approach is grounded in two key observations: (1) attention patterns demonstrate strong inter-head similarity, and (2) this similarity remains remarkably consistent across diverse inputs. By strategically sharing computed accurate patterns across attention heads, our method effectively captures actual patterns while requiring full attention computation for only a small subset of heads. Comprehensive evaluations demonstrate that our approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts</title>
<link>https://arxiv.org/abs/2505.19587</link>
<guid>https://arxiv.org/abs/2505.19587</guid>
<content:encoded><![CDATA[
arXiv:2505.19587v1 Announce Type: new 
Abstract: Conformal prediction (CP) provides a framework for constructing prediction sets with guaranteed coverage, assuming exchangeable data. However, real-world scenarios often involve distribution shifts that violate exchangeability, leading to unreliable coverage and inflated prediction sets. To address this challenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction (RLSCP), which utilizes reconstruction losses derived from a Variational Autoencoder (VAE) as an uncertainty metric to scale score functions. While RLSCP demonstrates performance improvements, mainly resulting in better coverage, it quantifies quantiles based on a fixed calibration dataset without considering the discrepancies between test and train datasets in an unexchangeable setting. In the next step, we propose Weighted Quantile Loss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating a weighted notion of exchangeability, adjusting the calibration quantile threshold based on weights with respect to the ratio of calibration and test loss values. This approach improves the CP-generated prediction set outputs in the presence of distribution shifts. Experiments on large-scale datasets, including ImageNet variants, demonstrate that WQLCP outperforms existing baselines by consistently maintaining coverage while reducing prediction set sizes, providing a robust solution for CP under distribution shifts.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Agnostic Differentially Private Causal Inference</title>
<link>https://arxiv.org/abs/2505.19589</link>
<guid>https://arxiv.org/abs/2505.19589</guid>
<content:encoded><![CDATA[
arXiv:2505.19589v1 Announce Type: new 
Abstract: Estimating causal effects from observational data is essential in fields such as medicine, economics and social sciences, where privacy concerns are paramount. We propose a general, model-agnostic framework for differentially private estimation of average treatment effects (ATE) that avoids strong structural assumptions on the data-generating process or the models used to estimate propensity scores and conditional outcomes. In contrast to prior work, which enforces differential privacy by directly privatizing these nuisance components and results in a privacy cost that scales with model complexity, our approach decouples nuisance estimation from privacy protection. This separation allows the use of flexible, state-of-the-art black-box models, while differential privacy is achieved by perturbing only predictions and aggregation steps within a fold-splitting scheme with ensemble techniques. We instantiate the framework for three classical estimators -- the G-formula, inverse propensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal utility and privacy guarantees. Empirical results show that our methods maintain competitive performance under realistic privacy budgets. We further extend our framework to support meta-analysis of multiple private ATE estimates. Our results bridge a critical gap between causal inference and privacy-preserving data analysis.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason without External Rewards</title>
<link>https://arxiv.org/abs/2505.19590</link>
<guid>https://arxiv.org/abs/2505.19590</guid>
<content:encoded><![CDATA[
arXiv:2505.19590v1 Announce Type: new 
Abstract: Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Optimization by Estimating the Ratio of the Data Distribution</title>
<link>https://arxiv.org/abs/2505.19601</link>
<guid>https://arxiv.org/abs/2505.19601</guid>
<content:encoded><![CDATA[
arXiv:2505.19601v1 Announce Type: new 
Abstract: Direct preference optimization (DPO) is widely used as a simple and stable method for aligning large language models (LLMs) with human preferences. This paper investigates a generalized DPO loss that enables a policy model to match the target policy from a likelihood ratio estimation perspective. The ratio of the target policy provides a unique identification of the policy distribution without relying on reward models or partition functions. This allows the generalized loss to retain both simplicity and theoretical guarantees, which prior work such as $f$-PO fails to achieve simultaneously. We propose Bregman preference optimization (BPO), a generalized framework for ratio matching that provides a family of objective functions achieving target policy optimality. BPO subsumes DPO as a special case and offers tractable forms for all instances, allowing implementation with a few lines of code. We further develop scaled Basu's power divergence (SBA), a gradient scaling method that can be used for BPO instances. The BPO framework complements other DPO variants and is applicable to target policies defined by these variants. In experiments, unlike other probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a trade-off between generation fidelity and diversity, instances of BPO improve both win rate and entropy compared with DPO. When applied to Llama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B backbones, with a 55.9\% length-controlled win rate on AlpacaEval2.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression</title>
<link>https://arxiv.org/abs/2505.19602</link>
<guid>https://arxiv.org/abs/2505.19602</guid>
<content:encoded><![CDATA[
arXiv:2505.19602v1 Announce Type: new 
Abstract: Visual Autoregressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction approach, which yields substantial improvements in efficiency, scalability, and zero-shot generalization. Nevertheless, the coarse-to-fine methodology inherent in VAR results in exponential growth of the KV cache during inference, causing considerable memory consumption and computational redundancy. To address these bottlenecks, we introduce ScaleKV, a novel KV cache compression framework tailored for VAR architectures. ScaleKV leverages two critical observations: varying cache demands across transformer layers and distinct attention patterns at different scales. Based on these insights, ScaleKV categorizes transformer layers into two functional groups: drafters and refiners. Drafters exhibit dispersed attention across multiple scales, thereby requiring greater cache capacity. Conversely, refiners focus attention on the current token map to process local details, consequently necessitating substantially reduced cache capacity. ScaleKV optimizes the multi-scale inference pipeline by identifying scale-specific drafters and refiners, facilitating differentiated cache management tailored to each scale. Evaluation on the state-of-the-art text-to-image VAR model family, Infinity, demonstrates that our approach effectively reduces the required KV cache memory to 10% while preserving pixel-level fidelity.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kuramoto-FedAvg: Using Synchronization Dynamics to Improve Federated Learning Optimization under Statistical Heterogeneity</title>
<link>https://arxiv.org/abs/2505.19605</link>
<guid>https://arxiv.org/abs/2505.19605</guid>
<content:encoded><![CDATA[
arXiv:2505.19605v1 Announce Type: new 
Abstract: Federated learning on heterogeneous (non-IID) client data experiences slow convergence due to client drift. To address this challenge, we propose Kuramoto-FedAvg, a federated optimization algorithm that reframes the weight aggregation step as a synchronization problem inspired by the Kuramoto model of coupled oscillators. The server dynamically weighs each client's update based on its phase alignment with the global update, amplifying contributions that align with the global gradient direction while minimizing the impact of updates that are out of phase. We theoretically prove that this synchronization mechanism reduces client drift, providing a tighter convergence bound compared to the standard FedAvg under heterogeneous data distributions. Empirical validation supports our theoretical findings, showing that Kuramoto-FedAvg significantly accelerates convergence and improves accuracy across multiple benchmark datasets. Our work highlights the potential of coordination and synchronization-based strategies for managing gradient diversity and accelerating federated optimization in realistic non-IID settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-based Preference Optimization for Test-time Adaptation</title>
<link>https://arxiv.org/abs/2505.19607</link>
<guid>https://arxiv.org/abs/2505.19607</guid>
<content:encoded><![CDATA[
arXiv:2505.19607v1 Announce Type: new 
Abstract: Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation to target distributions that differ from training distributions, improving real-world generalizability. Existing TTA approaches focus on adjusting the conditional distribution; however these methods often depend on uncertain predictions in the absence of label information, leading to unreliable performance. Energy-based frameworks suggest a promising alternative to address distribution shifts without relying on uncertain predictions, instead computing the marginal distribution of target data. However, they involve the critical challenge of requiring extensive SGLD sampling, which is impractical for test-time scenarios requiring immediate adaptation. In this work, we propose Energy-based Preference Optimization for Test-time Adaptation (EPOTTA), which is based on a sampling free strategy. We first parameterize the target model using a pretrained model and residual energy function, enabling marginal likelihood maximization of target data without sampling. Building on the observation that the parameterization is mathematically equivalent to DPO objective, we then directly adapt the model to a target distribution without explicitly training the residual. Our experiments verify that EPOTTA is well-calibrated and performant while achieving computational efficiency.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling</title>
<link>https://arxiv.org/abs/2505.19609</link>
<guid>https://arxiv.org/abs/2505.19609</guid>
<content:encoded><![CDATA[
arXiv:2505.19609v1 Announce Type: new 
Abstract: Long-context supervised fine-tuning (Long-SFT) plays a vital role in enhancing the performance of large language models (LLMs) on long-context tasks. To smoothly adapt LLMs to long-context scenarios, this process typically entails training on mixed datasets containing both long and short sequences. However, this heterogeneous sequence length distribution poses significant challenges for existing training systems, as they fail to simultaneously achieve high training efficiency for both long and short sequences, resulting in sub-optimal end-to-end system performance in Long-SFT. In this paper, we present a novel perspective on data scheduling to address the challenges posed by the heterogeneous data distributions in Long-SFT. We propose Skrull, a dynamic data scheduler specifically designed for efficient long-SFT. Through dynamic data scheduling, Skrull balances the computation requirements of long and short sequences, improving overall training efficiency. Furthermore, we formulate the scheduling process as a joint optimization problem and thoroughly analyze the trade-offs involved. Based on those analysis, Skrull employs a lightweight scheduling algorithm to achieve near-zero cost online scheduling in Long-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art distributed training system for LLMs. Experimental results demonstrate that Skrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world long-SFT scenarios.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning</title>
<link>https://arxiv.org/abs/2505.19614</link>
<guid>https://arxiv.org/abs/2505.19614</guid>
<content:encoded><![CDATA[
arXiv:2505.19614v1 Announce Type: new 
Abstract: Multimodal learning has seen remarkable progress, particularly with the emergence of large-scale pre-training across various modalities. However, most current approaches are built on the assumption of a deterministic, one-to-one alignment between modalities. This oversimplifies real-world multimodal relationships, where their nature is inherently many-to-many. This phenomenon, named multiplicity, is not a side-effect of noise or annotation error, but an inevitable outcome of semantic abstraction, representational asymmetry, and task-dependent ambiguity in multimodal tasks. This position paper argues that multiplicity is a fundamental bottleneck that manifests across all stages of the multimodal learning pipeline: from data construction to training and evaluation. This paper examines the causes and consequences of multiplicity, and highlights how multiplicity introduces training uncertainty, unreliable evaluation, and low dataset quality. This position calls for new research directions on multimodal learning: novel multiplicity-aware learning frameworks and dataset construction protocols considering multiplicity.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.19616</link>
<guid>https://arxiv.org/abs/2505.19616</guid>
<content:encoded><![CDATA[
arXiv:2505.19616v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across tasks, yet they often exhibit difficulty in distinguishing task-relevant from irrelevant signals, particularly in tasks like Visual Question Answering (VQA), which can lead to susceptibility to misleading or spurious inputs. We refer to this broader limitation as the Cross-Modality Competency Problem: the model's inability to fairly evaluate all modalities. This vulnerability becomes more evident in modality-specific tasks such as image classification or pure text question answering, where models are expected to rely solely on one modality. In such tasks, spurious information from irrelevant modalities often leads to significant performance degradation. We refer to this failure as Modality Interference, which serves as a concrete and measurable instance of the cross-modality competency problem. We further design a perturbation-based causal diagnostic experiment to verify and quantify this problem. To mitigate modality interference, we propose a novel framework to fine-tune MLLMs, including perturbation-based data augmentations with both heuristic perturbations and adversarial perturbations via Projected Gradient Descent (PGD), and a consistency regularization strategy applied to model outputs with original and perturbed inputs. Experiments on multiple benchmark datasets (image-heavy, text-heavy, and VQA tasks) and multiple model families with different scales demonstrate significant improvements in robustness and cross-modality competency, indicating our method's effectiveness in boosting unimodal reasoning ability while enhancing performance on multimodal tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows</title>
<link>https://arxiv.org/abs/2505.19619</link>
<guid>https://arxiv.org/abs/2505.19619</guid>
<content:encoded><![CDATA[
arXiv:2505.19619v1 Announce Type: new 
Abstract: Deep generative models have recently garnered significant attention across various fields, from physics to chemistry, where sampling from unnormalized Boltzmann-like distributions represents a fundamental challenge. In particular, autoregressive models and normalizing flows have become prominent due to their appealing ability to yield closed-form probability densities. Moreover, it is well-established that incorporating prior knowledge - such as symmetries - into deep neural networks can substantially improve training performances. In this context, recent advances have focused on developing symmetry-equivariant generative models, achieving remarkable results. Building upon these foundations, this paper introduces Symmetry-Enforcing Stochastic Modulation (SESaMo). Similar to equivariant normalizing flows, SESaMo enables the incorporation of inductive biases (e.g., symmetries) into normalizing flows through a novel technique called stochastic modulation. This approach enhances the flexibility of the generative model, allowing to effectively learn a variety of exact and broken symmetries. Our numerical experiments benchmark SESaMo in different scenarios, including an 8-Gaussian mixture model and physically relevant field theories, such as the $\phi^4$ theory and the Hubbard model.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs</title>
<link>https://arxiv.org/abs/2505.19620</link>
<guid>https://arxiv.org/abs/2505.19620</guid>
<content:encoded><![CDATA[
arXiv:2505.19620v1 Announce Type: new 
Abstract: Spatio-temporal prediction is a pivotal task with broad applications in traffic management, climate monitoring, energy scheduling, etc. However, existing methodologies often struggle to balance model expressiveness and computational efficiency, especially when scaling to large real-world datasets. To tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph Separation Networks), a novel framework that decouples temporal and spatial modeling to enhance both efficiency and precision. Therein, the temporal dimension is modeled using lightweight large language models, which effectively capture low-rank temporal dynamics. Concurrently, the spatial dimension is addressed through an adaptive hypergraph neural network, which dynamically constructs hyperedges to model intricate, higher-order interactions. A carefully designed gating mechanism is integrated to seamlessly fuse temporal and spatial representations. By leveraging the fundamental principles of low-rank temporal dynamics and spatial interactions, STH-SepNet offers a pragmatic and scalable solution for spatio-temporal prediction in real-world applications. Extensive experiments on large-scale real-world datasets across multiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting predictive performance while maintaining computational efficiency. This work may provide a promising lightweight framework for spatio-temporal prediction, aiming to reduce computational demands and while enhancing predictive performance. Our code is avaliable at https://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When fractional quasi p-norms concentrate</title>
<link>https://arxiv.org/abs/2505.19635</link>
<guid>https://arxiv.org/abs/2505.19635</guid>
<content:encoded><![CDATA[
arXiv:2505.19635v1 Announce Type: new 
Abstract: Concentration of distances in high dimension is an important factor for the development and design of stable and reliable data analysis algorithms. In this paper, we address the fundamental long-standing question about the concentration of distances in high dimension for fractional quasi $p$-norms, $p\in(0,1)$. The topic has been at the centre of various theoretical and empirical controversies. Here we, for the first time, identify conditions when fractional quasi $p$-norms concentrate and when they don't. We show that contrary to some earlier suggestions, for broad classes of distributions, fractional quasi $p$-norms admit exponential and uniform in $p$ concentration bounds. For these distributions, the results effectively rule out previously proposed approaches to alleviate concentration by "optimal" setting the values of $p$ in $(0,1)$. At the same time, we specify conditions and the corresponding families of distributions for which one can still control concentration rates by appropriate choices of $p$. We also show that in an arbitrarily small vicinity of a distribution from a large class of distributions for which uniform concentration occurs, there are uncountably many other distributions featuring anti-concentration properties. Importantly, this behavior enables devising relevant data encoding or representation schemes favouring or discouraging distance concentration. The results shed new light on this long-standing problem and resolve the tension around the topic in both theory and empirical evidence reported in the literature.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE</title>
<link>https://arxiv.org/abs/2505.19645</link>
<guid>https://arxiv.org/abs/2505.19645</guid>
<content:encoded><![CDATA[
arXiv:2505.19645v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less computation. Speculative decoding (SD) is a widely used technique to accelerate LLM inference without accuracy loss, but it has been considered efficient only for dense models. In this work, we first demonstrate that, under medium batch sizes, MoE surprisingly benefits more from SD than dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in MoE designs -- the batch size range where SD acceleration is expected to be effective becomes broader. To quantitatively understand tradeoffs involved in SD, we develop a reliable modeling based on theoretical analyses. While current SD research primarily focuses on improving acceptance rates of algorithms, changes in workload and model architecture can still lead to degraded SD acceleration even with high acceptance rates. To address this limitation, we introduce a new metric 'target efficiency' that characterizes these effects, thus helping researchers identify system bottlenecks and understand SD acceleration more comprehensively. For scenarios like private serving, this work unveils a new perspective to speed up MoE inference, where existing solutions struggle. Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-based generator matching: A neural sampler for general state space</title>
<link>https://arxiv.org/abs/2505.19646</link>
<guid>https://arxiv.org/abs/2505.19646</guid>
<content:encoded><![CDATA[
arXiv:2505.19646v1 Announce Type: new 
Abstract: We propose Energy-based generator matching (EGM), a modality-agnostic approach to train generative models from energy functions in the absence of data. Extending the recently proposed generator matching, EGM enables training of arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump, and can generate data from continuous, discrete, and a mixture of two modalities. To this end, we propose estimating the generator matching loss using self-normalized importance sampling with an additional bootstrapping trick to reduce variance in the importance weight. We validate EGM on both discrete and multimodal tasks up to 100 and 20 dimensions, respectively.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling</title>
<link>https://arxiv.org/abs/2505.19669</link>
<guid>https://arxiv.org/abs/2505.19669</guid>
<content:encoded><![CDATA[
arXiv:2505.19669v1 Announce Type: new 
Abstract: Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on https://anonymous.4open.science/w/demo_page-48B7/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning</title>
<link>https://arxiv.org/abs/2505.19680</link>
<guid>https://arxiv.org/abs/2505.19680</guid>
<content:encoded><![CDATA[
arXiv:2505.19680v1 Announce Type: new 
Abstract: Multi-Label Online Continual Learning (MOCL) requires models to learn continuously from endless multi-label data streams, facing complex challenges including persistent catastrophic forgetting, potential missing labels, and uncontrollable imbalanced class distributions. While existing MOCL methods attempt to address these challenges through various techniques, \textit{they all overlook label-specific region identifying and feature learning} - a fundamental solution rooted in multi-label learning but challenging to achieve in the online setting with incremental and partial supervision. To this end, we first leverage the inherent structural information of input data to evaluate and verify the innate localization capability of different pre-trained models. Then, we propose CUTER (CUT-out-and-Experience-Replay), a simple yet versatile strategy that provides fine-grained supervision signals by further identifying, strengthening and cutting out label-specific regions for efficient experience replay. It not only enables models to simultaneously address catastrophic forgetting, missing labels, and class imbalance challenges, but also serves as an orthogonal solution that seamlessly integrates with existing approaches. Extensive experiments on multiple multi-label image benchmarks demonstrate the superiority of our proposed method. The code is available at \href{https://github.com/wxr99/Cut-Replay}{https://github.com/wxr99/Cut-Replay}
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Actor-Critics with Tight Risk Certificates</title>
<link>https://arxiv.org/abs/2505.19682</link>
<guid>https://arxiv.org/abs/2505.19682</guid>
<content:encoded><![CDATA[
arXiv:2505.19682v1 Announce Type: new 
Abstract: After a period of research, deep actor-critic algorithms have reached a level where they influence our everyday lives. They serve as the driving force behind the continual improvement of large language models through user-collected feedback. However, their deployment in physical systems is not yet widely adopted, mainly because no validation scheme that quantifies their risk of malfunction. We demonstrate that it is possible to develop tight risk certificates for deep actor-critic algorithms that predict generalization performance from validation-time observations. Our key insight centers on the effectiveness of minimal evaluation data. Surprisingly, a small feasible of evaluation roll-outs collected from a pretrained policy suffices to produce accurate risk certificates when combined with a simple adaptation of PAC-Bayes theory. Specifically, we adopt a recently introduced recursive PAC-Bayes approach, which splits validation data into portions and recursively builds PAC-Bayes bounds on the excess loss of each portion's predictor, using the predictor from the previous portion as a data-informed prior. Our empirical results across multiple locomotion tasks and policy expertise levels demonstrate risk certificates that are tight enough to be considered for practical use.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation</title>
<link>https://arxiv.org/abs/2505.19685</link>
<guid>https://arxiv.org/abs/2505.19685</guid>
<content:encoded><![CDATA[
arXiv:2505.19685v1 Announce Type: new 
Abstract: Diffusion models have emerged as powerful generative models for graph generation, yet their use for conditional graph generation remains a fundamental challenge. In particular, guiding diffusion models on graphs under arbitrary reward signals is difficult: gradient-based methods, while powerful, are often unsuitable due to the discrete and combinatorial nature of graphs, and non-differentiable rewards further complicate gradient-based guidance. We propose Graph Guided Diffusion (GGDiff), a novel guidance framework that interprets conditional diffusion on graphs as a stochastic control problem to address this challenge. GGDiff unifies multiple guidance strategies, including gradient-based guidance (for differentiable rewards), control-based guidance (using control signals from forward reward evaluations), and zero-order approximations (bridging gradient-based and gradient-free optimization). This comprehensive, plug-and-play framework enables zero-shot guidance of pre-trained diffusion models under both differentiable and non-differentiable reward functions, adapting well-established guidance techniques to graph generation--a direction largely unexplored. Our formulation balances computational efficiency, reward alignment, and sample quality, enabling practical conditional generation across diverse reward types. We demonstrate the efficacy of GGDiff in various tasks, including constraints on graph motifs, fairness, and link prediction, achieving superior alignment with target rewards while maintaining diversity and fidelity.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19698</link>
<guid>https://arxiv.org/abs/2505.19698</guid>
<content:encoded><![CDATA[
arXiv:2505.19698v1 Announce Type: new 
Abstract: Recent advances in model-based reinforcement learning (MBRL) have achieved super-human level performance on the Atari100k benchmark, driven by reinforcement learning agents trained on powerful diffusion world models. However, we identify that the current aggregates mask a major performance asymmetry: MBRL agents dramatically outperform humans in some tasks despite drastically underperforming in others, with the former inflating the aggregate metrics. This is especially pronounced in pixel-based agents trained with diffusion world models. In this work, we address the pronounced asymmetry observed in pixel-based agents as an initial attempt to reverse the worrying upward trend observed in them. We address the problematic aggregates by delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal importance on metrics from both sets. Next, we hypothesize this pronounced asymmetry is due to the lack of temporally-structured latent space trained with the World Model objective in pixel-based methods. Lastly, to address this issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective. JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline. Overall, our work rethinks what it truly means to cross human-level performance in Atari100k.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments</title>
<link>https://arxiv.org/abs/2505.19699</link>
<guid>https://arxiv.org/abs/2505.19699</guid>
<content:encoded><![CDATA[
arXiv:2505.19699v1 Announce Type: new 
Abstract: Federated Learning (FL) is a decentralized machine learning paradigm that enables clients to collaboratively train models while preserving data privacy. However, the coexistence of model and data heterogeneity gives rise to inconsistent representations and divergent optimization dynamics across clients, ultimately hindering robust global performance. To transcend these challenges, we propose Mosaic, a novel data-free knowledge distillation framework tailored for heterogeneous distributed environments. Mosaic first trains local generative models to approximate each client's personalized distribution, enabling synthetic data generation that safeguards privacy through strict separation from real data. Subsequently, Mosaic forms a Mixture-of-Experts (MoE) from client models based on their specialized knowledge, and distills it into a global model using the generated data. To further enhance the MoE architecture, Mosaic integrates expert predictions via a lightweight meta model trained on a few representative prototypes. Extensive experiments on standard image classification benchmarks demonstrate that Mosaic consistently outperforms state-of-the-art approaches under both model and data heterogeneity. The source code has been published at https://github.com/Wings-Of-Disaster/Mosaic.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Relation between Rectified Flows and Optimal Transport</title>
<link>https://arxiv.org/abs/2505.19712</link>
<guid>https://arxiv.org/abs/2505.19712</guid>
<content:encoded><![CDATA[
arXiv:2505.19712v1 Announce Type: new 
Abstract: This paper investigates the connections between rectified flows, flow matching, and optimal transport. Flow matching is a recent approach to learning generative models by estimating velocity fields that guide transformations from a source to a target distribution. Rectified flow matching aims to straighten the learned transport paths, yielding more direct flows between distributions. Our first contribution is a set of invariance properties of rectified flows and explicit velocity fields. In addition, we also provide explicit constructions and analysis in the Gaussian (not necessarily independent) and Gaussian mixture settings and study the relation to optimal transport. Our second contribution addresses recent claims suggesting that rectified flows, when constrained such that the learned velocity field is a gradient, can yield (asymptotically) solutions to optimal transport problems. We study the existence of solutions for this problem and demonstrate that they only relate to optimal transport under assumptions that are significantly stronger than those previously acknowledged. In particular, we present several counter-examples that invalidate earlier equivalence results in the literature, and we argue that enforcing a gradient constraint on rectified flows is, in general, not a reliable method for computing optimal transport maps.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction</title>
<link>https://arxiv.org/abs/2505.19719</link>
<guid>https://arxiv.org/abs/2505.19719</guid>
<content:encoded><![CDATA[
arXiv:2505.19719v1 Announce Type: new 
Abstract: Common Neighbors (CNs) and their higher-order variants are important pairwise features widely used in state-of-the-art link prediction methods. However, existing methods often struggle with the repetition across different orders of CNs and fail to fully leverage their potential. We identify that these limitations stem from two key issues: redundancy and over-smoothing in high-order common neighbors. To address these challenges, we design orthogonalization to eliminate redundancy between different-order CNs and normalization to mitigate over-smoothing. By combining these two techniques, we propose Orthogonal Common Neighbor (OCN), a novel approach that significantly outperforms the strongest baselines by an average of 7.7% on popular link prediction benchmarks. A thorough theoretical analysis is provided to support our method. Ablation studies also verify the effectiveness of our orthogonalization and normalization techniques.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Algorithm for Noise Reduction and Disease-Causing Gene Feature Extraction in Gene Sequencing Data</title>
<link>https://arxiv.org/abs/2505.19740</link>
<guid>https://arxiv.org/abs/2505.19740</guid>
<content:encoded><![CDATA[
arXiv:2505.19740v1 Announce Type: new 
Abstract: In this study, we propose a machine learning-based method for noise reduction and disease-causing gene feature extraction in gene sequencing DeepSeqDenoise algorithm combines CNN and RNN to effectively remove the sequencing noise, and improves the signal-to-noise ratio by 9.4 dB. We screened 17 key features by feature engineering, and constructed an integrated learning model to predict disease-causing genes with 94.3% accuracy. We successfully identified 57 new candidate disease-causing genes in a cardiovascular disease cohort validation, and detected 3 missed variants in clinical applications. The method significantly outperforms existing tools and provides strong support for accurate diagnosis of genetic diseases.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Markov Bridge</title>
<link>https://arxiv.org/abs/2505.19752</link>
<guid>https://arxiv.org/abs/2505.19752</guid>
<content:encoded><![CDATA[
arXiv:2505.19752v1 Announce Type: new 
Abstract: Discrete diffusion has recently emerged as a promising paradigm in discrete data modeling. However, existing methods typically rely on a fixed rate transition matrix during training, which not only limits the expressiveness of latent representations, a fundamental strength of variational methods, but also constrains the overall design space. To address these limitations, we propose Discrete Markov Bridge, a novel framework specifically designed for discrete representation learning. Our approach is built upon two key components: Matrix Learning and Score Learning. We conduct a rigorous theoretical analysis, establishing formal performance guarantees for Matrix Learning and proving the convergence of the overall framework. Furthermore, we analyze the space complexity of our method, addressing practical constraints identified in prior studies. Extensive empirical evaluations validate the effectiveness of the proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO) of 1.38 on the Text8 dataset, outperforming established baselines. Moreover, the proposed model demonstrates competitive performance on the CIFAR-10 dataset, achieving results comparable to those obtained by image-specific generation approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfolding AlphaFold's Bayesian Roots in Probability Kinematics</title>
<link>https://arxiv.org/abs/2505.19763</link>
<guid>https://arxiv.org/abs/2505.19763</guid>
<content:encoded><![CDATA[
arXiv:2505.19763v1 Announce Type: new 
Abstract: We present a novel theoretical interpretation of AlphaFold1. The seminal breakthrough of AlphaFold1 in protein structure prediction by deep learning relied on a learned potential energy function, in contrast to the later end-to-end architectures of AlphaFold2 and AlphaFold3. While this potential was originally justified by referring to physical potentials of mean force (PMFs), we reinterpret AlphaFold1's potential as an instance of probability kinematics - also known as Jeffrey conditioning - a principled but underrecognised generalization of conventional Bayesian updating. Probability kinematics accommodates uncertain or soft evidence in the form of updated probabilities over a partition. This perspective reveals AlphaFold1's potential as a form of generalized Bayesian updating, rather than a thermodynamic potential. To confirm our probabilistic framework's scope and precision, we analyze a synthetic 2D model in which an angular random walk prior is updated with evidence on distances via probability kinematics, mirroring AlphaFold1's approach. This theoretical contribution connects AlphaFold1 to a broader class of well-justified Bayesian methods, allowing precise quantification, surpassing merely qualitative heuristics based on PMFs. More broadly, given the achievements of AlphaFold1, probability kinematics holds considerable promise for probabilistic deep learning, as it allows for the formulation of complex models from a few simpler components.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding</title>
<link>https://arxiv.org/abs/2505.19764</link>
<guid>https://arxiv.org/abs/2505.19764</guid>
<content:encoded><![CDATA[
arXiv:2505.19764v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but optimizing LLM-based agentic systems remains challenging due to the vast search space of agent configurations, prompting strategies, and communication patterns. Existing approaches often rely on heuristic-based tuning or exhaustive evaluation, which can be computationally expensive and suboptimal. This paper proposes Agentic Predictor, a lightweight predictor for efficient agentic workflow evaluation. Agentic Predictor is equipped with a multi-view workflow encoding technique that leverages multi-view representation learning of agentic systems by incorporating code architecture, textual prompts, and interaction graph features. To achieve high predictive accuracy while significantly reducing the number of required workflow evaluations for training a predictor, Agentic Predictor employs cross-domain unsupervised pretraining. By learning to approximate task success rates, Agentic Predictor enables fast and accurate selection of optimal agentic workflow configurations for a given task, significantly reducing the need for expensive trial-and-error evaluations. Experiments on a carefully curated benchmark spanning three domains show that our predictor outperforms state-of-the-art methods in both predictive accuracy and workflow utility, highlighting the potential of performance predictors in streamlining the design of LLM-based agentic workflows.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO</title>
<link>https://arxiv.org/abs/2505.19770</link>
<guid>https://arxiv.org/abs/2505.19770</guid>
<content:encoded><![CDATA[
arXiv:2505.19770v1 Announce Type: new 
Abstract: We present a fine-grained theoretical analysis of the performance gap between reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) under a representation gap. Our study decomposes this gap into two sources: an explicit representation gap under exact optimization and an implicit representation gap under finite samples. In the exact optimization setting, we characterize how the relative capacities of the reward and policy model classes influence the final policy qualities. We show that RLHF, DPO, or online DPO can outperform one another depending on the type of model mis-specifications. Notably, online DPO can outperform both RLHF and standard DPO when the reward and policy model classes are isomorphic and both mis-specified. In the approximate optimization setting, we provide a concrete construction where the ground-truth reward is implicitly sparse and show that RLHF requires significantly fewer samples than DPO to recover an effective reward model -- highlighting a statistical advantage of two-stage learning. Together, these results provide a comprehensive understanding of the performance gap between RLHF and DPO under various settings, and offer practical insights into when each method is preferred.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support</title>
<link>https://arxiv.org/abs/2505.19785</link>
<guid>https://arxiv.org/abs/2505.19785</guid>
<content:encoded><![CDATA[
arXiv:2505.19785v1 Announce Type: new 
Abstract: Timely and personalized treatment decisions are essential across a wide range of healthcare settings where patient responses vary significantly and evolve over time. Clinical data used to support these decisions are often irregularly sampled, sparse, and noisy. Existing decision support systems commonly rely on discretization and imputation, which can distort critical temporal dynamics and degrade decision quality. Moreover, they often overlook the clinical significance of irregular recording frequencies, filtering out patterns in how and when data is collected. Reinforcement Learning (RL) is a natural fit for clinical decision-making, enabling sequential, long-term optimization in dynamic, uncertain environments. However, most existing treatment recommendation systems are model-free and trained solely on offline data, making them sample-inefficient, sensitive to data quality, and poorly generalizable across tasks or cohorts. To address these limitations, we propose MedDreamer, a two-phase model-based RL framework for personalized treatment recommendation. MedDreamer uses a world model with an Adaptive Feature Integration (AFI) module to effectively model irregular, sparse clinical data. Through latent imagination, it simulates plausible patient trajectories to enhance learning, refining its policy using a mix of real and imagined experiences. This enables learning policies that go beyond suboptimal historical decisions while remaining grounded in clinical data. To our knowledge, this is the first application of latent imagination to irregular healthcare data. Evaluations on sepsis and mechanical ventilation (MV) treatment using two large-scale EHR datasets show that MedDreamer outperforms both model-free and model-based baselines in clinical outcomes and off-policy metrics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Can RL Bring to VLA Generalization? An Empirical Study</title>
<link>https://arxiv.org/abs/2505.19789</link>
<guid>https://arxiv.org/abs/2505.19789</guid>
<content:encoded><![CDATA[
arXiv:2505.19789v1 Announce Type: new 
Abstract: Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation</title>
<link>https://arxiv.org/abs/2505.19802</link>
<guid>https://arxiv.org/abs/2505.19802</guid>
<content:encoded><![CDATA[
arXiv:2505.19802v1 Announce Type: new 
Abstract: Understanding pain-related facial behaviors is essential for digital healthcare in terms of effective monitoring, assisted diagnostics, and treatment planning, particularly for patients unable to communicate verbally. Existing data-driven methods of detecting pain from facial expressions are limited due to interpretability and severity quantification. To this end, we propose GraphAU-Pain, leveraging a graph-based framework to model facial Action Units (AUs) and their interrelationships for pain intensity estimation. AUs are represented as graph nodes, with co-occurrence relationships as edges, enabling a more expressive depiction of pain-related facial behaviors. By utilizing a relational graph neural network, our framework offers improved interpretability and significant performance gains. Experiments conducted on the publicly available UNBC dataset demonstrate the effectiveness of the GraphAU-Pain, achieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity estimation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Density Ratio-Free Doubly Robust Proxy Causal Learning</title>
<link>https://arxiv.org/abs/2505.19807</link>
<guid>https://arxiv.org/abs/2505.19807</guid>
<content:encoded><![CDATA[
arXiv:2505.19807v1 Announce Type: new 
Abstract: We study the problem of causal function estimation in the Proxy Causal Learning (PCL) framework, where confounders are not observed but proxies for the confounders are available. Two main approaches have been proposed: outcome bridge-based and treatment bridge-based methods. In this work, we propose two kernel-based doubly robust estimators that combine the strengths of both approaches, and naturally handle continuous and high-dimensional variables. Our identification strategy builds on a recent density ratio-free method for treatment bridge-based PCL; furthermore, in contrast to previous approaches, it does not require indicator functions or kernel smoothing over the treatment variable. These properties make it especially well-suited for continuous or high-dimensional treatments. By using kernel mean embeddings, we have closed-form solutions and strong consistency guarantees. Our estimators outperform existing methods on PCL benchmarks, including a prior doubly robust method that requires both kernel smoothing and density ratio estimation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees</title>
<link>https://arxiv.org/abs/2505.19809</link>
<guid>https://arxiv.org/abs/2505.19809</guid>
<content:encoded><![CDATA[
arXiv:2505.19809v1 Announce Type: new 
Abstract: In many real-world applications of regression, conditional probability estimation, and uncertainty quantification, exploiting symmetries rooted in physics or geometry can dramatically improve generalization and sample efficiency. While geometric deep learning has made significant empirical advances by incorporating group-theoretic structure, less attention has been given to statistical learning guarantees. In this paper, we introduce an equivariant representation learning framework that simultaneously addresses regression, conditional probability estimation, and uncertainty quantification while providing first-of-its-kind non-asymptotic statistical learning guarantees. Grounded in operator and group representation theory, our framework approximates the spectral decomposition of the conditional expectation operator, building representations that are both equivariant and disentangled along independent symmetry subgroups. Empirical evaluations on synthetic datasets and real-world robotics applications confirm the potential of our approach, matching or outperforming existing equivariant baselines in regression while additionally providing well-calibrated parametric uncertainty estimates.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory</title>
<link>https://arxiv.org/abs/2505.19820</link>
<guid>https://arxiv.org/abs/2505.19820</guid>
<content:encoded><![CDATA[
arXiv:2505.19820v1 Announce Type: new 
Abstract: Interpretability of point cloud (PC) models becomes imperative given their deployment in safety-critical scenarios such as autonomous vehicles. We focus on attributing PC model outputs to interpretable critical concepts, defined as meaningful subsets of the input point cloud. To enable human-understandable diagnostics of model failures, an ideal critical subset should be *faithful* (preserving points that causally influence predictions) and *conceptually coherent* (forming semantically meaningful structures that align with human perception). We propose InfoCons, an explanation framework that applies information-theoretic principles to decompose the point cloud into 3D concepts, enabling the examination of their causal effect on model predictions with learnable priors. We evaluate InfoCons on synthetic datasets for classification, comparing it qualitatively and quantitatively with four baselines. We further demonstrate its scalability and flexibility on two real-world datasets and in two applications that utilize critical scores of PC.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments</title>
<link>https://arxiv.org/abs/2505.19823</link>
<guid>https://arxiv.org/abs/2505.19823</guid>
<content:encoded><![CDATA[
arXiv:2505.19823v1 Announce Type: new 
Abstract: Federated Learning (FL) is a distributed machine learning paradigm based on protecting data privacy of devices, which however, can still be broken by gradient leakage attack via parameter inversion techniques. Differential privacy (DP) technology reduces the risk of private data leakage by adding artificial noise to the gradients, but detrimental to the FL utility at the same time, especially in the scenario where the data is Non-Independent Identically Distributed (Non-IID). Based on the impact of heterogeneous data on aggregation performance, this paper proposes a Lightweight Adaptive Privacy Allocation (LAPA) strategy, which assigns personalized privacy budgets to devices in each aggregation round without transmitting any additional information beyond gradients, ensuring both privacy protection and aggregation efficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG) algorithm is employed to optimize the transmission power, in order to determine the optimal timing at which the adaptively attenuated artificial noise aligns with the communication noise, enabling an effective balance between DP and system utility. Finally, a reliable aggregation strategy is designed by integrating communication quality and data distribution characteristics, which improves aggregation performance while preserving privacy. Experimental results demonstrate that the personalized noise allocation and dynamic optimization strategy based on LAPA proposed in this paper enhances convergence performance while satisfying the privacy requirements of FL.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models for Tabular Data within Systemic Contexts Need Grounding</title>
<link>https://arxiv.org/abs/2505.19825</link>
<guid>https://arxiv.org/abs/2505.19825</guid>
<content:encoded><![CDATA[
arXiv:2505.19825v1 Announce Type: new 
Abstract: Current research on tabular foundation models often overlooks the complexities of large-scale, real-world data by treating tables as isolated entities and assuming information completeness, thereby neglecting the vital operational context. To address this, we introduce the concept of Semantically Linked Tables (SLT), recognizing that tables are inherently connected to both declarative and procedural operational knowledge. We propose Foundation Models for Semantically Linked Tables (FMSLT), which integrate these components to ground tabular data within its true operational context. This comprehensive representation unlocks the full potential of machine learning for complex, interconnected tabular data across diverse domains. Realizing FMSLTs requires access to operational knowledge that is often unavailable in public datasets, highlighting the need for close collaboration between domain experts and researchers. Our work exposes the limitations of current tabular foundation models and proposes a new direction centered on FMSLTs, aiming to advance robust, context-aware models for structured data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Glorot Initialization for Long-Range Linear Recurrences</title>
<link>https://arxiv.org/abs/2505.19827</link>
<guid>https://arxiv.org/abs/2505.19827</guid>
<content:encoded><![CDATA[
arXiv:2505.19827v1 Announce Type: new 
Abstract: Proper initialization is critical for Recurrent Neural Networks (RNNs), particularly in long-range reasoning tasks, where repeated application of the same weight matrix can cause vanishing or exploding signals. A common baseline for linear recurrences is Glorot initialization, designed to ensure stable signal propagation--but derived under the infinite-width, fixed-length regime--an unrealistic setting for RNNs processing long sequences. In this work, we show that Glorot initialization is in fact unstable: small positive deviations in the spectral radius are amplified through time and cause the hidden state to explode. Our theoretical analysis demonstrates that sequences of length $t = O(\sqrt{n})$, where $n$ is the hidden width, are sufficient to induce instability. To address this, we propose a simple, dimension-aware rescaling of Glorot that shifts the spectral radius slightly below one, preventing rapid signal explosion or decay. These results suggest that standard initialization schemes may break down in the long-sequence regime, motivating a separate line of theory for stable recurrent initialization.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints</title>
<link>https://arxiv.org/abs/2505.19842</link>
<guid>https://arxiv.org/abs/2505.19842</guid>
<content:encoded><![CDATA[
arXiv:2505.19842v1 Announce Type: new 
Abstract: Air quality forecasting (AQF) is critical for public health and environmental management, yet remains challenging due to the complex interplay of emissions, meteorology, and chemical transformations. Traditional numerical models, such as CMAQ and WRF-Chem, provide physically grounded simulations but are computationally expensive and rely on uncertain emission inventories. Deep learning models, while computationally efficient, often struggle with generalization due to their lack of physical constraints. To bridge this gap, we propose PCDCNet, a surrogate model that integrates numerical modeling principles with deep learning. PCDCNet explicitly incorporates emissions, meteorological influences, and domain-informed constraints to model pollutant formation, transport, and dissipation. By combining graph-based spatial transport modeling, recurrent structures for temporal accumulation, and representation enhancement for local interactions, PCDCNet achieves state-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3 forecasting while significantly reducing computational costs. Furthermore, our model is deployed in an online platform, providing free, real-time air quality forecasts, demonstrating its scalability and societal impact. By aligning deep learning with physical consistency, PCDCNet offers a practical and interpretable solution for AQF, enabling informed decision-making for both personal and regulatory applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19850</link>
<guid>https://arxiv.org/abs/2505.19850</guid>
<content:encoded><![CDATA[
arXiv:2505.19850v1 Announce Type: new 
Abstract: Sparse-reward reinforcement learning (RL) can model a wide range of highly complex tasks. Solving sparse-reward tasks is RL's core premise - requiring efficient exploration coupled with long-horizon credit assignment - and overcoming these challenges is key for building self-improving agents with superhuman ability. We argue that solving complex and high-dimensional tasks requires solving simpler tasks that are relevant to the target task. In contrast, most prior work designs strategies for selecting exploratory tasks with the objective of solving any task, making exploration of challenging high-dimensional, long-horizon tasks intractable. We find that the sense of direction, necessary for effective exploration, can be extracted from existing RL algorithms, without needing any prior information. Based on this finding, we propose a method for directed sparse-reward goal-conditioned very long-horizon RL (DISCOVER), which selects exploratory goals in the direction of the target task. We connect DISCOVER to principled exploration in bandits, formally bounding the time until the target task becomes achievable in terms of the agent's initial distance to the target, but independent of the volume of the space of all tasks. Empirically, we perform a thorough evaluation in high-dimensional environments. We find that the directed goal selection of DISCOVER solves exploration problems that are beyond the reach of prior state-of-the-art exploration methods in RL.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?</title>
<link>https://arxiv.org/abs/2505.19855</link>
<guid>https://arxiv.org/abs/2505.19855</guid>
<content:encoded><![CDATA[
arXiv:2505.19855v1 Announce Type: new 
Abstract: Large language Model (LLM) unlearning, i.e., selectively removing information from LLMs, is vital for responsible model deployment. Differently, LLM knowledge editing aims to modify LLM knowledge instead of removing it. Though editing and unlearning seem to be two distinct tasks, we find there is a tight connection between them. In this paper, we conceptualize unlearning as a special case of editing where information is modified to a refusal or "empty set" $\emptyset$ response, signifying its removal. This paper thus investigates if knowledge editing techniques are strong baselines for LLM unlearning. We evaluate state-of-the-art (SOTA) editing methods (e.g., ROME, MEMIT, GRACE, WISE, and AlphaEdit) against existing unlearning approaches on pretrained and finetuned knowledge. Results show certain editing methods, notably WISE and AlphaEdit, are effective unlearning baselines, especially for pretrained knowledge, and excel in generating human-aligned refusal answers. To better adapt editing methods for unlearning applications, we propose practical recipes including self-improvement and query merging. The former leverages the LLM's own in-context learning ability to craft a more human-aligned unlearning target, and the latter enables ROME and MEMIT to perform well in unlearning longer sample sequences. We advocate for the unlearning community to adopt SOTA editing methods as baselines and explore unlearning from an editing perspective for more holistic LLM memory control.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Active Inference Agents for Delayed and Long-Horizon Environments</title>
<link>https://arxiv.org/abs/2505.19867</link>
<guid>https://arxiv.org/abs/2505.19867</guid>
<content:encoded><![CDATA[
arXiv:2505.19867v1 Announce Type: new 
Abstract: With the recent success of world-model agents, which extend the core idea of model-based reinforcement learning by learning a differentiable model for sample-efficient control across diverse tasks, active inference (AIF) offers a complementary, neuroscience-grounded paradigm that unifies perception, learning, and action within a single probabilistic framework powered by a generative model. Despite this promise, practical AIF agents still rely on accurate immediate predictions and exhaustive planning, a limitation that is exacerbated in delayed environments requiring plans over long horizons, tens to hundreds of steps. Moreover, most existing agents are evaluated on robotic or vision benchmarks which, while natural for biological agents, fall short of real-world industrial complexity. We address these limitations with a generative-policy architecture featuring (i) a multi-step latent transition that lets the generative model predict an entire horizon in a single look-ahead, (ii) an integrated policy network that enables the transition and receives gradients of the expected free energy, (iii) an alternating optimization scheme that updates model and policy from a replay buffer, and (iv) a single gradient step that plans over long horizons, eliminating exhaustive planning from the control loop. We evaluate our agent in an environment that mimics a realistic industrial scenario with delayed and long-horizon settings. The empirical results confirm the effectiveness of the proposed approach, demonstrating the coupled world-model with the AIF formalism yields an end-to-end probabilistic controller capable of effective decision making in delayed, long-horizon settings without handcrafted rewards or expensive planning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations</title>
<link>https://arxiv.org/abs/2505.19888</link>
<guid>https://arxiv.org/abs/2505.19888</guid>
<content:encoded><![CDATA[
arXiv:2505.19888v1 Announce Type: new 
Abstract: Federated Learning (FL) aims to train models across decentralized clients or devices holding local data without the need for centralized data collection, thus enhancing data privacy and security. However, achieving both generalization and personalization in heterogeneous settings remains a significant challenge. To address this, we introduce FedOT, a novel approach that leverages black-box foundation models. FedOT shares only a global task-dependent classifier across clients while locally adapting features through orthogonal transformations. By enforcing orthogonality, FedOT mitigates gradient conflicts across diverse clients, preserves semantic integrity, and achieves robust performance even in the presence of substantial data heterogeneity. The strategy of combining global and local parameters enables a more balanced approach for both generalization and personalization, outperforming baseline FL methods across multiple benchmarks. Furthermore, our extensive analysis confirms that joint optimization of global classifiers and local orthogonal transformations yields superior performance and suggests broader applicability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining</title>
<link>https://arxiv.org/abs/2505.19893</link>
<guid>https://arxiv.org/abs/2505.19893</guid>
<content:encoded><![CDATA[
arXiv:2505.19893v1 Announce Type: new 
Abstract: Large language model pretraining is compute-intensive, yet many tokens contribute marginally to learning, resulting in inefficiency. We introduce Efficient Selective Language Modeling (ESLM), a risk-aware algorithm that improves training efficiency and distributional robustness by performing online token-level batch selection. ESLM leverages per-token statistics (e.g., entropy or loss) and applies value-at-risk thresholding to retain only the most informative tokens per batch. This data-centric mechanism reshapes the training loss, prioritizing high-risk tokens and eliminating redundant gradient computation. We frame ESLM as a bilevel game: the model competes with a masking adversary that selects worst-case token subsets under a constrained thresholding rule. In the loss-based setting, ESLM recovers conditional value-at-risk loss minimization, providing a principled connection to distributionally robust optimization. We extend our approach to Ada-ESLM, which adaptively tunes the selection confidence during training. Experiments on GPT-2 pretraining show that ESLM significantly reduces training FLOPs while maintaining or improving both perplexity and downstream performance compared to baselines. Our approach also scales across model sizes, pretraining corpora, and integrates naturally with knowledge distillation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL</title>
<link>https://arxiv.org/abs/2505.19923</link>
<guid>https://arxiv.org/abs/2505.19923</guid>
<content:encoded><![CDATA[
arXiv:2505.19923v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) aims to learn an effective policy from a static dataset. To alleviate extrapolation errors, existing studies often uniformly regularize the value function or policy updates across all states. However, due to substantial variations in data quality, the fixed regularization strength often leads to a dilemma: Weak regularization strength fails to address extrapolation errors and value overestimation, while strong regularization strength shifts policy learning toward behavior cloning, impeding potential performance enabled by Bellman updates. To address this issue, we propose the selective state-adaptive regularization method for offline RL. Specifically, we introduce state-adaptive regularization coefficients to trust state-level Bellman-driven results, while selectively applying regularization on high-quality actions, aiming to avoid performance degradation caused by tight constraints on low-quality actions. By establishing a connection between the representative value regularization method, CQL, and explicit policy constraint methods, we effectively extend selective state-adaptive regularization to these two mainstream offline RL approaches. Extensive experiments demonstrate that the proposed method significantly outperforms the state-of-the-art approaches in both offline and offline-to-online settings on the D4RL benchmark.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic Gate Neural Networks are Good for Verification</title>
<link>https://arxiv.org/abs/2505.19932</link>
<guid>https://arxiv.org/abs/2505.19932</guid>
<content:encoded><![CDATA[
arXiv:2505.19932v1 Announce Type: new 
Abstract: Learning-based systems are increasingly deployed across various domains, yet the complexity of traditional neural networks poses significant challenges for formal verification. Unlike conventional neural networks, learned Logic Gate Networks (LGNs) replace multiplications with Boolean logic gates, yielding a sparse, netlist-like architecture that is inherently more amenable to symbolic verification, while still delivering promising performance. In this paper, we introduce a SAT encoding for verifying global robustness and fairness in LGNs. We evaluate our method on five benchmark datasets, including a newly constructed 5-class variant, and find that LGNs are both verification-friendly and maintain strong predictive performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Oriented Low-Label Semantic Communication With Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.19940</link>
<guid>https://arxiv.org/abs/2505.19940</guid>
<content:encoded><![CDATA[
arXiv:2505.19940v1 Announce Type: new 
Abstract: Task-oriented semantic communication enhances transmission efficiency by conveying semantic information rather than exact messages. Deep learning (DL)-based semantic communication can effectively cultivate the essential semantic knowledge for semantic extraction, transmission, and interpretation by leveraging massive labeled samples for downstream task training. In this paper, we propose a self-supervised learning-based semantic communication framework (SLSCom) to enhance task inference performance, particularly in scenarios with limited access to labeled samples. Specifically, we develop a task-relevant semantic encoder using unlabeled samples, which can be collected by devices in real-world edge networks. To facilitate task-relevant semantic extraction, we introduce self-supervision for learning contrastive features and formulate the information bottleneck (IB) problem to balance the tradeoff between the informativeness of the extracted features and task inference performance. Given the computational challenges of the IB problem, we devise a practical and effective solution by employing self-supervised classification and reconstruction pretext tasks. We further propose efficient joint training methods to enhance end-to-end inference accuracy over wireless channels, even with few labeled samples. We evaluate the proposed framework on image classification tasks over multipath wireless channels. Extensive simulation results demonstrate that SLSCom significantly outperforms conventional digital coding methods and existing DL-based approaches across varying labeled data set sizes and SNR conditions, even when the unlabeled samples are irrelevant to the downstream tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Freezing: Sparse Tuning Enhances Plasticity in Continual Learning with Pre-Trained Models</title>
<link>https://arxiv.org/abs/2505.19943</link>
<guid>https://arxiv.org/abs/2505.19943</guid>
<content:encoded><![CDATA[
arXiv:2505.19943v1 Announce Type: new 
Abstract: Continual Learning with Pre-trained Models holds great promise for efficient adaptation across sequential tasks. However, most existing approaches freeze PTMs and rely on auxiliary modules like prompts or adapters, limiting model plasticity and leading to suboptimal generalization when facing significant distribution shifts. While full fine-tuning can improve adaptability, it risks disrupting crucial pre-trained knowledge. In this paper, we propose Mutual Information-guided Sparse Tuning (MIST), a plug-and-play method that selectively updates a small subset of PTM parameters, less than 5%, based on sensitivity to mutual information objectives. MIST enables effective task-specific adaptation while preserving generalization. To further reduce interference, we introduce strong sparsity regularization by randomly dropping gradients during tuning, resulting in fewer than 0.5% of parameters being updated per step. Applied before standard freeze-based methods, MIST consistently boosts performance across diverse continual learning benchmarks. Experiments show that integrating our method into multiple baselines yields significant performance gains. Our code is available at https://github.com/zhwhu/MIST.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^\pi$-Realizable MDPs</title>
<link>https://arxiv.org/abs/2505.19946</link>
<guid>https://arxiv.org/abs/2505.19946</guid>
<content:encoded><![CDATA[
arXiv:2505.19946v1 Announce Type: new 
Abstract: We study the problem of offline imitation learning in Markov decision processes (MDPs), where the goal is to learn a well-performing policy given a dataset of state-action pairs generated by an expert policy. Complementing a recent line of work on this topic that assumes the expert belongs to a tractable class of known policies, we approach this problem from a new angle and leverage a different type of structural assumption about the environment. Specifically, for the class of linear $Q^\pi$-realizable MDPs, we introduce a new algorithm called saddle-point offline imitation learning (\SPOIL), which is guaranteed to match the performance of any expert up to an additive error $\varepsilon$ with access to $\mathcal{O}(\varepsilon^{-2})$ samples. Moreover, we extend this result to possibly non-linear $Q^\pi$-realizable MDPs at the cost of a worse sample complexity of order $\mathcal{O}(\varepsilon^{-4})$. Finally, our analysis suggests a new loss function for training critic networks from expert data in deep imitation learning. Empirical evaluations on standard benchmarks demonstrate that the neural net implementation of \SPOIL is superior to behavior cloning and competitive with state-of-the-art algorithms.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees</title>
<link>https://arxiv.org/abs/2505.19947</link>
<guid>https://arxiv.org/abs/2505.19947</guid>
<content:encoded><![CDATA[
arXiv:2505.19947v1 Announce Type: new 
Abstract: Open-weight LLM zoos provide access to numerous high-quality models, but selecting the appropriate model for specific tasks remains challenging and requires technical expertise. Most users simply want factually correct, safe, and satisfying responses without concerning themselves with model technicalities, while inference service providers prioritize minimizing operating costs. These competing interests are typically mediated through service level agreements (SLAs) that guarantee minimum service quality. We introduce MESS+, a stochastic optimization algorithm for cost-optimal LLM request routing while providing rigorous SLA compliance guarantees. MESS+ learns request satisfaction probabilities of LLMs in real-time as users interact with the system, based on which model selection decisions are made by solving a per-request optimization problem. Our algorithm includes a novel combination of virtual queues and request satisfaction prediction, along with a theoretical analysis of cost optimality and constraint satisfaction. Across a wide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x cost savings compared to existing LLM routing techniques.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions</title>
<link>https://arxiv.org/abs/2505.19949</link>
<guid>https://arxiv.org/abs/2505.19949</guid>
<content:encoded><![CDATA[
arXiv:2505.19949v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable reasoning capabilities in math and coding, often bolstered by post-training on the chain-of-thoughts (CoTs) generated by stronger models. However, existing strategies for curating such training data predominantly rely on heuristics, limiting generalizability and failing to capture subtleties underlying in data. To address these limitations, we leverage influence functions to systematically attribute LLMs' reasoning ability on math and coding to individual training examples, sequences, and tokens, enabling deeper insights into effective data characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers nontrivial cross-domain effects across math and coding tasks: high-difficulty math examples improve both math and code reasoning, while low-difficulty code tasks most effectively benefit code reasoning. Based on these findings, we introduce a simple yet effective dataset reweighting strategy by flipping task difficulty, which doubles AIME24 accuracy from 10\% to 20\% and boosts LiveCodeBench accuracy from 33.8\% to 35.3\% for Qwen2.5-7B-Instruct. Moreover, our fine-grained attribution reveals that the sequence-level exploratory behaviors enhance reasoning performance in both math and code, and the token-level influence patterns are distinct for math and code reasoning: the former prefers natural language logic connectors and the latter emphasizes structural syntax.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.19954</link>
<guid>https://arxiv.org/abs/2505.19954</guid>
<content:encoded><![CDATA[
arXiv:2505.19954v1 Announce Type: new 
Abstract: The differential diagnosis of neurodegenerative dementias is a challenging clinical task, mainly because of the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging. To improve diagnostic efficiency and accuracy, deep learning-based methods such as Convolutional Neural Networks and Vision Transformers have been proposed for the automatic classification of brain MRIs. However, despite their strong predictive performance, these models find limited clinical utility due to their opaque decision making. In this work, we propose a framework that integrates two core components to enhance diagnostic transparency. First, we introduce a modular pipeline for converting 3D T1-weighted brain MRIs into textual radiology reports. Second, we explore the potential of modern Large Language Models (LLMs) to assist clinicians in the differential diagnosis between Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based on the generated reports. To bridge the gap between predictive accuracy and explainability, we employ reinforcement learning to incentivize diagnostic reasoning in LLMs. Without requiring supervised reasoning traces or distillation from larger models, our approach enables the emergence of structured diagnostic rationales grounded in neuroimaging findings. Unlike post-hoc explainability methods that retrospectively justify model decisions, our framework generates diagnostic rationales as part of the inference process-producing causally grounded explanations that inform and guide the model's decision-making process. In doing so, our framework matches the diagnostic performance of existing deep learning methods while offering rationales that support its diagnostic conclusions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research</title>
<link>https://arxiv.org/abs/2505.19955</link>
<guid>https://arxiv.org/abs/2505.19955</guid>
<content:encoded><![CDATA[
arXiv:2505.19955v1 Announce Type: new 
Abstract: Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Limits of Preference Data for Post-Training</title>
<link>https://arxiv.org/abs/2505.19964</link>
<guid>https://arxiv.org/abs/2505.19964</guid>
<content:encoded><![CDATA[
arXiv:2505.19964v1 Announce Type: new 
Abstract: Recent progress in strengthening the capabilities of large language models has stemmed from applying reinforcement learning to domains with automatically verifiable outcomes. A key question is whether we can similarly use RL to optimize for outcomes in domains where evaluating outcomes inherently requires human feedback; for example, in tasks like deep research and trip planning, outcome evaluation is qualitative and there are many possible degrees of success. One attractive and scalable modality for collecting human feedback is preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$ given outcomes, which one is preferred. In this work, we study a critical roadblock: preference data fundamentally and significantly limits outcome-based optimization. Even with idealized preference data (infinite, noiseless, and online), the use of ordinal feedback can prevent obtaining even approximately optimal solutions. We formalize this impossibility using voting theory, drawing an analogy between how a model chooses to answer a query with how voters choose a candidate to elect. This indicates that grounded human scoring and algorithmic innovations are necessary for extending the success of RL post-training to domains demanding human feedback. We also explore why these limitations have disproportionately impacted RLHF when it comes to eliciting reasoning behaviors (e.g., backtracking) versus situations where RLHF has been historically successful (e.g., instruction-tuning and safety training), finding that the limitations of preference data primarily suppress RLHF's ability to elicit robust strategies -- a class that encompasses most reasoning behaviors.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Select In-Context Demonstration Preferred by Large Language Model</title>
<link>https://arxiv.org/abs/2505.19966</link>
<guid>https://arxiv.org/abs/2505.19966</guid>
<content:encoded><![CDATA[
arXiv:2505.19966v1 Announce Type: new 
Abstract: In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks during inference using only a few demonstrations. However, ICL performance is highly dependent on the selection of these demonstrations. Recent work explores retrieval-based methods for selecting query-specific demonstrations, but these approaches often rely on surrogate objectives such as metric learning, failing to directly optimize ICL performance. Consequently, they struggle to identify truly beneficial demonstrations. Moreover, their discriminative retrieval paradigm is ineffective when the candidate pool lacks sufficient high-quality demonstrations. To address these challenges, we propose GenICL, a novel generative preference learning framework that leverages LLM feedback to directly optimize demonstration selection for ICL. Experiments on 19 datasets across 11 task categories demonstrate that GenICL achieves superior performance than existing methods in selecting the most effective demonstrations, leading to better ICL performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models</title>
<link>https://arxiv.org/abs/2505.19969</link>
<guid>https://arxiv.org/abs/2505.19969</guid>
<content:encoded><![CDATA[
arXiv:2505.19969v1 Announce Type: new 
Abstract: Fully decentralized training of machine learning models offers significant advantages in scalability, robustness, and fault tolerance. However, achieving differential privacy (DP) in such settings is challenging due to the absence of a central aggregator and varying trust assumptions among nodes. In this work, we present a novel privacy analysis of decentralized gossip-based averaging algorithms with additive node-level noise, both with and without secure summation over each node's direct neighbors. Our main contribution is a new analytical framework based on a linear systems formulation that accurately characterizes privacy leakage across these scenarios. This framework significantly improves upon prior analyses, for example, reducing the R\'enyi DP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of training rounds. We validate our analysis with numerical results demonstrating superior DP bounds compared to existing approaches. We further illustrate our analysis with a logistic regression experiment on MNIST image classification in a fully decentralized setting, demonstrating utility comparable to central aggregation methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Probabilistic Circuit Parameter Learning</title>
<link>https://arxiv.org/abs/2505.19982</link>
<guid>https://arxiv.org/abs/2505.19982</guid>
<content:encoded><![CDATA[
arXiv:2505.19982v1 Announce Type: new 
Abstract: Probabilistic Circuits (PCs) offer a computationally scalable framework for generative modeling, supporting exact and efficient inference of a wide range of probabilistic queries. While recent advances have significantly improved the expressiveness and scalability of PCs, effectively training their parameters remains a challenge. In particular, a widely used optimization method, full-batch Expectation-Maximization (EM), requires processing the entire dataset before performing a single update, making it ineffective for large datasets. While empirical extensions to the mini-batch setting have been proposed, it remains unclear what objective these algorithms are optimizing, making it difficult to assess their theoretical soundness. This paper bridges the gap by establishing a novel connection between the general EM objective and the standard full-batch EM algorithm. Building on this, we derive a theoretically grounded generalization to the mini-batch setting and demonstrate its effectiveness through preliminary empirical results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach</title>
<link>https://arxiv.org/abs/2505.19986</link>
<guid>https://arxiv.org/abs/2505.19986</guid>
<content:encoded><![CDATA[
arXiv:2505.19986v1 Announce Type: new 
Abstract: Actor-Critic methods are widely used for their scalability, yet existing theoretical guarantees for infinite-horizon average-reward Markov Decision Processes (MDPs) often rely on restrictive ergodicity assumptions. We propose NAC-B, a Natural Actor-Critic with Batching, that achieves order-optimal regret of $\tilde{O}(\sqrt{T})$ in infinite-horizon average-reward MDPs under the unichain assumption, which permits both transient states and periodicity. This assumption is among the weakest under which the classic policy gradient theorem remains valid for average-reward settings. NAC-B employs function approximation for both the actor and the critic, enabling scalability to problems with large state and action spaces. The use of batching in our algorithm helps mitigate potential periodicity in the MDP and reduces stochasticity in gradient estimates, and our analysis formalizes these benefits through the introduction of the constants $C_{\text{hit}}$ and $C_{\text{tar}}$, which characterize the rate at which empirical averages over Markovian samples converge to the stationary distribution.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Optimal Multimodal Information Bottleneck Representations</title>
<link>https://arxiv.org/abs/2505.19996</link>
<guid>https://arxiv.org/abs/2505.19996</guid>
<content:encoded><![CDATA[
arXiv:2505.19996v1 Announce Type: new 
Abstract: Leveraging high-quality joint representations from multimodal data can greatly enhance model performance in various machine-learning based applications. Recent multimodal learning methods, based on the multimodal information bottleneck (MIB) principle, aim to generate optimal MIB with maximal task-relevant information and minimal superfluous information via regularization. However, these methods often set ad hoc regularization weights and overlook imbalanced task-relevant information across modalities, limiting their ability to achieve optimal MIB. To address this gap, we propose a novel multimodal learning framework, Optimal Multimodal Information Bottleneck (OMIB), whose optimization objective guarantees the achievability of optimal MIB by setting the regularization weight within a theoretically derived bound. OMIB further addresses imbalanced task-relevant information by dynamically adjusting regularization weights per modality, promoting the inclusion of all task-relevant information. Moreover, we establish a solid information-theoretical foundation for OMIB's optimization and implement it under the variational approximation framework for computational efficiency. Finally, we empirically validate the OMIB's theoretical properties on synthetic data and demonstrate its superiority over the state-of-the-art benchmark methods in various downstream tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents</title>
<link>https://arxiv.org/abs/2505.19997</link>
<guid>https://arxiv.org/abs/2505.19997</guid>
<content:encoded><![CDATA[
arXiv:2505.19997v1 Announce Type: new 
Abstract: Large language models (LLMs) are revolutionizing education, with LLM-based agents playing a key role in simulating student behavior. A major challenge in student simulation is modeling the diverse learning patterns of students at various cognitive levels. However, current LLMs, typically trained as ``helpful assistants'', target at generating perfect responses. As a result, they struggle to simulate students with diverse cognitive abilities, as they often produce overly advanced answers, missing the natural imperfections that characterize student learning and resulting in unrealistic simulations. To address this issue, we propose a training-free framework for student simulation. We begin by constructing a cognitive prototype for each student using a knowledge graph, which captures their understanding of concepts from past learning records. This prototype is then mapped to new tasks to predict student performance. Next, we simulate student solutions based on these predictions and iteratively refine them using a beam search method to better replicate realistic mistakes. To validate our approach, we construct the \texttt{Student\_100} dataset, consisting of $100$ students working on Python programming and $5,000$ learning records. Experimental results show that our method consistently outperforms baseline models, achieving $100\%$ improvement in simulation accuracy.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabPFN: One Model to Rule Them All?</title>
<link>https://arxiv.org/abs/2505.20003</link>
<guid>https://arxiv.org/abs/2505.20003</guid>
<content:encoded><![CDATA[
arXiv:2505.20003v1 Announce Type: new 
Abstract: Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a transformer-based deep learning model for regression and classification on tabular data, which they claim "outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time." Furthermore, they have called TabPFN a "foundation model" for tabular data, as it can support "data generation, density estimation, learning reusable embeddings and fine-tuning". If these statements are well-supported, TabPFN may have the potential to supersede existing modeling approaches on a wide range of statistical tasks, mirroring a similar revolution in other areas of artificial intelligence that began with the advent of large language models. In this paper, we provide a tailored explanation of how TabPFN works for a statistics audience, by emphasizing its interpretation as approximate Bayesian inference. We also provide more evidence of TabPFN's "foundation model" capabilities: We show that an out-of-the-box application of TabPFN vastly outperforms specialized state-of-the-art methods for semi-supervised parameter estimation, prediction under covariate shift, and heterogeneous treatment effect estimation. We further show that TabPFN can outperform LASSO at sparse regression and can break a robustness-efficiency trade-off in classification. All experiments can be reproduced using the code provided at https://github.com/qinglong-tian/tabpfn_study (https://github.com/qinglong-tian/tabpfn_study).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Dependent Regret Bounds for Constrained MABs</title>
<link>https://arxiv.org/abs/2505.20010</link>
<guid>https://arxiv.org/abs/2505.20010</guid>
<content:encoded><![CDATA[
arXiv:2505.20010v1 Announce Type: new 
Abstract: This paper initiates the study of data-dependent regret bounds in constrained MAB settings. These bounds depend on the sequence of losses that characterize the problem instance. Thus, they can be much smaller than classical $\widetilde{\mathcal{O}}(\sqrt{T})$ regret bounds, while being equivalent to them in the worst case. Despite this, data-dependent regret bounds have been completely overlooked in constrained MAB settings. The goal of this paper is to answer the following question: Can data-dependent regret bounds be derived in the presence of constraints? We answer this question affirmatively in constrained MABs with adversarial losses and stochastic constraints. Specifically, our main focus is on the most challenging and natural settings with hard constraints, where the learner must ensure that the constraints are always satisfied with high probability. We design an algorithm with a regret bound consisting of two data-dependent terms. The first term captures the difficulty of satisfying the constraints, while the second one encodes the complexity of learning independently of the presence of constraints. We also prove a lower bound showing that these two terms are not artifacts of our specific approach and analysis, but rather the fundamental components that inherently characterize the complexities of the problem. Finally, in designing our algorithm, we also derive some novel results in the related (and easier) soft constraints settings, which may be of independent interest.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare</title>
<link>https://arxiv.org/abs/2505.20020</link>
<guid>https://arxiv.org/abs/2505.20020</guid>
<content:encoded><![CDATA[
arXiv:2505.20020v1 Announce Type: new 
Abstract: The rise of electronic health records (EHRs) has unlocked new opportunities for medical research, but privacy regulations and data heterogeneity remain key barriers to large-scale machine learning. Federated learning (FL) enables collaborative modeling without sharing raw data, yet faces challenges in harmonizing diverse clinical datasets. This paper presents a two-step data alignment strategy integrating ontologies and large language models (LLMs) to support secure, privacy-preserving FL in healthcare, demonstrating its effectiveness in a real-world project involving semantic mapping of EHR data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage</title>
<link>https://arxiv.org/abs/2505.20026</link>
<guid>https://arxiv.org/abs/2505.20026</guid>
<content:encoded><![CDATA[
arXiv:2505.20026v1 Announce Type: new 
Abstract: We propose Gradient Inversion Transcript (GIT), a novel generative approach for reconstructing training data from leaked gradients. GIT employs a generative attack model, whose architecture is tailored to align with the structure of the leaked model based on theoretical analysis. Once trained offline, GIT can be deployed efficiently and only relies on the leaked gradients to reconstruct the input data, rendering it applicable under various distributed learning environments. When used as a prior for other iterative optimization-based methods, GIT not only accelerates convergence but also enhances the overall reconstruction quality. GIT consistently outperforms existing methods across multiple datasets and demonstrates strong robustness under challenging conditions, including inaccurate gradients, data distribution shifts and discrepancies in model parameters.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions</title>
<link>https://arxiv.org/abs/2505.20030</link>
<guid>https://arxiv.org/abs/2505.20030</guid>
<content:encoded><![CDATA[
arXiv:2505.20030v1 Announce Type: new 
Abstract: We observe a novel 'multiple-descent' phenomenon during the training process of LSTM, in which the test loss goes through long cycles of up and down trend multiple times after the model is overtrained. By carrying out asymptotic stability analysis of the models, we found that the cycles in test loss are closely associated with the phase transition process between order and chaos, and the local optimal epochs are consistently at the critical transition point between the two phases. More importantly, the global optimal epoch occurs at the first transition from order to chaos, where the 'width' of the 'edge of chaos' is the widest, allowing the best exploration of better weight configurations for learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Wave Networks</title>
<link>https://arxiv.org/abs/2505.20034</link>
<guid>https://arxiv.org/abs/2505.20034</guid>
<content:encoded><![CDATA[
arXiv:2505.20034v1 Announce Type: new 
Abstract: Dynamics modeling has been introduced as a novel paradigm in message passing (MP) of graph neural networks (GNNs). Existing methods consider MP between nodes as a heat diffusion process, and leverage heat equation to model the temporal evolution of nodes in the embedding space. However, heat equation can hardly depict the wave nature of graph signals in graph signal processing. Besides, heat equation is essentially a partial differential equation (PDE) involving a first partial derivative of time, whose numerical solution usually has low stability, and leads to inefficient model training. In this paper, we would like to depict more wave details in MP, since graph signals are essentially wave signals that can be seen as a superposition of a series of waves in the form of eigenvector. This motivates us to consider MP as a wave propagation process to capture the temporal evolution of wave signals in the space. Based on wave equation in physics, we innovatively develop a graph wave equation to leverage the wave propagation on graphs. In details, we demonstrate that the graph wave equation can be connected to traditional spectral GNNs, facilitating the design of graph wave networks based on various Laplacians and enhancing the performance of the spectral GNNs. Besides, the graph wave equation is particularly a PDE involving a second partial derivative of time, which has stronger stability on graphs than the heat equation that involves a first partial derivative of time. Additionally, we theoretically prove that the numerical solution derived from the graph wave equation are constantly stable, enabling to significantly enhance model efficiency while ensuring its performance. Extensive experiments show that GWNs achieve SOTA and efficient performance on benchmark datasets, and exhibit outstanding performance in addressing challenging graph problems, such as over-smoothing and heterophily.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction</title>
<link>https://arxiv.org/abs/2505.20036</link>
<guid>https://arxiv.org/abs/2505.20036</guid>
<content:encoded><![CDATA[
arXiv:2505.20036v1 Announce Type: new 
Abstract: Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks</title>
<link>https://arxiv.org/abs/2505.20048</link>
<guid>https://arxiv.org/abs/2505.20048</guid>
<content:encoded><![CDATA[
arXiv:2505.20048v1 Announce Type: new 
Abstract: Time series forecasting plays a critical role in domains such as energy, finance, and healthcare, where accurate predictions inform decision-making under uncertainty. Although Transformer-based models have demonstrated success in sequential modeling, their adoption for time series remains limited by challenges such as noise sensitivity, long-range dependencies, and a lack of inductive bias for temporal structure. In this work, we present a unified and principled framework for benchmarking three prominent Transformer forecasting architectures-Autoformer, Informer, and Patchtst-each evaluated through three architectural variants: Minimal, Standard, and Full, representing increasing levels of complexity and modeling capacity.
  We conduct over 1500 controlled experiments on a suite of ten synthetic signals, spanning five patch lengths and five forecast horizons under both clean and noisy conditions. Our analysis reveals consistent patterns across model families.
  To advance this landscape further, we introduce the Koopman-enhanced Transformer framework, Deep Koopformer, which integrates operator-theoretic latent state modeling to improve stability and interpretability. We demonstrate its efficacy on nonlinear and chaotic dynamical systems. Our results highlight Koopman based Transformer as a promising hybrid approach for robust, interpretable, and theoretically grounded time series forecasting in noisy and complex real-world conditions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits</title>
<link>https://arxiv.org/abs/2505.20051</link>
<guid>https://arxiv.org/abs/2505.20051</guid>
<content:encoded><![CDATA[
arXiv:2505.20051v1 Announce Type: new 
Abstract: Regret minimization in stochastic non-stationary bandits gained popularity over the last decade, as it can model a broad class of real-world problems, from advertising to recommendation systems. Existing literature relies on various assumptions about the reward-generating process, such as Bernoulli or subgaussian rewards. However, in settings such as finance and telecommunications, heavy-tailed distributions naturally arise. In this work, we tackle the heavy-tailed piecewise-stationary bandit problem. Heavy-tailed bandits, introduced by Bubeck et al., 2013, operate on the minimal assumption that the finite absolute centered moments of maximum order $1+\epsilon$ are uniformly bounded by a constant $v<+\infty$, for some $\epsilon \in (0,1]$. We focus on the most popular non-stationary bandit setting, i.e., the piecewise-stationary setting, in which the mean of reward-generating distributions may change at unknown time steps. We provide a novel Catoni-style change-point detection strategy tailored for heavy-tailed distributions that relies on recent advancements in the theory of sequential estimation, which is of independent interest. We introduce Robust-CPD-UCB, which combines this change-point detection strategy with optimistic algorithms for bandits, providing its regret upper bound and an impossibility result on the minimum attainable regret for any policy. Finally, we validate our approach through numerical experiments on synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations</title>
<link>https://arxiv.org/abs/2505.20052</link>
<guid>https://arxiv.org/abs/2505.20052</guid>
<content:encoded><![CDATA[
arXiv:2505.20052v1 Announce Type: new 
Abstract: Protein language models (PLMs) have emerged as powerful tools to detect complex patterns of protein sequences. However, the capability of PLMs to fully capture information on protein sequences might be limited by focusing on single pre-training tasks. Although adding data modalities or supervised objectives can improve the performance of PLMs, pre-training often remains focused on denoising corrupted sequences. To push the boundaries of PLMs, our research investigated a multi-task pre-training strategy. We developed Ankh3, a model jointly optimized on two objectives: masked language modeling with multiple masking probabilities and protein sequence completion relying only on protein sequences as input. This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences. The results demonstrated improved performance in downstream tasks, such as secondary structure prediction, fluorescence, GB1 fitness, and contact prediction. The integration of multiple tasks gave the model a more comprehensive understanding of protein properties, leading to more robust and accurate predictions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEs Are Good for Steering -- If You Select the Right Features</title>
<link>https://arxiv.org/abs/2505.20063</link>
<guid>https://arxiv.org/abs/2505.20063</guid>
<content:encoded><![CDATA[
arXiv:2505.20063v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to learn a decomposition of a model's latent space. This enables useful applications such as steering - influencing the output of a model towards a desired concept - without requiring labeled data. Current methods identify SAE features to steer by analyzing the input tokens that activate them. However, recent work has highlighted that activations alone do not fully describe the effect of a feature on the model's output. In this work, we draw a distinction between two types of features: input features, which mainly capture patterns in the model's input, and output features, which have a human-understandable effect on the model's output. We propose input and output scores to characterize and locate these types of features, and show that high values for both scores rarely co-occur in the same features. These findings have practical implications: after filtering out features with low output scores, we obtain 2-3x improvements when steering with SAEs, making them competitive with supervised methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety</title>
<link>https://arxiv.org/abs/2505.20065</link>
<guid>https://arxiv.org/abs/2505.20065</guid>
<content:encoded><![CDATA[
arXiv:2505.20065v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) continue to advance and find applications across a growing number of fields, ensuring the safety of LLMs has become increasingly critical. To address safety concerns, recent studies have proposed integrating safety constraints into Reinforcement Learning from Human Feedback (RLHF). However, these approaches tend to be complex, as they encompass complicated procedures in RLHF along with additional steps required by the safety constraints. Inspired by Direct Preference Optimization (DPO), we introduce a new algorithm called SafeDPO, which is designed to directly optimize the safety alignment objective in a single stage of policy learning, without requiring relaxation. SafeDPO introduces only one additional hyperparameter to further enhance safety and requires only minor modifications to standard DPO. As a result, it eliminates the need to fit separate reward and cost models or to sample from the language model during fine-tuning, while still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO achieves competitive performance compared to state-of-the-art safety alignment algorithms, both in terms of aligning with human preferences and improving safety.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks</title>
<link>https://arxiv.org/abs/2505.20074</link>
<guid>https://arxiv.org/abs/2505.20074</guid>
<content:encoded><![CDATA[
arXiv:2505.20074v1 Announce Type: new 
Abstract: Graph Neural Network-based methods face privacy leakage risks due to the introduction of topological structures about the targets, which allows attackers to bypass the target's prior knowledge of the sensitive attributes and realize membership inference attacks (MIA) by observing and analyzing the topology distribution. As privacy concerns grow, the assumption of MIA, which presumes that attackers can obtain an auxiliary dataset with the same distribution, is increasingly deviating from reality. In this paper, we categorize the distribution diversity issue in real-world MIA scenarios as an Out-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership Inference Attack (GOOD-MIA) to achieve cross-domain graph attacks. Specifically, we construct shadow subgraphs with distributions from different domains to model the diversity of real-world data. We then explore the stable node representations that remain unchanged under external influences and consider eliminating redundant information from confounding environments and extracting task-relevant key information to more clearly distinguish between the characteristics of training data and unseen data. This OOD-based design makes cross-domain graph attacks possible. Finally, we perform risk extrapolation to optimize the attack's domain adaptability during attack inference to generalize the attack to other domains. Experimental results demonstrate that GOOD-MIA achieves superior attack performance in datasets designed for multiple domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior</title>
<link>https://arxiv.org/abs/2505.20076</link>
<guid>https://arxiv.org/abs/2505.20076</guid>
<content:encoded><![CDATA[
arXiv:2505.20076v1 Announce Type: new 
Abstract: Post-hoc interpretability methods typically attribute a model's behavior to its components, data, or training trajectory in isolation. This leads to explanations that lack a unified view and may miss key interactions. While combining existing methods or applying them at different training stages offers broader insights, these approaches usually lack theoretical support. In this work, we present ExPLAIND, a unified framework that integrates all three perspectives. First, we generalize recent work on gradient path kernels, which reformulate models trained by gradient descent as a kernel machine, to more realistic training settings. Empirically, we find that both a CNN and a Transformer model are replicated accurately by this reformulation. Second, we derive novel parameter- and step-wise influence scores from the kernel feature maps. We show their effectiveness in parameter pruning that is comparable to existing methods, reinforcing their value for model component attribution. Finally, jointly interpreting model components and data over the training process, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking. Among other things, our findings support previously proposed stages of Grokking, while refining the final phase as one of alignment of input embeddings and final layers around a representation pipeline learned after the memorization phase. Overall, ExPLAIND provides a theoretically grounded, unified framework to interpret model behavior and training dynamics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spurious Privacy Leakage in Neural Networks</title>
<link>https://arxiv.org/abs/2505.20095</link>
<guid>https://arxiv.org/abs/2505.20095</guid>
<content:encoded><![CDATA[
arXiv:2505.20095v1 Announce Type: new 
Abstract: Neural networks are vulnerable to privacy attacks aimed at stealing sensitive data. The risks can be amplified in a real-world scenario, particularly when models are trained on limited and biased data. In this work, we investigate the impact of spurious correlation bias on privacy vulnerability. We introduce \emph{spurious privacy leakage}, a phenomenon where spurious groups are significantly more vulnerable to privacy attacks than non-spurious groups. We further show that group privacy disparity increases in tasks with simpler objectives (e.g. fewer classes) due to the persistence of spurious features. Surprisingly, we find that reducing spurious correlation using spurious robust methods does not mitigate spurious privacy leakage. This leads us to introduce a perspective on privacy disparity based on memorization, where mitigating spurious correlation does not mitigate the memorization of spurious data, and therefore, neither the privacy level. Lastly, we compare the privacy of different model architectures trained with spurious data, demonstrating that, contrary to prior works, architectural choice can affect privacy outcomes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer in Protein: A Survey</title>
<link>https://arxiv.org/abs/2505.20098</link>
<guid>https://arxiv.org/abs/2505.20098</guid>
<content:encoded><![CDATA[
arXiv:2505.20098v1 Announce Type: new 
Abstract: As protein informatics advances rapidly, the demand for enhanced predictive accuracy, structural analysis, and functional understanding has intensified. Transformer models, as powerful deep learning architectures, have demonstrated unprecedented potential in addressing diverse challenges across protein research. However, a comprehensive review of Transformer applications in this field remains lacking. This paper bridges this gap by surveying over 100 studies, offering an in-depth analysis of practical implementations and research progress of Transformers in protein-related tasks. Our review systematically covers critical domains, including protein structure prediction, function prediction, protein-protein interaction analysis, functional annotation, and drug discovery/target identification. To contextualize these advancements across various protein domains, we adopt a domain-oriented classification system. We first introduce foundational concepts: the Transformer architecture and attention mechanisms, categorize Transformer variants tailored for protein science, and summarize essential protein knowledge. For each research domain, we outline its objectives and background, critically evaluate prior methods and their limitations, and highlight transformative contributions enabled by Transformer models. We also curate and summarize pivotal datasets and open-source code resources to facilitate reproducibility and benchmarking. Finally, we discuss persistent challenges in applying Transformers to protein informatics and propose future research directions. This review aims to provide a consolidated foundation for the synergistic integration of Transformer and protein informatics, fostering further innovation and expanded applications in the field.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.20107</link>
<guid>https://arxiv.org/abs/2505.20107</guid>
<content:encoded><![CDATA[
arXiv:2505.20107v1 Announce Type: new 
Abstract: Text-to-multiview (T2MV) generation, which produces coherent multiview images from a single text prompt, remains computationally intensive, while accelerated T2MV methods using few-step diffusion models often sacrifice image fidelity and view consistency. To address this, we propose a novel reinforcement learning (RL) finetuning framework tailored for few-step T2MV diffusion models to jointly optimize per-view fidelity and cross-view consistency. Specifically, we first reformulate T2MV denoising across all views as a single unified Markov decision process, enabling multiview-aware policy optimization driven by a joint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV sampling technique that adds an inversion-denoising pass to reinforce both viewpoint and text conditioning, resulting in improved T2MV generation at the cost of inference time. To internalize its performance gains into the base sampling policy, we develop MV-ZigAL, a novel policy optimization strategy that uses reward advantages of ZMV-Sampling over standard sampling as learning signals for policy updates. Finally, noting that the joint-view reward objective under-optimizes per-view fidelity but naively optimizing single-view metrics neglects cross-view alignment, we reframe RL finetuning for T2MV diffusion models as a constrained optimization problem that maximizes per-view fidelity subject to an explicit joint-view constraint, thereby enabling more efficient and balanced policy updates. By integrating this constrained optimization paradigm with MV-ZigAL, we establish our complete RL finetuning framework, referred to as MVC-ZigAL, which effectively refines the few-step T2MV diffusion baseline in both fidelity and consistency while preserving its few-step efficiency.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxy-Free GFlowNet</title>
<link>https://arxiv.org/abs/2505.20110</link>
<guid>https://arxiv.org/abs/2505.20110</guid>
<content:encoded><![CDATA[
arXiv:2505.20110v1 Announce Type: new 
Abstract: Generative Flow Networks (GFlowNets) are a promising class of generative models designed to sample diverse, high-reward structures by modeling distributions over compositional objects. In many real-world applications, obtaining the reward function for such objects is expensive, time-consuming, or requires human input, making it necessary to train GFlowNets from historical datasets. Most existing methods adopt a model-based approach, learning a proxy model from the dataset to approximate the reward function. However, this strategy inherently ties the quality of the learned policy to the accuracy of the proxy, introducing additional complexity and uncertainty into the training process. To overcome these limitations, we propose \textbf{Trajectory-Distilled GFlowNet (TD-GFN)}, a \emph{proxy-free} training framework that eliminates the need for out-of-dataset reward queries. Our method is motivated by the key observation that different edges in the associated directed acyclic graph (DAG) contribute unequally to effective policy learning. TD-GFN leverages inverse reinforcement learning to estimate edge-level rewards from the offline dataset, which are then used to ingeniously prune the DAG and guide backward trajectory sampling during training. This approach directs the policy toward high-reward regions while reducing the complexity of model fitting. Empirical results across multiple tasks show that TD-GFN trains both efficiently and reliably, significantly outperforming existing baselines in convergence speed and sample quality.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Generalization in Diffusion Models via Probability Flow Distance</title>
<link>https://arxiv.org/abs/2505.20123</link>
<guid>https://arxiv.org/abs/2505.20123</guid>
<content:encoded><![CDATA[
arXiv:2505.20123v1 Announce Type: new 
Abstract: Diffusion models have emerged as a powerful class of generative models, capable of producing high-quality samples that generalize beyond the training data. However, evaluating this generalization remains challenging: theoretical metrics are often impractical for high-dimensional data, while no practical metrics rigorously measure generalization. In this work, we bridge this gap by introducing probability flow distance ($\texttt{PFD}$), a theoretically grounded and computationally efficient metric to measure distributional generalization. Specifically, $\texttt{PFD}$ quantifies the distance between distributions by comparing their noise-to-data mappings induced by the probability flow ODE. Moreover, by using $\texttt{PFD}$ under a teacher-student evaluation protocol, we empirically uncover several key generalization behaviors in diffusion models, including: (1) scaling behavior from memorization to generalization, (2) early learning and double descent training dynamics, and (3) bias-variance decomposition. Beyond these insights, our work lays a foundation for future empirical and theoretical studies on generalization in diffusion models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach</title>
<link>https://arxiv.org/abs/2505.20130</link>
<guid>https://arxiv.org/abs/2505.20130</guid>
<content:encoded><![CDATA[
arXiv:2505.20130v1 Announce Type: new 
Abstract: This paper focuses on the design of spatial experiments to optimize the amount of information derived from the experimental data and enhance the accuracy of the resulting causal effect estimator. We propose a surrogate function for the mean squared error (MSE) of the estimator, which facilitates the use of classical graph cut algorithms to learn the optimal design. Our proposal offers three key advances: (1) it accommodates moderate to large spatial interference effects; (2) it adapts to different spatial covariance functions; (3) it is computationally efficient. Theoretical results and numerical experiments based on synthetic environments and a dispatch simulator that models a city-scale ridesharing market, further validate the effectiveness of our design. A python implementation of our method is available at https://github.com/Mamba413/CausalGraphCut.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.20131</link>
<guid>https://arxiv.org/abs/2505.20131</guid>
<content:encoded><![CDATA[
arXiv:2505.20131v1 Announce Type: new 
Abstract: Molecular editing aims to modify a given molecule to optimize desired chemical properties while preserving structural similarity. However, current approaches typically rely on string-based or continuous representations, which fail to adequately capture the discrete, graph-structured nature of molecules, resulting in limited structural fidelity and poor controllability. In this paper, we propose MolEditRL, a molecular editing framework that explicitly integrates structural constraints with precise property optimization. Specifically, MolEditRL consists of two stages: (1) a discrete graph diffusion model pretrained to reconstruct target molecules conditioned on source structures and natural language instructions; (2) an editing-aware reinforcement learning fine-tuning stage that further enhances property alignment and structural preservation by explicitly optimizing editing decisions under graph constraints. For comprehensive evaluation, we construct MolEdit-Instruct, the largest and most property-rich molecular editing dataset, comprising 3 million diverse examples spanning single- and multi-property tasks across 10 chemical attributes. Experimental results demonstrate that MolEditRL significantly outperforms state-of-the-art methods in both property optimization accuracy and structural fidelity, achieving a 74\% improvement in editing success rate while using 98\% fewer parameters.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks</title>
<link>https://arxiv.org/abs/2505.20132</link>
<guid>https://arxiv.org/abs/2505.20132</guid>
<content:encoded><![CDATA[
arXiv:2505.20132v1 Announce Type: new 
Abstract: Tensorizing a neural network involves reshaping some or all of its dense weight matrices into higher-order tensors and approximating them using low-rank tensor network decompositions. This technique has shown promise as a model compression strategy for large-scale neural networks. However, despite encouraging empirical results, tensorized neural networks (TNNs) remain underutilized in mainstream deep learning. In this position paper, we offer a perspective on both the potential and current limitations of TNNs. We argue that TNNs represent a powerful yet underexplored framework for deep learning--one that deserves greater attention from both engineering and theoretical communities. Beyond compression, we highlight the value of TNNs as a flexible class of architectures with distinctive scaling properties and increased interpretability. A central feature of TNNs is the presence of bond indices, which introduce new latent spaces not found in conventional networks. These internal representations may provide deeper insight into the evolution of features across layers, potentially advancing the goals of mechanistic interpretability. We conclude by outlining several key research directions aimed at overcoming the practical barriers to scaling and adopting TNNs in modern deep learning workflows.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning</title>
<link>https://arxiv.org/abs/2505.20135</link>
<guid>https://arxiv.org/abs/2505.20135</guid>
<content:encoded><![CDATA[
arXiv:2505.20135v1 Announce Type: new 
Abstract: Replay-based continual learning (CL) methods assume that models trained on a small subset can also effectively minimize the empirical risk of the complete dataset. These methods maintain a memory buffer that stores a sampled subset of data from previous tasks to consolidate past knowledge. However, this assumption is not guaranteed in practice due to the limited capacity of the memory buffer and the heuristic criteria used for buffer data selection. To address this issue, we propose a new dataset distillation framework tailored for CL, which maintains a learnable memory buffer to distill the global information from the current task data and accumulated knowledge preserved in the previous memory buffer. Moreover, to avoid the computational overhead and overfitting risks associated with parameterizing the entire buffer during distillation, we introduce a lightweight distillation module that can achieve global information distillation solely by generating learnable soft labels for the memory buffer data. Extensive experiments show that, our method can achieve competitive results and effectively mitigates forgetting across various datasets. The source code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks</title>
<link>https://arxiv.org/abs/2505.20137</link>
<guid>https://arxiv.org/abs/2505.20137</guid>
<content:encoded><![CDATA[
arXiv:2505.20137v1 Announce Type: new 
Abstract: Predictive Coding (PC) offers a biologically plausible alternative to backpropagation for neural network training, yet struggles with deeper architectures. This paper identifies the root cause: an inherent signal decay problem where gradients attenuate exponentially with depth, becoming computationally negligible due to numerical precision constraints. To address this fundamental limitation, we introduce Error Optimization (EO), a novel reparameterization that preserves PC's theoretical properties while eliminating signal decay. By optimizing over prediction errors rather than states, EO enables signals to reach all layers simultaneously and without attenuation, converging orders of magnitude faster than standard PC. Experiments across multiple architectures and datasets demonstrate that EO matches backpropagation's performance even for deeper models where conventional PC struggles. Besides practical improvements, our work provides theoretical insight into PC dynamics and establishes a foundation for scaling biologically-inspired learning to deeper architectures on digital hardware and beyond.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Stitching by Functional Latent Alignment</title>
<link>https://arxiv.org/abs/2505.20142</link>
<guid>https://arxiv.org/abs/2505.20142</guid>
<content:encoded><![CDATA[
arXiv:2505.20142v1 Announce Type: new 
Abstract: Evaluating functional similarity involves quantifying the degree to which independently trained neural networks learn functionally similar representations. Reliably inferring the functional similarity of these networks remains an open problem with far-reaching implications for AI. Model stitching has emerged as a promising paradigm, where an optimal affine transformation aligns two models to solve a task, with the stitched model serving as a proxy for functional similarity. In this work, we draw inspiration from the knowledge distillation literature and propose Functional Latent Alignment (FuLA) as a novel optimality condition for model stitching. We revisit previously explored functional similarity testbeds and introduce a new one, based on which FuLA emerges as an overall more reliable method of functional similarity. Specifically, our experiments in (a) adversarial training, (b) shortcut training and, (c) cross-layer stitching, reveal that FuLA is less prone to artifacts tied to training on task cues while achieving non-trivial alignments that are missed by stitch-level matching.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the (Non) Injectivity of Piecewise Linear Janossy Pooling</title>
<link>https://arxiv.org/abs/2505.20150</link>
<guid>https://arxiv.org/abs/2505.20150</guid>
<content:encoded><![CDATA[
arXiv:2505.20150v1 Announce Type: new 
Abstract: Multiset functions, which are functions that map multisets to vectors, are a fundamental tool in the construction of neural networks for multisets and graphs. To guarantee that the vector representation of the multiset is faithful, it is often desirable to have multiset mappings that are both injective and bi-Lipschitz. Currently, there are several constructions of multiset functions achieving both these guarantees, leading to improved performance in some tasks but often also to higher compute time than standard constructions. Accordingly, it is natural to inquire whether simpler multiset functions achieving the same guarantees are available. In this paper, we make a large step towards giving a negative answer to this question. We consider the family of k-ary Janossy pooling, which includes many of the most popular multiset models, and prove that no piecewise linear Janossy pooling function can be injective. On the positive side, we show that when restricted to multisets without multiplicities, even simple deep-sets models suffice for injectivity and bi-Lipschitzness.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.20161</link>
<guid>https://arxiv.org/abs/2505.20161</guid>
<content:encoded><![CDATA[
arXiv:2505.20161v1 Announce Type: new 
Abstract: Effective generalization in language models depends critically on the diversity of their training data. Yet existing diversity metrics often fall short of this goal, relying on surface-level heuristics that are decoupled from model behavior. This motivates us to ask: What kind of diversity in training data actually drives generalization in language models -- and how can we measure and amplify it? Through large-scale empirical analyses spanning over 300 training runs, carefully controlled for data scale and quality, we show that data diversity can be a strong predictor of generalization in LLM reasoning -- as measured by average model performance on unseen out-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies diversity via the entropy of model-induced gradients. Despite using a small off-the-shelf proxy model for gradients, G-Vendi consistently outperforms alternative measures, achieving strong correlation (Spearman's $\rho \approx 0.9$) with out-of-distribution (OOD) performance on both natural language inference (NLI) and math reasoning tasks. Building on this insight, we present Prismatic Synthesis, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space. Experimental results show that Prismatic Synthesis consistently improves model performance as we scale synthetic data -- not just on in-distribution test but across unseen, out-of-distribution benchmarks -- significantly outperforming state-of-the-art models that rely on 20 times larger data generator than ours. For example, PrismMath-7B, our model distilled from a 32B LLM, outperforms R1-Distill-Qwen-7B -- the same base model trained on proprietary data generated by 671B R1 -- on 6 out of 7 challenging benchmarks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Framework for Grokking: Interpolation followed by Riemannian Norm Minimisation</title>
<link>https://arxiv.org/abs/2505.20172</link>
<guid>https://arxiv.org/abs/2505.20172</guid>
<content:encoded><![CDATA[
arXiv:2505.20172v1 Announce Type: new 
Abstract: We study the dynamics of gradient flow with small weight decay on general training losses $F: \mathbb{R}^d \to \mathbb{R}$. Under mild regularity assumptions and assuming convergence of the unregularised gradient flow, we show that the trajectory with weight decay $\lambda$ exhibits a two-phase behaviour as $\lambda \to 0$. During the initial fast phase, the trajectory follows the unregularised gradient flow and converges to a manifold of critical points of $F$. Then, at time of order $1/\lambda$, the trajectory enters a slow drift phase and follows a Riemannian gradient flow minimising the $\ell_2$-norm of the parameters. This purely optimisation-based phenomenon offers a natural explanation for the \textit{grokking} effect observed in deep learning, where the training loss rapidly reaches zero while the test loss plateaus for an extended period before suddenly improving. We argue that this generalisation jump can be attributed to the slow norm reduction induced by weight decay, as explained by our analysis. We validate this mechanism empirically on several synthetic regression tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination</title>
<link>https://arxiv.org/abs/2505.20177</link>
<guid>https://arxiv.org/abs/2505.20177</guid>
<content:encoded><![CDATA[
arXiv:2505.20177v1 Announce Type: new 
Abstract: Inspired by recent work on learning with distribution shift, we give a general outlier removal algorithm called iterative polynomial filtering and show a number of striking applications for supervised learning with contamination: (1) We show that any function class that can be approximated by low-degree polynomials with respect to a hypercontractive distribution can be efficiently learned under bounded contamination (also known as nasty noise). This is a surprising resolution to a longstanding gap between the complexity of agnostic learning and learning with contamination, as it was widely believed that low-degree approximators only implied tolerance to label noise. (2) For any function class that admits the (stronger) notion of sandwiching approximators, we obtain near-optimal learning guarantees even with respect to heavy additive contamination, where far more than $1/2$ of the training set may be added adversarially. Prior related work held only for regression and in a list-decodable setting. (3) We obtain the first efficient algorithms for tolerant testable learning of functions of halfspaces with respect to any fixed log-concave distribution. Even the non-tolerant case for a single halfspace in this setting had remained open. These results significantly advance our understanding of efficient supervised learning under contamination, a setting that has been much less studied than its unsupervised counterpart.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on feature fusion and multimodal patent text based on graph attention network</title>
<link>https://arxiv.org/abs/2505.20188</link>
<guid>https://arxiv.org/abs/2505.20188</guid>
<content:encoded><![CDATA[
arXiv:2505.20188v1 Announce Type: new 
Abstract: Aiming at the problems of cross-modal feature fusion, low efficiency of long text modeling and lack of hierarchical semantic coherence in patent text semantic mining, this study proposes HGM-Net, a deep learning framework that integrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention Network (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a dynamic mask, contrast and cross-structural similarity constraints on the word, sentence and paragraph hierarchies through HCL. Contrast and cross-structural similarity constraints are constructed at the word and paragraph levels by HCL to strengthen the local semantic and global thematic consistency of patent text; M-GAT models patent classification codes, citation relations and text semantics as heterogeneous graph structures, and achieves dynamic fusion of multi-source features by cross-modal gated attention; MSA adopts a hierarchical sparsity strategy to optimize the computational efficiency of long text modeling at word, phrase, sentence and paragraph granularity. Experiments show that the framework demonstrates significant advantages over existing deep learning methods in tasks such as patent classification and similarity matching, and provides a solution with both theoretical innovation and practical value for solving the problems of patent examination efficiency improvement and technology relevance mining.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement</title>
<link>https://arxiv.org/abs/2505.20192</link>
<guid>https://arxiv.org/abs/2505.20192</guid>
<content:encoded><![CDATA[
arXiv:2505.20192v1 Announce Type: new 
Abstract: The integration of large language models (LLMs) with function calling has emerged as a crucial capability for enhancing their practical utility in real-world applications. However, effectively combining reasoning processes with accurate function execution remains a significant challenge. Traditional training approaches often struggle to balance the detailed reasoning steps with the precision of function calls, leading to suboptimal performance. To address these limitations, we introduce FunReason, a novel framework that enhances LLMs' function calling capabilities through an automated data refinement strategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason leverages LLMs' natural reasoning abilities to generate high-quality training examples, focusing on query parseability, reasoning coherence, and function call precision. The SRML approach dynamically balances the contribution of reasoning processes and function call accuracy during training, addressing the inherent trade-off between these two critical aspects. FunReason achieves performance comparable to GPT-4o while effectively mitigating catastrophic forgetting during fine-tuning. FunReason provides a comprehensive solution for enhancing LLMs' function calling capabilities by introducing a balanced training methodology and a data refinement pipeline. For code and dataset, please refer to our repository at GitHub https://github.com/BingguangHao/FunReason
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Fine-Tuning with Column Space Projection</title>
<link>https://arxiv.org/abs/2505.20211</link>
<guid>https://arxiv.org/abs/2505.20211</guid>
<content:encoded><![CDATA[
arXiv:2505.20211v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) with minimal computational overhead is essential for efficiently adapting them to downstream tasks under resource constraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), facilitate this by updating only a small subset of parameters. However, recent studies show that LoRA diverges from full fine-tuning (Full FT) in its learning behavior, particularly in terms of spectral properties. Motivated by these findings, we propose PiCa, the first theoretically grounded PEFT method based on the spectral properties of fine-tuned weights. PiCa projects gradients onto the low-rank column subspace of pre-trained weights and exhibits learning patterns more closely aligned with Full FT. Furthermore, we show that combining PiCa with weight sharing drastically reduces the number of trainable parameters without compromising performance, enabling to achieve superior performance than LoRA using 13x fewer trainable parameters. Extensive experiments demonstrate PiCa achieves the state-of-the-art performance compared to existing PEFT methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-grained List-wise Alignment for Generative Medication Recommendation</title>
<link>https://arxiv.org/abs/2505.20218</link>
<guid>https://arxiv.org/abs/2505.20218</guid>
<content:encoded><![CDATA[
arXiv:2505.20218v1 Announce Type: new 
Abstract: Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs). We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists. FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug. To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety-accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at https://github.com/cxfann/Flame.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Flow Matching for Learning Update Dynamics in Neural Network Training</title>
<link>https://arxiv.org/abs/2505.20221</link>
<guid>https://arxiv.org/abs/2505.20221</guid>
<content:encoded><![CDATA[
arXiv:2505.20221v1 Announce Type: new 
Abstract: Training deep neural networks remains computationally intensive due to the itera2 tive nature of gradient-based optimization. We propose Gradient Flow Matching (GFM), a continuous-time modeling framework that treats neural network training as a dynamical system governed by learned optimizer-aware vector fields. By leveraging conditional flow matching, GFM captures the underlying update rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth extrapolation of weight trajectories toward convergence. Unlike black-box sequence models, GFM incorporates structural knowledge of gradient-based updates into the learning objective, facilitating accurate forecasting of final weights from partial training sequences. Empirically, GFM achieves forecasting accuracy that is competitive with Transformer-based models and significantly outperforms LSTM and other classical baselines. Furthermore, GFM generalizes across neural architectures and initializations, providing a unified framework for studying optimization dynamics and accelerating convergence prediction.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance</title>
<link>https://arxiv.org/abs/2505.20229</link>
<guid>https://arxiv.org/abs/2505.20229</guid>
<content:encoded><![CDATA[
arXiv:2505.20229v1 Announce Type: new 
Abstract: Transformer-based CLIP models are widely used for text-image probing and feature extraction, making it relevant to understand the internal mechanisms behind their predictions. While recent works show that Sparse Autoencoders (SAEs) yield interpretable latent components, they focus on what these encode and miss how they drive predictions. We introduce a scalable framework that reveals what latent components activate for, how they align with expected semantics, and how important they are to predictions. To achieve this, we adapt attribution patching for instance-wise component attributions in CLIP and highlight key faithfulness limitations of the widely used Logit Lens technique. By combining attributions with semantic alignment scores, we can automatically uncover reliance on components that encode semantically unexpected or spurious concepts. Applied across multiple CLIP variants, our method uncovers hundreds of surprising components linked to polysemous words, compound nouns, visual typography and dataset artifacts. While text embeddings remain prone to semantic ambiguity, they are more robust to spurious correlations compared to linear classifiers trained on image embeddings. A case study on skin lesion detection highlights how such classifiers can amplify hidden shortcuts, underscoring the need for holistic, mechanistic interpretability. We provide code at https://github.com/maxdreyer/attributing-clip.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Federated Learning With Missing Modalities through Feature Imputation Network</title>
<link>https://arxiv.org/abs/2505.20232</link>
<guid>https://arxiv.org/abs/2505.20232</guid>
<content:encoded><![CDATA[
arXiv:2505.20232v1 Announce Type: new 
Abstract: Multimodal federated learning holds immense potential for collaboratively training models from multiple sources without sharing raw data, addressing both data scarcity and privacy concerns, two key challenges in healthcare. A major challenge in training multimodal federated models in healthcare is the presence of missing modalities due to multiple reasons, including variations in clinical practice, cost and accessibility constraints, retrospective data collection, privacy concerns, and occasional technical or human errors. Previous methods typically rely on publicly available real datasets or synthetic data to compensate for missing modalities. However, obtaining real datasets for every disease is impractical, and training generative models to synthesize missing modalities is computationally expensive and prone to errors due to the high dimensionality of medical data. In this paper, we propose a novel, lightweight, low-dimensional feature translator to reconstruct bottleneck features of the missing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH Open-I, and CheXpert), in both homogeneous and heterogeneous settings consistently improve the performance of competitive baselines. The code and implementation details are available at: https://github.com/bhattarailab/FedFeatGen
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Deep Learning via Implicit Regularization</title>
<link>https://arxiv.org/abs/2505.20235</link>
<guid>https://arxiv.org/abs/2505.20235</guid>
<content:encoded><![CDATA[
arXiv:2505.20235v1 Announce Type: new 
Abstract: Modern deep learning models generalize remarkably well in-distribution, despite being overparametrized and trained with little to no explicit regularization. Instead, current theory credits implicit regularization imposed by the choice of architecture, hyperparameters and optimization procedure. However, deploying deep learning models out-of-distribution, in sequential decision-making tasks, or in safety-critical domains, necessitates reliable uncertainty quantification, not just a point estimate. The machinery of modern approximate inference -- Bayesian deep learning -- should answer the need for uncertainty quantification, but its effectiveness has been challenged by our inability to define useful explicit inductive biases through priors, as well as the associated computational burden. Instead, in this work we demonstrate, both theoretically and empirically, how to regularize a variational deep network implicitly via the optimization procedure, just as for standard deep learning. We fully characterize the inductive bias of (stochastic) gradient descent in the case of an overparametrized linear model as generalized variational inference and demonstrate the importance of the choice of parametrization. Finally, we show empirically that our approach achieves strong in- and out-of-distribution performance without tuning of additional hyperparameters and with minimal time and memory overhead over standard deep learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.20241</link>
<guid>https://arxiv.org/abs/2505.20241</guid>
<content:encoded><![CDATA[
arXiv:2505.20241v1 Announce Type: new 
Abstract: Reasoning has substantially improved the performance of large language models (LLMs) on complicated tasks. Central to the current reasoning studies, Process Reward Models (PRMs) offer a fine-grained evaluation of intermediate reasoning steps and guide the reasoning process. However, extending PRMs to multimodal large language models (MLLMs) introduces challenges. Since multimodal reasoning covers a wider range of tasks compared to text-only scenarios, the resulting distribution shift from the training to testing sets is more severe, leading to greater generalization difficulty. Training a reliable multimodal PRM, therefore, demands large and diverse datasets to ensure sufficient coverage. However, current multimodal reasoning datasets suffer from a marked quality imbalance, which degrades PRM performance and highlights the need for an effective data selection strategy. To address the issues, we introduce DreamPRM, a domain-reweighted training framework for multimodal PRMs which employs bi-level optimization. In the lower-level optimization, DreamPRM performs fine-tuning on multiple datasets with domain weights, allowing the PRM to prioritize high-quality reasoning signals and alleviating the impact of dataset quality imbalance. In the upper-level optimization, the PRM is evaluated on a separate meta-learning dataset; this feedback updates the domain weights through an aggregation loss function, thereby improving the generalization capability of trained PRM. Extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that test-time scaling with DreamPRM consistently improves the performance of state-of-the-art MLLMs. Further comparisons reveal that DreamPRM's domain-reweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing test-time scaling approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large Language Models</title>
<link>https://arxiv.org/abs/2505.20242</link>
<guid>https://arxiv.org/abs/2505.20242</guid>
<content:encoded><![CDATA[
arXiv:2505.20242v1 Announce Type: new 
Abstract: Solving NP-hard combinatorial optimization problems (COPs) (e.g., traveling salesman problems (TSPs) and capacitated vehicle routing problems (CVRPs)) in practice traditionally involves handcrafting heuristics or specifying a search space for finding effective heuristics. The main challenges from these approaches, however, are the sheer amount of domain knowledge and implementation efforts required from human experts. Recently, significant progress has been made to address these challenges, particularly by using large language models (LLMs) to design heuristics within some predetermined generalized algorithmic framework (GAF, e.g., ant colony optimization and guided local search) for building key functions/components (e.g., a priori information on how promising it is to include each edge in a solution for TSP and CVRP). Although existing methods leveraging this idea have shown to yield impressive optimization performance, they are not fully end-to-end and still require considerable manual interventions. In this paper, we propose a novel end-to-end framework, named RedAHD, that enables these LLM-based heuristic design methods to operate without the need of GAFs. More specifically, RedAHD employs LLMs to automate the process of reduction, i.e., transforming the COP at hand into similar COPs that are better-understood, from which LLM-based heuristic design methods can design effective heuristics for directly solving the transformed COPs and, in turn, indirectly solving the original COP. Our experimental results, evaluated on six COPs, show that RedAHD is capable of designing heuristics with competitive or improved results over the state-of-the-art methods with minimal human involvement.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Extrapolative Sequence Transformations from Markov Chains</title>
<link>https://arxiv.org/abs/2505.20251</link>
<guid>https://arxiv.org/abs/2505.20251</guid>
<content:encoded><![CDATA[
arXiv:2505.20251v1 Announce Type: new 
Abstract: Most successful applications of deep learning involve similar training and test conditions. However, tasks such as biological sequence design involve searching for sequences that improve desirable properties beyond previously known values, which requires novel hypotheses that \emph{extrapolate} beyond training data. In these settings, extrapolation may be achieved by using random search methods such as Markov chain Monte Carlo (MCMC), which, given an initial state, sample local transformations to approximate a target density that rewards states with the desired properties. However, even with a well-designed proposal, MCMC may struggle to explore large structured state spaces efficiently. Rather than relying on stochastic search, it would be desirable to have a model that greedily optimizes the properties of interest, successfully extrapolating in as few steps as possible. We propose to learn such a model from the Markov chains resulting from MCMC search. Specifically, our approach uses selected states from Markov chains as a source of training data for an autoregressive model, which is then able to efficiently generate novel sequences that extrapolate along the sequence-level properties of interest. The proposed approach is validated on three problems: protein sequence design, text sentiment control, and text anonymization. We find that the autoregressive model can extrapolate as well or better than MCMC, but with the additional benefits of scalability and significantly higher sample efficiency.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs</title>
<link>https://arxiv.org/abs/2505.20254</link>
<guid>https://arxiv.org/abs/2505.20254</guid>
<content:encoded><![CDATA[
arXiv:2505.20254v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neural network activations into interpretable features. However, the aspiration to identify a canonical set of features is challenged by the observed inconsistency of learned SAE features across different training runs, undermining the reliability and efficiency of MI research. This position paper argues that mechanistic interpretability should prioritize feature consistency in SAEs -- the reliable convergence to equivalent feature sets across independent runs. We propose using the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to operationalize consistency and demonstrate that high levels are achievable (0.80 for TopK SAEs on LLM activations) with appropriate architectural choices. Our contributions include detailing the benefits of prioritizing consistency; providing theoretical grounding and synthetic validation using a model organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery; and extending these findings to real-world LLM data, where high feature consistency strongly correlates with the semantic similarity of learned feature explanations. We call for a community-wide shift towards systematically measuring feature consistency to foster robust cumulative progress in MI.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits</title>
<link>https://arxiv.org/abs/2505.20268</link>
<guid>https://arxiv.org/abs/2505.20268</guid>
<content:encoded><![CDATA[
arXiv:2505.20268v1 Announce Type: new 
Abstract: Reinforcement learning with outcome-based feedback faces a fundamental challenge: when rewards are only observed at trajectory endpoints, how do we assign credit to the right actions? This paper provides the first comprehensive analysis of this problem in online RL with general function approximation. We develop a provably sample-efficient algorithm achieving $\widetilde{O}({C_{\rm cov} H^3}/{\epsilon^2})$ sample complexity, where $C_{\rm cov}$ is the coverability coefficient of the underlying MDP. By leveraging general function approximation, our approach works effectively in large or infinite state spaces where tabular methods fail, requiring only that value functions and reward functions can be represented by appropriate function classes. Our results also characterize when outcome-based feedback is statistically separated from per-step rewards, revealing an unavoidable exponential separation for certain MDPs. For deterministic MDPs, we show how to eliminate the completeness assumption, dramatically simplifying the algorithm. We further extend our approach to preference-based feedback settings, proving that equivalent statistical efficiency can be achieved even under more limited information. Together, these results constitute a theoretical foundation for understanding the statistical properties of outcome-based reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Kernel Function for Fast Angle Testing</title>
<link>https://arxiv.org/abs/2505.20274</link>
<guid>https://arxiv.org/abs/2505.20274</guid>
<content:encoded><![CDATA[
arXiv:2505.20274v1 Announce Type: new 
Abstract: In this paper, we study the angle testing problem in high-dimensional Euclidean spaces and propose two projection-based probabilistic kernel functions, one designed for angle comparison and the other for angle thresholding. Unlike existing approaches that rely on random projection vectors drawn from Gaussian distributions, our approach leverages reference angles and employs a deterministic structure for the projection vectors. Notably, our kernel functions do not require asymptotic assumptions, such as the number of projection vectors tending to infinity, and can be both theoretically and experimentally shown to outperform Gaussian-distribution-based kernel functions. We further apply the proposed kernel function to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared to the state-of-the-art graph-based search algorithm HNSW.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Coverage Principle: A Framework for Understanding Compositional Generalization</title>
<link>https://arxiv.org/abs/2505.20278</link>
<guid>https://arxiv.org/abs/2505.20278</guid>
<content:encoded><![CDATA[
arXiv:2505.20278v1 Announce Type: new 
Abstract: Large language models excel at pattern matching, yet often fall short in systematic compositional generalization. We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituting fragments that yield identical results when used in the same contexts. We demonstrate that this framework has a strong predictive power for the generalization capabilities of Transformers. First, we derive and empirically confirm that the training data required for two-hop generalization grows at least quadratically with the token set size, and the training data efficiency does not improve with 20x parameter scaling. Second, for compositional tasks with path ambiguity where one variable affects the output through multiple computational paths, we show that Transformers learn context-dependent state representations that undermine both performance and interoperability. Third, Chain-of-Thought supervision improves training data efficiency for multi-hop tasks but still struggles with path ambiguity. Finally, we outline a \emph{mechanism-based} taxonomy that distinguishes three ways neural networks can generalize: structure-based (bounded by coverage), property-based (leveraging algebraic invariances), and shared-operator (through function reuse). This conceptual lens contextualizes our results and highlights where new architectural ideas are needed to achieve systematic compositionally. Overall, the coverage principle provides a unified lens for understanding compositional reasoning, and underscores the need for fundamental architectural or training innovations to achieve truly systematic compositionality.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset</title>
<link>https://arxiv.org/abs/2505.13069</link>
<guid>https://arxiv.org/abs/2505.13069</guid>
<content:encoded><![CDATA[
arXiv:2505.13069v1 Announce Type: cross 
Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide risk assessment in adolescents. This study investigates a multimodal approach for this challenge, integrating automatic transcription with WhisperX, linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM. Additionally, handcrafted acoustic features -- including MFCCs, spectral contrast, and pitch-related statistics -- were incorporated. We explored three fusion strategies: early concatenation, modality-specific processing, and weighted attention with mixup regularization. Results show that weighted attention provided the best generalization, achieving 69% accuracy on the development set, though a performance gap between development and test sets highlights generalization challenges. Our findings, strictly tied to the MINI-KID framework, emphasize the importance of refining embedding representations and fusion mechanisms to enhance classification reliability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language</title>
<link>https://arxiv.org/abs/2505.18159</link>
<guid>https://arxiv.org/abs/2505.18159</guid>
<content:encoded><![CDATA[
arXiv:2505.18159v1 Announce Type: cross 
Abstract: The digital exclusion of endangered languages remains a critical challenge in NLP, limiting both linguistic research and revitalization efforts. This study introduces the first computational investigation of Comanche, an Uto-Aztecan language on the verge of extinction, demonstrating how minimal-cost, community-informed NLP interventions can support language preservation. We present a manually curated dataset of 412 phrases, a synthetic data generation pipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language identification. Our experiments reveal that while LLMs struggle with Comanche in zero-shot settings, few-shot prompting significantly improves performance, achieving near-perfect accuracy with just five examples. Our findings highlight the potential of targeted NLP methodologies in low-resource contexts and emphasize that visibility is the first step toward inclusion. By establishing a foundation for Comanche in NLP, we advocate for computational approaches that prioritize accessibility, cultural sensitivity, and community engagement.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Battery Material Optimization through iterative Machine Learning</title>
<link>https://arxiv.org/abs/2505.18162</link>
<guid>https://arxiv.org/abs/2505.18162</guid>
<content:encoded><![CDATA[
arXiv:2505.18162v1 Announce Type: cross 
Abstract: The performance of battery materials is determined by their composition and the processing conditions employed during commercial-scale fabrication, where raw materials undergo complex processing steps with various additives to yield final products. As the complexity of these parameters expands with the development of industry, conventional one-factor-at-a-time (OFAT) experiment becomes old fashioned. While domain expertise aids in parameter optimization, this traditional approach becomes increasingly vulnerable to cognitive limitations and anthropogenic biases as the complexity of factors grows. Herein, we introduce an iterative machine learning (ML) framework that integrates active learning to guide targeted experimentation and facilitate incremental model refinement. This method systematically leverages comprehensive experimental observations, including both successful and unsuccessful results, effectively mitigating human-induced biases and alleviating data scarcity. Consequently, it significantly accelerates exploration within the high-dimensional design space. Our results demonstrate that active-learning-driven experimentation markedly reduces the total number of experimental cycles necessary, underscoring the transformative potential of ML-based strategies in expediting battery material optimization.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dim and Small Target Detection for Drone Broadcast Frames Based on Time-Frequency Analysis</title>
<link>https://arxiv.org/abs/2505.18167</link>
<guid>https://arxiv.org/abs/2505.18167</guid>
<content:encoded><![CDATA[
arXiv:2505.18167v1 Announce Type: cross 
Abstract: We propose a dim and small target detection algorithm for drone broadcast frames based on the time-frequency analysis of communication protocol. Specifically, by analyzing modulation parameters and frame structures, the prior knowledge of transmission frequency, signal bandwidth, Zadoff-Chu (ZC) sequences, and frame length of drone broadcast frames is established. The RF signals are processed through the designed filter banks, and the frequency domain parameters of bounding boxes generated by the detector are corrected with transmission frequency and signal bandwidth. Given the remarkable correlation characteristics of ZC sequences, the frequency domain parameters of bounding boxes with low confidence scores are corrected based on ZC sequences and frame length, which improves the detection accuracy of dim targets under low signal-to noise ratio (SNR) situations. Besides, a segmented energy refinement method is applied to mitigate the deviation caused by interference signals with high energy strength, which ulteriorly corrects the time domain detection parameters for dim targets. As the sampling duration increases, the detection speed improves while the detection accuracy of broadcast frames termed as small targets decreases. The trade-off between detection accuracy and speed versus sampling duration is established, which helps to meet different drone regulation requirements. Simulation results demonstrate that the proposed algorithm improves the average intersection over union, precision, and recall by 3\%, 1.4\%, and 2.4\%, respectively, compared to existing algorithms. The proposed algorithm also performs strong robustness under varying flight distances, diverse types of environment noise, and different flight visual environment.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Load Forecasting in the Era of Smart Grids: Opportunities and Advanced Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.18170</link>
<guid>https://arxiv.org/abs/2505.18170</guid>
<content:encoded><![CDATA[
arXiv:2505.18170v1 Announce Type: cross 
Abstract: Electric energy is difficult to store, requiring stricter control over its generation, transmission, and distribution. A persistent challenge in power systems is maintaining real-time equilibrium between electricity demand and supply. Oversupply contributes to resource wastage, while undersupply can strain the grid, increase operational costs, and potentially impact service reliability. To maintain grid stability, load forecasting is needed. Accurate load forecasting balances generation and demand by striving to predict future electricity consumption. This thesis examines and evaluates four machine learning frameworks for short term load forecasting, including gradient boosting decision tree methods such as Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM). A hybrid framework is also developed. In addition, two recurrent neural network architectures, Long Short Term Memory (LSTM) networks and Gated Recurrent Units (GRU), are designed and implemented. Pearson Correlation Coefficient is applied to assess the relationships between electricity demand and exogenous variables. The experimental results show that, for the specific dataset and forecasting task in this study, machine learning-based models achieved improved forecasting performance compared to a classical ARIMA baseline.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenAI Security: Outsmarting the Bots with a Proactive Testing Framework</title>
<link>https://arxiv.org/abs/2505.18172</link>
<guid>https://arxiv.org/abs/2505.18172</guid>
<content:encoded><![CDATA[
arXiv:2505.18172v1 Announce Type: cross 
Abstract: The increasing sophistication and integration of Generative AI (GenAI) models into diverse applications introduce new security challenges that traditional methods struggle to address. This research explores the critical need for proactive security measures to mitigate the risks associated with malicious exploitation of GenAI systems. We present a framework encompassing key approaches, tools, and strategies designed to outmaneuver even advanced adversarial attacks, emphasizing the importance of securing GenAI innovation against potential liabilities. We also empirically prove the effectiveness of the said framework by testing it against the SPML Chatbot Prompt Injection Dataset. This work highlights the shift from reactive to proactive security practices essential for the safe and responsible deployment of GenAI technologies
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NMCSE: Noise-Robust Multi-Modal Coupling Signal Estimation Method via Optimal Transport for Cardiovascular Disease Detection</title>
<link>https://arxiv.org/abs/2505.18174</link>
<guid>https://arxiv.org/abs/2505.18174</guid>
<content:encoded><![CDATA[
arXiv:2505.18174v1 Announce Type: cross 
Abstract: Electrocardiogram (ECG) and Phonocardiogram (PCG) signals are linked by a latent coupling signal representing the electrical-to-mechanical cardiac transformation. While valuable for cardiovascular disease (CVD) detection, this coupling signal is traditionally estimated using deconvolution methods that amplify noise, limiting clinical utility. In this paper, we propose Noise-Robust Multi-Modal Coupling Signal Estimation (NMCSE), which reformulates the problem as distribution matching via optimal transport theory. By jointly optimizing amplitude and temporal alignment, NMCSE mitigates noise amplification without additional preprocessing. Integrated with our Temporal-Spatial Feature Extraction network, NMCSE enables robust multi-modal CVD detection. Experiments on the PhysioNet 2016 dataset with realistic hospital noise demonstrate that NMCSE reduces estimation errors by approximately 30% in Mean Squared Error while maintaining higher Pearson Correlation Coefficients across all tested signal-to-noise ratios. Our approach achieves 97.38% accuracy and 0.98 AUC in CVD detection, outperforming state-of-the-art methods and demonstrating robust performance for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework</title>
<link>https://arxiv.org/abs/2505.18175</link>
<guid>https://arxiv.org/abs/2505.18175</guid>
<content:encoded><![CDATA[
arXiv:2505.18175v1 Announce Type: cross 
Abstract: Electroencephalography-based Emotion Recognition (EEG-ER) has become a growing research area in recent years. Analyzing 216 papers published between 2018 and 2023, we uncover that the field lacks a unified evaluation protocol, which is essential to fairly define the state of the art, compare new approaches and to track the field's progress. We report the main inconsistencies between the used evaluation protocols, which are related to ground truth definition, evaluation metric selection, data splitting types (e.g., subject-dependent or subject-independent) and the use of different datasets. Capitalizing on this state-of-the-art research, we propose a unified evaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which enables an easy and efficient evaluation of new methods and datasets. EEGain is a novel open source software framework, offering the capability to compare - and thus define - state-of-the-art results. EEGain includes standardized methods for data pre-processing, data splitting, evaluation metrics, and the ability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER, MAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In addition, we have assessed and validated EEGain using these six datasets on the four most common publicly available methods (EEGNet, DeepConvNet, ShallowConvNet, TSception). This is a significant step to make research on EEG-ER more reproducible and comparable, thereby accelerating the overall progress of the field.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering scientific publications: lessons learned through experiments with a real citation network</title>
<link>https://arxiv.org/abs/2505.18180</link>
<guid>https://arxiv.org/abs/2505.18180</guid>
<content:encoded><![CDATA[
arXiv:2505.18180v1 Announce Type: cross 
Abstract: Clustering scientific publications can reveal underlying research structures within bibliographic databases. Graph-based clustering methods, such as spectral, Louvain, and Leiden algorithms, are frequently utilized due to their capacity to effectively model citation networks. However, their performance may degrade when applied to real-world data. This study evaluates the performance of these clustering algorithms on a citation graph comprising approx. 700,000 papers and 4.6 million citations extracted from Web of Science. The results show that while scalable methods like Louvain and Leiden perform efficiently, their default settings often yield poor partitioning. Meaningful outcomes require careful parameter tuning, especially for large networks with uneven structures, including a dense core and loosely connected papers. These findings highlight practical lessons about the challenges of large-scale data, method selection and tuning based on specific structures of bibliometric clustering tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Analysis of ECG and PCG Signals for Rheumatic Heart Disease Detection: A Scoping Review (2015-2025)</title>
<link>https://arxiv.org/abs/2505.18182</link>
<guid>https://arxiv.org/abs/2505.18182</guid>
<content:encoded><![CDATA[
arXiv:2505.18182v1 Announce Type: cross 
Abstract: Objective: To conduct a systematic assessment of machine learning applications that utilize electrocardiogram (ECG) and heart sound data in the development of cost-effective detection tools for rheumatic heart disease (RHD) from the year 2015 to 2025, thereby supporting the World Heart Federation's "25 by 25" mortality reduction objective through the creation of alternatives to echocardiography in underserved regions. Methods: Following PRISMA-ScR guidelines, we conducted a comprehensive search across PubMed, IEEE Xplore, Scopus, and Embase for peer-reviewed literature focusing on ML-based ECG/PCG analysis for RHD detection. Two independent reviewers screened studies, and data extraction focused on methodology, validation approaches, and performance metrics. Results: Analysis of 37 relevant studies revealed that convolutional neural networks (CNNs) have become the predominant technology in post-2020 implementations, achieving a median accuracy of 93.7%. However, 73% of studies relied on single-center datasets, only 10.8% incorporated external validation, and none addressed cost-effectiveness. Performance varied markedly across different valvular lesions, and despite 44% of studies originating from endemic regions, significant gaps persisted in implementation science and demographic diversity. Conclusion: While ML-based ECG/PCG analysis shows promise for RHD detection, substantial methodological limitations hinder clinical translation. Future research must prioritize standardized benchmarking frameworks, multimodal architectures, cost-effectiveness assessments, and prospective trials in endemic settings. Significance: This review provides a critical roadmap for developing accessible ML-based RHD screening tools to help bridge the diagnostic gap in resourceconstrained settings where conventional auscultation misses up to 90% of cases and echocardiography remains inaccessible.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAME-C: A knowledge-augmented deep learning pipeline for classifying multi-electrode array electrophysiological signals</title>
<link>https://arxiv.org/abs/2505.18183</link>
<guid>https://arxiv.org/abs/2505.18183</guid>
<content:encoded><![CDATA[
arXiv:2505.18183v1 Announce Type: cross 
Abstract: Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disorder characterized by motor neuron degeneration, with alterations in neural excitability serving as key indicators. Recent advancements in induced pluripotent stem cell (iPSC) technology have enabled the generation of human iPSC-derived neuronal cultures, which, when combined with multi-electrode array (MEA) electrophysiology, provide rich spatial and temporal electrophysiological data. Traditionally, MEA data is analyzed using handcrafted features based on potentially imperfect domain knowledge, which while useful may not fully capture all useful characteristics inherent in the data. Machine learning, particularly deep learning, has the potential to automatically learn relevant characteristics from raw data without solely relying on handcrafted feature extraction. However, handcrafted features remain critical for encoding domain knowledge and improving interpretability, especially with limited or noisy data. This study introduces FRAME-C, a knowledge-augmented machine learning pipeline that combines domain knowledge, raw spike waveform data, and deep learning techniques to classify MEA signals and identify ALS-specific phenotypes. FRAME-C leverages deep learning to learn important features from spike waveforms while incorporating handcrafted features such as spike amplitude, inter-spike interval, and spike duration, preserving key spatial and temporal information. We validate FRAME-C on both simulated and real MEA data from human iPSC-derived neuronal cultures, demonstrating superior performance over existing classification methods. FRAME-C shows over 11% improvement on real data and up to 25% on simulated data. We also show FRAME-C can evaluate handcrafted feature importance, providing insights into ALS phenotypes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals</title>
<link>https://arxiv.org/abs/2505.18185</link>
<guid>https://arxiv.org/abs/2505.18185</guid>
<content:encoded><![CDATA[
arXiv:2505.18185v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural activity non-invasively by capturing electromagnetic fields generated by dendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit distinct signal patterns, further complicated by variations in sensor configurations across modalities and recording devices. Existing approaches typically rely on separate, modality- and dataset-specific models, which limits the performance and cross-domain scalability. This paper proposes BrainOmni, the first brain foundation model that generalises across heterogeneous EEG and MEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the first tokenizer that quantises spatiotemporal brain activity into discrete representations. Central to BrainTokenizer is a novel Sensor Encoder that encodes sensor properties such as spatial layout, orientation, and type, enabling compatibility across devices and modalities. Building upon the discrete representations, BrainOmni learns unified semantic embeddings of brain signals by self-supervised pretraining. To the best of our knowledge, it is the first foundation model to support both EEG and MEG signals, as well as the first to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG and 656 hours of MEG data are curated and standardised from publicly available sources for pretraining. Experiments show that BrainOmni outperforms both existing foundation models and state-of-the-art task-specific models on a range of downstream tasks. It also demonstrates strong generalisation to unseen EEG and MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training yields consistent improvements across both modalities. Code and model checkpoints will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Interpretable Concepts in Large Generative Music Models</title>
<link>https://arxiv.org/abs/2505.18186</link>
<guid>https://arxiv.org/abs/2505.18186</guid>
<content:encoded><![CDATA[
arXiv:2505.18186v1 Announce Type: cross 
Abstract: The fidelity with which neural networks can now generate content such as music presents a scientific opportunity: these systems appear to have learned implicit theories of the structure of such content through statistical learning alone. This could offer a novel lens on theories of human-generated media. Where these representations align with traditional constructs (e.g. chord progressions in music), they demonstrate how these can be inferred from statistical regularities. Where they diverge, they highlight potential limits in our theoretical frameworks -- patterns that we may have overlooked but that nonetheless hold significant explanatory power. In this paper, we focus on the specific case of music generators. We introduce a method to discover musical concepts using sparse autoencoders (SAEs), extracting interpretable features from the residual stream activations of a transformer model. We evaluate this approach by extracting a large set of features and producing an automatic labeling and evaluation pipeline for them. Our results reveal both familiar musical concepts and counterintuitive patterns that lack clear counterparts in existing theories or natural language altogether. Beyond improving model transparency, our work provides a new empirical tool that might help discover organizing principles in ways that have eluded traditional methods of analysis and synthesis.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generative Inverse Design of Rectangular Patch Antennas with Test Time Optimization</title>
<link>https://arxiv.org/abs/2505.18188</link>
<guid>https://arxiv.org/abs/2505.18188</guid>
<content:encoded><![CDATA[
arXiv:2505.18188v1 Announce Type: cross 
Abstract: We propose a two-stage deep learning framework for the inverse design of rectangular patch antennas. Our approach leverages generative modeling to learn a latent representation of antenna frequency response curves and conditions a subsequent generative model on these responses to produce feasible antenna geometries. We further demonstrate that leveraging search and optimization techniques at test-time improves the accuracy of the generated designs and enables consideration of auxiliary objectives such as manufacturability. Our approach generalizes naturally to different design criteria, and can be easily adapted to more complex geometric design spaces.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Realistic Multi-Beat ECG Signals</title>
<link>https://arxiv.org/abs/2505.18189</link>
<guid>https://arxiv.org/abs/2505.18189</guid>
<content:encoded><![CDATA[
arXiv:2505.18189v1 Announce Type: cross 
Abstract: Generating synthetic ECG data has numerous applications in healthcare, from educational purposes to simulating scenarios and forecasting trends. While recent diffusion models excel at generating short ECG segments, they struggle with longer sequences needed for many clinical applications. This paper proposes a novel three-layer synthesis framework for generating realistic long-form ECG signals. We first generate high-fidelity single beats using a diffusion model, then synthesize inter-beat features preserving critical temporal dependencies, and finally assemble beats into coherent long sequences using feature-guided matching. Our comprehensive evaluation demonstrates that the resulting synthetic ECGs maintain both beat-level morphological fidelity and clinically relevant inter-beat relationships. In arrhythmia classification tasks, our long-form synthetic ECGs significantly outperform end-to-end long-form ECG generation using the diffusion model, highlighting their potential for increasing utility for downstream applications. The approach enables generation of unprecedented multi-minute ECG sequences while preserving essential diagnostic characteristics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhySense: Sensor Placement Optimization for Accurate Physics Sensing</title>
<link>https://arxiv.org/abs/2505.18190</link>
<guid>https://arxiv.org/abs/2505.18190</guid>
<content:encoded><![CDATA[
arXiv:2505.18190v1 Announce Type: cross 
Abstract: Physics sensing plays a central role in many scientific and engineering domains, which inherently involves two coupled tasks: reconstructing dense physical fields from sparse observations and optimizing scattered sensor placements to observe maximum information. While deep learning has made rapid advances in sparse-data reconstruction, existing methods generally omit optimization of sensor placements, leaving the mutual enhancement between reconstruction and placement on the shelf. To change this suboptimal practice, we propose PhySense, a synergistic two-stage framework that learns to jointly reconstruct physical fields and to optimize sensor placements, both aiming for accurate physics sensing. The first stage involves a flow-based generative model enhanced by cross-attention to adaptively fuse sparse observations. \correct{Leveraging the reconstruction feedback, }the second stage performs sensor placement via projected gradient descent to satisfy spatial constraints. \correct{We further prove that the learning objectives of the two stages are consistent with classical variance-minimization principles, providing theoretical guarantees.} Extensive experiments across three challenging benchmarks, especially a 3D geometry dataset, indicate PhySense achieves state-of-the-art physics sensing accuracy and discovers informative sensor placements previously unconsidered.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SzCORE as a benchmark: report from the seizure detection challenge at the 2025 AI in Epilepsy and Neurological Disorders Conference</title>
<link>https://arxiv.org/abs/2505.18191</link>
<guid>https://arxiv.org/abs/2505.18191</guid>
<content:encoded><![CDATA[
arXiv:2505.18191v1 Announce Type: cross 
Abstract: Reliable automatic seizure detection from long-term EEG remains a challenge, as current machine learning models often fail to generalize across patients or clinical settings. Manual EEG review remains the clinical standard, underscoring the need for robust models and standardized evaluation. To rigorously assess algorithm performance, we organized a challenge using a private dataset of continuous EEG recordings from 65 subjects (4,360 hours). Expert neurophysiologists annotated the data, providing ground truth for seizure events. Participants were required to detect seizure onset and duration, with evaluation based on event-based metrics, including sensitivity, precision, F1-score, and false positives per day. The SzCORE framework ensured standardized evaluation. The primary ranking criterion was the event-based F1-score, reflecting clinical relevance by balancing sensitivity and false positives. The challenge received 30 submissions from 19 teams, with 28 algorithms evaluated. Results revealed wide variability in performance, with a top F1-score of 43% (sensitivity 37%, precision 45%), highlighting the ongoing difficulty of seizure detection. The challenge also revealed a gap between reported performance and real-world evaluation, emphasizing the importance of rigorous benchmarking. Compared to previous challenges and commercial systems, the best-performing algorithm in this contest showed improved performance. Importantly, the challenge platform now supports continuous benchmarking, enabling reproducible research, integration of new datasets, and clinical evaluation of seizure detection algorithms using a standardized framework.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustic and Machine Learning Methods for Speech-Based Suicide Risk Assessment: A Systematic Review</title>
<link>https://arxiv.org/abs/2505.18195</link>
<guid>https://arxiv.org/abs/2505.18195</guid>
<content:encoded><![CDATA[
arXiv:2505.18195v1 Announce Type: cross 
Abstract: Suicide remains a public health challenge, necessitating improved detection methods to facilitate timely intervention and treatment. This systematic review evaluates the role of Artificial Intelligence (AI) and Machine Learning (ML) in assessing suicide risk through acoustic analysis of speech. Following PRISMA guidelines, we analyzed 33 articles selected from PubMed, Cochrane, Scopus, and Web of Science databases. These studies primarily explored acoustic differences between individuals at risk of suicide (RS) and those not at risk (NRS), and evaluated ML classifier performance. Findings consistently showed significant acoustic feature variations between RS and NRS populations, particularly involving jitter, fundamental frequency (F0), Mel-frequency cepstral coefficients (MFCC), and power spectral density (PSD). Classifier effectiveness varied based on algorithms, modalities, and speech elicitation methods, with multimodal approaches integrating acoustic, linguistic, and metadata features demonstrating superior performance. However, limitations such as methodological variability, small sample sizes, lack of longitudinal data, and limited linguistic and demographic diversity restrict generalizability. Future research should focus on standardizing methods, expanding multimodal analyses, and utilizing larger, diverse datasets to support AI integration in clinical suicide risk assessment.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossRF: A Domain-Invariant Deep Learning Approach for RF Fingerprinting</title>
<link>https://arxiv.org/abs/2505.18200</link>
<guid>https://arxiv.org/abs/2505.18200</guid>
<content:encoded><![CDATA[
arXiv:2505.18200v1 Announce Type: cross 
Abstract: Radio Frequency (RF) fingerprinting offers a promising approach for drone identification and security, although it suffers from significant performance degradation when operating on different transmission channels. This paper presents CrossRF, a domain-invariant deep learning approach that addresses the problem of cross-channel RF fingerprinting for Unmanned Aerial Vehicle (UAV) identification. Our approach aims to minimize the domain gap between different RF channels by using adversarial learning to train a more robust model that maintains consistent identification performance despite channel variations. We validate our approach using the UAVSig dataset, comprising real-world over-the-air RF signals from identical drone models operating across several frequency channels, ensuring that the findings correspond to real-world scenarios. The experimental results show CrossRF's efficiency, achieving up to 99.03% accuracy when adapting from Channel 3 to Channel 4, compared to only 26.39% using conventional methods. The model maintains robust performance in more difficult multi-channel scenarios (87.57% accuracy adapting from Channels 1,3 to 2,4) and achieves 89.45% accuracy with 0.9 precision for controller classification. These results confirm CrossRF's ability to significantly reduce performance degradation due to cross-channel variations while maintaining high identification accuracy with minimal training data requirements, making it particularly suitable for practical drone security applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Twinning for Hybrid Control of Flapping-Wing Drones</title>
<link>https://arxiv.org/abs/2505.18201</link>
<guid>https://arxiv.org/abs/2505.18201</guid>
<content:encoded><![CDATA[
arXiv:2505.18201v1 Announce Type: cross 
Abstract: Controlling the flight of flapping-wing drones requires versatile controllers that handle their time-varying, nonlinear, and underactuated dynamics from incomplete and noisy sensor data. Model-based methods struggle with accurate modeling, while model-free approaches falter in efficiently navigating very high-dimensional and nonlinear control objective landscapes. This article presents a novel hybrid model-free/model-based approach to flight control based on the recently proposed reinforcement twinning algorithm. The model-based (MB) approach relies on an adjoint formulation using an adaptive digital twin, continuously identified from live trajectories, while the model-free (MF) approach relies on reinforcement learning. The two agents collaborate through transfer learning, imitation learning, and experience sharing using the real environment, the digital twin and a referee. The latter selects the best agent to interact with the real environment based on performance within the digital twin and a real-to-virtual environment consistency ratio. The algorithm is evaluated for controlling the longitudinal dynamics of a flapping-wing drone, with the environment simulated as a nonlinear, time-varying dynamical system under the influence of quasi-steady aerodynamic forces. The hybrid control learning approach is tested with three types of initialization of the adaptive model: (1) offline identification using previously available data, (2) random initialization with full online identification, and (3) offline pre-training with an estimation bias, followed by online adaptation. In all three scenarios, the proposed hybrid learning approach demonstrates superior performance compared to purely model-free and model-based methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text</title>
<link>https://arxiv.org/abs/2505.18207</link>
<guid>https://arxiv.org/abs/2505.18207</guid>
<content:encoded><![CDATA[
arXiv:2505.18207v1 Announce Type: cross 
Abstract: In scientific research, limitations refer to the shortcomings, constraints, or weaknesses within a study. Transparent reporting of such limitations can enhance the quality and reproducibility of research and improve public trust in science. However, authors often a) underreport them in the paper text and b) use hedging strategies to satisfy editorial requirements at the cost of readers' clarity and confidence. This underreporting behavior, along with an explosion in the number of publications, has created a pressing need to automatically extract or generate such limitations from scholarly papers. In this direction, we present a complete architecture for the computational analysis of research limitations. Specifically, we create a dataset of limitations in ACL, NeurIPS, and PeerJ papers by extracting them from papers' text and integrating them with external reviews; we propose methods to automatically generate them using a novel Retrieval Augmented Generation (RAG) technique; we create a fine-grained evaluation framework for generated limitations; and we provide a meta-evaluation for the proposed evaluation techniques.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&amp;A Without Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.18247</link>
<guid>https://arxiv.org/abs/2505.18247</guid>
<content:encoded><![CDATA[
arXiv:2505.18247v1 Announce Type: cross 
Abstract: Despite the widespread exploration of Retrieval-Augmented Generation (RAG), its deployment in enterprises for domain-specific datasets remains limited due to poor answer accuracy. These corpora, often shielded behind firewalls in private enterprise knowledge bases, having complex, domain-specific terminology, rarely seen by LLMs during pre-training; exhibit significant semantic variability across domains (like networking, military, or legal, etc.), or even within a single domain like medicine, and thus result in poor context precision for RAG systems. Currently, in such situations, fine-tuning or RAG with fine-tuning is attempted, but these approaches are slow, expensive, and lack generalization for accuracy as the new domain-specific data emerges. We propose an approach for Enterprise Search that focuses on enhancing the retriever for a domain-specific corpus through hybrid query indexes and metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata generation pipeline using key concepts, topics, and acronyms, and then creates a metadata-enriched hybrid index with boosted search queries. This approach avoids overfitting and generalizes effectively across domains. On the PubMedQA benchmark for the biomedical domain, the proposed method achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results without fine-tuning and sets a new benchmark for zero-shot results while outperforming much larger models like GPT3.5. The results are even comparable to the best fine-tuned models on this dataset, and we further demonstrate the robustness and scalability of the approach by evaluating it on other Q&amp;A datasets like SQuAD, NQ etc.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preconditioned Langevin Dynamics with Score-Based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems</title>
<link>https://arxiv.org/abs/2505.18276</link>
<guid>https://arxiv.org/abs/2505.18276</guid>
<content:encoded><![CDATA[
arXiv:2505.18276v1 Announce Type: cross 
Abstract: Designing algorithms for solving high-dimensional Bayesian inverse problems directly in infinite-dimensional function spaces - where such problems are naturally formulated - is crucial to ensure stability and convergence as the discretization of the underlying problem is refined. In this paper, we contribute to this line of work by analyzing a widely used sampler for linear inverse problems: Langevin dynamics driven by score-based generative models (SGMs) acting as priors, formulated directly in function space. Building on the theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition of this sampler in the infinite-dimensional setting and derive, for the first time, error estimates that explicitly depend on the approximation error of the score. As a consequence, we obtain sufficient conditions for global convergence in Kullback-Leibler divergence on the underlying function space. Preventing numerical instabilities requires preconditioning of the Langevin algorithm and we prove the existence and the form of an optimal preconditioner. The preconditioner depends on both the score error and the forward operator and guarantees a uniform convergence rate across all posterior modes. Our analysis applies to both Gaussian and a general class of non-Gaussian priors. Finally, we present examples that illustrate and validate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control</title>
<link>https://arxiv.org/abs/2505.18279</link>
<guid>https://arxiv.org/abs/2505.18279</guid>
<content:encoded><![CDATA[
arXiv:2505.18279v1 Announce Type: cross 
Abstract: Complex tasks are increasingly delegated to ensembles of specialized LLM-based agents that reason, communicate, and coordinate actions-both among themselves and through interactions with external tools, APIs, and databases. While persistent memory has been shown to enhance single-agent performance, most approaches assume a monolithic, single-user context-overlooking the benefits and challenges of knowledge transfer across users under dynamic, asymmetric permissions. We introduce Collaborative Memory, a framework for multi-user, multi-agent environments with asymmetric, time-evolving access controls encoded as bipartite graphs linking users, agents, and resources. Our system maintains two memory tiers: (1) private memory-private fragments visible only to their originating user; and (2) shared memory-selectively shared fragments. Each fragment carries immutable provenance attributes (contributing agents, accessed resources, and timestamps) to support retrospective permission checks. Granular read policies enforce current user-agent-resource constraints and project existing memory fragments into filtered transformed views. Write policies determine fragment retention and sharing, applying context-aware transformations to update the memory. Both policies may be designed conditioned on system, agent, and user-level information. Our framework enables safe, efficient, and interpretable cross-user knowledge sharing, with provable adherence to asymmetric, time-varying policies and full auditability of memory operations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-agent or Multi-agent Systems? Why Not Both?</title>
<link>https://arxiv.org/abs/2505.18286</link>
<guid>https://arxiv.org/abs/2505.18286</guid>
<content:encoded><![CDATA[
arXiv:2505.18286v1 Announce Type: cross 
Abstract: Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to different large language model (LLM) agents and tools. Prior studies have reported the superior accuracy performance of MAS across diverse domains, enabled by long-horizon context tracking and error correction through role-specific agents. However, the design and deployment of MAS incur higher complexity and runtime cost compared to single-agent systems (SAS). Meanwhile, frontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in long-context reasoning, memory retention, and tool usage, mitigating many limitations that originally motivated MAS designs. In this paper, we conduct an extensive empirical study comparing MAS and SAS across various popular agentic applications. We find that the benefits of MAS over SAS diminish as LLM capabilities improve, and we propose efficient mechanisms to pinpoint the error-prone agent in MAS. Furthermore, the performance discrepancy between MAS and SAS motivates our design of a hybrid agentic paradigm, request cascading between MAS and SAS, to improve both efficiency and capability. Our design improves accuracy by 1.1-12% while reducing deployment costs by up to 20% across various agentic applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operator Learning for Schr\"{o}dinger Equation: Unitarity, Error Bounds, and Time Generalization</title>
<link>https://arxiv.org/abs/2505.18288</link>
<guid>https://arxiv.org/abs/2505.18288</guid>
<content:encoded><![CDATA[
arXiv:2505.18288v1 Announce Type: cross 
Abstract: We consider the problem of learning the evolution operator for the time-dependent Schr\"{o}dinger equation, where the Hamiltonian may vary with time. Existing neural network-based surrogates often ignore fundamental properties of the Schr\"{o}dinger equation, such as linearity and unitarity, and lack theoretical guarantees on prediction error or time generalization. To address this, we introduce a linear estimator for the evolution operator that preserves a weak form of unitarity. We establish both upper and lower bounds on the prediction error that hold uniformly over all sufficiently smooth initial wave functions. Additionally, we derive time generalization bounds that quantify how the estimator extrapolates beyond the time points seen during training. Experiments across real-world Hamiltonians -- including hydrogen atoms, ion traps for qubit design, and optical lattices -- show that our estimator achieves relative errors $10^{-2}$ to $10^{-3}$ times smaller than state-of-the-art methods such as the Fourier Neural Operator and DeepONet.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A deep solver for backward stochastic Volterra integral equations</title>
<link>https://arxiv.org/abs/2505.18297</link>
<guid>https://arxiv.org/abs/2505.18297</guid>
<content:encoded><![CDATA[
arXiv:2505.18297v1 Announce Type: cross 
Abstract: We present the first deep-learning solver for backward stochastic Volterra integral equations (BSVIEs) and their fully-coupled forward-backward variants. The method trains a neural network to approximate the two solution fields in a single stage, avoiding the use of nested time-stepping cycles that limit classical algorithms. For the decoupled case we prove a non-asymptotic error bound composed of an a posteriori residual plus the familiar square root dependence on the time step. Numerical experiments confirm this rate and reveal two key properties: \emph{scalability}, in the sense that accuracy remains stable from low dimension up to 500 spatial variables while GPU batching keeps wall-clock time nearly constant; and \emph{generality}, since the same method handles coupled systems whose forward dynamics depend on the backward solution. These results open practical access to a family of high-dimensional, path-dependent problems in stochastic control and quantitative finance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation</title>
<link>https://arxiv.org/abs/2505.18323</link>
<guid>https://arxiv.org/abs/2505.18323</guid>
<content:encoded><![CDATA[
arXiv:2505.18323v1 Announce Type: cross 
Abstract: For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce a novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, a common technique for hardware utilization, enabling large-scale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, and represent a truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose a deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs a novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform a large scale analysis of models hosted through Hugging Face and find over 200 models that introduce (unintended) information leakage between batch entries due to the use of dynamic quantization.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary</title>
<link>https://arxiv.org/abs/2505.18325</link>
<guid>https://arxiv.org/abs/2505.18325</guid>
<content:encoded><![CDATA[
arXiv:2505.18325v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet they often refuse to answer legitimate queries-a phenomenon known as overrefusal. Overrefusal typically stems from over-conservative safety alignment, causing models to treat many reasonable prompts as potentially risky. To systematically understand this issue, we probe and leverage the models'safety decision boundaries to analyze and mitigate overrefusal. Our findings reveal that overrefusal is closely tied to misalignment at these boundary regions, where models struggle to distinguish subtle differences between benign and harmful content. Building on these insights, we present RASS, an automated framework for prompt generation and selection that strategically targets overrefusal prompts near the safety boundary. By harnessing steering vectors in the representation space, RASS efficiently identifies and curates boundary-aligned prompts, enabling more effective and targeted mitigation of overrefusal. This approach not only provides a more precise and interpretable view of model safety decisions but also seamlessly extends to multilingual scenarios.We have explored the safety decision boundaries of various LLMs and construct the MORBench evaluation set to facilitate robust assessment of model safety and helpfulness across multiple languages. Code and datasets will be released at https://anonymous.4open.science/r/RASS-80D3.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Statistical Inference of Constrained Stochastic Optimization via Random Scaling</title>
<link>https://arxiv.org/abs/2505.18327</link>
<guid>https://arxiv.org/abs/2505.18327</guid>
<content:encoded><![CDATA[
arXiv:2505.18327v1 Announce Type: cross 
Abstract: Constrained stochastic nonlinear optimization problems have attracted significant attention for their ability to model complex real-world scenarios in physics, economics, and biology. As datasets continue to grow, online inference methods have become crucial for enabling real-time decision-making without the need to store historical data. In this work, we develop an online inference procedure for constrained stochastic optimization by leveraging a method called Sketched Stochastic Sequential Quadratic Programming (SSQP). As a direct generalization of sketched Newton methods, SSQP approximates the objective with a quadratic model and the constraints with a linear model at each step, then applies a sketching solver to inexactly solve the resulting subproblem. Building on this design, we propose a new online inference procedure called random scaling. In particular, we construct a test statistic based on SSQP iterates whose limiting distribution is free of any unknown parameters. Compared to existing online inference procedures, our approach offers two key advantages: (i) it enables the construction of asymptotically valid confidence intervals; and (ii) it is matrix-free, i.e. the computation involves only primal-dual SSQP iterates $(\boldsymbol{x}_t, \boldsymbol{\lambda}_t)$ without requiring any matrix inversions. We validate our theory through numerical experiments on nonlinearly constrained regression problems and demonstrate the superior performance of our random scaling method over existing inference procedures.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs</title>
<link>https://arxiv.org/abs/2505.18332</link>
<guid>https://arxiv.org/abs/2505.18332</guid>
<content:encoded><![CDATA[
arXiv:2505.18332v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have led to the widespread adoption of third-party inference services, raising critical privacy concerns. Existing methods of performing private third-party inference, such as Secure Multiparty Computation (SMPC), often rely on cryptographic methods. However, these methods are thousands of times slower than standard unencrypted inference, and fail to scale to large modern LLMs. Therefore, recent lines of work have explored the replacement of expensive encrypted nonlinear computations in SMPC with statistical obfuscation methods - in particular, revealing permuted hidden states to the third parties, with accompanying strong claims of the difficulty of reversal into the unpermuted states. In this work, we begin by introducing a novel reconstruction technique that can recover original prompts from hidden states with nearly perfect accuracy across multiple state-of-the-art LLMs. We then show that extensions of our attack are nearly perfectly effective in reversing permuted hidden states of LLMs, demonstrating the insecurity of three recently proposed privacy schemes. We further dissect the shortcomings of prior theoretical `proofs' of permuation security which allow our attack to succeed. Our findings highlight the importance of rigorous security analysis in privacy-preserving LLM inference.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance</title>
<link>https://arxiv.org/abs/2505.18342</link>
<guid>https://arxiv.org/abs/2505.18342</guid>
<content:encoded><![CDATA[
arXiv:2505.18342v1 Announce Type: cross 
Abstract: Accurate and scalable quantification of animal pose and appearance is crucial for studying behavior. Current 3D pose estimation techniques, such as keypoint- and mesh-based techniques, often face challenges including limited representational detail, labor-intensive annotation requirements, and expensive per-frame optimization. These limitations hinder the study of subtle movements and can make large-scale analyses impractical. We propose Pose Splatter, a novel framework leveraging shape carving and 3D Gaussian splatting to model the complete pose and appearance of laboratory animals without prior knowledge of animal geometry, per-frame optimization, or manual annotations. We also propose a novel rotation-invariant visual embedding technique for encoding pose and appearance, designed to be a plug-in replacement for 3D keypoint data in downstream behavioral analyses. Experiments on datasets of mice, rats, and zebra finches show Pose Splatter learns accurate 3D animal geometries. Notably, Pose Splatter represents subtle variations in pose, provides better low-dimensional pose embeddings over state-of-the-art as evaluated by humans, and generalizes to unseen data. By eliminating annotation and per-frame optimization bottlenecks, Pose Splatter enables analysis of large-scale, longitudinal behavior needed to map genotype, neural activity, and micro-behavior at unprecedented resolution.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2505.18346</link>
<guid>https://arxiv.org/abs/2505.18346</guid>
<content:encoded><![CDATA[
arXiv:2505.18346v1 Announce Type: cross 
Abstract: Weak-to-strong generalization, where a student model trained on imperfect labels generated by a weaker teacher nonetheless surpasses that teacher, has been widely observed but the mechanisms that enable it have remained poorly understood. In this paper, through a theoretical analysis of simple models, we uncover three core mechanisms that can drive this phenomenon. First, by analyzing ridge regression, we study the interplay between the teacher and student regularization and prove that a student can compensate for a teacher's under-regularization and achieve lower test error. We also analyze the role of the parameterization regime of the models. Second, by analyzing weighted ridge regression, we show that a student model with a regularization structure more aligned to the target, can outperform its teacher. Third, in a nonlinear multi-index setting, we demonstrate that a student can learn easy, task-specific features from the teacher while leveraging its own broader pre-training to learn hard-to-learn features that the teacher cannot capture.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs</title>
<link>https://arxiv.org/abs/2505.18356</link>
<guid>https://arxiv.org/abs/2505.18356</guid>
<content:encoded><![CDATA[
arXiv:2505.18356v1 Announce Type: cross 
Abstract: Large language models (LLMs) still struggle across tasks outside of high-resource languages. In this work, we investigate cross-lingual transfer to lower-resource languages where task-specific post-training data is scarce. Building on prior work, we first validate that the subsets of model parameters that matter most for mathematical reasoning and multilingual capabilities are distinctly non-overlapping. To exploit this implicit separability between task and target language parameterization, we develop and analyze numerous modular frameworks to improve the composition of the two during fine-tuning. These methods generally employ freezing parameters or post hoc model merging to assign math and language improvement to different key parts of the LLM. In the absence of in-language math data, we demonstrate that the modular approaches successfully improve upon baselines across three languages, four models, and two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most consistently successful modular method to be fine-tuning separate language and math experts and model merging via Layer-Swapping, somewhat surprisingly. We offer possible explanations for this result via recent works on the linearity of task vectors. We further explain this by empirically showing that reverting less useful fine-tuning updates after training often outperforms freezing them from the start.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</title>
<link>https://arxiv.org/abs/2505.18361</link>
<guid>https://arxiv.org/abs/2505.18361</guid>
<content:encoded><![CDATA[
arXiv:2505.18361v1 Announce Type: cross 
Abstract: Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions</title>
<link>https://arxiv.org/abs/2505.18362</link>
<guid>https://arxiv.org/abs/2505.18362</guid>
<content:encoded><![CDATA[
arXiv:2505.18362v1 Announce Type: cross 
Abstract: We develop a general theoretical framework for optimal probability density control and propose a numerical algorithm that is scalable to solve the control problem in high dimensions. Specifically, we establish the Pontryagin Maximum Principle (PMP) for optimal density control and construct the Hamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous derivations without any concept from Wasserstein theory. To solve the density control problem numerically, we propose to use reduced-order models, such as deep neural networks (DNNs), to parameterize the control vector-field and the adjoint function, which allows us to tackle problems defined on high-dimensional state spaces. We also prove several convergence properties of the proposed algorithm. Numerical results demonstrate promising performances of our algorithm on a variety of density control problems with obstacles and nonlinear interaction challenges in high dimensions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems</title>
<link>https://arxiv.org/abs/2505.18366</link>
<guid>https://arxiv.org/abs/2505.18366</guid>
<content:encoded><![CDATA[
arXiv:2505.18366v1 Announce Type: cross 
Abstract: Enterprise search systems often struggle to retrieve accurate, domain-specific information due to semantic mismatches and overlapping terminologies. These issues can degrade the performance of downstream applications such as knowledge management, customer support, and retrieval-augmented generation agents. To address this challenge, we propose a scalable hard-negative mining framework tailored specifically for domain-specific enterprise data. Our approach dynamically selects semantically challenging but contextually irrelevant documents to enhance deployed re-ranking models.
  Our method integrates diverse embedding models, performs dimensionality reduction, and uniquely selects hard negatives, ensuring computational efficiency and semantic precision. Evaluation on our proprietary enterprise corpus (cloud services domain) demonstrates substantial improvements of 15\% in MRR@3 and 19\% in MRR@10 compared to state-of-the-art baselines and other negative sampling techniques. Further validation on public domain-specific datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability and readiness for real-world applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation</title>
<link>https://arxiv.org/abs/2505.18374</link>
<guid>https://arxiv.org/abs/2505.18374</guid>
<content:encoded><![CDATA[
arXiv:2505.18374v1 Announce Type: cross 
Abstract: Command-line interfaces (CLIs) provide structured textual environments for system administration. Explorations have been performed using pre-trained language models (PLMs) to simulate these environments for safe interaction in high-risk environments. However, their use has been constrained to frozen, large parameter models like GPT. For smaller architectures to reach a similar level of believability, a rich dataset of CLI interactions is required. Existing public datasets focus on mapping natural-language tasks to commands, omitting crucial execution data such as exit codes, outputs, and environmental side effects, limiting their usability for behavioral modeling. We introduce a Shell Input -Output Environment (ShIOEnv), which casts command construction as a Markov Decision Process whose state is the partially built sequence and whose actions append arguments. After each action, ShIOEnv executes the candidate and returns its exit status, output, and progress toward a minimal-length behavioral objective. Due to the intractable nature of the combinatorial argument state-action space, we derive a context-free grammar from man pages to mask invalid arguments from being emitted. We explore random and proximal-policy optimization (PPO)-optimized sampling of unrestricted and grammar-masked action spaces to produce four exploration strategies. We observed that grammar masking and PPO significantly improve sample efficiency to produce a higher quality dataset (maximizing the number of arguments while minimizing redundancies). Policy-generated datasets of shell input-output behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements in BLEU-4 when constraining the action space to grammar productions with an additional 26% improvement when applying PPO. The ShIOEnv environment and curated command behavior datasets are released for use in future research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training</title>
<link>https://arxiv.org/abs/2505.18377</link>
<guid>https://arxiv.org/abs/2505.18377</guid>
<content:encoded><![CDATA[
arXiv:2505.18377v1 Announce Type: cross 
Abstract: DONNs harness the physics of light propagation for efficient analog computation, with applications in AI and signal processing. Advances in nanophotonic fabrication and metasurface-based wavefront engineering have opened new pathways to realize high-capacity DONNs across various spectral regimes. Training such DONN systems to determine the metasurface structures remains challenging. Heuristic methods are fast but oversimplify metasurfaces modulation, often resulting in physically unrealizable designs and significant performance degradation. Simulation-in-the-loop training methods directly optimize a physically implementable metasurface using adjoint methods during end-to-end DONN training, but are inherently computationally prohibitive and unscalable.To address these limitations, we propose SP2RINT, a spatially decoupled, progressive training framework that formulates DONN training as a PDE-constrained learning problem. Metasurface responses are first relaxed into freely trainable transfer matrices with a banded structure. We then progressively enforce physical constraints by alternating between transfer matrix training and adjoint-based inverse design, avoiding per-iteration PDE solves while ensuring final physical realizability. To further reduce runtime, we introduce a physics-inspired, spatially decoupled inverse design strategy based on the natural locality of field interactions. This approach partitions the metasurface into independently solvable patches, enabling scalable and parallel inverse design with system-level calibration. Evaluated across diverse DONN training tasks, SP2RINT achieves digital-comparable accuracy while being 1825 times faster than simulation-in-the-loop approaches. By bridging the gap between abstract DONN models and implementable photonic hardware, SP2RINT enables scalable, high-performance training of physically realizable meta-optical neural systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification</title>
<link>https://arxiv.org/abs/2505.18380</link>
<guid>https://arxiv.org/abs/2505.18380</guid>
<content:encoded><![CDATA[
arXiv:2505.18380v1 Announce Type: cross 
Abstract: Ensuring clinical data privacy while preserving utility is critical for AI-driven healthcare and data analytics. Existing de-identification (De-ID) methods, including rule-based techniques, deep learning models, and large language models (LLMs), often suffer from recall errors, limited generalization, and inefficiencies, limiting their real-world applicability. We propose a fully automated, multi-modal framework, RedactOR for de-identifying structured and unstructured electronic health records, including clinical audio records. Our framework employs cost-efficient De-ID strategies, including intelligent routing, hybrid rule and LLM based approaches, and a two-step audio redaction approach. We present a retrieval-based entity relexicalization approach to ensure consistent substitutions of protected entities, thereby enhancing data coherence for downstream applications. We discuss key design desiderata, de-identification and relexicalization methodology, and modular architecture of RedactX and its integration with the Oracle Health Clinical AI system. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with strict recall, our approach achieves competitive performance while optimizing token usage to reduce LLM costs. Finally, we discuss key lessons and insights from deployment in real-world AI- driven healthcare data pipelines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems</title>
<link>https://arxiv.org/abs/2505.18397</link>
<guid>https://arxiv.org/abs/2505.18397</guid>
<content:encoded><![CDATA[
arXiv:2505.18397v1 Announce Type: cross 
Abstract: Multi-agent AI systems (MAS) offer a promising framework for distributed intelligence, enabling collaborative reasoning, planning, and decision-making across autonomous agents. This paper provides a systematic outlook on the current opportunities and challenges of MAS, drawing insights from recent advances in large language models (LLMs), federated optimization, and human-AI interaction. We formalize key concepts including agent topology, coordination protocols, and shared objectives, and identify major risks such as dependency, misalignment, and vulnerabilities arising from training data overlap. Through a biologically inspired simulation and comprehensive theoretical framing, we highlight critical pathways for developing robust, scalable, and secure MAS in real-world settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Diffusion for Dataset Distillation with High Representativeness</title>
<link>https://arxiv.org/abs/2505.18399</link>
<guid>https://arxiv.org/abs/2505.18399</guid>
<content:encoded><![CDATA[
arXiv:2505.18399v1 Announce Type: cross 
Abstract: Recent deep learning models demand larger datasets, driving the need for dataset distillation to create compact, cost-efficient datasets while maintaining performance. Due to the powerful image generation capability of diffusion, it has been introduced to this field for generating distilled images. In this paper, we systematically investigate issues present in current diffusion-based dataset distillation methods, including inaccurate distribution matching, distribution deviation with random noise, and separate sampling. Building on this, we propose D^3HR, a novel diffusion-based framework to generate distilled datasets with high representativeness. Specifically, we adopt DDIM inversion to map the latents of the full dataset from a low-normality latent domain to a high-normality Gaussian domain, preserving information and ensuring structural consistency to generate representative latents for the distilled dataset. Furthermore, we propose an efficient sampling scheme to better align the representative latents with the high-normality Gaussian distribution. Our comprehensive experiments demonstrate that D^3HR can achieve higher accuracy across different model architectures compared with state-of-the-art baselines in dataset distillation. Source code: https://github.com/lin-zhao-resoLve/D3HR.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifiability of latent causal graphical models without pure children</title>
<link>https://arxiv.org/abs/2505.18410</link>
<guid>https://arxiv.org/abs/2505.18410</guid>
<content:encoded><![CDATA[
arXiv:2505.18410v1 Announce Type: cross 
Abstract: This paper considers a challenging problem of identifying a causal graphical model under the presence of latent variables. While various identifiability conditions have been proposed in the literature, they often require multiple pure children per latent variable or restrictions on the latent causal graph. Furthermore, it is common for all observed variables to exhibit the same modality. Consequently, the existing identifiability conditions are often too stringent for complex real-world data. We consider a general nonparametric measurement model with arbitrary observed variable types and binary latent variables, and propose a double triangular graphical condition that guarantees identifiability of the entire causal graphical model. The proposed condition significantly relaxes the popular pure children condition. We also establish necessary conditions for identifiability and provide valuable insights into fundamental limits of identifiability. Simulation studies verify that latent structures satisfying our conditions can be accurately estimated from data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding</title>
<link>https://arxiv.org/abs/2505.18411</link>
<guid>https://arxiv.org/abs/2505.18411</guid>
<content:encoded><![CDATA[
arXiv:2505.18411v1 Announce Type: cross 
Abstract: We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance multi-modal Temporal Point Process (TPP) modeling in the era of Large Language Models (LLMs). While TPPs have been widely studied for modeling temporal event sequences, existing datasets are predominantly unimodal, hindering progress in models that require joint reasoning over temporal, textual, and visual information. To address this gap, DanmakuTPPBench comprises two complementary components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili video platform, where user-generated bullet comments (Danmaku) naturally form multi-modal events annotated with precise timestamps, rich textual content, and corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering dataset constructed via a novel multi-agent pipeline powered by state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex temporal-textual-visual reasoning. We conduct extensive evaluations using both classical TPP models and recent MLLMs, revealing significant performance gaps and limitations in current methods' ability to model multi-modal event dynamics. Our benchmark establishes strong baselines and calls for further integration of TPP modeling into the multi-modal language modeling landscape. The code and dataset have been released at https://github.com/FRENKIE-CHIANG/DanmakuTPPBench
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Ballbot Navigation in Uneven Terrain</title>
<link>https://arxiv.org/abs/2505.18417</link>
<guid>https://arxiv.org/abs/2505.18417</guid>
<content:encoded><![CDATA[
arXiv:2505.18417v1 Announce Type: cross 
Abstract: Ballbot (i.e. Ball balancing robot) navigation usually relies on methods rooted in control theory (CT), and works that apply Reinforcement learning (RL) to the problem remain rare while generally being limited to specific subtasks (e.g. balance recovery). Unlike CT based methods, RL does not require (simplifying) assumptions about environment dynamics (e.g. the absence of slippage between the ball and the floor). In addition to this increased accuracy in modeling, RL agents can easily be conditioned on additional observations such as depth-maps without the need for explicit formulations from first principles, leading to increased adaptivity. Despite those advantages, there has been little to no investigation into the capabilities, data-efficiency and limitations of RL based methods for ballbot control and navigation. Furthermore, there is a notable absence of an open-source, RL-friendly simulator for this task. In this paper, we present an open-source ballbot simulation based on MuJoCo, and show that with appropriate conditioning on exteroceptive observations as well as reward shaping, policies learned by classical model-free RL methods are capable of effectively navigating through randomly generated uneven terrain, using a reasonable amount of data (four to five hours on a system operating at 500hz).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocalKMeans: Convergence of Lloyd's Algorithm with Distributed Local Iterations</title>
<link>https://arxiv.org/abs/2505.18420</link>
<guid>https://arxiv.org/abs/2505.18420</guid>
<content:encoded><![CDATA[
arXiv:2505.18420v1 Announce Type: cross 
Abstract: In this paper, we analyze the classical $K$-means alternating-minimization algorithm, also known as Lloyd's algorithm (Lloyd, 1956), for a mixture of Gaussians in a data-distributed setting that incorporates local iteration steps. Assuming unlabeled data distributed across multiple machines, we propose an algorithm, LocalKMeans, that performs Lloyd's algorithm in parallel in the machines by running its iterations on local data, synchronizing only every $L$ of such local steps. We characterize the cost of these local iterations against the non-distributed setting, and show that the price paid for the local steps is a higher required signal-to-noise ratio. While local iterations were theoretically studied in the past for gradient-based learning methods, the analysis of unsupervised learning methods is more involved owing to the presence of latent variables, e.g. cluster identities, than that of an iterative gradient-based algorithm. To obtain our results, we adapt a virtual iterate method to work with a non-convex, non-smooth objective function, in conjunction with a tight statistical analysis of Lloyd steps.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts</title>
<link>https://arxiv.org/abs/2505.18455</link>
<guid>https://arxiv.org/abs/2505.18455</guid>
<content:encoded><![CDATA[
arXiv:2505.18455v1 Announce Type: cross 
Abstract: The softmax-contaminated mixture of experts (MoE) model is deployed when a large-scale pre-trained model, which plays the role of a fixed expert, is fine-tuned for learning downstream tasks by including a new contamination part, or prompt, functioning as a new, trainable expert. Despite its popularity and relevance, the theoretical properties of the softmax-contaminated MoE have remained unexplored in the literature. In the paper, we study the convergence rates of the maximum likelihood estimator of gating and prompt parameters in order to gain insights into the statistical properties and potential challenges of fine-tuning with a new prompt. We find that the estimability of these parameters is compromised when the prompt acquires overlapping knowledge with the pre-trained model, in the sense that we make precise by formulating a novel analytic notion of distinguishability. Under distinguishability of the pre-trained and prompt models, we derive minimax optimal estimation rates for all the gating and prompt parameters. By contrast, when the distinguishability condition is violated, these estimation rates become significantly slower due to their dependence on the prompt convergence rate to the pre-trained model. Finally, we empirically corroborate our theoretical findings through several numerical experiments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anchored Diffusion Language Model</title>
<link>https://arxiv.org/abs/2505.18456</link>
<guid>https://arxiv.org/abs/2505.18456</guid>
<content:encoded><![CDATA[
arXiv:2505.18456v1 Announce Type: cross 
Abstract: Diffusion Language Models (DLMs) promise parallel generation and bidirectional context, yet they underperform autoregressive (AR) models in both likelihood modeling and generated text quality. We identify that this performance gap arises when important tokens (e.g., key words or low-frequency words that anchor a sentence) are masked early in the forward process, limiting contextual information for accurate reconstruction. To address this, we introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage framework that first predicts distributions over important tokens via an anchor network, and then predicts the likelihoods of missing tokens conditioned on the anchored predictions. ADLM significantly improves test perplexity on LM1B and OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap with strong AR baselines. It also achieves state-of-the-art performance in zero-shot generalization across seven benchmarks and surpasses AR models in MAUVE score, which marks the first time a DLM generates better human-like text than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower Bound (ANELBO) objective and show that anchoring improves sample complexity and likelihood modeling. Beyond diffusion, anchoring boosts performance in AR models and enhances reasoning in math and logic tasks, outperforming existing chain-of-thought approaches
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks</title>
<link>https://arxiv.org/abs/2505.18457</link>
<guid>https://arxiv.org/abs/2505.18457</guid>
<content:encoded><![CDATA[
arXiv:2505.18457v1 Announce Type: cross 
Abstract: This paper introduces EdgeAgentX, a novel framework integrating federated learning (FL), multi-agent reinforcement learning (MARL), and adversarial defense mechanisms, tailored for military communication networks. EdgeAgentX significantly improves autonomous decision-making, reduces latency, enhances throughput, and robustly withstands adversarial disruptions, as evidenced by comprehensive simulations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of LLM $\times$ DATA</title>
<link>https://arxiv.org/abs/2505.18458</link>
<guid>https://arxiv.org/abs/2505.18458</guid>
<content:encoded><![CDATA[
arXiv:2505.18458v1 Announce Type: cross 
Abstract: The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek</title>
<link>https://arxiv.org/abs/2505.18486</link>
<guid>https://arxiv.org/abs/2505.18486</guid>
<content:encoded><![CDATA[
arXiv:2505.18486v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been widely explored for automated scoring in low-stakes assessment to facilitate learning and instruction. Empirical evidence related to which LLM produces the most reliable scores and induces least rater effects needs to be collected before the use of LLMs for automated scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4, ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini 2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in scoring two types of writing tasks. The accuracy of the holistic and analytic scores from LLMs compared with human raters was evaluated in terms of Quadratic Weighted Kappa. Intra-rater consistency across prompts was compared in terms of Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human raters using the Many-Facet Rasch model. The results in general supported the use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring accuracy, better rater reliability, and less rater effects.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning</title>
<link>https://arxiv.org/abs/2505.18487</link>
<guid>https://arxiv.org/abs/2505.18487</guid>
<content:encoded><![CDATA[
arXiv:2505.18487v1 Announce Type: cross 
Abstract: Learning effective visual representations for robotic manipulation remains a fundamental challenge due to the complex body dynamics involved in action execution. In this paper, we study how visual representations that carry body-relevant cues can enable efficient policy learning for downstream robotic manipulation tasks. We present $\textbf{I}$nter-token $\textbf{Con}$trast ($\textbf{ICon}$), a contrastive learning method applied to the token-level representations of Vision Transformers (ViTs). ICon enforces a separation in the feature space between agent-specific and environment-specific tokens, resulting in agent-centric visual representations that embed body-specific inductive biases. This framework can be seamlessly integrated into end-to-end policy learning by incorporating the contrastive loss as an auxiliary objective. Our experiments show that ICon not only improves policy performance across various manipulation tasks but also facilitates policy transfer across different robots. The project website: https://github.com/HenryWJL/icon
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference under Performativity</title>
<link>https://arxiv.org/abs/2505.18493</link>
<guid>https://arxiv.org/abs/2505.18493</guid>
<content:encoded><![CDATA[
arXiv:2505.18493v1 Announce Type: cross 
Abstract: Performativity of predictions refers to the phenomena that prediction-informed decisions may influence the target they aim to predict, which is widely observed in policy-making in social sciences and economics. In this paper, we initiate the study of statistical inference under performativity. Our contribution is two-fold. First, we build a central limit theorem for estimation and inference under performativity, which enables inferential purposes in policy-making such as constructing confidence intervals or testing hypotheses. Second, we further leverage the derived central limit theorem to investigate prediction-powered inference (PPI) under performativity, which is based on a small labeled dataset and a much larger dataset of machine-learning predictions. This enables us to obtain more precise estimation and improved confidence regions for the model parameter (i.e., policy) of interest in performative prediction. We demonstrate the power of our framework by numerical experiments. To the best of our knowledge, this paper is the first one to establish statistical inference under performativity, which brings up new challenges and inference settings that we believe will add significant values to policy-making, statistics, and machine learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Grafting of Large Language Models</title>
<link>https://arxiv.org/abs/2505.18502</link>
<guid>https://arxiv.org/abs/2505.18502</guid>
<content:encoded><![CDATA[
arXiv:2505.18502v1 Announce Type: cross 
Abstract: Cross-capability transfer is a key challenge in large language model (LLM) research, with applications in multi-task integration, model compression, and continual learning. Recent works like FuseLLM and FuseChat have demonstrated the potential of transferring multiple model capabilities to lightweight models, enhancing adaptability and efficiency, which motivates our investigation into more efficient cross-capability transfer methods. However, existing approaches primarily focus on small, homogeneous models, limiting their applicability. For large, heterogeneous models, knowledge distillation with full-parameter fine-tuning often overlooks the student model's intrinsic capacity and risks catastrophic forgetting, while PEFT methods struggle to effectively absorb knowledge from source LLMs. To address these issues, we introduce GraftLLM, a novel method that stores source model capabilities in a target model with SkillPack format. This approach preserves general capabilities, reduces parameter conflicts, and supports forget-free continual learning and model fusion. We employ a module-aware adaptive compression strategy to compress parameter updates, ensuring efficient storage while maintaining task-specific knowledge. The resulting SkillPack serves as a compact and transferable knowledge carrier, ideal for heterogeneous model fusion and continual learning. Experiments across various scenarios demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, knowledge fusion, and forget-free learning, providing a scalable and efficient solution for cross-capability transfer. The code is publicly available at: https://github.com/duguodong7/GraftLLM.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking</title>
<link>https://arxiv.org/abs/2505.18512</link>
<guid>https://arxiv.org/abs/2505.18512</guid>
<content:encoded><![CDATA[
arXiv:2505.18512v1 Announce Type: cross 
Abstract: Listwise reranking with large language models (LLMs) enhances top-ranked results in retrieval-based applications. Due to the limit in context size and high inference cost of long context, reranking is typically performed over a fixed size of small subsets, with the final ranking aggregated from these partial results. This fixed computation disregards query difficulty and document distribution, leading to inefficiencies. We propose AcuRank, an adaptive reranking framework that dynamically adjusts both the amount and target of computation based on uncertainty estimates over document relevance. Using a Bayesian TrueSkill model, we iteratively refine relevance estimates until reaching sufficient confidence levels, and our explicit modeling of ranking uncertainty enables principled control over reranking behavior and avoids unnecessary updates to confident predictions. Results on the TREC-DL and BEIR benchmarks show that our method consistently achieves a superior accuracy-efficiency trade-off and scales better with compute than fixed-computation baselines. These results highlight the effectiveness and generalizability of our method across diverse retrieval tasks and LLM-based reranking models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs</title>
<link>https://arxiv.org/abs/2505.18517</link>
<guid>https://arxiv.org/abs/2505.18517</guid>
<content:encoded><![CDATA[
arXiv:2505.18517v1 Announce Type: cross 
Abstract: Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition</title>
<link>https://arxiv.org/abs/2505.18526</link>
<guid>https://arxiv.org/abs/2505.18526</guid>
<content:encoded><![CDATA[
arXiv:2505.18526v1 Announce Type: cross 
Abstract: Kernels are key to encoding prior beliefs and data structures in Gaussian process (GP) models. The design of expressive and scalable kernels has garnered significant research attention. Deep kernel learning enhances kernel flexibility by feeding inputs through a neural network before applying a standard parametric form. However, this approach remains limited by the choice of base kernels, inherits high inference costs, and often demands sparse approximations. Drawing on Mercer's theorem, we introduce a fully data-driven, scalable deep kernel representation where a neural network directly represents a low-rank kernel through a small set of basis functions. This construction enables highly efficient exact GP inference in linear time and memory without invoking inducing points. It also supports scalable mini-batch training based on a principled variational inference framework. We further propose a simple variance correction procedure to guard against overconfidence in uncertainty estimates. Experiments on synthetic and real-world data demonstrate the advantages of our deep kernel GP in terms of predictive accuracy, uncertainty quantification, and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking</title>
<link>https://arxiv.org/abs/2505.18538</link>
<guid>https://arxiv.org/abs/2505.18538</guid>
<content:encoded><![CDATA[
arXiv:2505.18538v1 Announce Type: cross 
Abstract: Refractive errors are among the most common visual impairments globally, yet their diagnosis often relies on active user participation and clinical oversight. This study explores a passive method for estimating refractive power using two eye movement recording techniques: electrooculography (EOG) and video-based eye tracking. Using a publicly available dataset recorded under varying diopter conditions, we trained Long Short-Term Memory (LSTM) models to classify refractive power from unimodal (EOG or eye tracking) and multimodal configuration. We assess performance in both subject-dependent and subject-independent settings to evaluate model personalization and generalizability across individuals. Results show that the multimodal model consistently outperforms unimodal models, achieving the highest average accuracy in both settings: 96.207\% in the subject-dependent scenario and 8.882\% in the subject-independent scenario. However, generalization remains limited, with classification accuracy only marginally above chance in the subject-independent evaluations. Statistical comparisons in the subject-dependent setting confirmed that the multimodal model significantly outperformed the EOG and eye-tracking models. However, no statistically significant differences were found in the subject-independent setting. Our findings demonstrate both the potential and current limitations of eye movement data-based refractive error estimation, contributing to the development of continuous, non-invasive screening methods using EOG signals and eye-tracking data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Poisoning Attacks against Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.18543</link>
<guid>https://arxiv.org/abs/2505.18543</guid>
<content:encoded><![CDATA[
arXiv:2505.18543v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has proven effective in mitigating hallucinations in large language models by incorporating external knowledge during inference. However, this integration introduces new security vulnerabilities, particularly to poisoning attacks. Although prior work has explored various poisoning strategies, a thorough assessment of their practical threat to RAG systems remains missing. To address this gap, we propose the first comprehensive benchmark framework for evaluating poisoning attacks on RAG. Our benchmark covers 5 standard question answering (QA) datasets and 10 expanded variants, along with 13 poisoning attack methods and 7 defense mechanisms, representing a broad spectrum of existing techniques. Using this benchmark, we conduct a comprehensive evaluation of all included attacks and defenses across the full dataset spectrum. Our findings show that while existing attacks perform well on standard QA datasets, their effectiveness drops significantly on the expanded versions. Moreover, our results demonstrate that various advanced RAG architectures, such as sequential, branching, conditional, and loop RAG, as well as multi-turn conversational RAG, multimodal RAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning attacks. Notably, current defense techniques fail to provide robust protection, underscoring the pressing need for more resilient and generalizable defense strategies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery</title>
<link>https://arxiv.org/abs/2505.18546</link>
<guid>https://arxiv.org/abs/2505.18546</guid>
<content:encoded><![CDATA[
arXiv:2505.18546v1 Announce Type: cross 
Abstract: Soil organic carbon (SOC) is a critical indicator of soil health, but its accurate estimation from satellite imagery is hindered in vegetated regions due to spectral contamination from plant cover, which obscures soil reflectance and reduces model reliability. This study proposes the Reflectance Transformation Generative Adversarial Network (ReflectGAN), a novel paired GAN-based framework designed to reconstruct accurate bare soil reflectance from vegetated soil satellite observations. By learning the spectral transformation between vegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC estimation under mixed land cover conditions. Using the LUCAS 2018 dataset and corresponding Landsat 8 imagery, we trained multiple learning-based models on both original and ReflectGAN-reconstructed reflectance inputs. Models trained on ReflectGAN outputs consistently outperformed those using existing vegetation correction methods. For example, the best-performing model (RF) achieved an $R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the ReflectGAN-generated signals, representing a 35\% increase in $R^2$, a 43\% reduction in RMSE, and a 43\% improvement in RPD compared to the best existing method (PMM-SU). The performance of the models with ReflectGAN is also better compared to their counterparts when applied to another dataset, i.e., Sentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to improve SOC estimation accuracy in vegetated landscapes, supporting more reliable soil monitoring.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis</title>
<link>https://arxiv.org/abs/2505.18551</link>
<guid>https://arxiv.org/abs/2505.18551</guid>
<content:encoded><![CDATA[
arXiv:2505.18551v1 Announce Type: cross 
Abstract: Machine learning (ML)-based malware detection systems often fail to account for the dynamic nature of real-world training and test data distributions. In practice, these distributions evolve due to frequent changes in the Android ecosystem, adversarial development of new malware families, and the continuous emergence of both benign and malicious applications. Prior studies have shown that such concept drift -- distributional shifts in benign and malicious samples, leads to significant degradation in detection performance over time. Despite the practical importance of this issue, existing datasets are often outdated and limited in temporal scope, diversity of malware families, and sample scale, making them insufficient for the systematic evaluation of concept drift in malware detection.
  To address this gap, we present LAMDA, the largest and most temporally diverse Android malware benchmark to date, designed specifically for concept drift analysis. LAMDA spans 12 years (2013-2025, excluding 2015), includes over 1 million samples (approximately 37% labeled as malware), and covers 1,380 malware families and 150,000 singleton samples, reflecting the natural distribution and evolution of real-world Android applications. We empirically demonstrate LAMDA's utility by quantifying the performance degradation of standard ML models over time and analyzing feature stability across years. As the most comprehensive Android malware dataset to date, LAMDA enables in-depth research into temporal drift, generalization, explainability, and evolving detection challenges. The dataset and code are available at: https://iqsec-lab.github.io/LAMDA/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autocomp: LLM-Driven Code Optimization for Tensor Accelerators</title>
<link>https://arxiv.org/abs/2505.18574</link>
<guid>https://arxiv.org/abs/2505.18574</guid>
<content:encoded><![CDATA[
arXiv:2505.18574v1 Announce Type: cross 
Abstract: Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages like specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three categories of representative workloads and two different accelerators, we demonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x (convolution) faster than the vendor-provided library, and outperforms expert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x (fine-grained linear algebra). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs for Supply Chain Management</title>
<link>https://arxiv.org/abs/2505.18597</link>
<guid>https://arxiv.org/abs/2505.18597</guid>
<content:encoded><![CDATA[
arXiv:2505.18597v1 Announce Type: cross 
Abstract: The development of large language models (LLMs) has provided new tools for research in supply chain management (SCM). In this paper, we introduce a retrieval-augmented generation (RAG) framework that dynamically integrates external knowledge into the inference process, and develop a domain-specialized SCM LLM, which demonstrates expert-level competence by passing standardized SCM examinations and beer game tests. We further employ the use of LLMs to conduct horizontal and vertical supply chain games, in order to analyze competition and cooperation within supply chains. Our experiments show that RAG significantly improves performance on SCM tasks. Moreover, game-theoretic analysis reveals that the LLM can reproduce insights from the classical SCM literature, while also uncovering novel behaviors and offering fresh perspectives on phenomena such as the bullwhip effect. This paper opens the door for exploring cooperation and competition for complex supply chain network through the lens of LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment</title>
<link>https://arxiv.org/abs/2505.18600</link>
<guid>https://arxiv.org/abs/2505.18600</guid>
<content:encoded><![CDATA[
arXiv:2505.18600v1 Announce Type: cross 
Abstract: Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement with high perceptual quality and fidelity.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Meta-SR: Learning to Evolve Selection Operators for Symbolic Regression</title>
<link>https://arxiv.org/abs/2505.18602</link>
<guid>https://arxiv.org/abs/2505.18602</guid>
<content:encoded><![CDATA[
arXiv:2505.18602v1 Announce Type: cross 
Abstract: Large language models (LLMs) have revolutionized algorithm development, yet their application in symbolic regression, where algorithms automatically discover symbolic expressions from data, remains constrained and is typically designed manually by human experts. In this paper, we propose a learning-to-evolve framework that enables LLMs to automatically design selection operators for evolutionary symbolic regression algorithms. We first identify two key limitations in existing LLM-based algorithm evolution techniques: code bloat and a lack of semantic guidance. Bloat results in unnecessarily complex components, and the absence of semantic awareness can lead to ineffective exchange of useful code components, both of which can reduce the interpretability of the designed algorithm or hinder evolutionary learning progress. To address these issues, we enhance the LLM-based evolution framework for meta symbolic regression with two key innovations: bloat control and a complementary, semantics-aware selection operator. Additionally, we embed domain knowledge into the prompt, enabling the LLM to generate more effective and contextually relevant selection operators. Our experimental results on symbolic regression benchmarks show that LLMs can devise selection operators that outperform nine expert-designed baselines, achieving state-of-the-art performance. This demonstrates that LLMs can exceed expert-level algorithm design for symbolic regression.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLRan: A Behavioural Dataset for Ransomware Analysis and Detection</title>
<link>https://arxiv.org/abs/2505.18613</link>
<guid>https://arxiv.org/abs/2505.18613</guid>
<content:encoded><![CDATA[
arXiv:2505.18613v1 Announce Type: cross 
Abstract: Ransomware remains a critical threat to cybersecurity, yet publicly available datasets for training machine learning-based ransomware detection models are scarce and often have limited sample size, diversity, and reproducibility. In this paper, we introduce MLRan, a behavioural ransomware dataset, comprising over 4,800 samples across 64 ransomware families and a balanced set of goodware samples. The samples span from 2006 to 2024 and encompass the four major types of ransomware: locker, crypto, ransomware-as-a-service, and modern variants. We also propose guidelines (GUIDE-MLRan), inspired by previous work, for constructing high-quality behavioural ransomware datasets, which informed the curation of our dataset. We evaluated the ransomware detection performance of several machine learning (ML) models using MLRan. For this purpose, we performed feature selection by conducting mutual information filtering to reduce the initial 6.4 million features to 24,162, followed by recursive feature elimination, yielding 483 highly informative features. The ML models achieved an accuracy, precision and recall of up to 98.7%, 98.9%, 98.5%, respectively. Using SHAP and LIME, we identified critical indicators of malicious behaviour, including registry tampering, strings, and API misuse. The dataset and source code for feature extraction, selection, ML training, and evaluation are available publicly to support replicability and encourage future research, which can be found at https://github.com/faithfulco/mlran.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation</title>
<link>https://arxiv.org/abs/2505.18614</link>
<guid>https://arxiv.org/abs/2505.18614</guid>
<content:encoded><![CDATA[
arXiv:2505.18614v1 Announce Type: cross 
Abstract: Lyrics translation requires both accurate semantic transfer and preservation of musical rhythm, syllabic structure, and poetic style. In animated musicals, the challenge intensifies due to alignment with visual and auditory cues. We introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song Translation (MAVL), the first multilingual, multimodal benchmark for singable lyrics translation. By integrating text, audio, and video, MAVL enables richer and more expressive translations than text-only approaches. Building on this, we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints to produce natural-sounding lyrics. Experimental results demonstrate that SylAVL-CoT significantly outperforms text-based models in singability and contextual accuracy, emphasizing the value of multimodal, multilingual approaches for lyrics translation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind The Gap: Deep Learning Doesn't Learn Deeply</title>
<link>https://arxiv.org/abs/2505.18623</link>
<guid>https://arxiv.org/abs/2505.18623</guid>
<content:encoded><![CDATA[
arXiv:2505.18623v1 Announce Type: cross 
Abstract: This paper aims to understand how neural networks learn algorithmic reasoning by addressing two questions: How faithful are learned algorithms when they are effective, and why do neural networks fail to learn effective algorithms otherwise? To answer these questions, we use neural compilation, a technique that directly encodes a source algorithm into neural network parameters, enabling the network to compute the algorithm exactly. This enables comparison between compiled and conventionally learned parameters, intermediate vectors, and behaviors. This investigation is crucial for developing neural networks that robustly learn complexalgorithms from data. Our analysis focuses on graph neural networks (GNNs), which are naturally aligned with algorithmic reasoning tasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the spectrum of effective, faithful, and ineffective learned algorithms. Commonly, learning algorithmic reasoning is framed as induction over synthetic data, where a parameterized model is trained on inputs, traces, and outputs produced by an underlying ground truth algorithm. In contrast, we introduce a neural compilation method for GNNs, which sets network parameters analytically, bypassing training. Focusing on GNNs leverages their alignment with algorithmic reasoning, extensive algorithmic induction literature, and the novel application of neural compilation to GNNs. Overall, this paper aims to characterize expressability-trainability gaps - a fundamental shortcoming in learning algorithmic reasoning. We hypothesize that inductive learning is most effective for parallel algorithms contained within the computational class \texttt{NC}.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Emergence of Linear Analogies in Word Embeddings</title>
<link>https://arxiv.org/abs/2505.18651</link>
<guid>https://arxiv.org/abs/2505.18651</guid>
<content:encoded><![CDATA[
arXiv:2505.18651v1 Announce Type: cross 
Abstract: Models such as Word2Vec and GloVe construct word embeddings based on the co-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The resulting vectors $W_i$ not only group semantically similar words but also exhibit a striking linear analogy structure -- for example, $W_{\text{king}} - W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whose theoretical origin remains unclear. Previous observations indicate that this analogy structure: (i) already emerges in the top eigenvectors of the matrix $M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more eigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are included, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and (iv) persists even when all word pairs involved in a specific analogy relation (e.g., king-queen, man-woman) are removed from the corpus. To explain these phenomena, we introduce a theoretical generative model in which words are defined by binary semantic attributes, and co-occurrence probabilities are derived from attribute-based interactions. This model analytically reproduces the emergence of linear analogy structure and naturally accounts for properties (i)-(iv). It can be viewed as giving fine-grained resolution into the role of each additional embedding dimension. It is robust to various forms of noise and agrees well with co-occurrence statistics measured on Wikipedia and the analogy benchmark introduced by Mikolov et al.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2505.18658</link>
<guid>https://arxiv.org/abs/2505.18658</guid>
<content:encoded><![CDATA[
arXiv:2505.18658v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees</title>
<link>https://arxiv.org/abs/2505.18659</link>
<guid>https://arxiv.org/abs/2505.18659</guid>
<content:encoded><![CDATA[
arXiv:2505.18659v1 Announce Type: cross 
Abstract: Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference (\texttt{PPI}) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose \texttt{R-AutoEval+}, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of \texttt{R-AutoEval+} is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, and for prompt design in LLMs confirm the reliability and efficiency of \texttt{R-AutoEval+}.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy</title>
<link>https://arxiv.org/abs/2505.18664</link>
<guid>https://arxiv.org/abs/2505.18664</guid>
<content:encoded><![CDATA[
arXiv:2505.18664v1 Announce Type: cross 
Abstract: We present a memory-efficient algorithm for significantly enhancing the quality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks using a generative model. The proposed model achieves a 16x increase in resolution and corrects inaccuracies in segmentation caused by the overlapping X-ray attenuation in micro-CT measurements across different minerals. The generative model employed is a 3D Octree-based convolutional Wasserstein generative adversarial network with gradient penalty. To address the challenge of high memory consumption inherent in standard 3D convolutional layers, we implemented an Octree structure within the 3D progressive growing generator model. This enabled the use of memory-efficient 3D Octree-based convolutional layers. The approach is pivotal in overcoming the long-standing memory bottleneck in volumetric deep learning, making it possible to reach 16x super-resolution in 3D, a scale that is challenging to attain due to cubic memory scaling. For training, we utilized segmented 3D low-resolution micro-CT images along with unpaired segmented complementary 2D high-resolution laser scanning microscope images. Post-training, resolution improved from 7 to 0.44 micro-m/voxel with accurate segmentation of constituent minerals. Validated on Berea sandstone, this framework demonstrates substantial improvements in pore characterization and mineral differentiation, offering a robust solution to one of the primary computational limitations in modern geoscientific imaging.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in the Task of Automatic Validation of Text Classifier Predictions</title>
<link>https://arxiv.org/abs/2505.18688</link>
<guid>https://arxiv.org/abs/2505.18688</guid>
<content:encoded><![CDATA[
arXiv:2505.18688v1 Announce Type: cross 
Abstract: Machine learning models for text classification are trained to predict a class for a given text. To do this, training and validation samples must be prepared: a set of texts is collected, and each text is assigned a class. These classes are usually assigned by human annotators with different expertise levels, depending on the specific classification task. Collecting such samples from scratch is labor-intensive because it requires finding specialists and compensating them for their work; moreover, the number of available specialists is limited, and their productivity is constrained by human factors. While it may not be too resource-intensive to collect samples once, the ongoing need to retrain models (especially in incremental learning pipelines) to address data drift (also called model drift) makes the data collection process crucial and costly over the model's entire lifecycle. This paper proposes several approaches to replace human annotators with Large Language Models (LLMs) to test classifier predictions for correctness, helping ensure model quality and support high-quality incremental learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization</title>
<link>https://arxiv.org/abs/2505.18720</link>
<guid>https://arxiv.org/abs/2505.18720</guid>
<content:encoded><![CDATA[
arXiv:2505.18720v1 Announce Type: cross 
Abstract: Direct Preference Optimization (DPO) has emerged as a promising framework for aligning Large Language Models (LLMs) with human preferences by directly optimizing the log-likelihood difference between chosen and rejected responses. However, existing methods assign equal importance to all tokens in the response, while humans focus on more meaningful parts. This leads to suboptimal preference optimization, as irrelevant or noisy tokens disproportionately influence DPO loss. To address this limitation, we propose \textbf{O}ptimal \textbf{T}ransport-based token weighting scheme for enhancing direct \textbf{P}reference \textbf{O}ptimization (OTPO). By emphasizing semantically meaningful token pairs and de-emphasizing less relevant ones, our method introduces a context-aware token weighting scheme that yields a more contrastive reward difference estimate. This adaptive weighting enhances reward stability, improves interpretability, and ensures that preference optimization focuses on meaningful differences between responses. Extensive experiments have validated OTPO's effectiveness in improving instruction-following ability across various settings\footnote{Code is available at https://github.com/Mimasss2/OTPO.}.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Geolocation: A Natural Sounds Benchmark</title>
<link>https://arxiv.org/abs/2505.18726</link>
<guid>https://arxiv.org/abs/2505.18726</guid>
<content:encoded><![CDATA[
arXiv:2505.18726v1 Announce Type: cross 
Abstract: Can we determine someone's geographic location purely from the sounds they hear? Are acoustic signals enough to localize within a country, state, or even city? We tackle the challenge of global-scale audio geolocation, formalize the problem, and conduct an in-depth analysis with wildlife audio from the iNatSounds dataset. Adopting a vision-inspired approach, we convert audio recordings to spectrograms and benchmark existing image geolocation techniques. We hypothesize that species vocalizations offer strong geolocation cues due to their defined geographic ranges and propose an approach that integrates species range prediction with retrieval-based geolocation. We further evaluate whether geolocation improves when analyzing species-rich recordings or when aggregating across spatiotemporal neighborhoods. Finally, we introduce case studies from movies to explore multimodal geolocation using both audio and visual content. Our work highlights the advantages of integrating audio and visual cues, and sets the stage for future research in audio geolocation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MADCAT: Combating Malware Detection Under Concept Drift with Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2505.18734</link>
<guid>https://arxiv.org/abs/2505.18734</guid>
<content:encoded><![CDATA[
arXiv:2505.18734v1 Announce Type: cross 
Abstract: We present MADCAT, a self-supervised approach designed to address the concept drift problem in malware detection. MADCAT employs an encoder-decoder architecture and works by test-time training of the encoder on a small, balanced subset of the test-time data using a self-supervised objective. During test-time training, the model learns features that are useful for detecting both previously seen (old) data and newly arriving samples. We demonstrate the effectiveness of MADCAT in continuous Android malware detection settings. MADCAT consistently outperforms baseline methods in detection performance at test time. We also show the synergy between MADCAT and prior approaches in addressing concept drift in malware detection
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging</title>
<link>https://arxiv.org/abs/2505.18745</link>
<guid>https://arxiv.org/abs/2505.18745</guid>
<content:encoded><![CDATA[
arXiv:2505.18745v1 Announce Type: cross 
Abstract: Immunohistochemical (IHC) images reveal detailed information about structures and functions at the subcellular level. However, unlike natural images, IHC datasets pose challenges for deep learning models due to their inconsistencies in channel count and configuration, stemming from varying staining protocols across laboratories and studies. Existing approaches build channel-adaptive models, which unfortunately fail to support out-of-distribution (OOD) evaluation across IHC datasets and cannot be applied in a true zero-shot setting with mismatched channel counts. To address this, we introduce a structured view of cellular image channels by grouping them into either context or concept, where we treat the context channels as a reference to the concept channels in the image. We leverage this context-concept principle to develop Channel Conditioned Cell Representations (C3R), a framework designed for unified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold framework comprising a channel-adaptive encoder architecture and a masked knowledge distillation training strategy, both built around the context-concept principle. We find that C3R outperforms existing benchmarks on both ID and OOD tasks, while a trivial implementation of our core idea also outperforms the channel-adaptive methods reported on the CHAMMI benchmark. Our method opens a new pathway for cross-dataset generalization between IHC datasets, without requiring dataset-specific adaptation or retraining.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion</title>
<link>https://arxiv.org/abs/2505.18747</link>
<guid>https://arxiv.org/abs/2505.18747</guid>
<content:encoded><![CDATA[
arXiv:2505.18747v1 Announce Type: cross 
Abstract: With the advancement of energy Internet and energy system integration, the increasing adoption of distributed photovoltaic (PV) systems presents new challenges on smart monitoring and measurement for utility companies, particularly in separating PV generation from net electricity load. Existing methods struggle with feature extraction from net load and capturing the relevance between weather factors. This paper proposes a PV disaggregation method that integrates Hierarchical Interpolation (HI) and multi-head self-attention mechanisms. By using HI to extract net load features and multi-head self-attention to capture the complex dependencies between weather factors, the method achieves precise PV generation predictions. Simulation experiments demonstrate the effectiveness of the proposed method in real-world data, supporting improved monitoring and management of distributed energy systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation</title>
<link>https://arxiv.org/abs/2505.18759</link>
<guid>https://arxiv.org/abs/2505.18759</guid>
<content:encoded><![CDATA[
arXiv:2505.18759v1 Announce Type: cross 
Abstract: Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is shared in https://anonymous.4open.science/r/DC-COT-FF4C/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark</title>
<link>https://arxiv.org/abs/2505.18761</link>
<guid>https://arxiv.org/abs/2505.18761</guid>
<content:encoded><![CDATA[
arXiv:2505.18761v1 Announce Type: cross 
Abstract: We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic benchmark to evaluate Large Language Models' (LLMs) reasoning robustness against systematically controlled irrelevant context (IC). GSM-DC constructs symbolic reasoning graphs with precise distractor injections, enabling rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are significantly sensitive to IC, affecting both reasoning path selection and arithmetic accuracy. Additionally, training models with strong distractors improves performance in both in-distribution and out-of-distribution scenarios. We further propose a stepwise tree search guided by a process reward model, which notably enhances robustness in out-of-distribution conditions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Path Stable Soft Prompt Generation for Domain Generalization</title>
<link>https://arxiv.org/abs/2505.18770</link>
<guid>https://arxiv.org/abs/2505.18770</guid>
<content:encoded><![CDATA[
arXiv:2505.18770v1 Announce Type: cross 
Abstract: Domain generalization (DG) aims to learn a model using data from one or multiple related but distinct source domains that can generalize well to unseen out-of-distribution target domains. Inspired by the success of large pre-trained vision-language models (VLMs), prompt tuning has emerged as an effective generalization strategy. However, it often struggles to capture domain-specific features due to its reliance on manually or fixed prompt inputs. Recently, some prompt generation methods have addressed this limitation by dynamically generating instance-specific and domain-specific prompts for each input, enriching domain information and demonstrating potential for enhanced generalization. Through further investigation, we identify a notable issue in existing prompt generation methods: the same input often yields significantly different and suboptimal prompts across different random seeds, a phenomenon we term Prompt Variability. To address this, we introduce negative learning into the prompt generation process and propose Dual-Path Stable Soft Prompt Generation (DPSPG), a transformer-based framework designed to improve both the stability and generalization of prompts. Specifically, DPSPG incorporates a complementary prompt generator to produce negative prompts, thereby reducing the risk of introducing misleading information. Both theoretical and empirical analyses demonstrate that negative learning leads to more robust and effective prompts by increasing the effective margin and reducing the upper bound of the gradient norm. Extensive experiments on five DG benchmark datasets show that DPSPG consistently outperforms state-of-the-art methods while maintaining prompt stability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CageNet: A Meta-Framework for Learning on Wild Meshes</title>
<link>https://arxiv.org/abs/2505.18772</link>
<guid>https://arxiv.org/abs/2505.18772</guid>
<content:encoded><![CDATA[
arXiv:2505.18772v1 Announce Type: cross 
Abstract: Learning on triangle meshes has recently proven to be instrumental to a myriad of tasks, from shape classification, to segmentation, to deformation and animation, to mention just a few. While some of these applications are tackled through neural network architectures which are tailored to the application at hand, many others use generic frameworks for triangle meshes where the only customization required is the modification of the input features and the loss function. Our goal in this paper is to broaden the applicability of these generic frameworks to "wild", i.e. meshes in-the-wild which often have multiple components, non-manifold elements, disrupted connectivity, or a combination of these. We propose a configurable meta-framework based on the concept of caged geometry: Given a mesh, a cage is a single component manifold triangle mesh that envelopes it closely. Generalized barycentric coordinates map between functions on the cage, and functions on the mesh, allowing us to learn and test on a variety of data, in different applications. We demonstrate this concept by learning segmentation and skinning weights on difficult data, achieving better performance to state of the art techniques on wild meshes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models</title>
<link>https://arxiv.org/abs/2505.18773</link>
<guid>https://arxiv.org/abs/2505.18773</guid>
<content:encoded><![CDATA[
arXiv:2505.18773v1 Announce Type: cross 
Abstract: State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion</title>
<link>https://arxiv.org/abs/2505.18780</link>
<guid>https://arxiv.org/abs/2505.18780</guid>
<content:encoded><![CDATA[
arXiv:2505.18780v1 Announce Type: cross 
Abstract: Humanoid locomotion faces a critical scalability challenge: traditional reinforcement learning (RL) methods require task-specific rewards and struggle to leverage growing datasets, even as more training terrains are introduced. We propose DreamPolicy, a unified framework that enables a single policy to master diverse terrains and generalize zero-shot to unseen scenarios by systematically integrating offline data and diffusion-driven motion synthesis. At its core, DreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions synthesized through an autoregressive terrain-aware diffusion planner curated by aggregating rollouts from specialized policies across various distinct terrains. Unlike human motion datasets requiring laborious retargeting, our data directly captures humanoid kinematics, enabling the diffusion planner to synthesize "dreamed" trajectories that encode terrain-specific physical constraints. These trajectories act as dynamic objectives for our HMI-conditioned policy, bypassing manual reward engineering and enabling cross-terrain generalization. DreamPolicy addresses the scalability limitations of prior methods: while traditional RL fails to exploit growing datasets, our framework scales seamlessly with more offline data. As the dataset expands, the diffusion prior learns richer locomotion skills, which the policy leverages to master new terrains without retraining. Experiments demonstrate that DreamPolicy achieves average 90% success rates in training environments and an average of 20% higher success on unseen terrains than the prevalent method. It also generalizes to perturbed and composite scenarios where prior approaches collapse. By unifying offline data, diffusion-based trajectory synthesis, and policy optimization, DreamPolicy overcomes the "one task, one policy" bottleneck, establishing a paradigm for scalable, data-driven humanoid control.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A physics-guided smoothing method for material modeling with digital image correlation (DIC) measurements</title>
<link>https://arxiv.org/abs/2505.18784</link>
<guid>https://arxiv.org/abs/2505.18784</guid>
<content:encoded><![CDATA[
arXiv:2505.18784v1 Announce Type: cross 
Abstract: In this work, we present a novel approach to process the DIC measurements of multiple biaxial stretching protocols. In particular, we develop a optimization-based approach, which calculates the smoothed nodal displacements using a moving least-squares algorithm subject to positive strain constraints. As such, physically consistent displacement and strain fields are obtained. Then, we further deploy a data-driven workflow to heterogeneous material modeling from these physically consistent DIC measurements, by estimating a nonlocal constitutive law together with the material microstructure. To demonstrate the applicability of our approach, we apply it in learning a material model and fiber orientation field from DIC measurements of a porcine tricuspid valve anterior leaflet. Our results demonstrate that the proposed DIC data processing approach can significantly improve the accuracy of modeling biological materials.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided by Guardrails: Control Barrier Functions as Safety Instructors for Robotic Learning</title>
<link>https://arxiv.org/abs/2505.18858</link>
<guid>https://arxiv.org/abs/2505.18858</guid>
<content:encoded><![CDATA[
arXiv:2505.18858v1 Announce Type: cross 
Abstract: Safety stands as the primary obstacle preventing the widespread adoption of learning-based robotic systems in our daily lives. While reinforcement learning (RL) shows promise as an effective robot learning paradigm, conventional RL frameworks often model safety by using single scalar negative rewards with immediate episode termination, failing to capture the temporal consequences of unsafe actions (e.g., sustained collision damage). In this work, we introduce a novel approach that simulates these temporal effects by applying continuous negative rewards without episode termination. Our experiments reveal that standard RL methods struggle with this model, as the accumulated negative values in unsafe zones create learning barriers. To address this challenge, we demonstrate how Control Barrier Functions (CBFs), with their proven safety guarantees, effectively help robots avoid catastrophic regions while enhancing learning outcomes. We present three CBF-based approaches, each integrating traditional RL methods with Control Barrier Functions, guiding the agent to learn safe behavior. Our empirical analysis, conducted in both simulated environments and real-world settings using a four-wheel differential drive robot, explores the possibilities of employing these approaches for safe robotic learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing</title>
<link>https://arxiv.org/abs/2505.18867</link>
<guid>https://arxiv.org/abs/2505.18867</guid>
<content:encoded><![CDATA[
arXiv:2505.18867v1 Announce Type: cross 
Abstract: Lay paraphrasing aims to make scientific information accessible to audiences without technical backgrounds. However, most existing studies focus on a single domain, such as biomedicine. With the rise of interdisciplinary research, it is increasingly necessary to comprehend knowledge spanning multiple technical fields. To address this, we propose Sci-LoRA, a model that leverages a mixture of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA dynamically generates and applies weights for each LoRA, enabling it to adjust the impact of different domains based on the input text, without requiring explicit domain labels. To balance domain-specific knowledge and generalization across various domains, Sci-LoRA integrates information at both the data and model levels. This dynamic fusion enhances the adaptability and performance across various domains. Experimental results across twelve domains on five public datasets show that Sci-LoRA significantly outperforms state-of-the-art large language models and demonstrates flexible generalization and adaptability in cross-domain lay paraphrasing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Stationary Lipschitz Bandits</title>
<link>https://arxiv.org/abs/2505.18871</link>
<guid>https://arxiv.org/abs/2505.18871</guid>
<content:encoded><![CDATA[
arXiv:2505.18871v1 Announce Type: cross 
Abstract: We study the problem of non-stationary Lipschitz bandits, where the number of actions is infinite and the reward function, satisfying a Lipschitz assumption, can change arbitrarily over time. We design an algorithm that adaptively tracks the recently introduced notion of significant shifts, defined by large deviations of the cumulative reward function. To detect such reward changes, our algorithm leverages a hierarchical discretization of the action space. Without requiring any prior knowledge of the non-stationarity, our algorithm achieves a minimax-optimal dynamic regret bound of $\mathcal{\widetilde{O}}(\tilde{L}^{1/3}T^{2/3})$, where $\tilde{L}$ is the number of significant shifts and $T$ the horizon. This result provides the first optimal guarantee in this setting.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marginal Fairness: Fair Decision-Making under Risk Measures</title>
<link>https://arxiv.org/abs/2505.18895</link>
<guid>https://arxiv.org/abs/2505.18895</guid>
<content:encoded><![CDATA[
arXiv:2505.18895v1 Announce Type: cross 
Abstract: This paper introduces marginal fairness, a new individual fairness notion for equitable decision-making in the presence of protected attributes such as gender, race, and religion. This criterion ensures that decisions based on generalized distortion risk measures are insensitive to distributional perturbations in protected attributes, regardless of whether these attributes are continuous, discrete, categorical, univariate, or multivariate. To operationalize this notion and reflect real-world regulatory environments (such as the EU gender-neutral pricing regulation), we model business decision-making in highly regulated industries (such as insurance and finance) as a two-step process: (i) a predictive modeling stage, in which a prediction function for the target variable (e.g., insurance losses) is estimated based on both protected and non-protected covariates; and (ii) a decision-making stage, in which a generalized distortion risk measure is applied to the target variable, conditional only on non-protected covariates, to determine the decision. In this second step, we modify the risk measure such that the decision becomes insensitive to the protected attribute, thus enforcing fairness to ensure equitable outcomes under risk-sensitive, regulatory constraints. Furthermore, by utilizing the concept of cascade sensitivity, we extend the marginal fairness framework to capture how dependencies between covariates propagate the influence of protected attributes through the modeling pipeline. A numerical study and an empirical implementation using an auto insurance dataset demonstrate how the framework can be applied in practice.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos</title>
<link>https://arxiv.org/abs/2505.18899</link>
<guid>https://arxiv.org/abs/2505.18899</guid>
<content:encoded><![CDATA[
arXiv:2505.18899v1 Announce Type: cross 
Abstract: Imitation from videos often fails when expert demonstrations and learner environments exhibit domain shifts, such as discrepancies in lighting, color, or texture. While visual randomization partially addresses this problem by augmenting training data, it remains computationally intensive and inherently reactive, struggling with unseen scenarios. We propose a different approach: instead of randomizing appearances, we eliminate their influence entirely by rethinking the sensory representation itself. Inspired by biological vision systems that prioritize temporal transients (e.g., retinal ganglion cells) and by recent sensor advancements, we introduce event-inspired perception for visually robust imitation. Our method converts standard RGB videos into a sparse, event-based representation that encodes temporal intensity gradients, discarding static appearance features. This biologically grounded approach disentangles motion dynamics from visual style, enabling robust visual imitation from observations even in the presence of visual mismatches between expert and agent environments. By training policies on event streams, we achieve invariance to appearance-based distractors without requiring computationally expensive and environment-specific data augmentation techniques. Experiments across the DeepMind Control Suite and the Adroit platform for dynamic dexterous manipulation show the efficacy of our method. Our code is publicly available at Eb-LAIfO.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger Enforcement of Instruction Hierarchy via Augmented Intermediate Representations</title>
<link>https://arxiv.org/abs/2505.18907</link>
<guid>https://arxiv.org/abs/2505.18907</guid>
<content:encoded><![CDATA[
arXiv:2505.18907v1 Announce Type: cross 
Abstract: Prompt injection attacks are a critical security vulnerability in large language models (LLMs), allowing attackers to hijack model behavior by injecting malicious instructions within the input context. Recent defense mechanisms have leveraged an Instruction Hierarchy (IH) Signal, often implemented through special delimiter tokens or additive embeddings to denote the privilege level of input tokens. However, these prior works typically inject the IH signal exclusively at the initial input layer, which we hypothesize limits its ability to effectively distinguish the privilege levels of tokens as it propagates through the different layers of the model. To overcome this limitation, we introduce a novel approach that injects the IH signal into the intermediate token representations within the network. Our method augments these representations with layer-specific trainable embeddings that encode the privilege information. Our evaluations across multiple models and training methods reveal that our proposal yields between $1.6\times$ and $9.2\times$ reduction in attack success rate on gradient-based prompt injection attacks compared to state-of-the-art methods, without significantly degrading the model's utility.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Label Noise in the Feature Learning Process</title>
<link>https://arxiv.org/abs/2505.18909</link>
<guid>https://arxiv.org/abs/2505.18909</guid>
<content:encoded><![CDATA[
arXiv:2505.18909v1 Announce Type: cross 
Abstract: Deep learning with noisy labels presents significant challenges. In this work, we theoretically characterize the role of label noise from a feature learning perspective. Specifically, we consider a signal-noise data distribution, where each sample comprises a label-dependent signal and label-independent noise, and rigorously analyze the training dynamics of a two-layer convolutional neural network under this data setup, along with the presence of label noise. Our analysis identifies two key stages. In Stage I, the model perfectly fits all the clean samples (i.e., samples without label noise) while ignoring the noisy ones (i.e., samples with noisy labels). During this stage, the model learns the signal from the clean samples, which generalizes well on unseen data. In Stage II, as the training loss converges, the gradient in the direction of noise surpasses that of the signal, leading to overfitting on noisy samples. Eventually, the model memorizes the noise present in the noisy samples and degrades its generalization ability. Furthermore, our analysis provides a theoretical basis for two widely used techniques for tackling label noise: early stopping and sample selection. Experiments on both synthetic and real-world setups validate our theory.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALPCAHUS: Subspace Clustering for Heteroscedastic Data</title>
<link>https://arxiv.org/abs/2505.18918</link>
<guid>https://arxiv.org/abs/2505.18918</guid>
<content:encoded><![CDATA[
arXiv:2505.18918v1 Announce Type: cross 
Abstract: Principal component analysis (PCA) is a key tool in the field of data dimensionality reduction. Various methods have been proposed to extend PCA to the union of subspace (UoS) setting for clustering data that come from multiple subspaces like K-Subspaces (KSS). However, some applications involve heterogeneous data that vary in quality due to noise characteristics associated with each data sample. Heteroscedastic methods aim to deal with such mixed data quality. This paper develops a heteroscedastic-focused subspace clustering method, named ALPCAHUS, that can estimate the sample-wise noise variances and use this information to improve the estimate of the subspace bases associated with the low-rank structure of the data. This clustering algorithm builds on K-Subspaces (KSS) principles by extending the recently proposed heteroscedastic PCA method, named LR-ALPCAH, for clusters with heteroscedastic noise in the UoS setting. Simulations and real-data experiments show the effectiveness of accounting for data heteroscedasticity compared to existing clustering algorithms. Code available at https://github.com/javiersc1/ALPCAHUS.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Infer Causal Relationships from Real-World Text?</title>
<link>https://arxiv.org/abs/2505.18931</link>
<guid>https://arxiv.org/abs/2505.18931</guid>
<content:encoded><![CDATA[
arXiv:2505.18931v1 Announce Type: cross 
Abstract: Understanding and inferring causal relationships from texts is a core aspect of human cognition and is essential for advancing large language models (LLMs) towards artificial general intelligence. Existing work primarily focuses on synthetically generated texts which involve simple causal relationships explicitly mentioned in the text. This fails to reflect the complexities of real-world tasks. In this paper, we investigate whether LLMs are capable of inferring causal relationships from real-world texts. We develop a benchmark drawn from real-world academic literature which includes diverse texts with respect to length, complexity of relationships (different levels of explicitness, number of events, and causal relationships), and domains and sub-domains. To the best of our knowledge, our benchmark is the first-ever real-world dataset for this task. Our experiments on state-of-the-art LLMs evaluated on our proposed benchmark demonstrate significant challenges, with the best-performing model achieving an average F1 score of only 0.477. Analysis reveals common pitfalls: difficulty with implicitly stated information, in distinguishing relevant causal factors from surrounding contextual details, and with connecting causally relevant information spread across lengthy textual passages. By systematically characterizing these deficiencies, our benchmark offers targeted insights for further research into advancing LLM causal reasoning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Price of Format: Diversity Collapse in LLMs</title>
<link>https://arxiv.org/abs/2505.18949</link>
<guid>https://arxiv.org/abs/2505.18949</guid>
<content:encoded><![CDATA[
arXiv:2505.18949v1 Announce Type: cross 
Abstract: Instruction-tuned large language models (LLMs) employ structured templates, such as role markers and special tokens, to enforce format consistency during inference. However, we identify a critical limitation of such formatting: it induces a phenomenon we term diversity collapse, where the model generates semantically similar outputs for open-ended inputs, undermining creativity and variability. We systematically evaluate this effect across tasks like story completion and free-form generation, finding that (1) diversity collapse persists even under high-temperature sampling, and (2) structural tokens in templates significantly constrain the model's output space. To contextualize these findings, we fine-tune the same model using a range of structured prompts and then evaluate them across three axes: downstream task performance, alignment behavior, and output diversity. Our analysis shows that format consistency between fine-tuning and inference is crucial for structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity is primarily governed by the presence or absence of structural tokens, with minimal formatting yielding the most diverse outputs. These findings reveal that current prompting conventions, while beneficial for alignment, may inadvertently suppress output diversity, underscoring the need for diversity-aware prompt design and instruction tuning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation</title>
<link>https://arxiv.org/abs/2505.18956</link>
<guid>https://arxiv.org/abs/2505.18956</guid>
<content:encoded><![CDATA[
arXiv:2505.18956v1 Announce Type: cross 
Abstract: LiDAR-based 3D panoptic segmentation often struggles with the inherent sparsity of data from LiDAR sensors, which makes it challenging to accurately recognize distant or small objects. Recently, a few studies have sought to overcome this challenge by integrating LiDAR inputs with camera images, leveraging the rich and dense texture information provided by the latter. While these approaches have shown promising results, they still face challenges, such as misalignment during data augmentation and the reliance on post-processing steps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel multi-modal 3D panoptic segmentation framework. In IAL, we first introduce a modality-synchronized data augmentation strategy, PieAug, to ensure alignment between LiDAR and image inputs from the start. Next, we adopt a transformer decoder to directly predict panoptic segmentation results. To effectively fuse LiDAR and image features into tokens for the decoder, we design a Geometric-guided Token Fusion (GTF) module. Additionally, we leverage the complementary strengths of each modality as priors for query initialization through a Prior-based Query Generation (PQG) module, enhancing the decoder's ability to generate accurate instance masks. Our IAL framework achieves state-of-the-art performance compared to previous multi-modal 3D panoptic segmentation methods on two widely used benchmarks. Code and models are publicly available at .
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings</title>
<link>https://arxiv.org/abs/2505.18973</link>
<guid>https://arxiv.org/abs/2505.18973</guid>
<content:encoded><![CDATA[
arXiv:2505.18973v1 Announce Type: cross 
Abstract: Selective state-space models have achieved great success in long-sequence modeling. However, their capacity for language representation, especially in complex hierarchical reasoning tasks, remains underexplored. Most large language models rely on flat Euclidean embeddings, limiting their ability to capture latent hierarchies. To address this limitation, we propose Hierarchical Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved nature of hyperbolic geometry to learn hierarchy-aware language embeddings for deeper linguistic understanding. Mamba2-processed sequences are projected to the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via cosine and sine-based mapping) with "learnable" curvature, optimized with a combined hyperbolic loss. Our HiM model facilitates the capture of relational distances across varying hierarchical levels, enabling effective long-range reasoning. This makes it well-suited for tasks like mixed-hop prediction and multi-hop inference in hierarchical classification. We evaluated our HiM with four linguistic and medical datasets for mixed-hop prediction and multi-hop inference tasks. Experimental results demonstrated that: 1) Both HiM models effectively capture hierarchical relationships for four ontological datasets, surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic distinctions with higher h-norms, while HiM-Lorentz provides more stable, compact, and hierarchy-preserving embeddings favoring robustness over detail.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldEval: World Model as Real-World Robot Policies Evaluator</title>
<link>https://arxiv.org/abs/2505.19017</link>
<guid>https://arxiv.org/abs/2505.19017</guid>
<content:encoded><![CDATA[
arXiv:2505.19017v1 Announce Type: cross 
Abstract: The field of robotics has made significant strides toward developing generalist robot manipulation policies. However, evaluating these policies in real-world scenarios remains time-consuming and challenging, particularly as the number of tasks scales and environmental conditions change. In this work, we demonstrate that world models can serve as a scalable, reproducible, and reliable proxy for real-world robot policy evaluation. A key challenge is generating accurate policy videos from world models that faithfully reflect the robot actions. We observe that directly inputting robot actions or using high-dimensional encoding methods often fails to generate action-following videos. To address this, we propose Policy2Vec, a simple yet effective approach to turn a video generation model into a world simulator that follows latent action to generate the robot video. We then introduce WorldEval, an automated pipeline designed to evaluate real-world robot policies entirely online. WorldEval effectively ranks various robot policies and individual checkpoints within a single policy, and functions as a safety detector to prevent dangerous actions by newly developed robot models. Through comprehensive paired evaluations of manipulation policies in real-world environments, we demonstrate a strong correlation between policy performance in WorldEval and real-world scenarios. Furthermore, our method significantly outperforms popular methods such as real-to-sim approach.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking</title>
<link>https://arxiv.org/abs/2505.19023</link>
<guid>https://arxiv.org/abs/2505.19023</guid>
<content:encoded><![CDATA[
arXiv:2505.19023v1 Announce Type: cross 
Abstract: Monkeypox is a viral disease characterized by distinctive skin lesions and has been reported in many countries. The recent global outbreak has emphasized the urgent need for scalable, accessible, and accurate diagnostic solutions to support public health responses.
  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare system specifically designed to detect Monkeypox from skin lesion images using advanced deep learning techniques. Our system consists of three main components. First, we trained and evaluated several pretrained models using transfer learning on publicly available skin lesion datasets to identify the most effective models. For binary classification (Monkeypox vs. non-Monkeypox), the Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16 achieved the highest performance, each with an accuracy and F1-score of 97.8%. For multiclass classification, which contains images of patients with Monkeypox and five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox, and healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1 scores of 92.24% and 92.19%, respectively. The best-performing and most lightweight model, MobileViT, was deployed within the mobile application. The second component is a cross-platform smartphone application that enables users to detect Monkeypox through image analysis, track symptoms, and receive recommendations for nearby healthcare centers based on their location. The third component is a real-time monitoring dashboard designed for health authorities to support them in tracking cases, analyzing symptom trends, guiding public health interventions, and taking proactive measures.
  This system is fundamental in developing responsive healthcare infrastructure within smart cities. Our solution, ITMAINN, is part of revolutionizing public health management.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Conformal Prediction under Epistemic Uncertainty</title>
<link>https://arxiv.org/abs/2505.19033</link>
<guid>https://arxiv.org/abs/2505.19033</guid>
<content:encoded><![CDATA[
arXiv:2505.19033v1 Announce Type: cross 
Abstract: Conformal prediction (CP) is a popular frequentist framework for representing uncertainty by providing prediction sets that guarantee coverage of the true label with a user-adjustable probability. In most applications, CP operates on confidence scores coming from a standard (first-order) probabilistic predictor (e.g., softmax outputs). Second-order predictors, such as credal set predictors or Bayesian models, are also widely used for uncertainty quantification and are known for their ability to represent both aleatoric and epistemic uncertainty. Despite their popularity, there is still an open question on ``how they can be incorporated into CP''. In this paper, we discuss the desiderata for CP when valid second-order predictions are available. We then introduce Bernoulli prediction sets (BPS), which produce the smallest prediction sets that ensure conditional coverage in this setting. When given first-order predictions, BPS reduces to the well-known adaptive prediction sets (APS). Furthermore, when the validity assumption on the second-order predictions is compromised, we apply conformal risk control to obtain a marginal coverage guarantee while still accounting for epistemic uncertainty.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Models Don't Collapse: On the Consistency of Iterative MLE</title>
<link>https://arxiv.org/abs/2505.19046</link>
<guid>https://arxiv.org/abs/2505.19046</guid>
<content:encoded><![CDATA[
arXiv:2505.19046v1 Announce Type: cross 
Abstract: The widespread use of generative models has created a feedback loop, in which each generation of models is trained on data partially produced by its predecessors. This process has raised concerns about \emph{model collapse}: A critical degradation in performance caused by repeated training on synthetic data. However, different analyses in the literature have reached different conclusions as to the severity of model collapse. As such, it remains unclear how concerning this phenomenon is, and under which assumptions it can be avoided. To address this, we theoretically study model collapse for maximum likelihood estimation (MLE), in a natural setting where synthetic data is gradually added to the original data set. Under standard assumptions (similar to those long used for proving asymptotic consistency and normality of MLE), we establish non-asymptotic bounds showing that collapse can be avoided even as the fraction of real data vanishes. On the other hand, we prove that some assumptions (beyond MLE consistency) are indeed necessary: Without them, model collapse can occur arbitrarily quickly, even when the original data is still present in the training set. To the best of our knowledge, these are the first rigorous examples of iterative generative modeling with accumulating data that rapidly leads to model collapse.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Data Selection at Scale via Influence Distillation</title>
<link>https://arxiv.org/abs/2505.19051</link>
<guid>https://arxiv.org/abs/2505.19051</guid>
<content:encoded><![CDATA[
arXiv:2505.19051v1 Announce Type: cross 
Abstract: Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each sample's influence on a target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose a $\textit{landmark-based approximation}$: influence is precisely computed for a small subset of "landmark" samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to $3.5\times$ faster selection.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Embarrassingly Simple Defense Against LLM Abliteration Attacks</title>
<link>https://arxiv.org/abs/2505.19056</link>
<guid>https://arxiv.org/abs/2505.19056</guid>
<content:encoded><![CDATA[
arXiv:2505.19056v1 Announce Type: cross 
Abstract: Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. A recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose a defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with a full response that justifies the reason for refusal. We then fine-tune Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on a set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models' refusal rates drop by 70-80% after abliteration. A broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Initial Exploration of Fine-tuning Small Language Models for Smart Contract Reentrancy Vulnerability Detection</title>
<link>https://arxiv.org/abs/2505.19059</link>
<guid>https://arxiv.org/abs/2505.19059</guid>
<content:encoded><![CDATA[
arXiv:2505.19059v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are being used more and more for various coding tasks, including to help coders identify bugs and are a promising avenue to support coders in various tasks including vulnerability detection -- particularly given the flexibility of such generative AI models and tools. Yet for many tasks it may not be suitable to use LLMs, for which it may be more suitable to use smaller language models that can fit and easily execute and train on a developer's computer. In this paper we explore and evaluate whether smaller language models can be fine-tuned to achieve reasonable results for a niche area: vulnerability detection -- specifically focusing on detecting the reentrancy bug in Solidity smart contracts.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs</title>
<link>https://arxiv.org/abs/2505.19075</link>
<guid>https://arxiv.org/abs/2505.19075</guid>
<content:encoded><![CDATA[
arXiv:2505.19075v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable general capabilities, but enhancing skills such as reasoning often demands substantial computational resources and may compromise their generalization. While Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious alternative, they typically requires retraining for each LLM backbone due to architectural dependencies. To address these challenges, here we propose Universal Reasoner (UniR) - a single, lightweight, composable, and plug-and-play reasoning module that can be used with any frozen LLM to endow it with specialized reasoning capabilities. Specifically, UniR decomposes the reward into a standalone reasoning module that is trained independently using predefined rewards, effectively translating trajectory-level signals into token-level guidance. Once trained, UniR can be combined with any frozen LLM at inference time by simply adding its output logits to those of the LLM backbone. This additive structure naturally enables modular composition: multiple UniR modules trained for different tasks can be jointly applied by summing their logits, enabling complex reasoning via composition. Experimental results on mathematical reasoning and machine translation tasks show that UniR significantly outperforms \add{existing baseline fine-tuning methods using the Llama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong generalization: reasoning modules trained on smaller models effectively guide much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust solution for enhancing reasoning in LLMs without compromising their core capabilities. Code is open-sourced at https://github.com/hangeol/UniR
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Determinations Of Characteristic Redshifts From DESI-DR2 BAO and DES-SN5YR Observations: Hints For New Expansion Rate Anomalies</title>
<link>https://arxiv.org/abs/2505.19083</link>
<guid>https://arxiv.org/abs/2505.19083</guid>
<content:encoded><![CDATA[
arXiv:2505.19083v1 Announce Type: cross 
Abstract: In this work, we perform a model-agnostic reconstruction of the cosmic expansion history by combining DESI-DR2 BAO and DES-SN5YR data, with a focus on geometric determination of characteristic redshifts where notable tensions in the expansion rate are found to emerge. Employing Gaussian process regression alongside knot-based spline techniques, we reconstruct cosmic distances and their derivatives to pinpoint these characteristic redshifts and infer $E(z)$. Our analysis reveals significant deviations of approximately 4 to 5$\sigma$ from the Planck 2018 $\Lambda$CDM predictions, particularly pronounced in the redshift range $z \sim 0.35-0.55$. These anomalies are consistently observed across both reconstruction methods and combined datasets, indicating robust late-time departures that could signal new physics beyond the standard cosmological framework. The joint use of BAO and SN probes enhances the precision of our constraints, allowing us to isolate these deviations without reliance on specific cosmological assumptions. Our findings underscore the role of characteristic redshifts as sensitive indicators of expansion rate anomalies and motivate further scrutiny with forthcoming datasets from DESI-5YR BAO, Euclid, and LSST. These future surveys will tighten constraints and help distinguish whether these late-time anomalies arise from new fundamental physics or unresolved systematics in the data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jodi: Unification of Visual Generation and Understanding via Joint Modeling</title>
<link>https://arxiv.org/abs/2505.19084</link>
<guid>https://arxiv.org/abs/2505.19084</guid>
<content:encoded><![CDATA[
arXiv:2505.19084v1 Announce Type: cross 
Abstract: Visual generation and understanding are two deeply interconnected aspects of human intelligence, yet they have been traditionally treated as separate tasks in machine learning. In this paper, we propose Jodi, a diffusion framework that unifies visual generation and understanding by jointly modeling the image domain and multiple label domains. Specifically, Jodi is built upon a linear diffusion transformer along with a role switch mechanism, which enables it to perform three particular types of tasks: (1) joint generation, where the model simultaneously generates images and multiple labels; (2) controllable generation, where images are generated conditioned on any combination of labels; and (3) image perception, where multiple labels can be predicted at once from a given image. Furthermore, we present the Joint-1.6M dataset, which contains 200,000 high-quality images collected from public sources, automatic labels for 7 visual domains, and LLM-generated captions. Extensive experiments demonstrate that Jodi excels in both generation and understanding tasks and exhibits strong extensibility to a wider range of visual domains. Code is available at https://github.com/VIPL-GENUN/Jodi.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.19091</link>
<guid>https://arxiv.org/abs/2505.19091</guid>
<content:encoded><![CDATA[
arXiv:2505.19091v1 Announce Type: cross 
Abstract: Recent advancements in Large Vision-Language Models (VLMs), have greatly enhanced their capability to jointly process text and images. However, despite extensive benchmarks evaluating visual comprehension (e.g., diagrams, color schemes, OCR tasks...), there is limited assessment of VLMs' ability to read and reason about text-rich images effectively. To fill this gap, we introduce ReadBench, a multimodal benchmark specifically designed to evaluate the reading comprehension capabilities of VLMs. ReadBench transposes contexts from established text-only benchmarks into images of text while keeping textual prompts and questions intact. Evaluating leading VLMs with ReadBench, we find minimal-but-present performance degradation on short, text-image inputs, while performance sharply declines for longer, multi-page contexts. Our experiments further reveal that text resolution has negligible effects on multimodal performance. These findings highlight needed improvements in VLMs, particularly their reasoning over visually presented extensive textual content, a capability critical for practical applications. ReadBench is available at https://github.com/answerdotai/ReadBench .
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
<link>https://arxiv.org/abs/2505.19093</link>
<guid>https://arxiv.org/abs/2505.19093</guid>
<content:encoded><![CDATA[
arXiv:2505.19093v1 Announce Type: cross 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical inference for Linear Stochastic Approximation with Markovian Noise</title>
<link>https://arxiv.org/abs/2505.19102</link>
<guid>https://arxiv.org/abs/2505.19102</guid>
<content:encoded><![CDATA[
arXiv:2505.19102v1 Announce Type: cross 
Abstract: In this paper we derive non-asymptotic Berry-Esseen bounds for Polyak-Ruppert averaged iterates of the Linear Stochastic Approximation (LSA) algorithm driven by the Markovian noise. Our analysis yields $\mathcal{O}(n^{-1/4})$ convergence rates to the Gaussian limit in the Kolmogorov distance. We further establish the non-asymptotic validity of a multiplier block bootstrap procedure for constructing the confidence intervals, guaranteeing consistent inference under Markovian sampling. Our work provides the first non-asymptotic guarantees on the rate of convergence of bootstrap-based confidence intervals for stochastic approximation with Markov noise. Moreover, we recover the classical rate of order $\mathcal{O}(n^{-1/8})$ up to logarithmic factors for estimating the asymptotic variance of the iterates of the LSA algorithm.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable Representation Learning Approach for Diffusion Tensor Imaging</title>
<link>https://arxiv.org/abs/2505.19110</link>
<guid>https://arxiv.org/abs/2505.19110</guid>
<content:encoded><![CDATA[
arXiv:2505.19110v1 Announce Type: cross 
Abstract: Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the structural connectivity of the brain, but presents challenges in effective representation and interpretation in deep learning models. In this work, we propose a novel 2D representation of DTI tractography that encodes tract-level fractional anisotropy (FA) values into a 9x9 grayscale image. This representation is processed through a Beta-Total Correlation Variational Autoencoder with a Spatial Broadcast Decoder to learn a disentangled and interpretable latent embedding. We evaluate the quality of this embedding using supervised and unsupervised representation learning strategies, including auxiliary classification, triplet loss, and SimCLR-based contrastive learning. Compared to the 1D Group deep neural network (DNN) baselines, our approach improves the F1 score in a downstream sex classification task by 15.74% and shows a better disentanglement than the 3D representation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.19122</link>
<guid>https://arxiv.org/abs/2505.19122</guid>
<content:encoded><![CDATA[
arXiv:2505.19122v1 Announce Type: cross 
Abstract: Denoising diffusion models exhibit remarkable generative capabilities, but remain challenging to train due to their inherent stochasticity, where high-variance gradient estimates lead to slow convergence. Previous works have shown that magnitude preservation helps with stabilizing training in the U-net architecture. This work explores whether this effect extends to the Diffusion Transformer (DiT) architecture. As such, we propose a magnitude-preserving design that stabilizes training without normalization layers. Motivated by the goal of maintaining activation magnitudes, we additionally introduce rotation modulation, which is a novel conditioning method using learned rotations instead of traditional scaling or shifting. Through empirical evaluations and ablation studies on small-scale models, we show that magnitude-preserving strategies significantly improve performance, notably reducing FID scores by $\sim$12.8%. Further, we show that rotation modulation combined with scaling is competitive with AdaLN, while requiring $\sim$5.4% fewer parameters. This work provides insights into conditioning strategies and magnitude control. We will publicly release the implementation of our method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing High-Quality Human Annotations with Golden Questions</title>
<link>https://arxiv.org/abs/2505.19134</link>
<guid>https://arxiv.org/abs/2505.19134</guid>
<content:encoded><![CDATA[
arXiv:2505.19134v1 Announce Type: cross 
Abstract: Human-annotated data plays a vital role in training large language models (LLMs), such as supervised fine-tuning and human preference alignment. However, it is not guaranteed that paid human annotators produce high-quality data. In this paper, we study how to incentivize human annotators to do so. We start from a principal-agent model to model the dynamics between the company (the principal) and the annotator (the agent), where the principal can only monitor the annotation quality by examining $n$ samples. We investigate the maximum likelihood estimators (MLE) and the corresponding hypothesis testing to incentivize annotators: the agent is given a bonus if the MLE passes the test. By analyzing the variance of the outcome, we show that the strategic behavior of the agent makes the hypothesis testing very different from traditional ones: Unlike the exponential rate proved by the large deviation theory, the principal-agent model's hypothesis testing rate is of $\Theta(1/\sqrt{n \log n})$. Our theory implies two criteria for the \emph{golden questions} to monitor the performance of the annotators: they should be of (1) high certainty and (2) similar format to normal ones. In that light, we select a set of golden questions in human preference data. By doing incentive-compatible experiments, we find out that the annotators' behavior is better revealed by those golden questions, compared to traditional survey techniques such as instructed manipulation checks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference</title>
<link>https://arxiv.org/abs/2505.19136</link>
<guid>https://arxiv.org/abs/2505.19136</guid>
<content:encoded><![CDATA[
arXiv:2505.19136v1 Announce Type: cross 
Abstract: Uncertainty quantification (UQ) in scientific machine learning is increasingly critical as neural networks are widely adopted to tackle complex problems across diverse scientific disciplines. For physics-informed neural networks (PINNs), a prominent model in scientific machine learning, uncertainty is typically quantified using Bayesian or dropout methods. However, both approaches suffer from a fundamental limitation: the prior distribution or dropout rate required to construct honest confidence sets cannot be determined without additional information. In this paper, we propose a novel method within the framework of extended fiducial inference (EFI) to provide rigorous uncertainty quantification for PINNs. The proposed method leverages a narrow-neck hyper-network to learn the parameters of the PINN and quantify their uncertainty based on imputed random errors in the observations. This approach overcomes the limitations of Bayesian and dropout methods, enabling the construction of honest confidence sets based solely on observed data. This advancement represents a significant breakthrough for PINNs, greatly enhancing their reliability, interpretability, and applicability to real-world scientific and engineering challenges. Moreover, it establishes a new theoretical framework for EFI, extending its application to large-scale models, eliminating the need for sparse hyper-networks, and significantly improving the automaticity and robustness of statistical inference.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models (Really) Need Statistical Foundations?</title>
<link>https://arxiv.org/abs/2505.19145</link>
<guid>https://arxiv.org/abs/2505.19145</guid>
<content:encoded><![CDATA[
arXiv:2505.19145v1 Announce Type: cross 
Abstract: Large language models (LLMs) represent a new paradigm for processing unstructured data, with applications across an unprecedented range of domains. In this paper, we address, through two arguments, whether the development and application of LLMs would genuinely benefit from foundational contributions from the statistics discipline. First, we argue affirmatively, beginning with the observation that LLMs are inherently statistical models due to their profound data dependency and stochastic generation processes, where statistical insights are naturally essential for handling variability and uncertainty. Second, we argue that the persistent black-box nature of LLMs -- stemming from their immense scale, architectural complexity, and development practices often prioritizing empirical performance over theoretical interpretability -- renders closed-form or purely mechanistic analyses generally intractable, thereby necessitating statistical approaches due to their flexibility and often demonstrated effectiveness. To substantiate these arguments, the paper outlines several research areas -- including alignment, watermarking, uncertainty quantification, evaluation, and data mixture optimization -- where statistical methodologies are critically needed and are already beginning to make valuable contributions. We conclude with a discussion suggesting that statistical research concerning LLMs will likely form a diverse ``mosaic'' of specialized topics rather than deriving from a single unifying theory, and highlighting the importance of timely engagement by our statistics community in LLM research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations</title>
<link>https://arxiv.org/abs/2505.19164</link>
<guid>https://arxiv.org/abs/2505.19164</guid>
<content:encoded><![CDATA[
arXiv:2505.19164v1 Announce Type: cross 
Abstract: In the domain of sponsored search advertising, the focus of Keyphrase recommendation has largely been on exact match types, which pose issues such as high management expenses, limited targeting scope, and evolving search query patterns. Alternatives like Broad match types can alleviate certain drawbacks of exact matches but present challenges like poor targeting accuracy and minimal supervisory signals owing to limited advertiser usage. This research defines the criteria for an ideal broad match, emphasizing on both efficiency and effectiveness, ensuring that a significant portion of matched queries are relevant. We propose BroadGen, an innovative framework that recommends efficient and effective broad match keyphrases by utilizing historical search query data. Additionally, we demonstrate that BroadGen, through token correspondence modeling, maintains better query stability over time. BroadGen's capabilities allow it to serve daily, millions of sellers at eBay with over 2.3 billion items.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models</title>
<link>https://arxiv.org/abs/2505.19166</link>
<guid>https://arxiv.org/abs/2505.19166</guid>
<content:encoded><![CDATA[
arXiv:2505.19166v1 Announce Type: cross 
Abstract: We introduce JEDI, a test-time adaptation method that enhances subject separation and compositional alignment in diffusion models without requiring retraining or external supervision. JEDI operates by minimizing semantic entanglement in attention maps using a novel Jensen-Shannon divergence based objective. To improve efficiency, we leverage adversarial optimization, reducing the number of updating steps required.
  JEDI is model-agnostic and applicable to architectures such as Stable Diffusion 1.5 and 3.5, consistently improving prompt alignment and disentanglement in complex scenes. Additionally, JEDI provides a lightweight, CLIP-free disentanglement score derived from internal attention distributions, offering a principled benchmark for compositional alignment under test-time conditions. We will publicly release the implementation of our method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli</title>
<link>https://arxiv.org/abs/2505.19178</link>
<guid>https://arxiv.org/abs/2505.19178</guid>
<content:encoded><![CDATA[
arXiv:2505.19178v1 Announce Type: cross 
Abstract: Understanding the emotional impact of videos is crucial for applications in content creation, advertising, and Human-Computer Interaction (HCI). Traditional affective computing methods rely on self-reported emotions, facial expression analysis, and biosensing data, yet they often overlook the role of visual saliency -- the naturally attention-grabbing regions within a video. In this study, we utilize deep learning to introduce a novel saliency-based approach to emotion prediction by extracting two key features: saliency area and number of salient regions. Using the HD2S saliency model and OpenFace facial action unit analysis, we examine the relationship between video saliency and viewer emotions. Our findings reveal three key insights: (1) Videos with multiple salient regions tend to elicit high-valence, low-arousal emotions, (2) Videos with a single dominant salient region are more likely to induce low-valence, high-arousal responses, and (3) Self-reported emotions often misalign with facial expression-based emotion detection, suggesting limitations in subjective reporting. By leveraging saliency-driven insights, this work provides a computationally efficient and interpretable alternative for emotion modeling, with implications for content creation, personalized media experiences, and affective computing research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two LLMs debate, both are certain they've won</title>
<link>https://arxiv.org/abs/2505.19184</link>
<guid>https://arxiv.org/abs/2505.19184</guid>
<content:encoded><![CDATA[
arXiv:2505.19184v1 Announce Type: cross 
Abstract: Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed >=75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: models' private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLM outputs are deployed without careful review in assistant roles or agentic settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeakStream: Streaming Text-to-Speech with Interleaved Data</title>
<link>https://arxiv.org/abs/2505.19206</link>
<guid>https://arxiv.org/abs/2505.19206</guid>
<content:encoded><![CDATA[
arXiv:2505.19206v1 Announce Type: cross 
Abstract: The latency bottleneck of traditional text-to-speech (TTS) systems fundamentally hinders the potential of streaming large language models (LLMs) in conversational AI. These TTS systems, typically trained and inferenced on complete utterances, introduce unacceptable delays, even with optimized inference speeds, when coupled with streaming LLM outputs. This is particularly problematic for creating responsive conversational agents where low first-token latency is critical. In this paper, we present SpeakStream, a streaming TTS system that generates audio incrementally from streaming text using a decoder-only architecture. SpeakStream is trained using a next-step prediction loss on interleaved text-speech data. During inference, it generates speech incrementally while absorbing streaming input text, making it particularly suitable for cascaded conversational AI agents where an LLM streams text to a TTS system. Our experiments demonstrate that SpeakStream achieves state-of-the-art latency results in terms of first-token latency while maintaining the quality of non-streaming TTS systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding</title>
<link>https://arxiv.org/abs/2505.19219</link>
<guid>https://arxiv.org/abs/2505.19219</guid>
<content:encoded><![CDATA[
arXiv:2505.19219v1 Announce Type: cross 
Abstract: Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial intelligence and robotics, requiring the computation of collision-free paths for multiple agents navigating from their start locations to designated goals. As autonomous systems become increasingly prevalent in warehouses, urban transportation, and other complex environments, MAPF has evolved from a theoretical challenge to a critical enabler of real-world multi-robot coordination. This comprehensive survey bridges the long-standing divide between classical algorithmic approaches and emerging learning-based methods in MAPF research. We present a unified framework that encompasses search-based methods (including Conflict-Based Search, Priority-Based Search, and Large Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP formulations), and data-driven techniques (reinforcement learning, supervised learning, and hybrid strategies). Through systematic analysis of experimental practices across 200+ papers, we uncover significant disparities in evaluation methodologies, with classical methods typically tested on larger-scale instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy of evaluation metrics, environment types, and baseline selections, highlighting the need for standardized benchmarking protocols. Finally, we outline promising future directions including mixed-motive MAPF with game-theoretic considerations, language-grounded planning with large language models, and neural solver architectures that combine the rigor of classical methods with the flexibility of deep learning. This survey serves as both a comprehensive reference for researchers and a practical guide for deploying MAPF solutions in increasingly complex real-world applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models</title>
<link>https://arxiv.org/abs/2505.19240</link>
<guid>https://arxiv.org/abs/2505.19240</guid>
<content:encoded><![CDATA[
arXiv:2505.19240v1 Announce Type: cross 
Abstract: Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. In this survey, we conduct a data-driven, semi-automated review of research on limitations of LLM (LLLMs) from 2022 to 2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we identify 14,648 relevant papers using keyword filtering, LLM-based classification, validated against expert labels, and topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs research grows even faster, reaching over 30% of LLM papers by late 2024. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2024. We release a dataset of annotated abstracts and a validated methodology, and offer a quantitative view of trends in LLM limitations research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Augmented Online Bipartite Fractional Matching</title>
<link>https://arxiv.org/abs/2505.19252</link>
<guid>https://arxiv.org/abs/2505.19252</guid>
<content:encoded><![CDATA[
arXiv:2505.19252v1 Announce Type: cross 
Abstract: Online bipartite matching is a fundamental problem in online optimization, extensively studied both in its integral and fractional forms due to its theoretical significance and practical applications, such as online advertising and resource allocation. Motivated by recent progress in learning-augmented algorithms, we study online bipartite fractional matching when the algorithm is given advice in the form of a suggested matching in each iteration. We develop algorithms for both the vertex-weighted and unweighted variants that provably dominate the naive "coin flip" strategy of randomly choosing between the advice-following and advice-free algorithms. Moreover, our algorithm for the vertex-weighted setting extends to the AdWords problem under the small bids assumption, yielding a significant improvement over the seminal work of Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our positive results, we establish a hardness bound on the robustness-consistency tradeoff that is attainable by any algorithm. We empirically validate our algorithms through experiments on synthetic and real-world data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2505.19286</link>
<guid>https://arxiv.org/abs/2505.19286</guid>
<content:encoded><![CDATA[
arXiv:2505.19286v1 Announce Type: cross 
Abstract: Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?</title>
<link>https://arxiv.org/abs/2505.19293</link>
<guid>https://arxiv.org/abs/2505.19293</guid>
<content:encoded><![CDATA[
arXiv:2505.19293v1 Announce Type: cross 
Abstract: Long-context capability is considered one of the most important abilities of LLMs, as a truly long context-capable LLM enables users to effortlessly process many originally exhausting tasks -- e.g., digesting a long-form document to find answers vs. directly asking an LLM about it. However, existing real-task-based long-context evaluation benchmarks have two major shortcomings. First, benchmarks like LongBench often do not provide proper metrics to separate long-context performance from the model's baseline ability, making cross-model comparison unclear. Second, such benchmarks are usually constructed with fixed input lengths, which limits their applicability across different models and fails to reveal when a model begins to break down. To address these issues, we introduce a length-controllable long-context benchmark and a novel metric that disentangles baseline knowledge from true long-context capabilities. Experiments demonstrate the superiority of our approach in effectively evaluating LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Single Images to Motion Policies via Video-Generation Environment Representations</title>
<link>https://arxiv.org/abs/2505.19306</link>
<guid>https://arxiv.org/abs/2505.19306</guid>
<content:encoded><![CDATA[
arXiv:2505.19306v1 Announce Type: cross 
Abstract: Autonomous robots typically need to construct representations of their surroundings and adapt their motions to the geometry of their environment. Here, we tackle the problem of constructing a policy model for collision-free motion generation, consistent with the environment, from a single input RGB image. Extracting 3D structures from a single image often involves monocular depth estimation. Developments in depth estimation have given rise to large pre-trained models such as DepthAnything. However, using outputs of these models for downstream motion generation is challenging due to frustum-shaped errors that arise. Instead, we propose a framework known as Video-Generation Environment Representation (VGER), which leverages the advances of large-scale video generation models to generate a moving camera video conditioned on the input image. Frames of this video, which form a multiview dataset, are then input into a pre-trained 3D foundation model to produce a dense point cloud. We then introduce a multi-scale noise approach to train an implicit representation of the environment structure and build a motion generation model that complies with the geometry of the representation. We extensively evaluate VGER over a diverse set of indoor and outdoor environments. We demonstrate its ability to produce smooth motions that account for the captured geometry of a scene, all from a single RGB input image.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractional-Boundary-Regularized Deep Galerkin Method for Variational Inequalities in Mixed Optimal Stopping and Control</title>
<link>https://arxiv.org/abs/2505.19309</link>
<guid>https://arxiv.org/abs/2505.19309</guid>
<content:encoded><![CDATA[
arXiv:2505.19309v1 Announce Type: cross 
Abstract: Mixed optimal stopping and stochastic control problems define variational inequalities with non-linear Hamilton-Jacobi-Bellman (HJB) operators, whose numerical solution is notoriously difficult and lack of reliable benchmarks. We first use the dual approach to transform it into a linear operator, and then introduce a Fractional-Boundary-Regularized Deep Galerkin Method (FBR-DGM) that augments the classical $L^2$ loss with Sobolev-Slobodeckij norms on the parabolic boundary, enforcing regularity and yielding consistent improvements in the network approximation and its derivatives. The improved accuracy allows the network to be converted back to the original solution using the dual transform. The self-consistency and stability of the network can be tested by checking the primal-dual relationship among optimal value, optimal wealth, and optimal control, offering innovative benchmarks in the absence of analytical solutions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demand Selection for VRP with Emission Quota</title>
<link>https://arxiv.org/abs/2505.19315</link>
<guid>https://arxiv.org/abs/2505.19315</guid>
<content:encoded><![CDATA[
arXiv:2505.19315v1 Announce Type: cross 
Abstract: Combinatorial optimization (CO) problems are traditionally addressed using Operations Research (OR) methods, including metaheuristics. In this study, we introduce a demand selection problem for the Vehicle Routing Problem (VRP) with an emission quota, referred to as QVRP. The objective is to minimize the number of omitted deliveries while respecting the pollution quota. We focus on the demand selection part, called Maximum Feasible Vehicle Assignment (MFVA), while the construction of a routing for the VRP instance is solved using classical OR methods. We propose several methods for selecting the packages to omit, both from machine learning (ML) and OR. Our results show that, in this static problem setting, classical OR-based methods consistently outperform ML-based approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics</title>
<link>https://arxiv.org/abs/2505.19317</link>
<guid>https://arxiv.org/abs/2505.19317</guid>
<content:encoded><![CDATA[
arXiv:2505.19317v1 Announce Type: cross 
Abstract: Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed way to conceptualize and evaluate Effort-aware Fairness (EaF) based on the concept of Force, or temporal trajectory of predictive features coupled with inertia. In addition to our theoretical formulation of EaF metrics, our empirical contributions include: 1/ a pre-registered human subjects experiment, which demonstrates that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; 2/ pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who spent significant efforts to improve but are still stuck with systemic/early-life disadvantages outside their control.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIGPVAE: Physics-Informed Gaussian Process Variational Autoencoders</title>
<link>https://arxiv.org/abs/2505.19320</link>
<guid>https://arxiv.org/abs/2505.19320</guid>
<content:encoded><![CDATA[
arXiv:2505.19320v1 Announce Type: cross 
Abstract: Recent advances in generative AI offer promising solutions for synthetic data generation but often rely on large datasets for effective training. To address this limitation, we propose a novel generative model that learns from limited data by incorporating physical constraints to enhance performance. Specifically, we extend the VAE architecture by incorporating physical models in the generative process, enabling it to capture underlying dynamics more effectively. While physical models provide valuable insights, they struggle to capture complex temporal dependencies present in real-world data. To bridge this gap, we introduce a discrepancy term to account for unmodeled dynamics, represented within a latent Gaussian Process VAE (GPVAE). Furthermore, we apply regularization to ensure the generated data aligns closely with observed data, enhancing both the diversity and accuracy of the synthetic samples. The proposed method is applied to indoor temperature data, achieving state-of-the-art performance. Additionally, we demonstrate that PIGPVAE can produce realistic samples beyond the observed distribution, highlighting its robustness and usefulness under distribution shifts.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change</title>
<link>https://arxiv.org/abs/2505.19328</link>
<guid>https://arxiv.org/abs/2505.19328</guid>
<content:encoded><![CDATA[
arXiv:2505.19328v1 Announce Type: cross 
Abstract: Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can play a critical role in the personalization and effectiveness of digital behaviour change interventions. These subtle and conflicting emotions are manifested by a discord between multiple modalities, such as facial and vocal expressions, and body language. Although experts can be trained to identify A/H, integrating them into digital interventions is costly and less effective. Automatic learning systems provide a cost-effective alternative that can adapt to individual users, and operate seamlessly within real-time, and resource-limited environments. However, there are currently no datasets available for the design of ML models to recognize A/H. This paper introduces a first Behavioural Ambivalence/Hesitancy (BAH) dataset collected for subject-based multimodal recognition of A/H in videos. It contains videos from 224 participants captured across 9 provinces in Canada, with different age, and ethnicity. Through our web platform, we recruited participants to answer 7 questions, some of which were designed to elicit A/H while recording themselves via webcam with microphone. BAH amounts to 1,118 videos for a total duration of 8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp segments to indicate where A/H occurs, and provide frame- and video-level annotations with the A/H cues. Video transcripts and their timestamps are also included, along with cropped and aligned faces in each frame, and a variety of participants meta-data. We include results baselines for BAH at frame- and video-level recognition in multi-modal setups, in addition to zero-shot prediction, and for personalization using unsupervised domain adaptation. The limited performance of baseline models highlights the challenges of recognizing A/H in real-world videos. The data, code, and pretrained weights are available.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashMD: long-stride, universal prediction of molecular dynamics</title>
<link>https://arxiv.org/abs/2505.19350</link>
<guid>https://arxiv.org/abs/2505.19350</guid>
<content:encoded><![CDATA[
arXiv:2505.19350v1 Announce Type: cross 
Abstract: Molecular dynamics (MD) provides insights into atomic-scale processes by integrating over time the equations that describe the motion of atoms under the action of interatomic forces. Machine learning models have substantially accelerated MD by providing inexpensive predictions of the forces, but they remain constrained to minuscule time integration steps, which are required by the fast time scale of atomic motion. In this work, we propose FlashMD, a method to predict the evolution of positions and momenta over strides that are between one and two orders of magnitude longer than typical MD time steps. We incorporate considerations on the mathematical and physical properties of Hamiltonian dynamics in the architecture, generalize the approach to allow the simulation of any thermodynamic ensemble, and carefully assess the possible failure modes of such a long-stride MD approach. We validate FlashMD's accuracy in reproducing equilibrium and time-dependent properties, using both system-specific and general-purpose models, extending the ability of MD simulation to reach the long time scales needed to model microscopic processes of high scientific and technological relevance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval</title>
<link>https://arxiv.org/abs/2505.19356</link>
<guid>https://arxiv.org/abs/2505.19356</guid>
<content:encoded><![CDATA[
arXiv:2505.19356v1 Announce Type: cross 
Abstract: Neural retrieval methods using transformer-based pre-trained language models have advanced multilingual and cross-lingual retrieval. However, their effectiveness for low-resource, morphologically rich languages such as Amharic remains underexplored due to data scarcity and suboptimal tokenization. We address this gap by introducing Amharic-specific dense retrieval models based on pre-trained Amharic BERT and RoBERTa backbones. Our proposed RoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative improvement in MRR@10 and a 9.86% gain in Recall@10 over the strongest multilingual baseline, Arctic Embed 2.0 (568M parameters). More compact variants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while being over 13x smaller. Additionally, we train a ColBERT-based late interaction retrieval model that achieves the highest MRR@10 score (0.843) among all evaluated models. We benchmark our proposed models against both sparse and dense retrieval baselines to systematically assess retrieval effectiveness in Amharic. Our analysis highlights key challenges in low-resource settings and underscores the importance of language-specific adaptation. To foster future research in low-resource IR, we publicly release our dataset, codebase, and trained models at https://github.com/kidist-amde/amharic-ir-benchmarks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments</title>
<link>https://arxiv.org/abs/2505.19361</link>
<guid>https://arxiv.org/abs/2505.19361</guid>
<content:encoded><![CDATA[
arXiv:2505.19361v1 Announce Type: cross 
Abstract: The deployment of pre-trained perception models in novel environments often leads to performance degradation due to distributional shifts. Although recent artificial intelligence approaches for metacognition use logical rules to characterize and filter model errors, improving precision often comes at the cost of reduced recall. This paper addresses the hypothesis that leveraging multiple pre-trained models can mitigate this recall reduction. We formulate the challenge of identifying and managing conflicting predictions from various models as a consistency-based abduction problem. The input predictions and the learned error detection rules derived from each model are encoded in a logic program. We then seek an abductive explanation--a subset of model predictions--that maximizes prediction coverage while ensuring the rate of logical inconsistencies (derived from domain constraints) remains below a specified threshold. We propose two algorithms for this knowledge representation task: an exact method based on Integer Programming (IP) and an efficient Heuristic Search (HS). Through extensive experiments on a simulated aerial imagery dataset featuring controlled, complex distributional shifts, we demonstrate that our abduction-based framework outperforms individual models and standard ensemble baselines, achieving, for instance, average relative improvements of approximately 13.6% in F1-score and 16.6% in accuracy across 15 diverse test datasets when compared to the best individual model. Our results validate the use of consistency-based abduction as an effective mechanism to robustly integrate knowledge from multiple imperfect reasoners in challenging, novel scenarios.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Diffusion Guidance via Stochastic Optimal Control</title>
<link>https://arxiv.org/abs/2505.19367</link>
<guid>https://arxiv.org/abs/2505.19367</guid>
<content:encoded><![CDATA[
arXiv:2505.19367v1 Announce Type: cross 
Abstract: Guidance is a cornerstone of modern diffusion models, playing a pivotal role in conditional generation and enhancing the quality of unconditional samples. However, current approaches to guidance scheduling--determining the appropriate guidance weight--are largely heuristic and lack a solid theoretical foundation. This work addresses these limitations on two fronts. First, we provide a theoretical formalization that precisely characterizes the relationship between guidance strength and classifier confidence. Second, building on this insight, we introduce a stochastic optimal control framework that casts guidance scheduling as an adaptive optimization problem. In this formulation, guidance strength is not fixed but dynamically selected based on time, the current sample, and the conditioning class, either independently or in combination. By solving the resulting control problem, we establish a principled foundation for more effective guidance in diffusion models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundations of Top-$k$ Decoding For Language Models</title>
<link>https://arxiv.org/abs/2505.19371</link>
<guid>https://arxiv.org/abs/2505.19371</guid>
<content:encoded><![CDATA[
arXiv:2505.19371v1 Announce Type: cross 
Abstract: Top-$k$ decoding is a widely used method for sampling from LLMs: at each token, only the largest $k$ next-token-probabilities are kept, and the next token is sampled after re-normalizing them to sum to unity. Top-$k$ and other sampling methods are motivated by the intuition that true next-token distributions are sparse, and the noisy LLM probabilities need to be truncated. However, to our knowledge, a precise theoretical motivation for the use of top-$k$ decoding is missing. In this work, we develop a theoretical framework that both explains and generalizes top-$k$ decoding. We view decoding at a fixed token as the recovery of a sparse probability distribution. We consider \emph{Bregman decoders} obtained by minimizing a separable Bregman divergence (for both the \emph{primal} and \emph{dual} cases) with a sparsity-inducing $\ell_0$ regularization. Despite the combinatorial nature of the objective, we show how to optimize it efficiently for a large class of divergences. We show that the optimal decoding strategies are greedy, and further that the loss function is discretely convex in $k$, so that binary search provably and efficiently finds the optimal $k$. We show that top-$k$ decoding arises as a special case for the KL divergence, and identify new decoding strategies that have distinct behaviors (e.g., non-linearly up-weighting larger probabilities after re-normalization).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uniform convergence of the smooth calibration error and its relationship with functional gradient</title>
<link>https://arxiv.org/abs/2505.19396</link>
<guid>https://arxiv.org/abs/2505.19396</guid>
<content:encoded><![CDATA[
arXiv:2505.19396v1 Announce Type: cross 
Abstract: Calibration is a critical requirement for reliable probabilistic prediction, especially in high-risk applications. However, the theoretical understanding of which learning algorithms can simultaneously achieve high accuracy and good calibration remains limited, and many existing studies provide empirical validation or a theoretical guarantee in restrictive settings. To address this issue, in this work, we focus on the smooth calibration error (CE) and provide a uniform convergence bound, showing that the smooth CE is bounded by the sum of the smooth CE over the training dataset and a generalization gap. We further prove that the functional gradient of the loss function can effectively control the training smooth CE. Based on this framework, we analyze three representative algorithms: gradient boosting trees, kernel boosting, and two-layer neural networks. For each, we derive conditions under which both classification and calibration performances are simultaneously guaranteed. Our results offer new theoretical insights and practical guidance for designing reliable probabilistic models with provable calibration guarantees.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study</title>
<link>https://arxiv.org/abs/2505.19414</link>
<guid>https://arxiv.org/abs/2505.19414</guid>
<content:encoded><![CDATA[
arXiv:2505.19414v1 Announce Type: cross 
Abstract: Data centers are the backbone of computing capacity. Operating data centers in the tropical regions faces unique challenges due to consistently high ambient temperature and elevated relative humidity throughout the year. These conditions result in increased cooling costs to maintain the reliability of the computing systems. While existing machine learning-based approaches have demonstrated potential to elevate operations to a more proactive and intelligent level, their deployment remains dubious due to concerns about model extrapolation capabilities and associated system safety issues. To address these concerns, this article proposes incorporating the physical characteristics of data centers into traditional data-driven machine learning solutions. We begin by introducing the data center system, including the relevant multiphysics processes and the data-physics availability. Next, we outline the associated modeling and optimization problems and propose an integrated, physics-informed machine learning system to address them. Using the proposed system, we present relevant applications across varying levels of operational intelligence. A case study on an industry-grade tropical data center is provided to demonstrate the effectiveness of our approach. Finally, we discuss key challenges and highlight potential future directions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation</title>
<link>https://arxiv.org/abs/2505.19425</link>
<guid>https://arxiv.org/abs/2505.19425</guid>
<content:encoded><![CDATA[
arXiv:2505.19425v1 Announce Type: cross 
Abstract: The rapid advancement of diffusion models has enhanced their image inpainting and editing capabilities but also introduced significant societal risks. Adversaries can exploit user images from social media to generate misleading or harmful content. While adversarial perturbations can disrupt inpainting, global perturbation-based methods fail in mask-guided editing tasks due to spatial constraints. To address these challenges, we propose Structure Disruption Attack (SDA), a powerful protection framework for safeguarding sensitive image regions against inpainting-based editing. Building upon the contour-focused nature of self-attention mechanisms of diffusion models, SDA optimizes perturbations by disrupting queries in self-attention during the initial denoising step to destroy the contour generation process. This targeted interference directly disrupts the structural generation capability of diffusion models, effectively preventing them from producing coherent images. We validate our motivation through visualization techniques and extensive experiments on public datasets, demonstrating that SDA achieves state-of-the-art (SOTA) protection performance while maintaining strong robustness.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Role of Diversity in In-Context Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.19426</link>
<guid>https://arxiv.org/abs/2505.19426</guid>
<content:encoded><![CDATA[
arXiv:2505.19426v1 Announce Type: cross 
Abstract: In-context learning (ICL) is a crucial capability of current large language models (LLMs), where the selection of examples plays a key role in performance. While most existing approaches focus on selecting the most similar examples to the query, the impact of diversity in example selection remains underexplored. We systematically investigate the role of diversity in in-context example selection through experiments across a range of tasks, from sentiment classification to more challenging math and code problems. Experiments on Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that diversity-aware selection methods improve performance, particularly on complex tasks like math and code, and enhance robustness to out-of-distribution queries. To support these findings, we introduce a theoretical framework that explains the benefits of incorporating diversity in in-context example selection.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models</title>
<link>https://arxiv.org/abs/2505.19440</link>
<guid>https://arxiv.org/abs/2505.19440</guid>
<content:encoded><![CDATA[
arXiv:2505.19440v1 Announce Type: cross 
Abstract: This paper studies the emergence of interpretable categorical features within large language models (LLMs), analyzing their behavior across training checkpoints (time), transformer layers (space), and varying model sizes (scale). Using sparse autoencoders for mechanistic interpretability, we identify when and where specific semantic concepts emerge within neural activations. Results indicate clear temporal and scale-specific thresholds for feature emergence across multiple domains. Notably, spatial analysis reveals unexpected semantic reactivation, with early-layer features re-emerging at later layers, challenging standard assumptions about representational dynamics in transformer models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems</title>
<link>https://arxiv.org/abs/2505.19441</link>
<guid>https://arxiv.org/abs/2505.19441</guid>
<content:encoded><![CDATA[
arXiv:2505.19441v1 Announce Type: cross 
Abstract: The rapid proliferation of recommender systems necessitates robust fairness practices to address inherent biases. Assessing fairness, though, is challenging due to constantly evolving metrics and best practices. This paper analyzes how industry practitioners perceive and incorporate these changing fairness standards in their workflows. Through semi-structured interviews with 11 practitioners from technical teams across a range of large technology companies, we investigate industry implementations of fairness in recommendation system products. We focus on current debiasing practices, applied metrics, collaborative strategies, and integrating academic research into practice. Findings show a preference for multi-dimensional debiasing over traditional demographic methods, and a reliance on intuitive rather than academic metrics. This study also highlights the difficulties in balancing fairness with both the practitioner's individual (bottom-up) roles and organizational (top-down) workplace constraints, including the interplay with legal and compliance experts. Finally, we offer actionable recommendations for the recommender system community and algorithmic fairness practitioners, underlining the need to refine fairness practices continually.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.19455</link>
<guid>https://arxiv.org/abs/2505.19455</guid>
<content:encoded><![CDATA[
arXiv:2505.19455v1 Announce Type: cross 
Abstract: Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs) has achieved promising progress by leveraging prompt tuning to enable continual multi-modal learning. However, most existing methods adopt cross-modal prompt isolation, constructing visual and textual prompts separately, which exacerbates modality imbalance and leads to degraded performance over time. To tackle this issue, we propose MM-Prompt, a novel framework incorporating cross-modal prompt query and cross-modal prompt recovery. The former enables balanced prompt selection by incorporating cross-modal signals during query formation, while the latter promotes joint prompt reconstruction through iterative cross-modal interactions, guided by an alignment loss to prevent representational drift. Extensive experiments show that MM-Prompt surpasses prior approaches in accuracy and knowledge retention, while maintaining balanced modality engagement throughout continual learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs</title>
<link>https://arxiv.org/abs/2505.19466</link>
<guid>https://arxiv.org/abs/2505.19466</guid>
<content:encoded><![CDATA[
arXiv:2505.19466v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to advance, their deployment often involves fine-tuning to enhance performance on specific downstream tasks. However, this customization is sometimes accompanied by misleading claims about the origins, raising significant concerns about transparency and trust within the open-source community. Existing model verification techniques typically assess functional, representational, and weight similarities. However, these approaches often struggle against obfuscation techniques, such as permutations and scaling transformations. To address this limitation, we propose a novel detection method Origin-Tracer that rigorously determines whether a model has been fine-tuned from a specified base model. This method includes the ability to extract the LoRA rank utilized during the fine-tuning process, providing a more robust verification framework. This framework is the first to provide a formalized approach specifically aimed at pinpointing the sources of model fine-tuning. We empirically validated our method on thirty-one diverse open-source models under conditions that simulate real-world obfuscation scenarios. We empirically analyze the effectiveness of our framework and finally, discuss its limitations. The results demonstrate the effectiveness of our approach and indicate its potential to establish new benchmarks for model verification.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables</title>
<link>https://arxiv.org/abs/2505.19470</link>
<guid>https://arxiv.org/abs/2505.19470</guid>
<content:encoded><![CDATA[
arXiv:2505.19470v1 Announce Type: cross 
Abstract: Latent variables (LVs) play a crucial role in encoder-decoder models by enabling effective data compression, prediction, and generation. Although their theoretical properties, such as generalization, have been extensively studied in supervised learning, similar analyses for unsupervised models such as variational autoencoders (VAEs) remain insufficiently underexplored. In this work, we extend information-theoretic generalization analysis to vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel data-dependent prior to rigorously analyze the relationship among LVs, generalization, and data generation. We derive a novel generalization error bound of the reconstruction loss of VQ-VAEs, which depends solely on the complexity of LVs and the encoder, independent of the decoder. Additionally, we provide the upper bound of the 2-Wasserstein distance between the distributions of the true data and the generated data, explaining how the regularization of the LVs contributes to the data generation performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach</title>
<link>https://arxiv.org/abs/2505.19479</link>
<guid>https://arxiv.org/abs/2505.19479</guid>
<content:encoded><![CDATA[
arXiv:2505.19479v1 Announce Type: cross 
Abstract: Over 8,024 wildfire incidents have been documented in 2024 alone, affecting thousands of fatalities and significant damage to infrastructure and ecosystems. Wildfires in the United States have inflicted devastating losses. Wildfires are becoming more frequent and intense, which highlights how urgently efficient warning systems are needed to avoid disastrous outcomes. The goal of this study is to enhance the accuracy of wildfire detection by using Convolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE dataset, which includes several kinds of wildfire and non-wildfire images, was employed in the study. Low-resolution images, dataset imbalance, and the necessity for real-time applicability are some of the main challenges. These problems were resolved by enriching the dataset using data augmentation techniques and optimizing the VGG16 model for binary classification. The model produced a low false negative rate, which is essential for reducing unexplored fires, despite dataset boundaries. In order to help authorities execute fast responses, this work shows that deep learning models such as VGG16 can offer a reliable, automated approach for early wildfire recognition. For the purpose of reducing the impact of wildfires, our future work will concentrate on connecting to systems with real-time surveillance networks and enlarging the dataset to cover more varied fire situations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning</title>
<link>https://arxiv.org/abs/2505.19486</link>
<guid>https://arxiv.org/abs/2505.19486</guid>
<content:encoded><![CDATA[
arXiv:2505.19486v1 Announce Type: cross 
Abstract: Traffic signal control (TSC) is a core challenge in urban mobility, where real-time decisions must balance efficiency and safety. Existing methods - ranging from rule-based heuristics to reinforcement learning (RL) - often struggle to generalize to complex, dynamic, and safety-critical scenarios. We introduce VLMLight, a novel TSC framework that integrates vision-language meta-control with dual-branch reasoning. At the core of VLMLight is the first image-based traffic simulator that enables multi-view visual perception at intersections, allowing policies to reason over rich cues such as vehicle type, motion, and spatial density. A large language model (LLM) serves as a safety-prioritized meta-controller, selecting between a fast RL policy for routine traffic and a structured reasoning branch for critical cases. In the latter, multiple LLM agents collaborate to assess traffic phases, prioritize emergency vehicles, and verify rule compliance. Experiments show that VLMLight reduces waiting times for emergency vehicles by up to 65% over RL-only systems, while preserving real-time performance in standard conditions with less than 1% degradation. VLMLight offers a scalable, interpretable, and safety-aware solution for next-generation traffic signal control.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Machine Translation with Visual Scene Graph Pruning</title>
<link>https://arxiv.org/abs/2505.19507</link>
<guid>https://arxiv.org/abs/2505.19507</guid>
<content:encoded><![CDATA[
arXiv:2505.19507v1 Announce Type: cross 
Abstract: Multimodal machine translation (MMT) seeks to address the challenges posed by linguistic polysemy and ambiguity in translation tasks by incorporating visual information. A key bottleneck in current MMT research is the effective utilization of visual data. Previous approaches have focused on extracting global or region-level image features and using attention or gating mechanisms for multimodal information fusion. However, these methods have not adequately tackled the issue of visual information redundancy in MMT, nor have they proposed effective solutions. In this paper, we introduce a novel approach--multimodal machine translation with visual Scene Graph Pruning (PSG), which leverages language scene graph information to guide the pruning of redundant nodes in visual scene graphs, thereby reducing noise in downstream translation tasks. Through extensive comparative experiments with state-of-the-art methods and ablation studies, we demonstrate the effectiveness of the PSG model. Our results also highlight the promising potential of visual information pruning in advancing the field of MMT.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback</title>
<link>https://arxiv.org/abs/2505.19514</link>
<guid>https://arxiv.org/abs/2505.19514</guid>
<content:encoded><![CDATA[
arXiv:2505.19514v1 Announce Type: cross 
Abstract: Prompt quality plays a critical role in the performance of large language models (LLMs), motivating a growing body of work on prompt optimization. Most existing methods optimize prompts over a fixed dataset, assuming static input distributions and offering limited support for iterative improvement. We introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process. SIPDO couples a synthetic data generator with a prompt optimizer, where the generator produces new examples that reveal current prompt weaknesses and the optimizer incrementally refines the prompt in response. This feedback-driven loop enables systematic improvement of prompt performance without assuming access to external supervision or new tasks. Experiments across question answering and reasoning benchmarks show that SIPDO outperforms standard prompt tuning methods, highlighting the value of integrating data synthesis into prompt learning workflows.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures</title>
<link>https://arxiv.org/abs/2505.19521</link>
<guid>https://arxiv.org/abs/2505.19521</guid>
<content:encoded><![CDATA[
arXiv:2505.19521v1 Announce Type: cross 
Abstract: Learning unknown dynamics under environmental (or external) constraints is fundamental to many fields (e.g., modern robotics), particularly challenging when constraint information is only locally available and uncertain. Existing approaches requiring global constraints or using probabilistic filtering fail to fully exploit the geometric structure inherent in local measurements (by using, e.g., sensors) and constraints. This paper presents a geometric framework unifying measurements, constraints, and dynamics learning through a fiber bundle structure over the state space. This naturally induced geometric structure enables measurement-aware Control Barrier Functions that adapt to local sensing (or measurement) conditions. By integrating Neural ODEs, our framework learns continuous-time dynamics while preserving geometric constraints, with theoretical guarantees of learning convergence and constraint satisfaction dependent on sensing quality. The geometric framework not only enables efficient dynamics learning but also suggests promising directions for integration with reinforcement learning approaches. Extensive simulations demonstrate significant improvements in both learning efficiency and constraint satisfaction over traditional methods, especially under limited and uncertain sensing conditions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.19522</link>
<guid>https://arxiv.org/abs/2505.19522</guid>
<content:encoded><![CDATA[
arXiv:2505.19522v1 Announce Type: cross 
Abstract: In recent years, image classification, as a core task in computer vision, relies on high-quality labelled data, which restricts the wide application of deep learning models in practical scenarios. To alleviate the problem of insufficient labelled samples, semi-supervised learning has gradually become a research hotspot. In this paper, we construct a semi-supervised image classification model based on Generative Adversarial Networks (GANs), and through the introduction of the collaborative training mechanism of generators, discriminators and classifiers, we achieve the effective use of limited labelled data and a large amount of unlabelled data, improve the quality of image generation and classification accuracy, and provide an effective solution for the task of image recognition in complex environments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Multi-Step Audio Source Separation</title>
<link>https://arxiv.org/abs/2505.19534</link>
<guid>https://arxiv.org/abs/2505.19534</guid>
<content:encoded><![CDATA[
arXiv:2505.19534v1 Announce Type: cross 
Abstract: Audio source separation aims to separate a mixture into target sources. Previous audio source separation systems usually conduct one-step inference, which does not fully explore the separation ability of models. In this work, we reveal that pretrained one-step audio source separation models can be leveraged for multi-step separation without additional training. We propose a simple yet effective inference method that iteratively applies separation by optimally blending the input mixture with the previous step's separation result. At each step, we determine the optimal blending ratio by maximizing a metric. We prove that our method always yield improvement over one-step inference, provide error bounds based on model smoothness and metric robustness, and provide theoretical analysis connecting our method to denoising along linear interpolation paths between noise and clean distributions, a property we link to denoising diffusion bridge models. Our approach effectively delivers improved separation performance as a "free lunch" from existing models. Our empirical results demonstrate that our multi-step separation approach consistently outperforms one-step inference across both speech enhancement and music source separation tasks, and can achieve scaling performance similar to training a larger model, using more data, or in some cases employing a multi-step training objective. These improvements appear not only on the optimization metric during multi-step inference, but also extend to nearly all non-optimized metrics (with one exception). We also discuss limitations of our approach and directions for future research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games</title>
<link>https://arxiv.org/abs/2505.19537</link>
<guid>https://arxiv.org/abs/2505.19537</guid>
<content:encoded><![CDATA[
arXiv:2505.19537v1 Announce Type: cross 
Abstract: Since Polyak's pioneering work, heavy ball (HB) momentum has been widely studied in minimization. However, its role in min-max games remains largely unexplored. As a key component of practical min-max algorithms like Adam, this gap limits their effectiveness. In this paper, we present a continuous-time analysis for HB with simultaneous and alternating update schemes in min-max games. Locally, we prove smaller momentum enhances algorithmic stability by enabling local convergence across a wider range of step sizes, with alternating updates generally converging faster. Globally, we study the implicit regularization of HB, and find smaller momentum guides algorithms trajectories towards shallower slope regions of the loss landscapes, with alternating updates amplifying this effect. Surprisingly, all these phenomena differ from those observed in minimization, where larger momentum yields similar effects. Our results reveal fundamental differences between HB in min-max games and minimization, and numerical experiments further validate our theoretical results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach</title>
<link>https://arxiv.org/abs/2505.19544</link>
<guid>https://arxiv.org/abs/2505.19544</guid>
<content:encoded><![CDATA[
arXiv:2505.19544v1 Announce Type: cross 
Abstract: In this paper, we focus on the often-overlooked issue of embedding collapse in existing diffusion-based sequential recommendation models and propose ADRec, an innovative framework designed to mitigate this problem. Diverging from previous diffusion-based methods, ADRec applies an independent noise process to each token and performs diffusion across the entire target sequence during training. ADRec captures token interdependency through auto-regression while modeling per-token distributions through token-level diffusion. This dual approach enables the model to effectively capture both sequence dynamics and item representations, overcoming the limitations of existing methods. To further mitigate embedding collapse, we propose a three-stage training strategy: (1) pre-training the embedding weights, (2) aligning these weights with the ADRec backbone, and (3) fine-tuning the model. During inference, ADRec applies the denoising process only to the last token, ensuring that the meaningful patterns in historical interactions are preserved. Our comprehensive empirical evaluation across six datasets underscores the effectiveness of ADRec in enhancing both the accuracy and efficiency of diffusion-based sequential recommendation systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding</title>
<link>https://arxiv.org/abs/2505.19558</link>
<guid>https://arxiv.org/abs/2505.19558</guid>
<content:encoded><![CDATA[
arXiv:2505.19558v1 Announce Type: cross 
Abstract: Achieving political consensus is crucial yet challenging for the effective functioning of social governance. However, although frontier AI systems represented by large language models (LLMs) have developed rapidly in recent years, their capabilities on this scope are still understudied. In this paper, we introduce EuroCon, a novel benchmark constructed from 2,225 high-quality deliberation records of the European Parliament over 13 years, ranging from 2009 to 2022, to evaluate the ability of LLMs to reach political consensus among divergent party positions across diverse parliament settings. Specifically, EuroCon incorporates four factors to build each simulated parliament setting: specific political issues, political goals, participating parties, and power structures based on seat distribution. We also develop an evaluation framework for EuroCon to simulate real voting outcomes in different parliament settings, assessing whether LLM-generated resolutions meet predefined political goals. Our experimental results demonstrate that even state-of-the-art models remain undersatisfied with complex tasks like passing resolutions by a two-thirds majority and addressing security issues, while revealing some common strategies LLMs use to find consensus under different power structures, such as prioritizing the stance of the dominant party, highlighting EuroCon's promise as an effective platform for studying LLMs' ability to find political consensus.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model</title>
<link>https://arxiv.org/abs/2505.19568</link>
<guid>https://arxiv.org/abs/2505.19568</guid>
<content:encoded><![CDATA[
arXiv:2505.19568v1 Announce Type: cross 
Abstract: Maritime transportation is the backbone of global trade, making ship inspection essential for ensuring maritime safety and environmental protection. Port State Control (PSC), conducted by national ports, enforces compliance with safety regulations, with ship detention being the most severe consequence, impacting both ship schedules and company reputations. Traditional machine learning methods for ship detention prediction are limited by the capacity of representation learning and thus suffer from low accuracy. Meanwhile, autoencoder-based deep learning approaches face challenges due to the severe data imbalance in learning historical PSC detention records. To address these limitations, we propose Maritime Ship Detention with Large Language Models (MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based autoencoder with a progressive learning pipeline to handle imbalanced data and extract meaningful PSC representations. Then, a large language model groups and ranks features to identify likely detention cases, enabling dynamic thresholding for flexible detention predictions. Extensive evaluations on 31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM outperforms state-of-the-art methods more than 12\% on Area Under the Curve (AUC) for Singapore ports. Additionally, it demonstrates robustness to real-world challenges, making it adaptable to diverse maritime risk assessment scenarios.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Situationally-Aware Dynamics Learning</title>
<link>https://arxiv.org/abs/2505.19574</link>
<guid>https://arxiv.org/abs/2505.19574</guid>
<content:encoded><![CDATA[
arXiv:2505.19574v1 Announce Type: cross 
Abstract: Autonomous robots operating in complex, unstructured environments face significant challenges due to latent, unobserved factors that obscure their understanding of both their internal state and the external world. Addressing this challenge would enable robots to develop a more profound grasp of their operational context. To tackle this, we propose a novel framework for online learning of hidden state representations, with which the robots can adapt in real-time to uncertain and dynamic conditions that would otherwise be ambiguous and result in suboptimal or erroneous behaviors. Our approach is formalized as a Generalized Hidden Parameter Markov Decision Process, which explicitly models the influence of unobserved parameters on both transition dynamics and reward structures. Our core innovation lies in learning online the joint distribution of state transitions, which serves as an expressive representation of latent ego- and environmental-factors. This probabilistic approach supports the identification and adaptation to different operational situations, improving robustness and safety. Through a multivariate extension of Bayesian Online Changepoint Detection, our method segments changes in the underlying data generating process governing the robot's dynamics. The robot's transition model is then informed with a symbolic representation of the current situation derived from the joint distribution of latest state transitions, enabling adaptive and context-aware decision-making. To showcase the real-world effectiveness, we validate our approach in the challenging task of unstructured terrain navigation, where unmodeled and unmeasured terrain characteristics can significantly impact the robot's motion. Extensive experiments in both simulation and real world reveal significant improvements in data efficiency, policy performance, and the emergence of safer, adaptive navigation strategies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar</title>
<link>https://arxiv.org/abs/2505.19599</link>
<guid>https://arxiv.org/abs/2505.19599</guid>
<content:encoded><![CDATA[
arXiv:2505.19599v1 Announce Type: cross 
Abstract: Typical methods for evaluating the performance of language models evaluate their ability to answer questions accurately. These evaluation metrics are acceptable for determining the extent to which language models can understand and reason about text in a general sense, but fail to capture nuanced capabilities, such as the ability of language models to recognize and obey rare grammar points, particularly in languages other than English. We measure the perplexity of language models when confronted with the "first person psych predicate restriction" grammar point in Japanese. Weblab is the only tested open source model in the 7-10B parameter range which consistently assigns higher perplexity to ungrammatical psych predicate sentences than grammatical ones. We give evidence that Weblab's uniformly bad tokenization is a possible root cause for its good performance, and show that Llama 3's perplexity on grammatical psych predicate sentences can be reduced by orders of magnitude (28x difference) by restricting test sentences to those with uniformly well-behaved tokenizations. We show in further experiments on machine translation tasks that language models will use alternative grammar patterns in order to produce grammatical sentences when tokenization issues prevent the most natural sentence from being output.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging</title>
<link>https://arxiv.org/abs/2505.19603</link>
<guid>https://arxiv.org/abs/2505.19603</guid>
<content:encoded><![CDATA[
arXiv:2505.19603v1 Announce Type: cross 
Abstract: In contrast to vision transformers, which model long-range dependencies through global self-attention, large kernel convolutions provide a more efficient and scalable alternative, particularly in high-resolution 3D volumetric settings. However, naively increasing kernel size often leads to optimization instability and degradation in performance. Motivated by the spatial bias observed in effective receptive fields (ERFs), we hypothesize that different kernel elements converge at variable rates during training. To support this, we derive a theoretical connection between element-wise gradients and first-order optimization, showing that structurally re-parameterized convolution blocks inherently induce spatially varying learning rates. Building on this insight, we introduce Rep3D, a 3D convolutional framework that incorporates a learnable spatial prior into large kernel training. A lightweight two-stage modulation network generates a receptive-biased scaling mask, adaptively re-weighting kernel updates and enabling local-to-global convergence behavior. Rep3D adopts a plain encoder design with large depthwise convolutions, avoiding the architectural complexity of multi-branch compositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks and demonstrate consistent improvements over state-of-the-art baselines, including transformer-based and fixed-prior re-parameterization methods. By unifying spatial inductive bias with optimization-aware learning, Rep3D offers an interpretable, and scalable solution for 3D medical image analysis. The source code is publicly available at https://github.com/leeh43/Rep3D.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis</title>
<link>https://arxiv.org/abs/2505.19604</link>
<guid>https://arxiv.org/abs/2505.19604</guid>
<content:encoded><![CDATA[
arXiv:2505.19604v1 Announce Type: cross 
Abstract: Machine translation has become a critical tool in bridging linguistic gaps, especially between languages as diverse as English and Hindi. This paper comprehensively evaluates various machine translation models for translating between English and Hindi. We assess the performance of these models using a diverse set of automatic evaluation metrics, both lexical and machine learning-based metrics. Our evaluation leverages an 18000+ corpus of English Hindi parallel dataset and a custom FAQ dataset comprising questions from government websites. The study aims to provide insights into the effectiveness of different machine translation approaches in handling both general and specialized language domains. Results indicate varying performance levels across different metrics, highlighting strengths and areas for improvement in current translation systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?</title>
<link>https://arxiv.org/abs/2505.19663</link>
<guid>https://arxiv.org/abs/2505.19663</guid>
<content:encoded><![CDATA[
arXiv:2505.19663v1 Announce Type: cross 
Abstract: We present a framework to foster the evaluation of deep learning-based audio watermarking algorithms, establishing a standardized benchmark and allowing systematic comparisons. To simulate real-world usage, we introduce a comprehensive audio attack pipeline, featuring various distortions such as compression, background noise, and reverberation, and propose a diverse test dataset, including speech, environmental sounds, and music recordings. By assessing the performance of four existing watermarking algorithms on our framework, two main insights stand out: (i) neural compression techniques pose the most significant challenge, even when algorithms are trained with such compressions; and (ii) training with audio attacks generally improves robustness, although it is insufficient in some cases. Furthermore, we find that specific distortions, such as polarity inversion, time stretching, or reverb, seriously affect certain algorithms. Our contributions strengthen the robustness and perceptual assessment of audio watermarking algorithms across a wide range of applications, while ensuring a fair and consistent evaluation approach. The evaluation framework, including the attack pipeline, is accessible at github.com/SonyResearch/wm_robustness_eval.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19714</link>
<guid>https://arxiv.org/abs/2505.19714</guid>
<content:encoded><![CDATA[
arXiv:2505.19714v1 Announce Type: cross 
Abstract: Text Image Machine Translation (TIMT)-the task of translating textual content embedded in images-is critical for applications in accessibility, cross-lingual information access, and real-world document understanding. However, TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation, often requiring cascading multi-stage pipelines. Recent advances in large-scale Reinforcement Learning (RL) have improved reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is still underexplored. To bridge this gap, we introduce MT$^{3}$, the first framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism that adapts rule-based RL strategies to TIMT's intricacies, offering fine-grained, non-binary feedback across tasks. Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural and real-world social media contexts, we introduced XHSPost, the first social media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines such as Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics. Additionally, the model shows strong generalization to out-of-distribution language pairs and datasets. In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graceful Forgetting in Generative Language Models</title>
<link>https://arxiv.org/abs/2505.19715</link>
<guid>https://arxiv.org/abs/2505.19715</guid>
<content:encoded><![CDATA[
arXiv:2505.19715v1 Announce Type: cross 
Abstract: Recently, the pretrain-finetune paradigm has become a cornerstone in various deep learning areas. While in general the pre-trained model would promote both effectiveness and efficiency of downstream tasks fine-tuning, studies have shown that not all knowledge acquired during pre-training is beneficial. Some of the knowledge may actually bring detrimental effects to the fine-tuning tasks, which is also known as negative transfer. To address this problem, graceful forgetting has emerged as a promising approach. The core principle of graceful forgetting is to enhance the learning plasticity of the target task by selectively discarding irrelevant knowledge. However, this approach remains underexplored in the context of generative language models, and it is often challenging to migrate existing forgetting algorithms to these models due to architecture incompatibility. To bridge this gap, in this paper we propose a novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting in generative language models. With Fisher Information Matrix weighting the intended parameter updates, LWF computes forgetting confidence to evaluate self-generated knowledge regarding the forgetting task, and consequently, knowledge with high confidence is periodically unlearned during fine-tuning. Our experiments demonstrate that, although thoroughly uncovering the mechanisms of knowledge interaction remains challenging in pre-trained language models, applying graceful forgetting can contribute to enhanced fine-tuning performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Tour of Optimization with Finite Differences</title>
<link>https://arxiv.org/abs/2505.19720</link>
<guid>https://arxiv.org/abs/2505.19720</guid>
<content:encoded><![CDATA[
arXiv:2505.19720v1 Announce Type: cross 
Abstract: Finite-difference methods are widely used for zeroth-order optimization in settings where gradient information is unavailable or expensive to compute. These procedures mimic first-order strategies by approximating gradients through function evaluations along a set of random directions. From a theoretical perspective, recent studies indicate that imposing structure (such as orthogonality) on the chosen directions allows for the derivation of convergence rates comparable to those achieved with unstructured random directions (i.e., directions sampled independently from a distribution). Empirically, although structured directions are expected to enhance performance, they often introduce additional computational costs, which can limit their applicability in high-dimensional settings. In this work, we examine the impact of structured direction selection in finite-difference methods. We review and extend several strategies for constructing structured direction matrices and compare them with unstructured approaches in terms of computational cost, gradient approximation quality, and convergence behavior. Our evaluation spans both synthetic tasks and real-world applications such as adversarial perturbation. The results demonstrate that structured directions can be generated with computational costs comparable to unstructured ones while significantly improving gradient estimation accuracy and optimization performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Nash Learning from Human Feedback via Mirror Prox</title>
<link>https://arxiv.org/abs/2505.19731</link>
<guid>https://arxiv.org/abs/2505.19731</guid>
<content:encoded><![CDATA[
arXiv:2505.19731v1 Announce Type: cross 
Abstract: Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on reward models, frequently assuming preference structures like the Bradley-Terry model, which may not accurately capture the complexities of real human preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF) offers a more direct alternative by framing the problem as finding a Nash equilibrium of a game defined by these preferences. In this work, we introduce Nash Mirror Prox ($\mathtt{Nash-MP}$), an online NLHF algorithm that leverages the Mirror Prox optimization scheme to achieve fast and stable convergence to the Nash equilibrium. Our theoretical analysis establishes that Nash-MP exhibits last-iterate linear convergence towards the $\beta$-regularized Nash equilibrium. Specifically, we prove that the KL-divergence to the optimal policy decreases at a rate of order $(1+2\beta)^{-N/2}$, where $N$ is a number of preference queries. We further demonstrate last-iterate linear convergence for the exploitability gap and uniformly for the span semi-norm of log-probabilities, with all these rates being independent of the size of the action space. Furthermore, we propose and analyze an approximate version of Nash-MP where proximal steps are estimated using stochastic policy gradients, making the algorithm closer to applications. Finally, we detail a practical implementation strategy for fine-tuning large language models and present experiments that demonstrate its competitive performance and compatibility with existing methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weighted Leave-One-Out Cross Validation</title>
<link>https://arxiv.org/abs/2505.19737</link>
<guid>https://arxiv.org/abs/2505.19737</guid>
<content:encoded><![CDATA[
arXiv:2505.19737v1 Announce Type: cross 
Abstract: We present a weighted version of Leave-One-Out (LOO) cross-validation for estimating the Integrated Squared Error (ISE) when approximating an unknown function by a predictor that depends linearly on evaluations of the function over a finite collection of sites. The method relies on the construction of the best linear estimator of the squared prediction error at an arbitrary unsampled site based on squared LOO residuals, assuming that the function is a realization of a Gaussian Process (GP). A theoretical analysis of performance of the ISE estimator is presented, and robustness with respect to the choice of the GP kernel is investigated first analytically, then through numerical examples. Overall, the estimation of ISE is significantly more precise than with classical, unweighted, LOO cross validation. Application to model selection is briefly considered through examples.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models</title>
<link>https://arxiv.org/abs/2505.19743</link>
<guid>https://arxiv.org/abs/2505.19743</guid>
<content:encoded><![CDATA[
arXiv:2505.19743v1 Announce Type: cross 
Abstract: With the rapid development of Large Language Models (LLMs), aligning these models with human preferences and values is critical to ensuring ethical and safe applications. However, existing alignment techniques such as RLHF or DPO often require direct fine-tuning on LLMs with billions of parameters, resulting in substantial computational costs and inefficiencies. To address this, we propose Micro token-level Accept-Reject Aligning (MARA) approach designed to operate independently of the language models. MARA simplifies the alignment process by decomposing sentence-level preference learning into token-level binary classification, where a compact three-layer fully-connected network determines whether candidate tokens are "Accepted" or "Rejected" as part of the response. Extensive experiments across seven different LLMs and three open-source datasets show that MARA achieves significant improvements in alignment performance while reducing computational costs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement</title>
<link>https://arxiv.org/abs/2505.19757</link>
<guid>https://arxiv.org/abs/2505.19757</guid>
<content:encoded><![CDATA[
arXiv:2505.19757v1 Announce Type: cross 
Abstract: Effective generation of structured code comments requires robust quality metrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS) suffer from limited code-comment analysis. We propose CIDRe, a language-agnostic reference-free quality criterion combining four synergistic aspects: (1) relevance (code-comment semantic alignment), (2) informativeness (functional coverage), (3) completeness (presence of all structure sections), and (4) description length (detail sufficiency). We validate our criterion on a manually annotated dataset. Experiments demonstrate CIDRe's superiority over existing metrics, achieving improvement in cross-entropy evaluation. When applied to filter comments, the models finetuned on CIDRe-filtered data show statistically significant quality gains in GPT-4o-mini assessments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models</title>
<link>https://arxiv.org/abs/2505.19779</link>
<guid>https://arxiv.org/abs/2505.19779</guid>
<content:encoded><![CDATA[
arXiv:2505.19779v1 Announce Type: cross 
Abstract: Using massive datasets, foundation models are large-scale, pre-trained models that perform a wide range of tasks. These models have shown consistently improved results with the introduction of new methods. It is crucial to analyze how these trends impact the medical field and determine whether these advancements can drive meaningful change. This study investigates the application of recent state-of-the-art foundation models, DINOv2, MAE, VMamba, CoCa, SAM2, and AIMv2, for medical image classification. We explore their effectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for skin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest radiographs. By fine-tuning these models and evaluating their configurations, we aim to understand the potential of these advancements in medical image classification. The results indicate that these advanced models significantly enhance classification outcomes, demonstrating robust performance despite limited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models outperformed others, demonstrating that progress in natural domain training has positively impacted the medical domain and improved classification outcomes. Our code is publicly available at: https://github.com/sajjad-sh33/Medical-Transfer-Learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Missing Point in Vision Transformers for Universal Image Segmentation</title>
<link>https://arxiv.org/abs/2505.19795</link>
<guid>https://arxiv.org/abs/2505.19795</guid>
<content:encoded><![CDATA[
arXiv:2505.19795v1 Announce Type: cross 
Abstract: Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks</title>
<link>https://arxiv.org/abs/2505.19806</link>
<guid>https://arxiv.org/abs/2505.19806</guid>
<content:encoded><![CDATA[
arXiv:2505.19806v1 Announce Type: cross 
Abstract: Consciousness stands as one of the most profound and distinguishing features of the human mind, fundamentally shaping our understanding of existence and agency. As large language models (LLMs) develop at an unprecedented pace, questions concerning intelligence and consciousness have become increasingly significant. However, discourse on LLM consciousness remains largely unexplored territory. In this paper, we first clarify frequently conflated terminologies (e.g., LLM consciousness and LLM awareness). Then, we systematically organize and synthesize existing research on LLM consciousness from both theoretical and empirical perspectives. Furthermore, we highlight potential frontier risks that conscious LLMs might introduce. Finally, we discuss current challenges and outline future directions in this emerging field. The references discussed in this paper are organized at https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poison in the Well: Feature Embedding Disruption in Backdoor Attacks</title>
<link>https://arxiv.org/abs/2505.19821</link>
<guid>https://arxiv.org/abs/2505.19821</guid>
<content:encoded><![CDATA[
arXiv:2505.19821v1 Announce Type: cross 
Abstract: Backdoor attacks embed malicious triggers into training data, enabling attackers to manipulate neural network behavior during inference while maintaining high accuracy on benign inputs. However, existing backdoor attacks face limitations manifesting in excessive reliance on training data, poor stealth, and instability, which hinder their effectiveness in real-world applications. Therefore, this paper introduces ShadowPrint, a versatile backdoor attack that targets feature embeddings within neural networks to achieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint reduces reliance on training data access and operates effectively with exceedingly low poison rates (as low as 0.01%). It leverages a clustering-based optimization strategy to align feature embeddings, ensuring robust performance across diverse scenarios while maintaining stability and stealth. Extensive evaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%), steady CA (with decay no more than 1% in most cases), and low DDR (averaging below 5%) across both clean-label and dirty-label settings, and with poison rates ranging from as low as 0.01% to 0.05%, setting a new standard for backdoor attack capabilities and emphasizing the need for advanced defense strategies focused on feature space manipulations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications</title>
<link>https://arxiv.org/abs/2505.19837</link>
<guid>https://arxiv.org/abs/2505.19837</guid>
<content:encoded><![CDATA[
arXiv:2505.19837v1 Announce Type: cross 
Abstract: Multi-Agent Reinforcement Learning (MARL) has shown great potential as an adaptive solution for addressing modern cybersecurity challenges. MARL enables decentralized, adaptive, and collaborative defense strategies and provides an automated mechanism to combat dynamic, coordinated, and sophisticated threats. This survey investigates the current state of research in MARL applications for automated cyber defense (ACD), focusing on intruder detection and lateral movement containment. Additionally, it examines the role of Autonomous Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and validating MARL agents. Finally, the paper outlines existing challenges, such as scalability and adversarial robustness, and proposes future research directions. This also discusses how MARL integrates in AICA to provide adaptive, scalable, and dynamic solutions to counter the increasingly sophisticated landscape of cyber threats. It highlights the transformative potential of MARL in areas like intrusion detection and lateral movement containment, and underscores the value of Cyber Gyms for training and validation of AICA.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP</title>
<link>https://arxiv.org/abs/2505.19840</link>
<guid>https://arxiv.org/abs/2505.19840</guid>
<content:encoded><![CDATA[
arXiv:2505.19840v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) have achieved widespread success yet remain prone to adversarial attacks. Typically, such attacks either involve frequent queries to the target model or rely on surrogate models closely mirroring the target model -- often trained with subsets of the target model's training data -- to achieve high attack success rates through transferability. However, in realistic scenarios where training data is inaccessible and excessive queries can raise alarms, crafting adversarial examples becomes more challenging. In this paper, we present UnivIntruder, a novel attack framework that relies solely on a single, publicly available CLIP model and publicly available datasets. By using textual concepts, UnivIntruder generates universal, transferable, and targeted adversarial perturbations that mislead DNNs into misclassifying inputs into adversary-specified classes defined by textual concepts.
  Our extensive experiments show that our approach achieves an Attack Success Rate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly outperforming existing transfer-based methods. Additionally, we reveal real-world vulnerabilities, showing that even without querying target models, UnivIntruder compromises image search engines like Google and Baidu with ASR rates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR rates up to 80%. These findings underscore the practicality of our attack in scenarios where traditional avenues are blocked, highlighting the need to reevaluate security paradigms in AI applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Deconvolution in Populational Inverse Problems</title>
<link>https://arxiv.org/abs/2505.19841</link>
<guid>https://arxiv.org/abs/2505.19841</guid>
<content:encoded><![CDATA[
arXiv:2505.19841v1 Announce Type: cross 
Abstract: This work is focussed on the inversion task of inferring the distribution over parameters of interest leading to multiple sets of observations. The potential to solve such distributional inversion problems is driven by increasing availability of data, but a major roadblock is blind deconvolution, arising when the observational noise distribution is unknown. However, when data originates from collections of physical systems, a population, it is possible to leverage this information to perform deconvolution. To this end, we propose a methodology leveraging large data sets of observations, collected from different instantiations of the same physical processes, to simultaneously deconvolve the data corrupting noise distribution, and to identify the distribution over model parameters defining the physical processes. A parameter-dependent mathematical model of the physical process is employed. A loss function characterizing the match between the observed data and the output of the mathematical model is defined; it is minimized as a function of the both the parameter inputs to the model of the physics and the parameterized observational noise. This coupled problem is addressed with a modified gradient descent algorithm that leverages specific structure in the noise model. Furthermore, a new active learning scheme is proposed, based on adaptive empirical measures, to train a surrogate model to be accurate in parameter regions of interest; this approach accelerates computation and enables automatic differentiation of black-box, potentially nondifferentiable, code computing parameter-to-solution maps. The proposed methodology is demonstrated on porous medium flow, damped elastodynamics, and simplified models of atmospheric dynamics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.19862</link>
<guid>https://arxiv.org/abs/2505.19862</guid>
<content:encoded><![CDATA[
arXiv:2505.19862v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks but often face the challenge of overthinking, leading to substantially high inference costs. Existing approaches synthesize shorter reasoning responses for LRMs to learn, but are inefficient for online usage due to the time-consuming data generation and filtering processes. Meanwhile, online reinforcement learning mainly adopts a length reward to encourage short reasoning responses, but tends to lose the reflection ability and harm the performance. To address these issues, we propose REA-RL, which introduces a small reflection model for efficient scaling in online training, offering both parallel sampling and sequential revision. Besides, a reflection reward is designed to further prevent LRMs from favoring short yet non-reflective responses. Experiments show that both methods maintain or enhance performance while significantly improving inference efficiency. Their combination achieves a good balance between performance and efficiency, reducing inference costs by 35% without compromising performance. Further analysis demonstrates that our methods are effective by maintaining reflection frequency for hard problems while appropriately reducing it for simpler ones without losing reflection ability. Codes are available at https://github.com/hexuandeng/REA-RL.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2505.19863</link>
<guid>https://arxiv.org/abs/2505.19863</guid>
<content:encoded><![CDATA[
arXiv:2505.19863v1 Announce Type: cross 
Abstract: We introduce FruitNeRF++, a novel fruit-counting approach that combines contrastive learning with neural radiance fields to count fruits from unstructured input photographs of orchards. Our work is based on FruitNeRF, which employs a neural semantic field combined with a fruit-specific clustering approach. The requirement for adaptation for each fruit type limits the applicability of the method, and makes it difficult to use in practice. To lift this limitation, we design a shape-agnostic multi-fruit counting framework, that complements the RGB and semantic data with instance masks predicted by a vision foundation model. The masks are used to encode the identity of each fruit as instance embeddings into a neural instance field. By volumetrically sampling the neural fields, we extract a point cloud embedded with the instance features, which can be clustered in a fruit-agnostic manner to obtain the fruit count. We evaluate our approach using a synthetic dataset containing apples, plums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark apple dataset. Our results demonstrate that FruitNeRF++ is easier to control and compares favorably to other state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation</title>
<link>https://arxiv.org/abs/2505.19866</link>
<guid>https://arxiv.org/abs/2505.19866</guid>
<content:encoded><![CDATA[
arXiv:2505.19866v1 Announce Type: cross 
Abstract: Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of large language models (LLMs) by leveraging self-generated responses for self-training. Recent studies have incorporated reward models to guide response selection or decoding, aiming to obtain higher-quality data. However, they typically allocate a uniform sampling budget across all problems, overlooking the varying utility of problems at different difficulty levels. In this work, we conduct an empirical study and find that problems near the boundary of the LLM's reasoning capability offer significantly greater learning utility than both easy and overly difficult ones. To identify and exploit such problems, we propose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners. Given a fixed sampling budget, HS-STaR first performs lightweight pre-sampling with a reward-guided difficulty estimation strategy to efficiently identify boundary-level problems. Subsequently, it dynamically reallocates the remaining budget toward these high-utility problems during a re-sampling phase, maximizing the generation of valuable training data. Extensive experiments across multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR significantly outperforms other baselines without requiring additional sampling budget.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization</title>
<link>https://arxiv.org/abs/2505.19912</link>
<guid>https://arxiv.org/abs/2505.19912</guid>
<content:encoded><![CDATA[
arXiv:2505.19912v1 Announce Type: cross 
Abstract: We present Adjacent Possible Exploration (APE), a simple yet effective method for adapting large language models to specific tasks using minimal computational resources. Unlike traditional fine-tuning that requires extensive compute, APE iteratively fine-tunes models on small, carefully selected data batches (200 examples), retaining only improvements. On news summarization, APE achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes, matching or exceeding more complex methods like LoRA while remaining conceptually simple. Our approach is particularly valuable for researchers and practitioners with limited computational resources. We provide open-source code and demonstrate APE's effectiveness through both automatic metrics and human evaluation. While inspired by evolutionary theory's "adjacent possible", APE's core insight has a very practical application: small, iterative data perturbations can efficiently guide LLMs toward task-specific performance without expensive retraining.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cellwise and Casewise Robust Covariance in High Dimensions</title>
<link>https://arxiv.org/abs/2505.19925</link>
<guid>https://arxiv.org/abs/2505.19925</guid>
<content:encoded><![CDATA[
arXiv:2505.19925v1 Announce Type: cross 
Abstract: The sample covariance matrix is a cornerstone of multivariate statistics, but it is highly sensitive to outliers. These can be casewise outliers, such as cases belonging to a different population, or cellwise outliers, which are deviating cells (entries) of the data matrix. Recently some robust covariance estimators have been developed that can handle both types of outliers, but their computation is only feasible up to at most 20 dimensions. To remedy this we propose the cellRCov method, a robust covariance estimator that simultaneously handles casewise outliers, cellwise outliers, and missing data. It relies on a decomposition of the covariance on principal and orthogonal subspaces, leveraging recent work on robust PCA. It also employs a ridge-type regularization to stabilize the estimated covariance matrix. We establish some theoretical properties of cellRCov, including its casewise and cellwise influence functions as well as consistency and asymptotic normality. A simulation study demonstrates the superior performance of cellRCov in contaminated and missing data scenarios. Furthermore, its practical utility is illustrated in a real-world application to anomaly detection. We also construct and illustrate the cellRCCA method for robust and regularized canonical correlation analysis.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Visual Encoder Learn to See Arrows?</title>
<link>https://arxiv.org/abs/2505.19944</link>
<guid>https://arxiv.org/abs/2505.19944</guid>
<content:encoded><![CDATA[
arXiv:2505.19944v1 Announce Type: cross 
Abstract: The diagram is a visual representation of a relationship illustrated with edges (lines or arrows), which is widely used in industrial and scientific communication. Although recognizing diagrams is essential for vision language models (VLMs) to comprehend domain-specific knowledge, recent studies reveal that many VLMs fail to identify edges in images. We hypothesize that these failures stem from an over-reliance on textual and positional biases, preventing VLMs from learning explicit edge features. Based on this idea, we empirically investigate whether the image encoder in VLMs can learn edge representation through training on a diagram dataset in which edges are biased neither by textual nor positional information. To this end, we conduct contrastive learning on an artificially generated diagram--caption dataset to train an image encoder and evaluate its diagram-related features on three tasks: probing, image retrieval, and captioning. Our results show that the finetuned model outperforms pretrained CLIP in all tasks and surpasses zero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings confirm that eliminating textual and positional biases fosters accurate edge recognition in VLMs, offering a promising path for advancing diagram understanding.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection</title>
<link>https://arxiv.org/abs/2505.19948</link>
<guid>https://arxiv.org/abs/2505.19948</guid>
<content:encoded><![CDATA[
arXiv:2505.19948v1 Announce Type: cross 
Abstract: Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for imaging macromolecular complexes in their near-native states. However, the localization of 3D particles in cellular environments still presents a significant challenge due to low signal-to-noise ratios and missing wedge artifacts. Deep learning approaches have shown great potential, but they need huge amounts of data, which can be a challenge in cryo-ET scenarios where labeled data is often scarce. In this paper, we propose a novel Self-augmented and Self-interpreted (SaSi) deep learning approach towards few-shot particle detection in 3D cryo-ET images. Our method builds upon self-augmentation techniques to further boost data utilization and introduces a self-interpreted segmentation strategy for alleviating dependency on labeled data, hence improving generalization and robustness. As demonstrated by experiments conducted on both simulated and real-world cryo-ET datasets, the SaSi approach significantly outperforms existing state-of-the-art methods for particle localization. This research increases understanding of how to detect particles with very few labels in cryo-ET and thus sets a new benchmark for few-shot learning in structural biology.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Bandits with Non-i.i.d. Noise</title>
<link>https://arxiv.org/abs/2505.20017</link>
<guid>https://arxiv.org/abs/2505.20017</guid>
<content:encoded><![CDATA[
arXiv:2505.20017v1 Announce Type: cross 
Abstract: We study the linear stochastic bandit problem, relaxing the standard i.i.d. assumption on the observation noise. As an alternative to this restrictive assumption, we allow the noise terms across rounds to be sub-Gaussian but interdependent, with dependencies that decay over time. To address this setting, we develop new confidence sequences using a recently introduced reduction scheme to sequential probability assignment, and use these to derive a bandit algorithm based on the principle of optimism in the face of uncertainty. We provide regret bounds for the resulting algorithm, expressed in terms of the decay rate of the strength of dependence between observations. Among other results, we show that our bounds recover the standard rates up to a factor of the mixing time for geometrically mixing observation noise.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal brain encoding models for multi-modal stimuli</title>
<link>https://arxiv.org/abs/2505.20027</link>
<guid>https://arxiv.org/abs/2505.20027</guid>
<content:encoded><![CDATA[
arXiv:2505.20027v1 Announce Type: cross 
Abstract: Despite participants engaging in unimodal stimuli, such as watching images or silent videos, recent work has demonstrated that multi-modal Transformer models can predict visual brain activity impressively well, even with incongruent modality representations. This raises the question of how accurately these multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli. As these models grow increasingly popular, their use in studying neural activity provides insights into how our brains respond to such multi-modal naturalistic stimuli, i.e., where it separates and integrates information across modalities through a hierarchy of early sensory regions to higher cognition. We investigate this question by using multiple unimodal and two types of multi-modal models-cross-modal and jointly pretrained-to determine which type of model is more relevant to fMRI brain activity when participants are engaged in watching movies. We observe that both types of multi-modal models show improved alignment in several language and visual regions. This study also helps in identifying which brain regions process unimodal versus multi-modal information. We further investigate the contribution of each modality to multi-modal alignment by carefully removing unimodal features one by one from multi-modal representations, and find that there is additional information beyond the unimodal embeddings that is processed in the visual and language regions. Based on this investigation, we find that while for cross-modal models, their brain alignment is partially attributed to the video modality; for jointly pretrained models, it is partially attributed to both the video and audio modalities. This serves as a strong motivation for the neuroscience community to investigate the interpretability of these models for deepening our understanding of multi-modal information processing in brain.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)</title>
<link>https://arxiv.org/abs/2505.20029</link>
<guid>https://arxiv.org/abs/2505.20029</guid>
<content:encoded><![CDATA[
arXiv:2505.20029v1 Announce Type: cross 
Abstract: Transformer-based language models, though not explicitly trained to mimic brain recordings, have demonstrated surprising alignment with brain activity. Progress in these models-through increased size, instruction-tuning, and multimodality-has led to better representational alignment with neural data. Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have emerged, showing remarkable zero-shot capabilities in open-ended multimodal vision tasks. However, it is unknown whether MLLMs, when prompted with natural instructions, lead to better brain alignment and effectively capture instruction-specific representations. To address this, we first investigate brain alignment, i.e., measuring the degree of predictivity of neural visual activity using text output response embeddings from MLLMs as participants engage in watching natural scenes. Experiments with 10 different instructions show that MLLMs exhibit significantly better brain alignment than vision-only models and perform comparably to non-instruction-tuned multimodal models like CLIP. We also find that while these MLLMs are effective at generating high-quality responses suitable to the task-specific instructions, not all instructions are relevant for brain alignment. Further, by varying instructions, we make the MLLMs encode instruction-specific visual concepts related to the input image. This analysis shows that MLLMs effectively capture count-related and recognition-related concepts, demonstrating strong alignment with brain activity. Notably, the majority of the explained variance of the brain encoding models is shared between MLLM embeddings of image captioning and other instructions. These results suggest that enhancing MLLMs' ability to capture task-specific information could lead to better differentiation between various types of instructions, and thereby improving their precision in predicting brain responses.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers</title>
<link>https://arxiv.org/abs/2505.20032</link>
<guid>https://arxiv.org/abs/2505.20032</guid>
<content:encoded><![CDATA[
arXiv:2505.20032v1 Announce Type: cross 
Abstract: Tactile sensing provides local essential information that is complementary to visual perception, such as texture, compliance, and force. Despite recent advances in visuotactile representation learning, challenges remain in fusing these modalities and generalizing across tasks and environments without heavy reliance on pre-trained vision-language models. Moreover, existing methods do not study positional encodings, thereby overlooking the multi-scale spatial reasoning needed to capture fine-grained visuotactile correlations. We introduce ViTaPEs, a transformer-based framework that robustly integrates visual and tactile input data to learn task-agnostic representations for visuotactile perception. Our approach exploits a novel multi-scale positional encoding scheme to capture intra-modal structures, while simultaneously modeling cross-modal cues. Unlike prior work, we provide provable guarantees in visuotactile fusion, showing that our encodings are injective, rigid-motion-equivariant, and information-preserving, validating these properties empirically. Experiments on multiple large-scale real-world datasets show that ViTaPEs not only surpasses state-of-the-art baselines across various recognition tasks but also demonstrates zero-shot generalization to unseen, out-of-domain scenarios. We further demonstrate the transfer-learning strength of ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art baselines in predicting grasp success. Project page: https://sites.google.com/view/vitapes
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fast sound power prediction tool for genset noise using machine learning</title>
<link>https://arxiv.org/abs/2505.20079</link>
<guid>https://arxiv.org/abs/2505.20079</guid>
<content:encoded><![CDATA[
arXiv:2505.20079v1 Announce Type: cross 
Abstract: This paper investigates the application of machine learning regression algorithms Kernel Ridge Regression (KRR), Huber Regressor (HR), and Gaussian Process Regression (GPR) for predicting sound power levels of gensets, offering significant value for marketing and sales teams during the early bidding process. When engine sizes and genset enclosure dimensions are tentative, and measured noise data is unavailable, these algorithms enable reliable noise level estimation for unbuilt gensets. The study utilizes high fidelity datasets from over 100 experiments conducted at Cummins Acoustics Technology Center (ATC) in a hemi-anechoic chamber, adhering to ISO 3744 standards. By using readily available information from the bidding and initial design stages, KRR predicts sound power with an average accuracy of within 5 dBA. While HR and GPR show slightly higher prediction errors, all models effectively capture the overall noise trends across various genset configurations. These findings present a promising method for early-stage noise estimation in genset design.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings</title>
<link>https://arxiv.org/abs/2505.20133</link>
<guid>https://arxiv.org/abs/2505.20133</guid>
<content:encoded><![CDATA[
arXiv:2505.20133v1 Announce Type: cross 
Abstract: Current language models rely on static vocabularies determined at pretraining time, which can lead to decreased performance and increased computational cost for domains underrepresented in the original vocabulary. New tokens can be added to solve this problem, when coupled with a good initialization for their new embeddings. However, existing embedding initialization methods either require expensive further training or pretraining of additional modules. In this paper, we propose AweDist and show that by distilling representations obtained using the original tokenization, we can quickly learn high-quality input embeddings for new tokens. Experimental results with a wide range of open-weight models show that AweDist is able to outperform even strong baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeMe: Training-Free Language Model Merging via Semantic Alignment</title>
<link>https://arxiv.org/abs/2505.20144</link>
<guid>https://arxiv.org/abs/2505.20144</guid>
<content:encoded><![CDATA[
arXiv:2505.20144v1 Announce Type: cross 
Abstract: Despite the remarkable capabilities of Language Models (LMs) across diverse tasks, no single model consistently outperforms others, necessitating efficient methods to combine their strengths without expensive retraining. Existing model merging techniques, such as parameter averaging and task-guided fusion, often rely on data-dependent computations or fail to preserve internal knowledge, limiting their robustness and scalability. We introduce SeMe (Semantic-based Merging), a novel, data-free, and training-free approach that leverages latent semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike prior work, SeMe not only preserves model behaviors but also explicitly stabilizes internal knowledge, addressing a critical gap in LM fusion. Through extensive experiments across diverse architectures and tasks, we demonstrate that SeMe outperforms existing methods in both performance and efficiency while eliminating reliance on external data. Our work establishes a new paradigm for knowledge-aware model merging and provides insights into the semantic structure of LMs, paving the way for more scalable and interpretable model composition.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data</title>
<link>https://arxiv.org/abs/2505.20166</link>
<guid>https://arxiv.org/abs/2505.20166</guid>
<content:encoded><![CDATA[
arXiv:2505.20166v1 Announce Type: cross 
Abstract: Audio-aware large language models (ALLMs) have recently made great strides in understanding and processing audio inputs. These models are typically adapted from text-based large language models (LLMs) through additional training on audio-related tasks. However, this adaptation process presents two major limitations. First, ALLMs often suffer from catastrophic forgetting, where important textual capabilities such as instruction-following are lost after training on audio data. In some cases, models may even hallucinate sounds that are not present in the input audio, raising concerns about their reliability. Second, achieving cross-modal alignment between audio and language typically relies on large collections of task-specific question-answer pairs for instruction tuning, making the process resource-intensive. To address these issues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose caption-style alignment data. We refer to this process as bootstrapping audio-language alignment via synthetic data generation from backbone LLMs (BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds Through Extended Negative Samples), a contrastive-like training method designed to improve ALLMs' ability to distinguish between present and absent sounds. We further extend BALSa to multi-audio scenarios, where the model either explains the differences between audio inputs or produces a unified caption that describes them all, thereby enhancing audio-language alignment. Experimental results indicate that our method effectively mitigates audio hallucinations while reliably maintaining strong performance in audio understanding, reasoning, and instruction-following skills. Moreover, incorporating multi-audio training further enhances the model's comprehension and reasoning capabilities. Overall, BALSa offers an efficient and scalable approach to the development of ALLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding</title>
<link>https://arxiv.org/abs/2505.20176</link>
<guid>https://arxiv.org/abs/2505.20176</guid>
<content:encoded><![CDATA[
arXiv:2505.20176v1 Announce Type: cross 
Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising alternative to traditional neural architectures, yet their application to speech processing remains under explored. This work presents the first investigation of KANs for Spoken Language Understanding (SLU) tasks. We experiment with 2D-CNN models on two datasets, integrating KAN layers in five different configurations within the dense block. The best-performing setup, which places a KAN layer between two linear layers, is directly applied to transformer-based models and evaluated on five SLU datasets with increasing complexity. Our results show that KAN layers can effectively replace the linear layers, achieving comparable or superior performance in most cases. Finally, we provide insights into how KAN and linear layers on top of transformers differently attend to input regions of the raw waveforms.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Free Lunch: Non-Asymptotic Analysis of Prediction-Powered Inference</title>
<link>https://arxiv.org/abs/2505.20178</link>
<guid>https://arxiv.org/abs/2505.20178</guid>
<content:encoded><![CDATA[
arXiv:2505.20178v1 Announce Type: cross 
Abstract: Prediction-Powered Inference (PPI) is a popular strategy for combining gold-standard and possibly noisy pseudo-labels to perform statistical estimation. Prior work has shown an asymptotic "free lunch" for PPI++, an adaptive form of PPI, showing that the *asymptotic* variance of PPI++ is always less than or equal to the variance obtained from using gold-standard labels alone. Notably, this result holds *regardless of the quality of the pseudo-labels*. In this work, we demystify this result by conducting an exact finite-sample analysis of the estimation error of PPI++ on the mean estimation problem. We give a "no free lunch" result, characterizing the settings (and sample sizes) where PPI++ has provably worse estimation error than using gold-standard labels alone. Specifically, PPI++ will outperform if and only if the correlation between pseudo- and gold-standard is above a certain level that depends on the number of labeled samples ($n$). In some cases our results simplify considerably: For Gaussian data, the correlation must be at least $1/\sqrt{n - 2}$ in order to see improvement, and a similar result holds for binary labels. In experiments, we illustrate that our theoretical findings hold on real-world datasets, and give insights into trade-offs between single-sample and sample-splitting variants of PPI++.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Private Geometric Median in Nearly-Linear Time</title>
<link>https://arxiv.org/abs/2505.20189</link>
<guid>https://arxiv.org/abs/2505.20189</guid>
<content:encoded><![CDATA[
arXiv:2505.20189v1 Announce Type: cross 
Abstract: Estimating the geometric median of a dataset is a robust counterpart to mean estimation, and is a fundamental problem in computational geometry. Recently, [HSU24] gave an $(\varepsilon, \delta)$-differentially private algorithm obtaining an $\alpha$-multiplicative approximation to the geometric median objective, $\frac 1 n \sum_{i \in [n]} \|\cdot - \mathbf{x}_i\|$, given a dataset $\mathcal{D} := \{\mathbf{x}_i\}_{i \in [n]} \subset \mathbb{R}^d$. Their algorithm requires $n \gtrsim \sqrt d \cdot \frac 1 {\alpha\varepsilon}$ samples, which they prove is information-theoretically optimal. This result is surprising because its error scales with the \emph{effective radius} of $\mathcal{D}$ (i.e., of a ball capturing most points), rather than the worst-case radius. We give an improved algorithm that obtains the same approximation quality, also using $n \gtrsim \sqrt d \cdot \frac 1 {\alpha\epsilon}$ samples, but in time $\widetilde{O}(nd + \frac d {\alpha^2})$. Our runtime is nearly-linear, plus the cost of the cheapest non-private first-order method due to [CLM+16]. To achieve our results, we use subsampling and geometric aggregation tools inspired by FriendlyCore [TCK+22] to speed up the "warm start" component of the [HSU24] algorithm, combined with a careful custom analysis of DP-SGD's sensitivity for the geometric median objective.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Sampling for Forgotten Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2505.20196</link>
<guid>https://arxiv.org/abs/2505.20196</guid>
<content:encoded><![CDATA[
arXiv:2505.20196v1 Announce Type: cross 
Abstract: Fine-tuning large language models (LLMs) is intended to improve their reasoning capabilities, yet we uncover a counterintuitive effect: models often forget how to solve problems they previously answered correctly during training. We term this phenomenon temporal forgetting and show that it is widespread across model sizes, fine-tuning methods (both Reinforcement Learning and Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this gap, we introduce Temporal Sampling, a simple decoding strategy that draws outputs from multiple checkpoints along the training trajectory. This approach recovers forgotten solutions without retraining or ensembling, and leads to substantial improvements in reasoning performance, gains from 4 to 19 points in Pass@k and consistent gains in Majority@k across several benchmarks. We further extend our method to LoRA-adapted models, demonstrating that storing only adapter weights across checkpoints achieves similar benefits with minimal storage cost. By leveraging the temporal diversity inherent in training, Temporal Sampling offers a practical, compute-efficient way to surface hidden reasoning ability and rethink how we evaluate LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results</title>
<link>https://arxiv.org/abs/2505.20219</link>
<guid>https://arxiv.org/abs/2505.20219</guid>
<content:encoded><![CDATA[
arXiv:2505.20219v1 Announce Type: cross 
Abstract: The Polyak stepsize has been proven to be a fundamental stepsize in convex optimization, giving near optimal gradient descent rates across a wide range of assumptions. The universality of the Polyak stepsize has also inspired many stochastic variants, with theoretical guarantees and strong empirical performance. Despite the many theoretical results, our understanding of the convergence properties and shortcomings of the Polyak stepsize or its variants is both incomplete and fractured across different analyses. We propose a new, unified, and simple perspective for the Polyak stepsize and its variants as gradient descent on a surrogate loss. We show that each variant is equivalent to minimize a surrogate function with stepsizes that adapt to a guaranteed local curvature. Our general surrogate loss perspective is then used to provide a unified analysis of existing variants across different assumptions. Moreover, we show a number of negative results proving that the non-convergence results in some of the upper bounds is indeed real.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects</title>
<link>https://arxiv.org/abs/2505.20223</link>
<guid>https://arxiv.org/abs/2505.20223</guid>
<content:encoded><![CDATA[
arXiv:2505.20223v1 Announce Type: cross 
Abstract: The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the system's ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at https://github.com/cuiyx1720/Awesome-CoT4AD.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models</title>
<link>https://arxiv.org/abs/2505.20225</link>
<guid>https://arxiv.org/abs/2505.20225</guid>
<content:encoded><![CDATA[
arXiv:2505.20225v1 Announce Type: cross 
Abstract: Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Optimization Accelerator Framework for Multistate Ising Problems</title>
<link>https://arxiv.org/abs/2505.20250</link>
<guid>https://arxiv.org/abs/2505.20250</guid>
<content:encoded><![CDATA[
arXiv:2505.20250v1 Announce Type: cross 
Abstract: Ising Machines are a prominent class of hardware architectures that aim to solve NP-hard combinatorial optimization problems. These machines consist of a network of interacting binary spins/neurons that evolve to represent the optimum ground state energy solution. Generally, combinatorial problems are transformed into quadratic unconstrained binary optimization (QUBO) form to harness the computational efficiency of these Ising machines. However, this transformation, especially for multi-state problems, often leads to a more complex exploration landscape than the original problem, thus severely impacting the solution quality. To address this challenge, we model the spin interactions as a generalized boolean logic function to significantly reduce the exploration space. We benchmark the graph coloring problem from the class of multi-state NP-hard optimization using probabilistic Ising solvers to illustrate the effectiveness of our framework. The proposed methodology achieves similar accuracy compared to state-of-the-art heuristics and machine learning algorithms, and demonstrates significant improvement over the existing Ising methods. Additionally, we demonstrate that combining parallel tempering with our existing framework further reduces the coloring error by up to 50% compared to the conventionally used Gibbs sampling algorithm. We also design a 1024-neuron all-to-all connected probabilistic Ising accelerator that shows up to 10000x performance acceleration compared to heuristics while reducing the number of required physical neurons by 1.5-4x compared to conventional Ising machines. Indeed, this accelerator solution demonstrates improvement across all metrics over the current methods, i.e., energy, performance, area, and solution quality. Thus, this work expands the potential of existing Ising hardware to solve a broad class of these multistate optimization problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifelong Safety Alignment for Language Models</title>
<link>https://arxiv.org/abs/2505.20259</link>
<guid>https://arxiv.org/abs/2505.20259</guid>
<content:encoded><![CDATA[
arXiv:2505.20259v1 Announce Type: cross 
Abstract: LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at https://github.com/sail-sg/LifelongSafetyAlignment.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>syftr: Pareto-Optimal Generative AI</title>
<link>https://arxiv.org/abs/2505.20266</link>
<guid>https://arxiv.org/abs/2505.20266</guid>
<content:encoded><![CDATA[
arXiv:2505.20266v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) pipelines are central to applying large language models (LLMs) to proprietary or dynamic data. However, building effective RAG flows is complex, requiring careful selection among vector databases, embedding models, text splitters, retrievers, and synthesizing LLMs. The challenge deepens with the rise of agentic paradigms. Modules like verifiers, rewriters, and rerankers-each with intricate hyperparameter dependencies have to be carefully tuned. Balancing tradeoffs between latency, accuracy, and cost becomes increasingly difficult in performance-sensitive applications.
  We introduce syftr, a framework that performs efficient multi-objective search over a broad space of agentic and non-agentic RAG configurations. Using Bayesian Optimization, syftr discovers Pareto-optimal flows that jointly optimize task accuracy and cost. A novel early-stopping mechanism further improves efficiency by pruning clearly suboptimal candidates. Across multiple RAG benchmarks, syftr finds flows which are on average approximately 9 times cheaper while preserving most of the accuracy of the most accurate flows on the Pareto-frontier. Furthermore, syftr's ability to design and optimize allows integrating new modules, making it even easier and faster to realize high-performing generative AI pipelines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Neural Network Encodings for Logic-based Explainability</title>
<link>https://arxiv.org/abs/2505.20269</link>
<guid>https://arxiv.org/abs/2505.20269</guid>
<content:encoded><![CDATA[
arXiv:2505.20269v1 Announce Type: cross 
Abstract: Providing explanations for the outputs of artificial neural networks (ANNs) is crucial in many contexts, such as critical systems, data protection laws and handling adversarial examples. Logic-based methods can offer explanations with correctness guarantees, but face scalability challenges. Due to these issues, it is necessary to compare different encodings of ANNs into logical constraints, which are used in logic-based explainability. This work compares two encodings of ANNs: one has been used in the literature to provide explanations, while the other will be adapted for our context of explainability. Additionally, the second encoding uses fewer variables and constraints, thus, potentially enhancing efficiency. Experiments showed similar running times for computing explanations, but the adapted encoding performed up to 18\% better in building logical constraints and up to 16\% better in overall time.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lorentz Local Canonicalization: How to Make Any Network Lorentz-Equivariant</title>
<link>https://arxiv.org/abs/2505.20280</link>
<guid>https://arxiv.org/abs/2505.20280</guid>
<content:encoded><![CDATA[
arXiv:2505.20280v1 Announce Type: cross 
Abstract: Lorentz-equivariant neural networks are becoming the leading architectures for high-energy physics. Current implementations rely on specialized layers, limiting architectural choices. We introduce Lorentz Local Canonicalization (LLoCa), a general framework that renders any backbone network exactly Lorentz-equivariant. Using equivariantly predicted local reference frames, we construct LLoCa-transformers and graph networks. We adapt a recent approach to geometric message passing to the non-compact Lorentz group, allowing propagation of space-time tensorial features. Data augmentation emerges from LLoCa as a special choice of reference frame. Our models surpass state-of-the-art accuracy on relevant particle physics tasks, while being $4\times$ faster and using $5$-$100\times$ fewer FLOPs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?</title>
<link>https://arxiv.org/abs/2505.20295</link>
<guid>https://arxiv.org/abs/2505.20295</guid>
<content:encoded><![CDATA[
arXiv:2505.20295v1 Announce Type: cross 
Abstract: To reveal when a large language model (LLM) is uncertain about a response, uncertainty quantification commonly produces percentage numbers along with the output. But is this all we can do? We argue that in the output space of LLMs, the space of strings, exist strings expressive enough to summarize the distribution over output strings the LLM deems possible. We lay a foundation for this new avenue of uncertainty explication and present SelfReflect, a theoretically-motivated metric to assess how faithfully a string summarizes an LLM's internal answer distribution. We show that SelfReflect is able to discriminate even subtle differences of candidate summary strings and that it aligns with human judgement, outperforming alternative metrics such as LLM judges and embedding comparisons. With SelfReflect, we investigate a number of self-summarization methods and find that even state-of-the-art reasoning models struggle to explicate their internal uncertainty. But we find that faithful summarizations can be generated by sampling and summarizing. Our metric enables future works towards this universal form of LLM uncertainties.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning LLMs are Wandering Solution Explorers</title>
<link>https://arxiv.org/abs/2505.20296</link>
<guid>https://arxiv.org/abs/2505.20296</guid>
<content:encoded><![CDATA[
arXiv:2505.20296v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning abilities through test-time computation (TTC) techniques such as chain-of-thought prompting and tree-based reasoning. However, we argue that current reasoning LLMs (RLLMs) lack the ability to systematically explore the solution space. This paper formalizes what constitutes systematic problem solving and identifies common failure modes that reveal reasoning LLMs to be wanderers rather than systematic explorers. Through qualitative and quantitative analysis across multiple state-of-the-art LLMs, we uncover persistent issues: invalid reasoning steps, redundant explorations, hallucinated or unfaithful conclusions, and so on. Our findings suggest that current models' performance can appear to be competent on simple tasks yet degrade sharply as complexity increases. Based on the findings, we advocate for new metrics and tools that evaluate not just final outputs but the structure of the reasoning process itself.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PySAD: A Streaming Anomaly Detection Framework in Python</title>
<link>https://arxiv.org/abs/2009.02572</link>
<guid>https://arxiv.org/abs/2009.02572</guid>
<content:encoded><![CDATA[
arXiv:2009.02572v2 Announce Type: replace 
Abstract: Streaming anomaly detection requires algorithms that operate under strict constraints: bounded memory, single-pass processing, and constant-time complexity. We present PySAD, a comprehensive Python framework addressing these challenges through a unified architecture. The framework implements 17+ streaming algorithms (LODA, Half-Space Trees, xStream) with specialized components including projectors, probability calibrators, and postprocessors. Unlike existing batch-focused frameworks, PySAD enables efficient real-time processing with bounded memory while maintaining compatibility with PyOD and scikit-learn. Supporting all learning paradigms for univariate and multivariate streams, PySAD provides the most comprehensive streaming anomaly detection toolkit in Python. The source code is publicly available at github.com/selimfirat/pysad.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Gradient with Tree Expansion</title>
<link>https://arxiv.org/abs/2301.13236</link>
<guid>https://arxiv.org/abs/2301.13236</guid>
<content:encoded><![CDATA[
arXiv:2301.13236v2 Announce Type: replace 
Abstract: Policy gradient methods are notorious for having a large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax -- a generalization of softmax that employs planning. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We analyze SoftTreeMax and explain how tree expansion helps to reduce its gradient variance. We prove that the variance depends on the chosen tree-expansion policy. Specifically, we show that the closer the induced transitions are to being state-independent, the stronger the variance decay. With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance reduction. Ours is the first result to bound the gradient bias for an approximate model. In a practical implementation of SoftTreeMax, we utilize a parallel GPU-based simulator for fast and efficient tree expansion. Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude. This leads to better sample complexity and improved performance compared to distributed PPO.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training of Multi-task Neural Solver for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2305.06361</link>
<guid>https://arxiv.org/abs/2305.06361</guid>
<content:encoded><![CDATA[
arXiv:2305.06361v5 Announce Type: replace 
Abstract: Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. Naive application of conventional multi-task learning approaches often falls short in delivering a high-quality, unified neural solver. This deficiency primarily stems from the significant computational demands and a lack of adequate consideration for the complexities inherent in COPs. In this paper, we propose a general and efficient training paradigm to deliver a unified combinatorial multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. By employing theoretically grounded approximations, our method significantly enhances overall performance, regardless of whether it is within constrained training budgets, across equivalent training epochs, or in terms of generalization capabilities, when compared to conventional training schedules. On the real-world datasets of TSPLib and CVRPLib, our method also achieved the best results compared to single task learning and multi-task learning approaches. Additionally, the influence matrix provides empirical evidence supporting common practices in the field of learning to optimize, further substantiating the effectiveness of our approach. Our code is open-sourced and available at https://github.com/LOGO-CUHKSZ/MTL-COP.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Reinforcement Learning with Dual Expectile-Quantile Regression</title>
<link>https://arxiv.org/abs/2305.16877</link>
<guid>https://arxiv.org/abs/2305.16877</guid>
<content:encoded><![CDATA[
arXiv:2305.16877v4 Announce Type: replace 
Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and extracts rich feedback from environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, asymmetric hybrid $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution in a way that allows efficient learning while keeping an estimate of the full distribution of returns. We prove that our proposed operator converges to the distributional Bellman operator in the limit of infinite estimated quantile and expectile fractions, and we benchmark a practical implementation on a toy example and at scale. On the Atari benchmark, our approach matches the performance of the Huber-based IQN-1 baseline after $200$M training frames but avoids distributional collapse and keeps estimates of the full distribution of returns.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Optimisation Against Climate Change: Applications and Benchmarks</title>
<link>https://arxiv.org/abs/2306.04343</link>
<guid>https://arxiv.org/abs/2306.04343</guid>
<content:encoded><![CDATA[
arXiv:2306.04343v2 Announce Type: replace 
Abstract: Bayesian optimisation is a powerful method for optimising black-box functions, popular in settings where the true function is expensive to evaluate and no gradient information is available. Bayesian optimisation can improve responses to many optimisation problems within climate change for which simulator models are unavailable or expensive to sample from. While there have been several demonstrations of climate-related applications, there has been no unifying review of applications and benchmarks. We provide such a review here, to encourage the use of Bayesian optimisation for important and well-suited applications. We identify four main application domains: material discovery, wind farm layout, optimal renewable control and environmental monitoring. For each domain we identify a public benchmark or data set that is easy to use and evaluate systems against, while being representative of real-world problems. Due to the lack of a suitable benchmark for environmental monitoring, we propose LAQN-BO, based on air pollution data. Our contributions are: a) summarising Bayesian optimisation applications related to climate change; b) identifying a representative range of benchmarks, providing example code where necessary; and c) introducing a new benchmark, LAQN-BO.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Limitations and Possibilities of Nash Regret Minimization in Zero-Sum Matrix Games under Noisy Feedback</title>
<link>https://arxiv.org/abs/2306.13233</link>
<guid>https://arxiv.org/abs/2306.13233</guid>
<content:encoded><![CDATA[
arXiv:2306.13233v3 Announce Type: replace 
Abstract: This paper studies a variant of two-player zero-sum matrix games, where, at each timestep, the row player selects row $i$, the column player selects column $j$, and the row player receives a noisy reward with expected value $A_{i,j}$, along with noisy feedback on the input matrix $A$. The row player's goal is to maximize their total reward against an adversarial column player. Nash regret, defined as the difference between the player's total reward and the game's Nash equilibrium value scaled by the time horizon $T$, is often used to evaluate algorithmic performance in zero-sum games.
  We begin by studying the limitations of existing algorithms for minimizing Nash regret. We show that standard algorithm--including Hedge, FTRL, and OMD--as well as the strategy of playing the Nash equilibrium of the empirical matrix--all incur $\Omega(\sqrt{T})$ Nash regret, even when the row player receives noisy feedback on the entire matrix $A$. Furthermore, we show that UCB for matrix games, a natural adaptation of the well-known bandit algorithm, also suffers $\Omega(\sqrt{T})$ Nash regret under bandit feedback. Notably, these lower bounds hold even in the simplest case of $2 \times 2$ matrix games, where the instance-dependent matrix parameters are constant.
  We next ask whether instance-dependent $\text{polylog}(T)$ Nash regret is achievable against adversarial opponents. We answer this affirmatively. In the full-information setting, we present the first algorithm for general $n \times m$ matrix games that achieves instance-dependent $\text{polylog}(T)$ Nash regret. In the bandit feedback setting, we design an algorithm with similar guarantees for the special case of $2 \times 2$ game--the same regime in which existing algorithms provably suffer $\Omega(\sqrt{T})$ regret despite the simplicity of the instance. Finally, we validate our theoretical results with empirical evidence.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distortion Resilience for Goal-Oriented Semantic Communication</title>
<link>https://arxiv.org/abs/2309.14587</link>
<guid>https://arxiv.org/abs/2309.14587</guid>
<content:encoded><![CDATA[
arXiv:2309.14587v2 Announce Type: replace 
Abstract: Recent research efforts on Semantic Communication (SemCom) have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of Artificial Intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate distortion theory to analyze distortions induced by communication and compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented SemCom problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and experiments that demonstrate its effectiveness. The experimental results indicate that our proposed method enables accurate AI task performance while adhering to network constraints, establishing it as a valuable contribution to the field of signal processing. Furthermore, this work advances research in goal-oriented SemCom and highlights the significance of data-driven approaches in optimizing the performance of intelligent systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Continuity of Robust and Accurate Classifiers</title>
<link>https://arxiv.org/abs/2309.17048</link>
<guid>https://arxiv.org/abs/2309.17048</guid>
<content:encoded><![CDATA[
arXiv:2309.17048v2 Announce Type: replace 
Abstract: The reliability of a learning model is key to the successful deployment of machine learning in various applications. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. It has been shown that adversarial training can improve the robustness of the hypothesis. However, this improvement usually comes at the cost of decreased performance on natural samples. Hence, it has been suggested that robustness and accuracy of a hypothesis are at odds with each other. In this paper, we put forth the alternative proposal that it is the continuity of a hypothesis that is incompatible with its robustness and accuracy in many of these scenarios. In other words, a continuous function cannot effectively learn the optimal robust hypothesis. We introduce a framework for a rigorous study of harmonic and holomorphic hypothesis in learning theory terms and provide empirical evidence that continuous hypotheses do not perform as well as discontinuous hypotheses in some common machine learning tasks. From a practical point of view, our results suggests that a robust and accurate learning rule would train different continuous hypotheses for different regions of the domain. From a theoretical perspective, our analysis explains the adversarial examples phenomenon in these situations as a conflict between the continuity of a sequence of functions and its uniform convergence to a discontinuous function. Given that many of the contemporary machine learning models are continuous functions, it is important to theoretically study the continuity of robust and accurate classifiers as it is consequential in their construction, analysis and evaluation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linearization of ReLU Activation Function for Neural Network-Embedded Optimization: Optimal Day-Ahead Energy Scheduling</title>
<link>https://arxiv.org/abs/2310.01758</link>
<guid>https://arxiv.org/abs/2310.01758</guid>
<content:encoded><![CDATA[
arXiv:2310.01758v2 Announce Type: replace 
Abstract: Recently, neural networks have been widely applied in the power system area. They can be used for better predicting input information and modeling system performance with increased accuracy. In some applications such as battery degradation neural network-based microgrid day-ahead energy scheduling, the input features of the trained learning model are variables to be solved in optimization models that enforce limits on the output of the same learning model. This will create a neural network-embedded optimization problem; the use of nonlinear activation functions in the neural network will make such problems extremely hard to solve if not unsolvable. To address this emerging challenge, this paper investigated different methods for linearizing the nonlinear activation functions with a particular focus on the widely used rectified linear unit (ReLU) function. Four linearization methods tailored for the ReLU activation function are developed, analyzed and compared in this paper. Each method employs a set of linear constraints to replace the ReLU function, effectively linearizing the optimization problem, which can overcome the computational challenges associated with the nonlinearity of the neural network model. These proposed linearization methods provide valuable tools for effectively solving optimization problems that integrate neural network models with ReLU activation functions
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Interpretable Deep-Learning Framework for Predicting Hospital Readmissions From Electronic Health Records</title>
<link>https://arxiv.org/abs/2310.10187</link>
<guid>https://arxiv.org/abs/2310.10187</guid>
<content:encoded><![CDATA[
arXiv:2310.10187v2 Announce Type: replace 
Abstract: With the increasing availability of patient data, modern medicine is shifting towards prospective healthcare. Electronic health records offer a variety of information useful for clinical patient characterization and the development of predictive models, given that similar medical histories often lead to analogous health progressions. One application is the prediction of unplanned hospital readmissions, an essential task for reducing healthcare costs and improving patient outcomes. While predictive models demonstrate strong performances especially with deep learning approaches, they are often criticized for their lack of interpretability, a critical requirement in the medical domain where incorrect predictions may have severe consequences for patient safety. In this paper, we propose a novel and interpretable deep learning framework for predicting unplanned hospital readmissions, supported by NLP findings on word embeddings and by ConvLSTM neural networks for better handling temporal data. We validate the framework on two predictive tasks for hospital readmission within 30 and 180 days, using real-world data. Additionally, we introduce and evaluate a model-dependent technique designed to enhance result interpretability for medical professionals. Our solution outperforms traditional machine learning models in prediction accuracy while simultaneously providing more interpretable results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Transformer-Based Self-Supervised Learning Method to Enhance Photoplethysmogram Signal Artifact Detection</title>
<link>https://arxiv.org/abs/2401.01013</link>
<guid>https://arxiv.org/abs/2401.01013</guid>
<content:encoded><![CDATA[
arXiv:2401.01013v2 Announce Type: replace 
Abstract: Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired by InfoNCE, we introduce a novel contrastive loss function that facilitates smoother training and better convergence, thereby enhancing performance in artifact classification. In summary, this study establishes the efficacy of SSL in leveraging unlabeled data, particularly in enhancing the capabilities of the Transformer model. This approach holds promise for broader applications in PICU environments, where annotated data is often limited.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models</title>
<link>https://arxiv.org/abs/2402.03299</link>
<guid>https://arxiv.org/abs/2402.03299</guid>
<content:encoded><![CDATA[
arXiv:2402.03299v5 Announce Type: replace 
Abstract: The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD's versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2402.17516</link>
<guid>https://arxiv.org/abs/2402.17516</guid>
<content:encoded><![CDATA[
arXiv:2402.17516v5 Announce Type: replace 
Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting explanations but also generates more certain counterfactual examples. We showcase the performance of the QUCE method by comparing it with competing methods for both path-based explanations and generative counterfactual examples.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedGuCci: Making Local Models More Connected in Landscape for Federated Learning</title>
<link>https://arxiv.org/abs/2402.18949</link>
<guid>https://arxiv.org/abs/2402.18949</guid>
<content:encoded><![CDATA[
arXiv:2402.18949v3 Announce Type: replace 
Abstract: Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propose FedGuCci(+), improving group connectivity for better generalization. It is shown that our methods can boost the generalization of FL under client heterogeneity across various tasks (4 CV datasets and 6 NLP datasets) and model architectures (e.g., ViTs and PLMs). The code is available here: \href{https://github.com/ZexiLee/fedgucci}{\faGithub~FedGuCci Codebase}.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Unstructured Sparse Acceleration on Structured Sparse Accelerators</title>
<link>https://arxiv.org/abs/2403.07953</link>
<guid>https://arxiv.org/abs/2403.07953</guid>
<content:encoded><![CDATA[
arXiv:2403.07953v3 Announce Type: replace 
Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area for meeting the growing computation requirements. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparsity support, but it provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse HW cannot be accelerated by other structured hardware. To enable acceleration using unstructured sparsity of DNNs on structured sparse hardware, we propose an approximation method leveraging the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. We also develop a software framework, TASDER, to apply high-quality structured approximation on weights and activations of DNNs. Our method accelerates dense and sparse DNNs without fine-tuning and improves energy-delay-product (EDP) by up to 83% and 74%. It achieves up to 39% speed-up on a real system.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning</title>
<link>https://arxiv.org/abs/2403.13522</link>
<guid>https://arxiv.org/abs/2403.13522</guid>
<content:encoded><![CDATA[
arXiv:2403.13522v2 Announce Type: replace 
Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning (CIL) without available historical training samples as exemplars. Compared with its exemplar-based CIL counterpart that stores exemplars, EFCIL suffers more from forgetting issues. Recently, a new EFCIL branch named Analytic Continual Learning (ACL) introduces a gradient-free paradigm via Recursive Least-Square, achieving a forgetting-resistant classifier training with a frozen backbone during CIL. However, existing ACL suffers from ineffective representations and insufficient utilization of backbone knowledge. In this paper, we propose a representation-enhanced analytic learning (REAL) to address these problems. To enhance the representation, REAL constructs a dual-stream base pretraining followed by representation enhancing distillation process. The dual-stream base pretraining combines self-supervised contrastive learning for general features and supervised learning for class-specific knowledge, followed by the representation enhancing distillation to merge both streams, enhancing representations for subsequent CIL paradigm. To utilize more knowledge from the backbone, REAL presents a feature fusion buffer to multi-layer backbone features, providing informative features for the subsequent classifier training. Our method can be incorporated into existing ACL techniques and provides more competitive performance. Empirical results demonstrate that, REAL achieves state-of-the-art performance on CIFAR-100, ImageNet-100 and ImageNet-1k benchmarks, outperforming exemplar-free methods and rivaling exemplar-based approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Privacy and Robustness for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2403.16591</link>
<guid>https://arxiv.org/abs/2403.16591</guid>
<content:encoded><![CDATA[
arXiv:2403.16591v4 Announce Type: replace 
Abstract: The advent of machine learning has led to transformative changes across various domains, but the sensitive nature of data raises concerns about privacy and security. While Local Differential Privacy (LDP) has been a cornerstone in addressing these concerns, recent research has proposed privacy concepts aligned with the Bayesian inference perspective of an adversary, such as Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP). This paper explores the intricate relationships between LDP, ABP, and MBP, and their implications for algorithmic robustness. We establish theoretical connections between these privacy notions, proving that LDP implies MBP and vice versa under certain conditions, and deriving bounds connecting MBP and ABP. We also investigate the relationship between PAC robust learning and privacy preservation, demonstrating how to derive PAC robustness from privacy-preserving algorithms and construct privacy-preserving algorithms from PAC robust ones. Our findings provide valuable insights for constructing privacy-preserving and robust machine learning algorithms.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TG-NAS: Generalizable Zero-Cost Proxies with Operator Description Embedding and Graph Learning for Efficient Neural Architecture Search</title>
<link>https://arxiv.org/abs/2404.00271</link>
<guid>https://arxiv.org/abs/2404.00271</guid>
<content:encoded><![CDATA[
arXiv:2404.00271v2 Announce Type: replace 
Abstract: Neural Architecture Search (NAS) is a powerful technique for discovering high-performing CNN architectures, but most existing methods rely on costly training or extensive sampling. Zero-shot NAS offers a training-free alternative by using proxies to predict architecture performance. However, existing proxies are often suboptimal -- frequently outperformed by simple metrics like parameter count or FLOPs -- and they generalize poorly across different search spaces. Moreover, current model-based proxies struggle to adapt to new operators without access to ground-truth accuracy, limiting their transferability. We propose TG-NAS, a universal, model-based zero-cost (ZC) proxy that combines a Transformer-based operator embedding generator with a Graph Convolutional Network (GCN) to predict architecture performance. Unlike prior model-based predictors, TG-NAS requires no retraining and generalizes across arbitrary search spaces. It serves as a standalone ZC proxy with strong data efficiency, robustness, and cross-space consistency. Extensive evaluations across diverse NAS benchmarks demonstrate TG-NAS's superior rank correlation and generalizability compared to existing proxies. Additionally, it improves search efficiency by up to 300x and discovers architectures achieving 93.75% CIFAR-10 accuracy on NAS-Bench-201 and 74.9% ImageNet top-1 accuracy on the DARTS space, establishing TG-NAS as a promising foundation for efficient, generalizable NAS.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2404.16366</link>
<guid>https://arxiv.org/abs/2404.16366</guid>
<content:encoded><![CDATA[
arXiv:2404.16366v2 Announce Type: replace 
Abstract: Unsupervised graph anomaly detection aims at identifying rare patterns that deviate from the majority in a graph without the aid of labels, which is important for a variety of real-world applications. Recent advances have utilized Graph Neural Networks (GNNs) to learn effective node representations by aggregating information from neighborhoods. This is motivated by the hypothesis that nodes in the graph tend to exhibit consistent behaviors with their neighborhoods. However, such consistency can be disrupted by graph anomalies in multiple ways. Most existing methods directly employ GNNs to learn representations, disregarding the negative impact of graph anomalies on GNNs, resulting in sub-optimal node representations and anomaly detection performance. While a few recent approaches have redesigned GNNs for graph anomaly detection under semi-supervised label guidance, how to address the adverse effects of graph anomalies on GNNs in unsupervised scenarios and learn effective representations for anomaly detection are still under-explored. To bridge this gap, in this paper, we propose a simple yet effective framework for Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD). Specifically, G3AD first introduces two auxiliary networks along with correlation constraints to guard the GNNs against inconsistent information encoding. Furthermore, G3AD introduces an adaptive caching module to guard the GNNs from directly reconstructing the observed graph data that contains anomalies. Extensive experiments demonstrate that our G3AD can outperform twenty state-of-the-art methods on both synthetic and real-world graph anomaly datasets, with flexible generalization ability in different GNN backbones.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Extrapolation Expedites Alignment</title>
<link>https://arxiv.org/abs/2404.16792</link>
<guid>https://arxiv.org/abs/2404.16792</guid>
<content:encoded><![CDATA[
arXiv:2404.16792v4 Announce Type: replace 
Abstract: Given the high computational cost of preference alignment training of large language models (LLMs), exploring efficient methods to reduce the training overhead remains an important and compelling research problem. Motivated by the observation that alignment training typically involves only small parameter changes without injecting new knowledge into models, we propose a straightforward method called ExPO (model extrapolation) to expedite LLMs' alignment with human preferences. Given a partially-trained model and its initial SFT checkpoint, ExPO improves the implicit optimization objective of alignment training by simply amplifying the parameter change based on a first-order approximation, without any additional training overhead. Through controlled experiments, we demonstrate that ExPO boosts a DPO model trained with only 20% steps to outperform the fully-trained one. Moreover, we show that ExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B parameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which highlights ExPO's broader utility in efficiently enhancing LLM alignment.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Schr\"odinger Diffusion Models</title>
<link>https://arxiv.org/abs/2405.04795</link>
<guid>https://arxiv.org/abs/2405.04795</guid>
<content:encoded><![CDATA[
arXiv:2405.04795v5 Announce Type: replace 
Abstract: Schr\"odinger bridge (SB) has emerged as the go-to method for optimizing transportation plans in diffusion models. However, SB requires estimating the intractable forward score functions, inevitably resulting in the costly implicit training loss based on simulated trajectories. To improve the scalability while preserving efficient transportation plans, we leverage variational inference to linearize the forward score functions (variational scores) of SB and restore simulation-free properties in training backward scores. We propose the variational Schr\"odinger diffusion model (VSDM), where the forward process is a multivariate diffusion and the variational scores are adaptively optimized for efficient transport. Theoretically, we use stochastic approximation to prove the convergence of the variational scores and show the convergence of the adaptively generated samples based on the optimal variational scores. Empirically, we test the algorithm in simulated examples and observe that VSDM is efficient in generations of anisotropic shapes and yields straighter sample trajectories compared to the single-variate diffusion. We also verify the scalability of the algorithm in real-world data and achieve competitive unconditional generation performance in CIFAR10 and conditional generation in time series modeling. Notably, VSDM no longer depends on warm-up initializations and has become tuning-friendly in training large-scale experiments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAGES: Cost-Aware Gradient Entropy Search for Efficient Local Multi-Fidelity Bayesian Optimization</title>
<link>https://arxiv.org/abs/2405.07760</link>
<guid>https://arxiv.org/abs/2405.07760</guid>
<content:encoded><![CDATA[
arXiv:2405.07760v2 Announce Type: replace 
Abstract: Bayesian optimization (BO) is a popular approach for optimizing expensive-to-evaluate black-box objective functions. An important challenge in BO is its application to high-dimensional search spaces due in large part to the curse of dimensionality. One way to overcome this challenge is to focus on local BO methods that aim to efficiently learn gradients, which have shown strong empirical performance on high-dimensional problems including policy search in reinforcement learning (RL). Current local BO methods assume access to only a single high-fidelity information source whereas, in many problems, one has access to multiple cheaper approximations of the objective. We propose a novel algorithm, Cost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelity black-box functions. CAGES makes no assumption about the relationship between different information sources, making it more flexible than other multi-fidelity methods. It also employs a new information-theoretic acquisition function, which enables systematic identification of samples that maximize the information gain about the unknown gradient per evaluation cost. We demonstrate CAGES can achieve significant performance improvements compared to other state-of-the-art methods on synthetic and benchmark RL problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Volatility of Shapley-Based Contribution Metrics in Federated Learning</title>
<link>https://arxiv.org/abs/2405.08044</link>
<guid>https://arxiv.org/abs/2405.08044</guid>
<content:encoded><![CDATA[
arXiv:2405.08044v4 Announce Type: replace 
Abstract: Federated learning (FL) is a collaborative and privacy-preserving Machine Learning paradigm, allowing the development of robust models without the need to centralize sensitive data. A critical challenge in FL lies in fairly and accurately allocating contributions from diverse participants. Inaccurate allocation can undermine trust, lead to unfair compensation, and thus participants may lack the incentive to join or actively contribute to the federation. Various remuneration strategies have been proposed to date, including auction-based approaches and Shapley-value-based methods, the latter offering a means to quantify the contribution of each participant. However, little to no work has studied the stability of these contribution evaluation methods. In this paper, we evaluate participant contributions in federated learning using gradient-based model reconstruction techniques with Shapley values and compare the round-based contributions to a classic data contribution measurement scheme. We provide an extensive analysis of the discrepancies of Shapley values across a set of aggregation strategies and examine them on an overall and a per-client level. We show that, between different aggregation techniques, Shapley values lead to unstable reward allocations among participants. Our analysis spans various data heterogeneity distributions, including independent and identically distributed (IID) and non-IID scenarios.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Type Point Cloud Autoencoder: A Complete Equivariant Embedding for Molecule Conformation and Pose</title>
<link>https://arxiv.org/abs/2405.13791</link>
<guid>https://arxiv.org/abs/2405.13791</guid>
<content:encoded><![CDATA[
arXiv:2405.13791v3 Announce Type: replace 
Abstract: Representations are a foundational component of any modelling protocol, including on molecules and molecular solids. For tasks that depend on knowledge of both molecular conformation and 3D orientation, such as the modelling of molecular dimers, clusters, or condensed phases, we desire a rotatable representation that is provably complete in the types and positions of atomic nuclei and roto-inversion equivariant with respect to the input point cloud. In this paper, we develop, train, and evaluate a new type of autoencoder, molecular O(3) encoding net (Mo3ENet), for multi-type point clouds, for which we propose a new reconstruction loss, capitalizing on a Gaussian mixture representation of the input and output point clouds. Mo3ENet is end-to-end equivariant, meaning the learned representation can be manipulated on O(3), a practical bonus. An appropriately trained Mo3ENet latent space comprises a universal embedding for scalar and vector molecule property prediction tasks, as well as other downstream tasks incorporating the 3D molecular pose, and we demonstrate its fitness on several such tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models</title>
<link>https://arxiv.org/abs/2405.14917</link>
<guid>https://arxiv.org/abs/2405.14917</guid>
<content:encoded><![CDATA[
arXiv:2405.14917v2 Announce Type: replace 
Abstract: Post-training quantization (PTQ) is an effective technique for compressing large language models (LLMs). However, while uniform-precision quantization is computationally efficient, it often compromises model performance. To address this, we propose SliM-LLM, a salience-driven mixed-precision quantization framework that allocates bit-widths at the group-wise. Our approach leverages the observation that important weights follow a structured distribution and introduces two key components: \textbf{1)} \textit{Salience-Determined Bit Allocation} adaptively assigns bit-widths to groups within each layer based on their salience; and \textbf{2)} \textit{Salience-Weighted Quantizer Calibration} optimizes quantizer parameters by incorporating element-level salience. With its structured partitioning, SliM-LLM provides a hardware-friendly solution that matches the efficiency of uniform quantization methods while improving accuracy. Experiments show that SliM-LLM achieves superior performance across various LLMs at low bit-widths. For example, a 2-bit quantized LLaMA-7B model reduces memory usage by nearly 6x compared to the floating-point baseline, decreases perplexity by 48\% compared to state-of-the-art gradient-free PTQ methods, and maintains GPU inference speed. Additionally, the extended version, SliM-LLM$^+$, which incorporates gradient-based quantization, further reduces perplexity by 35.1\%. Our code is available at https://github.com/Aaronhuang-778/SliM-LLM
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-theoretic Generalization Analysis for Expected Calibration Error</title>
<link>https://arxiv.org/abs/2405.15709</link>
<guid>https://arxiv.org/abs/2405.15709</guid>
<content:encoded><![CDATA[
arXiv:2405.15709v2 Announce Type: replace 
Abstract: While the expected calibration error (ECE), which employs binning, is widely adopted to evaluate the calibration performance of machine learning models, theoretical understanding of its estimation bias is limited. In this paper, we present the first comprehensive analysis of the estimation bias in the two common binning strategies, uniform mass and uniform width binning. Our analysis establishes upper bounds on the bias, achieving an improved convergence rate. Moreover, our bounds reveal, for the first time, the optimal number of bins to minimize the estimation bias. We further extend our bias analysis to generalization error analysis based on the information-theoretic approach, deriving upper bounds that enable the numerical evaluation of how small the ECE is for unknown data. Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AFL: A Single-Round Analytic Approach for Federated Learning with Pre-trained Models</title>
<link>https://arxiv.org/abs/2405.16240</link>
<guid>https://arxiv.org/abs/2405.16240</guid>
<content:encoded><![CDATA[
arXiv:2405.16240v2 Announce Type: replace 
Abstract: In this paper, we introduce analytic federated learning (AFL), a new training paradigm that brings analytical (i.e., closed-form) solutions to the federated learning (FL) with pre-trained models. Our AFL draws inspiration from analytic learning -- a gradient-free technique that trains neural networks with analytical solutions in one epoch. In the local client training stage, the AFL facilitates a one-epoch training, eliminating the necessity for multi-epoch updates. In the aggregation stage, we derive an absolute aggregation (AA) law. This AA law allows a single-round aggregation, reducing heavy communication overhead and achieving fast convergence by removing the need for multiple aggregation rounds. More importantly, the AFL exhibits a property that $\textit{invariance to data partitioning}$, meaning that regardless of how the full dataset is distributed among clients, the aggregated result remains identical. This could spawn various potentials, such as data heterogeneity invariance and client-number invariance. We conduct experiments across various FL settings including extremely non-IID ones, and scenarios with a large number of clients (e.g., $\ge 1000$). In all these settings, our AFL constantly performs competitively while existing FL techniques encounter various obstacles. Our codes are available at https://github.com/ZHUANGHP/Analytic-federated-learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining the role of Intrinsic Dimensionality in Adversarial Training</title>
<link>https://arxiv.org/abs/2405.17130</link>
<guid>https://arxiv.org/abs/2405.17130</guid>
<content:encoded><![CDATA[
arXiv:2405.17130v2 Announce Type: replace 
Abstract: Adversarial Training (AT) impacts different architectures in distinct ways: vision models gain robustness but face reduced generalization, encoder-based models exhibit limited robustness improvements with minimal generalization loss, and recent work in latent-space adversarial training (LAT) demonstrates that decoder-based models achieve improved robustness by applying AT across multiple layers. We provide the first explanation for these trends by leveraging the manifold conjecture: off-manifold adversarial examples (AEs) enhance robustness, while on-manifold AEs improve generalization. We show that vision and decoder-based models exhibit low intrinsic dimensionality in earlier layers (favoring off-manifold AEs), whereas encoder-based models do so in later layers (favoring on-manifold AEs). Exploiting this property, we introduce SMAAT, which improves the scalability of AT for encoder-based models by perturbing the layer with the lowest intrinsic dimensionality. This reduces the projected gradient descent (PGD) chain length required for AE generation, cutting GPU time by 25-33% while significantly boosting robustness. We validate SMAAT across multiple tasks, including text generation, sentiment classification, safety filtering, and retrieval augmented generation setups, demonstrating superior robustness with comparable generalization to standard training.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Time Series Processing for Transformers and State-Space Models through Token Merging</title>
<link>https://arxiv.org/abs/2405.17951</link>
<guid>https://arxiv.org/abs/2405.17951</guid>
<content:encoded><![CDATA[
arXiv:2405.17951v2 Announce Type: replace 
Abstract: Despite recent advances in subquadratic attention mechanisms or state-space models, processing long token sequences still imposes significant computational requirements. Token merging has emerged as a solution to increase computational efficiency in computer vision architectures. In this work, we perform the first investigations of token merging in time series analysis on both transformers and state-space models. We further introduce local merging, a domain-specific token merging algorithm that selectively combines tokens within a local neighborhood, achieving two major benefits: a) Local merging can adjust its computational complexity from quadratic to linear based on the neighborhood size to effectively scale to long sequences; b) Local merging is the first causal merging scheme enabling token merging in transformer decoders. Further, we identify spectral properties of the input data that reliably predict the potential benefits of local merging without requiring evaluation on downstream tasks. Our comprehensive empirical evaluation demonstrates that local merging offers substantial efficiency gains with minimal impact on accuracy, achieving up to 5400% acceleration on the recently proposed Chronos foundation model.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-distribution Reject Option Method for Dataset Shift Problem in Early Disease Onset Prediction</title>
<link>https://arxiv.org/abs/2405.19864</link>
<guid>https://arxiv.org/abs/2405.19864</guid>
<content:encoded><![CDATA[
arXiv:2405.19864v2 Announce Type: replace 
Abstract: Machine learning is increasingly used to predict lifestyle-related disease onset using health and medical data. However, its predictive accuracy for use is often hindered by dataset shift, which refers to discrepancies in data distribution between the training and testing datasets. This issue leads to the misclassification of out-of-distribution (OOD) data. To diminish dataset shift in real-world settings, this paper proposes the out-of-distribution reject option for prediction (ODROP). This method integrates an OOD detection model to preclude OOD data from the prediction phase. We used two real-world health checkup datasets (Hirosaki and Wakayama) with dataset shift, across three disease onset prediction tasks: diabetes, dyslipidemia, and hypertension. Both components of ODROP method -- the OOD detection model and the prediction model -- were trained on the Hirosaki dataset. We assessed the effectiveness of ODROP on the Wakayama dataset using AUROC-rejection rate curve plot. In the five OOD detection approaches (the variational autoencoder, neural network ensemble std, neural network ensemble epistemic, neural network energy, and neural network gaussian mixture based energy measurement), the variational autoencoder method demonstrated notably higher stability and a greater improvement in AUROC. For example, in the Wakayama dataset, the AUROC for diabetes onset increased from 0.80 without ODROP to 0.90 at a 31.1% rejection rate, and for dyslipidemia, it improved from 0.70 without ODROP to 0.76 at a 34% rejection rate. In addition, we categorized dataset shifts into two types using SHAP clustering -- those that considerably affect predictions and those that do not. This study is the first to apply OOD detection to actual health and medical data, demonstrating its potential to substantially improve the accuracy and reliability of disease prediction models amidst dataset shift.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Length independent generalization bounds for deep SSM architectures via Rademacher contraction and stability constraints</title>
<link>https://arxiv.org/abs/2405.20278</link>
<guid>https://arxiv.org/abs/2405.20278</guid>
<content:encoded><![CDATA[
arXiv:2405.20278v3 Announce Type: replace 
Abstract: Many state-of-the-art models trained on long-range sequences, for example S4, S5 or LRU, are made of sequential blocks combining State-Space Models (SSMs) with neural networks. In this paper we provide a PAC bound that holds for these kind of architectures with \emph{stable} SSM blocks and does not depend on the length of the input sequence. Imposing stability of the SSM blocks is a standard practice in the literature, and it is known to help performance. Our results provide a theoretical justification for the use of stable SSM blocks as the proposed PAC bound decreases as the degree of stability of the SSM blocks increases.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffPuter: Empowering Diffusion Models for Missing Data Imputation</title>
<link>https://arxiv.org/abs/2405.20690</link>
<guid>https://arxiv.org/abs/2405.20690</guid>
<content:encoded><![CDATA[
arXiv:2405.20690v2 Announce Type: replace 
Abstract: Generative models play an important role in missing data imputation in that they aim to learn the joint distribution of full data. However, applying advanced deep generative models (such as Diffusion models) to missing data imputation is challenging due to 1) the inherent incompleteness of the training data and 2) the difficulty in performing conditional inference from unconditional generative models. To deal with these challenges, this paper introduces DiffPuter, a tailored diffusion model combined with the Expectation-Maximization (EM) algorithm for missing data imputation. DiffPuter iteratively trains a diffusion model to learn the joint distribution of missing and observed data and performs an accurate conditional sampling to update the missing values using a tailored reversed sampling strategy. Our theoretical analysis shows that DiffPuter's training step corresponds to the maximum likelihood estimation of data density (M-step), and its sampling step represents the Expected A Posteriori estimation of missing values (E-step). Extensive experiments across ten diverse datasets and comparisons with 17 different imputation methods demonstrate DiffPuter's superior performance. Notably, DiffPuter achieves an average improvement of 6.94% in MAE and 4.78% in RMSE compared to the most competitive existing method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Multiclass Neural Network Classifier Criterion with Task Performance Metrics</title>
<link>https://arxiv.org/abs/2405.20954</link>
<guid>https://arxiv.org/abs/2405.20954</guid>
<content:encoded><![CDATA[
arXiv:2405.20954v2 Announce Type: replace 
Abstract: Multiclass neural network classifiers are typically trained using cross-entropy loss but evaluated using metrics derived from the confusion matrix, such as Accuracy, $F_\beta$-Score, and Matthews Correlation Coefficient. This mismatch between the training objective and evaluation metric can lead to suboptimal performance, particularly when the user's priorities differ from what cross-entropy implicitly optimizes. For example, in the presence of class imbalance, $F_1$-Score may be preferred over Accuracy. Similarly, given a preference towards precision, the $F_{\beta=0.25}$-Score will better reflect this preference than $F_1$-Score. However, standard cross-entropy loss does not accommodate such a preference. Building on prior work leveraging soft-set confusion matrices and a continuous piecewise-linear Heaviside approximation, we propose Evaluation Aligned Surrogate Training (EAST), a novel approach to train multiclass classifiers using close surrogates of confusion-matrix based metrics, thereby aligning a neural network classifier's predictions more closely to a target evaluation metric than typical cross-entropy loss. EAST introduces three key innovations: First, we propose a novel dynamic thresholding approach during training. Second, we propose using a multiclass soft-set confusion matrix. Third, we introduce an annealing process that gradually aligns the surrogate loss with the target evaluation metric. Our theoretical analysis shows that EAST results in consistent estimators of the target evaluation metric. Furthermore, we show that the learned network parameters converge asymptotically to values that optimize for the target evaluation metric. Extensive experiments validate the effectiveness of our approach, demonstrating improved alignment between training objectives and evaluation metrics, while outperforming existing methods across many datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating Conflicting Views: Harnessing Trust for Learning</title>
<link>https://arxiv.org/abs/2406.00958</link>
<guid>https://arxiv.org/abs/2406.00958</guid>
<content:encoded><![CDATA[
arXiv:2406.00958v3 Announce Type: replace 
Abstract: Resolving conflicts is critical for improving the reliability of multi-view classification. While prior work focuses on learning consistent and informative representations across views, it often assumes perfect alignment and equal importance of all views, an assumption rarely met in real-world scenarios, as some views may express distinct information. To address this, we develop a computational trust-based discounting method that enhances the Evidential Multi-view framework by accounting for the instance-wise reliability of each view through a probability-sensitive trust mechanism. We evaluate our method on six real-world datasets using Top-1 Accuracy, Fleiss' Kappa, and a new metric, Multi-View Agreement with Ground Truth, to assess prediction reliability. We also assess the effectiveness of uncertainty in indicating prediction correctness via AUROC.Additionally, we test the scalability of our method through end-to-end training on a large-scale dataset. The experimental results show that computational trust can effectively resolve conflicts, paving the way for more reliable multi-view classification models in real-world applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Class-Incremental Learning with Hierarchical Generative Prototypes</title>
<link>https://arxiv.org/abs/2406.02447</link>
<guid>https://arxiv.org/abs/2406.02447</guid>
<content:encoded><![CDATA[
arXiv:2406.02447v4 Announce Type: replace 
Abstract: Federated Learning (FL) aims at unburdening the training of deep models by distributing computation across multiple devices (clients) while safeguarding data privacy. On top of that, Federated Continual Learning (FCL) also accounts for data distribution evolving over time, mirroring the dynamic nature of real-world environments. While previous studies have identified Catastrophic Forgetting and Client Drift as primary causes of performance degradation in FCL, we shed light on the importance of Incremental Bias and Federated Bias, which cause models to prioritize classes that are recently introduced or locally predominant, respectively. Our proposal constrains both biases in the last layer by efficiently finetuning a pre-trained backbone using learnable prompts, resulting in clients that produce less biased representations and more biased classifiers. Therefore, instead of solely relying on parameter aggregation, we leverage generative prototypes to effectively balance the predictions of the global model. Our method significantly improves the current State Of The Art, providing an average increase of +7.8% in accuracy.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Residual Connections and Normalization Can Provably Prevent Oversmoothing in GNNs</title>
<link>https://arxiv.org/abs/2406.02997</link>
<guid>https://arxiv.org/abs/2406.02997</guid>
<content:encoded><![CDATA[
arXiv:2406.02997v3 Announce Type: replace 
Abstract: Residual connections and normalization layers have become standard design choices for graph neural networks (GNNs), and were proposed as solutions to the mitigate the oversmoothing problem in GNNs. However, how exactly these methods help alleviate the oversmoothing problem from a theoretical perspective is not well understood. In this work, we provide a formal and precise characterization of (linearized) GNNs with residual connections and normalization layers. We establish that (a) for residual connections, the incorporation of the initial features at each layer can prevent the signal from becoming too smooth, and determines the subspace of possible node representations; (b) batch normalization prevents a complete collapse of the output embedding space to a one-dimensional subspace through the individual rescaling of each column of the feature matrix. This results in the convergence of node representations to the top-$k$ eigenspace of the message-passing operator; (c) moreover, we show that the centering step of a normalization layer -- which can be understood as a projection -- alters the graph signal in message-passing in such a way that relevant information can become harder to extract. We therefore introduce a novel, principled normalization layer called GraphNormv2 in which the centering step is learned such that it does not distort the original graph signal in an undesirable way. Experimental results confirm the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Multi-Fidelity Best-Arm Identification</title>
<link>https://arxiv.org/abs/2406.03033</link>
<guid>https://arxiv.org/abs/2406.03033</guid>
<content:encoded><![CDATA[
arXiv:2406.03033v2 Announce Type: replace 
Abstract: In bandit best-arm identification, an algorithm is tasked with finding the arm with highest mean reward with a specified accuracy as fast as possible. We study multi-fidelity best-arm identification, in which the algorithm can choose to sample an arm at a lower fidelity (less accurate mean estimate) for a lower cost. Several methods have been proposed for tackling this problem, but their optimality remain elusive, notably due to loose lower bounds on the total cost needed to identify the best arm. Our first contribution is a tight, instance-dependent lower bound on the cost complexity. The study of the optimization problem featured in the lower bound provides new insights to devise computationally efficient algorithms, and leads us to propose a gradient-based approach with asymptotically optimal cost complexity. We demonstrate the benefits of the new algorithm compared to existing methods in experiments. Our theoretical and empirical findings also shed light on an intriguing concept of optimal fidelity for each arm.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modulated differentiable STFT and balanced spectrum metric for freight train wheelset bearing cross-machine transfer monitoring under speed fluctuations</title>
<link>https://arxiv.org/abs/2406.11917</link>
<guid>https://arxiv.org/abs/2406.11917</guid>
<content:encoded><![CDATA[
arXiv:2406.11917v3 Announce Type: replace 
Abstract: The service conditions of wheelset bearings has a direct impact on the safe operation of railway heavy haul freight trains as the key components. However, speed fluctuation of the trains and few fault samples are the two main problems that restrict the accuracy of bearing fault diagnosis. Therefore, a cross-machine transfer diagnosis (pyDSN) network coupled with interpretable modulated differentiable short-time Fourier transform (STFT) and physics-informed balanced spectrum quality metric is proposed to learn domain-invariant and discriminative features under time-varying speeds. Firstly, due to insufficiency in extracting extract frequency components of time-varying speed signals using fixed windows, a modulated differentiable STFT (MDSTFT) that is interpretable with STFT-informed theoretical support, is proposed to extract the robust time-frequency spectrum (TFS). During training process, multiple windows with different lengths dynamically change. Also, in addition to the classification metric and domain discrepancy metric, we creatively introduce a third kind of metric, referred to as the physics-informed metric, to enhance transferable TFS. A physics-informed balanced spectrum quality (BSQ) regularization loss is devised to guide an optimization direction for MDSTFT and model. With it, not only can model acquire high-quality TFS, but also a physics-restricted domain adaptation network can be also acquired, making it learn real-world physics knowledge, ultimately diminish the domain discrepancy across different datasets. The experiment is conducted in the scenario of migrating from the laboratory datasets to the freight train dataset, indicating that the hybrid-driven pyDSN outperforms existing methods and has practical value.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Robustness of Graph Neural Networks against Adversarial Attacks</title>
<link>https://arxiv.org/abs/2406.13920</link>
<guid>https://arxiv.org/abs/2406.13920</guid>
<content:encoded><![CDATA[
arXiv:2406.13920v2 Announce Type: replace 
Abstract: Recent studies have shown that graph neural networks (GNNs) are vulnerable to adversarial attacks, posing significant challenges to their deployment in safety-critical scenarios. This vulnerability has spurred a growing focus on designing robust GNNs. Despite this interest, current advancements have predominantly relied on empirical trial and error, resulting in a limited understanding of the robustness of GNNs against adversarial attacks. To address this issue, we conduct the first large-scale systematic study on the adversarial robustness of GNNs by considering the patterns of input graphs, the architecture of GNNs, and their model capacity, along with discussions on sensitive neurons and adversarial transferability. This work proposes a comprehensive empirical framework for analyzing the adversarial robustness of GNNs. To support the analysis of adversarial robustness in GNNs, we introduce two evaluation metrics: the confidence-based decision surface and the accuracy-based adversarial transferability rate. Through experimental analysis, we derive 11 actionable guidelines for designing robust GNNs, enabling model developers to gain deeper insights. The code of this study is available at https://github.com/star4455/GraphRE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MissionGNN: Hierarchical Multimodal GNN-based Weakly Supervised Video Anomaly Recognition with Mission-Specific Knowledge Graph Generation</title>
<link>https://arxiv.org/abs/2406.18815</link>
<guid>https://arxiv.org/abs/2406.18815</guid>
<content:encoded><![CDATA[
arXiv:2406.18815v3 Announce Type: replace 
Abstract: In the context of escalating safety concerns across various domains, the tasks of Video Anomaly Detection (VAD) and Video Anomaly Recognition (VAR) have emerged as critically important for applications in intelligent surveillance, evidence investigation, violence alerting, etc. These tasks, aimed at identifying and classifying deviations from normal behavior in video data, face significant challenges due to the rarity of anomalies which leads to extremely imbalanced data and the impracticality of extensive frame-level data annotation for supervised learning. This paper introduces a novel hierarchical graph neural network (GNN) based model MissionGNN that addresses these challenges by leveraging a state-of-the-art large language model and a comprehensive knowledge graph for efficient weakly supervised learning in VAR. Our approach circumvents the limitations of previous methods by avoiding heavy gradient computations on large multimodal models and enabling fully frame-level training without fixed video segmentation. Utilizing automated, mission-specific knowledge graph generation, our model provides a practical and efficient solution for real-time video analysis without the constraints of previous segmentation-based or multimodal approaches. Experimental validation on benchmark datasets demonstrates our model's performance in VAD and VAR, highlighting its potential to redefine the landscape of anomaly detection and recognition in video surveillance systems. The code is available here: https://github.com/c0510gy/MissionGNN.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting</title>
<link>https://arxiv.org/abs/2406.19976</link>
<guid>https://arxiv.org/abs/2406.19976</guid>
<content:encoded><![CDATA[
arXiv:2406.19976v2 Announce Type: replace 
Abstract: Bilevel optimization has shown its utility across various machine learning settings, yet most algorithms in practice require second-order information, making it challenging to scale them up. Only recently, a paradigm of first-order algorithms has emerged in the theoretical literature, capable of effectively addressing bilevel optimization problems. Nevertheless, the practical efficiency of this paradigm remains unverified, particularly in the context of large language models (LLMs). This paper introduces the first scalable instantiation of this paradigm called ScaleBiO, focusing on bilevel optimization for large-scale LLM data reweighting. By combining with a recently proposed memory-efficient training technique called LISA, our novel algorithm allows the paradigm to scale to $\sim$30B-sized LLMs on $8\times$H100 GPUs, marking the first successful application of bilevel optimization under practical scenarios for large-sized LLMs. Empirically, extensive experiments on data reweighting verify the effectiveness of ScaleBiO for different-scaled models, including Llama-3-8B, Gemma-2-9B, Qwen-2-7B, and Qwen-2.5-32B, where bilevel optimization succeeds in instruction-following and math reasoning tasks, outperforming several popular baselines, including uniform sampling, influence-aware data filtering, and reference-model-based sampling methods. Theoretically, ScaleBiO ensures the optimality of the learned data weights, along with a convergence guarantee matching the conventional first-order bilevel optimization paradigm on smooth and strongly convex objectives.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully tensorial approach to hypercomplex neural networks</title>
<link>https://arxiv.org/abs/2407.00449</link>
<guid>https://arxiv.org/abs/2407.00449</guid>
<content:encoded><![CDATA[
arXiv:2407.00449v3 Announce Type: replace 
Abstract: Fully tensorial theory of hypercomplex neural networks is given. It allows neural networks to use arithmetic based on arbitrary algebras. The key point is to observe that algebra multiplication can be represented as a rank three tensor and use this tensor in every algebraic operation. This approach is attractive for neural network libraries that support effective tensorial operations. It agrees with previous implementations for four-dimensional algebras.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convex Approximation of Two-Layer ReLU Networks for Hidden State Differential Privacy</title>
<link>https://arxiv.org/abs/2407.04884</link>
<guid>https://arxiv.org/abs/2407.04884</guid>
<content:encoded><![CDATA[
arXiv:2407.04884v3 Announce Type: replace 
Abstract: The hidden state threat model of differential privacy (DP) assumes that the adversary has access only to the final trained machine learning (ML) model, without seeing intermediate states during training. However, the current privacy analyses under this model are restricted to convex optimization problems, reducing their applicability to multi-layer neural networks, which are essential in modern deep learning applications. Notably, the most successful applications of the hidden state privacy analyses in classification tasks have only been for logistic regression models. We demonstrate that it is possible to privately train convex problems with privacy-utility trade-offs comparable to those of 2-layer ReLU networks trained with DP stochastic gradient descent (DP-SGD). This is achieved through a stochastic approximation of a dual formulation of the ReLU minimization problem, resulting in a strongly convex problem. This enables the use of existing hidden state privacy analyses and provides accurate privacy bounds also for the noisy cyclic mini-batch gradient descent (NoisyCGD) method with fixed disjoint mini-batches. Empirical results on benchmark classification tasks demonstrate that NoisyCGD can achieve privacy-utility trade-offs on par with DP-SGD applied to 2-layer ReLU networks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift</title>
<link>https://arxiv.org/abs/2407.18676</link>
<guid>https://arxiv.org/abs/2407.18676</guid>
<content:encoded><![CDATA[
arXiv:2407.18676v2 Announce Type: replace 
Abstract: Reinforcement learning from human feedback (RLHF) aligns Large Language Models (LLMs) with human preferences. However, these preferences can often change over time due to external factors (e.g. environment change and societal influence). Consequently, what was wrong then might be right now. Current preference optimization algorithms do not account for temporal preference drift in their modeling, which can lead to severe misalignment. To address this limitation, we use a Dynamic Bradley-Terry model that models preferences via time-dependent reward functions, and propose Non-Stationary Direct Preference Optimisation (NS-DPO). By introducing a discount parameter in the loss function, NS-DPO applies exponential weighting, which proportionally focuses learning on more time-relevant datapoints. We theoretically analyse the convergence of NS-DPO in the offline setting, providing upper bounds on the estimation error caused by non-stationary preferences. Finally, we demonstrate the effectiveness of NS-DPO for fine-tuning LLMs in scenarios with drifting preferences. By simulating preference drift using renowned reward models and modifying popular LLM datasets accordingly, we show that NS-DPO fine-tuned LLMs remain robust under non-stationarity, significantly outperforming baseline algorithms that ignore temporal preference changes, without sacrificing performance in stationary cases.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence Analysis of Natural Gradient Descent for Over-parameterized Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2408.00573</link>
<guid>https://arxiv.org/abs/2408.00573</guid>
<content:encoded><![CDATA[
arXiv:2408.00573v3 Announce Type: replace 
Abstract: In the context of over-parameterization, there is a line of work demonstrating that randomly initialized (stochastic) gradient descent (GD) converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. However, the learning rate of GD for training two-layer neural networks exhibits poor dependence on the sample size and the Gram matrix, leading to a slow training process. In this paper, we show that for training two-layer $\text{ReLU}^3$ Physics-Informed Neural Networks (PINNs), the learning rate can be improved from $\mathcal{O}(\lambda_0)$ to $\mathcal{O}(1/\|\bm{H}^{\infty}\|_2)$, implying that GD actually enjoys a faster convergence rate. Despite such improvements, the convergence rate is still tied to the least eigenvalue of the Gram matrix, leading to slow convergence. We then develop the positive definiteness of Gram matrices with general smooth activation functions and provide the convergence analysis of natural gradient descent (NGD) in training two-layer PINNs, demonstrating that the learning rate can be $\mathcal{O}(1)$ and at this rate, the convergence rate is independent of the Gram matrix. In particular, for smooth activation functions, the convergence rate of NGD is quadratic. Numerical experiments are conducted to verify our theoretical results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream Machine Learning Services</title>
<link>https://arxiv.org/abs/2408.02814</link>
<guid>https://arxiv.org/abs/2408.02814</guid>
<content:encoded><![CDATA[
arXiv:2408.02814v2 Announce Type: replace 
Abstract: Pre-trained encoders available online have been widely adopted to build downstream machine learning (ML) services, but various attacks against these encoders also post security and privacy threats toward such a downstream ML service paradigm. We unveil a new vulnerability: the Pre-trained Encoder Inference (PEI) attack, which can extract sensitive encoder information from a targeted downstream ML service that can then be used to promote other ML attacks against the targeted service. By only providing API accesses to a targeted downstream service and a set of candidate encoders, the PEI attack can successfully infer which encoder is secretly used by the targeted service based on candidate ones. Compared with existing encoder attacks, which mainly target encoders on the upstream side, the PEI attack can compromise encoders even after they have been deployed and hidden in downstream ML services, which makes it a more realistic threat. We empirically verify the effectiveness of the PEI attack on vision encoders. we first conduct PEI attacks against two downstream services (i.e., image classification and multimodal generation), and then show how PEI attacks can facilitate other ML attacks (i.e., model stealing attacks vs. image classification models and adversarial attacks vs. multimodal generative models). Our results call for new security and privacy considerations when deploying encoders in downstream services. The code is available at https://github.com/fshp971/encoder-inference.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation</title>
<link>https://arxiv.org/abs/2408.09865</link>
<guid>https://arxiv.org/abs/2408.09865</guid>
<content:encoded><![CDATA[
arXiv:2408.09865v2 Announce Type: replace 
Abstract: The Explainable Recommendation task is designed to receive a pair of user and item and output explanations to justify why an item is recommended to a user. Many models approach review generation as a proxy for explainable recommendations. While these models can produce fluent and grammatically correct sentences, they often lack precision and fail to provide personalized, informative recommendations. To address this issue, we propose a personalized, aspect-controlled model called Multi-Aspect Prompt LEarner (MAPLE), which integrates aspect category as another input dimension to facilitate memorizing fine-grained aspect terms. Experiments conducted on two real-world review datasets in the restaurant domain demonstrate that MAPLE significantly outperforms baseline review-generation models. MAPLE excels in both text and feature diversity, ensuring that the generated content covers a wide range of aspects. Additionally, MAPLE delivers good generation quality while maintaining strong coherence and factual relevance. The code and dataset used in this paper can be found here https://github.com/Nana2929/MAPLE.git.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sum of Squares Circuits</title>
<link>https://arxiv.org/abs/2408.11778</link>
<guid>https://arxiv.org/abs/2408.11778</guid>
<content:encoded><![CDATA[
arXiv:2408.11778v3 Announce Type: replace 
Abstract: Designing expressive generative models that support exact and efficient inference is a core question in probabilistic ML. Probabilistic circuits (PCs) offer a framework where this tractability-vs-expressiveness trade-off can be analyzed theoretically. Recently, squared PCs encoding subtractive mixtures via negative parameters have emerged as tractable models that can be exponentially more expressive than monotonic PCs, i.e., PCs with positive parameters only. In this paper, we provide a more precise theoretical characterization of the expressiveness relationships among these models. First, we prove that squared PCs can be less expressive than monotonic ones. Second, we formalize a novel class of PCs -- sum of squares PCs -- that can be exponentially more expressive than both squared and monotonic PCs. Around sum of squares PCs, we build an expressiveness hierarchy that allows us to precisely unify and separate different tractable model classes such as Born Machines and PSD models, and other recently introduced tractable probabilistic models by using complex parameters. Finally, we empirically show the effectiveness of sum of squares circuits in performing distribution estimation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Graph Inductive Representation Learning for Large-Scale Urban Rail Demand Prediction under Disruptions</title>
<link>https://arxiv.org/abs/2408.15619</link>
<guid>https://arxiv.org/abs/2408.15619</guid>
<content:encoded><![CDATA[
arXiv:2408.15619v2 Announce Type: replace 
Abstract: With the expansion of cities over time, URT (Urban Rail Transit) networks have also grown significantly. Demand prediction plays an important role in supporting planning, scheduling, fleet management, and other operational decisions. In this study, we propose an Origin-Destination (OD) demand prediction model called Multi-Graph Inductive Representation Learning (mGraphSAGE) for large-scale URT networks under operational uncertainties. Our main contributions are twofold: we enhance prediction results while ensuring scalability for large networks by relying simultaneously on multiple graphs, where each OD pair is a node on a graph and distinct OD relationships, such as temporal and spatial correlations; we show the importance of including operational uncertainties such as train delays and cancellations as inputs in demand prediction for daily operations. The model is validated on three different scales of the URT network in Copenhagen, Denmark. Experimental results show that by leveraging information from neighboring ODs and learning node representations via sampling and aggregation, mGraphSAGE is particularly suitable for OD demand prediction in large-scale URT networks, outperforming reference machine learning methods. Furthermore, during periods with train cancellations and delays, the performance gap between mGraphSAGE and other methods improves compared to normal operating conditions, demonstrating its ability to leverage system reliability information for predicting OD demand under uncertainty.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in Large Models</title>
<link>https://arxiv.org/abs/2409.02628</link>
<guid>https://arxiv.org/abs/2409.02628</guid>
<content:encoded><![CDATA[
arXiv:2409.02628v2 Announce Type: replace 
Abstract: Epistemic uncertainty is crucial for safety-critical applications and data acquisition tasks. Yet, we find an important phenomenon in deep learning models: an epistemic uncertainty collapse as model complexity increases, challenging the assumption that larger models invariably offer better uncertainty quantification. We introduce implicit ensembling as a possible explanation for this phenomenon. To investigate this hypothesis, we provide theoretical analysis and experiments that demonstrate uncertainty collapse in explicit ensembles of ensembles and show experimental evidence of similar collapse in wider models across various architectures, from simple MLPs to state-of-the-art vision models including ResNets and Vision Transformers. We further develop implicit ensemble extraction techniques to decompose larger models into diverse sub-models, showing we can thus recover epistemic uncertainty. We explore the implications of these findings for uncertainty estimation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Filtration for RLHF to Mitigate Noise in Reward Models</title>
<link>https://arxiv.org/abs/2409.06957</link>
<guid>https://arxiv.org/abs/2409.06957</guid>
<content:encoded><![CDATA[
arXiv:2409.06957v3 Announce Type: replace 
Abstract: While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in the tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination (R2) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation and math reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) which is a more challenging benchmark created by us. In math reasoning, PF-PPO yields performance increase using different reward models and benchmarks (Ape210K and CMATH). Code is available on https://github.com/swtheing/PF-PPO-RLHF.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partial Distribution Matching via Partial Wasserstein Adversarial Networks</title>
<link>https://arxiv.org/abs/2409.10499</link>
<guid>https://arxiv.org/abs/2409.10499</guid>
<content:encoded><![CDATA[
arXiv:2409.10499v2 Announce Type: replace 
Abstract: This paper studies the problem of distribution matching (DM), which is a fundamental machine learning problem seeking to robustly align two probability distributions. Our approach is established on a relaxed formulation, called partial distribution matching (PDM), which seeks to match a fraction of the distributions instead of matching them completely. We theoretically derive the Kantorovich-Rubinstein duality for the partial Wasserstain-1 (PW) discrepancy, and develop a partial Wasserstein adversarial network (PWAN) that efficiently approximates the PW discrepancy based on this dual form. Partial matching can then be achieved by optimizing the network using gradient descent. Two practical tasks, point set registration and partial domain adaptation are investigated, where the goals are to partially match distributions in 3D space and high-dimensional feature space respectively. The experiment results confirm that the proposed PWAN effectively produces highly robust matching results, performing better or on par with the state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nteasee: Understanding Needs in AI for Health in Africa -- A Mixed-Methods Study of Expert and General Population Perspectives</title>
<link>https://arxiv.org/abs/2409.12197</link>
<guid>https://arxiv.org/abs/2409.12197</guid>
<content:encoded><![CDATA[
arXiv:2409.12197v4 Announce Type: replace 
Abstract: Artificial Intelligence (AI) for health has the potential to significantly change and improve healthcare. However in most African countries, identifying culturally and contextually attuned approaches for deploying these solutions is not well understood. To bridge this gap, we conduct a qualitative study to investigate the best practices, fairness indicators, and potential biases to mitigate when deploying AI for health in African countries, as well as explore opportunities where artificial intelligence could make a positive impact in health. We used a mixed methods approach combining in-depth interviews (IDIs) and surveys. We conduct 1.5-2 hour long IDIs with 50 experts in health, policy, and AI across 17 countries, and through an inductive approach we conduct a qualitative thematic analysis on expert IDI responses. We administer a blinded 30-minute survey with case studies to 672 general population participants across 5 countries in Africa and analyze responses on quantitative scales, statistically comparing responses by country, age, gender, and level of familiarity with AI. We thematically summarize open-ended responses from surveys. Our results find generally positive attitudes, high levels of trust, accompanied by moderate levels of concern among general population participants for AI usage for health in Africa. This contrasts with expert responses, where major themes revolved around trust/mistrust, ethical concerns, and systemic barriers to integration, among others. This work presents the first-of-its-kind qualitative research study of the potential of AI for health in Africa from an algorithmic fairness angle, with perspectives from both experts and the general population. We hope that this work guides policymakers and drives home the need for further research and the inclusion of general population perspectives in decision-making around AI usage.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Learning of Linear Systems: Generalization Theory and Applications to Operator Learning</title>
<link>https://arxiv.org/abs/2409.12293</link>
<guid>https://arxiv.org/abs/2409.12293</guid>
<content:encoded><![CDATA[
arXiv:2409.12293v3 Announce Type: replace 
Abstract: We study theoretical guarantees for solving linear systems in-context using a linear transformer architecture. For in-domain generalization, we provide neural scaling laws that bound the generalization error in terms of the number of tasks and sizes of samples used in training and inference. For out-of-domain generalization, we find that the behavior of trained transformers under task distribution shifts depends crucially on the distribution of the tasks seen during training. We introduce a novel notion of task diversity and show that it defines a necessary and sufficient condition for pre-trained transformers generalize under task distribution shifts. We also explore applications of learning linear systems in-context, such as to in-context operator learning for PDEs. Finally, we provide some numerical experiments to validate the established theory.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flat-LoRA: Low-Rank Adaptation over a Flat Loss Landscape</title>
<link>https://arxiv.org/abs/2409.14396</link>
<guid>https://arxiv.org/abs/2409.14396</guid>
<content:encoded><![CDATA[
arXiv:2409.14396v2 Announce Type: replace 
Abstract: Fine-tuning large-scale pre-trained models is prohibitively expensive in terms of computation and memory costs. Low-Rank Adaptation (LoRA), a popular Parameter-Efficient Fine-Tuning (PEFT) method, offers an efficient solution by optimizing only low-rank matrices. Despite recent progress in improving LoRA's performance, the relationship between the LoRA optimization space and the full parameter space is often overlooked. A solution that appears flat in the loss landscape of the LoRA space may still exhibit sharp directions in the full parameter space, potentially compromising generalization. We introduce Flat-LoRA, which aims to identify a low-rank adaptation situated in a flat region of the full parameter space. Instead of adopting the well-established sharpness-aware minimization approach, which incurs significant computation and memory overheads, we employ a Bayesian expectation loss objective to preserve training efficiency. Further, we design a refined random perturbation generation strategy for improved performance and carefully manage memory overhead using random seeds. Experiments across diverse tasks-including mathematical reasoning, coding abilities, dialogue generation, instruction following, and text-to-image generation-demonstrate that Flat-LoRA improves both in-domain and out-of-domain generalization. Code is available at https://github.com/nblt/Flat-LoRA.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CauSkelNet: Causal Representation Learning for Human Behaviour Analysis</title>
<link>https://arxiv.org/abs/2409.15564</link>
<guid>https://arxiv.org/abs/2409.15564</guid>
<content:encoded><![CDATA[
arXiv:2409.15564v3 Announce Type: replace 
Abstract: Traditional machine learning methods for movement recognition often struggle with limited model interpretability and a lack of insight into human movement dynamics. This study introduces a novel representation learning framework based on causal inference to address these challenges. Our two-stage approach combines the Peter-Clark (PC) algorithm and Kullback-Leibler (KL) divergence to identify and quantify causal relationships between human joints. By capturing joint interactions, the proposed causal Graph Convolutional Network (GCN) produces interpretable and robust representations. Experimental results on the EmoPain dataset demonstrate that the causal GCN outperforms traditional GCNs in accuracy, F1 score, and recall, particularly in detecting protective behaviors. This work contributes to advancing human motion analysis and lays a foundation for adaptive and intelligent healthcare solutions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truncated Kernel Stochastic Gradient Descent on Spheres</title>
<link>https://arxiv.org/abs/2410.01570</link>
<guid>https://arxiv.org/abs/2410.01570</guid>
<content:encoded><![CDATA[
arXiv:2410.01570v5 Announce Type: replace 
Abstract: Inspired by the structure of spherical harmonics, we propose the truncated kernel stochastic gradient descent (T-kernel SGD) algorithm with a least-square loss function for spherical data fitting. T-kernel SGD introduces a novel regularization strategy by implementing stochastic gradient descent through a closed-form solution of the projection of the stochastic gradient in a low-dimensional subspace. In contrast to traditional kernel SGD, the regularization strategy implemented by T-kernel SGD is more effective in balancing bias and variance by dynamically adjusting the hypothesis space during iterations. The most significant advantage of the proposed algorithm is that it can achieve theoretically optimal convergence rates using a constant step size (independent of the sample size) while overcoming the inherent saturation problem of kernel SGD. Additionally, we leverage the structure of spherical polynomials to derive an equivalent T-kernel SGD, significantly reducing storage and computational costs compared to kernel SGD. Typically, T-kernel SGD requires only $\mathcal{O}(n^{1+\frac{d}{d-1}\epsilon})$ computational complexity and $\mathcal{O}(n^{\frac{d}{d-1}\epsilon})$ storage to achieve optimal rates for the d-dimensional sphere, where $0<\epsilon<\frac{1}{2}$ can be arbitrarily small if the optimal fitting or the underlying space possesses sufficient regularity. This regularity is determined by the smoothness parameter of the objective function and the decaying rate of the eigenvalues of the integral operator associated with the kernel function, both of which reflect the difficulty of the estimation problem. Our main results quantitatively characterize how this prior information influences the convergence of T-kernel SGD. The numerical experiments further validate the theoretical findings presented in this paper.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sable: a Performant, Efficient and Scalable Sequence Model for MARL</title>
<link>https://arxiv.org/abs/2410.01706</link>
<guid>https://arxiv.org/abs/2410.01706</guid>
<content:encoded><![CDATA[
arXiv:2410.01706v5 Announce Type: replace 
Abstract: As multi-agent reinforcement learning (MARL) progresses towards solving larger and more complex problems, it becomes increasingly important that algorithms exhibit the key properties of (1) strong performance, (2) memory efficiency, and (3) scalability. In this work, we introduce Sable, a performant, memory-efficient, and scalable sequence modeling approach to MARL. Sable works by adapting the retention mechanism in Retentive Networks (Sun et al., 2023) to achieve computationally efficient processing of multi-agent observations with long context memory for temporal reasoning. Through extensive evaluations across six diverse environments, we demonstrate how Sable is able to significantly outperform existing state-of-the-art methods in a large number of diverse tasks (34 out of 45 tested). Furthermore, Sable maintains performance as we scale the number of agents, handling environments with more than a thousand agents while exhibiting a linear increase in memory usage. Finally, we conduct ablation studies to isolate the source of Sable's performance gains and confirm its efficient computational memory usage.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis</title>
<link>https://arxiv.org/abs/2410.02167</link>
<guid>https://arxiv.org/abs/2410.02167</guid>
<content:encoded><![CDATA[
arXiv:2410.02167v3 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) is an efficient prompting method that enables the reasoning ability of large language models by augmenting the query using multiple examples with multiple intermediate steps. Despite the empirical success, the theoretical understanding of how to train a Transformer to achieve the CoT ability remains less explored. This is primarily due to the technical challenges involved in analyzing the nonconvex optimization on nonlinear attention models. To the best of our knowledge, this work provides the first theoretical study of training Transformers with nonlinear attention to obtain the CoT generalization capability so that the resulting model can inference on unseen tasks when the input is augmented by examples of the new task. We first quantify the required training samples and iterations to train a Transformer model towards CoT ability. We then prove the success of its CoT generalization on unseen tasks with distribution-shifted testing data. Moreover, we theoretically characterize the conditions for an accurate reasoning output by CoT even when the provided reasoning examples contain noises and are not always accurate. In contrast, in-context learning (ICL), which can be viewed as one-step CoT without intermediate steps, may fail to provide an accurate output when CoT does. These theoretical findings are justified through experiments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling AI's Blind Spots: An Oracle for In-Domain, Out-of-Domain, and Adversarial Errors</title>
<link>https://arxiv.org/abs/2410.02384</link>
<guid>https://arxiv.org/abs/2410.02384</guid>
<content:encoded><![CDATA[
arXiv:2410.02384v3 Announce Type: replace 
Abstract: AI models make mistakes when recognizing images-whether in-domain, out-of-domain, or adversarial. Predicting these errors is critical for improving system reliability, reducing costly mistakes, and enabling proactive corrections in real-world applications such as healthcare, finance, and autonomous systems. However, understanding what mistakes AI models make, why they occur, and how to predict them remains an open challenge. Here, we conduct comprehensive empirical evaluations using a "mentor" model-a deep neural network designed to predict another "mentee" model's errors. Our findings show that the mentor excels at learning from a mentee's mistakes on adversarial images with small perturbations and generalizes effectively to predict in-domain and out-of-domain errors of the mentee. Additionally, transformer-based mentor models excel at predicting errors across various mentee architectures. Subsequently, we draw insights from these observations and develop an "oracle" mentor model, dubbed SuperMentor, that can outperform baseline mentors in predicting errors across different error types from the ImageNet-1K dataset. Our framework paves the way for future research on anticipating and correcting AI model behaviors, ultimately increasing trust in AI systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2410.02681</link>
<guid>https://arxiv.org/abs/2410.02681</guid>
<content:encoded><![CDATA[
arXiv:2410.02681v2 Announce Type: replace 
Abstract: Confidence calibration is critical for the safe deployment of machine learning models in the real world. However, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed. In this work, we demonstrate that existing prompt tuning methods usually lead to a trade-off of calibration between base and new classes: the cross-entropy loss in CoOp causes overconfidence in new classes by increasing textual label divergence, whereas the regularization of KgCoOp maintains the confidence level but results in underconfidence in base classes due to the improved accuracy. Inspired by the observations, we introduce Dynamic Outlier Regularization (DOR) to ensure the confidence calibration on both base and new classes after fine-tuning. In particular, we propose to minimize the feature deviation of novel textual labels (instead of base classes) sampled from a large vocabulary. In effect, DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes. Extensive experiments demonstrate that DOR can enhance the calibration performance of current fine-tuning methods on base and new classes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying perturbation targets through causal differential networks</title>
<link>https://arxiv.org/abs/2410.03380</link>
<guid>https://arxiv.org/abs/2410.03380</guid>
<content:encoded><![CDATA[
arXiv:2410.03380v3 Announce Type: replace 
Abstract: Identifying variables responsible for changes to a biological system enables applications in drug target discovery and cell engineering. Given a pair of observational and interventional datasets, the goal is to isolate the subset of observed variables that were the targets of the intervention. Directly applying causal discovery algorithms is challenging: the data may contain thousands of variables with as few as tens of samples per intervention, and biological systems do not adhere to classical causality assumptions. We propose a causality-inspired approach to address this practical setting. First, we infer noisy causal graphs from the observational and interventional data. Then, we learn to map the differences between these graphs, along with additional statistical features, to sets of variables that were intervened upon. Both modules are jointly trained in a supervised framework, on simulated and real data that reflect the nature of biological interventions. This approach consistently outperforms baselines for perturbation modeling on seven single-cell transcriptomics datasets. We also demonstrate significant improvements over current causal discovery methods for predicting soft and hard intervention targets across a variety of synthetic data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs</title>
<link>https://arxiv.org/abs/2410.06431</link>
<guid>https://arxiv.org/abs/2410.06431</guid>
<content:encoded><![CDATA[
arXiv:2410.06431v3 Announce Type: replace 
Abstract: Accurate uncertainty quantification in large language models (LLMs) is essential for providing credible confidence estimates over their outputs. However, fine-tuned LLMs often exhibit overconfidence in uncertain predictions, which stems from their limited ability to generalize with sparse data. Existing parameter efficient fine-tuning (PEFT) uncertainty quantification methods for LLMs focus on post fine-tuning stage, and thus fail to address the core issue: limited specialization of PEFT adapters to accurately capture task-specific input-output relationships. To address these limitations, we propose Functional-Level Uncertainty Quantification for Calibrated Fine-Tuning (UQ4CT), which captures and calibrates uncertainty over the space of functions that map input prompts to outputs. We implement UQ4CT during the fine-tuning stage via a mixture-of-experts framework that hierarchically decomposes the functional space. Empirically, UQ4CT achieves over $25\%$ reduction in Expected Calibration Error (ECE) while preserving high accuracy across five benchmarks. Even under distribution shift, UQ4CT maintains superior ECE performance with high accuracy, showcasing improved generalizability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Why Large Language Models Can Be Ineffective in Time Series Analysis: The Impact of Modality Alignment</title>
<link>https://arxiv.org/abs/2410.12326</link>
<guid>https://arxiv.org/abs/2410.12326</guid>
<content:encoded><![CDATA[
arXiv:2410.12326v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance in time series analysis and seems to understand the time temporal relationship well than traditional transformer-based approaches. However, since LLMs are not designed for time series tasks, simpler models like linear regressions can often achieve comparable performance with far less complexity. In this study, we perform extensive experiments to assess the effectiveness of applying LLMs to key time series tasks, including forecasting, classification, imputation, and anomaly detection. We compare the performance of LLMs against simpler baseline models, such as single layer linear models and randomly initialized LLMs. Our results reveal that LLMs offer minimal advantages for these core time series tasks and may even distort the temporal structure of the data. In contrast, simpler models consistently outperform LLMs while requiring far fewer parameters. Furthermore, we analyze existing reprogramming techniques and show, through data manifold analysis, that these methods fail to effectively align time series data with language and display "pseudo-alignment" behavior in embedding space. Our findings suggest that the performance of LLM based methods in time series tasks arises from the intrinsic characteristics and structure of time series data, rather than any meaningful alignment with the language model architecture.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Step Diffusion via Shortcut Models</title>
<link>https://arxiv.org/abs/2410.12557</link>
<guid>https://arxiv.org/abs/2410.12557</guid>
<content:encoded><![CDATA[
arXiv:2410.12557v2 Announce Type: replace 
Abstract: Diffusion models and flow-matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce shortcut models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-Guided Multi-Property Molecular Optimization with a Diffusion Language Model</title>
<link>https://arxiv.org/abs/2410.13597</link>
<guid>https://arxiv.org/abs/2410.13597</guid>
<content:encoded><![CDATA[
arXiv:2410.13597v2 Announce Type: replace 
Abstract: Molecular optimization (MO) is a crucial stage in drug discovery in which task-oriented generated molecules are optimized to meet practical industrial requirements. Existing mainstream MO approaches primarily utilize external property predictors to guide iterative property optimization. However, learning all molecular samples in the vast chemical space is unrealistic for predictors. As a result, errors and noise are inevitably introduced during property prediction due to the nature of approximation. This leads to discrepancy accumulation, generalization reduction and suboptimal molecular candidates. In this paper, we propose a text-guided multi-property molecular optimization method utilizing transformer-based diffusion language model (TransDLM). TransDLM leverages standardized chemical nomenclature as semantic representations of molecules and implicitly embeds property requirements into textual descriptions, thereby mitigating error propagation during diffusion process. By fusing physically and chemically detailed textual semantics with specialized molecular representations, TransDLM effectively integrates diverse information sources to guide precise optimization, which enhances the model's ability to balance structural retention and property enhancement. Additionally, the success of a case study further demonstrates TransDLM's ability to solve practical problems. Experimentally, our approach surpasses state-of-the-art methods in maintaining molecular structural similarity and enhancing chemical properties on the benchmark dataset. The code is available at: https://github.com/Cello2195/TransDLM.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence Functions for Scalable Data Attribution in Diffusion Models</title>
<link>https://arxiv.org/abs/2410.13850</link>
<guid>https://arxiv.org/abs/2410.13850</guid>
<content:encoded><![CDATA[
arXiv:2410.13850v5 Announce Type: replace 
Abstract: Diffusion models have led to significant advancements in generative modelling. Yet their widespread adoption poses challenges regarding data attribution and interpretability. In this paper, we aim to help address such challenges in diffusion models by developing an influence functions framework. Influence function-based data attribution methods approximate how a model's output would have changed if some training data were removed. In supervised learning, this is usually used for predicting how the loss on a particular example would change. For diffusion models, we focus on predicting the change in the probability of generating a particular example via several proxy measurements. We show how to formulate influence functions for such quantities and how previously proposed methods can be interpreted as particular design choices in our framework. To ensure scalability of the Hessian computations in influence functions, we systematically develop K-FAC approximations based on generalised Gauss-Newton matrices specifically tailored to diffusion models. We recast previously proposed methods as specific design choices in our framework and show that our recommended method outperforms previous data attribution approaches on common evaluations, such as the Linear Data-modelling Score (LDS) or retraining without top influences, without the need for method-specific hyperparameter tuning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On The Global Convergence Of Online RLHF With Neural Parametrization</title>
<link>https://arxiv.org/abs/2410.15610</link>
<guid>https://arxiv.org/abs/2410.15610</guid>
<content:encoded><![CDATA[
arXiv:2410.15610v2 Announce Type: replace 
Abstract: The importance of Reinforcement Learning from Human Feedback (RLHF) in aligning large language models (LLMs) with human values cannot be overstated. RLHF is a three-stage process that includes supervised fine-tuning (SFT), reward learning, and policy learning. Although there are several offline and online approaches to aligning LLMs, they often suffer from distribution shift issues. These issues arise from the inability to accurately capture the distributional interdependence between the reward learning and policy learning stages. Consequently, this has led to various approximated approaches, but the theoretical insights and motivations remain largely limited to tabular settings, which do not hold in practice. This gap between theoretical insights and practical implementations is critical. It is challenging to address this gap as it requires analyzing the performance of AI alignment algorithms in neural network-parameterized settings. Although bi-level formulations have shown promise in addressing distribution shift issues, they suffer from the hyper-gradient problem, and current approaches lack efficient algorithms to solve this. In this work, we tackle these challenges employing the bi-level formulation laid out in Kwon et al. (2024) along with the assumption \emph{Weak Gradient Domination} to demonstrate convergence in an RLHF setup, obtaining a sample complexity of $\epsilon^{-\frac{7}{2}}$ . Our key contributions are twofold: (i) We propose a bi-level formulation for AI alignment in parameterized settings and introduce a first-order approach to solve this problem. (ii) We analyze the theoretical convergence rates of the proposed algorithm and derive state-of-the-art bounds. To the best of our knowledge, this is the first work to establish convergence rate bounds and global optimality for the RLHF framework in neural network-parameterized settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interacting Large Language Model Agents. Interpretable Models and Social Learning</title>
<link>https://arxiv.org/abs/2411.01271</link>
<guid>https://arxiv.org/abs/2411.01271</guid>
<content:encoded><![CDATA[
arXiv:2411.01271v2 Announce Type: replace 
Abstract: This paper discusses the theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making involving interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors. This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under 2 settings: (a) centrally controlled LLMAs (b) autonomous LLMAs with incentives. We demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like LLaMA and closed-source models like ChatGPT. The main takeaway of this paper, based on empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best-Arm Identification in Unimodal Bandits</title>
<link>https://arxiv.org/abs/2411.01898</link>
<guid>https://arxiv.org/abs/2411.01898</guid>
<content:encoded><![CDATA[
arXiv:2411.01898v2 Announce Type: replace 
Abstract: We study the fixed-confidence best-arm identification problem in unimodal bandits, in which the means of the arms increase with the index of the arm up to their maximum, then decrease. We derive two lower bounds on the stopping time of any algorithm. The instance-dependent lower bound suggests that due to the unimodal structure, only three arms contribute to the leading confidence-dependent cost. However, a worst-case lower bound shows that a linear dependence on the number of arms is unavoidable in the confidence-independent cost. We propose modifications of Track-and-Stop and a Top Two algorithm that leverage the unimodal structure. Both versions of Track-and-Stop are asymptotically optimal for one-parameter exponential families. The Top Two algorithm is asymptotically near-optimal for Gaussian distributions and we prove a non-asymptotic guarantee matching the worse-case lower bound. The algorithms can be implemented efficiently and we demonstrate their competitive empirical performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Aware Resource Management for C-V2X Platooning via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2411.04672</link>
<guid>https://arxiv.org/abs/2411.04672</guid>
<content:encoded><![CDATA[
arXiv:2411.04672v2 Announce Type: replace 
Abstract: Semantic communication transmits the extracted features of information rather than raw data, significantly reducing redundancy, which is crucial for addressing spectrum and energy challenges in 6G networks. In this paper, we introduce semantic communication into a cellular vehicle-to-everything (C-V2X)- based autonomous vehicle platoon system for the first time, aiming to achieve efficient management of communication resources in a dynamic environment. Firstly, we construct a mathematical model for semantic communication in platoon systems, in which the DeepSC model and MU-DeepSC model are used to semantically encode and decode unimodal and multi-modal data, respectively. Then, we propose the quality of experience (QoE) metric based on semantic similarity and semantic rate. Meanwhile, we consider the success rate of semantic information transmission (SRS) metric to ensure the fairness of channel resource allocation. Next, the optimization problem is posed with the aim of maximizing the QoE in vehicle-to-vehicle (V2V) links while improving SRS. To solve this mixed integer nonlinear programming problem (MINLP) and adapt to time-varying channel conditions, the paper proposes a distributed semantic-aware multi-modal resource allocation (SAMRA) algorithm based on multi-agent reinforcement learning (MARL), referred to as SAMRAMARL. The algorithm can dynamically allocate channels and power and determine semantic symbol length based on the contextual importance of the transmitted information, ensuring efficient resource utilization. Finally, extensive simulations have demonstrated that SAMRAMARL outperforms existing methods, achieving significant gains in QoE, SRS, and communication delay in C-V2X platooning scenarios.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Hidden Monotone Variational Inequalities with Surrogate Losses</title>
<link>https://arxiv.org/abs/2411.05228</link>
<guid>https://arxiv.org/abs/2411.05228</guid>
<content:encoded><![CDATA[
arXiv:2411.05228v3 Announce Type: replace 
Abstract: Deep learning has proven to be effective in a wide variety of loss minimization problems. However, many applications of interest, like minimizing projected Bellman error and min-max optimization, cannot be modelled as minimizing a scalar loss function but instead correspond to solving a variational inequality (VI) problem. This difference in setting has caused many practical challenges as naive gradient-based approaches from supervised learning tend to diverge and cycle in the VI case. In this work, we propose a principled surrogate-based approach compatible with deep learning to solve VIs. We show that our surrogate-based approach has three main benefits: (1) under assumptions that are realistic in practice (when hidden monotone structure is present, interpolation, and sufficient optimization of the surrogates), it guarantees convergence, (2) it provides a unifying perspective of existing methods, and (3) is amenable to existing deep learning optimizers like ADAM. Experimentally, we demonstrate our surrogate-based approach is effective in min-max optimization and minimizing projected Bellman error. Furthermore, in the deep reinforcement learning case, we propose a novel variant of TD(0) which is more compute and sample efficient.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Mixtures of Experts with EM: A Mirror Descent Perspective</title>
<link>https://arxiv.org/abs/2411.06056</link>
<guid>https://arxiv.org/abs/2411.06056</guid>
<content:encoded><![CDATA[
arXiv:2411.06056v2 Announce Type: replace 
Abstract: Classical Mixtures of Experts (MoE) are Machine Learning models that involve partitioning the input space, with a separate "expert" model trained on each partition. Recently, MoE-based model architectures have become popular as a means to reduce training and inference costs. There, the partitioning function and the experts are both learnt jointly via gradient descent-type methods on the log-likelihood. In this paper we study theoretical guarantees of the Expectation Maximization (EM) algorithm for the training of MoE models. We first rigorously analyze EM for MoE where the conditional distribution of the target and latent variable conditioned on the feature variable belongs to an exponential family of distributions and show its equivalence to projected Mirror Descent with unit step size and a Kullback-Leibler Divergence regularizer. This perspective allows us to derive new convergence results and identify conditions for local linear convergence; In the special case of mixture of $2$ linear or logistic experts, we additionally provide guarantees for linear convergence based on the signal-to-noise ratio. Experiments on synthetic and (small-scale) real-world data supports that EM outperforms the gradient descent algorithm both in terms of convergence rate and the achieved accuracy.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolMiner: Towards Controllable, 3D-Aware, Fragment-Based Molecular Design</title>
<link>https://arxiv.org/abs/2411.06608</link>
<guid>https://arxiv.org/abs/2411.06608</guid>
<content:encoded><![CDATA[
arXiv:2411.06608v2 Announce Type: replace 
Abstract: We introduce MolMiner, a fragment-based, geometry-aware, and order-agnostic autoregressive model for molecular design. MolMiner supports conditional generation of molecules over twelve properties, enabling flexible control across physicochemical and structural targets. Molecules are built via symmetry-aware fragment attachments, with 3D geometry dynamically updated during generation using forcefields. A probabilistic conditioning mechanism allows users to specify any subset of target properties while sampling the rest. MolMiner achieves calibrated conditional generation across most properties and offers competitive unconditional performance. We also propose improved benchmarking methods for both unconditional and conditional generation, including distributional comparisons via Wasserstein distance and calibration plots for property control. To our knowledge, this is the first model to unify dynamic geometry, symmetry handling, order-agnostic fragment-based generation, and high-dimensional multi-property conditioning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lean and Mean Adaptive Optimization via Subset-Norm and Subspace-Momentum with Convergence Guarantees</title>
<link>https://arxiv.org/abs/2411.07120</link>
<guid>https://arxiv.org/abs/2411.07120</guid>
<content:encoded><![CDATA[
arXiv:2411.07120v2 Announce Type: replace 
Abstract: We introduce two complementary techniques for efficient optimization that reduce memory requirements while accelerating training of large-scale neural networks. The first technique, Subset-Norm step size, generalizes AdaGrad-Norm and AdaGrad(-Coordinate) through step-size sharing. Subset-Norm (SN) reduces AdaGrad's memory footprint from $O(d)$ to $O(\sqrt{d})$, where $d$ is the model size. For non-convex smooth objectives under coordinate-wise sub-gaussian noise, we show a noise-adapted high-probability convergence guarantee with improved dimensional dependence of SN over existing methods. Our second technique, Subspace-Momentum, reduces the momentum state's memory footprint by restricting momentum to a low-dimensional subspace while performing SGD in the orthogonal complement. We prove a high-probability convergence result for Subspace-Momentum under standard assumptions. Empirical evaluation on pre-training and fine-tuning LLMs demonstrates the effectiveness of our methods. For instance, combining Subset-Norm with Subspace-Momentum achieves Adam's validation perplexity for LLaMA 1B in approximately half the training tokens (6.8B vs 13.1B) while reducing Adam's optimizer-states memory footprint by more than 80\% with minimal additional hyperparameter tuning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Comparisons Between Representations</title>
<link>https://arxiv.org/abs/2411.08739</link>
<guid>https://arxiv.org/abs/2411.08739</guid>
<content:encoded><![CDATA[
arXiv:2411.08739v3 Announce Type: replace 
Abstract: Which neural networks are similar is a fundamental question for both machine learning and neuroscience. Here, it is proposed to base comparisons on the predictive distributions of linear readouts from intermediate representations. In Bayesian statistics, the prior predictive distribution is a full description of the inductive bias and generalization of a model, making it a great basis for comparisons. This distribution directly gives the evidence a dataset would provide in favor of the model. If we want to compare multiple models to each other, we can use a metric for probability distributions like the Jensen-Shannon distance or the total variation distance. As these are metrics, this induces pseudo-metrics for representations, which measure how well two representations could be distinguished based on a linear read out. For a linear readout with a Gaussian prior on the read-out weights and Gaussian noise, we can analytically compute the (prior and posterior) predictive distributions without approximations. These distributions depend only on the linear kernel matrix of the representations in the model. Thus, the Bayesian metrics connect to both linear read-out based comparisons and kernel based metrics like centered kernel alignment and representational similarity analysis. The new methods are demonstrated with deep neural networks trained on ImageNet-1k comparing them to each other and a small subset of the Natural Scenes Dataset. The Bayesian comparisons are correlated to but distinct from existing metrics. Evaluations vary slightly less across random image samples and yield informative results with full uncertainty information. Thus the proposed Bayesian metrics nicely extend our toolkit for comparing representations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stealing Training Graphs from Graph Neural Networks</title>
<link>https://arxiv.org/abs/2411.11197</link>
<guid>https://arxiv.org/abs/2411.11197</guid>
<content:encoded><![CDATA[
arXiv:2411.11197v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have shown promising results in modeling graphs in various tasks. The training of GNNs, especially on specialized tasks such as bioinformatics, demands extensive expert annotations, which are expensive and usually contain sensitive information of data providers. The trained GNN models are often shared for deployment in the real world. As neural networks can memorize the training samples, the model parameters of GNNs have a high risk of leaking private training data. Our theoretical analysis shows the strong connections between trained GNN parameters and the training graphs used, confirming the training graph leakage issue. However, explorations into training data leakage from trained GNNs are rather limited. Therefore, we investigate a novel problem of stealing graphs from trained GNNs. To obtain high-quality graphs that resemble the target training set, a graph diffusion model with diffusion noise optimization is deployed as a graph generator. Furthermore, we propose a selection method that effectively leverages GNN model parameters to identify training graphs from samples generated by the graph diffusion model. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed framework in stealing training graphs from the trained GNN.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Best Arm Identification in Stochastic Linear Bandits</title>
<link>https://arxiv.org/abs/2411.13690</link>
<guid>https://arxiv.org/abs/2411.13690</guid>
<content:encoded><![CDATA[
arXiv:2411.13690v2 Announce Type: replace 
Abstract: We study the problem of collaborative best-arm identification in stochastic linear bandits under a fixed-budget scenario. In our learning model, we first consider multiple agents connected through a star network, interacting with a linear bandit instance in parallel. We then extend our analysis to arbitrary network topologies. The objective of the agents is to collaboratively identify the best arm of the given bandit instance with the help of a central server while minimizing the probability of error in best arm estimation. To this end, we propose two algorithms, MaLinBAI-Star and MaLinBAI-Gen for star networks and networks with arbitrary structure, respectively. Both algorithms utilize the technique of G-optimal design along with the successive elimination based strategy where agents share their knowledge through a central server at each communication round. We demonstrate, both theoretically and empirically, that our algorithms achieve exponentially decaying probability of error in the allocated time budget. Furthermore, experimental results on both synthetic and real-world data validate the effectiveness of our algorithms over the state-of-the art existing multi-agent algorithms.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers</title>
<link>https://arxiv.org/abs/2411.14507</link>
<guid>https://arxiv.org/abs/2411.14507</guid>
<content:encoded><![CDATA[
arXiv:2411.14507v2 Announce Type: replace 
Abstract: Generative Pre-trained Transformers (GPTs) have demonstrated remarkable performance across diverse domains, largely due to the extensive scaling of model parameters. Recent works have observed redundancy within transformer blocks and developed compression methods by structured pruning of less important blocks. However, such direct removal often leads to irreversible performance degradation. In this paper, we propose FuseGPT, a novel methodology designed to recycle pruned transformer blocks, thereby recovering the model's performance. Firstly, we introduce a new importance detection metric, Macro Influence (MI), which evaluates the long-term impact of each transformer block by quantifying the information loss incurred upon its removal. Next, we propose group-level layer fusion, which leverages the parameters from layers of less important blocks and integrates them into the corresponding layers of neighboring blocks. This fusion process is not a one-time operation but is refined through iterative parameter updates by lightweight group-level fine-tuning. Specifically, the injected parameters are frozen but are weighted with learnable rank decomposition matrices to reduce the computational overhead during fine-tuning. Our approach not only works well for large language models but also for large multimodal models. Experimental results indicate that, even with modest amounts of data, FuseGPT surpasses previous methods in both perplexity and zero-shot task performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Continual Graph Learning</title>
<link>https://arxiv.org/abs/2411.18919</link>
<guid>https://arxiv.org/abs/2411.18919</guid>
<content:encoded><![CDATA[
arXiv:2411.18919v2 Announce Type: replace 
Abstract: In the era of big data, managing evolving graph data poses substantial challenges due to storage costs and privacy issues. Training graph neural networks (GNNs) on such evolving data usually causes catastrophic forgetting, impairing performance on earlier tasks. Despite existing continual graph learning (CGL) methods mitigating this to some extent, they rely on centralized architectures and ignore the potential of distributed graph databases to leverage collective intelligence. To address these challenges, we present a pioneering study on Federated Continual Graph Learning (FCGL), which adapts GNNs to multiple evolving graphs within decentralized settings while adhering to storage and privacy constraints. Our work begins with a comprehensive empirical analysis of FCGL, assessing its data characteristics, feasibility, and effectiveness, and reveals two non-trivial challenges: local graph forgetting (LGF), where local GNNs forget prior knowledge when adapting to new tasks, and global expertise conflict (GEC), where the global GNN exhibits sub-optimal performance in both adapting to new tasks and retaining old ones, arising from inconsistent client expertise during server-side parameter aggregation. To tackle these, we propose the POWER framework, which mitigates LGF by preserving and replaying experience nodes with maximum local-global coverage at each client and addresses GEC by using a pseudo prototype reconstruction strategy and trajectory-aware knowledge transfer at the central server. Experiments on various graph datasets demonstrate POWER's superiority over federated adaptations of CGL baselines and vision-centric federated continual learning approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiMoE: Heterogeneity-Informed Mixture-of-Experts for Fair Spatial-Temporal Forecasting</title>
<link>https://arxiv.org/abs/2412.00316</link>
<guid>https://arxiv.org/abs/2412.00316</guid>
<content:encoded><![CDATA[
arXiv:2412.00316v3 Announce Type: replace 
Abstract: Achieving both accurate and consistent predictive performance across spatial nodes is crucial for ensuring the validity and reliability of outcomes in fair spatial-temporal forecasting tasks. However, existing training methods treat heterogeneous nodes with a fully averaged perspective, resulting in inherently biased prediction targets. Balancing accuracy and consistency is particularly challenging due to the multi-objective nature of spatial-temporal forecasting. To address this issue, we propose a novel Heterogeneity-Informed Mixture-of-Experts (HiMoE) framework that delivers both uniform and precise spatial-temporal predictions. From a model architecture perspective, we design the Heterogeneity-Informed Graph Convolutional Network (HiGCN) to address trend heterogeneity, and we introduce the Node-wise Mixture-of-Experts (NMoE) module to handle cardinality heterogeneity across nodes. From an evaluation perspective, we propose STFairBench, a benchmark that handles fairness in spatial-temporal prediction from both training and evaluation stages. Extensive experiments on four real-world datasets demonstrate that HiMoE achieves state-of-the-art performance, outperforming the best baseline by at least 9.22% across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Mamba as a Continual Learner: Meta-learning Selective State Space Models for Efficient Continual Learning</title>
<link>https://arxiv.org/abs/2412.00776</link>
<guid>https://arxiv.org/abs/2412.00776</guid>
<content:encoded><![CDATA[
arXiv:2412.00776v4 Announce Type: replace 
Abstract: Continual learning (CL) aims to efficiently learn from a non-stationary data stream, without storing or recomputing all seen samples. CL enables prediction on new tasks by incorporating sequential training samples. Building on this connection between CL and sequential modeling, meta-continual learning (MCL) aims to meta-learn an efficient continual learner as a sequence prediction model, with advanced sequence models like Transformers being natural choices. However, despite decent performance, Transformers rely on a linearly growing cache to store all past representations, conflicting with CL's objective of not storing all seen samples and limiting efficiency. In this paper, we focus on meta-learning sequence-prediction-based continual learners without retaining all past representations. While attention-free models with fixed-size hidden states (e.g., Linear Transformers) align with CL's essential goal and efficiency needs, they have shown limited effectiveness in MCL in previous literature. Given Mamba's strong sequence modeling performance and attention-free nature, we explore a key question: Can attention-free models like Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks, we propose MambaCL, a meta-learned continual learner. To enhance MambaCL's training, we introduce selectivity regularization, leveraging the connection between Mamba and Transformers to guide its behavior over sequences. Furthermore, we study how Mamba and other models perform across various MCL scenarios through extensive and well-designed experiments. Our results highlight the promising performance and strong generalization of Mamba and attention-free models in MCL, demonstrating its potential for efficient continual learning and adaptation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration</title>
<link>https://arxiv.org/abs/2412.16216</link>
<guid>https://arxiv.org/abs/2412.16216</guid>
<content:encoded><![CDATA[
arXiv:2412.16216v2 Announce Type: replace 
Abstract: The sparse Mixture-of-Experts (MoE) architecture of large language models (LLMs) confronts an inherent issue of load imbalance arising from the simplistic linear router strategy, which ultimately causes the instability and inefficient learning of LLMs. To address this challenge, we introduce a novel MoE graph-based framework $\textbf{GMoE}$, aimed at enhancing the collaboration among multiple experts. In GMoE, a graph router function is designed to capture the collaboration signals among experts. This enables all experts to dynamically allocate information derived from input data by sharing information with their neighboring experts. Moreover, we put forward two coordination strategies in GMoE: the $\textit{Poisson distribution-based distinction strategy}$ and the $\textit{Normal distribution-based balance strategy}$, to further release the capacity of each expert and increase the model stability in the fine-tuning of LLMs. Specifically, we leverage a parameter-efficient fine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph MoE architecture. Extensive experiments on four real-world benchmark datasets demonstrate the effectiveness of GMoE, showing the benefits of facilitating collaborations of multiple experts in LLM fine-tuning. The code of experimental implementation is available at https://github.com/BAI-LAB/GMoE
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-Trees</title>
<link>https://arxiv.org/abs/2412.16441</link>
<guid>https://arxiv.org/abs/2412.16441</guid>
<content:encoded><![CDATA[
arXiv:2412.16441v3 Announce Type: replace 
Abstract: Foundation models are pretrained on large-scale corpora to learn generalizable patterns across domains and tasks -- such as contours, textures, and edges in images, or tokens and sentences in text. In contrast, discovering such generalities in graph-structured data, especially across heterogeneous graph tasks, remains an open challenge. To address this, we propose a novel approach to cross-task generalization in graphs via task-trees, which serve as unified learning instances aligning node-, edge-, and graph-level tasks. We theoretically analyze the stability, transferability, and generalization properties of task-trees, showing that pretraining a graph neural network (GNN) on diverse task-trees with a reconstruction objective induces transferable knowledge. This enables efficient adaptation to downstream tasks with minimal fine-tuning. To validate our framework, we introduce Graph Generality Identifier on Task-Trees (GIT), a graph foundation model that demonstrates strong performance on over 30 graphs across five domains via fine-tuning, in-context learning, and zero-shot generalization. Code and data are available at https://github.com/Zehong-Wang/GIT.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Multi-Task Model Merging as Adaptive Projective Gradient Descent</title>
<link>https://arxiv.org/abs/2501.01230</link>
<guid>https://arxiv.org/abs/2501.01230</guid>
<content:encoded><![CDATA[
arXiv:2501.01230v3 Announce Type: replace 
Abstract: Merging multiple expert models offers a promising approach for performing multi-task learning without accessing their original data. Existing methods attempt to alleviate task conflicts by sparsifying task vectors or promoting orthogonality among them. However, they overlook the fundamental target of model merging: the merged model performs as closely as possible to task-specific models on respective tasks. We find these methods inevitably discard task-specific information that, while causing conflicts, is crucial for performance. Based on our findings, we frame model merging as a constrained optimization problem ($\textit{i.e.}$, minimizing the gap between the merged model and individual models, subject to the constraint of retaining shared knowledge) and solve it via adaptive projective gradient descent. Specifically, we align the merged model with individual models by decomposing and reconstituting the loss function, alleviating conflicts through $\textit{data-free}$ optimization of task vectors. To retain shared knowledge, we optimize this objective by projecting gradients within a $\textit{shared subspace}$ spanning all tasks. Moreover, we view merging coefficients as adaptive learning rates and propose a task-aware, training-free strategy. Experiments show that our plug-and-play approach consistently outperforms previous methods, achieving state-of-the-art results across diverse architectures and tasks in both vision and NLP domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusion-DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent Hypersonic and Supersonic Flows</title>
<link>https://arxiv.org/abs/2501.01934</link>
<guid>https://arxiv.org/abs/2501.01934</guid>
<content:encoded><![CDATA[
arXiv:2501.01934v2 Announce Type: replace 
Abstract: Shape optimization is essential in aerospace vehicle design, including reentry systems, and propulsion system components, as it directly influences aerodynamic efficiency, structural integrity, and overall mission success. Rapid and accurate prediction of external and internal flows accelerates design iterations. To this end, we develop a new variant of DeepONet, called Fusion-DeepONet as a fast surrogate model for geometry-dependent hypersonic and supersonic flow fields. We evaluated Fusion-DeepONet in learning two external hypersonic flows and a supersonic shape-dependent internal flow problem. First, we compare the performance of Fusion-DeepONet with state-of-the-art neural operators to learn inviscid hypersonic flow around semi-elliptic blunt bodies for two grid types: uniform Cartesian and irregular grids. Fusion-DeepONet provides comparable accuracy to parameter-conditioned U-Net on uniform grids while outperforming MeshGraphNet and Vanilla-DeepONet on irregular grids. Fusion-DeepONet requires significantly fewer trainable parameters than U-Net, MeshGraphNet, and FNO. For the second hypersonic problem, we set up Fusion-DeepONet to map from geometry and free stream Mach number to the temperature field around a reentry capsule traveling at hypersonic speed. This fast surrogate is then improved to predict the spatial derivative of the temperature, resulting in an accurate prediction of heat flux at the surfaces of the capsule. To enhance the accuracy of spatial derivative prediction, we introduce a derivative-enhanced loss term with the least computation overhead. For the third problem, we show that Fusion-DeepONet outperforms MeshGraphNet in learning geometry-dependent supersonic flow in a converging-diverging nozzle configuration. For all the problems, we used high-fidelity simulations with a high-order entropy-stable DGSEM solver to generate training datasets with limited samples.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Tables to Time: How TabPFN-v2 Outperforms Specialized Time Series Forecasting Models</title>
<link>https://arxiv.org/abs/2501.02945</link>
<guid>https://arxiv.org/abs/2501.02945</guid>
<content:encoded><![CDATA[
arXiv:2501.02945v3 Announce Type: replace 
Abstract: Foundation models have become increasingly popular for forecasting due to their ability to provide predictions without requiring a lot of training data. In this work, we demonstrate how TabPFN-v2, a general tabular foundation model, can be effectively applied to time series forecasting. We introduce TabPFN-TS, a simple method that combines TabPFN-v2 with lightweight feature engineering to enable both point and probabilistic forecasting. Despite its simplicity and compact size (11M parameters), TabPFN-TS achieves top rank on the public GIFT-Eval leaderboard in both forecasting tasks. Through ablation studies, we investigate factors contributing to this surprising effectiveness, especially considering TabPFN-v2 was pretrained solely on synthetic tabular data with no exposure to time series. Our results highlights the potential of tabular foundation models like TabPFN-v2 as a valuable new approach for time series forecasting. Our implementation is available at https://github.com/PriorLabs/tabpfn-time-series.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation</title>
<link>https://arxiv.org/abs/2501.12432</link>
<guid>https://arxiv.org/abs/2501.12432</guid>
<content:encoded><![CDATA[
arXiv:2501.12432v2 Announce Type: replace 
Abstract: Although current Large Language Models (LLMs) exhibit impressive capabilities, performing complex real-world tasks still requires tool learning. Mainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to interact with external environments, but they are limited in perceptual scope and lack adequate task-planning capability. To address these limitations, other studies introduce the first Search-based Decision Tree (DFSDT), which still suffers from the high computational cost. In this paper, we introduce a novel parallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama). First, we transform traditional tree-based tool search paths into Directed Acyclic Graph (DAG) structure, generating a high-quality parallel tool invocation dataset. The DTA-Llama is then trained on the dataset to learn to iteratively divide the current task into several parallel tool invocation sub-tasks and aggregate the invocation results to decide the next actions. Furthermore, we introduce an efficient inference framework inspired by the Process/Threads mechanism when applying the DTA-Llama to practical tasks. Experimental results show that our approach substantially enhances task performance while reducing token consumption and inference time. Llama2-7B, using our method, is comparable to the official parallel function calling method of GPT-3.5. The relevant code, dataset, and model weights are available at https://corn0205.github.io/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Probabilistic Model for Non-Contrastive Learning</title>
<link>https://arxiv.org/abs/2501.13031</link>
<guid>https://arxiv.org/abs/2501.13031</guid>
<content:encoded><![CDATA[
arXiv:2501.13031v2 Announce Type: replace 
Abstract: Self-supervised learning (SSL) aims to find meaningful representations from unlabeled data by encoding semantic similarities through data augmentations. Despite its current popularity, theoretical insights about SSL are still scarce. For example, it is not yet known whether commonly used SSL loss functions can be related to a statistical model, much in the same as OLS, generalized linear models or PCA naturally emerge as maximum likelihood estimates of an underlying generative process. In this short paper, we consider a latent variable statistical model for SSL that exhibits an interesting property: Depending on the informativeness of the data augmentations, the MLE of the model either reduces to PCA, or approaches a simple non-contrastive loss. We analyze the model and also empirically illustrate our findings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Embedding Methods for Classification Tasks: A Review</title>
<link>https://arxiv.org/abs/2501.13392</link>
<guid>https://arxiv.org/abs/2501.13392</guid>
<content:encoded><![CDATA[
arXiv:2501.13392v2 Announce Type: replace 
Abstract: Time series analysis has become crucial in various fields, from engineering and finance to healthcare and social sciences. Due to their multidimensional nature, time series often need to be embedded into a fixed-dimensional feature space to enable processing with various machine learning algorithms. In this paper, we present a comprehensive review and quantitative evaluation of time series embedding methods for effective representations in machine learning and deep learning models. We introduce a taxonomy of embedding techniques, categorizing them based on their theoretical foundations and application contexts. Our work provides a quantitative evaluation of representative methods from each category by assessing their performance on downstream classification tasks across diverse real-world datasets. Our experimental results demonstrate that the performance of embedding methods varies significantly depending on the dataset and classification algorithm used, highlighting the importance of careful model selection and extensive experimentation for specific applications. To facilitate further research and practical applications, we provide an open-source code repository implementing these embedding methods. This study contributes to the field by offering a systematic comparison of time series embedding techniques, guiding practitioners in selecting appropriate methods for their specific applications, and providing a foundation for future advancements in time series analysis.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Domain Generalization with Data-free On-server Matching Gradient</title>
<link>https://arxiv.org/abs/2501.14653</link>
<guid>https://arxiv.org/abs/2501.14653</guid>
<content:encoded><![CDATA[
arXiv:2501.14653v2 Announce Type: replace 
Abstract: Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. One of the key approaches in DG is training an encoder which generates domain-invariant representations. However, this approach is not applicable in Federated Domain Generalization (FDG), where data from various domains are distributed across different clients. In this paper, we introduce a novel approach, dubbed Federated Learning via On-server Matching Gradient (FedOMG), which can \emph{efficiently leverage domain information from distributed domains}. Specifically, we utilize the local gradients as information about the distributed models to find an invariant gradient direction across all domains through gradient inner product maximization. The advantages are two-fold: 1) FedOMG can aggregate the characteristics of distributed models on the centralized server without incurring any additional communication cost, and 2) FedOMG is orthogonal to many existing FL/FDG methods, allowing for additional performance improvements by being seamlessly integrated with them. Extensive experimental evaluations on various settings to demonstrate the robustness of FedOMG compared to other FL/FDG baselines. Our method outperforms recent SOTA baselines on four FL benchmark datasets (MNIST, EMNIST, CIFAR-10, and CIFAR-100), and three FDG benchmark datasets (PACS, VLCS, and OfficeHome).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDDM: A Molecular Dynamics Diffusion Model to Predict Particle Self-Assembly</title>
<link>https://arxiv.org/abs/2501.17319</link>
<guid>https://arxiv.org/abs/2501.17319</guid>
<content:encoded><![CDATA[
arXiv:2501.17319v2 Announce Type: replace 
Abstract: The discovery and study of new material systems relies on molecular simulations that often come with significant computational expense. We propose MDDM, a Molecular Dynamics Diffusion Model, which is capable of predicting a valid output conformation for a given input pair potential function. After training MDDM on a large dataset of molecular dynamics self-assembly results, the proposed model can convert uniform noise into a meaningful output particle structure corresponding to an arbitrary input potential. The model's architecture has domain-specific properties built-in, such as satisfying periodic boundaries and being invariant to translation. The model significantly outperforms the baseline point-cloud diffusion model for both unconditional and conditional generation tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Message Passing: Neural Graph Pattern Machine</title>
<link>https://arxiv.org/abs/2501.18739</link>
<guid>https://arxiv.org/abs/2501.18739</guid>
<content:encoded><![CDATA[
arXiv:2501.18739v2 Announce Type: replace 
Abstract: Graph learning tasks often hinge on identifying key substructure patterns -- such as triadic closures in social networks or benzene rings in molecular graphs -- that underpin downstream performance. However, most existing graph neural networks (GNNs) rely on message passing, which aggregates local neighborhood information iteratively and struggles to explicitly capture such fundamental motifs, like triangles, k-cliques, and rings. This limitation hinders both expressiveness and long-range dependency modeling. In this paper, we introduce the Neural Graph Pattern Machine (GPM), a novel framework that bypasses message passing by learning directly from graph substructures. GPM efficiently extracts, encodes, and prioritizes task-relevant graph patterns, offering greater expressivity and improved ability to capture long-range dependencies. Empirical evaluations across four standard tasks -- node classification, link prediction, graph classification, and graph regression -- demonstrate that GPM outperforms state-of-the-art baselines. Further analysis reveals that GPM exhibits strong out-of-distribution generalization, desirable scalability, and enhanced interpretability. Code and datasets are available at: https://github.com/Zehong-Wang/GPM.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Generalization via Forced Rendering of Disentangled Latents</title>
<link>https://arxiv.org/abs/2501.18797</link>
<guid>https://arxiv.org/abs/2501.18797</guid>
<content:encoded><![CDATA[
arXiv:2501.18797v2 Announce Type: replace 
Abstract: Composition-the ability to generate myriad variations from finite means-is believed to underlie powerful generalization. However, compositional generalization remains a key challenge for deep learning. A widely held assumption is that learning disentangled (factorized) representations naturally supports this kind of extrapolation. Yet, empirical results are mixed, with many generative models failing to recognize and compose factors to generate out-of-distribution (OOD) samples. In this work, we investigate a controlled 2D Gaussian "bump" generation task with fully disentangled (x,y) inputs, demonstrating that standard generative architectures still fail in OOD regions when training with partial data, by re-entangling latent representations in subsequent layers. By examining the model's learned kernels and manifold geometry, we show that this failure reflects a "memorization" strategy for generation via data superposition rather than via composition of the true factorized features. We show that when models are forced-through architectural modifications with regularization or curated training data-to render the disentangled latents into the full-dimensional representational (pixel) space, they can be highly data-efficient and effective at composing in OOD regions. These findings underscore that disentangled latents in an abstract representation are insufficient and show that if models can represent disentangled factors directly in the output representational space, it can achieve robust compositional generalization.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Norm-Bounded Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2501.19050</link>
<guid>https://arxiv.org/abs/2501.19050</guid>
<content:encoded><![CDATA[
arXiv:2501.19050v2 Announce Type: replace 
Abstract: In this work, we propose norm-bounded low-rank adaptation (NB-LoRA) for parameter-efficient fine tuning. NB-LoRA is a novel parameterization of low-rank weight adaptations that admits explicit bounds on each singular value of the adaptation matrix, which can thereby satisfy any prescribed unitarily invariant norm bound, including the Schatten norms (e.g., nuclear, Frobenius, spectral norm). The proposed parameterization is unconstrained, smooth, and complete, i.e. it covers all matrices satisfying the prescribed rank and singular-value bounds. Comparative experiments on large language models show that NB-LoRA achieves superior adaptation performance and faster training over a range of models, tasks and ranks. Vision fine-tuning experiments show that NB-LoRA can achieve strong adaptation performance while avoiding model catastrophic forgetting, and compared to existing approaches it is substantially more robust to a hyper-parameters such as including adaptation rank, learning rate and number of training epochs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAL: A Practical Prior-Free Black-Box Framework for Non-Stationary Bandit Environments</title>
<link>https://arxiv.org/abs/2501.19401</link>
<guid>https://arxiv.org/abs/2501.19401</guid>
<content:encoded><![CDATA[
arXiv:2501.19401v2 Announce Type: replace 
Abstract: We introduce a practical, black-box framework termed Detection Augmenting Learning (DAL) for the problem of non-stationary bandits without prior knowledge of the underlying non-stationarity. DAL is modular, accepting any stationary bandit algorithm as input and augmenting it with a change detector. Our approach is applicable to all common parametric and non-parametric bandit variants. Extensive experimentation demonstrates that DAL consistently surpasses current state-of-the-art methods across diverse non-stationary scenarios, including synthetic benchmarks and real-world datasets, underscoring its versatility and scalability. We provide theoretical insights into DAL's strong empirical performance on piecewise stationary and drift settings, complemented by thorough experimental validation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sigmoid Self-Attention has Lower Sample Complexity than Softmax Self-Attention: A Mixture-of-Experts Perspective</title>
<link>https://arxiv.org/abs/2502.00281</link>
<guid>https://arxiv.org/abs/2502.00281</guid>
<content:encoded><![CDATA[
arXiv:2502.00281v2 Announce Type: replace 
Abstract: At the core of the popular Transformer architecture is the self-attention mechanism, which dynamically assigns softmax weights to each input token so that the model can focus on the most salient information. However, the softmax structure slows down the attention computation due to its row-wise nature, and it inherently introduces competition among tokens: as the weight assigned to one token increases, the weights of others decrease. This competitive dynamic may narrow the focus of self-attention to a limited set of features, potentially overlooking other informative characteristics. Recent experimental studies have shown that using the element-wise sigmoid function helps eliminate token competition and reduce the computational overhead. Despite these promising empirical results, a rigorous comparison between sigmoid and softmax self-attention mechanisms remains absent in the literature. This paper closes this gap by theoretically demonstrating that sigmoid self-attention is more sample-efficient than its softmax counterpart. Toward that goal, we represent the self-attention matrix as a mixture of experts and show that ``experts'' in sigmoid self-attention require significantly less data to achieve the same approximation error as those in softmax self-attention.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polynomial, trigonometric, and tropical activations</title>
<link>https://arxiv.org/abs/2502.01247</link>
<guid>https://arxiv.org/abs/2502.01247</guid>
<content:encoded><![CDATA[
arXiv:2502.01247v2 Announce Type: replace 
Abstract: Which functions can be used as activations in deep neural networks? This article explores families of functions based on orthonormal bases, including the Hermite polynomial basis and the Fourier trigonometric basis, as well as a basis resulting from the tropicalization of a polynomial basis. Our study shows that, through simple variance-preserving initialization and without additional clamping mechanisms, these activations can successfully be used to train deep models, such as GPT-2 for next-token prediction on OpenWebText and ConvNeXt for image classification on ImageNet. Our work addresses the issue of exploding and vanishing activations and gradients, particularly prevalent with polynomial activations, and opens the door for improving the efficiency of large-scale learning tasks. Furthermore, our approach provides insight into the structure of neural networks, revealing that networks with polynomial activations can be interpreted as multivariate polynomial mappings. Finally, using Hermite interpolation, we show that our activations can closely approximate classical ones in pre-trained models by matching both the function and its derivative, making them especially useful for fine-tuning tasks. These activations are available in the torchortho library, which can be accessed via: https://github.com/K-H-Ismail/torchortho.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfoBridge: Mutual Information estimation via Bridge Matching</title>
<link>https://arxiv.org/abs/2502.01383</link>
<guid>https://arxiv.org/abs/2502.01383</guid>
<content:encoded><![CDATA[
arXiv:2502.01383v2 Announce Type: replace 
Abstract: Diffusion bridge models have recently become a powerful tool in the field of generative modeling. In this work, we leverage their power to address another important problem in machine learning and information theory, the estimation of the mutual information (MI) between two random variables. We show that by using the theory of diffusion bridges, one can construct an unbiased estimator for data posing difficulties for conventional MI estimators. We showcase the performance of our estimator on two standard MI estimation benchmarks, i.e., low-dimensional and image-based, and on real-world data, i.e., protein language model embeddings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Leakage: A Contamination Problem in LLM-as-a-judge</title>
<link>https://arxiv.org/abs/2502.01534</link>
<guid>https://arxiv.org/abs/2502.01534</guid>
<content:encoded><![CDATA[
arXiv:2502.01534v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between the data generator LLM and the judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations</title>
<link>https://arxiv.org/abs/2502.01657</link>
<guid>https://arxiv.org/abs/2502.01657</guid>
<content:encoded><![CDATA[
arXiv:2502.01657v2 Announce Type: replace 
Abstract: Large language models (LLMs) continue to face challenges in reliably solving reasoning tasks, particularly those that require precise rule following, as often found in mathematical reasoning. This paper introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, enabling problem-solving within a neurosymbolic vector space. The results are decoded and merged with the original hidden state, significantly boosting the model's performance on numerical reasoning tasks. By offloading computation through neurosymbolic representations, this method enhances efficiency, reliability, and interpretability. Experimental results demonstrate an average of 88.6% lower cross-entropy loss and 15.4 times more problems correctly solved on a suite of mathematical reasoning tasks compared to chain-of-thought prompting and supervised fine-tuning (LoRA), without degrading performance on other tasks. We make our code available at: https://github.com/vdhanraj/Neurosymbolic-LLM.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Guidance of Flow Matching</title>
<link>https://arxiv.org/abs/2502.02150</link>
<guid>https://arxiv.org/abs/2502.02150</guid>
<content:encoded><![CDATA[
arXiv:2502.02150v3 Announce Type: replace 
Abstract: Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where generation under energy guidance (abbreviated as guidance in the following) is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/flow_guidance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mPOLICE: Provable Enforcement of Multi-Region Affine Constraints in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2502.02434</link>
<guid>https://arxiv.org/abs/2502.02434</guid>
<content:encoded><![CDATA[
arXiv:2502.02434v2 Announce Type: replace 
Abstract: Deep neural networks are increasingly used in safety-critical domains such as robotics and scientific modeling, where strict adherence to output constraints is essential. Methods like POLICE, which are tailored for single convex regions, face challenges when extended to multiple disjoint regions, often leading to constraint violations or unwanted affine behavior across regions. This paper proposes mPOLICE, a new approach that generalizes POLICE to provably enforce affine constraints over multiple disjoint convex regions. At its core, mPOLICE assigns distinct neuron activation patterns to each constrained region, enabling localized affine behavior and avoiding unintended generalization. This is implemented through a layer-wise optimization of the network parameters. Additionally, we introduce a training algorithm that incorporates mPOLICE into conventional deep learning pipelines, balancing task-specific performance with constraint enforcement using periodic sign pattern enforcement. We validate the flexibility and effectiveness of mPOLICE through experiments across various applications, including safety-critical reinforcement learning, implicit 3D shape representation with geometric constraints, and fluid dynamics simulations with boundary condition enforcement. Importantly, mPOLICE incurs no runtime overhead during inference, making it a practical and reliable solution for constraint handling in deep neural networks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Q-Learning</title>
<link>https://arxiv.org/abs/2502.02538</link>
<guid>https://arxiv.org/abs/2502.02538</guid>
<content:encoded><![CDATA[
arXiv:2502.02538v2 Announce Type: replace 
Abstract: We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data. Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process. We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values. This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity. We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL. Project page: https://seohong.me/projects/fql/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization</title>
<link>https://arxiv.org/abs/2502.02810</link>
<guid>https://arxiv.org/abs/2502.02810</guid>
<content:encoded><![CDATA[
arXiv:2502.02810v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have led to models that tackle diverse molecular tasks, such as chemical reaction prediction and molecular property prediction. Large-scale molecular instruction-tuning datasets have enabled sequence-only (e.g., SMILES or SELFIES) generalist molecular LLMs, and researchers are now exploring multimodal approaches that incorporate molecular structural information for further gains. However, a genuinely multimodal, generalist LLM that covers a broad spectrum of molecular tasks has yet to be fully investigated. We observe that naive next token prediction training ignores graph-structural information, limiting an LLM's ability to exploit molecular graphs. To address this, we propose (i) Molecular structure Preference Optimization (MolPO), which facilitates graph usage by optimizing preferences between pairs of correct and perturbed molecular structures, and (ii) an advanced graph encoder with a tailored pre-training strategy to improve the effect of graph utilization by MolPO. Building on these contributions, we introduce Mol-LLM, the first multimodal generalist model that (a) handles a broad spectrum of molecular tasks among molecular LLMs, (b) explicitly leverages molecular-structure information, and (c) takes advantage of extensive instruction tuning. Mol-LLM attains state-of-the-art or comparable results across the most comprehensive molecular-LLM benchmark-even on out-of-distribution datasets for reaction and property prediction, where it surpasses prior generalist molecular LLMs by a large margin.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPE: Covariate-Adjusted Pre-Training for Generalized Epidemic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.03393</link>
<guid>https://arxiv.org/abs/2502.03393</guid>
<content:encoded><![CDATA[
arXiv:2502.03393v3 Announce Type: replace 
Abstract: Accurate forecasting of epidemic infection trajectories is crucial for safeguarding public health. However, limited data availability during emerging outbreaks and the complex interaction between environmental factors and disease dynamics present significant challenges for effective forecasting. In response, we introduce CAPE, a novel epidemic pre-training framework designed to harness extensive disease datasets from diverse regions and integrate environmental factors directly into the modeling process for more informed decision-making on downstream diseases. Based on a covariate adjustment framework, CAPE utilizes pre-training combined with hierarchical environment contrasting to identify universal patterns across diseases while estimating latent environmental influences. We have compiled a diverse collection of epidemic time series datasets and validated the effectiveness of CAPE under various evaluation scenarios, including full-shot, few-shot, zero-shot, cross-location, and cross-disease settings, where it outperforms the leading baseline by an average of 9.9% in full-shot and 14.3% in zero-shot settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators</title>
<link>https://arxiv.org/abs/2502.03424</link>
<guid>https://arxiv.org/abs/2502.03424</guid>
<content:encoded><![CDATA[
arXiv:2502.03424v3 Announce Type: replace 
Abstract: Fire safety is crucial for ensuring the stability of building structures, yet evaluating whether a structure meets fire safety requirement is challenging. Fires can originate at any point within a structure, and simulating every potential fire scenario is both expensive and time-consuming. To address this challenge, we propose the concept of the Most Fire-Sensitive Point (MFSP) and an efficient machine learning framework for its identification. The MFSP is defined as the location at which a fire, if initiated, would cause the most severe detrimental impact on the building's stability, effectively representing the worst-case fire scenario. In our framework, a Graph Neural Network (GNN) serves as an efficient and differentiable agent for conventional Finite Element Analysis (FEA) simulators by predicting the Maximum Interstory Drift Ratio (MIDR) under fire, which then guides the training and evaluation of the MFSP predictor. Additionally, we enhance our framework with a novel edge update mechanism and a transfer learning-based training scheme. Evaluations on a large-scale simulation dataset demonstrate the good performance of the proposed framework in identifying the MFSP, offering a transformative tool for optimizing fire safety assessments in structural design. All developed datasets and codes are open-sourced online.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Control for Guidance in Diffusion Models</title>
<link>https://arxiv.org/abs/2502.03686</link>
<guid>https://arxiv.org/abs/2502.03686</guid>
<content:encoded><![CDATA[
arXiv:2502.03686v2 Announce Type: replace 
Abstract: Diffusion models exhibit excellent sample quality, but existing guidance methods often require additional model training or are limited to specific tasks. We revisit guidance in diffusion models from the perspective of variational inference and control, introducing Diffusion Trajectory Matching (DTM) that enables guiding pretrained diffusion trajectories to satisfy a terminal cost. DTM unifies a broad class of guidance methods and enables novel instantiations. We introduce a new method within this framework that achieves state-of-the-art results on several linear, non-linear, and blind inverse problems without requiring additional model training or specificity to pixel or latent space diffusion models. Our code will be available at https://github.com/czi-ai/oc-guidance
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMoE: Converting Mixture-of-Experts from Dense to Accelerate LLM Inference</title>
<link>https://arxiv.org/abs/2502.04416</link>
<guid>https://arxiv.org/abs/2502.04416</guid>
<content:encoded><![CDATA[
arXiv:2502.04416v2 Announce Type: replace 
Abstract: Scaling large language models (LLMs) improves performance but dramatically increases inference costs. The feed-forward network (FFN), consuming approximately 70\% of inference compute, represents a critical bottleneck, particularly in large batch size scenarios. While mixture-of-experts (MoE) architectures leverage activation sparsity for efficiency, converting existing dense models to MoEs traditionally requires resource-intensive continual pre-training. We present CMoE, a framework that rapidly transforms dense LLMs into MoEs without training. The key innovation lies in analyzing FFN neuron activations to partition them into shared (always active) and routed experts. Routed neurons are clustered using a balanced assignment algorithm, and a differentiable router is constructed analytically from activation statistics, enabling immediate deployment or optional lightweight fine-tuning. Experiments demonstrate that, with activation ratio of 75\%, it achieves remarkable results, delivering lossless precision in terms of perplexity while still maintaining a 5\% acceleration. Further experiments reveal that a CMoE configuration activating just 25\% of parameters reduces end-to-end latency by 1.5x while preserving usable perplexity without additional training. Moreover, a brief LoRA fine-tuning process (requiring only 1 hour and 2,000 samples) successfully recovers over 76\% of the dense model's downstream accuracy. By effectively balancing performance and efficiency, CMoE offers a viable path forward for deploying LLMs in real-world scenarios where computational resources are limited. We make our code publicly available at https://github.com/JarvisPei/CMoE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabICL: A Tabular Foundation Model for In-Context Learning on Large Data</title>
<link>https://arxiv.org/abs/2502.05564</link>
<guid>https://arxiv.org/abs/2502.05564</guid>
<content:encoded><![CDATA[
arXiv:2502.05564v2 Announce Type: replace 
Abstract: The long-standing dominance of gradient-boosted decision trees on tabular data is currently challenged by tabular foundation models using In-Context Learning (ICL): setting the training data as context for the test data and predicting in a single forward pass without parameter updates. While TabPFNv2 foundation model excels on tables with up to 10K samples, its alternating column- and row-wise attentions make handling large training sets computationally prohibitive. So, can ICL be effectively scaled and deliver a benefit for larger tables? We introduce TabICL, a tabular foundation model for classification, pretrained on synthetic datasets with up to 60K samples and capable of handling 500K samples on affordable resources. This is enabled by a novel two-stage architecture: a column-then-row attention mechanism to build fixed-dimensional embeddings of rows, followed by a transformer for efficient ICL. Across 200 classification datasets from the TALENT benchmark, TabICL is on par with TabPFNv2 while being systematically faster (up to 10 times), and significantly outperforms all other approaches. On 53 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost, demonstrating the potential of ICL for large data. Pretraining code, inference code, and pre-trained models are available at https://github.com/soda-inria/tabicl.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Overwhelming Transformer Models with Designed Inputs</title>
<link>https://arxiv.org/abs/2502.06038</link>
<guid>https://arxiv.org/abs/2502.06038</guid>
<content:encoded><![CDATA[
arXiv:2502.06038v2 Announce Type: replace 
Abstract: We develop an algorithm which, given a trained transformer model $\mathcal{M}$ as input, as well as a string of tokens $s$ of length $n_{fix}$ and an integer $n_{free}$, can generate a mathematical proof that $\mathcal{M}$ is ``overwhelmed'' by $s$, in time and space $\widetilde{O}(n_{fix}^2 + n_{free}^3)$. We say that $\mathcal{M}$ is ``overwhelmed'' by $s$ when the output of the model evaluated on this string plus any additional string $t$, $\mathcal{M}(s + t)$, is completely insensitive to the value of the string $t$ whenever length($t$) $\leq n_{free}$. Along the way, we prove a particularly strong worst-case form of ``over-squashing'', which we use to bound the model's behavior. Our technique uses computer-aided proofs to establish this type of operationally relevant guarantee about transformer models. We empirically test our algorithm on a single layer transformer complete with an attention head, layer-norm, MLP/ReLU layers, and RoPE positional encoding. We believe that this work is a stepping stone towards the difficult task of obtaining useful guarantees for trained transformer models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Instruction Tuning</title>
<link>https://arxiv.org/abs/2502.06814</link>
<guid>https://arxiv.org/abs/2502.06814</guid>
<content:encoded><![CDATA[
arXiv:2502.06814v2 Announce Type: replace 
Abstract: We introduce Lavender, a simple supervised fine-tuning (SFT) method that boosts the performance of advanced vision-language models (VLMs) by leveraging state-of-the-art image generation models such as Stable Diffusion. Specifically, Lavender aligns the text-vision attention in the VLM transformer with the equivalent used by Stable Diffusion during SFT, instead of adapting separate encoders. This alignment enriches the model's visual understanding and significantly boosts performance across in- and out-of-distribution tasks. Lavender requires just 0.13 million training examples, 2.5% of typical large-scale SFT datasets, and fine-tunes on standard hardware (8 GPUs) in a single day. It consistently improves state-of-the-art open-source multimodal LLMs (e.g., Llama-3.2-11B, MiniCPM-Llama3-v2.5), achieving up to 30% gains and a 68% boost on challenging out-of-distribution medical QA tasks. By efficiently transferring the visual expertise of image generators with minimal supervision, Lavender offers a scalable solution for more accurate vision-language systems. All code, training data, and models will be shared at https://astrazeneca.github.io/vlm/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRP: High-Rank Preheating for Superior LoRA Initialization</title>
<link>https://arxiv.org/abs/2502.07739</link>
<guid>https://arxiv.org/abs/2502.07739</guid>
<content:encoded><![CDATA[
arXiv:2502.07739v3 Announce Type: replace 
Abstract: This paper studies the crucial impact of initialization in Low-Rank Adaptation (LoRA). Through theoretical analysis, we demonstrate that the fine-tuned result of LoRA is highly sensitive to initialization, which is likely to lead suboptimal low-rank results. While this issue can be mitigated by adjusting the initial direction towards the main singular vectors of the target $\Delta W$, which is, however, typically unknown in real-world scenarios. To approximate this initial direction, we propose High-Rank Preheating (HRP), which first trains LoRA with a higher preheating rank for a few steps, then uses the main singular vectors of the derived $BA^\top$ as initialization for the main fine-tuning process. With only a modification in the initial direction, we prove that HRP makes LoRA achieve better fine-tuned results than random initialization in expectation, and the enhancement grows with the preheating rank. We validate our theoretical findings through extensive experiments in various models and tasks, where HRP significantly enhances LoRA's effectiveness and outperforms other initialization strategies and other LoRA variants.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Boundary of Diffusion-based Methods for Solving Constrained Optimization</title>
<link>https://arxiv.org/abs/2502.10330</link>
<guid>https://arxiv.org/abs/2502.10330</guid>
<content:encoded><![CDATA[
arXiv:2502.10330v2 Announce Type: replace 
Abstract: Diffusion models have achieved remarkable success in generative tasks such as image and video synthesis, and in control domains like robotics, owing to their strong generalization capabilities and proficiency in fitting complex multimodal distributions. However, their full potential in solving Continuous Constrained Optimization problems remains largely underexplored. Our work commences by investigating a two-dimensional constrained quadratic optimization problem as an illustrative example to explore the inherent challenges and issues when applying diffusion models to such optimization tasks and providing theoretical analyses for these observations. To address the identified gaps and harness diffusion models for Continuous Constrained Optimization, we build upon this analysis to propose a novel diffusion-based framework for optimization problems called DiOpt. This framework operates in two distinct phases: an initial warm-start phase, implemented via supervised learning, followed by a bootstrapping phase. This dual-phase architecture is designed to iteratively refine solutions, thereby improving the objective function while rigorously satisfying problem constraints. Finally, multiple candidate solutions are sampled, and the optimal one is selected through a screening process. We present extensive experiments detailing the training dynamics of DiOpt, its performance across a diverse set of Continuous Constrained Optimization problems, and an analysis of the impact of DiOpt's various hyperparameters.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expert-Agnostic Learning to Defer</title>
<link>https://arxiv.org/abs/2502.10533</link>
<guid>https://arxiv.org/abs/2502.10533</guid>
<content:encoded><![CDATA[
arXiv:2502.10533v2 Announce Type: replace 
Abstract: Learning to Defer (L2D) trains autonomous systems to handle straightforward cases while deferring uncertain ones to human experts. Recent advancements in this field have introduced methods that offer flexibility to unseen experts at test time. However, we find these approaches struggle to generalise to experts with behaviours not seen during training, require extensive human annotation, and lack mechanisms for incorporating prior knowledge of expert capabilities. To address these challenges, we introduce Expert-Agnostic Learning to Defer (EA-L2D), a novel L2D framework that employs a Bayesian approach to model expert behaviour in an \textit{expert-agnostic} fashion. Across benchmark medical imaging datasets (HAM10000, Blood Cells, Retinal OCT, and Liver Tumours), EA-L2D significantly outperforms prior methods on unseen experts, achieving up to a 28\% relative improvement, while also matching or exceeding state-of-the-art performance on seen experts.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Neural Collapse Enhances Out-of-Distribution Detection and Transfer Learning</title>
<link>https://arxiv.org/abs/2502.10691</link>
<guid>https://arxiv.org/abs/2502.10691</guid>
<content:encoded><![CDATA[
arXiv:2502.10691v2 Announce Type: replace 
Abstract: Out-of-distribution (OOD) detection and OOD generalization are widely studied in Deep Neural Networks (DNNs), yet their relationship remains poorly understood. We empirically show that the degree of Neural Collapse (NC) in a network layer is inversely related with these objectives: stronger NC improves OOD detection but degrades generalization, while weaker NC enhances generalization at the cost of detection. This trade-off suggests that a single feature space cannot simultaneously achieve both tasks. To address this, we develop a theoretical framework linking NC to OOD detection and generalization. We show that entropy regularization mitigates NC to improve generalization, while a fixed Simplex Equiangular Tight Frame (ETF) projector enforces NC for better detection. Based on these insights, we propose a method to control NC at different DNN layers. In experiments, our method excels at both tasks across OOD datasets and DNN architectures.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiSCo: Device-Server Collaborative LLM-Based Text Streaming Services</title>
<link>https://arxiv.org/abs/2502.11417</link>
<guid>https://arxiv.org/abs/2502.11417</guid>
<content:encoded><![CDATA[
arXiv:2502.11417v2 Announce Type: replace 
Abstract: The rapid rise of large language models (LLMs) in text streaming services has introduced significant cost and Quality of Experience (QoE) challenges in serving millions of daily requests, especially in meeting Time-To-First-Token (TTFT) and Time-Between-Token (TBT) requirements for real-time interactions. Our real-world measurements show that both server-based and on-device deployments struggle to meet diverse QoE demands: server deployments face high costs and last-hop issues (e.g., Internet latency and dynamics), while on-device LLM inference is constrained by resources.
  We introduce DiSCo, a device-server cooperative scheduler designed to optimize users' QoE by adaptively routing requests and migrating response generation between endpoints while maintaining cost constraints. DiSCo employs cost-aware scheduling, leveraging the predictable speed of on-device LLM inference with the flexible capacity of server-based inference to dispatch requests on the fly, while introducing a token-level migration mechanism to ensure consistent token delivery during migration. Evaluations on real-world workloads -- including commercial services like OpenAI GPT and DeepSeek, and open-source deployments such as LLaMA3 -- show that DiSCo can improve users' QoE by reducing tail TTFT (11-52\%) and mean TTFT (6-78\%) across different model-device configurations, while dramatically reducing serving costs by up to 84\% through its migration mechanism while maintaining comparable QoE levels.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs</title>
<link>https://arxiv.org/abs/2502.12085</link>
<guid>https://arxiv.org/abs/2502.12085</guid>
<content:encoded><![CDATA[
arXiv:2502.12085v2 Announce Type: replace 
Abstract: While long-context inference is crucial for advancing large language model (LLM) applications, its prefill speed remains a significant bottleneck. Current approaches, including sequence parallelism strategies and compute reduction through approximate attention mechanisms, still fall short of delivering optimal inference efficiency. This hinders scaling the inputs to longer sequences and processing long-context queries in a timely manner. To address this, we introduce APB, an efficient long-context inference framework that leverages multi-host approximate attention to enhance prefill speed by reducing compute and enhancing parallelism simultaneously. APB introduces a communication mechanism for essential key-value pairs within a sequence parallelism framework, enabling a faster inference speed while maintaining task performance. We implement APB by incorporating a tailored FlashAttn kernel alongside optimized distribution strategies, supporting diverse models and parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x compared with FlashAttn, RingAttn, and StarAttn, respectively, without any observable task performance degradation. We provide the implementation and experiment code of APB in https://github.com/thunlp/APB.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees</title>
<link>https://arxiv.org/abs/2502.12678</link>
<guid>https://arxiv.org/abs/2502.12678</guid>
<content:encoded><![CDATA[
arXiv:2502.12678v2 Announce Type: replace 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has been highly successful in aligning large language models with human preferences. While prevalent methods like DPO have demonstrated strong performance, they frame interactions with the language model as a bandit problem, which limits their applicability in real-world scenarios where multi-turn conversations are common. Additionally, DPO relies on the Bradley-Terry model assumption, which does not adequately capture the non-transitive nature of human preferences. In this paper, we address these challenges by modeling the alignment problem as a two-player constant-sum Markov game, where each player seeks to maximize their winning rate against the other across all steps of the conversation. Our approach Optimistic Multi-step Preference Optimization (OMPO) is built upon the optimistic online mirror descent algorithm~\citep{rakhlin2013online,joulani17a}. Theoretically, we provide a rigorous analysis for the convergence of OMPO and show that OMPO requires $\mathcal{O}(\epsilon^{-1})$ policy updates to converge to an $\epsilon$-approximate Nash equilibrium. We also validate the effectiveness of our method on multi-turn conversations dataset and math reasoning dataset.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction</title>
<link>https://arxiv.org/abs/2502.13344</link>
<guid>https://arxiv.org/abs/2502.13344</guid>
<content:encoded><![CDATA[
arXiv:2502.13344v2 Announce Type: replace 
Abstract: Biomedical knowledge graphs (KGs) encode rich, structured information critical for drug discovery tasks, but extracting meaningful insights from large-scale KGs remains challenging due to their complex structure. Existing biomedical subgraph retrieval methods are tailored for graph neural networks (GNNs), limiting compatibility with other paradigms, including large language models (LLMs). We introduce K-Paths, a model-agnostic retrieval framework that extracts structured, diverse, and biologically meaningful multi-hop paths from dense biomedical KGs. These paths enable the prediction of unobserved drug-drug and drug-disease interactions, including those involving entities not seen during training, thus supporting inductive reasoning. K-Paths is training-free and employs a diversity-aware adaptation of Yen's algorithm to extract the K shortest loopless paths between entities in a query, prioritizing biologically relevant and relationally diverse connections. These paths serve as concise, interpretable reasoning chains that can be directly integrated with LLMs or GNNs to improve generalization, accuracy, and enable explainable inference. Experiments on benchmark datasets show that K-Paths improves zero-shot reasoning across state-of-the-art LLMs. For instance, Tx-Gemma 27B improves by 19.8 and 4.0 F1 points on interaction severity prediction and drug repurposing tasks, respectively. Llama 70B achieves gains of 8.5 and 6.2 points on the same tasks. K-Paths also boosts the training efficiency of EmerGNN, a state-of-the-art GNN, by reducing the KG size by 90% while maintaining predictive performance. Beyond efficiency, K-Paths bridges the gap between KGs and LLMs, enabling scalable and explainable LLM-augmented scientific discovery. We release our code and the retrieved paths as a benchmark for inductive reasoning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CER: Confidence Enhanced Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2502.14634</link>
<guid>https://arxiv.org/abs/2502.14634</guid>
<content:encoded><![CDATA[
arXiv:2502.14634v2 Announce Type: replace 
Abstract: Ensuring the reliability of Large Language Models (LLMs) in complex reasoning tasks remains a formidable challenge, particularly in scenarios that demand precise mathematical calculations and knowledge-intensive open-domain generation. In this work, we introduce an uncertainty-aware framework designed to enhance the accuracy of LLM responses by systematically incorporating model confidence at critical decision points. We propose an approach that encourages multi-step reasoning in LLMs and quantify the confidence of intermediate answers such as numerical results in mathematical reasoning and proper nouns in open-domain generation. Then, the overall confidence of each reasoning chain is evaluated based on confidence of these critical intermediate steps. Finally, we aggregate the answer of generated response paths in a way that reflects the reliability of each generated content (as opposed to self-consistency in which each generated chain contributes equally to majority voting). We conducted extensive experiments in five datasets, three mathematical datasets and two open-domain datasets, using four LLMs. The results consistently validate the effectiveness of our novel confidence aggregation method, leading to an accuracy improvement of up to 7.4% and 5.8% over baseline approaches in math and open-domain generation tasks, respectively. Code is publicly available at https://github.com/ Aquasar11/CER.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces</title>
<link>https://arxiv.org/abs/2502.14790</link>
<guid>https://arxiv.org/abs/2502.14790</guid>
<content:encoded><![CDATA[
arXiv:2502.14790v4 Announce Type: replace 
Abstract: We develop a form Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling over the $d$-dimensional unit cube, using a certain Gaussian process prior widely-used in the Bayesian optimization literature, has a $\mathcal{O}\Big(\beta\sqrt{Td\log(1+\sqrt{d}\frac{\lambda}{\beta})}\Big)$ rate against a $\beta$-bounded $\lambda$-Lipschitz adversary.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GiGL: Large-Scale Graph Neural Networks at Snapchat</title>
<link>https://arxiv.org/abs/2502.15054</link>
<guid>https://arxiv.org/abs/2502.15054</guid>
<content:encoded><![CDATA[
arXiv:2502.15054v2 Announce Type: replace 
Abstract: Recent advances in graph machine learning (ML) with the introduction of Graph Neural Networks (GNNs) have led to a widespread interest in applying these approaches to business applications at scale. GNNs enable differentiable end-to-end (E2E) learning of model parameters given graph structure which enables optimization towards popular node, edge (link) and graph-level tasks. While the research innovation in new GNN layers and training strategies has been rapid, industrial adoption and utility of GNNs has lagged considerably due to the unique scale challenges that large-scale graph ML problems create. In this work, we share our approach to training, inference, and utilization of GNNs at Snapchat. To this end, we present GiGL (Gigantic Graph Learning), an open-source library to enable large-scale distributed graph ML to the benefit of researchers, ML engineers, and practitioners. We use GiGL internally at Snapchat to manage the heavy lifting of GNN workflows, including graph data preprocessing from relational DBs, subgraph sampling, distributed training, inference, and orchestration. GiGL is designed to interface cleanly with open-source GNN modeling libraries prominent in academia like PyTorch Geometric (PyG), while handling scaling and productionization challenges that make it easier for internal practitioners to focus on modeling. GiGL is used in multiple production settings, and has powered over 35 launches across multiple business domains in the last 2 years in the contexts of friend recommendation, content recommendation and advertising. This work details high-level design and tools the library provides, scaling properties, case studies in diverse business settings with industry-scale graphs, and several key lessons learned in employing graph ML at scale on large social data. GiGL is open-sourced at https://github.com/Snapchat/GiGL.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALSA-RL: Stability Analysis in the Latent Space of Actions for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.15512</link>
<guid>https://arxiv.org/abs/2502.15512</guid>
<content:encoded><![CDATA[
arXiv:2502.15512v2 Announce Type: replace 
Abstract: Modern deep reinforcement learning (DRL) methods have made significant advances in handling continuous action spaces. However, real-world control systems--especially those requiring precise and reliable performance--often demand interpretability in the sense of a-priori assessments of agent behavior to identify safe or failure-prone interactions with environments. To address this limitation, we propose SALSA-RL (Stability Analysis in the Latent Space of Actions), a novel RL framework that models control actions as dynamic, time-dependent variables evolving within a latent space. By employing a pre-trained encoder-decoder and a state-dependent linear system, our approach enables interpretability through local stability analysis, where instantaneous growth in action-norms can be predicted before their execution. We demonstrate that SALSA-RL can be deployed in a non-invasive manner for assessing the local stability of actions from pretrained RL agents without compromising on performance across diverse benchmark environments. By enabling a more interpretable analysis of action generation, SALSA-RL provides a powerful tool for advancing the design, analysis, and theoretical understanding of RL systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Shift Distillation for Diffusion Mode Seeking</title>
<link>https://arxiv.org/abs/2502.15989</link>
<guid>https://arxiv.org/abs/2502.15989</guid>
<content:encoded><![CDATA[
arXiv:2502.15989v2 Announce Type: replace 
Abstract: We present mean-shift distillation, a novel diffusion distillation technique that provides a provably good proxy for the gradient of the diffusion output distribution. This is derived directly from mean-shift mode seeking on the distribution, and we show that its extrema are aligned with the modes. We further derive an efficient product distribution sampling procedure to evaluate the gradient. Our method is formulated as a drop-in replacement for score distillation sampling (SDS), requiring neither model retraining nor extensive modification of the sampling procedure. We show that it exhibits superior mode alignment as well as improved convergence in both synthetic and practical setups, yielding higher-fidelity results when applied to both text-to-image and text-to-3D applications with Stable Diffusion.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quasi Zigzag Persistence: A Topological Framework for Analyzing Time-Varying Data</title>
<link>https://arxiv.org/abs/2502.16049</link>
<guid>https://arxiv.org/abs/2502.16049</guid>
<content:encoded><![CDATA[
arXiv:2502.16049v2 Announce Type: replace 
Abstract: In this paper, we propose Quasi Zigzag Persistent Homology (QZPH) as a framework for analyzing time-varying data by integrating multiparameter persistence and zigzag persistence. To this end, we introduce a stable topological invariant that captures both static and dynamic features at different scales. We present an algorithm to compute this invariant efficiently. We show that it enhances the machine learning models when applied to tasks such as sleep-stage detection, demonstrating its effectiveness in capturing the evolving patterns in time-varying datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking like a CHEMIST: Combined Heterogeneous Embedding Model Integrating Structure and Tokens</title>
<link>https://arxiv.org/abs/2502.17986</link>
<guid>https://arxiv.org/abs/2502.17986</guid>
<content:encoded><![CDATA[
arXiv:2502.17986v2 Announce Type: replace 
Abstract: Representing molecular structures effectively in chemistry remains a challenging task. Language models and graph-based models are extensively utilized within this domain, consistently achieving state-of-the-art results across an array of tasks. However, the prevailing practice of representing chemical compounds in the SMILES format - used by most data sets and many language models - presents notable limitations as a training data format. In this study, we present a novel approach that decomposes molecules into substructures and computes descriptor-based representations for these fragments, providing more detailed and chemically relevant input for model training. We use this substructure and descriptor data as input for language model and also propose a bimodal architecture that integrates this language model with graph-based models. As LM we use RoBERTa, Graph Isomorphism Networks (GIN), Graph Convolutional Networks (GCN) and Graphormer as graph ones. Our framework shows notable improvements over traditional methods in various tasks such as Quantitative Structure-Activity Relationship (QSAR) prediction.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Alignment Improves Graph Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2502.18334</link>
<guid>https://arxiv.org/abs/2502.18334</guid>
<content:encoded><![CDATA[
arXiv:2502.18334v2 Announce Type: replace 
Abstract: Graph-based learning excels at capturing interaction patterns in diverse domains like recommendation, fraud detection, and particle physics. However, its performance often degrades under distribution shifts, especially those altering network connectivity. Current methods to address these shifts typically require retraining with the source dataset, which is often infeasible due to computational or privacy limitations. We introduce Test-Time Structural Alignment (TSA), a novel algorithm for Graph Test-Time Adaptation (GTTA) that aligns graph structures during inference without accessing the source data. Grounded in a theoretical understanding of graph data distribution shifts, TSA employs three synergistic strategies: uncertainty-aware neighborhood weighting to accommodate neighbor label distribution shifts, adaptive balancing of self-node and aggregated neighborhood representations based on their signal-to-noise ratio, and decision boundary refinement to correct residual label and feature shifts. Extensive experiments on synthetic and real-world datasets demonstrate TSA's consistent outperformance of both non-graph TTA methods and state-of-the-art GTTA baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference-Based Gradient Estimation for ML-Guided Approximate Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2502.19377</link>
<guid>https://arxiv.org/abs/2502.19377</guid>
<content:encoded><![CDATA[
arXiv:2502.19377v2 Announce Type: replace 
Abstract: Combinatorial optimization (CO) problems arise across a broad spectrum of domains, including medicine, logistics, and manufacturing. While exact solutions are often computationally infeasible, many practical applications require high-quality solutions within a given time budget. To address this, we propose a learning-based approach that enhances existing non-learned approximation algorithms for CO. Specifically, we parameterize these approximation algorithms and train graph neural networks (GNNs) to predict parameter values that yield near-optimal solutions. Our method is trained end-to-end in a self-supervised fashion, using a novel gradient estimation scheme that treats the approximation algorithm as a black box. This approach combines the strengths of learning and traditional algorithms: the GNN learns from data to guide the algorithm toward better solutions, while the approximation algorithm ensures feasibility. We validate our method on two well-known combinatorial optimization problems: the travelling salesman problem (TSP) and the minimum k-cut problem. Our results demonstrate that the proposed approach is competitive with state-of-the-art learned CO solvers.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimax Optimal Reinforcement Learning with Quasi-Optimism</title>
<link>https://arxiv.org/abs/2503.00810</link>
<guid>https://arxiv.org/abs/2503.00810</guid>
<content:encoded><![CDATA[
arXiv:2503.00810v2 Announce Type: replace 
Abstract: In our quest for a reinforcement learning (RL) algorithm that is both practical and provably optimal, we introduce EQO (Exploration via Quasi-Optimism). Unlike existing minimax optimal approaches, EQO avoids reliance on empirical variances and employs a simple bonus term proportional to the inverse of the state-action visit count. Central to EQO is the concept of quasi-optimism, where estimated values need not be fully optimistic, allowing for a simpler yet effective exploration strategy. The algorithm achieves the sharpest known regret bound for tabular RL under the mildest assumptions, proving that fast convergence can be attained with a practical and computationally efficient approach. Empirical evaluations demonstrate that EQO consistently outperforms existing algorithms in both regret performance and computational efficiency, providing the best of both theoretical soundness and practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Transformer-based World Models with Contrastive Predictive Coding</title>
<link>https://arxiv.org/abs/2503.04416</link>
<guid>https://arxiv.org/abs/2503.04416</guid>
<content:encoded><![CDATA[
arXiv:2503.04416v2 Announce Type: replace 
Abstract: The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer architecture for its superior training efficiency and favorable scaling properties, recent works such as STORM have proposed replacing RNN-based world models with Transformer-based world models using masked self-attention. However, despite the improved training efficiency of these methods, their impact on performance remains limited compared to the Dreamer algorithm, struggling to learn competitive Transformer-based world models. In this work, we show that the next state prediction objective adopted in previous approaches is insufficient to fully exploit the representation capabilities of Transformers. We propose to extend world model predictions to longer time horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE Representations), a world model using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations and improve the agent performance. TWISTER achieves a human-normalized mean score of 162% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ look-ahead search.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To See a World in a Spark of Neuron: Disentangling Multi-task Interference for Training-free Model Merging</title>
<link>https://arxiv.org/abs/2503.05320</link>
<guid>https://arxiv.org/abs/2503.05320</guid>
<content:encoded><![CDATA[
arXiv:2503.05320v2 Announce Type: replace 
Abstract: Fine-tuning pre-trained models on targeted datasets enhances task-specific performance but often comes at the expense of generalization. Model merging techniques, which integrate multiple fine-tuned models into a single multi-task model through task arithmetic, offer a promising solution. However, task interference remains a fundamental challenge, leading to performance degradation and suboptimal merged models. Existing approaches largely overlook the fundamental roles of neurons, their connectivity, and activation, resulting in a merging process and a merged model that does not consider how neurons relay and process information. In this work, we present the first study that relies on neuronal mechanisms for model merging. We decompose task-specific representations into two complementary neuronal subspaces that regulate neuron sensitivity and input adaptability. Leveraging this decomposition, we introduce NeuroMerging, a novel merging framework developed to mitigate task interference within neuronal subspaces, enabling training-free model fusion across diverse tasks. Through extensive experiments, we demonstrate that NeuroMerging achieves superior performance compared to existing methods on multi-task benchmarks across both natural language and vision domains. Our findings highlight the importance of aligning neuronal mechanisms in model merging, offering new insights into mitigating task interference and improving knowledge fusion. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Can Differential Privacy Be Audited in One Run?</title>
<link>https://arxiv.org/abs/2503.07199</link>
<guid>https://arxiv.org/abs/2503.07199</guid>
<content:encoded><![CDATA[
arXiv:2503.07199v2 Announce Type: replace 
Abstract: Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. (2024) prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering by Nonparametric Smoothing</title>
<link>https://arxiv.org/abs/2503.09134</link>
<guid>https://arxiv.org/abs/2503.09134</guid>
<content:encoded><![CDATA[
arXiv:2503.09134v2 Announce Type: replace 
Abstract: A novel formulation of the clustering problem is introduced in which the task is expressed as an estimation problem, where the object to be estimated is a function which maps a point to its distribution of cluster membership. Unlike existing approaches which implicitly estimate such a function, like Gaussian Mixture Models (GMMs), the proposed approach bypasses any explicit modelling assumptions and exploits the flexible estimation potential of nonparametric smoothing. An intuitive approach for selecting the tuning parameters governing estimation is provided, which allows the proposed method to automatically determine both an appropriate level of flexibility and also the number of clusters to extract from a given data set. Experiments on a large collection of publicly available data sets are used to document the strong performance of the proposed approach, in comparison with relevant benchmarks from the literature. R code to implement the proposed approach is available from https://github.com/DavidHofmeyr/ CNS
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More</title>
<link>https://arxiv.org/abs/2503.10542</link>
<guid>https://arxiv.org/abs/2503.10542</guid>
<content:encoded><![CDATA[
arXiv:2503.10542v3 Announce Type: replace 
Abstract: This work concerns the path-star task, a minimal example of searching over a graph. The graph, $G$, is star-shaped with $D$ arms radiating from a start node, $s$. A language model (LM) is given $G$, $s$, and a target node $t$, which ends one of the arms and is tasked with generating the arm containing $t$. The minimal nature of this task means only a single choice needs to be made: which of the $D$ arms contains $t$?
  Decoder-only LMs fail to solve this elementary task above $1/D$ chance due to a learned shortcut that absorbs training supervision. We show how this pathology is caused by excess supervision and we present a series of solutions demonstrating that the task is solvable via decoder-only LMs. We find that the task's minimal nature causes its difficulty, as it prevents task decomposition. Our solutions provide insight into the pathology and its implications for LMs trained via next-token prediction.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Privacy Variance</title>
<link>https://arxiv.org/abs/2503.12314</link>
<guid>https://arxiv.org/abs/2503.12314</guid>
<content:encoded><![CDATA[
arXiv:2503.12314v2 Announce Type: replace 
Abstract: We propose the notion of empirical privacy variance and study it in the context of differentially private fine-tuning of language models. Specifically, we show that models calibrated to the same $(\varepsilon, \delta)$-DP guarantee using DP-SGD with different hyperparameter configurations can exhibit significant variations in empirical privacy, which we quantify through the lens of memorization. We investigate the generality of this phenomenon across multiple dimensions and discuss why it is surprising and relevant. Through regression analysis, we examine how individual and composite hyperparameters influence empirical privacy. The results reveal a no-free-lunch trade-off: existing practices of hyperparameter tuning in DP-SGD, which focus on optimizing utility under a fixed privacy budget, often come at the expense of empirical privacy. To address this, we propose refined heuristics for hyperparameter selection that explicitly account for empirical privacy, showing that they are both precise and practically useful. Finally, we take preliminary steps to understand empirical privacy variance. We propose two hypotheses, identify limitations in existing techniques like privacy auditing, and outline open questions for future research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Simplicial Neural Networks</title>
<link>https://arxiv.org/abs/2503.12919</link>
<guid>https://arxiv.org/abs/2503.12919</guid>
<content:encoded><![CDATA[
arXiv:2503.12919v2 Announce Type: replace 
Abstract: Simplicial complexes provide a powerful framework for modeling high-order interactions in structured data, making them particularly suitable for applications such as trajectory prediction and mesh processing. However, existing simplicial neural networks (SNNs), whether convolutional or attention-based, rely primarily on discrete filtering techniques, which can be restrictive. In contrast, partial differential equations (PDEs) on simplicial complexes offer a principled approach to capture continuous dynamics in such structures. In this work, we introduce continuous simplicial neural network (COSIMO), a novel SNN architecture derived from PDEs on simplicial complexes. We provide theoretical and experimental justifications of COSIMO's stability under simplicial perturbations. Furthermore, we investigate the over-smoothing phenomenon, a common issue in geometric deep learning, demonstrating that COSIMO offers better control over this effect than discrete SNNs. Our experiments on real-world datasets demonstrate that COSIMO achieves competitive performance compared to state-of-the-art SNNs in complex and noisy environments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning on LLM Output Signatures for gray-box Behavior Analysis</title>
<link>https://arxiv.org/abs/2503.14043</link>
<guid>https://arxiv.org/abs/2503.14043</guid>
<content:encoded><![CDATA[
arXiv:2503.14043v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved widespread adoption, yet our understanding of their behavior remains limited, particularly in detecting data contamination and hallucinations. While recently proposed probing techniques provide insights through activation analysis, they require ``white-box'' access to model internals, often unavailable. Current ``gray-box'' approaches typically analyze only the probability of the actual tokens in the sequence with simple task-specific heuristics. Importantly, these methods overlook the rich information contained in the full token distribution at each processing step. To address these limitations, we propose that gray-box analysis should leverage the complete observable output of LLMs, consisting of both the previously used token probabilities as well as the complete token distribution sequences - a unified data type we term LOS (LLM Output Signature). To this end, we develop a transformer-based approach to process LOS that theoretically guarantees approximation of existing techniques while enabling more nuanced analysis. Our approach achieves superior performance on hallucination and data contamination detection in gray-box settings, significantly outperforming existing baselines. Furthermore, it demonstrates strong transfer capabilities across datasets and LLMs, suggesting that LOS captures fundamental patterns in LLM behavior. Our code is available at: https://github.com/BarSGuy/LLM-Output-Signatures-Network.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COPA: Comparing the incomparable in multi-objective model evaluation</title>
<link>https://arxiv.org/abs/2503.14321</link>
<guid>https://arxiv.org/abs/2503.14321</guid>
<content:encoded><![CDATA[
arXiv:2503.14321v2 Announce Type: replace 
Abstract: As machine learning (ML) practitioners, we often have hundreds of (trained) ML models at hand from which we need to choose one, based on various objectives such as accuracy, robustness, fairness, scalability, etc. However, how to compare, aggregate and, ultimately, trade-off these objectives is usually a time-consuming task that requires of expert knowledge, as they may be measured in different units or scales. In this work, we investigate how objectives can be automatically normalized and aggregated to systematically navigate their Pareto front. To do so, we make incomparable objectives comparable using their CDFs, approximated by their relative rankings. As a result, we can aggregate them while matching user-specific preferences, allowing practitioners to meaningfully navigate and search for models in the Pareto front. We demonstrate the potential impact of our approach, COPA, in both model selection and benchmarking tasks across diverse ML areas such as fair ML, domain generalization, AutoML and foundation models, where classical ways to normalize and aggregate objectives fall short.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCaFlow: A Deconfounding Causal Generative Model</title>
<link>https://arxiv.org/abs/2503.15114</link>
<guid>https://arxiv.org/abs/2503.15114</guid>
<content:encoded><![CDATA[
arXiv:2503.15114v2 Announce Type: replace 
Abstract: We introduce DeCaFlow, a deconfounding causal generative model. Training once per dataset using just observational data and the underlying causal graph, DeCaFlow enables accurate causal inference on continuous variables under the presence of hidden confounders. Specifically, we extend previous results on causal estimation under hidden confounding to show that a single instance of DeCaFlow provides correct estimates for all causal queries identifiable with do-calculus, leveraging proxy variables to adjust for the causal effects when do-calculus alone is insufficient. Moreover, we show that counterfactual queries are identifiable as long as their interventional counterparts are identifiable, and thus are also correctly estimated by DeCaFlow. Our empirical results on diverse settings (including the Ecoli70 dataset, with 3 independent hidden confounders, tens of observed variables and hundreds of causal queries) show that DeCaFlow outperforms existing approaches, while demonstrating its out-of-the-box applicability to any given causal graph. An implementation can be found in https://github.com/aalmodovares/DeCaFlow
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Lookup Experts</title>
<link>https://arxiv.org/abs/2503.15798</link>
<guid>https://arxiv.org/abs/2503.15798</guid>
<content:encoded><![CDATA[
arXiv:2503.15798v2 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) activates only a subset of experts during inference, allowing the model to maintain low inference FLOPs and latency even as the parameter count scales up. However, since MoE dynamically selects the experts, all the experts need to be loaded into VRAM. Their large parameter size still limits deployment, and offloading, which load experts into VRAM only when needed, significantly increase inference latency. To address this, we propose Mixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in both communication and VRAM usage. In MoLE, the experts are Feed-Forward Networks (FFNs) during training, taking the output of the embedding layer as input. Before inference, these experts can be re-parameterized as lookup tables (LUTs) that retrieves expert outputs based on input ids, and offloaded to storage devices. Therefore, we do not need to perform expert computations during inference. Instead, we directly retrieve the expert's computation results based on input ids and load them into VRAM, and thus the resulting communication overhead is negligible. Experiments show that, with the same FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models and significantly faster than MoE with experts offloading, while maintaining performance on par with MoE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos</title>
<link>https://arxiv.org/abs/2503.16400</link>
<guid>https://arxiv.org/abs/2503.16400</guid>
<content:encoded><![CDATA[
arXiv:2503.16400v3 Announce Type: replace 
Abstract: Video diffusion models (VDMs) facilitate the generation of high-quality videos, with current research predominantly concentrated on scaling efforts during training through improvements in data quality, computational resources, and model complexity. However, inference-time scaling has received less attention, with most approaches restricting models to a single generation attempt. Recent studies have uncovered the existence of "golden noises" that can enhance video quality during generation. Building on this, we find that guiding the scaling inference-time search of VDMs to identify better noise candidates not only evaluates the quality of the frames generated in the current step but also preserves the high-level object features by referencing the anchor frame from previous multi-chunks, thereby delivering long-term value. Our analysis reveals that diffusion models inherently possess flexible adjustments of computation by varying denoising steps, and even a one-step denoising approach, when guided by a reward signal, yields significant long-term benefits. Based on the observation, we proposeScalingNoise, a plug-and-play inference-time search strategy that identifies golden initial noises for the diffusion sampling process to improve global content consistency and visual diversity. Specifically, we perform one-step denoising to convert initial noises into a clip and subsequently evaluate its long-term value, leveraging a reward model anchored by previously generated content. Moreover, to preserve diversity, we sample candidates from a tilted noise distribution that up-weights promising noises. In this way, ScalingNoise significantly reduces noise-induced errors, ensuring more coherent and spatiotemporally consistent video generation. Extensive experiments on benchmark datasets demonstrate that the proposed ScalingNoise effectively improves long video generation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On The Sample Complexity Bounds In Bilevel Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.17644</link>
<guid>https://arxiv.org/abs/2503.17644</guid>
<content:encoded><![CDATA[
arXiv:2503.17644v2 Announce Type: replace 
Abstract: Bilevel reinforcement learning (BRL) has emerged as a powerful framework for aligning generative models, yet its theoretical foundations, especially sample complexity bounds, remain underexplored. In this work, we present the first sample complexity bound for BRL, establishing a rate of $\mathcal{O}(\epsilon^{-3})$ in continuous state-action spaces. Traditional MDP analysis techniques do not extend to BRL due to its nested structure and non-convex lower-level problems. We overcome these challenges by leveraging the Polyak-{\L}ojasiewicz (PL) condition and the MDP structure to obtain closed-form gradients, enabling tight sample complexity analysis. Our analysis also extends to general bi-level optimization settings with non-convex lower levels, where we achieve state-of-the-art sample complexity results of $\mathcal{O}(\epsilon^{-3})$ improving upon existing bounds of $\mathcal{O}(\epsilon^{-6})$. Additionally, we address the computational bottleneck of hypergradient estimation by proposing a fully first-order, Hessian-free algorithm suitable for large-scale problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding</title>
<link>https://arxiv.org/abs/2503.18578</link>
<guid>https://arxiv.org/abs/2503.18578</guid>
<content:encoded><![CDATA[
arXiv:2503.18578v3 Announce Type: replace 
Abstract: Modern vision-language models (VLMs) develop patch embedding and convolution backbone within vector space, especially Euclidean ones, at the very founding. When expanding VLMs to a galaxy scale for understanding astronomical phenomena, the integration of spherical space for planetary orbits and hyperbolic spaces for black holes raises two formidable challenges. a) The current pre-training model is confined to Euclidean space rather than a comprehensive geometric embedding. b) The predominant architecture lacks suitable backbones for anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a geometry-aware VLM, for the universe-level vision understanding tasks. We proposed the geometry prompt that generates geometry tokens by random walks across diverse spaces on a multi-scale physical graph, along with a geometry adapter that compresses and reshapes the space anisotropy in a mixture-of-experts manner. Extensive experiments demonstrate the effectiveness of our approach, with Galaxy-Walker achieving state-of-the-art performance in both galaxy property estimation ($R^2$ scores up to $0.91$) and morphology classification tasks (up to $+0.17$ F1 improvement in challenging features), significantly outperforming both domain-specific models and general-purpose VLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniMoMo: Unified Generative Modeling of 3D Molecules for De Novo Binder Design</title>
<link>https://arxiv.org/abs/2503.19300</link>
<guid>https://arxiv.org/abs/2503.19300</guid>
<content:encoded><![CDATA[
arXiv:2503.19300v3 Announce Type: replace 
Abstract: The design of target-specific molecules such as small molecules, peptides, and antibodies is vital for biological research and drug discovery. Existing generative methods are restricted to single-domain molecules, failing to address versatile therapeutic needs or utilize cross-domain transferability to enhance model performance. In this paper, we introduce Unified generative Modeling of 3D Molecules (UniMoMo), the first framework capable of designing binders of multiple molecular domains using a single model. In particular, UniMoMo unifies the representations of different molecules as graphs of blocks, where each block corresponds to either a standard amino acid or a molecular fragment. Subsequently, UniMoMo utilizes a geometric latent diffusion model for 3D molecular generation, featuring an iterative full-atom autoencoder to compress blocks into latent space points, followed by an E(3)-equivariant diffusion process. Extensive benchmarks across peptides, antibodies, and small molecules demonstrate the superiority of our unified framework over existing domain-specific models, highlighting the benefits of multi-domain training.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoLAE: Mixture of Latent Experts for Parameter-Efficient Language Models</title>
<link>https://arxiv.org/abs/2503.23100</link>
<guid>https://arxiv.org/abs/2503.23100</guid>
<content:encoded><![CDATA[
arXiv:2503.23100v2 Announce Type: replace 
Abstract: Mixture of Experts (MoE) has become a key architectural paradigm for efficiently scaling Large Language Models (LLMs) by selectively activating a subset of parameters for each input token. However, standard MoE architectures face significant challenges, including high memory consumption and communication overhead during distributed training. In this paper, we introduce Mixture of Latent Experts (MoLAE), a novel parameterization that addresses these limitations by reformulating expert operations through a shared projection into a lower-dimensional latent space, followed by expert-specific transformations. This factorized approach substantially reduces parameter count and computational requirements, particularly in existing LLMs where hidden dimensions significantly exceed MoE intermediate dimensions. We provide a rigorous mathematical framework for transforming pre-trained MoE models into MoLAE architecture, characterizing conditions for optimal factorization, and developing a systematic two-step algorithm for this conversion. Our comprehensive theoretical analysis demonstrates that MoLAE significantly improves efficiency across multiple dimensions while preserving model capabilities. Experimental results confirm that MoLAE achieves comparable performance to standard MoE with substantially reduced resource requirements.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CITRAS: Covariate-Informed Transformer for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2503.24007</link>
<guid>https://arxiv.org/abs/2503.24007</guid>
<content:encoded><![CDATA[
arXiv:2503.24007v2 Announce Type: replace 
Abstract: In practical time series forecasting, covariates provide rich contextual information that can potentially enhance the forecast of target variables. Although some covariates extend into the future forecasting horizon (e.g., calendar events, discount schedules), most multivariate models fail to leverage this pivotal insight due to the length discrepancy with target variables. Additionally, capturing the dependency between target variables and covariates is non-trivial, as models must precisely reflect the local impact of covariates while also capturing global cross-variate dependencies. To overcome these challenges, we propose CITRAS, a decoder-only Transformer that flexibly leverages multiple targets, past covariates, and future covariates. While preserving strong autoregressive capabilities, CITRAS introduces two novel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and Attention Score Smoothing. KV Shift seamlessly incorporates future covariates into the forecasting of target variables based on their concurrent dependencies. Additionally, Attention Score Smoothing refines locally accurate patch-wise cross-variate dependencies into global variate-level dependencies by smoothing the past series of attention scores. Experimentally, CITRAS outperforms state-of-the-art models on thirteen real-world benchmarks from both covariate-informed and multivariate settings, demonstrating its versatile ability to leverage cross-variate and cross-time dependencies for improved forecasting accuracy.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lorentzian Graph Isomorphic Network</title>
<link>https://arxiv.org/abs/2504.00142</link>
<guid>https://arxiv.org/abs/2504.00142</guid>
<content:encoded><![CDATA[
arXiv:2504.00142v4 Announce Type: replace 
Abstract: While hyperbolic GNNs show promise for hierarchical data, they often have limited discriminative power compared to Euclidean counterparts or the WL test, due to non-injective aggregation. To address this expressivity gap, we propose the Lorentzian Graph Isomorphic Network (LGIN), a novel HGNN designed for enhanced discrimination within the Lorentzian model. LGIN introduces a new update rule that preserves the Lorentzian metric while effectively capturing richer structural information. This marks a significant step towards more expressive GNNs on Riemannian manifolds. Extensive evaluations across nine benchmark datasets demonstrate LGIN's superior performance, consistently outperforming or matching state-of-the-art hyperbolic and Euclidean baselines, showcasing its ability to capture complex graph structures. LGIN is the first to adapt principles of powerful, highly discriminative GNN architectures to a Riemannian manifold. The code for our paper can be found at https://github.com/Deceptrax123/LGIN
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Introductory Survey to Autoencoder-based Deep Clustering -- Sandboxes for Combining Clustering with Deep Learning</title>
<link>https://arxiv.org/abs/2504.02087</link>
<guid>https://arxiv.org/abs/2504.02087</guid>
<content:encoded><![CDATA[
arXiv:2504.02087v2 Announce Type: replace 
Abstract: Autoencoders offer a general way of learning low-dimensional, non-linear representations from data without labels. This is achieved without making any particular assumptions about the data type or other domain knowledge. The generality and domain agnosticism in combination with their simplicity make autoencoders a perfect sandbox for researching and developing novel (deep) clustering algorithms. Clustering methods group data based on similarity, a task that benefits from the lower-dimensional representation learned by an autoencoder, mitigating the curse of dimensionality. Specifically, the combination of deep learning with clustering, called Deep Clustering, enables to learn a representation tailored to specific clustering tasks, leading to high-quality results. This survey provides an introduction to fundamental autoencoder-based deep clustering algorithms that serve as building blocks for many modern approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-Agent vs. Multi-Agent LLM Strategies for Automated Student Reflection Assessment</title>
<link>https://arxiv.org/abs/2504.05716</link>
<guid>https://arxiv.org/abs/2504.05716</guid>
<content:encoded><![CDATA[
arXiv:2504.05716v2 Announce Type: replace 
Abstract: We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educators' workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust-Region Twisted Policy Improvement</title>
<link>https://arxiv.org/abs/2504.06048</link>
<guid>https://arxiv.org/abs/2504.06048</guid>
<content:encoded><![CDATA[
arXiv:2504.06048v3 Announce Type: replace 
Abstract: Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep reinforcement learning (RL). However, scaling MCTS to parallel compute has proven challenging in practice which has motivated alternative planners like sequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters for smoothing through a reformulation of RL as a policy inference problem. Yet, persisting design choices of these particle filters often conflict with the aim of online planning in RL, which is to obtain a policy improvement at the start of planning. Drawing inspiration from MCTS, we tailor SMC planners specifically for RL by improving data generation within the planner through constrained action sampling and explicit terminal state handling, as well as improving policy and value target estimation. This leads to our Trust-Region Twisted SMC (TRT-SMC), which shows improved runtime and sample-efficiency over baseline MCTS and SMC methods in both discrete and continuous domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STaRFormer: Semi-Supervised Task-Informed Representation Learning via Dynamic Attention-Based Regional Masking for Sequential Data</title>
<link>https://arxiv.org/abs/2504.10097</link>
<guid>https://arxiv.org/abs/2504.10097</guid>
<content:encoded><![CDATA[
arXiv:2504.10097v2 Announce Type: replace 
Abstract: Accurate predictions using sequential spatiotemporal data are crucial for various applications. Utilizing real-world data, we aim to learn the intent of a smart device user within confined areas of a vehicle's surroundings. However, in real-world scenarios, environmental factors and sensor limitations result in non-stationary and irregularly sampled data, posing significant challenges. To address these issues, we developed a Transformer-based approach, STaRFormer, which serves as a universal framework for sequential modeling. STaRFormer employs a novel, dynamic attention-based regional masking scheme combined with semi-supervised contrastive learning to enhance task-specific latent representations. Comprehensive experiments on 15 datasets varying in types (including non-stationary and irregularly sampled), domains, sequence lengths, training samples, and applications, demonstrate the efficacy and practicality of STaRFormer. We achieve notable improvements over state-of-the-art approaches. Code and data will be made available.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers</title>
<link>https://arxiv.org/abs/2504.10957</link>
<guid>https://arxiv.org/abs/2504.10957</guid>
<content:encoded><![CDATA[
arXiv:2504.10957v3 Announce Type: replace 
Abstract: Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roll the dice &amp; look before you leap: Going beyond the creative limits of next-token prediction</title>
<link>https://arxiv.org/abs/2504.15266</link>
<guid>https://arxiv.org/abs/2504.15266</guid>
<content:encoded><![CDATA[
arXiv:2504.15266v2 Announce Type: replace 
Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic and memorizes excessively; multi-token approaches, namely teacherless training and diffusion models, comparatively excel in producing diverse and original output. Secondly, to elicit randomness without hurting coherence, we find that injecting noise at the input layer (dubbed as seed-conditioning) works surprisingly as well as (and in some conditions, better than) temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and temperature sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BackSlash: Rate Constrained Optimized Training of Large Language Models</title>
<link>https://arxiv.org/abs/2504.16968</link>
<guid>https://arxiv.org/abs/2504.16968</guid>
<content:encoded><![CDATA[
arXiv:2504.16968v3 Announce Type: replace 
Abstract: The rapid advancement of large-language models (LLMs) has driven extensive research into parameter compression after training has been completed, yet compression during the training phase remains largely unexplored. In this work, we introduce Rate-Constrained Training (BackSlash), a novel training-time compression approach based on rate-distortion optimization (RDO). BackSlash enables a flexible trade-off between model accuracy and complexity, significantly reducing parameter redundancy while preserving performance. Experiments in various architectures and tasks demonstrate that BackSlash can reduce memory usage by 60% - 90% without accuracy loss and provides significant compression gain compared to compression after training. Moreover, BackSlash proves to be highly versatile: it enhances generalization with small Lagrange multipliers, improves model robustness to pruning (maintaining accuracy even at 80% pruning rates), and enables network simplification for accelerated inference on edge devices.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDI: An adversarial robustness evaluation metric for deep neural networks based on model statistical features</title>
<link>https://arxiv.org/abs/2504.18556</link>
<guid>https://arxiv.org/abs/2504.18556</guid>
<content:encoded><![CDATA[
arXiv:2504.18556v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) are highly susceptible to adversarial samples, raising concerns about their reliability in safety-critical tasks. Currently, methods of evaluating adversarial robustness are primarily categorized into attack-based and certified robustness evaluation approaches. The former not only relies on specific attack algorithms but also is highly time-consuming, while the latter due to its analytical nature, is typically difficult to implement for large and complex models. A few studies evaluate model robustness based on the model's decision boundary, but they suffer from low evaluation accuracy. To address the aforementioned issues, we propose a novel adversarial robustness evaluation metric, Robustness Difference Index (RDI), which is based on model statistical features. RDI draws inspiration from clustering evaluation by analyzing the intra-class and inter-class distances of feature vectors separated by the decision boundary to quantify model robustness. It is attack-independent and has high computational efficiency. Experiments show that, RDI demonstrates a stronger correlation with the gold-standard adversarial robustness metric of attack success rate (ASR). The average computation time of RDI is only 1/30 of the evaluation method based on the PGD attack. Our open-source code is available at: https://github.com/BUPTAIOC/RDI.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks</title>
<link>https://arxiv.org/abs/2504.19274</link>
<guid>https://arxiv.org/abs/2504.19274</guid>
<content:encoded><![CDATA[
arXiv:2504.19274v2 Announce Type: replace 
Abstract: Verification of the integrity of deep learning inference is crucial for understanding whether a model is being applied correctly. However, such verification typically requires access to model weights and (potentially sensitive or private) training data. So-called Zero-knowledge Succinct Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the capability to verify model inference without access to such sensitive data. However, applying ZK-SNARKs to modern neural networks, such as transformers and large vision models, introduces significant computational overhead.
  We present TeleSparse, a ZK-friendly post-processing mechanisms to produce practical solutions to this problem. TeleSparse tackles two fundamental challenges inherent in applying ZK-SNARKs to modern neural networks: (1) Reducing circuit constraints: Over-parameterized models result in numerous constraints for ZK-SNARK verification, driving up memory and proof generation costs. We address this by applying sparsification to neural network models, enhancing proof efficiency without compromising accuracy or security. (2) Minimizing the size of lookup tables required for non-linear functions, by optimizing activation ranges through neural teleportation, a novel adaptation for narrowing activation functions' range.
  TeleSparse reduces prover memory usage by 67% and proof generation time by 46% on the same model, with an accuracy trade-off of approximately 1%. We implement our framework using the Halo2 proving system and demonstrate its effectiveness across multiple architectures (Vision-transformer, ResNet, MobileNet) and datasets (ImageNet,CIFAR-10,CIFAR-100). This work opens new directions for ZK-friendly model design, moving toward scalable, resource-efficient verifiable deep learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20073</link>
<guid>https://arxiv.org/abs/2504.20073</guid>
<content:encoded><![CDATA[
arXiv:2504.20073v2 Announce Type: replace 
Abstract: Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on four stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and gradient stabilization. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Reasoning in Large Language Models with One Training Example</title>
<link>https://arxiv.org/abs/2504.20571</link>
<guid>https://arxiv.org/abs/2504.20571</guid>
<content:encoded><![CDATA[
arXiv:2504.20571v2 Announce Type: replace 
Abstract: We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the mathematical reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Furthermore, RLVR with only two examples even slightly exceeds these results (MATH500: 74.8%, average: 36.6%). Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the "grokking" phenomenon. We also show the critical role of promoting exploration (e.g., by incorporating entropy loss with an appropriate coefficient) in 1-shot RLVR training. We also further discuss related observations about format correction, label robustness and prompt modification. These findings can inspire future work on RLVR efficiency and encourage a re-examination of recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs</title>
<link>https://arxiv.org/abs/2504.21206</link>
<guid>https://arxiv.org/abs/2504.21206</guid>
<content:encoded><![CDATA[
arXiv:2504.21206v2 Announce Type: replace 
Abstract: Federated Graph Learning (FGL) empowers clients to collaboratively train Graph neural networks (GNNs) in a distributed manner while preserving data privacy. However, FGL methods usually require that the graph data owned by all clients is homophilic to ensure similar neighbor distribution patterns of nodes. Such an assumption ensures that the learned knowledge is consistent across the local models from all clients. Therefore, these local models can be properly aggregated as a global model without undermining the overall performance. Nevertheless, when the neighbor distribution patterns of nodes vary across different clients (e.g., when clients hold graphs with different levels of heterophily), their local models may gain different and even conflict knowledge from their node-level predictive tasks. Consequently, aggregating these local models usually leads to catastrophic performance deterioration on the global model. To address this challenge, we propose FedHERO, an FGL framework designed to harness and share insights from heterophilic graphs effectively. At the heart of FedHERO is a dual-channel GNN equipped with a structure learner, engineered to discern the structural knowledge encoded in the local graphs. With this specialized component, FedHERO enables the local model for each client to identify and learn patterns that are universally applicable across graphs with different patterns of node neighbor distributions. FedHERO not only enhances the performance of individual client models by leveraging both local and shared structural insights but also sets a new precedent in this field to effectively handle graph data with various node neighbor distribution patterns. We conduct extensive experiments to validate the superior performance of FedHERO against existing alternatives.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization</title>
<link>https://arxiv.org/abs/2505.00812</link>
<guid>https://arxiv.org/abs/2505.00812</guid>
<content:encoded><![CDATA[
arXiv:2505.00812v2 Announce Type: replace 
Abstract: Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees</title>
<link>https://arxiv.org/abs/2505.01049</link>
<guid>https://arxiv.org/abs/2505.01049</guid>
<content:encoded><![CDATA[
arXiv:2505.01049v2 Announce Type: replace 
Abstract: Consistency models have recently emerged as a compelling alternative to traditional SDE-based diffusion models. They offer a significant acceleration in generation by producing high-quality samples in very few steps. Despite their empirical success, a proper theoretic justification for their speed-up is still lacking. In this work, we address the gap by providing a theoretical analysis of consistency models capable of mapping inputs at a given time to arbitrary points along the reverse trajectory. We show that one can achieve a KL divergence of order $ O(\varepsilon^2) $ using only $ O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with a constant step size. Additionally, under minimal assumptions on the data distribution (non smooth case) an increasingly common setting in recent diffusion model analyses we show that a similar KL convergence guarantee can be obtained, with the number of steps scaling as $ O\left(d \log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide a theoretical analysis for estimation of such consistency models, concluding that accurate learning is feasible using small discretization steps, both in smooth and non-smooth settings. Notably, our results for the non-smooth case yield best in class convergence rates compared to existing SDE or ODE based analyses under minimal assumptions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-preserved Transfer Learning Method for Differential Equations</title>
<link>https://arxiv.org/abs/2505.01281</link>
<guid>https://arxiv.org/abs/2505.01281</guid>
<content:encoded><![CDATA[
arXiv:2505.01281v2 Announce Type: replace 
Abstract: While data-driven methods such as neural operator have achieved great success in solving differential equations (DEs), they suffer from domain shift problems caused by different learning environments (with data bias or equation changes), which can be alleviated by transfer learning (TL). However, existing TL methods adopted in DEs problems lack either generalizability in general DEs problems or physics preservation during training. In this work, we focus on a general transfer learning method that adaptively correct the domain shift and preserve physical information. Mathematically, we characterize the data domain as product distribution and the essential problems as distribution bias and operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that simultaneously admits generalizability to common DEs and physics preservation of specific problem is proposed to adapt the data-driven model to target domain utilizing the push-forward distribution induced by the POTT map. Extensive experiments demonstrate the superior performance, generalizability and physics preservation of the proposed POTT method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Model Inversion Evaluation: From Misleading Standards to Reliable Privacy Assessment</title>
<link>https://arxiv.org/abs/2505.03519</link>
<guid>https://arxiv.org/abs/2505.03519</guid>
<content:encoded><![CDATA[
arXiv:2505.03519v3 Announce Type: replace 
Abstract: Model Inversion (MI) attacks aim to reconstruct information from private training data by exploiting access to machine learning models T. To evaluate such attacks, the standard evaluation framework for such attacks relies on an evaluation model E, trained under the same task design as T. This framework has become the de facto standard for assessing progress in MI research, used across nearly all recent MI attacks and defenses without question. In this paper, we present the first in-depth study of this MI evaluation framework. In particular, we identify a critical issue of this standard MI evaluation framework: Type-I adversarial examples. These are reconstructions that do not capture the visual features of private training data, yet are still deemed successful by the target model T and ultimately transferable to E. Such false positives undermine the reliability of the standard MI evaluation framework. To address this issue, we introduce a new MI evaluation framework that replaces the evaluation model E with advanced Multimodal Large Language Models (MLLMs). By leveraging their general-purpose visual understanding, our MLLM-based framework does not depend on training of shared task design as in T, thus reducing Type-I transferability and providing more faithful assessments of reconstruction success. Using our MLLM-based evaluation framework, we reevaluate 26 diverse MI attack setups and empirically reveal consistently high false positive rates under the standard evaluation framework. Importantly, we demonstrate that many state-of-the-art (SOTA) MI methods report inflated attack accuracy, indicating that actual privacy leakage is significantly lower than previously believed. By uncovering this critical issue and proposing a robust solution, our work enables a reassessment of progress in MI research and sets a new standard for reliable and robust evaluation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.03806</link>
<guid>https://arxiv.org/abs/2505.03806</guid>
<content:encoded><![CDATA[
arXiv:2505.03806v2 Announce Type: replace 
Abstract: This article introduces Perception-Informed Neural Networks (PrINNs), a framework designed to incorporate perception-based information into neural networks, addressing both systems with known and unknown physics laws or differential equations. Moreover, PrINNs extend the concept of Physics-Informed Neural Networks (PINNs) and their variants, offering a platform for the integration of diverse forms of perception precisiation, including singular, probability distribution, possibility distribution, interval, and fuzzy graph. In fact, PrINNs allow neural networks to model dynamical systems by integrating expert knowledge and perception-based information through loss functions, enabling the creation of modern data-driven models. Some of the key contributions include Mixture of Experts Informed Neural Networks (MOEINNs), which combine heterogeneous expert knowledge into the network, and Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the incorporation of meta-information for enhanced model performance. Additionally, Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural networks leverage fuzzy logic constraints within a deep learning architecture, allowing online training without pre-training and eliminating the need for defuzzification. PrINNs represent a significant step forward in bridging the gap between traditional physics-based modeling and modern data-driven approaches, enabling neural networks to learn from both structured physics laws and flexible perception-based rules. This approach empowers neural networks to operate in uncertain environments, model complex systems, and discover new forms of differential equations, making PrINNs a powerful tool for advancing computational science and engineering.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering</title>
<link>https://arxiv.org/abs/2505.05738</link>
<guid>https://arxiv.org/abs/2505.05738</guid>
<content:encoded><![CDATA[
arXiv:2505.05738v2 Announce Type: replace 
Abstract: Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cluster-Aware Multi-Round Update for Wireless Federated Learning in Heterogeneous Environments</title>
<link>https://arxiv.org/abs/2505.06268</link>
<guid>https://arxiv.org/abs/2505.06268</guid>
<content:encoded><![CDATA[
arXiv:2505.06268v2 Announce Type: replace 
Abstract: The aggregation efficiency and accuracy of wireless Federated Learning (FL) are significantly affected by resource constraints, especially in heterogeneous environments where devices exhibit distinct data distributions and communication capabilities. This paper proposes a clustering strategy that leverages prior knowledge similarity to group devices with similar data and communication characteristics, mitigating performance degradation from heterogeneity. On this basis, a novel Cluster- Aware Multi-round Update (CAMU) strategy is proposed, which treats clusters as the basic units and adjusts the local update frequency based on the clustered contribution threshold, effectively reducing update bias and enhancing aggregation accuracy. The theoretical convergence of the CAMU strategy is rigorously validated. Meanwhile, based on the convergence upper bound, the local update frequency and transmission power of each cluster are jointly optimized to achieve an optimal balance between computation and communication resources under constrained conditions, significantly improving the convergence efficiency of FL. Experimental results demonstrate that the proposed method effectively improves the model performance of FL in heterogeneous environments and achieves a better balance between communication cost and computational load under limited resources.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety</title>
<link>https://arxiv.org/abs/2505.06843</link>
<guid>https://arxiv.org/abs/2505.06843</guid>
<content:encoded><![CDATA[
arXiv:2505.06843v2 Announce Type: replace 
Abstract: Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across seven mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards. Codes are available at https://github.com/GuanZihan/Benign-Samples-Matter.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks</title>
<link>https://arxiv.org/abs/2505.07895</link>
<guid>https://arxiv.org/abs/2505.07895</guid>
<content:encoded><![CDATA[
arXiv:2505.07895v2 Announce Type: replace 
Abstract: Nowadays, numerous online platforms can be described as multi-modal heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's product review networks. Accurately categorizing nodes within these networks is crucial for analyzing the corresponding entities, which requires effective representation learning on nodes. However, existing multi-modal fusion methods often adopt either early fusion strategies which may lose the unique characteristics of individual modalities, or late fusion approaches overlooking the cross-modal guidance in GNN-based information propagation. In this paper, we propose a novel model for node classification in MMHNs, named Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node representations by capturing the mutual influence of multiple modalities during the information propagation process, within the framework of heterogeneous graph transformer. Specifically, a nested inter-modal attention mechanism is integrated into the inter-node attention to achieve adaptive multi-modal fusion, and modality alignment is also taken into account to encourage the propagation among nodes with consistent similarities across all modalities. Moreover, an attention loss is augmented to mitigate the impact of missing modalities. Extensive experiments validate the superiority of the model in the node classification task, providing an innovative view to handle multi-modal data, especially when accompanied with network structures.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods</title>
<link>https://arxiv.org/abs/2505.09218</link>
<guid>https://arxiv.org/abs/2505.09218</guid>
<content:encoded><![CDATA[
arXiv:2505.09218v2 Announce Type: replace 
Abstract: We propose a new unifying framework, Birch SGD, for analyzing and designing distributed SGD methods. The central idea is to represent each method as a weighted directed tree, referred to as a computation tree. Leveraging this representation, we introduce a general theoretical result that reduces convergence analysis to studying the geometry of these trees. This perspective yields a purely graph-based interpretation of optimization dynamics, offering a new and intuitive foundation for method development. Using Birch SGD, we design eight new methods and analyze them alongside previously known ones, with at least six of the new methods shown to have optimal computational time complexity. Our research leads to two key insights: (i) all methods share the same "iteration rate" of $O\left(\frac{(R + 1) L \Delta}{\varepsilon} + \frac{\sigma^2 L \Delta}{\varepsilon^2}\right)$, where $R$ the maximum "tree distance" along the main branch of a tree; and (ii) different methods exhibit different trade-offs-for example, some update iterates more frequently, improving practical performance, while others are more communication-efficient or focus on other aspects. Birch SGD serves as a unifying framework for navigating these trade-offs. We believe these results provide a unified foundation for understanding, analyzing, and designing efficient asynchronous and parallel optimization methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations</title>
<link>https://arxiv.org/abs/2505.09284</link>
<guid>https://arxiv.org/abs/2505.09284</guid>
<content:encoded><![CDATA[
arXiv:2505.09284v2 Announce Type: replace 
Abstract: Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations presents a fundamental challenge in scientific research. Recently, diffusion-based generative modeling shows promising potential for physical simulation. However, current approaches typically operate on on-grid data with preset spatiotemporal resolution, but struggle with the sparsely observed and continuous nature of real-world physical dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in Functional Tucker space, a novel framework that generates full-field evolution of physical dynamics from irregular sparse observations. SDIFT leverages the functional Tucker model as the latent space representer with proven universal approximation property, and represents observations as latent functions and Tucker core sequences. We then construct a sequential diffusion model with temporally augmented UNet in the functional Tucker space, denoising noise drawn from a Gaussian process to generate the sequence of core tensors.
  At the posterior sampling stage, we propose a Message-Passing Posterior Sampling mechanism, enabling conditional generation of the entire sequence guided by observations at limited time steps. We validate SDIFT on three physical systems spanning astronomical (supernova explosions, light-year scale), environmental (ocean sound speed fields, kilometer scale), and molecular (organic liquid, millimeter scale) domains, demonstrating significant improvements in both reconstruction accuracy and computational efficiency compared to state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Final Layer Holds the Key: A Unified and Efficient GNN Calibration Framework</title>
<link>https://arxiv.org/abs/2505.11335</link>
<guid>https://arxiv.org/abs/2505.11335</guid>
<content:encoded><![CDATA[
arXiv:2505.11335v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness on graph-based tasks. However, their predictive confidence is often miscalibrated, typically exhibiting under-confidence, which harms the reliability of their decisions. Existing calibration methods for GNNs normally introduce additional calibration components, which fail to capture the intrinsic relationship between the model and the prediction confidence, resulting in limited theoretical guarantees and increased computational overhead. To address this issue, we propose a simple yet efficient graph calibration method. We establish a unified theoretical framework revealing that model confidence is jointly governed by class-centroid-level and node-level calibration at the final layer. Based on this insight, we theoretically show that reducing the weight decay of the final-layer parameters alleviates GNN under-confidence by acting on the class-centroid level, while node-level calibration acts as a finer-grained complement to class-centroid level calibration, which encourages each test node to be closer to its predicted class centroid at the final-layer representations. Extensive experiments validate the superiority of our method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models</title>
<link>https://arxiv.org/abs/2505.11731</link>
<guid>https://arxiv.org/abs/2505.11731</guid>
<content:encoded><![CDATA[
arXiv:2505.11731v2 Announce Type: replace 
Abstract: Recent advances in uncertainty estimation for Large Language Models (LLMs) during downstream adaptation have addressed key challenges of reliability and simplicity. However, existing Bayesian methods typically require multiple sampling iterations during inference, creating significant efficiency issues that limit practical deployment. In this paper, we investigate the possibility of eliminating the need for test-time sampling for LLM uncertainty estimation. Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned confidence into a non-Bayesian student LLM by minimizing the divergence between their predictive distributions. Unlike typical calibration methods, our distillation is carried out solely on the training dataset without the need of an additional validation dataset. This simple yet effective approach achieves N-times more efficient uncertainty estimation during testing, where N is the number of samples traditionally required by Bayesian LLMs. Our extensive experiments demonstrate that uncertainty estimation capabilities on training data can successfully generalize to unseen test data through our distillation technique, consistently producing results comparable to (or even better than) state-of-the-art Bayesian LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11896</link>
<guid>https://arxiv.org/abs/2505.11896</guid>
<content:encoded><![CDATA[
arXiv:2505.11896v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities but often face challenges with tasks requiring sophisticated reasoning. While Chain-of-Thought (CoT) prompting significantly enhances reasoning, it indiscriminately generates lengthy reasoning steps for all queries, leading to substantial computational costs and inefficiency, especially for simpler inputs. To address this critical issue, we introduce AdaCoT (Adaptive Chain-of-Thought), a novel framework enabling LLMs to adaptively decide when to invoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem that seeks to balance model performance with the costs associated with CoT invocation (both frequency and computational overhead). We propose a reinforcement learning (RL) based method, specifically utilizing Proximal Policy Optimization (PPO), to dynamically control the CoT triggering decision boundary by adjusting penalty coefficients, thereby allowing the model to determine CoT necessity based on implicit query complexity. A key technical contribution is Selective Loss Masking (SLM), designed to counteract decision boundary collapse during multi-stage RL training, ensuring robust and stable adaptive triggering. Experimental results demonstrate that AdaCoT successfully navigates the Pareto frontier, achieving substantial reductions in CoT usage for queries not requiring elaborate reasoning. For instance, on our production traffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\% and decreased average response tokens by 69.06%, while maintaining high performance on complex tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When the Left Foot Leads to the Right Path: Bridging Initial Prejudice and Trainability</title>
<link>https://arxiv.org/abs/2505.12096</link>
<guid>https://arxiv.org/abs/2505.12096</guid>
<content:encoded><![CDATA[
arXiv:2505.12096v2 Announce Type: replace 
Abstract: Understanding the statistical properties of deep neural networks (DNNs) at initialization is crucial for elucidating both their trainability and the intrinsic architectural biases they encode prior to data exposure. Mean-field (MF) analyses have demonstrated that the parameter distribution in randomly initialized networks dictates whether gradients vanish or explode. Concurrently, untrained DNNs were found to exhibit an initial-guessing bias (IGB), in which large regions of the input space are assigned to a single class. In this work, we derive a theoretical proof establishing the correspondence between IGB and previous MF theories, thereby connecting a network prejudice toward specific classes with the conditions for fast and accurate learning. This connection yields the counter-intuitive conclusion: the initialization that optimizes trainability is necessarily biased, rather than neutral. Furthermore, we extend the MF/IGB framework to multi-node activation functions, offering practical guidelines for designing initialization schemes that ensure stable optimization in architectures employing max- and average-pooling layers.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractured Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.12992</link>
<guid>https://arxiv.org/abs/2505.12992</guid>
<content:encoded><![CDATA[
arXiv:2505.12992v2 Announce Type: replace 
Abstract: Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning. Code is available at https://github.com/BaohaoLiao/frac-cot.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KHRONOS: a Kernel-Based Neural Architecture for Rapid, Resource-Efficient Scientific Computation</title>
<link>https://arxiv.org/abs/2505.13315</link>
<guid>https://arxiv.org/abs/2505.13315</guid>
<content:encoded><![CDATA[
arXiv:2505.13315v2 Announce Type: replace 
Abstract: Contemporary models of high dimensional physical systems are constrained by the curse of dimensionality and a reliance on dense data. We introduce KHRONOS (Kernel Expansion Hierarchy for Reduced Order, Neural Optimized Surrogates), an AI framework for model based, model free and model inversion tasks. KHRONOS constructs continuously differentiable target fields with a hierarchical composition of per-dimension kernel expansions, which are tensorized into modes and then superposed. We evaluate KHRONOS on a canonical 2D, Poisson equation benchmark: across 16 to 512 degrees of freedom (DoFs), it obtained L_2-square errors of 5e-4 down to 6e-11. This represents a greater than 100-fold gain over Kolmogorov Arnold Networks (which itself reports a 100 times improvement on MLPs/PINNs with 100 times fewer parameters) when controlling for the number of parameters. This also represents a 1e6-fold improvement in L_2-square error compared to standard linear FEM at comparable DoFs. Inference complexity is dominated by inner products, yielding sub-millisecond full-field predictions that scale to an arbitrary resolution. For inverse problems, KHRONOS facilitates rapid, iterative level set recovery in only a few forward evaluations, with sub-microsecond per sample latency. KHRONOS's scalability, expressivity, and interpretability open new avenues in constrained edge computing, online control, computer vision, and beyond.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Compositional Generation with Diffusion Models Using Lift Scores</title>
<link>https://arxiv.org/abs/2505.13740</link>
<guid>https://arxiv.org/abs/2505.13740</guid>
<content:encoded><![CDATA[
arXiv:2505.13740v2 Announce Type: replace 
Abstract: We introduce a novel resampling criterion using lift scores, for improving compositional generation in diffusion models. By leveraging the lift scores, we evaluate whether generated samples align with each single condition and then compose the results to determine whether the composed prompt is satisfied. Our key insight is that lift scores can be efficiently approximated using only the original diffusion model, requiring no additional training or external modules. We develop an optimized variant that achieves relatively lower computational overhead during inference while maintaining effectiveness. Through extensive experiments, we demonstrate that lift scores significantly improved the condition alignment for compositional generation across 2D synthetic data, CLEVR position tasks, and text-to-image synthesis. Our code is available at http://rainorangelemon.github.io/complift.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis</title>
<link>https://arxiv.org/abs/2505.13768</link>
<guid>https://arxiv.org/abs/2505.13768</guid>
<content:encoded><![CDATA[
arXiv:2505.13768v2 Announce Type: replace 
Abstract: This paper investigates a hybrid learning framework for reinforcement learning (RL) in which the agent can leverage both an offline dataset and online interactions to learn the optimal policy. We present a unified algorithm and analysis and show that augmenting confidence-based online RL algorithms with the offline dataset outperforms any pure online or offline algorithm alone and achieves state-of-the-art results under two learning metrics, i.e., sub-optimality gap and online learning regret. Specifically, we show that our algorithm achieves a sub-optimality gap $\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where $\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$ are the numbers of offline and online samples, respectively. For regret minimization, we show that it achieves a constant $\tilde{O}( \sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability coefficient over all sub-optimal policies. Our results also reveal an interesting separation on the desired coverage properties of the offline dataset for sub-optimality gap minimization and regret minimization. We further validate our theoretical findings in several experiments in special RL models such as linear contextual bandits and Markov decision processes (MDPs).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEVER: A Curated Benchmark for Formally Verified Code Generation</title>
<link>https://arxiv.org/abs/2505.13938</link>
<guid>https://arxiv.org/abs/2505.13938</guid>
<content:encoded><![CDATA[
arXiv:2505.13938v3 Announce Type: replace 
Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of 161 problems for end-to-end verified code generation in Lean. Each problem consists of (1) the task of generating a specification that matches a held-out ground-truth specification, and (2) the task of generating a Lean implementation that provably satisfies this specification. Unlike prior benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated annotations, and specifications that leak implementation logic or allow vacuous solutions. All outputs are verified post-hoc using Lean's type checker to ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to evaluate several few-shot and agentic approaches based on state-of-the-art language models. These methods all struggle to achieve full verification, establishing it as a challenging frontier benchmark for program synthesis and formal reasoning. Our benchmark can be found on GitHub(https://github.com/trishullab/clever) as well as HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our evaluation code is also available online(https://github.com/trishullab/clever-prover).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Cyclic Diffusion for Inference Scaling</title>
<link>https://arxiv.org/abs/2505.14036</link>
<guid>https://arxiv.org/abs/2505.14036</guid>
<content:encoded><![CDATA[
arXiv:2505.14036v2 Announce Type: replace 
Abstract: Diffusion models have demonstrated strong generative capabilities across domains ranging from image synthesis to complex reasoning tasks. However, most inference-time scaling methods rely on fixed denoising schedules, limiting their ability to allocate computation based on instance difficulty or task-specific demands adaptively. We introduce the challenge of adaptive inference-time scaling-dynamically adjusting computational effort during inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a flexible, search-based inference framework. ABCD refines outputs through bi-directional diffusion cycles while adaptively controlling exploration depth and termination. It comprises three components: Cyclic Diffusion Search, Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time. Experiments show that ABCD improves performance across diverse tasks while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Cells: Predict, Explain, Discover</title>
<link>https://arxiv.org/abs/2505.14613</link>
<guid>https://arxiv.org/abs/2505.14613</guid>
<content:encoded><![CDATA[
arXiv:2505.14613v2 Announce Type: replace 
Abstract: Drug discovery is fundamentally a process of inferring the effects of treatments on patients, and would therefore benefit immensely from computational models that can reliably simulate patient responses, enabling researchers to generate and test large numbers of therapeutic hypotheses safely and economically before initiating costly clinical trials. Even a more specific model that predicts the functional response of cells to a wide range of perturbations would be tremendously valuable for discovering safe and effective treatments that successfully translate to the clinic. Creating such virtual cells has long been a goal of the computational research community that unfortunately remains unachieved given the daunting complexity and scale of cellular biology. Nevertheless, recent advances in AI, computing power, lab automation, and high-throughput cellular profiling provide new opportunities for reaching this goal. In this perspective, we present a vision for developing and evaluating virtual cells that builds on our experience at Recursion. We argue that in order to be a useful tool to discover novel biology, virtual cells must accurately predict the functional response of a cell to perturbations and explain how the predicted response is a consequence of modifications to key biomolecular interactions. We then introduce key principles for designing therapeutically-relevant virtual cells, describe a lab-in-the-loop approach for generating novel insights with them, and advocate for biologically-grounded benchmarks to guide virtual cell development. Finally, we make the case that our approach to virtual cells provides a useful framework for building other models at higher levels of organization, including virtual patients. We hope that these directions prove useful to the research community in developing virtual models optimized for positive impact on drug discovery outcomes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards End-to-End Training of Automatic Speech Recognition for Nigerian Pidgin</title>
<link>https://arxiv.org/abs/2010.11123</link>
<guid>https://arxiv.org/abs/2010.11123</guid>
<content:encoded><![CDATA[
arXiv:2010.11123v2 Announce Type: replace-cross 
Abstract: The prevalence of automatic speech recognition (ASR) systems in spoken language applications has increased significantly in recent years. Notably, many African languages lack sufficient linguistic resources to support the robustness of these systems. This paper focuses on the development of an end-to-end speech recognition system customized for Nigerian Pidgin English. We investigated and evaluated different pretrained state-of-the-art architectures on a new dataset. Our empirical results demonstrate a notable performance of the variant Wav2Vec2 XLSR-53 on our dataset, achieving a word error rate (WER) of 29.6% on the test set, surpassing other architectures such as NEMO QUARTZNET and Wav2Vec2.0 BASE-100H in quantitative assessments. Additionally, we demonstrate that pretrained state-of-the-art architectures do not work well out-of-the-box. We performed zero-shot evaluation using XLSR-English as the baseline, chosen for its similarity to Nigerian Pidgin. This yielded a higher WER of 73.7%. By adapting this architecture to nuances represented in our dataset, we reduce error by 59.84%. Our dataset comprises 4,288 recorded utterances from 10 native speakers, partitioned into training, validation, and test sets. This study underscores the potential for improving ASR systems for under-resourced languages like Nigerian Pidgin English, contributing to greater inclusion in speech technology applications. We publicly release our unique parallel dataset (speech-to-text) on Nigerian Pidgin, as well as the model weights on Hugging Face. Our code would be made available to foster future research from the community.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks for Knowledge Enhanced Visual Representation of Paintings</title>
<link>https://arxiv.org/abs/2105.08190</link>
<guid>https://arxiv.org/abs/2105.08190</guid>
<content:encoded><![CDATA[
arXiv:2105.08190v2 Announce Type: replace-cross 
Abstract: We propose ArtSAGENet, a novel multimodal architecture that integrates Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), to jointly learn visual and semantic-based artistic representations. First, we illustrate the significant advantages of multi-task learning for fine art analysis and argue that it is conceptually a much more appropriate setting in the fine art domain than the single-task alternatives. We further demonstrate that several GNN architectures can outperform strong CNN baselines in a range of fine art analysis tasks, such as style classification, artist attribution, creation period estimation, and tag prediction, while training them requires an order of magnitude less computational time and only a small amount of labeled data. Finally, through extensive experimentation we show that our proposed ArtSAGENet captures and encodes valuable relational dependencies between the artists and the artworks, surpassing the performance of traditional methods that rely solely on the analysis of visual content. Our findings underline a great potential of integrating visual content and semantics for fine art analysis and curation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Can Wash Hands Better: Accurate Daily Handwashing Assessment with a Smartwatch</title>
<link>https://arxiv.org/abs/2112.06657</link>
<guid>https://arxiv.org/abs/2112.06657</guid>
<content:encoded><![CDATA[
arXiv:2112.06657v5 Announce Type: replace-cross 
Abstract: Hand hygiene is among the most effective daily practices for preventing infectious diseases such as influenza, malaria, and skin infections. While professional guidelines emphasize proper handwashing to reduce the risk of viral infections, surveys reveal that adherence to these recommendations remains low. To address this gap, we propose UWash, a wearable solution leveraging smartwatches to evaluate handwashing procedures, aiming to raise awareness and cultivate high-quality handwashing habits. We frame the task of handwashing assessment as an action segmentation problem, similar to those in computer vision, and introduce a simple yet efficient two-stream UNet-like network to achieve this goal. Experiments involving 51 subjects demonstrate that UWash achieves 92.27% accuracy in handwashing gesture recognition, an error of <0.5 seconds in onset/offset detection, and an error of <5 points in gesture scoring under user-dependent settings. The system also performs robustly in user-independent and user-independent-location-independent evaluations. Remarkably, UWash maintains high performance in real-world tests, including evaluations with 10 random passersby at a hospital 9 months later and 10 passersby in an in-the-wild test conducted 2 years later. UWash is the first system to score handwashing quality based on gesture sequences, offering actionable guidance for improving daily hand hygiene. The code and dataset are publicly available at https://github.com/aiotgroup/UWash
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection</title>
<link>https://arxiv.org/abs/2112.11479</link>
<guid>https://arxiv.org/abs/2112.11479</guid>
<content:encoded><![CDATA[
arXiv:2112.11479v4 Announce Type: replace-cross 
Abstract: Recent advancements in technology have led to a boost in social media usage which has ultimately led to large amounts of user-generated data which also includes hateful and offensive speech. The language used in social media is often a combination of English and the native language in the region. In India, Hindi is used predominantly and is often code-switched with English, giving rise to the Hinglish (Hindi+English) language. Various approaches have been made in the past to classify the code-mixed Hinglish hate speech using different machine learning and deep learning-based techniques. However, these techniques make use of recurrence on convolution mechanisms which are computationally expensive and have high memory requirements. Past techniques also make use of complex data processing making the existing techniques very complex and non-sustainable to change in data. Proposed work gives a much simpler approach which is not only at par with these complex networks but also exceeds performance with the use of subword tokenization algorithms like BPE and Unigram, along with multi-head attention-based techniques, giving an accuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use of BPE and Unigram algorithms help handle the nonconventional Hinglish vocabulary making the proposed technique simple, efficient and sustainable to use in the real world.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Compositional Optimization with Compositional Constraints</title>
<link>https://arxiv.org/abs/2209.04086</link>
<guid>https://arxiv.org/abs/2209.04086</guid>
<content:encoded><![CDATA[
arXiv:2209.04086v2 Announce Type: replace-cross 
Abstract: Stochastic compositional optimization (SCO) has attracted considerable attention because of its broad applicability to important real-world problems. However, existing works on SCO assume that the projection within a solution update is simple, which fails to hold for problem instances where the constraints are in the form of expectations, such as empirical conditional value-at-risk constraints. We study a novel model that incorporates single-level expected value and two-level compositional constraints into the current SCO framework. Our model can be applied widely to data-driven optimization and risk management, including risk-averse optimization and high-moment portfolio selection, and can handle multiple constraints. We further propose a class of primal-dual algorithms that generates sequences converging to the optimal solution at the rate of $\cO(\frac{1}{\sqrt{N}})$under both single-level expected value and two-level compositional constraints, where $N$ is the iteration counter, establishing the benchmarks in expected value constrained SCO.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems</title>
<link>https://arxiv.org/abs/2305.02251</link>
<guid>https://arxiv.org/abs/2305.02251</guid>
<content:encoded><![CDATA[
arXiv:2305.02251v2 Announce Type: replace-cross 
Abstract: The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality scientific discoveries highly autonomously at a level comparable, and possibly superior, to the best human scientists by 2050.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Quantum Approximation Scheme for k-Means</title>
<link>https://arxiv.org/abs/2308.08167</link>
<guid>https://arxiv.org/abs/2308.08167</guid>
<content:encoded><![CDATA[
arXiv:2308.08167v3 Announce Type: replace-cross 
Abstract: We give a quantum approximation scheme (i.e., $(1 + \varepsilon)$-approximation for every $\varepsilon > 0$) for the classical $k$-means clustering problem in the QRAM model with a running time that has only polylogarithmic dependence on the number of data points. More specifically, given a dataset $V$ with $N$ points in $\mathbb{R}^d$ stored in QRAM data structure, our quantum algorithm runs in time $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ and with high probability outputs a set $C$ of $k$ centers such that $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$. Here $C_{OPT}$ denotes the optimal $k$-centers, $cost(.)$ denotes the standard $k$-means cost function (i.e., the sum of the squared distance of points to the closest center), and $\eta$ is the aspect ratio (i.e., the ratio of maximum distance to minimum distance). This is the first quantum algorithm with a polylogarithmic running time that gives a provable approximation guarantee of $(1+\varepsilon)$ for the $k$-means problem. Also, unlike previous works on unsupervised learning, our quantum algorithm does not require quantum linear algebra subroutines and has a running time independent of parameters (e.g., condition number) that appear in such procedures.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Pauli channel estimation with logarithmic quantum memory</title>
<link>https://arxiv.org/abs/2309.14326</link>
<guid>https://arxiv.org/abs/2309.14326</guid>
<content:encoded><![CDATA[
arXiv:2309.14326v4 Announce Type: replace-cross 
Abstract: Here we revisit one of the prototypical tasks for characterizing the structure of noise in quantum devices: estimating every eigenvalue of an $n$-qubit Pauli noise channel to error $\epsilon$. Prior work [14] proved no-go theorems for this task in the practical regime where one has a limited amount of quantum memory, e.g. any protocol with $\le 0.99n$ ancilla qubits of quantum memory must make exponentially many measurements, provided it is non-concatenating. Such protocols can only interact with the channel by repeatedly preparing a state, passing it through the channel, and measuring immediately afterward.
  This left open a natural question: does the lower bound hold even for general protocols, i.e. ones which chain together many queries to the channel, interleaved with arbitrary data-processing channels, before measuring? Surprisingly, in this work we show the opposite: there is a protocol that can estimate the eigenvalues of a Pauli channel to error $\epsilon$ using only $O(\log n/\epsilon^2)$ ancilla and $\tilde{O}(n^2/\epsilon^2)$ measurements. In contrast, we show that any protocol with zero ancilla, even a concatenating one, must make $\Omega(2^n/\epsilon^2)$ measurements, which is tight.
  Our results imply, to our knowledge, the first quantum learning task where logarithmically many qubits of quantum memory suffice for an exponential statistical advantage. Our protocol can be naturally extended to a protocol that learns the eigenvalues of Pauli terms within any subset $A$ of a Pauli channel with $O(\log\log(|A|)/\epsilon^2)$ ancilla and $\tilde{O}(n^2/\epsilon^2)$ measurements.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefinedFields: Radiance Fields Refinement for Planar Scene Representations</title>
<link>https://arxiv.org/abs/2312.00639</link>
<guid>https://arxiv.org/abs/2312.00639</guid>
<content:encoded><![CDATA[
arXiv:2312.00639v4 Announce Type: replace-cross 
Abstract: Planar scene representations have recently witnessed increased interests for modeling scenes from images, as their lightweight planar structure enables compatibility with image-based models. Notably, K-Planes have gained particular attention as they extend planar scene representations to support in-the-wild scenes, in addition to object-level scenes. However, their visual quality has recently lagged behind that of state-of-the-art techniques. To reduce this gap, we propose RefinedFields, a method that leverages pre-trained networks to refine K-Planes scene representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and improves upon its base representation on the task of novel view synthesis. Our project page can be found at https://refinedfields.github.io .
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FERGI: Automatic Scoring of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction</title>
<link>https://arxiv.org/abs/2312.03187</link>
<guid>https://arxiv.org/abs/2312.03187</guid>
<content:encoded><![CDATA[
arXiv:2312.03187v4 Announce Type: replace-cross 
Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically score user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. We develop an FAU-Net (Facial Action Units Neural Network), which receives inputs from an AU estimation model, to automatically score user preferences for text-to-image generation based on their facial expression reactions, which is complementary to the pre-trained scoring models based on the input text prompts and generated images. Integrating our FAU-Net valence score with the pre-trained scoring models improves their consistency with human preferences. This method of automatic annotation with facial expression analysis can be potentially generalized to other generation tasks. The code is available at https://github.com/ShuangquanFeng/FERGI, and the dataset is also available at the same link for research purposes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs</title>
<link>https://arxiv.org/abs/2402.05668</link>
<guid>https://arxiv.org/abs/2402.05668</guid>
<content:encoded><![CDATA[
arXiv:2402.05668v3 Announce Type: replace-cross 
Abstract: Jailbreak attacks aim to bypass the LLMs' safeguards. While researchers have proposed different jailbreak attacks in depth, they have done so in isolation -- either with unaligned settings or comparing a limited range of methods. To fill this gap, we present a large-scale evaluation of various jailbreak attacks. We collect 17 representative jailbreak attacks, summarize their features, and establish a novel jailbreak attack taxonomy. Then we conduct comprehensive measurement and ablation studies across nine aligned LLMs on 160 forbidden questions from 16 violation categories. Also, we test jailbreak attacks under eight advanced defenses. Based on our taxonomy and experiments, we identify some important patterns, such as heuristic-based attacks could achieve high attack success rates but are easy to mitigate by defenses, causing low practicality. Our study offers valuable insights for future research on jailbreak attacks and defenses. We hope our work could help the community avoid incremental work and serve as an effective benchmark tool for practitioners.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Multi-Task Pre-training for Malicious URL Detection and Webpage Classification</title>
<link>https://arxiv.org/abs/2402.11495</link>
<guid>https://arxiv.org/abs/2402.11495</guid>
<content:encoded><![CDATA[
arXiv:2402.11495v2 Announce Type: replace-cross 
Abstract: Malicious URL detection and webpage classification are critical tasks in cybersecurity and information management. In recent years, extensive research has explored using BERT or similar language models to replace traditional machine learning methods for detecting malicious URLs and classifying webpages. While previous studies show promising results, they often apply existing language models to these tasks without accounting for the inherent differences in domain data (e.g., URLs being loosely structured and semantically sparse compared to text), leaving room for performance improvement. Furthermore, current approaches focus on single tasks and have not been tested in multi-task scenarios. To address these challenges, we propose urlBERT, a pre-trained URL encoder leveraging Transformer to encode foundational knowledge from billions of unlabeled URLs. To achieve it, we propose to use 5 unsupervised pretraining tasks to capture multi-level information of URL lexical, syntax, and semantics, and generate contrastive and adversarial representations. Furthermore, to avoid inter-pre-training competition and interference, we proposed a grouped sequential learning method to ensure effective training across multi-tasks. Finally, we leverage a two-stage fine-tuning approach to improve the training stability and efficiency of the task model. To assess the multitasking potential of urlBERT, we fine-tune the task model in both single-task and multi-task modes. The former creates a classification model for a single task, while the latter builds a classification model capable of handling multiple tasks. We evaluate urlBERT on three downstream tasks: phishing URL detection, advertising URL detection, and webpage classification. The results demonstrate that urlBERT outperforms standard pre-trained models, and its multi-task mode is capable of addressing the real-world demands of multitasking.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Hessian Fittings with Lie Groups</title>
<link>https://arxiv.org/abs/2402.11858</link>
<guid>https://arxiv.org/abs/2402.11858</guid>
<content:encoded><![CDATA[
arXiv:2402.11858v5 Announce Type: replace-cross 
Abstract: This report investigates the fitting of Hessian or its inverse for stochastic optimizations using a Hessian fitting criterion derived from the preconditioned stochastic gradient descent (PSGD) method. This criterion is closely related to many widely used second-order and adaptive gradient optimization methods, including BFGS, the Gauss-Newton algorithm, natural gradient descent, and AdaGrad. Our analyses reveal the efficiency and reliability differences of a broad range of preconditioner fitting methods, ranging from closed-form to iterative approaches, using Hessian-vector products or stochastic gradients only, with Hessian fittings across various geometric settings (the Euclidean space, the manifold of symmetric positive definite (SPD) matrices and a variety of Lie groups). The most intriguing finding is that the Hessian fitting problem is strongly convex under mild conditions in certain general Lie groups. This result turns the Hessian fitting into a well-behaved Lie group optimization problem and facilitates the designs of highly efficient and elegant Lie group sparse preconditioner fitting methods for large-scale stochastic optimizations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases</title>
<link>https://arxiv.org/abs/2403.16776</link>
<guid>https://arxiv.org/abs/2403.16776</guid>
<content:encoded><![CDATA[
arXiv:2403.16776v2 Announce Type: replace-cross 
Abstract: Anatomical atlases are widely used for population studies and analysis. Conditional atlases target a specific sub-population defined via certain conditions, such as demographics or pathologies, and allow for the investigation of fine-grained anatomical differences like morphological changes associated with ageing or disease. Existing approaches use either registration-based methods that are often unable to handle large anatomical variations or generative adversarial models, which are challenging to train since they can suffer from training instabilities. Instead of generating atlases directly in as intensities, we propose using latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. Our approach ensures structural integrity, enhances interpretability and avoids hallucinations that may arise during direct image synthesis by generating this deformation field and regularising it using a neighbourhood of images. We compare our method to several state-of-the-art atlas generation methods using brain MR images from the UK Biobank. Our method generates highly realistic atlases with smooth transformations and high anatomical fidelity, outperforming existing baselines. We demonstrate the quality of these atlases through comprehensive evaluations, including quantitative metrics for anatomical accuracy, perceptual similarity, and qualitative analyses displaying the consistency and realism of the generated atlases.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query Performance Prediction using Relevance Judgments Generated by Large Language Models</title>
<link>https://arxiv.org/abs/2404.01012</link>
<guid>https://arxiv.org/abs/2404.01012</guid>
<content:encoded><![CDATA[
arXiv:2404.01012v3 Announce Type: replace-cross 
Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of predicting the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgments as pseudo-labels. This also allows us to interpret predicted IR evaluation measures, and identify, track and rectify errors in generated relevance judgments to improve QPP quality. We predict an item's relevance by using open-source large language models (LLMs) to ensure scientific reproducibility. We face two main challenges: (i) excessive computational costs of judging an entire corpus for predicting a metric considering recall, and (ii) limited performance in prompting open-source LLMs in a zero-/few-shot manner. To solve the challenges, we devise an approximation strategy to predict an IR measure considering recall and propose to fine-tune open-source LLMs using human-labeled relevance judgments. Experiments on the TREC 2019 to 2022 deep learning tracks and CAsT-19 and 20 datasets show that QPP-GenRE achieves state-of-the-art QPP quality for both lexical and neural rankers.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constructing a BPE Tokenization DFA</title>
<link>https://arxiv.org/abs/2405.07671</link>
<guid>https://arxiv.org/abs/2405.07671</guid>
<content:encoded><![CDATA[
arXiv:2405.07671v2 Announce Type: replace-cross 
Abstract: Many natural language processing systems operate over tokenizations of text to address the open-vocabulary problem. In this paper, we give and analyze an algorithm for the efficient construction of deterministic finite automata (DFA) designed to operate directly on tokenizations produced by the popular byte pair encoding (BPE) technique. This makes it possible to apply many existing techniques and algorithms to the tokenized case, such as pattern matching, equivalence checking of tokenization dictionaries, and composing tokenized languages in various ways. The construction preserves some key properties of the automaton, and we use this to establish asymptotic bounds on the state complexity of the automata that result. Finally, we demonstrate how to construct an input-deterministic (subsequential) string-to-string transducer which precisely describes the relationship between strings and their correct tokenizations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalarisation-based risk concepts for robust multi-objective optimisation</title>
<link>https://arxiv.org/abs/2405.10221</link>
<guid>https://arxiv.org/abs/2405.10221</guid>
<content:encoded><![CDATA[
arXiv:2405.10221v4 Announce Type: replace-cross 
Abstract: Robust optimisation is a well-established framework for optimising functions in the presence of uncertainty. The inherent goal of this problem is to identify a collection of inputs whose outputs are both desirable for the decision maker, whilst also being robust to the underlying uncertainties in the problem. In this work, we study the multi-objective case of this problem. We identify that the majority of all robust multi-objective algorithms rely on two key operations: robustification and scalarisation. Robustification refers to the strategy that is used to account for the uncertainty in the problem. Scalarisation refers to the procedure that is used to encode the relative importance of each objective to a scalar-valued reward. As these operations are not necessarily commutative, the order that they are performed in has an impact on the resulting solutions that are identified and the final decisions that are made. The purpose of this work is to give a thorough exposition on the effects of these different orderings and in particular highlight when one should opt for one ordering over the other. As part of our analysis, we showcase how many existing risk concepts can be integrated into the specification and solution of a robust multi-objective optimisation problem. Besides this, we also demonstrate how one can principally define the notion of a robust Pareto front and a robust performance metric based on our ``robustify and scalarise'' methodology. To illustrate the efficacy of these new ideas, we present two insightful case studies which are based on real-world data sets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomized Midpoint Method for Log-Concave Sampling under Constraints</title>
<link>https://arxiv.org/abs/2405.15379</link>
<guid>https://arxiv.org/abs/2405.15379</guid>
<content:encoded><![CDATA[
arXiv:2405.15379v2 Announce Type: replace-cross 
Abstract: In this paper, we study the problem of sampling from log-concave distributions supported on convex, compact sets, with a particular focus on the randomized midpoint discretization of both vanilla and kinetic Langevin diffusions in this constrained setting. We propose a unified proximal framework for handling constraints via a broad class of projection operators, including Euclidean, Bregman, and Gauge projections. Within this framework, we establish non-asymptotic bounds in both $\mathcal{W}_1$ and $\mathcal{W}_2$ distances, providing precise complexity guarantees and performance comparisons. In addition, our analysis leads to sharper convergence guarantees for both vanilla and kinetic Langevin Monte Carlo under constraints, improving upon existing theoretical results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetries in Overparametrized Neural Networks: A Mean-Field View</title>
<link>https://arxiv.org/abs/2405.19995</link>
<guid>https://arxiv.org/abs/2405.19995</guid>
<content:encoded><![CDATA[
arXiv:2405.19995v3 Announce Type: replace-cross 
Abstract: We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under data symmetric in law wrt the action of a general compact group $G$. We consider for this a class of generalized shallow NNs given by an ensemble of $N$ multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA). We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to $G$-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking $N\to\infty$ and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI laws and minimizes therein the population risk. We also give a counterexample to the general attainability of an optimum over SI laws. Despite this, quite remarkably, we show that the set of SI laws is also preserved by the MF dynamics even when freely trained. This sharply contrasts the finite-$N$ setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as $N$ gets larger in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes. We last deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parrot: Multilingual Visual Instruction Tuning</title>
<link>https://arxiv.org/abs/2406.02539</link>
<guid>https://arxiv.org/abs/2406.02539</guid>
<content:encoded><![CDATA[
arXiv:2406.02539v3 Announce Type: replace-cross 
Abstract: The rapid development of Multimodal Large Language Models (MLLMs), such as GPT-4o, marks a significant step toward artificial general intelligence. Existing methods typically align vision encoders with LLMs via supervised fine-tuning (SFT), but this often deteriorates their ability to handle multiple languages as training progresses. We empirically observe that imbalanced SFT datasets, largely English-centric, degrade performance on non-English languages due to the failure in multilingual token alignment. To address this, we propose PARROT, a novel approach that leverages textual guidance for visual token alignment at the language level. PARROT conditions visual tokens on diverse language inputs and uses Mixture-of-Experts (MoE) to align multilingual tokens. By computing cross-attention between initial visual features and textual embeddings, we select the most relevant experts, converting visual tokens into language-specific representations. Additionally, we introduce the Massive Multilingual Multimodal Benchmark (MMMB), a new benchmark comprising 6 languages, 15 categories, and 12,000 questions, to assess multilingual capabilities. PARROT achieves state-of-the-art performance on both the multilingual benchmarks and a wide range of multimodal tasks. Code and dataset are available at: https://github.com/AIDC-AI/Parrot
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operator-Informed Score Matching for Markov Diffusion Models</title>
<link>https://arxiv.org/abs/2406.09084</link>
<guid>https://arxiv.org/abs/2406.09084</guid>
<content:encoded><![CDATA[
arXiv:2406.09084v2 Announce Type: replace-cross 
Abstract: Diffusion models are typically trained using score matching, a learning objective agnostic to the underlying noising process that guides the model. This paper argues that Markov noising processes enjoy an advantage over alternatives, as the Markov operators that govern the noising process are well-understood. Specifically, by leveraging the spectral decomposition of the infinitesimal generator of the Markov noising process, we obtain parametric estimates of the score functions simultaneously for all marginal distributions, using only sample averages with respect to the data distribution. The resulting operator-informed score matching provides both a standalone approach to sample generation for low-dimensional distributions, as well as a recipe for better informed neural score estimators in high-dimensional settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations</title>
<link>https://arxiv.org/abs/2406.16833</link>
<guid>https://arxiv.org/abs/2406.16833</guid>
<content:encoded><![CDATA[
arXiv:2406.16833v2 Announce Type: replace-cross 
Abstract: Analyzing user opinion changes in long conversation threads is extremely critical for applications like enhanced personalization, market research, political campaigns, customer service, targeted advertising, and content moderation. Unfortunately, previous studies on stance and dogmatism in user conversations have focused on training models using datasets annotated at the post level, treating each post as independent and randomly sampling posts from conversation threads. Hence, first, we build a dataset for studying user opinion fluctuations in 764 long multi-user Reddit conversation threads, called USDC. USDC contains annotations for 2 tasks: i) User Stance classification, which involves labeling a user's stance in a post within a conversation on a five-point scale; ii) User Dogmatism classification, which involves labeling a user's overall opinion in the conversation on a four-point scale. Besides being time-consuming and costly, manual annotations for USDC are challenging because: 1) Conversation threads could be very long, increasing the chances of noisy annotations; and 2) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Hence, we leverage majority voting on zero-shot, one-shot, and few-shot annotations from Mistral Large and GPT-4 to automate the annotation process. Human annotations on 200 test conversations achieved inter-annotator agreement scores of 0.49 for stance and 0.50 for dogmatism with these LLM annotations, indicating a reasonable level of consistency between human and LLM annotations. USDC is then used to finetune and instruction-tune multiple deployable small language models like LLaMA, Falcon and Vicuna for the stance and dogmatism classification tasks. We make the code and dataset publicly available [https://github.com/mounikamarreddy/USDC].
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>We Need Variations in Speech Generation: Sub-center Modelling for Speaker Embeddings</title>
<link>https://arxiv.org/abs/2407.04291</link>
<guid>https://arxiv.org/abs/2407.04291</guid>
<content:encoded><![CDATA[
arXiv:2407.04291v2 Announce Type: replace-cross 
Abstract: Modeling the rich prosodic variations inherent in human speech is essential for generating natural-sounding speech. While speaker embeddings are commonly used as conditioning inputs in personalized speech generation, they are typically optimized for speaker recognition, which encourages the loss of intra-speaker variation. This strategy makes them suboptimal for speech generation in terms of modeling the rich variations at the output speech distribution. In this work, we propose a novel speaker embedding network that employs multiple sub-centers per speaker class during training, instead of a single center as in conventional approaches. This sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance. We demonstrate the effectiveness of the proposed embeddings on a voice conversion task, showing improved naturalness and prosodic expressiveness in the synthesized speech.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Anomaly Detection Using Diffusion Trend Analysis for Display Inspection</title>
<link>https://arxiv.org/abs/2407.09578</link>
<guid>https://arxiv.org/abs/2407.09578</guid>
<content:encoded><![CDATA[
arXiv:2407.09578v2 Announce Type: replace-cross 
Abstract: Reconstruction-based anomaly detection via denoising diffusion model has limitations in determining appropriate noise parameters that can degrade anomalies while preserving normal characteristics. Also, normal regions can fluctuate considerably during reconstruction, resulting in false detection. In this paper, we propose a method to detect anomalies by analysis of reconstruction trend depending on the degree of degradation, effectively solving the both problems that impede practical application in display inspection.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual Inversion</title>
<link>https://arxiv.org/abs/2408.00458</link>
<guid>https://arxiv.org/abs/2408.00458</guid>
<content:encoded><![CDATA[
arXiv:2408.00458v2 Announce Type: replace-cross 
Abstract: Recent years have seen a tremendous improvement in the quality of video generation and editing approaches. While several techniques focus on editing appearance, few address motion. Current approaches using text, trajectories, or bounding boxes are limited to simple motions, so we specify motions with a single motion reference video instead. We further propose to use a pre-trained image-to-video model rather than a text-to-video model. This approach allows us to preserve the exact appearance and position of a target object or scene and helps disentangle appearance from motion. Our method, called motion-textual inversion, leverages our observation that image-to-video models extract appearance mainly from the (latent) image input, while the text/image embedding injected via cross-attention predominantly controls motion. We thus represent motion using text/image embedding tokens. By operating on an inflated motion-text embedding containing multiple text/image embedding tokens per frame, we achieve a high temporal motion granularity. Once optimized on the motion reference video, this embedding can be applied to various target images to generate videos with semantically similar motions. Our approach does not require spatial alignment between the motion reference video and target image, generalizes across various domains, and can be applied to various tasks such as full-body and face reenactment, as well as controlling the motion of inanimate objects and the camera. We empirically demonstrate the effectiveness of our method in the semantic video motion transfer task, significantly outperforming existing methods in this context.
  Project website: https://mkansy.github.io/reenact-anything/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KAN we improve on HEP classification tasks? Kolmogorov-Arnold Networks applied to an LHC physics example</title>
<link>https://arxiv.org/abs/2408.02743</link>
<guid>https://arxiv.org/abs/2408.02743</guid>
<content:encoded><![CDATA[
arXiv:2408.02743v2 Announce Type: replace-cross 
Abstract: Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an alternative to multilayer perceptrons, suggesting advantages in performance and interpretability. We study a typical binary event classification task in high-energy physics including high-level features and comment on the performance and interpretability of KANs in this context. Consistent with expectations, we find that the learned activation functions of a one-layer KAN resemble the univariate log-likelihood ratios of the respective input features. In deeper KANs, the activations in the first layer differ from those in the one-layer KAN, which indicates that the deeper KANs learn more complex representations of the data, a pattern commonly observed in other deep-learning architectures. We study KANs with different depths and widths and we compare them to multilayer perceptrons in terms of performance and number of trainable parameters. For the chosen classification task, we do not find that KANs are more parameter efficient. However, small KANs may offer advantages in terms of interpretability that come at the cost of only a moderate loss in performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variance-Reduced Cascade Q-learning: Algorithms and Sample Complexity</title>
<link>https://arxiv.org/abs/2408.06544</link>
<guid>https://arxiv.org/abs/2408.06544</guid>
<content:encoded><![CDATA[
arXiv:2408.06544v2 Announce Type: replace-cross 
Abstract: We study the problem of estimating the optimal Q-function of $\gamma$-discounted Markov decision processes (MDPs) under the synchronous setting, where independent samples for all state-action pairs are drawn from a generative model at each iteration. We introduce and analyze a novel model-free algorithm called Variance-Reduced Cascade Q-learning (VRCQ). VRCQ comprises two key building blocks: (i) the established direct variance reduction technique and (ii) our proposed variance reduction scheme, Cascade Q-learning. By leveraging these techniques, VRCQ provides superior guarantees in the $\ell_\infty$-norm compared with the existing model-free stochastic approximation-type algorithms. Specifically, we demonstrate that VRCQ is minimax optimal. Additionally, when the action set is a singleton (so that the Q-learning problem reduces to policy evaluation), it achieves non-asymptotic instance optimality while requiring the minimum number of samples theoretically possible. Our theoretical results and their practical implications are supported by numerical experiments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling</title>
<link>https://arxiv.org/abs/2408.08696</link>
<guid>https://arxiv.org/abs/2408.08696</guid>
<content:encoded><![CDATA[
arXiv:2408.08696v3 Announce Type: replace-cross 
Abstract: Massive parameters of LLMs have made inference latency a fundamental bottleneck. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm. Some methods rely on additional architectures to guess draft tokens, which need extra training before use. Alternatively, retrieval-based training-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. It stores candidate tokens in an adjacency matrix and employs a breadth-first-search (BFS)-like algorithm to construct a draft tree, which is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\% and even a widely recognized training method by 25\%.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>(Un)supervised Learning of Maximal Lyapunov Functions</title>
<link>https://arxiv.org/abs/2408.17246</link>
<guid>https://arxiv.org/abs/2408.17246</guid>
<content:encoded><![CDATA[
arXiv:2408.17246v2 Announce Type: replace-cross 
Abstract: In this paper, we address the problem of discovering maximal Lyapunov functions, as a means of determining the region of attraction of a dynamical system. To this end, we design a novel neural network architecture, which we prove to be a universal approximator of (maximal) Lyapunov functions. The architecture combines a local quadratic approximation with the output of a neural network, which models global higher-order terms in the Taylor expansion. We formulate the problem of training the Lyapunov function as an unsupervised optimization problem with dynamical constraints, which can be solved leveraging techniques from physics-informed learning. We propose and analyze a tailored training algorithm, based on the primal-dual algorithm, that can efficiently solve the problem. Additionally, we show how the learning problem formulation can be adapted to integrate data, when available. We apply the proposed approach to different classes of systems, showing that it matches or outperforms state-of-the-art alternatives in the accuracy of the approximated regions of attraction.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework</title>
<link>https://arxiv.org/abs/2409.10289</link>
<guid>https://arxiv.org/abs/2409.10289</guid>
<content:encoded><![CDATA[
arXiv:2409.10289v3 Announce Type: replace-cross 
Abstract: Empathetic response generation necessitates the integration of emotional and intentional dynamics to foster meaningful interactions. Existing research either neglects the intricate interplay between emotion and intent, leading to suboptimal controllability of empathy, or resorts to large language models (LLMs), which incur significant computational overhead. In this paper, we introduce ReflectDiffu, a lightweight and comprehensive framework for empathetic response generation. This framework incorporates emotion contagion to augment emotional expressiveness and employs an emotion-reasoning mask to pinpoint critical emotional elements. Additionally, it integrates intent mimicry within reinforcement learning for refinement during diffusion. By harnessing an intent twice reflect mechanism of Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional decision-making into precise intent actions, thereby addressing empathetic response misalignments stemming from emotional misrecognition. Through reflection, the framework maps emotional states to intents, markedly enhancing both response empathy and flexibility. Comprehensive experiments reveal that ReflectDiffu outperforms existing models regarding relevance, controllability, and informativeness, achieving state-of-the-art results in both automatic and human evaluations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-asymptotic convergence analysis of the stochastic gradient Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with applications to training of ReLU neural networks</title>
<link>https://arxiv.org/abs/2409.17107</link>
<guid>https://arxiv.org/abs/2409.17107</guid>
<content:encoded><![CDATA[
arXiv:2409.17107v2 Announce Type: replace-cross 
Abstract: In this paper, we provide a non-asymptotic analysis of the convergence of the stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a target measure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to the existing literature on SGHMC, we allow its stochastic gradient to be discontinuous. This allows us to provide explicit upper bounds, which can be controlled to be arbitrarily small, for the expected excess risk of non-convex stochastic optimization problems with discontinuous stochastic gradients, including, among others, the training of neural networks with ReLU activation function. To illustrate the applicability of our main results, we consider numerical experiments on quantile estimation and on several optimization problems involving ReLU neural networks relevant in finance and artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement</title>
<link>https://arxiv.org/abs/2410.03124</link>
<guid>https://arxiv.org/abs/2410.03124</guid>
<content:encoded><![CDATA[
arXiv:2410.03124v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved great success across diverse tasks, and fine-tuning is sometimes needed to further enhance generation quality. Most existing methods rely on human supervision or parameter retraining, both of which are costly in terms of data collection and computational resources. To handle these challenges, a direct solution is to generate ``high-confidence'' data from unsupervised downstream tasks and use them for in-context prompting or prompt optimization to refine the pseudo-supervision. However, relying solely on such data may lead to overfitting. In this paper, we leverage the in-context learning (ICL) abilities of LLMs and propose a novel approach, pseudo-supervised demonstrations aligned prompt optimization (PAPO) algorithm, which jointly refines both the prompt and the overall pseudo-supervision. The proposed learning objective ensures that the optimized prompt guides the LLM to generate consistent responses for a given input when pseudo-supervised data from the downstream task are used as demonstrations, enabling refinement over the entire pseudo-supervision. The prompt is optimized by translating gradient signals into textual critiques, which serve as feedback to iteratively refine the prompt and model responses. Theoretical analysis in a simplified classification setting shows that the refined pseudo-supervision exhibits a geometric clustering structure, helping to mitigate overfitting. Experiments on question answering, natural language inference benchmarks, and a real-world molecule optimization task, show the effectiveness of the proposed algorithm.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SITCOM: Step-wise Triple-Consistent Diffusion Sampling for Inverse Problems</title>
<link>https://arxiv.org/abs/2410.04479</link>
<guid>https://arxiv.org/abs/2410.04479</guid>
<content:encoded><![CDATA[
arXiv:2410.04479v2 Announce Type: replace-cross 
Abstract: Diffusion models (DMs) are a class of generative models that allow sampling from a distribution learned over a training set. When applied to solving inverse problems, the reverse sampling steps are modified to approximately sample from a measurement-conditioned distribution. However, these modifications may be unsuitable for certain settings (e.g., presence of measurement noise) and non-linear tasks, as they often struggle to correct errors from earlier steps and generally require a large number of optimization and/or sampling steps. To address these challenges, we state three conditions for achieving measurement-consistent diffusion trajectories. Building on these conditions, we propose a new optimization-based sampling method that not only enforces standard data manifold measurement consistency and forward diffusion consistency, as seen in previous studies, but also incorporates our proposed step-wise and network-regularized backward diffusion consistency that maintains a diffusion trajectory by optimizing over the input of the pre-trained model at every sampling step. By enforcing these conditions (implicitly or explicitly), our sampler requires significantly fewer reverse steps. Therefore, we refer to our method as Step-wise Triple-Consistent Sampling (SITCOM). Compared to SOTA baselines, our experiments across several linear and non-linear tasks (with natural and medical images) demonstrate that SITCOM achieves competitive or superior results in terms of standard similarity metrics and run-time.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs</title>
<link>https://arxiv.org/abs/2410.06704</link>
<guid>https://arxiv.org/abs/2410.06704</guid>
<content:encoded><![CDATA[
arXiv:2410.06704v2 Announce Type: replace-cross 
Abstract: In this work, we introduce PII-Scope, a comprehensive benchmark designed to evaluate state-of-the-art methodologies for PII extraction attacks targeting LLMs across diverse threat settings. Our study provides a deeper understanding of these attacks by uncovering several hyperparameters (e.g., demonstration selection) crucial to their effectiveness. Building on this understanding, we extend our study to more realistic attack scenarios, exploring PII attacks that employ advanced adversarial strategies, including repeated and diverse querying, and leveraging iterative learning for continual PII extraction. Through extensive experimentation, our results reveal a notable underestimation of PII leakage in existing single-query attacks. In fact, we show that with sophisticated adversarial capabilities and a limited query budget, PII extraction rates can increase by up to fivefold when targeting the pretrained model. Moreover, we evaluate PII leakage on finetuned models, showing that they are more vulnerable to leakage than pretrained models. Overall, our work establishes a rigorous empirical benchmark for PII extraction attacks in realistic threat scenarios and provides a strong foundation for developing effective mitigation strategies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stuffed Mamba: Oversized States Lead to the Inability to Forget</title>
<link>https://arxiv.org/abs/2410.07145</link>
<guid>https://arxiv.org/abs/2410.07145</guid>
<content:encoded><![CDATA[
arXiv:2410.07145v2 Announce Type: replace-cross 
Abstract: Recent advancements in recurrent architectures, such as Mamba and RWKV, have showcased strong language capabilities. Unlike transformer-based models, these architectures encode all contextual information into a fixed-size state, leading to great inference efficiency. However, this approach can cause information interference, where different token data conflicts, resulting in performance degradation and incoherent outputs beyond a certain context length. To prevent this, most RNNs incorporate mechanisms designed to "forget" earlier tokens. In this paper, we reveal that Mamba-based models struggle to effectively forget earlier tokens even with built-in forgetting mechanisms. We demonstrate that this issue stems from training on contexts that are too short for the state size, enabling the model to perform well without needing to learn how to forget. Then, we show that the minimum training length required for the model to learn forgetting scales linearly with the state size, and the maximum context length for accurate retrieval of a 5-digit passkey scales exponentially with the state size, indicating that the model retains some information beyond the point where forgetting begins. These findings highlight a critical limitation in current RNN architectures and provide valuable insights for improving long-context modeling. Our work suggests that future RNN designs must account for the interplay between state size, training length, and forgetting mechanisms to achieve robust performance in long-context tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery</title>
<link>https://arxiv.org/abs/2410.09032</link>
<guid>https://arxiv.org/abs/2410.09032</guid>
<content:encoded><![CDATA[
arXiv:2410.09032v3 Announce Type: replace-cross 
Abstract: Millions of abandoned oil and gas wells are scattered across the world, leaching methane into the atmosphere and toxic compounds into the groundwater. Many of these locations are unknown, preventing the wells from being plugged and their polluting effects averted. Remote sensing is a relatively unexplored tool for pinpointing abandoned wells at scale. We introduce the first large-scale benchmark dataset for this problem, leveraging medium-resolution multi-spectral satellite imagery from Planet Labs. Our curated dataset comprises over 213,000 wells (abandoned, suspended, and active) from Alberta, a region with especially high well density, sourced from the Alberta Energy Regulator and verified by domain experts. We evaluate baseline algorithms for well detection and segmentation, showing the promise of computer vision approaches but also significant room for improvement.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System</title>
<link>https://arxiv.org/abs/2410.09403</link>
<guid>https://arxiv.org/abs/2410.09403</guid>
<content:encoded><![CDATA[
arXiv:2410.09403v3 Announce Type: replace-cross 
Abstract: The rapid advancement of scientific progress requires innovative tools that can accelerate knowledge discovery. Although recent AI methods, particularly large language models (LLMs), have shown promise in tasks such as hypothesis generation and experimental design, they fall short of replicating the collaborative nature of real-world scientific practices, where diverse experts work together in teams to tackle complex problems. To address the limitations, we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci), designed to mimic the teamwork inherent in scientific research. VirSci organizes a team of agents to collaboratively generate, evaluate, and refine research ideas. Through comprehensive experiments, we demonstrate that this multi-agent approach outperforms the state-of-the-art method in producing novel scientific ideas. We further investigate the collaboration mechanisms that contribute to its tendency to produce ideas with higher novelty, offering valuable insights to guide future research and illuminating pathways toward building a robust system for autonomous scientific discovery. The code is available at https://github.com/open-sciencelab/Virtual-Scientists.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Solver Selection for Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2410.09693</link>
<guid>https://arxiv.org/abs/2410.09693</guid>
<content:encoded><![CDATA[
arXiv:2410.09693v2 Announce Type: replace-cross 
Abstract: Machine learning has increasingly been employed to solve NP-hard combinatorial optimization problems, resulting in the emergence of neural solvers that demonstrate remarkable performance, even with minimal domain-specific knowledge. To date, the community has created numerous open-source neural solvers with distinct motivations and inductive biases. While considerable efforts are devoted to designing powerful single solvers, our findings reveal that existing solvers typically demonstrate complementary performance across different problem instances. This suggests that significant improvements could be achieved through effective coordination of neural solvers at the instance level. In this work, we propose the first general framework to coordinate the neural solvers, which involves feature extraction, selection model, and selection strategy, aiming to allocate each instance to the most suitable solvers. To instantiate, we collect several typical neural solvers with state-of-the-art performance as alternatives, and explore various methods for each component of the framework. We evaluated our framework on two extensively studied combinatorial optimization problems, Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP). Experimental results show that the proposed framework can effectively distribute instances and the resulting composite solver can achieve significantly better performance (e.g., reduce the optimality gap by 0.88\% on TSPLIB and 0.71\% on CVRPLIB) than the best individual neural solver with little extra time cost.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inductive Gradient Adjustment For Spectral Bias In Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2410.13271</link>
<guid>https://arxiv.org/abs/2410.13271</guid>
<content:encoded><![CDATA[
arXiv:2410.13271v2 Announce Type: replace-cross 
Abstract: Implicit Neural Representations (INRs), as a versatile representation paradigm, have achieved success in various computer vision tasks. Due to the spectral bias of the vanilla multi-layer perceptrons (MLPs), existing methods focus on designing MLPs with sophisticated architectures or repurposing training techniques for highly accurate INRs. In this paper, we delve into the linear dynamics model of MLPs and theoretically identify the empirical Neural Tangent Kernel (eNTK) matrix as a reliable link between spectral bias and training dynamics. Based on this insight, we propose a practical Inductive Gradient Adjustment (IGA) method, which could purposefully improve the spectral bias via inductive generalization of eNTK-based gradient transformation matrix. Theoretical and empirical analyses validate impacts of IGA on spectral bias. Further, we evaluate our method on different INRs tasks with various INR architectures and compare to existing training techniques. The superior and consistent improvements clearly validate the advantage of our IGA. Armed with our gradient adjustment method, better INRs with more enhanced texture details and sharpened edges can be learned from data by tailored impacts on spectral bias.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.14972</link>
<guid>https://arxiv.org/abs/2410.14972</guid>
<content:encoded><![CDATA[
arXiv:2410.14972v2 Announce Type: replace-cross 
Abstract: Visual deep reinforcement learning (RL) enables robots to acquire skills from visual input for unstructured tasks. However, current algorithms suffer from low sample efficiency, limiting their practical applicability. In this work, we present MENTOR, a method that improves both the architecture and optimization of RL agents. Specifically, MENTOR replaces the standard multi-layer perceptron (MLP) with a mixture-of-experts (MoE) backbone and introduces a task-oriented perturbation mechanism. MENTOR outperforms state-of-the-art methods across three simulation benchmarks and achieves an average of 83% success rate on three challenging real-world robotic manipulation tasks, significantly surpassing the 32% success rate of the strongest existing model-free visual RL algorithm. These results underscore the importance of sample efficiency in advancing visual RL for real-world robotics. Experimental videos are available at https://suninghuang19.github.io/mentor_page/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving $\tilde{\mathcal{O}}(1/N)$ Optimality Gap in Restless Bandits through Gaussian Approximation</title>
<link>https://arxiv.org/abs/2410.15003</link>
<guid>https://arxiv.org/abs/2410.15003</guid>
<content:encoded><![CDATA[
arXiv:2410.15003v2 Announce Type: replace-cross 
Abstract: We study the finite-horizon Restless Multi-Armed Bandit (RMAB) problem with $N$ homogeneous arms. Prior work has shown that when an RMAB satisfies a non-degeneracy condition, Linear-Programming-based (LP-based) policies derived from the fluid approximation, which captures the mean dynamics of the system, achieve an exponentially small optimality gap. However, it is common for RMABs to be degenerate, in which case LP-based policies can result in a $\Theta(1/\sqrt{N})$ optimality gap per arm. In this paper, we propose a novel Stochastic-Programming-based (SP-based) policy that, under a uniqueness assumption, achieves an $\tilde{\mathcal{O}}(1/N)$ optimality gap for degenerate RMABs. Our approach is based on the construction of a Gaussian stochastic system that captures not only the mean but also the variance of the RMAB dynamics, resulting in a more accurate approximation than the fluid approximation. We then solve a stochastic program for this system to obtain our policy. This is the first result to establish an $\tilde{\mathcal{O}}(1/N)$ optimality gap for degenerate RMABs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Dataset Copyright Auditing in Machine Learning Systems</title>
<link>https://arxiv.org/abs/2410.16618</link>
<guid>https://arxiv.org/abs/2410.16618</guid>
<content:encoded><![CDATA[
arXiv:2410.16618v2 Announce Type: replace-cross 
Abstract: As the implementation of machine learning (ML) systems becomes more widespread, especially with the introduction of larger ML models, we perceive a spring demand for massive data. However, it inevitably causes infringement and misuse problems with the data, such as using unauthorized online artworks or face images to train ML models. To address this problem, many efforts have been made to audit the copyright of the model training dataset. However, existing solutions vary in auditing assumptions and capabilities, making it difficult to compare their strengths and weaknesses. In addition, robustness evaluations usually consider only part of the ML pipeline and hardly reflect the performance of algorithms in real-world ML applications. Thus, it is essential to take a practical deployment perspective on the current dataset copyright auditing tools, examining their effectiveness and limitations. Concretely, we categorize dataset copyright auditing research into two prominent strands: intrusive methods and non-intrusive methods, depending on whether they require modifications to the original dataset. Then, we break down the intrusive methods into different watermark injection options and examine the non-intrusive methods using various fingerprints. To summarize our results, we offer detailed reference tables, highlight key points, and pinpoint unresolved issues in the current literature. By combining the pipeline in ML systems and analyzing previous studies, we highlight several future directions to make auditing tools more suitable for real-world copyright protection requirements.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMScan: Causal Scan for LLM Misbehavior Detection</title>
<link>https://arxiv.org/abs/2410.16638</link>
<guid>https://arxiv.org/abs/2410.16638</guid>
<content:encoded><![CDATA[
arXiv:2410.16638v4 Announce Type: replace-cross 
Abstract: Despite the success of Large Language Models (LLMs) across various fields, their potential to generate untruthful, biased and harmful responses poses significant risks, particularly in critical applications. This highlights the urgent need for systematic methods to detect and prevent such misbehavior. While existing approaches target specific issues such as harmful responses, this work introduces LLMScan, an innovative LLM monitoring technique based on causality analysis, offering a comprehensive solution. LLMScan systematically monitors the inner workings of an LLM through the lens of causal inference, operating on the premise that the LLM's `brain' behaves differently when misbehaving. By analyzing the causal contributions of the LLM's input tokens and transformer layers, LLMScan effectively detects misbehavior. Extensive experiments across various tasks and models reveal clear distinctions in the causal distributions between normal behavior and misbehavior, enabling the development of accurate, lightweight detectors for a variety of misbehavior detection tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SaVe-TAG: Semantic-aware Vicinal Risk Minimization for Long-Tailed Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2410.16882</link>
<guid>https://arxiv.org/abs/2410.16882</guid>
<content:encoded><![CDATA[
arXiv:2410.16882v3 Announce Type: replace-cross 
Abstract: Real-world graph data often follows long-tailed distributions, making it difficult for Graph Neural Networks (GNNs) to generalize well across both head and tail classes. Recent advances in Vicinal Risk Minimization (VRM) have shown promise in mitigating class imbalance with numeric interpolation; however, existing approaches largely rely on embedding-space arithmetic, which fails to capture the rich semantics inherent in text-attributed graphs. In this work, we propose our method, SaVe-TAG (Semantic-aware Vicinal Risk Minimization for Long-Tailed Text-Attributed Graphs), a novel VRM framework that leverages Large Language Models (LLMs) to perform text-level interpolation, generating on-manifold, boundary-enriching synthetic samples for minority classes. To mitigate the risk of noisy generation, we introduce a confidence-based edge assignment mechanism that uses graph topology as a natural filter to ensure structural consistency. We provide theoretical justification for our method and conduct extensive experiments on benchmark datasets, showing that our approach consistently outperforms both numeric interpolation and prior long-tailed node classification baselines. Our results highlight the importance of integrating semantic and structural signals for balanced and effective learning on text-attributed graphs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models</title>
<link>https://arxiv.org/abs/2411.02083</link>
<guid>https://arxiv.org/abs/2411.02083</guid>
<content:encoded><![CDATA[
arXiv:2411.02083v2 Announce Type: replace-cross 
Abstract: While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the Cross Entropy loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the Lp norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the Cross Entropy objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope that this work can inspire LLM developers to improve their pretraining objectives. The code is available via: https://tum-ai.github.io/number-token-loss/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P$^2$ Law: Scaling Law for Post-Training After Model Pruning</title>
<link>https://arxiv.org/abs/2411.10272</link>
<guid>https://arxiv.org/abs/2411.10272</guid>
<content:encoded><![CDATA[
arXiv:2411.10272v3 Announce Type: replace-cross 
Abstract: Pruning has become a widely adopted technique for reducing the hardware requirements of large language models (LLMs). To recover model performance after pruning, post-training is commonly employed to mitigate the resulting performance degradation. While post-training benefits from larger datasets, once the dataset size is already substantial, increasing the training data provides only limited performance gains. To balance post-training cost and model performance, it is necessary to explore the optimal amount of post-training data.Through extensive experiments on the Llama-3 and Qwen-2.5 series models, pruned using various common pruning methods, we uncover the scaling \textbf{Law} for \textbf{P}ost-training after model \textbf{P}runing, referred to as the P$^2$ Law.This law identifies four key factors for predicting the pruned model's post-training loss: the model size before pruning, the number of post-training tokens, the pruning rate, and the model's loss before pruning. Moreover, P$^2$ Law can generalize to larger dataset sizes, larger model sizes, and higher pruning rates, offering valuable insights for the post-training of pruned LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust multi-coil MRI reconstruction via self-supervised denoising</title>
<link>https://arxiv.org/abs/2411.12919</link>
<guid>https://arxiv.org/abs/2411.12919</guid>
<content:encoded><![CDATA[
arXiv:2411.12919v4 Announce Type: replace-cross 
Abstract: We study the effect of incorporating self-supervised denoising as a pre-processing step for training deep learning (DL) based reconstruction methods on data corrupted by Gaussian noise. K-space data employed for training are typically multi-coil and inherently noisy. Although DL-based reconstruction methods trained on fully sampled data can enable high reconstruction quality, obtaining large, noise-free datasets is impractical. We leverage Generalized Stein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based reconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based Deep Learning (MoDL). We evaluate the impact of denoising on the performance of these DL-based methods in solving accelerated multi-coil magnetic resonance imaging (MRI) reconstruction. The experiments were carried out on T2-weighted brain and fat-suppressed proton-density knee scans. We observed that self-supervised denoising enhances the quality and efficiency of MRI reconstructions across various scenarios. Specifically, employing denoised images rather than noisy counterparts when training DL networks results in lower normalized root mean squared error (NRMSE), higher structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR levels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB, 14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising is an essential pre-processing technique capable of improving the efficacy of DL-based MRI reconstruction methods under diverse conditions. By refining the quality of input data, denoising enables training more effective DL networks, potentially bypassing the need for noise-free reference MRI scans.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?</title>
<link>https://arxiv.org/abs/2411.15821</link>
<guid>https://arxiv.org/abs/2411.15821</guid>
<content:encoded><![CDATA[
arXiv:2411.15821v3 Announce Type: replace-cross 
Abstract: This study investigates the relative impact of training data quality versus quantity on the performance of small language models (SLMs), utilizing the TinyStories dataset for empirical analysis. Analysis of dataset variations with respect to size (25% and 50% of the original size) and duplication (controlled rates of 25%, 50%, 75%, and 100%) were performed. Model performance was evaluated based on the validation loss, accuracy, and perplexity metrics. Results indicate training data quality plays a more significant role in the overall performance of SLMs, especially given scale of this experiment. Minimal duplication positively impacted model accuracy (+0.87% increase in accuracy at 25% duplication) without significantly increasing perplexity (+0.52% increase going from 0% to 25% duplication) but excessive duplication led to pronounced performance degradation (-40% drop in accuracy at 100% duplication). The implications of this exploration extend beyond just model performance; training large-scale models imposes significant financial and computational burdens, which can be prohibitive for organizations, individuals, and the public at large, especially in developing countries. Additionally, the energy consumption associated with large-scale training raises environmental concerns. Understanding the relative importance of data quality versus quantity could democratize AI technology, making advanced models more accessible and sustainable for all.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Annealed Importance Sampling Bootstrap meets Differentiable Particle Physics</title>
<link>https://arxiv.org/abs/2411.16234</link>
<guid>https://arxiv.org/abs/2411.16234</guid>
<content:encoded><![CDATA[
arXiv:2411.16234v2 Announce Type: replace-cross 
Abstract: High-energy physics requires the generation of large numbers of simulated data samples from complex but analytically tractable distributions called matrix elements. Surrogate models, such as normalizing flows, are gaining popularity for this task due to their computational efficiency. We adopt an approach based on Flow Annealed importance sampling Bootstrap (FAB) that evaluates the differentiable target density during training and helps avoid the costly generation of training data in advance. We show that FAB reaches higher sampling efficiency with fewer target evaluations in high dimensions in comparison to other methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes</title>
<link>https://arxiv.org/abs/2412.00592</link>
<guid>https://arxiv.org/abs/2412.00592</guid>
<content:encoded><![CDATA[
arXiv:2412.00592v3 Announce Type: replace-cross 
Abstract: We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data for autonomous driving. Our framework edits real-world LiDAR scans by introducing new object layouts while preserving the realism of the background environment. Compared to end-to-end frameworks that generate LiDAR point clouds from scratch, LiDAR-EDIT offers users full control over the object layout, including the number, type, and pose of objects, while keeping most of the original real-world background. Our method also provides object labels for the generated data. Compared to novel view synthesis techniques, our framework allows for the creation of counterfactual scenarios with object layouts significantly different from the original real-world scene. LiDAR-EDIT uses spherical voxelization to enforce correct LiDAR projective geometry in the generated point clouds by construction. During object removal and insertion, generative models are employed to fill the unseen background and object parts that were occluded in the original real LiDAR scans. Experimental results demonstrate that our framework produces realistic LiDAR scans with practical value for downstream tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACO: Training-free Sound Prompted Segmentation via Semantically Constrained Audio-visual CO-factorization</title>
<link>https://arxiv.org/abs/2412.01488</link>
<guid>https://arxiv.org/abs/2412.01488</guid>
<content:encoded><![CDATA[
arXiv:2412.01488v3 Announce Type: replace-cross 
Abstract: Large-scale pre-trained audio and image models demonstrate an unprecedented degree of generalization, making them suitable for a wide range of applications. Here, we tackle the specific task of sound-prompted segmentation, aiming to segment image regions corresponding to objects heard in an audio signal. Most existing approaches tackle this problem by fine-tuning pre-trained models or by training additional modules specifically for the task. We adopt a different strategy: we introduce a training-free approach that leverages Non-negative Matrix Factorization (NMF) to co-factorize audio and visual features from pre-trained models so as to reveal shared interpretable concepts. These concepts are passed on to an open-vocabulary segmentation model for precise segmentation maps. By using frozen pre-trained models, our method achieves high generalization and establishes state-of-the-art performance in unsupervised sound-prompted segmentation, significantly surpassing previous unsupervised methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Company Similarity with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2412.02605</link>
<guid>https://arxiv.org/abs/2412.02605</guid>
<content:encoded><![CDATA[
arXiv:2412.02605v3 Announce Type: replace-cross 
Abstract: Determining company similarity is a vital task in finance, underpinning risk management, hedging, and portfolio diversification. Practitioners often rely on sector and industry classifications such as SIC and GICS codes to gauge similarity, the former being used by the U.S. Securities and Exchange Commission (SEC), and the latter widely used by the investment community. Since these classifications lack granularity and need regular updating, using clusters of embeddings of company descriptions has been proposed as a potential alternative, but the lack of interpretability in token embeddings poses a significant barrier to adoption in high-stakes contexts. Sparse Autoencoders (SAEs) have shown promise in enhancing the interpretability of Large Language Models (LLMs) by decomposing Large Language Model (LLM) activations into interpretable features. Moreover, SAEs capture an LLM's internal representation of a company description, as opposed to semantic similarity alone, as is the case with embeddings. We apply SAEs to company descriptions, and obtain meaningful clusters of equities. We benchmark SAE features against SIC-codes, Industry codes, and Embeddings. Our results demonstrate that SAE features surpass sector classifications and embeddings in capturing fundamental company characteristics. This is evidenced by their superior performance in correlating logged monthly returns - a proxy for similarity - and generating higher Sharpe ratios in co-integration trading strategies, which underscores deeper fundamental similarities among companies. Finally, we verify the interpretability of our clusters, and demonstrate that sparse features form simple and interpretable explanations for our clusters.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi</title>
<link>https://arxiv.org/abs/2412.06333</link>
<guid>https://arxiv.org/abs/2412.06333</guid>
<content:encoded><![CDATA[
arXiv:2412.06333v3 Announce Type: replace-cross 
Abstract: The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, partial observability, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of "rules" or principles. Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting an agent's action space using conventions, which act as a sequence of special cooperative actions that span over and include multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play for various number of cooperators within Hanabi.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProcessBench: Identifying Process Errors in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2412.06559</link>
<guid>https://arxiv.org/abs/2412.06559</guid>
<content:encoded><![CDATA[
arXiv:2412.06559v4 Announce Type: replace-cross 
Abstract: As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HARP: Hesitation-Aware Reframing in Transformer Inference Pass</title>
<link>https://arxiv.org/abs/2412.07282</link>
<guid>https://arxiv.org/abs/2412.07282</guid>
<content:encoded><![CDATA[
arXiv:2412.07282v2 Announce Type: replace-cross 
Abstract: This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to "off-the-shelf" Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP provides insights into the potential of adaptive computation for enhancing the performance of Transformer-based language models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity</title>
<link>https://arxiv.org/abs/2412.09856</link>
<guid>https://arxiv.org/abs/2412.09856</guid>
<content:encoded><![CDATA[
arXiv:2412.09856v2 Announce Type: replace-cross 
Abstract: Text-to-video generation enhances content creation but is highly computationally intensive: The computational cost of Diffusion Transformers (DiTs) scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. We propose a Linear-complexity text-to-video Generation (LinGen) framework whose cost scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch. The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation. The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens. The MATE block addresses the adjacency preservation issue of Mamba and improves the consistency of generated videos significantly. Experimental results show that LinGen outperforms DiT (with a 75.6% win rate) in video quality with up to 15$\times$ (11.5$\times$) FLOPs (latency) reduction. Furthermore, both automatic metrics and human evaluation demonstrate our LinGen-4B yields comparable video quality to state-of-the-art models (with a 50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling, respectively). This paves the way to hour-length movie generation and real-time interactive video generation. We provide 68s video generation results and more examples in our project website: https://lineargen.github.io/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models</title>
<link>https://arxiv.org/abs/2412.13328</link>
<guid>https://arxiv.org/abs/2412.13328</guid>
<content:encoded><![CDATA[
arXiv:2412.13328v2 Announce Type: replace-cross 
Abstract: The "state" of State Space Models (SSMs) represents their memory, which fades exponentially over an unbounded span. By contrast, Attention-based models have "eidetic" (i.e., verbatim, or photographic) memory over a finite span (context size). Hybrid architectures combine State Space layers with Attention, but still cannot recall the distant past and can access only the most recent tokens eidetically. Unlike current methods of combining SSM and Attention layers, we allow the state to be allocated based on relevancy rather than recency. In this way, for every new set of query tokens, our models can "eidetically" access tokens from beyond the Attention span of current Hybrid SSMs without requiring extra hardware resources. We introduce a method to expand the memory span of the hybrid state by "reserving" a fraction of the Attention context for tokens retrieved from arbitrarily distant in the past, thus expanding the eidetic memory span of the overall state. We call this reserved fraction of tokens the "expansion span," and the mechanism to retrieve and aggregate it "Span-Expanded Attention" (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose a novel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows efficient adaptation on long spans of tokens. We show that SE-Attn enables us to efficiently adapt pre-trained Hybrid models on sequences of tokens up to 8 times longer than the ones used for pre-training. We show that HyLoRA with SE-Attn is cheaper and more performant than alternatives like LongLoRA when applied to Hybrid models on natural language benchmarks with long-range dependencies, such as PG-19, RULER, and other common natural language downstream tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents</title>
<link>https://arxiv.org/abs/2412.13549</link>
<guid>https://arxiv.org/abs/2412.13549</guid>
<content:encoded><![CDATA[
arXiv:2412.13549v2 Announce Type: replace-cross 
Abstract: Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench, a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Synthesize Text Data without Model Collapse?</title>
<link>https://arxiv.org/abs/2412.14689</link>
<guid>https://arxiv.org/abs/2412.14689</guid>
<content:encoded><![CDATA[
arXiv:2412.14689v2 Announce Type: replace-cross 
Abstract: Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves model performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fricke signs from Maass form Coefficients</title>
<link>https://arxiv.org/abs/2501.02105</link>
<guid>https://arxiv.org/abs/2501.02105</guid>
<content:encoded><![CDATA[
arXiv:2501.02105v2 Announce Type: replace-cross 
Abstract: In this paper, we conduct a data-scientific investigation of Maass forms. We find that averaging the Fourier coefficients of Maass forms with the same Fricke sign reveals patterns analogous to the recently discovered "murmuration" phenomenon, and that these patterns become more pronounced when parity is incorporated as an additional feature. Approximately 43% of the forms in our dataset have an unknown Fricke sign. For the remaining forms, we employ Linear Discriminant Analysis (LDA) to machine learn their Fricke sign, achieving 96% (resp. 94%) accuracy for forms with even (resp. odd) parity. We apply the trained LDA model to forms with unknown Fricke signs to make predictions. The average values based on the predicted Fricke signs are computed and compared to those for forms with known signs to verify the reasonableness of the predictions. Additionally, a subset of these predictions is evaluated against heuristic guesses provided by Hejhal's algorithm, showing a match approximately 95% of the time. We also use neural networks to obtain results comparable to those from the LDA model.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use</title>
<link>https://arxiv.org/abs/2501.09766</link>
<guid>https://arxiv.org/abs/2501.09766</guid>
<content:encoded><![CDATA[
arXiv:2501.09766v4 Announce Type: replace-cross 
Abstract: Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities, especially for complex tasks. Synthesizing tool-use data through real-world simulations is an effective way to achieve this. However, our investigation reveals that training gains significantly decay as synthetic data increases. The model struggles to benefit from more synthetic data, and it can not equip the model with advanced tool-use capabilities in complex scenarios. Moreover, we discovered that the above limitation usually manifests as a fragment deficiency (i.e., parameter errors) in response. To this end, we propose an iterative reinforced fine-tuning strategy designed to alleviate this limitation. This strategy involves: (1) enhancing the diversity of response for synthetic data through path exploration of Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency by constructing fine-grained preference pairs, and then improving it by preference optimization algorithms for targeted improvement. The experiments show that our method achieves 13.11% better performance than the same-size base model. It achieves an improvement of 6.5% in complex scenarios compared to the baseline, and it also outperforms larger open-source and closed-source models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Each Graph is a New Language: Graph Learning with LLMs</title>
<link>https://arxiv.org/abs/2501.11478</link>
<guid>https://arxiv.org/abs/2501.11478</guid>
<content:encoded><![CDATA[
arXiv:2501.11478v3 Announce Type: replace-cross 
Abstract: Recent efforts leverage Large Language Models (LLMs) for modeling text-attributed graph structures in node classification tasks. These approaches describe graph structures for LLMs to understand or aggregate LLM-generated textual attribute embeddings through graph structure. However, these approaches face two main limitations in modeling graph structures with LLMs. (i) Graph descriptions become verbose in describing high-order graph structure. (ii) Textual attributes alone do not contain adequate graph structure information. It is challenging to model graph structure concisely and adequately with LLMs. LLMs lack built-in mechanisms to model graph structures directly. They also struggle with complex long-range dependencies between high-order nodes and target nodes.
  Inspired by the observation that LLMs pre-trained on one language can achieve exceptional performance on another with minimal additional training, we propose \textbf{G}raph-\textbf{D}efined \textbf{L}anguage for \textbf{L}arge \textbf{L}anguage \textbf{M}odel (GDL4LLM). This novel framework enables LLMs to transfer their powerful language understanding capabilities to graph-structured data. GDL4LLM translates graphs into a graph language corpus instead of graph descriptions and pre-trains LLMs on this corpus to adequately understand graph structures. During fine-tuning, this corpus describes the structural information of target nodes concisely with only a few tokens. By treating graphs as a new language, GDL4LLM enables LLMs to model graph structures adequately and concisely for node classification tasks. Extensive experiments on three real-world datasets demonstrate that GDL4LLM outperforms description-based and textual attribute embeddings-based baselines by efficiently modeling different orders of graph structure with LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging The Multi-Modality Gaps of Audio, Visual and Linguistic for Speech Enhancement</title>
<link>https://arxiv.org/abs/2501.13375</link>
<guid>https://arxiv.org/abs/2501.13375</guid>
<content:encoded><![CDATA[
arXiv:2501.13375v2 Announce Type: replace-cross 
Abstract: Speech enhancement (SE) aims to improve the quality and intelligibility of speech in noisy environments. Recent studies have shown that incorporating visual cues in audio signal processing can enhance SE performance. Given that human speech communication naturally involves audio, visual, and linguistic modalities, it is reasonable to expect additional improvements by integrating linguistic information. However, effectively bridging these modality gaps, particularly during knowledge transfer remains a significant challenge. In this paper, we propose a novel multi-modal learning framework, termed DLAV-SE, which leverages a diffusion-based model integrating audio, visual, and linguistic information for audio-visual speech enhancement (AVSE). Within this framework, the linguistic modality is modeled using a pretrained language model (PLM), which transfers linguistic knowledge to the audio-visual domain through a cross-modal knowledge transfer (CMKT) mechanism during training. After training, the PLM is no longer required at inference, as its knowledge is embedded into the AVSE model through the CMKT process. We conduct a series of SE experiments to evaluate the effectiveness of our approach. Results show that the proposed DLAV-SE system significantly improves speech quality and reduces generative artifacts, such as phonetic confusion, compared to state-of-the-art (SOTA) methods. Furthermore, visualization analyses confirm that the CMKT method enhances the generation quality of the AVSE outputs. These findings highlight both the promise of diffusion-based methods for advancing AVSE and the value of incorporating linguistic information to further improve system performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models</title>
<link>https://arxiv.org/abs/2501.13772</link>
<guid>https://arxiv.org/abs/2501.13772</guid>
<content:encoded><![CDATA[
arXiv:2501.13772v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate impressive zero-shot performance across a wide range of natural language processing tasks. Integrating various modality encoders further expands their capabilities, giving rise to Multimodal Large Language Models (MLLMs) that process not only text but also visual and auditory modality inputs. However, these advanced capabilities may also pose significant security risks, as models can be exploited to generate harmful or inappropriate content through jailbreak attacks. While prior work has extensively explored how manipulating textual or visual modality inputs can circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific Jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored. To address this gap, we introduce Jailbreak-AudioBench, which consists of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox supports not only text-to-audio conversion but also a range of audio editing techniques. The curated Dataset provides diverse explicit and implicit jailbreak audio examples in both original and edited forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs, establishing the most comprehensive audio jailbreak benchmark to date. Finally, Jailbreak-AudioBench establishes a foundation for advancing future research on LALMs safety alignment by enabling the in-depth exposure of more powerful jailbreak threats, such as query-based audio editing, and by facilitating the development of effective defense mechanisms.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization</title>
<link>https://arxiv.org/abs/2501.14635</link>
<guid>https://arxiv.org/abs/2501.14635</guid>
<content:encoded><![CDATA[
arXiv:2501.14635v2 Announce Type: replace-cross 
Abstract: The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a fundamental notion of averaging that extends from the Euclidean space to the Wasserstein space of probability distributions. Computation of the unregularized barycenter for discretized probability distributions on point clouds is a challenging task when the domain dimension $d > 1$. Most practical algorithms for approximating the barycenter problem are based on entropic regularization. In this paper, we introduce a nearly linear time $O(m \log{m})$ and linear space complexity $O(m)$ primal-dual algorithm, the Wasserstein-Descent $\dot{\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing the exact barycenter when the input probability density functions are discretized on an $m$-point grid. The key success of the WDHA algorithm hinges on alternating between two different yet closely related Wasserstein and Sobolev optimization geometries for the primal barycenter and dual Kantorovich potential subproblems. Under reasonable assumptions, we establish the convergence rate and iteration complexity of WDHA to its stationary point when the step size is appropriately chosen. Superior computational efficacy, scalability, and accuracy over the existing Sinkhorn-type algorithms are demonstrated on high-resolution (e.g., $1024 \times 1024$ images) 2D synthetic and real data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</title>
<link>https://arxiv.org/abs/2501.17161</link>
<guid>https://arxiv.org/abs/2501.17161</guid>
<content:encoded><![CDATA[
arXiv:2501.17161v2 Announce Type: replace-cross 
Abstract: Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Overvaluation Attack and Truthful Data Valuation in Federated Learning</title>
<link>https://arxiv.org/abs/2502.00494</link>
<guid>https://arxiv.org/abs/2502.00494</guid>
<content:encoded><![CDATA[
arXiv:2502.00494v3 Announce Type: replace-cross 
Abstract: In collaborative machine learning (CML), data valuation, i.e., evaluating the contribution of each client's data to the machine learning model, has become a critical task for incentivizing and selecting positive data contributions. However, existing studies often assume that clients engage in data valuation truthfully, overlooking the practical motivation for clients to exaggerate their contributions. To unlock this threat, this paper introduces the data overvaluation attack, enabling strategic clients to have their data significantly overvalued in federated learning, a widely adopted paradigm for decentralized CML. Furthermore, we propose a Bayesian truthful data valuation metric, named Truth-Shapley. Truth-Shapley is the unique metric that guarantees some promising axioms for data valuation while ensuring that clients' optimal strategy is to perform truthful data valuation under certain conditions. Our experiments demonstrate the vulnerability of existing data valuation metrics to the proposed attack and validate the robustness and effectiveness of Truth-Shapley.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach</title>
<link>https://arxiv.org/abs/2502.00577</link>
<guid>https://arxiv.org/abs/2502.00577</guid>
<content:encoded><![CDATA[
arXiv:2502.00577v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have shown promising capabilities but struggle under distribution shifts, where evaluation data differ from instruction tuning distributions. Although previous works have provided empirical evaluations, we argue that establishing a formal framework that can characterize and quantify the risk of MLLMs is necessary to ensure the safe and reliable application of MLLMs in the real world. By taking an information-theoretic perspective, we propose the first theoretical framework that enables the quantification of the maximum risk of MLLMs under distribution shifts. Central to our framework is the introduction of Effective Mutual Information (EMI), a principled metric that quantifies the relevance between input queries and model responses. We derive an upper bound for the EMI difference between in-distribution (ID) and out-of-distribution (OOD) data, connecting it to visual and textual distributional discrepancies. Extensive experiments on real benchmark datasets, spanning 61 shift scenarios, empirically validate our theoretical insights.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIP-UP: A Simple and Efficient Mixture-of-Experts CLIP Training Recipe with Sparse Upcycling</title>
<link>https://arxiv.org/abs/2502.00965</link>
<guid>https://arxiv.org/abs/2502.00965</guid>
<content:encoded><![CDATA[
arXiv:2502.00965v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) models are crucial for scaling model capacity while controlling inference costs. While integrating MoE into multimodal models like CLIP improves performance, training these models is notoriously challenging and expensive. We propose CLIP-Upcycling (CLIP-UP), an efficient alternative training strategy that converts a pre-trained dense CLIP model into a sparse MoE architecture. Through extensive experimentation with various settings and auxiliary losses, we demonstrate that CLIP-UP significantly reduces training complexity and cost. Remarkably, our sparse CLIP B/16 model, trained with CLIP-UP, outperforms its dense counterpart by 7.2% and 6.6% on COCO and Flickr30k text-to-image Recall@1 benchmarks respectively. It even surpasses the larger CLIP L/14 model on this task while using only 30% of the inference FLOPs. We further demonstrate the generalizability of our training recipe across different scales, establishing sparse upcycling as a practical and scalable approach for building efficient, high-performance CLIP models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Active Speech Cancellation with Mamba-Masking Network</title>
<link>https://arxiv.org/abs/2502.01185</link>
<guid>https://arxiv.org/abs/2502.01185</guid>
<content:encoded><![CDATA[
arXiv:2502.01185v2 Announce Type: replace-cross 
Abstract: We present a novel deep learning network for Active Speech Cancellation (ASC), advancing beyond Active Noise Cancellation (ANC) methods by effectively canceling both noise and speech signals. The proposed Mamba-Masking architecture introduces a masking mechanism that directly interacts with the encoded reference signal, enabling adaptive and precisely aligned anti-signal generation-even under rapidly changing, high-frequency conditions, as commonly found in speech. Complementing this, a multi-band segmentation strategy further improves phase alignment across frequency bands. Additionally, we introduce an optimization-driven loss function that provides near-optimal supervisory signals for anti-signal generation. Experimental results demonstrate substantial performance gains, achieving up to 7.2dB improvement in ANC scenarios and 6.2dB in ASC, significantly outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Change Point Detection in the Frequency Domain with Statistical Reliability</title>
<link>https://arxiv.org/abs/2502.03062</link>
<guid>https://arxiv.org/abs/2502.03062</guid>
<content:encoded><![CDATA[
arXiv:2502.03062v2 Announce Type: replace-cross 
Abstract: Effective condition monitoring in complex systems requires identifying change points (CPs) in the frequency domain, as the structural changes often arise across multiple frequencies. This paper extends recent advancements in statistically significant CP detection, based on Selective Inference (SI), to the frequency domain. The proposed SI method quantifies the statistical significance of detected CPs in the frequency domain using $p$-values, ensuring that the detected changes reflect genuine structural shifts in the target system. We address two major technical challenges to achieve this. First, we extend the existing SI framework to the frequency domain by appropriately utilizing the properties of discrete Fourier transform (DFT). Second, we develop an SI method that provides valid $p$-values for CPs where changes occur across multiple frequencies. Experimental results demonstrate that the proposed method reliably identifies genuine CPs with strong statistical guarantees, enabling more accurate root-cause analysis in the frequency domain of complex systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-Aligned Image Models Improve Visual Decoding from the Brain</title>
<link>https://arxiv.org/abs/2502.03081</link>
<guid>https://arxiv.org/abs/2502.03081</guid>
<content:encoded><![CDATA[
arXiv:2502.03081v2 Announce Type: replace-cross 
Abstract: Decoding visual images from brain activity has significant potential for advancing brain-computer interaction and enhancing the understanding of human perception. Recent approaches align the representation spaces of images and brain activity to enable visual decoding. In this paper, we introduce the use of human-aligned image encoders to map brain signals to images. We hypothesize that these models more effectively capture perceptual attributes associated with the rapid visual stimuli presentations commonly used in visual brain data recording experiments. Our empirical results support this hypothesis, demonstrating that this simple modification improves image retrieval accuracy by up to 21% compared to state-of-the-art methods. Comprehensive experiments confirm consistent performance improvements across diverse EEG architectures, image encoders, alignment methods, participants, and brain imaging modalities
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SymmCD: Symmetry-Preserving Crystal Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2502.03638</link>
<guid>https://arxiv.org/abs/2502.03638</guid>
<content:encoded><![CDATA[
arXiv:2502.03638v3 Announce Type: replace-cross 
Abstract: Generating novel crystalline materials has the potential to lead to advancements in fields such as electronics, energy storage, and catalysis. The defining characteristic of crystals is their symmetry, which plays a central role in determining their physical properties. However, existing crystal generation methods either fail to generate materials that display the symmetries of real-world crystals, or simply replicate the symmetry information from examples in a database. To address this limitation, we propose SymmCD, a novel diffusion-based generative model that explicitly incorporates crystallographic symmetry into the generative process. We decompose crystals into two components and learn their joint distribution through diffusion: 1) the asymmetric unit, the smallest subset of the crystal which can generate the whole crystal through symmetry transformations, and; 2) the symmetry transformations needed to be applied to each atom in the asymmetric unit. We also use a novel and interpretable representation for these transformations, enabling generalization across different crystallographic symmetry groups. We showcase the competitive performance of SymmCD on a subset of the Materials Project, obtaining diverse and valid crystals with realistic symmetries and predicted properties.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation</title>
<link>https://arxiv.org/abs/2502.03930</link>
<guid>https://arxiv.org/abs/2502.03930</guid>
<content:encoded><![CDATA[
arXiv:2502.03930v3 Announce Type: replace-cross 
Abstract: Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-Powered E-Values</title>
<link>https://arxiv.org/abs/2502.04294</link>
<guid>https://arxiv.org/abs/2502.04294</guid>
<content:encoded><![CDATA[
arXiv:2502.04294v2 Announce Type: replace-cross 
Abstract: Quality statistical inference requires a sufficient amount of data, which can be missing or hard to obtain. To this end, prediction-powered inference has risen as a promising methodology, but existing approaches are largely limited to Z-estimation problems such as inference of means and quantiles. In this paper, we apply ideas of prediction-powered inference to e-values. By doing so, we inherit all the usual benefits of e-values -- such as anytime-validity, post-hoc validity and versatile sequential inference -- as well as greatly expand the set of inferences achievable in a prediction-powered manner. In particular, we show that every inference procedure that can be framed in terms of e-values has a prediction-powered counterpart, given by our method. We showcase the effectiveness of our framework across a wide range of inference tasks, from simple hypothesis testing and confidence intervals to more involved procedures for change-point detection and causal discovery, which were out of reach of previous techniques. Our approach is modular and easily integrable into existing algorithms, making it a compelling choice for practical applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JingFang: An Expert-Level Large Language Model for Traditional Chinese Medicine Clinical Consultation and Syndrome Differentiation-Based Treatment</title>
<link>https://arxiv.org/abs/2502.04345</link>
<guid>https://arxiv.org/abs/2502.04345</guid>
<content:encoded><![CDATA[
arXiv:2502.04345v2 Announce Type: replace-cross 
Abstract: The effective application of traditional Chinese medicine (TCM) requires extensive knowledge of TCM and clinical experience. The emergence of Large Language Models (LLMs) provides a solution to this, while existing LLMs for TCM exhibit critical limitations of incomplete clinical consultation and diagnoses, as well as inaccurate syndrome differentiation. To address these issues, we establish JingFang (JF), a novel TCM LLM that demonstrates the level of expertise in clinical consultation and syndrome differentiation. We propose a Multi-Agent Collaborative Chain-of-Thought Mechanism (MACCTM) for comprehensive and targeted clinical consultation, enabling JF with effective and accurate diagnostic ability. In addition, a Syndrome Agent and a Dual-Stage Recovery Scheme (DSRS) are developed to accurately enhance the differentiation of the syndrome and the subsequent corresponding treatment. JingFang not only facilitates the application of LLMs but also promotes the effective application of TCM for healthcare.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.04395</link>
<guid>https://arxiv.org/abs/2502.04395</guid>
<content:encoded><![CDATA[
arXiv:2502.04395v2 Announce Type: replace-cross 
Abstract: Recent advancements in time series forecasting have explored augmenting models with text or vision modalities to improve accuracy. While text provides contextual understanding, it often lacks fine-grained temporal details. Conversely, vision captures intricate temporal patterns but lacks semantic context, limiting the complementary potential of these modalities. To address this, we propose \method, a novel multimodal framework that leverages pre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and textual modalities for enhanced forecasting. Our framework comprises three key components: (1) a Retrieval-Augmented Learner, which extracts enriched temporal features through memory bank interactions; (2) a Vision-Augmented Learner, which encodes time series as informative images; and (3) a Text-Augmented Learner, which generates contextual textual descriptions. These components collaborate with frozen pre-trained VLMs to produce multimodal embeddings, which are then fused with temporal features for final prediction. Extensive experiments demonstrate that Time-VLM achieves superior performance, particularly in few-shot and zero-shot scenarios, thereby establishing a new direction for multimodal time series forecasting. Code is available at https://github.com/CityMind-Lab/ICML25-TimeVLM.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection</title>
<link>https://arxiv.org/abs/2502.04528</link>
<guid>https://arxiv.org/abs/2502.04528</guid>
<content:encoded><![CDATA[
arXiv:2502.04528v4 Announce Type: replace-cross 
Abstract: The advancement of large language models (LLMs) has made it difficult to differentiate human-written text from AI-generated text. Several AI-text detectors have been developed in response, which typically utilize a fixed global threshold (e.g., $\theta = 0.5$) to classify machine-generated text. However, one universal threshold could fail to account for distributional variations by subgroups. For example, when using a fixed threshold, detectors make more false positive errors on shorter human-written text, and more positive classifications of neurotic writing styles among long texts. These discrepancies can lead to misclassifications that disproportionately affect certain groups. We address this critical limitation by introducing FairOPT, an algorithm for group-specific threshold optimization for probabilistic AI-text detectors. We partitioned data into subgroups based on attributes (e.g., text length and writing style) and implemented FairOPT to learn decision thresholds for each group to reduce discrepancy. In experiments with nine AI text classifiers on three datasets, FairOPT decreases overall balanced error rate (BER) discrepancy by 12\% while minimally sacrificing accuracy by 0.003\%. Our framework paves the way for more robust classification in AI-generated content detection via post-processing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Collusion by Collectives on Learning Platforms</title>
<link>https://arxiv.org/abs/2502.04879</link>
<guid>https://arxiv.org/abs/2502.04879</guid>
<content:encoded><![CDATA[
arXiv:2502.04879v3 Announce Type: replace-cross 
Abstract: As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Problem Sampling in Latent Space Using Sequential Monte Carlo</title>
<link>https://arxiv.org/abs/2502.05908</link>
<guid>https://arxiv.org/abs/2502.05908</guid>
<content:encoded><![CDATA[
arXiv:2502.05908v2 Announce Type: replace-cross 
Abstract: In image processing, solving inverse problems is the task of finding plausible reconstructions of an image that was corrupted by some (usually known) degradation operator. Commonly, this process is done using a generative image model that can guide the reconstruction towards solutions that appear natural. The success of diffusion models over the last few years has made them a leading candidate for this task. However, the sequential nature of diffusion models makes this conditional sampling process challenging. Furthermore, since diffusion models are often defined in the latent space of an autoencoder, the encoder-decoder transformations introduce additional difficulties. To address these challenges, we suggest a novel sampling method based on sequential Monte Carlo (SMC) in the latent space of diffusion models. We name our method LD-SMC. We define a generative model for the data using additional auxiliary observations and perform posterior inference with SMC sampling based on a backward diffusion process. Empirical evaluations on ImageNet and FFHQ show the benefits of LD-SMC over competing methods in various inverse problem tasks and especially in challenging inpainting tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When More is Less: Understanding Chain-of-Thought Length in LLMs</title>
<link>https://arxiv.org/abs/2502.07266</link>
<guid>https://arxiv.org/abs/2502.07266</guid>
<content:encoded><![CDATA[
arXiv:2502.07266v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) employ Chain-of-Thought (CoT) reasoning to deconstruct complex problems. While longer CoTs are often presumed superior, this paper challenges that notion, arguing that longer is not always better. Drawing on combined evidence from real-world observations, controlled experiments, and theoretical analysis, we demonstrate that task accuracy typically follows an inverted U-shaped curve with CoT length, where performance initially improves but eventually decreases as the number of CoT steps increases. With controlled experiments, we further uncover the scaling behaviors of the optimal CoT length: it increases with task difficulty but decreases with model capability, exposing an inherent simplicity bias where more capable models favor shorter, more efficient CoT reasoning. This bias is also evident in Reinforcement Learning (RL) training, where models gravitate towards shorter CoTs as their accuracy improves. To have a deep understanding of these dynamics, we establish a simple theoretical model that formally proves these phenomena, including the optimal length's scaling laws and the emergence of simplicity bias during RL. Guided by this framework, we demonstrate significant practical benefits from training with optimally-lengthed CoTs and employing length-aware filtering at inference. These findings offer both a principled understanding of the "overthinking" phenomenon and multiple practical guidelines for CoT calibration, enabling LLMs to achieve optimal reasoning performance with adaptive CoTs tailored to task complexity and model capability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weighted quantization using MMD: From mean field to mean shift via gradient flows</title>
<link>https://arxiv.org/abs/2502.10600</link>
<guid>https://arxiv.org/abs/2502.10600</guid>
<content:encoded><![CDATA[
arXiv:2502.10600v2 Announce Type: replace-cross 
Abstract: Approximating a probability distribution using a set of particles is a fundamental problem in machine learning and statistics, with applications including clustering and quantization. Formally, we seek a weighted mixture of Dirac measures that best approximates the target distribution. While much existing work relies on the Wasserstein distance to quantify approximation errors, maximum mean discrepancy (MMD) has received comparatively less attention, especially when allowing for variable particle weights. We argue that a Wasserstein-Fisher-Rao gradient flow is well-suited for designing quantizations optimal under MMD. We show that a system of interacting particles satisfying a set of ODEs discretizes this flow. We further derive a new fixed-point algorithm called mean shift interacting particles (MSIP). We show that MSIP extends the classical mean shift algorithm, widely used for identifying modes in kernel density estimators. Moreover, we show that MSIP can be interpreted as preconditioned gradient descent and that it acts as a relaxation of Lloyd's algorithm for clustering. Our unification of gradient flows, mean shift, and MMD-optimal quantization yields algorithms that are more robust than state-of-the-art methods, as demonstrated via high-dimensional and multi-modal numerical experiments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMART: Self-Aware Agent for Tool Overuse Mitigation</title>
<link>https://arxiv.org/abs/2502.11435</link>
<guid>https://arxiv.org/abs/2502.11435</guid>
<content:encoded><![CDATA[
arXiv:2502.11435v2 Announce Type: replace-cross 
Abstract: Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines</title>
<link>https://arxiv.org/abs/2502.12051</link>
<guid>https://arxiv.org/abs/2502.12051</guid>
<content:encoded><![CDATA[
arXiv:2502.12051v2 Announce Type: replace-cross 
Abstract: Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Likelihood-Ratio Regularized Quantile Regression: Adapting Conformal Prediction to High-Dimensional Covariate Shifts</title>
<link>https://arxiv.org/abs/2502.13030</link>
<guid>https://arxiv.org/abs/2502.13030</guid>
<content:encoded><![CDATA[
arXiv:2502.13030v2 Announce Type: replace-cross 
Abstract: We consider the problem of conformal prediction under covariate shift. Given labeled data from a source domain and unlabeled data from a covariate shifted target domain, we seek to construct prediction sets with valid marginal coverage in the target domain. Most existing methods require estimating the unknown likelihood ratio function, which can be prohibitive for high-dimensional data such as images. To address this challenge, we introduce the likelihood ratio regularized quantile regression (LR-QR) algorithm, which combines the pinball loss with a novel choice of regularization in order to construct a threshold function without directly estimating the unknown likelihood ratio. We show that the LR-QR method has coverage at the desired level in the target domain, up to a small error term that we can control. Our proofs draw on a novel analysis of coverage via stability bounds from learning theory. Our experiments demonstrate that the LR-QR algorithm outperforms existing methods on high-dimensional prediction tasks, including a regression task for the Communities and Crime dataset, an image classification task from the WILDS repository, and an LLM question-answering task on the MMLU benchmark.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Generation from Visual Events: Challenges and Future Directions</title>
<link>https://arxiv.org/abs/2502.13034</link>
<guid>https://arxiv.org/abs/2502.13034</guid>
<content:encoded><![CDATA[
arXiv:2502.13034v2 Announce Type: replace-cross 
Abstract: The ability to use natural language to talk about visual events is at the core of human intelligence and a crucial feature of any artificial intelligence system. In recent years, a substantial body of work in visually grounded NLP has focused on describing content depicted in single images. By contrast, comparatively less attention has been devoted to exhaustively modeling scenarios in which natural language is employed to interpret and talk about events presented through videos or sequences of images. In this position paper, we argue that any NLG task dealing with sequences of images or frames is an instance of the broader, more general problem of modeling the intricate relationships between visual events unfolding over time and the features of the language used to interpret, describe, or narrate them. Therefore, solving these tasks requires models to be capable of identifying and managing such intricacies. We consider five seemingly different tasks, which we argue are compelling instances of this broader multimodal problem. Consistently, we claim that these tasks pose a common set of challenges and share similarities in terms of modeling and evaluation approaches. Building on this perspective, we identify key open questions and propose several research directions for future investigation. We claim that improving language-and-vision models' understanding of visual events is both timely and essential, given their growing applications. Additionally, this challenge offers significant scientific insight, advancing model development through principles of human cognition and language use.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative diffusion for perceptron problems: statistical physics analysis and efficient algorithms</title>
<link>https://arxiv.org/abs/2502.16292</link>
<guid>https://arxiv.org/abs/2502.16292</guid>
<content:encoded><![CDATA[
arXiv:2502.16292v2 Announce Type: replace-cross 
Abstract: We consider random instances of non-convex perceptron problems in the high-dimensional limit of a large number of examples $M$ and weights $N$, with finite load $\alpha = M/N$. We develop a formalism based on replica theory to predict the fundamental limits of efficiently sampling the solution space using generative diffusion algorithms, conjectured to be saturated when the score function is provided by Approximate Message Passing. For the spherical perceptron with negative margin $\kappa$, we find that the uniform distribution over solutions can be efficiently sampled in most of the Replica Symmetric region of the $\alpha-\kappa$ plane. In contrast, for binary weights, sampling from the uniform distribution remains intractable. A theoretical analysis of this obstruction leads us to identify a potential $U(s) = -\log(s)$, under which the corresponding tilted distribution becomes efficiently samplable via diffusion. Moreover, we show numerically that an annealing procedure over the shape of this potential yields a fast and robust Markov Chain Monte Carlo algorithm for sampling the solution space of the binary perceptron.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter</title>
<link>https://arxiv.org/abs/2502.16880</link>
<guid>https://arxiv.org/abs/2502.16880</guid>
<content:encoded><![CDATA[
arXiv:2502.16880v3 Announce Type: replace-cross 
Abstract: Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for LLM-Based Survey Simulations</title>
<link>https://arxiv.org/abs/2502.17773</link>
<guid>https://arxiv.org/abs/2502.17773</guid>
<content:encoded><![CDATA[
arXiv:2502.17773v3 Announce Type: replace-cross 
Abstract: We investigate the use of large language models (LLMs) to simulate human responses to survey questions, and perform uncertainty quantification to gain reliable insights. Our approach converts imperfect LLM-simulated responses into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Structural Knowledge in Diffusion Models for Source Localization in Data-Limited Graph Scenarios</title>
<link>https://arxiv.org/abs/2502.17928</link>
<guid>https://arxiv.org/abs/2502.17928</guid>
<content:encoded><![CDATA[
arXiv:2502.17928v2 Announce Type: replace-cross 
Abstract: The source localization problem in graph information propagation is crucial for managing various network disruptions, from misinformation spread to infrastructure failures. While recent deep generative approaches have shown promise in this domain, their effectiveness is limited by the scarcity of real-world propagation data. This paper introduces SIDSL (\textbf{S}tructure-prior \textbf{I}nformed \textbf{D}iffusion model for \textbf{S}ource \textbf{L}ocalization), a novel framework that addresses three key challenges in limited-data scenarios: unknown propagation patterns, complex topology-propagation relationships, and class imbalance between source and non-source nodes. SIDSL incorporates topology-aware priors through graph label propagation and employs a propagation-enhanced conditional denoiser with a GNN-parameterized label propagation module (GNN-LP). Additionally, we propose a structure-prior biased denoising scheme that initializes from structure-based source estimations rather than random noise, effectively countering class imbalance issues. Experimental results across four real-world datasets demonstrate SIDSL's superior performance, achieving 7.5-13.3% improvements in F1 scores compared to state-of-the-art methods. Notably, when pretrained with simulation data of synthetic patterns, SIDSL maintains robust performance with only 10% of training data, surpassing baselines by more than 18.8%. These results highlight SIDSL's effectiveness in real-world applications where labeled data is scarce.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems</title>
<link>https://arxiv.org/abs/2502.18632</link>
<guid>https://arxiv.org/abs/2502.18632</guid>
<content:encoded><![CDATA[
arXiv:2502.18632v2 Announce Type: replace-cross 
Abstract: Knowledge components (KCs) mapped to problems help model student learning, tracking their mastery levels on fine-grained skills thereby facilitating personalized learning and feedback in online learning platforms. However, crafting and tagging KCs to problems, traditionally performed by human domain experts, is highly labor-intensive. We present a fully automated, LLM-based pipeline for KC generation and tagging for open-ended programming problems. We also develop an LLM-based knowledge tracing (KT) framework to leverage these LLM-generated KCs, which we refer to as KCGen-KT. We conduct extensive quantitative and qualitative evaluations on a real-world student code submission dataset. We find that KCGen-KT outperforms existing KT methods and human-written KCs on future student response prediction. We investigate the learning curves of generated KCs and show that LLM-generated KCs result in a better fit than human-written KCs under a cognitive model. We also conduct a human evaluation with course instructors to show that our pipeline generates reasonably accurate problem-KC mappings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs</title>
<link>https://arxiv.org/abs/2502.18791</link>
<guid>https://arxiv.org/abs/2502.18791</guid>
<content:encoded><![CDATA[
arXiv:2502.18791v3 Announce Type: replace-cross 
Abstract: The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEvalDB. We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches. We validate LLMEvalDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding & multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through LLMEvalDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs</title>
<link>https://arxiv.org/abs/2502.19148</link>
<guid>https://arxiv.org/abs/2502.19148</guid>
<content:encoded><![CDATA[
arXiv:2502.19148v2 Announce Type: replace-cross 
Abstract: How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse regarding culture, values, or time. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking)</title>
<link>https://arxiv.org/abs/2502.21009</link>
<guid>https://arxiv.org/abs/2502.21009</guid>
<content:encoded><![CDATA[
arXiv:2502.21009v2 Announce Type: replace-cross 
Abstract: In physics, complex systems are often simplified into minimal, solvable models that retain only the core principles. In machine learning, layerwise linear models (e.g., linear neural networks) act as simplified representations of neural network dynamics. These models follow the dynamical feedback principle, which describes how layers mutually govern and amplify each other's evolution. This principle extends beyond the simplified models, successfully explaining a wide range of dynamical phenomena in deep neural networks, including neural collapse, emergence, lazy and rich regimes, and grokking. In this position paper, we call for the use of layerwise linear models retaining the core principles of neural dynamical phenomena to accelerate the science of deep learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SteerConf: Steering LLMs for Confidence Elicitation</title>
<link>https://arxiv.org/abs/2503.02863</link>
<guid>https://arxiv.org/abs/2503.02863</guid>
<content:encoded><![CDATA[
arXiv:2503.02863v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit impressive performance across diverse domains but often suffer from overconfidence, limiting their reliability in critical applications. We propose SteerConf, a novel framework that systematically steers LLMs' confidence scores to improve their calibration and reliability. SteerConf introduces three key components: (1) a steering prompt strategy that guides LLMs to produce confidence scores in specified directions (e.g., conservative or optimistic) by leveraging prompts with varying steering levels; (2) a steered confidence consistency measure that quantifies alignment across multiple steered confidences to enhance calibration; and (3) a steered confidence calibration method that aggregates confidence scores using consistency measures and applies linear quantization for answer selection. SteerConf operates without additional training or fine-tuning, making it broadly applicable to existing LLMs. Experiments on seven benchmarks spanning professional knowledge, common sense, ethics, and reasoning tasks, using advanced LLM models (GPT-3.5, LLaMA 3, GPT-4), demonstrate that SteerConf significantly outperforms existing methods, often by a significant margin. Our findings highlight the potential of steering the confidence of LLMs to enhance their reliability for safer deployment in real-world applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRANSIT your events into a new mass: Fast background interpolation for weakly-supervised anomaly searches</title>
<link>https://arxiv.org/abs/2503.04342</link>
<guid>https://arxiv.org/abs/2503.04342</guid>
<content:encoded><![CDATA[
arXiv:2503.04342v2 Announce Type: replace-cross 
Abstract: We introduce a new model for conditional and continuous data morphing called TRansport Adversarial Network for Smooth InTerpolation (TRANSIT). We apply it to create a background data template for weakly-supervised searches at the LHC. The method smoothly transforms sideband events to match signal region mass distributions. We demonstrate the performance of TRANSIT using the LHC Olympics R\&amp;D dataset. The model captures non-linear mass correlations of features and produces a template that offers a competitive anomaly sensitivity compared to state-of-the-art transport-based template generators. Moreover, the computational training time required for TRANSIT is an order of magnitude lower than that of competing deep learning methods. This makes it ideal for analyses that iterate over many signal regions and signal models. Unlike generative models, which must learn a full probability density distribution, i.e., the correlations between all the variables, the proposed transport model only has to learn a smooth conditional shift of the distribution. This allows for a simpler, more efficient residual architecture, enabling mass uncorrelated features to pass the network unchanged while the mass correlated features are adjusted accordingly. Furthermore, we show that the latent space of the model provides a set of mass decorrelated features useful for anomaly detection without background sculpting.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel-based estimators for functional causal effects</title>
<link>https://arxiv.org/abs/2503.05024</link>
<guid>https://arxiv.org/abs/2503.05024</guid>
<content:encoded><![CDATA[
arXiv:2503.05024v3 Announce Type: replace-cross 
Abstract: We propose causal effect estimators based on empirical Fr\'{e}chet means and operator-valued kernels, tailored to functional data spaces. These methods address the challenges of high-dimensionality, sequential ordering, and model complexity while preserving robustness to treatment misspecification. Using structural assumptions, we obtain compact representations of potential outcomes, enabling scalable estimation of causal effects over time and across covariates. We provide both theoretical, regarding the consistency of functional causal effects, as well as empirical comparison of a range of proposed causal effect estimators.
  Applications to binary treatment settings with functional outcomes illustrate the framework's utility in biomedical monitoring, where outcomes exhibit complex temporal dynamics. Our estimators accommodate scenarios with registered covariates and outcomes, aligning them to the Fr\'{e}chet means, as well as cases requiring higher-order representations to capture intricate covariate-outcome interactions. These advancements extend causal inference to dynamic and non-linear domains, offering new tools for understanding complex treatment effects in functional data settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the status of current quantum machine learning software</title>
<link>https://arxiv.org/abs/2503.08962</link>
<guid>https://arxiv.org/abs/2503.08962</guid>
<content:encoded><![CDATA[
arXiv:2503.08962v2 Announce Type: replace-cross 
Abstract: The recent advancements in noisy intermediate-scale quantum (NISQ) devices implementation allow us to study their application to real-life computational problems. However, hardware challenges are not the only ones that hinder our quantum computation capabilities. Software limitations are the other, less explored side of this medal. Using satellite image segmentation as a task example, we investigated how difficult it is to run a hybrid quantum-classical model on a real, publicly available quantum device. We also analyzed the costs of such endeavor and the change in quality of model.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DitHub: A Modular Framework for Incremental Open-Vocabulary Object Detection</title>
<link>https://arxiv.org/abs/2503.09271</link>
<guid>https://arxiv.org/abs/2503.09271</guid>
<content:encoded><![CDATA[
arXiv:2503.09271v2 Announce Type: replace-cross 
Abstract: Open-Vocabulary object detectors can generalize to an unrestricted set of categories through simple textual prompting. However, adapting these models to rare classes or reinforcing their abilities on multiple specialized domains remains essential. While recent methods rely on monolithic adaptation strategies with a single set of weights, we embrace modular deep learning. We introduce DitHub, a framework designed to build and maintain a library of efficient adaptation modules. Inspired by Version Control Systems, DitHub manages expert modules as branches that can be fetched and merged as needed. This modular approach allows us to conduct an in-depth exploration of the compositional properties of adaptation modules, marking the first such study in Object Detection. Our method achieves state-of-the-art performance on the ODinW-13 benchmark and ODinW-O, a newly introduced benchmark designed to assess class reappearance. For more details, visit our project page: https://aimagelab.github.io/DitHub/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Real-World Energy Management Dataset from a Smart Company Building for Optimization and Machine Learning</title>
<link>https://arxiv.org/abs/2503.11469</link>
<guid>https://arxiv.org/abs/2503.11469</guid>
<content:encoded><![CDATA[
arXiv:2503.11469v2 Announce Type: replace-cross 
Abstract: We present a large real-world dataset obtained from monitoring a smart company facility over the course of six years, from 2018 to 2023. The dataset includes energy consumption data from various facility areas and components, energy production data from a photovoltaic system and a combined heat and power plant, operational data from heating and cooling systems, and weather data from an on-site weather station. The measurement sensors installed throughout the facility are organized in a hierarchical metering structure with multiple sub-metering levels, which is reflected in the dataset. The dataset contains measurement data from 72 energy meters, 9 heat meters and a weather station. Both raw and processed data at different processing levels, including labeled issues, is available. In this paper, we describe the data acquisition and post-processing employed to create the dataset. The dataset enables the application of a wide range of methods in the domain of energy management, including optimization, modeling, and machine learning to optimize building operations and reduce costs and carbon emissions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Angle Selection in X-Ray CT: A Reinforcement Learning Approach to Optimal Stopping</title>
<link>https://arxiv.org/abs/2503.12688</link>
<guid>https://arxiv.org/abs/2503.12688</guid>
<content:encoded><![CDATA[
arXiv:2503.12688v2 Announce Type: replace-cross 
Abstract: In industrial X-ray Computed Tomography (CT), the need for rapid in-line inspection is critical. Sparse-angle tomography plays a significant role in this by reducing the required number of projections, thereby accelerating processing and conserving resources. Most existing methods aim to balance reconstruction quality and scanning time, typically relying on fixed scan durations. Adaptive adjustment of the number of angles is essential; for instance, more angles may be required for objects with complex geometries or noisier projections. The concept of optimal stopping, which dynamically adjusts this balance according to varying industrial needs, remains overlooked. Building on our previous work, we integrate optimal stopping into sequential Optimal Experimental Design (sOED) and Reinforcement Learning (RL). We propose a novel method for computing the policy gradient within the Actor-Critic framework, enabling the development of adaptive policies for informative angle selection and scan termination. Additionally, we investigate the gap between simulation and real-world applications in the context of the developed learning-based method. Our trained model, developed using synthetic data, demonstrates reliable performance when applied to experimental X-ray CT data. This approach enhances the flexibility of CT operations and expands the applicability of sparse-angle tomography in industrial settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization</title>
<link>https://arxiv.org/abs/2503.15704</link>
<guid>https://arxiv.org/abs/2503.15704</guid>
<content:encoded><![CDATA[
arXiv:2503.15704v2 Announce Type: replace-cross 
Abstract: The performance of sequential Monte Carlo (SMC) samplers heavily depends on the tuning of the Markov kernels used in the path proposal. For SMC samplers with unadjusted Markov kernels, standard tuning objectives, such as the Metropolis-Hastings acceptance rate or the expected-squared jump distance, are no longer applicable. While stochastic gradient-based end-to-end optimization has been explored for tuning SMC samplers, they often incur excessive training costs, even for tuning just the kernel step sizes. In this work, we propose a general adaptation framework for tuning the Markov kernels in SMC samplers by minimizing the incremental Kullback-Leibler (KL) divergence between the proposal and target paths. For step size tuning, we provide a gradient- and tuning-free algorithm that is generally applicable for kernels such as Langevin Monte Carlo (LMC). We further demonstrate the utility of our approach by providing a tailored scheme for tuning kinetic LMC used in SMC samplers. Our implementations are able to obtain a full schedule of tuned parameters at the cost of a few vanilla SMC runs, which is a fraction of gradient-based approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing</title>
<link>https://arxiv.org/abs/2503.19385</link>
<guid>https://arxiv.org/abs/2503.19385</guid>
<content:encoded><![CDATA[
arXiv:2503.19385v3 Announce Type: replace-cross 
Abstract: We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTR: Graph-Table-RAG for Cross-Table Question Answering</title>
<link>https://arxiv.org/abs/2504.01346</link>
<guid>https://arxiv.org/abs/2504.01346</guid>
<content:encoded><![CDATA[
arXiv:2504.01346v3 Announce Type: replace-cross 
Abstract: Beyond pure text, a substantial amount of knowledge is stored in tables. In real-world scenarios, user questions often require retrieving answers that are distributed across multiple tables. GraphRAG has recently attracted much attention for enhancing LLMs' reasoning capabilities by organizing external knowledge to address ad-hoc and complex questions, exemplifying a promising direction for cross-table question answering. In this paper, to address the current gap in available data, we first introduce a multi-table benchmark, MutliTableQA, comprising 60k tables and 25k user queries collected from real-world sources. Then, we propose the first Graph-Table-RAG framework, namely GTR, which reorganizes table corpora into a heterogeneous graph, employs a hierarchical coarse-to-fine retrieval process to extract the most relevant tables, and integrates graph-aware prompting for downstream LLMs' tabular reasoning. Extensive experiments show that GTR exhibits superior cross-table question-answering performance while maintaining high deployment efficiency, demonstrating its real-world practical applicability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Encoding and Decoding at Scale</title>
<link>https://arxiv.org/abs/2504.08201</link>
<guid>https://arxiv.org/abs/2504.08201</guid>
<content:encoded><![CDATA[
arXiv:2504.08201v4 Announce Type: replace-cross 
Abstract: Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behavior from neural activity (decoding), limiting their ability to capture the bidirectional relationship between neural activity and behavior. To bridge this gap, we introduce a multimodal, multi-task model that enables simultaneous Neural Encoding and Decoding at Scale (NEDS). Central to our approach is a novel multi-task-masking strategy, which alternates between neural, behavioral, within-modality, and cross-modality masking. We pretrain our method on the International Brain Laboratory (IBL) repeated site dataset, which includes recordings from 83 animals performing the same visual decision-making task. In comparison to other large-scale models, we demonstrate that NEDS achieves state-of-the-art performance for both encoding and decoding when pretrained on multi-animal data and then fine-tuned on new animals. Surprisingly, NEDS's learned embeddings exhibit emergent properties: even without explicit training, they are highly predictive of the brain regions in each recording. Altogether, our approach is a step towards a foundation model of the brain that enables seamless translation between neural activity and behavior.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocAgent: A Multi-Agent System for Automated Code Documentation Generation</title>
<link>https://arxiv.org/abs/2504.08725</link>
<guid>https://arxiv.org/abs/2504.08725</guid>
<content:encoded><![CDATA[
arXiv:2504.08725v3 Announce Type: replace-cross 
Abstract: High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for Dynamic Data Acquisition in Digital Twins</title>
<link>https://arxiv.org/abs/2504.10248</link>
<guid>https://arxiv.org/abs/2504.10248</guid>
<content:encoded><![CDATA[
arXiv:2504.10248v2 Announce Type: replace-cross 
Abstract: This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisation strategies, which limit their applicability for online applications requiring continuous informative data assimilation. The proposed approach addresses this limitation by offering an adaptive framework for sensor placement within the digital twin paradigm. The sensor placement problem is formulated as a Markov decision process, enabling the training and deployment of an agent capable of dynamically repositioning sensors in response to the evolving conditions of the physical structure as represented by the digital twin. This ensures that the digital twin maintains a highly representative and reliable connection to its physical counterpart. The proposed framework is validated through a series of comprehensive case studies involving a cantilever plate structure subjected to diverse conditions, including healthy and damaged conditions. The results demonstrate the capability of the deep reinforcement learning agent to adaptively reposition sensors improving the quality of data acquisition and hence enhancing the overall accuracy of digital twins.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Time Structure to Estimate Causal Effects</title>
<link>https://arxiv.org/abs/2504.11076</link>
<guid>https://arxiv.org/abs/2504.11076</guid>
<content:encoded><![CDATA[
arXiv:2504.11076v2 Announce Type: replace-cross 
Abstract: There exist several approaches for estimating causal effects in time series when latent confounding is present. Many of these approaches rely on additional auxiliary observed variables or time series such as instruments, negative controls or time series that satisfy the front- or backdoor criterion in certain graphs. In this paper, we present a novel approach for estimating direct (and via Wright's path rule total) causal effects in a time series setup which does not rely on additional auxiliary observed variables or time series. This approach assumes that the underlying time series is a Structural Vector Autoregressive (SVAR) process and estimates direct causal effects by solving certain linear equation systems made up of different covariances and model parameters. We state sufficient graphical criteria in terms of the so-called full time graph under which these linear equations systems are uniquely solvable and under which their solutions contain the to-be-identified direct causal effects as components. We also state sufficient lag-based criteria under which the previously mentioned graphical conditions are satisfied and, thus, under which direct causal effects are identifiable. Several numerical experiments underline the correctness and applicability of our results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextArena</title>
<link>https://arxiv.org/abs/2504.11442</link>
<guid>https://arxiv.org/abs/2504.11442</guid>
<content:encoded><![CDATA[
arXiv:2504.11442v2 Announce Type: replace-cross 
Abstract: TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterogeneous networks in drug-target interaction prediction</title>
<link>https://arxiv.org/abs/2504.16152</link>
<guid>https://arxiv.org/abs/2504.16152</guid>
<content:encoded><![CDATA[
arXiv:2504.16152v2 Announce Type: replace-cross 
Abstract: Drug discovery requires a tremendous amount of time and cost. Computational drug-target interaction prediction, a significant part of this process, can reduce these requirements by narrowing the search space for wet lab experiments. In this survey, we provide comprehensive details of graph machine learning-based methods in predicting drug-target interaction, as they have shown promising results in this field. These details include the overall framework, main contribution, datasets, and their source codes. The selected papers were mainly published from 2020 to 2024. Prior to discussing papers, we briefly introduce the datasets commonly used with these methods and measurements to assess their performance. Finally, future challenges and some crucial areas that need to be explored are discussed.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Langevin sampling algorithm inspired by the Adam optimizer</title>
<link>https://arxiv.org/abs/2504.18911</link>
<guid>https://arxiv.org/abs/2504.18911</guid>
<content:encoded><![CDATA[
arXiv:2504.18911v2 Announce Type: replace-cross 
Abstract: We present a framework for adaptive-stepsize MCMC sampling based on time-rescaled Langevin dynamics, in which the stepsize variation is dynamically driven by an additional degree of freedom. Our approach augments the phase space by an additional variable which in turn defines a time reparameterization. The use of an auxiliary relaxation equation allows accumulation of a moving average of a local monitor function and provides for precise control of the timestep while circumventing the need to modify the drift term in the physical system. Our algorithm is straightforward to implement and can be readily combined with any off-the-peg fixed-stepsize Langevin integrator. As a particular example, we consider control of the stepsize by monitoring the norm of the log-posterior gradient, which takes inspiration from the Adam optimizer, the stepsize being automatically reduced in regions of steep change of the log posterior and increased on plateaus, improving numerical stability and convergence speed. As in Adam, the stepsize variation depends on the recent history of the gradient norm, which enhances stability and improves accuracy compared to more immediate control approaches. We demonstrate the potential benefit of this method--both in accuracy and in stability--in numerical experiments including Neal's funnel and a Bayesian neural network for classification of MNIST data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explanatory Summarization with Discourse-Driven Planning</title>
<link>https://arxiv.org/abs/2504.19339</link>
<guid>https://arxiv.org/abs/2504.19339</guid>
<content:encoded><![CDATA[
arXiv:2504.19339v3 Announce Type: replace-cross 
Abstract: Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction</title>
<link>https://arxiv.org/abs/2505.00237</link>
<guid>https://arxiv.org/abs/2505.00237</guid>
<content:encoded><![CDATA[
arXiv:2505.00237v2 Announce Type: replace-cross 
Abstract: This paper proposes an integrated approach for the safe and efficient control of mobile robots in dynamic and uncertain environments. The approach consists of two key steps: one-shot multimodal motion prediction to anticipate motions of dynamic obstacles and model predictive control to incorporate these predictions into the motion planning process. Motion prediction is driven by an energy-based neural network that generates high-resolution, multi-step predictions in a single operation. The prediction outcomes are further utilized to create geometric shapes formulated as mathematical constraints. Instead of treating each dynamic obstacle individually, predicted obstacles are grouped by proximity in an unsupervised way to improve performance and efficiency. The overall collision-free navigation is handled by model predictive control with a specific design for proactive dynamic obstacle avoidance. The proposed approach allows mobile robots to navigate effectively in dynamic environments. Its performance is accessed across various scenarios that represent typical warehouse settings. The results demonstrate that the proposed approach outperforms other existing dynamic obstacle avoidance methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rank-One Modified Value Iteration</title>
<link>https://arxiv.org/abs/2505.01828</link>
<guid>https://arxiv.org/abs/2505.01828</guid>
<content:encoded><![CDATA[
arXiv:2505.01828v2 Announce Type: replace-cross 
Abstract: In this paper, we provide a novel algorithm for solving planning and learning problems of Markov decision processes. The proposed algorithm follows a policy iteration-type update by using a rank-one approximation of the transition probability matrix in the policy evaluation step. This rank-one approximation is closely related to the stationary distribution of the corresponding transition probability matrix, which is approximated using the power method. We provide theoretical guarantees for the convergence of the proposed algorithm to optimal (action-)value function with the same rate and computational complexity as the value iteration algorithm in the planning problem and as the Q-learning algorithm in the learning problem. Through our extensive numerical simulations, however, we show that the proposed algorithm consistently outperforms first-order algorithms and their accelerated versions for both planning and learning problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath</title>
<link>https://arxiv.org/abs/2505.03397</link>
<guid>https://arxiv.org/abs/2505.03397</guid>
<content:encoded><![CDATA[
arXiv:2505.03397v3 Announce Type: replace-cross 
Abstract: Qubit control protocols have traditionally leveraged a characterisation of the qubit-bath coupling via its power spectral density. Previous work proposed the inference of noise operators that characterise the influence of a classical bath using a grey-box approach that combines deep neural networks with physics-encoded layers. This overall structure is complex and poses challenges in scaling and real-time operations. Here, we show that no expensive neural networks are needed and that this noise operator description admits an efficient parameterisation. We refer to the resulting parameter space as the \textit{quantum feature space} of the qubit dynamics resulting from the coupled bath. We show that the Euclidean distance defined over the quantum feature space provides an effective method for classifying noise processes in the presence of a given set of controls. Using the quantum feature space as the input space for a simple machine learning algorithm (random forest, in this case), we demonstrate that it can effectively classify the stationarity and the broad class of noise processes perturbing a qubit. Finally, we explore how control pulse parameters map to the quantum feature space.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Flow Matching using Latent Variables</title>
<link>https://arxiv.org/abs/2505.04486</link>
<guid>https://arxiv.org/abs/2505.04486</guid>
<content:encoded><![CDATA[
arXiv:2505.04486v2 Announce Type: replace-cross 
Abstract: Flow matching models have shown great potential in image generation tasks among probabilistic generative models. However, most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present $\texttt{Latent-CFM}$, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that $\texttt{Latent-CFM}$ exhibits improved generation quality with significantly less training (up to $\sim 50\%$ less) and computation than state-of-the-art flow matching models by incorporating extracted data features using pretrained lightweight latent variable models. Moving beyond natural images to generating fields arising from processes governed by physics, using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features, which adds interpretability to the generation process.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustly optimal dynamics for active matter reservoir computing</title>
<link>https://arxiv.org/abs/2505.05420</link>
<guid>https://arxiv.org/abs/2505.05420</guid>
<content:encoded><![CDATA[
arXiv:2505.05420v2 Announce Type: replace-cross 
Abstract: Information processing abilities of active matter are studied in the reservoir computing (RC) paradigm to infer the future state of a chaotic signal. We uncover an exceptional regime of agent dynamics that has been overlooked previously. It appears robustly optimal for performance under many conditions, thus providing valuable insights into computation with physical systems more generally. The key to forming effective mechanisms for information processing appears in the system's intrinsic relaxation abilities. These are probed without actually enforcing a specific inference goal. The dynamical regime that achieves optimal computation is located just below a critical damping threshold, involving a relaxation with multiple stages, and is readable at the single-particle level. At the many-body level, it yields substrates robustly optimal for RC across varying physical parameters and inference tasks. A system in this regime exhibits a strong diversity of dynamic mechanisms under highly fluctuating driving forces. Correlations of agent dynamics can express a tight relationship between the responding system and the fluctuating forces driving it. As this model is interpretable in physical terms, it facilitates re-framing inquiries regarding learning and unconventional computing with a fresh rationale for many-body physics out of equilibrium.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates</title>
<link>https://arxiv.org/abs/2505.05940</link>
<guid>https://arxiv.org/abs/2505.05940</guid>
<content:encoded><![CDATA[
arXiv:2505.05940v2 Announce Type: replace-cross 
Abstract: Modal methods for simulating vibrations of strings, membranes, and plates are widely used in acoustics and physically informed audio synthesis. However, traditional implementations, particularly for non-linear models like the von K\'arm\'an plate, are computationally demanding and lack differentiability, limiting inverse modelling and real-time applications. We introduce a fast, differentiable, GPU-accelerated modal framework built with the JAX library, providing efficient simulations and enabling gradient-based inverse modelling. Benchmarks show that our approach significantly outperforms CPU and GPU-based implementations, particularly for simulations with many modes. Inverse modelling experiments demonstrate that our approach can recover physical parameters, including tension, stiffness, and geometry, from both synthetic and experimental data. Although fitting physical parameters is more sensitive to initialisation compared to other methods, it provides greater interpretability and more compact parameterisation. The code is released as open source to support future research and applications in differentiable physical modelling and sound synthesis.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction</title>
<link>https://arxiv.org/abs/2505.08266</link>
<guid>https://arxiv.org/abs/2505.08266</guid>
<content:encoded><![CDATA[
arXiv:2505.08266v2 Announce Type: replace-cross 
Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called Graph Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Malicious Clients Detection in Federated Learning</title>
<link>https://arxiv.org/abs/2505.09110</link>
<guid>https://arxiv.org/abs/2505.09110</guid>
<content:encoded><![CDATA[
arXiv:2505.09110v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables multiple clients to collaboratively train a global machine learning model without sharing their raw data. However, the decentralized nature of FL introduces vulnerabilities, particularly to poisoning attacks, where malicious clients manipulate their local models to disrupt the training process. While Byzantine-robust aggregation rules have been developed to mitigate such attacks, they remain inadequate against more advanced threats. In response, recent advancements have focused on FL detection techniques to identify potentially malicious participants. Unfortunately, these methods often misclassify numerous benign clients as threats or rely on unrealistic assumptions about the server's capabilities. In this paper, we propose a novel algorithm, SafeFL, specifically designed to accurately identify malicious clients in FL. The SafeFL approach involves the server collecting a series of global models to generate a synthetic dataset, which is then used to distinguish between malicious and benign models based on their behavior. Extensive testing demonstrates that SafeFL outperforms existing methods, offering superior efficiency and accuracy in detecting malicious clients.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRoQ-LoCO: Generalist and Robot-agnostic Quadruped Locomotion Control using Offline Datasets</title>
<link>https://arxiv.org/abs/2505.10973</link>
<guid>https://arxiv.org/abs/2505.10973</guid>
<content:encoded><![CDATA[
arXiv:2505.10973v3 Announce Type: replace-cross 
Abstract: Recent advancements in large-scale offline training have demonstrated the potential of generalist policy learning for complex robotic tasks. However, applying these principles to legged locomotion remains a challenge due to continuous dynamics and the need for real-time adaptation across diverse terrains and robot morphologies. In this work, we propose GRoQ-LoCO, a scalable, attention-based framework that learns a single generalist locomotion policy across multiple quadruped robots and terrains, relying solely on offline datasets. Our approach leverages expert demonstrations from two distinct locomotion behaviors - stair traversal (non-periodic gaits) and flat terrain traversal (periodic gaits) - collected across multiple quadruped robots, to train a generalist model that enables behavior fusion. Crucially, our framework operates solely on proprioceptive data from all robots without incorporating any robot-specific encodings. The policy is directly deployable on an Intel i7 nuc, producing low-latency control outputs without any test-time optimization. Our extensive experiments demonstrate zero-shot transfer across highly diverse quadruped robots and terrains, including hardware deployment on the Unitree Go1, a commercially available 12kg robot. Notably, we evaluate challenging cross-robot training setups where different locomotion skills are unevenly distributed across robots, yet observe successful transfer of both flat walking and stair traversal behaviors to all robots at test time. We also show preliminary walking on Stoch 5, a 70kg quadruped, on flat and outdoor terrains without requiring any fine tuning. These results demonstrate the potential of offline, data-driven learning to generalize locomotion across diverse quadruped morphologies and behaviors.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLOT: Sample-specific Language Model Optimization at Test-time</title>
<link>https://arxiv.org/abs/2505.12392</link>
<guid>https://arxiv.org/abs/2505.12392</guid>
<content:encoded><![CDATA[
arXiv:2505.12392v2 Announce Type: replace-cross 
Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at https://github.com/maple-research-lab/SLOT.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Latent Diffusion Models with Interacting Particle Algorithms</title>
<link>https://arxiv.org/abs/2505.12412</link>
<guid>https://arxiv.org/abs/2505.12412</guid>
<content:encoded><![CDATA[
arXiv:2505.12412v2 Announce Type: replace-cross 
Abstract: We introduce a novel particle-based algorithm for end-to-end training of latent diffusion models. We reformulate the training task as minimizing a free energy functional and obtain a gradient flow that does so. By approximating the latter with a system of interacting particles, we obtain the algorithm, which we underpin theoretically by providing error guarantees. The novel algorithm compares favorably in experiments with previous particle-based methods and variational inference analogues.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R3: Robust Rubric-Agnostic Reward Models</title>
<link>https://arxiv.org/abs/2505.13388</link>
<guid>https://arxiv.org/abs/2505.13388</guid>
<content:encoded><![CDATA[
arXiv:2505.13388v2 Announce Type: replace-cross 
Abstract: Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Program Quantum Measurements for Machine Learning</title>
<link>https://arxiv.org/abs/2505.13525</link>
<guid>https://arxiv.org/abs/2505.13525</guid>
<content:encoded><![CDATA[
arXiv:2505.13525v2 Announce Type: replace-cross 
Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML) have sparked significant interest, driving extensive exploration of quantum machine learning (QML) algorithms to address a wide range of complex challenges. The development of high-performance QML models requires expert-level expertise, presenting a key challenge to the widespread adoption of QML. Critical obstacles include the design of effective data encoding strategies and parameterized quantum circuits, both of which are vital for the performance of QML models. Furthermore, the measurement process is often neglected-most existing QML models employ predefined measurement schemes that may not align with the specific requirements of the targeted problem. We propose an innovative framework that renders the observable of a quantum system-specifically, the Hermitian matrix-trainable. This approach employs an end-to-end differentiable learning framework, enabling simultaneous optimization of the neural network used to program the parameterized observables and the standard quantum circuit parameters. Notably, the quantum observable parameters are dynamically programmed by the neural network, allowing the observables to adapt in real time based on the input data stream. Through numerical simulations, we demonstrate that the proposed method effectively programs observables dynamically within variational quantum circuits, achieving superior results compared to existing approaches. Notably, it delivers enhanced performance metrics, such as higher classification accuracy, thereby significantly improving the overall effectiveness of QML models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2505.15634</link>
<guid>https://arxiv.org/abs/2505.15634</guid>
<content:encoded><![CDATA[
arXiv:2505.15634v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate the ability to solve reasoning and mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT length, as seen in models such as DeepSeek-R1, significantly enhances this reasoning for complex problems, but requires costly and high-quality long CoT data and fine-tuning. This work, inspired by the deep thinking paradigm of DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of an LLM without external datasets. Our method first employs Sparse Autoencoders (SAEs) to extract interpretable features from vanilla CoT. These features are then used to steer the LLM's internal states during generation. Recognizing that many LLMs do not have corresponding pre-trained SAEs, we further introduce a novel SAE-free steering algorithm, which directly computes steering directions from the residual activations of an LLM, obviating the need for an explicit SAE. Experimental results demonstrate that both our SAE-based and subsequent SAE-free steering algorithms significantly enhance the reasoning capabilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities</title>
<link>https://arxiv.org/abs/2505.15692</link>
<guid>https://arxiv.org/abs/2505.15692</guid>
<content:encoded><![CDATA[
arXiv:2505.15692v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model's output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance ("thought patterns"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO's potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.14005</link>
<guid>https://arxiv.org/abs/2505.14005</guid>
<content:encoded><![CDATA[
<div> XGNN, explainability, GNN, reliability, credibility <br />
<br />
Summary: 
The article introduces OPEN, a Comprehensive and Prerequisite-free Explainer for GNNs, addressing limitations in current XGNN methods. OPEN can capture the complete decision logic of GNNs across diverse dataset distributions without strict prerequisites. It partitions the dataset into different environments, each with distinct graph distributions, and learns GNN decision logic by analyzing predictions on sampled subgraphs. Experimental results show that OPEN outperforms existing methods in fidelity, efficiency, and robustness in real-world scenarios. This novel approach enhances the transparency and reliability of GNNs while providing valuable insights into their decision-making processes. <div>
arXiv:2505.14005v2 Announce Type: replace 
Abstract: To enhance the reliability and credibility of graph neural networks (GNNs) and improve the transparency of their decision logic, a new field of explainability of GNNs (XGNN) has emerged. However, two major limitations severely degrade the performance and hinder the generalizability of existing XGNN methods: they (a) fail to capture the complete decision logic of GNNs across diverse distributions in the entire dataset's sample space, and (b) impose strict prerequisites on edge properties and GNN internal accessibility. To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as the first work in the literature, can infer and partition the entire dataset's sample space into multiple environments, each containing graphs that follow a distinct distribution. OPEN further learns the decision logic of GNNs across different distributions by sampling subgraphs from each environment and analyzing their predictions, thus eliminating the need for strict prerequisites. Experimental results demonstrate that OPEN captures nearly complete decision logic of GNNs, outperforms state-of-the-art methods in fidelity while maintaining similar efficiency, and enhances robustness in real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting</title>
<link>https://arxiv.org/abs/2505.14555</link>
<guid>https://arxiv.org/abs/2505.14555</guid>
<content:encoded><![CDATA[
<div> physics-guided deep learning, weather forecasting, numerical weather prediction, deep learning models, PhyDL-NWP

Summary:
PhyDL-NWP is a framework that combines physical equations with deep learning to improve weather forecasting. It addresses the computational inefficiency and lack of physical completeness in traditional methods by integrating physical laws into data-driven models. This approach allows for the prediction of weather variables at any spatiotemporal location, while also computing physical terms through automatic differentiation. By using a physics-informed loss function, PhyDL-NWP ensures that predictions align with governing dynamics, improving both forecasting performance and physical consistency. The framework enables resolution-free downscaling by modeling weather as a continuous function and fine-tuning pre-trained models with minimal overhead. It achieves significantly faster inference speeds and requires fewer parameters, making it an efficient and accurate tool for weather forecasting. <div>
arXiv:2505.14555v2 Announce Type: replace 
Abstract: Weather forecasting is essential but remains computationally intensive and physically incomplete in traditional numerical weather prediction (NWP) methods. Deep learning (DL) models offer efficiency and accuracy but often ignore physical laws, limiting interpretability and generalization. We propose PhyDL-NWP, a physics-guided deep learning framework that integrates physical equations with latent force parameterization into data-driven models. It predicts weather variables from arbitrary spatiotemporal coordinates, computes physical terms via automatic differentiation, and uses a physics-informed loss to align predictions with governing dynamics. PhyDL-NWP enables resolution-free downscaling by modeling weather as a continuous function and fine-tunes pre-trained models with minimal overhead, achieving up to 170x faster inference with only 55K parameters. Experiments show that PhyDL-NWP improves both forecasting performance and physical consistency.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Log-Concavity and Score Regularity: Improved Convergence Bounds for Score-Based Generative Models in W2-distance</title>
<link>https://arxiv.org/abs/2501.02298</link>
<guid>https://arxiv.org/abs/2501.02298</guid>
<content:encoded><![CDATA[
<div> framework, analysis, convergence, regularization, data distribution<br />
Summary:<br />
Score-based Generative Models (SGMs) aim to sample from a target distribution by learning score functions using samples perturbed by Gaussian noise. Existing convergence bounds for SGMs in the $\mathcal{W}_2$-distance rely on stringent assumptions about the data distribution. In this work, a novel framework for analyzing $\mathcal{W}_2$-convergence in SGMs is presented, relaxing traditional assumptions such as log-concavity and score regularity. The regularization properties of the Ornstein--Uhlenbeck (OU) process are leveraged to show that weak log-concavity of the data distribution evolves into log-concavity over time. This transition is rigorously quantified through a PDE-based analysis of the Hamilton--Jacobi--Bellman equation. It is established that the drift of the time-reversed OU process alternates between contractive and non-contractive regimes, reflecting the dynamics of concavity. This approach avoids stringent regularity conditions on the score function and its estimators, relying on more practical assumptions. The framework's applicability is demonstrated through computations on Gaussian mixture models, showcasing its versatility and potential for broader data distribution classes.<br /> <div>
arXiv:2501.02298v3 Announce Type: replace-cross 
Abstract: Score-based Generative Models (SGMs) aim to sample from a target distribution by learning score functions using samples perturbed by Gaussian noise. Existing convergence bounds for SGMs in the $\mathcal{W}_2$-distance rely on stringent assumptions about the data distribution. In this work, we present a novel framework for analyzing $\mathcal{W}_2$-convergence in SGMs, significantly relaxing traditional assumptions such as log-concavity and score regularity. Leveraging the regularization properties of the Ornstein--Uhlenbeck (OU) process, we show that weak log-concavity of the data distribution evolves into log-concavity over time. This transition is rigorously quantified through a PDE-based analysis of the Hamilton--Jacobi--Bellman equation governing the log-density of the forward process. Moreover, we establish that the drift of the time-reversed OU process alternates between contractive and non-contractive regimes, reflecting the dynamics of concavity. Our approach circumvents the need for stringent regularity conditions on the score function and its estimators, relying instead on milder, more practical assumptions. We demonstrate the wide applicability of this framework through explicit computations on Gaussian mixture models, illustrating its versatility and potential for broader classes of data distributions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Normalized Cut with Reinforcement Learning in Constrained Action Space</title>
<link>https://arxiv.org/abs/2505.13986</link>
<guid>https://arxiv.org/abs/2505.13986</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, combinatorial optimization, constrained action spaces, normalized cut problem, transportation networks

Summary:
This paper introduces a novel approach using reinforcement learning (RL) with constrained action spaces to solve combinatorial optimization problems, specifically focusing on the normalized cut problem. By utilizing a Wedge and Ring Transformer in the context of transportation networks, the RL solution aims to guide graph partitions towards predefined template instances, such as Wedges and Rings, leading to partitions closer to optimal solutions. This approach demonstrates the potential of integrating external knowledge into RL algorithms to improve solution outcomes in specific domains. The proposed method can be generalized to other problem domains, highlighting the versatility and effectiveness of incorporating domain-specific constraints in RL-based combinatorial optimization. <div>
arXiv:2505.13986v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve combinatorial optimization problems primarily due to its ability to learn heuristics that can generalize across problem instances. However, integrating external knowledge that will steer combinatorial optimization problem solutions towards domain appropriate outcomes remains an extremely challenging task. In this paper, we propose the first RL solution that uses constrained action spaces to guide the normalized cut problem towards pre-defined template instances. Using transportation networks as an example domain, we create a Wedge and Ring Transformer that results in graph partitions that are shaped in form of Wedges and Rings and which are likely to be closer to natural optimal partitions. However, our approach is general as it is based on principles that can be generalized to other domains.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14238</link>
<guid>https://arxiv.org/abs/2505.14238</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Parameter-Efficient Fine-Tuning, ABBA architecture, expressivity, state-of-the-art results

Summary:
ABBA is a new Parameter-Efficient Fine-Tuning (PEFT) architecture that improves adaptability of Large Language Models to new domains. Unlike existing methods, ABBA fully decouples updates from pre-trained weights, allowing for independent optimization of components. This enhances expressivity and performance, as validated through matrix reconstruction experiments. ABBA surpasses previous PEFT methods on arithmetic and commonsense reasoning benchmarks, achieving state-of-the-art results across various models. The architecture's code is publicly available on GitHub for further exploration and implementation. By reparameterizing updates as a Hadamard product of two learnable low-rank matrices, ABBA offers significant advantages in terms of expressivity and efficiency, making it a promising approach for fine-tuning large language models in diverse domains.<br /><br />Summary: <div>
arXiv:2505.14238v2 Announce Type: replace-cross 
Abstract: Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA's expressive capacity and validate its advantages through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: https://github.com/CERT-Lab/abba.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Large Language Model Usability Across Resource-Constrained</title>
<link>https://arxiv.org/abs/2505.17040</link>
<guid>https://arxiv.org/abs/2505.17040</guid>
<content:encoded><![CDATA[
<div> alignment framework, multimodal, robustness, low-resource, adaptability

Summary:
- The dissertation focuses on enhancing the usability of Large Language Models (LLMs) in real-world scenarios.
- It introduces a text-centric alignment framework to integrate diverse modalities seamlessly.
- An adversarial prompting technique is proposed to improve robustness against noisy and missing modalities.
- Inference-time optimization strategies, such as prompt search and uncertainty quantification, are investigated to enhance LLM performance.
- The work also addresses low-resource domains, achieving state-of-the-art performance in Verilog code generation using synthetic data pipelines and logic-enhanced reasoning models.
- Overall, the dissertation aims to improve the adaptability, scalability, and efficiency of LLMs under practical constraints.<br /><br /> <div>
arXiv:2505.17040v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, and recent efforts have sought to extend their capabilities to multimodal domains and resource-constrained environments. However, existing approaches often rely on costly supervised fine-tuning or assume fixed training conditions, limiting their generalization when facing unseen modalities, limited data, or restricted compute resources. This dissertation presents a systematic study toward generalizing LLM usability under real-world constraints. First, it introduces a robust text-centric alignment framework that enables LLMs to seamlessly integrate diverse modalities-including text, images, tables, and any modalities - via natural language interfaces. This approach supports in-context adaptation to unseen or dynamically changing modalities without requiring retraining. To enhance robustness against noisy and missing modalities, an adversarial prompting technique is proposed, generating semantically challenging perturbations at the prompt level to stress-test model reliability. Beyond multimodal setting, the dissertation investigates inference-time optimization strategies for LLMs, leveraging prompt search and uncertainty quantification to improve performance without additional model training. This perspective offers an efficient alternative to scaling model parameters or retraining from scratch. Additionally, the work addresses low-resource domains such as Verilog code generation by designing correct-by-construction synthetic data pipelines and logic-enhanced reasoning models, achieving state-of-the-art performance with minimal data. Together, these contributions form a unified effort to enhance the adaptability, scalability, and efficiency of large language models under practical constraints.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAP: Runtime-Adaptive Pruning for LLM Inference</title>
<link>https://arxiv.org/abs/2505.17138</link>
<guid>https://arxiv.org/abs/2505.17138</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, compression, reinforcement learning, adaptive pruning, runtime memory

Summary:
- Large language models (LLMs) are powerful but computationally intensive, hindering deployment.
- Existing compression methods for LLMs often fail to adapt to runtime memory variations or heterogeneous KV-cache demands.
- RAP is a dynamic pruning framework driven by reinforcement learning (RL) that adjusts compression strategies based on immediate memory constraints and workload demands.
- RAP tracks the evolving ratio between model parameters and KV-cache during execution, focusing on retaining components that maximize utility within the current memory budget.
- Experimental results show that RAP surpasses state-of-the-art baselines by dynamically considering both model weights and KV-cache in real-time. 

<br /><br />Summary: 

Large language models have significant computational and memory requirements that limit their deployment. Current compression methods for these models struggle to adapt to changing memory conditions and diverse user requests. To address these limitations, RAP, a novel framework driven by reinforcement learning, dynamically adjusts compression strategies based on real-time memory constraints and workload demands. By tracking the ratio between model parameters and KV-cache and retaining components that maximize utility within current memory limits, RAP outperforms existing methods by considering both model weights and KV-cache dynamically. <div>
arXiv:2505.17138v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at language understanding and generation, but their enormous computational and memory requirements hinder deployment. Compression offers a potential solution to mitigate these constraints. However, most existing methods rely on fixed heuristics and thus fail to adapt to runtime memory variations or heterogeneous KV-cache demands arising from diverse user requests. To address these limitations, we propose RAP, an elastic pruning framework driven by reinforcement learning (RL) that dynamically adjusts compression strategies in a runtime-aware manner. Specifically, RAP dynamically tracks the evolving ratio between model parameters and KV-cache across practical execution. Recognizing that FFNs house most parameters, whereas parameter -light attention layers dominate KV-cache formation, the RL agent retains only those components that maximize utility within the current memory budget, conditioned on instantaneous workload and device state. Extensive experiments results demonstrate that RAP outperforms state-of-the-art baselines, marking the first time to jointly consider model weights and KV-cache on the fly.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSTH-Sleep: Towards Effective Few-Shot Sleep Stage Classification with Spatial-Temporal Hypergraph Enhanced Meta-Learning</title>
<link>https://arxiv.org/abs/2505.17142</link>
<guid>https://arxiv.org/abs/2505.17142</guid>
<content:encoded><![CDATA[
<div> deep learning, sleep stage classification, bio-signals, meta-learning, spatial-temporal hypergraph

Summary: 
MetaSTH-Sleep is introduced as a few-shot sleep stage classification framework using spatial-temporal hypergraph enhanced meta-learning. This approach addresses challenges faced in accurate classification of sleep stages such as limited annotated data, inter-individual variability, and neglect of high-order signal relationships. By enabling rapid adaptation to new subjects with minimal labeled samples, MetaSTH-Sleep improves model performance and generalization. The use of hypergraph structure effectively captures spatial interconnections and temporal dynamics in EEG signals, enhancing the understanding of signal heterogeneity. Experimental results showcase significant performance enhancements across diverse subjects, providing valuable insights for clinicians in sleep stage annotation. <div>
arXiv:2505.17142v1 Announce Type: new 
Abstract: Accurate classification of sleep stages based on bio-signals is fundamental for automatic sleep stage annotation. Traditionally, this task relies on experienced clinicians to manually annotate data, a process that is both time-consuming and labor-intensive. In recent years, deep learning methods have shown promise in automating this task. However, three major challenges remain: (1) deep learning models typically require large-scale labeled datasets, making them less effective in real-world settings where annotated data is limited; (2) significant inter-individual variability in bio-signals often results in inconsistent model performance when applied to new subjects, limiting generalization; and (3) existing approaches often overlook the high-order relationships among bio-signals, failing to simultaneously capture signal heterogeneity and spatial-temporal dependencies. To address these issues, we propose MetaSTH-Sleep, a few-shot sleep stage classification framework based on spatial-temporal hypergraph enhanced meta-learning. Our approach enables rapid adaptation to new subjects using only a few labeled samples, while the hypergraph structure effectively models complex spatial interconnections and temporal dynamics simultaneously in EEG signals. Experimental results demonstrate that MetaSTH-Sleep achieves substantial performance improvements across diverse subjects, offering valuable insights to support clinicians in sleep stage annotation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training of Neural SDEs Using Stochastic Optimal Control</title>
<link>https://arxiv.org/abs/2505.17150</link>
<guid>https://arxiv.org/abs/2505.17150</guid>
<content:encoded><![CDATA[
<div> control theory, variational inference, neural stochastic differential equations, stochastic optimal control, training efficiency

Summary:
The article introduces a hierarchical method for variational inference for neural stochastic differential equations (SDEs), inspired by control theory. By decomposing the control term into linear and non-linear components, the authors derive an optimal control term for linear SDEs using stochastic optimal control. The non-linear component is modeled by a neural network, allowing for efficient training of neural SDEs without sacrificing expressive power. Training is initialized at a lower cost since the linear part of the control term is optimal and does not need to be learned. This approach leads to faster convergence in training neural SDEs, making uncertainty-aware reasoning in time-series more computationally feasible. <div>
arXiv:2505.17150v1 Announce Type: new 
Abstract: We present a hierarchical, control theory inspired method for variational inference (VI) for neural stochastic differential equations (SDEs). While VI for neural SDEs is a promising avenue for uncertainty-aware reasoning in time-series, it is computationally challenging due to the iterative nature of maximizing the ELBO. In this work, we propose to decompose the control term into linear and residual non-linear components and derive an optimal control term for linear SDEs, using stochastic optimal control. Modeling the non-linear component by a neural network, we show how to efficiently train neural SDEs without sacrificing their expressive power. Since the linear part of the control term is optimal and does not need to be learned, the training is initialized at a lower cost and we observe faster convergence.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.17155</link>
<guid>https://arxiv.org/abs/2505.17155</guid>
<content:encoded><![CDATA[
<div> compression, reasoning models, test-time scaling, verifier-based framework, inference efficiency

Summary:
TrimR is a new framework designed to improve the efficiency of Large Reasoning Models (LRMs) by dynamically compressing their Chain-of-Thought reasoning process. LRMs often generate redundant thinking processes, leading to inefficiencies during inference. TrimR uses a pre-trained verifier to detect and truncate these redundant thoughts without the need for fine-tuning. The framework is specifically engineered for production-level deployment and has been shown to enhance test-time scaling and inference efficiency on various benchmarks. Empirical evaluations on different LRMs demonstrate up to a 70% improvement in reasoning runtime without sacrificing accuracy. TrimR offers a solution to the decoding overhead incurred by LRMs using test-time scaling methods, making it suitable for high-throughput industrial applications. <div>
arXiv:2505.17155v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) demonstrate exceptional capability in tackling complex mathematical, logical, and coding tasks by leveraging extended Chain-of-Thought (CoT) reasoning. Test-time scaling methods, such as prolonging CoT with explicit token-level exploration, can push LRMs' accuracy boundaries, but they incur significant decoding overhead. A key inefficiency source is LRMs often generate redundant thinking CoTs, which demonstrate clear structured overthinking and underthinking patterns. Inspired by human cognitive reasoning processes and numerical optimization theories, we propose TrimR, a verifier-based, training-free, efficient framework for dynamic CoT compression to trim reasoning and enhance test-time scaling, explicitly tailored for production-level deployment. Our method employs a lightweight, pretrained, instruction-tuned verifier to detect and truncate redundant intermediate thoughts of LRMs without any LRM or verifier fine-tuning. We present both the core algorithm and asynchronous online system engineered for high-throughput industrial applications. Empirical evaluations on Ascend NPUs and vLLM show that our framework delivers substantial gains in inference efficiency under large-batch workloads. In particular, on the four MATH500, AIME24, AIME25, and GPQA benchmarks, the reasoning runtime of Pangu-R-38B, QwQ-32B, and DeepSeek-R1-Distill-Qwen-32B is improved by up to 70% with negligible impact on accuracy.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning</title>
<link>https://arxiv.org/abs/2505.17163</link>
<guid>https://arxiv.org/abs/2505.17163</guid>
<content:encoded><![CDATA[
<div> OCR-Reasoning, multimodal slow-thinking systems, benchmark, reasoning tasks, text-rich image reasoning.  
Summary:  
OCR-Reasoning is a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on text-rich image reasoning tasks. It consists of 1,069 human-annotated examples covering 6 core reasoning abilities and 18 practical reasoning tasks. Unlike other benchmarks, OCR-Reasoning annotates both the final answers and the reasoning process, allowing for a more comprehensive analysis of model performance. Evaluation of state-of-the-art MLLMs on this benchmark revealed significant challenges, with none achieving accuracy above 50%. This highlights the urgent need to address the difficulties in text-rich image reasoning tasks. The benchmark and evaluation scripts are publicly available, providing a valuable resource for researchers in this field. <br /><br />Summary: <div>
arXiv:2505.17163v1 Announce Type: new 
Abstract: Recent advancements in multimodal slow-thinking systems have demonstrated remarkable performance across diverse visual reasoning tasks. However, their capabilities in text-rich image reasoning tasks remain understudied due to the lack of a systematic benchmark. To address this gap, we propose OCR-Reasoning, a comprehensive benchmark designed to systematically assess Multimodal Large Language Models on text-rich image reasoning tasks. The benchmark comprises 1,069 human-annotated examples spanning 6 core reasoning abilities and 18 practical reasoning tasks in text-rich visual scenarios. Furthermore, unlike other text-rich image understanding benchmarks that only annotate the final answers, OCR-Reasoning also annotates the reasoning process simultaneously. With the annotated reasoning process and the final answers, OCR-Reasoning evaluates not only the final answers generated by models but also their reasoning processes, enabling a holistic analysis of their problem-solving abilities. Leveraging this benchmark, we conducted a comprehensive evaluation of state-of-the-art MLLMs. Our results demonstrate the limitations of existing methodologies. Notably, even state-of-the-art MLLMs exhibit substantial difficulties, with none achieving accuracy surpassing 50\% across OCR-Reasoning, indicating that the challenges of text-rich image reasoning are an urgent issue to be addressed. The benchmark and evaluation scripts are available at https://github.com/SCUT-DLVCLab/OCR-Reasoning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms</title>
<link>https://arxiv.org/abs/2505.17190</link>
<guid>https://arxiv.org/abs/2505.17190</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic programming, Tropical attention, Neural Algorithmic Reasoning, Max-plus semiring, Adversarial attacks

Summary:
Dynamic programming algorithms for optimization problems typically use max-plus semirings for value functions, but existing Neural Algorithmic Reasoning models rely on softmax attention, which can blur sharp polyhedral structures. This paper introduces Tropical attention, a new attention function operating in the max-plus semiring, improving reasoning in out-of-distribution settings. Tropical attention approximates tropical circuits of DP-type algorithms and enhances OOD performance in algorithmic reasoning tasks. It surpasses softmax baselines in length and value generalization while remaining stable under adversarial attacks. The study also introduces adversarial-attack generalization as a benchmark for Neural Algorithmic Reasoning. Tropical attention restores sharp, scale-invariant reasoning that softmax lacks, improving performance and stability in challenging scenarios. 

<br /><br />Summary: <div>
arXiv:2505.17190v1 Announce Type: new 
Abstract: Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shape it Up! Restoring LLM Safety during Finetuning</title>
<link>https://arxiv.org/abs/2505.17196</link>
<guid>https://arxiv.org/abs/2505.17196</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, safety risks, dynamic shaping, guardrail models, safety assessment

Summary:
Finetuning large language models (LLMs) can lead to safety risks as even a few harmful examples can compromise alignment. To address this issue, the authors propose dynamic safety shaping (DSS) as a framework that fine-tunes models by reinforcing learning from safe segments of a response while suppressing unsafe content, instead of treating all parts equally. They introduce the idea of guardrail models being repurposed to evaluate partial responses, enabling the Safety Trajectory Assessment of Response (STAR) for token-level signals to operate dynamically during training. STAR-DSS, guided by STAR scores, effectively mitigates risks without sacrificing task performance across various threats and datasets. The research encourages future safety efforts to adopt dynamic shaping principles for enhanced mitigation against evolving finetuning risks. 

<br /><br />Summary: <div>
arXiv:2505.17196v1 Announce Type: new 
Abstract: Finetuning large language models (LLMs) enables user-specific customization but introduces critical safety risks: even a few harmful examples can compromise safety alignment. A common mitigation strategy is to update the model more strongly on examples deemed safe, while downweighting or excluding those flagged as unsafe. However, because safety context can shift within a single example, updating the model equally on both harmful and harmless parts of a response is suboptimal-a coarse treatment we term static safety shaping. In contrast, we propose dynamic safety shaping (DSS), a framework that uses fine-grained safety signals to reinforce learning from safe segments of a response while suppressing unsafe content. To enable such fine-grained control during finetuning, we introduce a key insight: guardrail models, traditionally used for filtering, can be repurposed to evaluate partial responses, tracking how safety risk evolves throughout the response, segment by segment. This leads to the Safety Trajectory Assessment of Response (STAR), a token-level signal that enables shaping to operate dynamically over the training sequence. Building on this, we present STAR-DSS, guided by STAR scores, that robustly mitigates finetuning risks and delivers substantial safety improvements across diverse threats, datasets, and model families-all without compromising capability on intended tasks. We encourage future safety research to build on dynamic shaping principles for stronger mitigation against evolving finetuning risks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LengthLogD: A Length-Stratified Ensemble Framework for Enhanced Peptide Lipophilicity Prediction via Multi-Scale Feature Integration</title>
<link>https://arxiv.org/abs/2505.17198</link>
<guid>https://arxiv.org/abs/2505.17198</guid>
<content:encoded><![CDATA[
<div> Keywords: peptide compounds, membrane permeability, predictive framework, molecular length stratification, ensemble learning

Summary: 
LengthLogD is a novel predictive framework designed to accurately predict the logD of peptide compounds, considering the impact of molecular weight and length on membrane permeability. This framework utilizes multi-scale molecular representations and employs a length-stratified strategy, along with an adaptive weight allocation mechanism for long peptides. Experimental results show superior performance in predicting logD values for short, medium, and long peptides, with a notable reduction in prediction errors for long peptides. The study highlights the significance of topological features and the effectiveness of the length-stratified approach, contributing to the overall improvement in predictive accuracy. Compared to existing models, LengthLogD offers precise logD predictions for peptide drug development, particularly showcasing its value in optimizing long peptide lead compounds. <br /><br />Summary: <div>
arXiv:2505.17198v1 Announce Type: new 
Abstract: Peptide compounds demonstrate considerable potential as therapeutic agents due to their high target affinity and low toxicity, yet their drug development is constrained by their low membrane permeability. Molecular weight and peptide length have significant effects on the logD of peptides, which in turn influences their ability to cross biological membranes. However, accurate prediction of peptide logD remains challenging due to the complex interplay between sequence, structure, and ionization states. This study introduces LengthLogD, a predictive framework that establishes specialized models through molecular length stratification while innovatively integrating multi-scale molecular representations. We constructed feature spaces across three hierarchical levels: atomic (10 molecular descriptors), structural (1024-bit Morgan fingerprints), and topological (3 graph-based features including Wiener index), optimized through stratified ensemble learning. An adaptive weight allocation mechanism specifically developed for long peptides significantly enhances model generalizability. Experimental results demonstrate superior performance across all categories: short peptides (R^2=0.855), medium peptides (R^2=0.816), and long peptides (R^2=0.882), with a 34.7% reduction in prediction error for long peptides compared to conventional single-model approaches. Ablation studies confirm: 1) The length-stratified strategy contributes 41.2% to performance improvement; 2) Topological features account for 28.5% of predictive importance. Compared to state-of-the-art models, our method maintains short peptide prediction accuracy while achieving a 25.7% increase in the coefficient of determination (R^2) for long peptides. This research provides a precise logD prediction tool for peptide drug development, particularly demonstrating unique value in optimizing long peptide lead compounds.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure and Private Federated Learning: Achieving Adversarial Resilience through Robust Aggregation</title>
<link>https://arxiv.org/abs/2505.17226</link>
<guid>https://arxiv.org/abs/2505.17226</guid>
<content:encoded><![CDATA[
<div> rKrum, Average-rKrum, Federated Learning, Byzantine attacks, Privacy-preserving AI
<br />
Summary: 
Average-rKrum (ArKrum) is introduced as a novel aggregation strategy to enhance the resilience and privacy guarantees of Federated Learning (FL) systems against Byzantine attacks from malicious participants. It incorporates a median-based filtering mechanism to remove outliers and estimate the number of adversarial clients accurately. Additionally, ArKrum applies a multi-update averaging scheme to improve stability and performance, especially in cases where client data distributions differ. Evaluation on image and text datasets under various Byzantine attack types demonstrates that ArKrum consistently achieves high accuracy and stability, performing comparably or better than existing robust aggregation methods. These results highlight ArKrum as an effective and practical solution for securing FL systems in adversarial environments. 
<br /> <div>
arXiv:2505.17226v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative machine learning across decentralized data sources without sharing raw data. It offers a promising approach to privacy-preserving AI. However, FL remains vulnerable to adversarial threats from malicious participants, referred to as Byzantine clients, who can send misleading updates to corrupt the global model. Traditional aggregation methods, such as simple averaging, are not robust to such attacks. More resilient approaches, like the Krum algorithm, require prior knowledge of the number of malicious clients, which is often unavailable in real-world scenarios. To address these limitations, we propose Average-rKrum (ArKrum), a novel aggregation strategy designed to enhance both the resilience and privacy guarantees of FL systems. Building on our previous work (rKrum), ArKrum introduces two key innovations. First, it includes a median-based filtering mechanism that removes extreme outliers before estimating the number of adversarial clients. Second, it applies a multi-update averaging scheme to improve stability and performance, particularly when client data distributions are not identical. We evaluate ArKrum on benchmark image and text datasets under three widely studied Byzantine attack types. Results show that ArKrum consistently achieves high accuracy and stability. It performs as well as or better than other robust aggregation methods. These findings demonstrate that ArKrum is an effective and practical solution for secure FL systems in adversarial environments.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Capability Evaluation of Foundation Models</title>
<link>https://arxiv.org/abs/2505.17228</link>
<guid>https://arxiv.org/abs/2505.17228</guid>
<content:encoded><![CDATA[
<div> framework, evaluation, foundation models, active learning, capability

Summary:
- The paper introduces the Active learning for Capability Evaluation (ACE) framework for evaluating foundation models in a scalable and automated manner.
- ACE decomposes domains into meaningful capabilities using language models, reducing human effort in creating evaluation tasks.
- The framework models a subject model's performance as a capability function over a semantic space and prioritizes evaluating informative capabilities using active learning.
- ACE's adaptive evaluation strategy allows for the discovery of strengths, weaknesses, and failure modes that static benchmarks may overlook.
- Results indicate that ACE offers a more comprehensive and informative assessment of model capabilities, crucial for the safe and informed deployment of foundation models. 

<br /><br />Summary: <div>
arXiv:2505.17228v1 Announce Type: new 
Abstract: Current evaluation frameworks for foundation models rely heavily on fixed, manually curated benchmarks, limiting their ability to capture the full breadth of model capabilities. This paper introduces Active learning for Capability Evaluation (ACE), a novel framework for scalable, automated, and fine-grained evaluation of foundation models. ACE leverages the knowledge embedded in powerful language models to decompose a domain into semantically meaningful capabilities and generate diverse evaluation tasks, significantly reducing human effort. To maximize coverage and efficiency, ACE models a subject model's performance as a capability function over a latent semantic space and uses active learning to prioritize the evaluation of the most informative capabilities. This adaptive evaluation strategy enables cost-effective discovery of strengths, weaknesses, and failure modes that static benchmarks may miss. Our results suggest that ACE provides a more complete and informative picture of model capabilities, which is essential for safe and well-informed deployment of foundation models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Aware Interpretable Multimodal Music Auto-Tagging</title>
<link>https://arxiv.org/abs/2505.17233</link>
<guid>https://arxiv.org/abs/2505.17233</guid>
<content:encoded><![CDATA[
<div> Keywords: music auto-tagging, interpretable framework, multimodal features, clustering, transparent system

Summary:
This article introduces a novel framework for music auto-tagging that focuses on interpretability. By combining various musically relevant features from signal processing, deep learning, ontology engineering, and natural language processing, the framework enhances the understanding of the tagging process. The method clusters these features based on their semantic meaning and utilizes an expectation maximization algorithm to assign weights to each group. This approach not only achieves competitive performance in music tagging but also provides insight into the decision-making process, making the system more transparent and user-centric. Overall, the framework offers a deeper understanding of how music auto-tagging algorithms work, improving trust and usability for both researchers and end-users.  

<br /><br />Summary: <div>
arXiv:2505.17233v1 Announce Type: new 
Abstract: Music auto-tagging is essential for organizing and discovering music in extensive digital libraries. While foundation models achieve exceptional performance in this domain, their outputs often lack interpretability, limiting trust and usability for researchers and end-users alike. In this work, we present an interpretable framework for music auto-tagging that leverages groups of musically meaningful multimodal features, derived from signal processing, deep learning, ontology engineering, and natural language processing. To enhance interpretability, we cluster features semantically and employ an expectation maximization algorithm, assigning distinct weights to each group based on its contribution to the tagging process. Our method achieves competitive tagging performance while offering a deeper understanding of the decision-making process, paving the way for more transparent and user-centric music tagging systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Policy Minimum Bayesian Risk</title>
<link>https://arxiv.org/abs/2505.17242</link>
<guid>https://arxiv.org/abs/2505.17242</guid>
<content:encoded><![CDATA[
<div> Inference scaling, LLMs, long-chain-of-thought generation, best-of-N sampling, minimum Bayes risk decoding<br />
<br />
Summary: In the study, a novel method for enhancing large language models' (LLMs') accuracy in solving complex reasoning problems through extended computation time is proposed. By incorporating reward and risk/similarity signals into minimum Bayes risk decoding (MBRD), the framework offers improved robustness, accuracy, and well-defined asymptotic behavior compared to traditional methods. This approach also allows for a sample-efficient variant of MBRD that adjusts the number of generated samples based on problem difficulty. Empirical results on math and coding tasks show the advantages of the proposed method using open-source models, along with a comprehensive analysis of accuracy-compute trade-offs. The method's effectiveness lies in leveraging optimal policy concepts in KL-controlled reinforcement learning to enhance LLM performance in generating multiple candidate solutions and aggregating them effectively. <br /><br />Summary: <div>
arXiv:2505.17242v1 Announce Type: new 
Abstract: Inference scaling can help LLMs solve complex reasoning problems through extended runtime computation. On top of targeted supervision for long chain-of-thought (long-CoT) generation, purely inference-time techniques such as best-of-N (BoN) sampling, majority voting, or more generally, minimum Bayes risk decoding (MBRD), can further improve LLM accuracy by generating multiple candidate solutions and aggregating over them. These methods typically leverage additional signals in the form of reward models and risk/similarity functions that compare generated samples, e.g., exact match in some normalized space or standard similarity metrics such as Rouge. Here we present a novel method for incorporating reward and risk/similarity signals into MBRD. Based on the concept of optimal policy in KL-controlled reinforcement learning, our framework provides a simple and well-defined mechanism for leveraging such signals, offering several advantages over traditional inference-time methods: higher robustness, improved accuracy, and well-understood asymptotic behavior. In addition, it allows for the development of a sample-efficient variant of MBRD that can adjust the number of samples to generate according to the difficulty of the problem, without relying on majority vote counts. We empirically demonstrate the advantages of our approach on math (MATH-$500$) and coding (HumanEval) tasks using recent open-source models. We also present a comprehensive analysis of its accuracy-compute trade-offs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoors in DRL: Four Environments Focusing on In-distribution Triggers</title>
<link>https://arxiv.org/abs/2505.17248</link>
<guid>https://arxiv.org/abs/2505.17248</guid>
<content:encoded><![CDATA[
<div> backdoor attacks, trojans, deep neural network models, reinforcement learning agents, security threat <br />
<br />
Summary: <br />
Backdoor attacks, also known as trojans, are a concerning security risk in deep neural network models, especially in open-source or third-party models. This research focuses on in-distribution triggers in deep reinforcement learning agents, which are more dangerous as they can be easily activated within the natural data distribution. The study develops trojans in four RL environments and finds that in-distribution triggers, though requiring more effort to implement, are still viable threats in DRL through basic data poisoning attacks. This research sheds light on the potential vulnerabilities in DRL models and the importance of understanding and mitigating backdoor attacks. <div>
arXiv:2505.17248v1 Announce Type: new 
Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable behavior in deep neural network models. Open-source neural networks are downloaded from the internet daily, possibly containing backdoors, and third-party model developers are common. To advance research on backdoor attack mitigation, we develop several trojans for deep reinforcement learning (DRL) agents. We focus on in-distribution triggers, which occur within the agent's natural data distribution, since they pose a more significant security threat than out-of-distribution triggers due to their ease of activation by the attacker during model deployment. We implement backdoor attacks in four reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld, Colorful Memory, and Modified Safety Gymnasium. We train various models, both clean and backdoored, to characterize these attacks. We find that in-distribution triggers can require additional effort to implement and be more challenging for models to learn, but are nevertheless viable threats in DRL even using basic data poisoning attacks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approach to Finding a Robust Deep Learning Model</title>
<link>https://arxiv.org/abs/2505.17254</link>
<guid>https://arxiv.org/abs/2505.17254</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, artificial intelligence, model robustness, deep learning models, training sample size<br />
Summary: <br />
The article introduces a new approach for assessing the robustness of machine learning models, particularly deep learning models. The approach includes a model selection algorithm and meta-algorithm that can be applied to any machine learning model. The study focuses on small models made up of convolutional and fully connected layers, using common optimizers. The impact of training sample size, model weight initialization, and inductive bias on model robustness is examined within this framework. The aim is to train models without human supervision while ensuring reliable predictions, addressing the growing demand for efficient and accurate model training in the field of artificial intelligence. <div>
arXiv:2505.17254v1 Announce Type: new 
Abstract: The rapid development of machine learning (ML) and artificial intelligence (AI) applications requires the training of large numbers of models. This growing demand highlights the importance of training models without human supervision, while ensuring that their predictions are reliable. In response to this need, we propose a novel approach for determining model robustness. This approach, supplemented with a proposed model selection algorithm designed as a meta-algorithm, is versatile and applicable to any machine learning model, provided that it is appropriate for the task at hand. This study demonstrates the application of our approach to evaluate the robustness of deep learning models. To this end, we study small models composed of a few convolutional and fully connected layers, using common optimizers due to their ease of interpretation and computational efficiency. Within this framework, we address the influence of training sample size, model weight initialization, and inductive bias on the robustness of deep learning models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model</title>
<link>https://arxiv.org/abs/2505.17257</link>
<guid>https://arxiv.org/abs/2505.17257</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, genomics, JanusDNA, bidirectional understanding, pretraining paradigm

Summary:
JanusDNA is a novel bidirectional DNA foundation model that addresses the challenges of applying large language models to genomics. By combining the efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling, JanusDNA can capture long-range dependencies within DNA sequences efficiently. It adopts a hybrid Mamba, Attention, and Mixture of Experts (MoE) architecture to achieve high performance on genomic representation benchmarks, outperforming models with significantly more parameters. JanusDNA can process up to 1 million base pairs with single nucleotide resolution on a single 80GB GPU, showcasing its scalability and computational efficiency. This innovative approach opens up new possibilities for utilizing large language models in genomics research. 

<br /><br />Summary: JanusDNA is a novel bidirectional DNA foundation model that efficiently captures long-range dependencies within DNA sequences by combining autoregressive and masked modeling approaches. It outperforms models with more parameters on genomic representation benchmarks and can process large sequences on a single GPU, showcasing its scalability and efficiency in genomics research. <div>
arXiv:2505.17257v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zebra-Llama: Towards Extremely Efficient Hybrid Models</title>
<link>https://arxiv.org/abs/2505.17272</link>
<guid>https://arxiv.org/abs/2505.17272</guid>
<content:encoded><![CDATA[
<div> Efficient Language Models, Hybrid Models, State Space Models, Multi-head Latent Attention, Knowledge Transfer

Summary:
Zebra-Llama proposes composing efficient hybrid language models from existing pre-trained models as a scalable alternative to expensive retraining. By combining State Space Models and Multi-head Latent Attention layers, Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using significantly fewer training tokens and smaller teachers. The model reduces KV cache size while preserving high performance on LM tasks. Compared to other models, Zebra-Llama delivers competitive or superior accuracy with smaller teachers and memory requirements. Zebra-Llama-8B outperforms Minitron-8B in few-shot accuracy while using fewer tokens, smaller KV cache, and achieving higher throughput. The approach showcases the potential for practical and sustainable improvements in inference efficiency for large language models.<br /><br />Summary: <div>
arXiv:2505.17272v1 Announce Type: new 
Abstract: With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and >97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparator-Adaptive $\Phi$-Regret: Improved Bounds, Simpler Algorithms, and Applications to Games</title>
<link>https://arxiv.org/abs/2505.17277</link>
<guid>https://arxiv.org/abs/2505.17277</guid>
<content:encoded><![CDATA[
<div> Sparsity-based complexity measure, Adaptive algorithm, Comparator-adaptive regret, Prior distribution, Binary transformations<br />
<br />
Summary:
This article introduces a new approach to the classic expert problem, focusing on achieving better comparator-adaptive $\Phi$-regret bounds with simpler algorithms compared to previous works. The proposed method involves a prior distribution over binary transformations, leading to prior-dependent regret against these transformations. Two efficient algorithms are presented, leveraging a prior-aware kernelized MWU algorithm and a BM-reduction approach. These methods offer improved regret bounds and simplicity in comparison to existing techniques. Additionally, the second algorithm can be extended to general-sum games, providing accelerated and adaptive convergence rates to $\Phi$-equilibria. In the context of correlated equilibria, the proposed bound surpasses existing results, showcasing the effectiveness and advantages of the proposed approach. <br /><br />Summary: <div>
arXiv:2505.17277v1 Announce Type: new 
Abstract: In the classic expert problem, $\Phi$-regret measures the gap between the learner's total loss and that achieved by applying the best action transformation $\phi \in \Phi$. A recent work by Lu et al., [2025] introduces an adaptive algorithm whose regret against a comparator $\phi$ depends on a certain sparsity-based complexity measure of $\phi$, (almost) recovering and interpolating optimal bounds for standard regret notions such as external, internal, and swap regret. In this work, we propose a general idea to achieve an even better comparator-adaptive $\Phi$-regret bound via much simpler algorithms compared to Lu et al., [2025]. Specifically, we discover a prior distribution over all possible binary transformations and show that it suffices to achieve prior-dependent regret against these transformations. Then, we propose two concrete and efficient algorithms to achieve so, where the first one learns over multiple copies of a prior-aware variant of the Kernelized MWU algorithm of Farina et al., [2022], and the second one learns over multiple copies of a prior-aware variant of the BM-reduction [Blum and Mansour, 2007]. To further showcase the power of our methods and the advantages over Lu et al., [2025] besides the simplicity and better regret bounds, we also show that our second approach can be extended to the game setting to achieve accelerated and adaptive convergence rate to $\Phi$-equilibria for a class of general-sum games. When specified to the special case of correlated equilibria, our bound improves over the existing ones from Anagnostides et al., [2022a,b]
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention with Trained Embeddings Provably Selects Important Tokens</title>
<link>https://arxiv.org/abs/2505.17282</link>
<guid>https://arxiv.org/abs/2505.17282</guid>
<content:encoded><![CDATA[
<div> embeddings, language modeling, gradient descent, softmax attention model, binary classification <br />
<br />
Summary: 
The study focuses on understanding the structure of token embeddings in language modeling through gradient descent. They analyze a one-layer softmax attention model with a linear head for binary classification. After a single step of training, the embeddings align with the output vector based on token frequency in the dataset. Training the embedding of the special token $\mathrm{\langle cls \rangle}$ results in selecting important tokens predictive of the label, maximizing the margin for this selection. Real-world experiments on IMDB and Yelp datasets confirm the theory's findings, showing that the embeddings capture token importance and align with predictive tokens, showcasing a practical understanding of token embeddings in language modeling. <br /><br />Summary: <div>
arXiv:2505.17282v1 Announce Type: new 
Abstract: Token embeddings play a crucial role in language modeling but, despite this practical relevance, their theoretical understanding remains limited. Our paper addresses the gap by characterizing the structure of embeddings obtained via gradient descent. Specifically, we consider a one-layer softmax attention model with a linear head for binary classification, i.e., $\texttt{Softmax}( p^\top E_X^\top ) E_X v = \frac{ \sum_{i=1}^T \exp(p^\top E_{x_i}) E_{x_i}^\top v}{\sum_{j=1}^T \exp(p^\top E_{x_{j}}) }$, where $E_X = [ E_{x_1} , \dots, E_{x_T} ]^\top$ contains the embeddings of the input sequence, $p$ is the embedding of the $\mathrm{\langle cls \rangle}$ token and $v$ the output vector. First, we show that, already after a single step of gradient training with the logistic loss, the embeddings $E_X$ capture the importance of tokens in the dataset by aligning with the output vector $v$ proportionally to the frequency with which the corresponding tokens appear in the dataset. Then, after training $p$ via gradient flow until convergence, the softmax selects the important tokens in the sentence (i.e., those that are predictive of the label), and the resulting $\mathrm{\langle cls \rangle}$ embedding maximizes the margin for such a selection. Experiments on real-world datasets (IMDB, Yelp) exhibit a phenomenology close to that unveiled by our theory.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Free Graph Data Selection under Distribution Shift</title>
<link>https://arxiv.org/abs/2505.17293</link>
<guid>https://arxiv.org/abs/2505.17293</guid>
<content:encoded><![CDATA[
<div> Graph domain adaptation, GDA, shift-robust, graph neural networks, GRADATE<br />
Summary: <br />
Graph domain adaptation (GDA) is a crucial task in graph machine learning, addressing distribution shifts with model-centric techniques like shift-robust graph neural networks (GNNs). However, these approaches can struggle with severe shifts and limited computational resources. To overcome these challenges, GRADATE, a model-free framework, has been proposed. GRADATE selects optimal training data from the source domain for classification on the target domain, utilizing optimal transport theory to adapt to distribution changes. It is data-efficient, scalable, and complements existing model-centric GDA methods. Empirical studies on real-world graph-level datasets show that GRADATE outperforms other selection methods and improves off-the-shelf GDA methods with fewer training data. <div>
arXiv:2505.17293v1 Announce Type: new 
Abstract: Graph domain adaptation (GDA) is a fundamental task in graph machine learning, with techniques like shift-robust graph neural networks (GNNs) and specialized training procedures to tackle the distribution shift problem. Although these model-centric approaches show promising results, they often struggle with severe shifts and constrained computational resources. To address these challenges, we propose a novel model-free framework, GRADATE (GRAph DATa sElector), that selects the best training data from the source domain for the classification task on the target domain. GRADATE picks training samples without relying on any GNN model's predictions or training recipes, leveraging optimal transport theory to capture and adapt to distribution changes. GRADATE is data-efficient, scalable and meanwhile complements existing model-centric GDA approaches. Through comprehensive empirical studies on several real-world graph-level datasets and multiple covariate shift types, we demonstrate that GRADATE outperforms existing selection methods and enhances off-the-shelf GDA methods with much fewer training data.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Regularization of Infinitesimally-perturbed Gradient Descent Toward Low-dimensional Solutions</title>
<link>https://arxiv.org/abs/2505.17304</link>
<guid>https://arxiv.org/abs/2505.17304</guid>
<content:encoded><![CDATA[
<div> implicit regularization, gradient-based methods, second-order stationary points, strict saddle points, infinitesimally perturbed gradient descent

Summary:
This paper investigates implicit regularization in optimization problems, particularly in over-parameterized scenarios. It explores the conditions necessary for gradient-based methods to converge to second-order stationary points in a low-dimensional region of a smooth, possibly nonconvex function. Key to successful implicit regularization are the efficient escape from strict saddle points and maintaining proximity to the implicit region. The study introduces infinitesimally perturbed gradient descent (IPGD), incorporating small perturbations to escape strict saddle points while staying close to the implicit region. By applying this framework to over-parameterized matrix sensing, the paper establishes formal guarantees for implicit regularization behavior of IPGD. Extensive experiments validate these findings across various learning problems.<br /><br />Summary: <div>
arXiv:2505.17304v1 Announce Type: new 
Abstract: Implicit regularization refers to the phenomenon where local search algorithms converge to low-dimensional solutions, even when such structures are neither explicitly specified nor encoded in the optimization problem. While widely observed, this phenomenon remains theoretically underexplored, particularly in modern over-parameterized problems. In this paper, we study the conditions that enable implicit regularization by investigating when gradient-based methods converge to second-order stationary points (SOSPs) within an implicit low-dimensional region of a smooth, possibly nonconvex function. We show that successful implicit regularization hinges on two key conditions: $(i)$ the ability to efficiently escape strict saddle points, while $(ii)$ maintaining proximity to the implicit region. Existing analyses enabling the convergence of gradient descent (GD) to SOSPs often rely on injecting large perturbations to escape strict saddle points. However, this comes at the cost of deviating from the implicit region. The central premise of this paper is that it is possible to achieve the best of both worlds: efficiently escaping strict saddle points using infinitesimal perturbations, while controlling deviation from the implicit region via a small deviation rate. We show that infinitesimally perturbed gradient descent (IPGD), which can be interpreted as GD with inherent ``round-off errors'', can provably satisfy both conditions. We apply our framework to the problem of over-parameterized matrix sensing, where we establish formal guarantees for the implicit regularization behavior of IPGD. We further demonstrate through extensive experiments that these insights extend to a broader class of learning problems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet Probabilistic Recurrent Convolutional Network for Multivariate Time Series Classification</title>
<link>https://arxiv.org/abs/2505.17307</link>
<guid>https://arxiv.org/abs/2505.17307</guid>
<content:encoded><![CDATA[
<div> Wavelet Probabilistic Recurrent Convolutional Network, Multivariate Time Series Classification, non-stationary environments, data scarcity, noise perturbations <br />
Summary:
The paper introduces the Wavelet Probabilistic Recurrent Convolutional Network (WPRCN) for Multivariate Time Series Classification (MTSC), focusing on dealing with non-stationary environments, data scarcity, and noise perturbations. This network includes a versatile wavelet probabilistic module with an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN). The AWPG creates a probabilistic model suited for varying data scarcities and non-stationarity, while the APTCN analyzes feature correlations to enhance classification in collaboration with existing MTSC models like Long Short-Term Memory (LSTM) and Causal Fully Convolutional Network (C-FCN). Evaluation on 30 datasets demonstrates WPRCN's superiority over benchmark algorithms in accuracy and robustness, particularly in scenarios with limited data and perturbations in physiological data. <br /><br />Summary: <div>
arXiv:2505.17307v1 Announce Type: new 
Abstract: This paper presents a Wavelet Probabilistic Recurrent Convolutional Network (WPRCN) for Multivariate Time Series Classification (MTSC), especially effective in handling non-stationary environments, data scarcity and noise perturbations. We introduce a versatile wavelet probabilistic module designed to extract and analyse the probabilistic features, which can seamlessly integrate with a variety of neural network architectures. This probabilistic module comprises an Adaptive Wavelet Probabilistic Feature Generator (AWPG) and a Channel Attention-based Probabilistic Temporal Convolutional Network (APTCN). Such formulation extends the application of wavelet probabilistic neural networks to deep neural networks for MTSC. The AWPG constructs an ensemble probabilistic model addressing different data scarcities and non-stationarity; it adaptively selects the optimal ones and generates probabilistic features for APTCN. The APTCN analyses the correlations of the features and forms a comprehensive feature space with existing MTSC models for classification. Here, we instantiate the proposed module to work in parallel with a Long Short-Term Memory (LSTM) network and a Causal Fully Convolutional Network (C-FCN), demonstrating its broad applicability in time series analysis. The WPRCN is evaluated on 30 diverse MTS datasets and outperforms all the benchmark algorithms on average accuracy and rank, exhibiting pronounced strength in handling scarce data and physiological data subject to perturbations and non-stationarities.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training</title>
<link>https://arxiv.org/abs/2505.17331</link>
<guid>https://arxiv.org/abs/2505.17331</guid>
<content:encoded><![CDATA[
<div> Keywords: ECHO-LLaMA, LLaMA architecture, training speed, inference throughput, language performance

Summary: 
ECHO-LLaMA is an efficient LLaMA architecture that enhances training speed and inference throughput while maintaining learning capacity. It achieves up to 77% higher token-per-second throughput during training, up to 16% higher Model FLOPs Utilization (MFU), and up to 14% lower loss compared to baseline models. By transforming LLaMA models into shared KV caching, ECHO-LLaMA significantly reduces computational complexity without sacrificing language performance. Additionally, it provides approximately 7% higher test-time throughput on the 1.1B model. With a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for training large language models, enabling faster and more resource-efficient training. This innovation ensures that pretraining and finetuning tasks can be completed more efficiently without compromising performance.<br /><br />Summary: <div>
arXiv:2505.17331v1 Announce Type: new 
Abstract: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher token-per-second throughput during training, up to 16\% higher Model FLOPs Utilization (MFU), and up to 14\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Predictive Distributions for Order Fulfillment Time Forecasting</title>
<link>https://arxiv.org/abs/2505.17340</link>
<guid>https://arxiv.org/abs/2505.17340</guid>
<content:encoded><![CDATA[
<div> machine learning, order fulfillment time, e-commerce logistics, distributional forecasting, Conformal Predictive Systems
Summary:
This paper presents a novel framework for accurate estimation of order fulfillment time in e-commerce logistics using machine learning techniques. It leverages Conformal Predictive Systems and Cross Venn-Abers Predictors to provide rigorous coverage guarantees. The framework integrates granular spatiotemporal features to capture fulfillment location and carrier performance dynamics, enhancing predictive accuracy. A cost-sensitive decision rule is developed to convert probabilistic forecasts into reliable point predictions. Experimental evaluation on a large-scale industrial dataset shows that the proposed methods generate competitive distributional forecasts and significantly outperform existing rule-based systems. The machine learning-based point predictions achieve up to 14% higher prediction accuracy and up to 75% improvement in identifying late deliveries.<br /><br />Summary: <div>
arXiv:2505.17340v1 Announce Type: new 
Abstract: Accurate estimation of order fulfillment time is critical for e-commerce logistics, yet traditional rule-based approaches often fail to capture the inherent uncertainties in delivery operations. This paper introduces a novel framework for distributional forecasting of order fulfillment time, leveraging Conformal Predictive Systems and Cross Venn-Abers Predictors--model-agnostic techniques that provide rigorous coverage or validity guarantees. The proposed machine learning methods integrate granular spatiotemporal features, capturing fulfillment location and carrier performance dynamics to enhance predictive accuracy. Additionally, a cost-sensitive decision rule is developed to convert probabilistic forecasts into reliable point predictions. Experimental evaluation on a large-scale industrial dataset demonstrates that the proposed methods generate competitive distributional forecasts, while machine learning-based point predictions significantly outperform the existing rule-based system--achieving up to 14% higher prediction accuracy and up to 75% improvement in identifying late deliveries.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TI-DeepONet: Learnable Time Integration for Stable Long-Term Extrapolation</title>
<link>https://arxiv.org/abs/2505.17341</link>
<guid>https://arxiv.org/abs/2505.17341</guid>
<content:encoded><![CDATA[
<div> framework, neural operators, numerical time-stepping, dynamical systems, predictive accuracy <br />
Summary: <br />
The article introduces TI-DeepONet, a framework that combines neural operators with adaptive numerical time-stepping to improve accurate temporal extrapolation in dynamical systems modeling. By focusing on approximating instantaneous time-derivative fields and integrating them using numerical schemes, the approach supports continuous-time prediction and allows for the use of higher-precision integrators during inference. The TI(L)-DeepONet variant incorporates learnable coefficients for intermediate slopes to enhance prediction accuracy. Evaluation on three PDEs shows that both TI-DeepONet and TI(L)-DeepONet outperform traditional methods, reducing relative L2 extrapolation errors by approximately 81% and 70%, respectively. Notably, the models maintain stability in predictions for temporal domains twice the training interval. This research bridges neural approximation and numerical analysis, offering a physics-aware learning paradigm for dynamical systems. <br /> <div>
arXiv:2505.17341v1 Announce Type: new 
Abstract: Accurate temporal extrapolation presents a fundamental challenge for neural operators in modeling dynamical systems, where reliable predictions must extend significantly beyond the training time horizon. Conventional Deep Operator Network (DeepONet) approaches employ two inherently limited training paradigms - fixed-horizon rollouts that predict complete spatiotemporal solutions while disregarding temporal causality, and autoregressive formulations that accumulate errors through sequential predictions. We introduce TI-DeepONet, a framework that integrates neural operators with adaptive numerical time-stepping techniques to preserve the Markovian structure of dynamical systems while mitigating error propagation in extended temporal forecasting. Our approach reformulates the learning objective from direct state prediction to the approximation of instantaneous time-derivative fields, which are then integrated using established numerical schemes. This architecture supports continuous-time prediction and enables deployment of higher-precision integrators during inference than those used during training, balancing computational efficiency with predictive accuracy. We further develop TI(L)-DeepONet, which incorporates learnable coefficients for intermediate slopes in the integration process, adapting to solution-specific variations and enhancing fidelity. Evaluation across three canonical PDEs shows that TI(L)-DeepONet marginally outperforms TI-DeepONet, with both reducing relative L2 extrapolation errors: approximately 81% over autoregressive and 70% over fixed-horizon methods. Notably, both maintain prediction stability for temporal domains extending to about twice the training interval. This research establishes a physics-aware operator learning paradigm that bridges neural approximation with numerical analysis while preserving the causal structure of dynamical systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety</title>
<link>https://arxiv.org/abs/2505.17342</link>
<guid>https://arxiv.org/abs/2505.17342</guid>
<content:encoded><![CDATA[
<div> Keywords: Safe Reinforcement Learning, Constrained Markov Decision Processes (CMDPs), Multi-Agent Safe RL (SafeMARL), policy gradient methods, safe exploration strategies 

Summary: 
Safe Reinforcement Learning (SafeRL) focuses on incorporating safety constraints into the learning and deployment of agents. This survey delves into SafeRL formulations based on Constrained Markov Decision Processes (CMDPs) and Multi-Agent Safe RL (SafeMARL). It covers the theoretical foundations of CMDPs, outlining definitions, constrained optimization techniques, and important theorems. The survey also discusses state-of-the-art algorithms for single agents, including policy gradient methods with safety guarantees and safe exploration strategies. Furthermore, it explores recent advances in SafeMARL for both cooperative and competitive scenarios. The survey concludes by proposing five open research problems to advance the field, with a particular emphasis on SafeMARL. These problems are presented with their motivations, key challenges, and relevant prior research. This comprehensive survey serves as a technical guide for researchers interested in exploring SafeRL and SafeMARL, offering insights into key concepts, methodologies, and potential future research directions. 

<br /><br />Summary: <div>
arXiv:2505.17342v1 Announce Type: new 
Abstract: Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement learning that explicitly deals with safety constraints during the learning and deployment of agents. This survey provides a mathematically rigorous overview of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs) and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical foundations of CMDPs, covering definitions, constrained optimization techniques, and fundamental theorems. We then summarize state-of-the-art algorithms in SafeRL for single agents, including policy gradient methods with safety guarantees and safe exploration strategies, as well as recent advances in SafeMARL for cooperative and competitive settings. Additionally, we propose five open research problems to advance the field, with three focusing on SafeMARL. Each problem is described with motivation, key challenges, and related prior work. This survey is intended as a technical guide for researchers interested in SafeRL and SafeMARL, highlighting key concepts, methods, and open future research directions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction</title>
<link>https://arxiv.org/abs/2505.17344</link>
<guid>https://arxiv.org/abs/2505.17344</guid>
<content:encoded><![CDATA[
<div> Keywords: patient no-shows, machine learning, attention mechanisms, random forest, predictive modeling

Summary: 
The study introduces a new predictive model, the Multi-Head Attention Soft Random Forest (MHASRF), to address the issue of patient no-shows in healthcare settings. Patient no-shows can disrupt healthcare operations and resource allocation. The MHASRF model integrates attention mechanisms into a random forest model, allowing for soft splitting instead of hard decision splits. This enhances adaptability to specific patient behaviors. The model outperformed traditional models like decision trees, logistic regression, random forest, and naive Bayes with an accuracy of 93.56%. It also provides deeper insights into patient no-show predictors by identifying key features at both the tree and attention mechanism levels. The MHASRF model is robust, adaptable, and interpretable, offering healthcare providers a valuable tool to optimize resources and improve patient care.<br /><br />Summary: <div>
arXiv:2505.17344v1 Announce Type: new 
Abstract: Unattended scheduled appointments, defined as patient no-shows, adversely affect both healthcare providers and patients' health, disrupting the continuity of care, operational efficiency, and the efficient allocation of medical resources. Accurate predictive modelling is needed to reduce the impact of no-shows. Although machine learning methods, such as logistic regression, random forest models, and decision trees, are widely used in predicting patient no-shows, they often rely on hard decision splits and static feature importance, limiting their adaptability to specific or complex patient behaviors. To address this limitation, we propose a new hybrid Multi-Head Attention Soft Random Forest (MHASRF) model that integrates attention mechanisms into a random forest model using probabilistic soft splitting instead of hard splitting. The MHASRF model assigns attention weights differently across the trees, enabling attention on specific patient behaviors. The model exhibited 93.56% accuracy, 93.67% precision, 93.56% recall, and a 93.59% F1 score, surpassing the performance of decision tree, logistic regression, random forest, and naive Bayes models. Furthermore, MHASRF was able to identify key predictors of patient no-shows using two levels of feature importance (tree level and attention mechanism level), offering deeper insights into patient no-show predictors. The proposed model is a robust, adaptable, and interpretable method for predicting patient no-shows that will help healthcare providers in optimizing resources.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems</title>
<link>https://arxiv.org/abs/2505.17351</link>
<guid>https://arxiv.org/abs/2505.17351</guid>
<content:encoded><![CDATA[
<div> Transformer, U-Net, convolutional ResNet, diffusion model, spatio-temporal modeling

Summary: 

The article introduces FLEX, a novel architecture for generative modeling of spatio-temporal physical systems using diffusion models. FLEX operates in the residual space instead of raw data, reducing velocity field variance and enhancing training stability. It combines a latent Transformer with a U-Net and convolutional ResNet layers, enabling capture of local spatial detail and long-range dependencies in latent space. FLEX leverages a task-specific encoder for improved spatio-temporal conditioning, incorporating weak conditioning for generalization and strong conditioning for reconstruction fidelity. It achieves accurate predictions for super-resolution and forecasting tasks with only two reverse diffusion steps and provides calibrated uncertainty estimates through sampling. Evaluation on high-resolution 2D turbulence data demonstrates FLEX's superior performance over strong baselines, showcasing its generalization to out-of-distribution settings. <div>
arXiv:2505.17351v1 Announce Type: new 
Abstract: We introduce FLEX (FLow EXpert), a backbone architecture for generative modeling of spatio-temporal physical systems using diffusion models. FLEX operates in the residual space rather than on raw data, a modeling choice that we motivate theoretically, showing that it reduces the variance of the velocity field in the diffusion model, which helps stabilize training. FLEX integrates a latent Transformer into a U-Net with standard convolutional ResNet layers and incorporates a redesigned skip connection scheme. This hybrid design enables the model to capture both local spatial detail and long-range dependencies in latent space. To improve spatio-temporal conditioning, FLEX uses a task-specific encoder that processes auxiliary inputs such as coarse or past snapshots. Weak conditioning is applied to the shared encoder via skip connections to promote generalization, while strong conditioning is applied to the decoder through both skip and bottleneck features to ensure reconstruction fidelity. FLEX achieves accurate predictions for super-resolution and forecasting tasks using as few as two reverse diffusion steps. It also produces calibrated uncertainty estimates through sampling. Evaluations on high-resolution 2D turbulence data show that FLEX outperforms strong baselines and generalizes to out-of-distribution settings, including unseen Reynolds numbers, physical observables (e.g., fluid flow velocity fields), and boundary conditions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots</title>
<link>https://arxiv.org/abs/2505.17354</link>
<guid>https://arxiv.org/abs/2505.17354</guid>
<content:encoded><![CDATA[
<div> method, continuous-time dynamics, optimal transport, data reconstruction, temporal discretization<br />
Summary:<br />
The article introduces Continuous-Time Optimal Transport Flow (CT-OT Flow) to recover continuous-time dynamics from discrete-time snapshots with noisy timestamps. This method involves inferring high-resolution time labels using partial optimal transport and reconstructing continuous-time data distribution through temporal kernel smoothing. The reconstructed data distribution enables accurate training of dynamics models like ODEs and SDEs. CT-OT Flow outperforms other existing methods on synthetic benchmarks and achieves lower reconstruction errors on real datasets like scRNA-seq and typhoon-track datasets. The results emphasize the importance of explicitly considering temporal discretization and timestamp uncertainty in accurately modeling discrete snapshots and continuous-time processes. <br />Summary: <div>
arXiv:2505.17354v1 Announce Type: new 
Abstract: In many real-world scenarios, such as single-cell RNA sequencing, data are observed only as discrete-time snapshots spanning finite time intervals and subject to noisy timestamps, with no continuous trajectories available. Recovering the underlying continuous-time dynamics from these snapshots with coarse and noisy observation times is a critical and challenging task. We propose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers high-resolution time labels via partial optimal transport and then reconstructs a continuous-time data distribution through a temporal kernel smoothing. This reconstruction enables accurate training of dynamics models such as ODEs and SDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic benchmarks and achieves lower reconstruction errors on real scRNA-seq and typhoon-track datasets. Our results highlight the benefits of explicitly modeling temporal discretization and timestamp uncertainty, offering an accurate and general framework for bridging discrete snapshots and continuous-time processes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness of Nonparametric Regression</title>
<link>https://arxiv.org/abs/2505.17356</link>
<guid>https://arxiv.org/abs/2505.17356</guid>
<content:encoded><![CDATA[
<div> adversarial robustness, regression, nonparametric, minimax lower bound, smoothing spline<br />
<br />
Summary:<br />
This paper investigates the adversarial robustness of nonparametric regression in the presence of arbitrarily corrupted input data. The study focuses on the second-order Sobolev space, with two main contributions. Firstly, a minimax lower bound on estimation error is established, revealing an inherent limit for all estimators. Secondly, it is shown that the properly regularized smoothing spline estimator displays robustness against adversarial corruption, with its estimation error diminishing as the number of corrupted samples approaches zero. However, when a constant fraction of data is corrupted, no estimator can ensure vanishing estimation error, highlighting the effectiveness of the smoothing spline in tolerating a maximal number of corrupted samples. <div>
arXiv:2505.17356v1 Announce Type: new 
Abstract: In this paper, we investigate the adversarial robustness of regression, a fundamental problem in machine learning, under the setting where an adversary can arbitrarily corrupt a subset of the input data. While the robustness of parametric regression has been extensively studied, its nonparametric counterpart remains largely unexplored. We characterize the adversarial robustness in nonparametric regression, assuming the regression function belongs to the second-order Sobolev space (i.e., it is square integrable up to its second derivative).
  The contribution of this paper is two-fold: (i) we establish a minimax lower bound on the estimation error, revealing a fundamental limit that no estimator can overcome, and (ii) we show that, perhaps surprisingly, the classical smoothing spline estimator, when properly regularized, exhibits robustness against adversarial corruption. These results imply that if $o(n)$ out of $n$ samples are corrupted, the estimation error of the smoothing spline vanishes as $n \to \infty$. On the other hand, when a constant fraction of the data is corrupted, no estimator can guarantee vanishing estimation error, implying the optimality of the smoothing spline in terms of maximum tolerable number of corrupted samples.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction</title>
<link>https://arxiv.org/abs/2505.17357</link>
<guid>https://arxiv.org/abs/2505.17357</guid>
<content:encoded><![CDATA[
<div> Keywords: IoT-based botnet attacks, Graph Neural Networks (GNNs), attention mechanisms, dimension reduction techniques, botnet attack detection

Summary:
In the realm of IoT-based botnet attack detection, researchers have been exploring different learning models, including traditional machine learning, deep learning, and hybrid approaches. One key development involves the use of attention mechanisms to capture long-term dependencies among features, improving detection accuracy significantly. Graph Neural Networks (GNNs) offer a solution to the oversight of inter-instance relationships in most models by learning an embedding space through iterative message passing. To enhance classification performance, attention mechanisms have been incorporated within GNNs to leverage long-range dependencies and inter-instance connections. However, transforming high-dimensional IoT attack datasets into graph structures poses challenges due to the computational overhead. To address this, a framework is proposed in this paper that reduces the dimensionality of NetFlow-based IoT attack datasets before transforming them into graph datasets. Three dimension reduction techniques are evaluated--Variational Autoencoder (VAE-encoder), classical autoencoder (AE-encoder), and Principal Component Analysis (PCA)--and their impact on a Graph Attention neural network (GAT) model for botnet attack detection is compared. 

<br /><br />Summary: <div>
arXiv:2505.17357v1 Announce Type: new 
Abstract: With the rise of IoT-based botnet attacks, researchers have explored various learning models for detection, including traditional machine learning, deep learning, and hybrid approaches. A key advancement involves deploying attention mechanisms to capture long-term dependencies among features, significantly improving detection accuracy. However, most models treat attack instances independently, overlooking inter-instance relationships. Graph Neural Networks (GNNs) address this limitation by learning an embedding space via iterative message passing where similar instances are placed closer based on node features and relationships, enhancing classification performance. To further improve detection, attention mechanisms have been embedded within GNNs, leveraging both long-range dependencies and inter-instance connections. However, transforming the high dimensional IoT attack datasets into a graph structured dataset poses challenges, such as large graph structures leading computational overhead. To mitigate this, this paper proposes a framework that first reduces dimensionality of the NetFlow-based IoT attack dataset before transforming it into a graph dataset. We evaluate three dimension reduction techniques--Variational Autoencoder (VAE-encoder), classical autoencoder (AE-encoder), and Principal Component Analysis (PCA)--and compare their effects on a Graph Attention neural network (GAT) model for botnet attack detection
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards VM Rescheduling Optimization Through Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17359</link>
<guid>https://arxiv.org/abs/2505.17359</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, virtual machine rescheduling, data centers, performance optimization, open-source<br />
Summary:<br />
The article introduces VM2RL, a reinforcement learning system for virtual machine (VM) rescheduling in data centers. Existing VM rescheduling algorithms are hindered by long inference times due to dynamic VM state changes, impacting performance as data centers grow. VM2RL addresses this challenge with a two-stage framework accommodating diverse constraints and workload conditions. It includes a feature extraction module capturing relational information specific to rescheduling and a risk-seeking evaluation to optimize the trade-off between latency and accuracy. Extensive experiments with industry-scale data center data demonstrate that VM2RL can achieve performance comparable to the optimal solution in a matter of seconds. The code and datasets are open-sourced for further research and development. <div>
arXiv:2505.17359v1 Announce Type: new 
Abstract: Modern industry-scale data centers need to manage a large number of virtual machines (VMs). Due to the continual creation and release of VMs, many small resource fragments are scattered across physical machines (PMs). To handle these fragments, data centers periodically reschedule some VMs to alternative PMs, a practice commonly referred to as VM rescheduling. Despite the increasing importance of VM rescheduling as data centers grow in size, the problem remains understudied. We first show that, unlike most combinatorial optimization tasks, the inference time of VM rescheduling algorithms significantly influences their performance, due to dynamic VM state changes during this period. This causes existing methods to scale poorly. Therefore, we develop a reinforcement learning system for VM rescheduling, VM2RL, which incorporates a set of customized techniques, such as a two-stage framework that accommodates diverse constraints and workload conditions, a feature extraction module that captures relational information specific to rescheduling, as well as a risk-seeking evaluation enabling users to optimize the trade-off between latency and accuracy. We conduct extensive experiments with data from an industry-scale data center. Our results show that VM2RL can achieve a performance comparable to the optimal solution but with a running time of seconds. Code and datasets are open-sourced: https://github.com/zhykoties/VMR2L_eurosys, https://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved and Oracle-Efficient Online $\ell_1$-Multicalibration</title>
<link>https://arxiv.org/abs/2505.17365</link>
<guid>https://arxiv.org/abs/2505.17365</guid>
<content:encoded><![CDATA[
<div> online multicalibration, framework, adversarial settings, linear-product optimization, oracle-efficient<br />
<br />
Summary:<br />
The study introduces a framework for online multicalibration to ensure calibrated predictions across multiple groups in adversarial scenarios over T rounds. Unlike previous approaches, this study directly addresses online multicalibration in the ℓ1 norm, achieving improved rates of Õ(T^-1/3) and Õ(T^-1/4) for online ℓ1-multicalibration. The novel method involves a reduction of online ℓ1-multicalibration to online linear-product optimization (OLPO) and introduces a linearization technique combined with a no-regret algorithm to achieve the desired sublinear rate. To enhance scalability, an oracle-efficient approach to OLPO is proposed, making a polynomial number of calls to an offline optimization oracle. The framework's applicability extends to infinite group families by leveraging the Lipschitz property of the ℓ1-multicalibration error with respect to the group family. <div>
arXiv:2505.17365v1 Announce Type: new 
Abstract: We study \emph{online multicalibration}, a framework for ensuring calibrated predictions across multiple groups in adversarial settings, across $T$ rounds. Although online calibration is typically studied in the $\ell_1$ norm, prior approaches to online multicalibration have taken the indirect approach of obtaining rates in other norms (such as $\ell_2$ and $\ell_{\infty}$) and then transferred these guarantees to $\ell_1$ at additional loss. In contrast, we propose a direct method that achieves improved and oracle-efficient rates of $\widetilde{\mathcal{O}}(T^{-1/3})$ and $\widetilde{\mathcal{O}}(T^{-1/4})$ respectively, for online $\ell_1$-multicalibration. Our key insight is a novel reduction of online \(\ell_1\)-multicalibration to an online learning problem with product-based rewards, which we refer to as \emph{online linear-product optimization} ($\mathtt{OLPO}$).
  To obtain the improved rate of $\widetilde{\mathcal{O}}(T^{-1/3})$, we introduce a linearization of $\mathtt{OLPO}$ and design a no-regret algorithm for this linearized problem. Although this method guarantees the desired sublinear rate (nearly matching the best rate for online calibration), it becomes computationally expensive when the group family \(\mathcal{H}\) is large or infinite, since it enumerates all possible groups. To address scalability, we propose a second approach to $\mathtt{OLPO}$ that makes only a polynomial number of calls to an offline optimization (\emph{multicalibration evaluation}) oracle, resulting in \emph{oracle-efficient} online \(\ell_1\)-multicalibration with a rate of $\widetilde{\mathcal{O}}(T^{-1/4})$. Our framework also extends to certain infinite families of groups (e.g., all linear functions on the context space) by exploiting a $1$-Lipschitz property of the \(\ell_1\)-multicalibration error with respect to \(\mathcal{H}\).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIREN: Beyond Trajectories -- A Spectral Lens on Time</title>
<link>https://arxiv.org/abs/2505.17370</link>
<guid>https://arxiv.org/abs/2505.17370</guid>
<content:encoded><![CDATA[
<div> latent representation, Wasserstein-2 distance, spectral analysis, long-term time-series forecasting, chaotic systems

Summary: 
The study introduces FRIREN, a new model for long-term time-series forecasting that focuses on geometric structures rather than pointwise prediction. FRIREN utilizes a normalizing flow block to embed data into a normally distributed latent representation, allowing for efficient prediction that preserves geometric changes. By minimizing the Wasserstein-2 distance, FRIREN achieves accurate and interpretable forecasts for chaotic systems like Lorenz-63 and Rossler. The model also provides a spectral view of dynamics, enabling practitioners to analyze mode behavior locally and system-wide. FRIREN outperforms existing models like TimeMixer on forecasting accuracy for various datasets, showcasing its effectiveness in long-horizon predictions. The connection of modern generative flows with spectral analysis sets a new benchmark for LTSF model design. <div>
arXiv:2505.17370v1 Announce Type: new 
Abstract: Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End-to-End Approach for Child Reading Assessment in the Xhosa Language</title>
<link>https://arxiv.org/abs/2505.17371</link>
<guid>https://arxiv.org/abs/2505.17371</guid>
<content:encoded><![CDATA[
<div> Keywords: Child literacy, Reading assessments, Automatic reading assessment systems, Xhosa language, End-to-end models

Summary: 
Child literacy is a crucial predictor of future life outcomes, especially in vulnerable populations. Targeted interventions are needed to bridge the gap between literacy levels in low-income areas and higher-income regions. The development of automatic reading assessment systems using AI can support educators in evaluating the effectiveness of literacy programs. This study focuses on Xhosa, a South African language, to improve child speech recognition capabilities. A novel dataset of Xhosa child speech samples is created, with each recording labeled using a cost-effective approach. Three state-of-the-art end-to-end models are evaluated on the dataset, showing the importance of training data amount and balance for optimal performance. The wav2vec 2.0 model benefits from training on multiple classes simultaneously, even with limited samples available.

<br /><br />Summary: <div>
arXiv:2505.17371v1 Announce Type: new 
Abstract: Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual's life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children's voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Value-Guided Search for Efficient Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.17373</link>
<guid>https://arxiv.org/abs/2505.17373</guid>
<content:encoded><![CDATA[
<div> Method, Value model training, Long-context reasoning traces, Dataset, DeepSeek models

Summary: 
- The paper proposes a new method for training value models on long-context reasoning traces without the need for a fine-grained notion of "step."
- A dataset of 2.5 million reasoning traces is collected to train a 1.5B token-level value model, enhancing DeepSeek models' performance with test-time compute scaling.
- Block-wise value-guided search (VGS) with weighted majority voting outperforms standard methods like majority voting or best-of-n, achieving an average accuracy of 45.7% across four competition math benchmarks with an inference budget of 64 generations.
- VGS with DeepSeek-R1-Distill-1.5B reaches parity with o3-mini-medium while significantly reducing the needed inference FLOPs compared to majority voting.
- The dataset, model, and codebase are openly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2505.17373v1 Announce Type: new 
Abstract: In this paper, we propose a simple and efficient method for value model training on long-context reasoning traces. Compared to existing process reward models (PRMs), our method does not require a fine-grained notion of "step," which is difficult to define for long-context reasoning models. By collecting a dataset of 2.5 million reasoning traces, we train a 1.5B token-level value model and apply it to DeepSeek models for improved performance with test-time compute scaling. We find that block-wise value-guided search (VGS) with a final weighted majority vote achieves better test-time scaling than standard methods such as majority voting or best-of-n. With an inference budget of 64 generations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of 45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024 & 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly reduces the inference FLOPs required to achieve the same performance of majority voting. Our dataset, model and codebase are open-sourced.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition</title>
<link>https://arxiv.org/abs/2505.17379</link>
<guid>https://arxiv.org/abs/2505.17379</guid>
<content:encoded><![CDATA[
<div> Algorithm, Scoring rule, Principal-agent framework, Information acquisition, Sample complexity 
<br />
Summary: 
The article focuses on determining the optimal scoring rule in the principal-agent framework for online information acquisition. Two algorithms, OIAFC and OIAFB, are proposed for fixed confidence and fixed budget settings, respectively. OIAFC can identify the desired $(\epsilon, \delta)$-scoring rule with efficient sample complexity, while OIAFB matches its performance but operates in both settings. The study shows that both algorithms have the same complexity level across fixed confidence and fixed budget scenarios. <div>
arXiv:2505.17379v1 Announce Type: new 
Abstract: We investigate the problem of identifying the optimal scoring rule within the principal-agent framework for online information acquisition problem. We focus on the principal's perspective, seeking to determine the desired scoring rule through interactions with the agent. To address this challenge, we propose two algorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget settings, respectively. Our theoretical analysis demonstrates that OIAFC can extract the desired $(\epsilon, \delta)$-scoring rule with a efficient instance-dependent sample complexity or an instance-independent sample complexity. Our analysis also shows that OIAFB matches the instance-independent performance bound of OIAFC, while both algorithms share the same complexity across fixed confidence and fixed budget settings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling</title>
<link>https://arxiv.org/abs/2505.17384</link>
<guid>https://arxiv.org/abs/2505.17384</guid>
<content:encoded><![CDATA[
<div> Variational Autoencoding Discrete Diffusion, masked diffusion models, latent variable modeling, inter-dimensional dependencies, sample quality<br />
Summary:<br />
Discrete diffusion models have proven effective in modeling complex discrete data, with masked diffusion models (MDMs) offering a balance between quality and speed. However, the performance of MDMs can degrade with fewer denoising steps due to limited modeling of inter-dimensional dependencies. The proposed Variational Autoencoding Discrete Diffusion (VADD) framework enhances discrete diffusion models by incorporating latent variable modeling to capture correlations among dimensions. VADD introduces an auxiliary recognition model for stable training through variational lower bounds maximization and amortized inference. Empirical results across 2D toy data, pixel-level image generation, and text generation show that VADD outperforms MDM baselines in sample quality, particularly with a small number of denoising steps.<br />
Summary: <div>
arXiv:2505.17384v1 Announce Type: new 
Abstract: Discrete diffusion models have recently shown great promise for modeling complex discrete data, with masked diffusion models (MDMs) offering a compelling trade-off between quality and generation speed. MDMs denoise by progressively unmasking multiple dimensions from an all-masked input, but their performance can degrade when using few denoising steps due to limited modeling of inter-dimensional dependencies. In this paper, we propose Variational Autoencoding Discrete Diffusion (VADD), a novel framework that enhances discrete diffusion with latent variable modeling to implicitly capture correlations among dimensions. By introducing an auxiliary recognition model, VADD enables stable training via variational lower bounds maximization and amortized inference over the training set. Our approach retains the efficiency of traditional MDMs while significantly improving sample quality, especially when the number of denoising steps is small. Empirical results on 2D toy data, pixel-level image generation, and text generation demonstrate that VADD consistently outperforms MDM baselines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Mixture Kernels for Bayesian Optimization</title>
<link>https://arxiv.org/abs/2505.17393</link>
<guid>https://arxiv.org/abs/2505.17393</guid>
<content:encoded><![CDATA[
<div> Gaussian Process, Bayesian Optimization, Surrogate Model, Spectral Mixture Kernels, Black-Box Optimization

Summary: 
The article introduces a novel Gaussian Process-based Bayesian Optimization (BO) method that incorporates spectral mixture kernels derived from scale-location mixtures of Cauchy and Gaussian distributions. This approach significantly improves efficiency and optimization performance, matching simpler kernels in computational speed while surpassing more complex models and automatic BO methods in results. The authors provide bounds on information gain and cumulative regret related to finding the optimum. Extensive numerical experiments demonstrate the method's superiority over existing baselines in various synthetic and real-world problems across low- and high-dimensional settings. <div>
arXiv:2505.17393v1 Announce Type: new 
Abstract: Bayesian Optimization (BO) is a widely used approach for solving expensive black-box optimization tasks. However, selecting an appropriate probabilistic surrogate model remains an important yet challenging problem. In this work, we introduce a novel Gaussian Process (GP)-based BO method that incorporates spectral mixture kernels, derived from spectral densities formed by scale-location mixtures of Cauchy and Gaussian distributions. This method achieves a significant improvement in both efficiency and optimization performance, matching the computational speed of simpler kernels while delivering results that outperform more complex models and automatic BO methods. We provide bounds on the information gain and cumulative regret associated with obtaining the optimum. Extensive numerical experiments demonstrate that our method consistently outperforms existing baselines across a diverse range of synthetic and real-world problems, including both low- and high-dimensional settings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Transfer Learning</title>
<link>https://arxiv.org/abs/2505.17404</link>
<guid>https://arxiv.org/abs/2505.17404</guid>
<content:encoded><![CDATA[
<div> probability distributions, transfer learning, regression models, Wasserstein space, domain similarity

Summary:
This article introduces a novel framework for transfer learning in regression models, focusing on probability distributions in the Wasserstein space. It addresses the limitations of traditional transfer learning approaches that deal with scalar or multivariate data in Euclidean spaces. The proposed estimator for known informative source domains demonstrates provable asymptotic convergence rates, quantifying the impact of domain similarity on transfer efficiency. Additionally, a data-driven transfer learning procedure is developed for cases where the informative subset is unknown, aiming to mitigate negative transfer. The methods are supported by rigorous theoretical analysis and validated through simulations and real-world applications. This research contributes to enhancing learning in target domains by leveraging knowledge from source domains through the transfer of probability distributions. <div>
arXiv:2505.17404v1 Announce Type: new 
Abstract: Transfer learning is a powerful paradigm for leveraging knowledge from source domains to enhance learning in a target domain. However, traditional transfer learning approaches often focus on scalar or multivariate data within Euclidean spaces, limiting their applicability to complex data structures such as probability distributions. To address this, we introduce a novel framework for transfer learning in regression models, where outputs are probability distributions residing in the Wasserstein space. When the informative subset of transferable source domains is known, we propose an estimator with provable asymptotic convergence rates, quantifying the impact of domain similarity on transfer efficiency. For cases where the informative subset is unknown, we develop a data-driven transfer learning procedure designed to mitigate negative transfer. The proposed methods are supported by rigorous theoretical analysis and are validated through extensive simulations and real-world applications.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.17431</link>
<guid>https://arxiv.org/abs/2505.17431</guid>
<content:encoded><![CDATA[
<div> Keywords: Irregular multivariate time series, Hypergraph neural network, Forecasting, Variable dependencies, Message passing

Summary:
HyperIMTS is a novel approach for forecasting irregular multivariate time series data. It utilizes a Hypergraph neural network to capture dependencies among unaligned observations across variables. This model represents observed values as nodes in a hypergraph, interconnected by temporal and variable hyperedges for efficient message passing. HyperIMTS adapts to the irregular time intervals within variables, allowing for accurate forecasting without the need for padded samples. By leveraging irregularity-aware message passing, HyperIMTS successfully captures variable dependencies in a time-adaptive manner. Experimental results demonstrate the competitive performance of HyperIMTS compared to existing models in IMTS forecasting, all while maintaining low computational costs. Overall, HyperIMTS provides a unified framework for learning temporal and variable dependencies from original observations in irregular multivariate time series data.<br /><br />Summary: <div>
arXiv:2505.17431v1 Announce Type: new 
Abstract: Irregular multivariate time series (IMTS) are characterized by irregular time intervals within variables and unaligned observations across variables, posing challenges in learning temporal and variable dependencies. Many existing IMTS models either require padded samples to learn separately from temporal and variable dimensions, or represent original samples via bipartite graphs or sets. However, the former approaches often need to handle extra padding values affecting efficiency and disrupting original sampling patterns, while the latter ones have limitations in capturing dependencies among unaligned observations. To represent and learn both dependencies from original observations in a unified form, we propose HyperIMTS, a Hypergraph neural network for Irregular Multivariate Time Series forecasting. Observed values are converted as nodes in the hypergraph, interconnected by temporal and variable hyperedges to enable message passing among all observations. Through irregularity-aware message passing, HyperIMTS captures variable dependencies in a time-adaptive way to achieve accurate forecasting. Experiments demonstrate HyperIMTS's competitive performance among state-of-the-art models in IMTS forecasting with low computational cost.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discretization-free Multicalibration through Loss Minimization over Tree Ensembles</title>
<link>https://arxiv.org/abs/2505.17435</link>
<guid>https://arxiv.org/abs/2505.17435</guid>
<content:encoded><![CDATA[
<div> multicalibration, learning objective, predictor, empirical risk minimization, decision trees 

Summary:
This article introduces a novel discretization-free method for achieving multicalibration in predictors across diverse subpopulations. Unlike existing approaches that rely on discretization, this method directly optimizes an empirical risk objective using an ensemble of depth-two decision trees, enabling implementation with common tree ensemble learning methods. The algorithm ensures multicalibration under a condition termed loss saturation, which is found to hold in practical datasets. Empirical evaluations demonstrate that the proposed algorithm consistently matches or surpasses existing methods, even when evaluated using comparable metrics. This approach eliminates the need for discretization, avoiding associated issues such as rounding error and hyperparameter sensitivity, and allows for improved downstream decision-making. <div>
arXiv:2505.17435v1 Announce Type: new 
Abstract: In recent years, multicalibration has emerged as a desirable learning objective for ensuring that a predictor is calibrated across a rich collection of overlapping subpopulations. Existing approaches typically achieve multicalibration by discretizing the predictor's output space and iteratively adjusting its output values. However, this discretization approach departs from the standard empirical risk minimization (ERM) pipeline, introduces rounding error and additional sensitive hyperparameter, and may distort the predictor's outputs in ways that hinder downstream decision-making.
  In this work, we propose a discretization-free multicalibration method that directly optimizes an empirical risk objective over an ensemble of depth-two decision trees. Our ERM approach can be implemented using off-the-shelf tree ensemble learning methods such as LightGBM. Our algorithm provably achieves multicalibration, provided that the data distribution satisfies a technical condition we term as loss saturation. Across multiple datasets, our empirical evaluation shows that this condition is always met in practice. Our discretization-free algorithm consistently matches or outperforms existing multicalibration approaches--even when evaluated using a discretization-based multicalibration metric that shares its discretization granularity with the baselines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning</title>
<link>https://arxiv.org/abs/2505.17439</link>
<guid>https://arxiv.org/abs/2505.17439</guid>
<content:encoded><![CDATA[
<div> Keywords: humanitarian supply chain, reinforcement learning, PPO, heuristic algorithms, satisfaction rate

Summary:
This study introduces a novel approach to designing a dynamic and efficient humanitarian supply chain using reinforcement learning, specifically Proximal Policy Optimization (PPO). The model constructed with PPO prioritizes optimizing the average satisfaction rate in an equitable manner. By comparing the performance of PPO with traditional heuristic algorithms, the study demonstrates the superiority of the reinforcement learning-based approach in achieving higher levels of satisfaction and efficiency in humanitarian supply chain management. This research contributes to enhancing the responsiveness and effectiveness of humanitarian operations by leveraging state-of-the-art machine learning techniques to optimize decision-making processes. <div>
arXiv:2505.17439v1 Announce Type: new 
Abstract: This study designs an efficient and equitable humanitarian supply chain dynamically by using reinforcement learning, PPO, and compared with heuristic algorithms. This study demonstrates the model of PPO always treats average satisfaction rate as the priority.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning</title>
<link>https://arxiv.org/abs/2505.17448</link>
<guid>https://arxiv.org/abs/2505.17448</guid>
<content:encoded><![CDATA[
<div> algorithm, clickbait, YouTube, deep learning, video

Summary:
The study introduces an algorithm called BaitRadar to tackle the issue of clickbait on YouTube. Clickbait entices users to click on videos through appealing titles and thumbnails, misleading them about the video content. BaitRadar employs six inference models that analyze different attributes of a video, such as title, comments, thumbnail, tags, video statistics, and audio transcript. By considering multiple models and averaging their outputs, BaitRadar enhances classification accuracy, even in cases of missing data. Testing on 1,400 YouTube videos demonstrates an average test accuracy of 98% with an inference time of less than 2 seconds. BaitRadar aims to combat clickbait and ensure users receive accurate video content based on the information presented. 

<br /><br />Summary: <div>
arXiv:2505.17448v1 Announce Type: new 
Abstract: Following the rising popularity of YouTube, there is an emerging problem on this platform called clickbait, which provokes users to click on videos using attractive titles and thumbnails. As a result, users ended up watching a video that does not have the content as publicized in the title. This issue is addressed in this study by proposing an algorithm called BaitRadar, which uses a deep learning technique where six inference models are jointly consulted to make the final classification decision. These models focus on different attributes of the video, including title, comments, thumbnail, tags, video statistics and audio transcript. The final classification is attained by computing the average of multiple models to provide a robust and accurate output even in situation where there is missing data. The proposed method is tested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved with an inference time of less than 2s.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIMB: Class-imbalanced Learning Benchmark on Tabular Data</title>
<link>https://arxiv.org/abs/2505.17451</link>
<guid>https://arxiv.org/abs/2505.17451</guid>
<content:encoded><![CDATA[
<div> benchmark, class-imbalanced learning, tabular data, algorithms, Python package

Summary: 
The paper introduces CLIMB, a benchmark for class-imbalanced learning on tabular data, comprising 73 real-world datasets and 29 CIL algorithms. The benchmark aims to address the imbalance in critical yet rare outcomes in various domains. The CLIMB includes a high-quality open-source Python package with standardized API designs, documentation, and code quality controls. Through extensive experiments, the paper provides insights on method accuracy and efficiency, emphasizing the limitations of simple rebalancing methods, the effectiveness of ensembles, and the significance of data quality. The comprehensive benchmark and experiments aim to facilitate the implementation and comparison of CIL algorithms for different imbalanced learning tasks. The code, documentation, and examples of CLIMB can be accessed at https://github.com/ZhiningLiu1998/imbalanced-ensemble. <div>
arXiv:2505.17451v1 Announce Type: new 
Abstract: Class-imbalanced learning (CIL) on tabular data is important in many real-world applications where the minority class holds the critical but rare outcomes. In this paper, we present CLIMB, a comprehensive benchmark for class-imbalanced learning on tabular data. CLIMB includes 73 real-world datasets across diverse domains and imbalance levels, along with unified implementations of 29 representative CIL algorithms. Built on a high-quality open-source Python package with unified API designs, detailed documentation, and rigorous code quality controls, CLIMB supports easy implementation and comparison between different CIL algorithms. Through extensive experiments, we provide practical insights on method accuracy and efficiency, highlighting the limitations of naive rebalancing, the effectiveness of ensembles, and the importance of data quality. Our code, documentation, and examples are available at https://github.com/ZhiningLiu1998/imbalanced-ensemble.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Training Large Language Models with Confident Reasoning</title>
<link>https://arxiv.org/abs/2505.17454</link>
<guid>https://arxiv.org/abs/2505.17454</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, self-training, reasoning-level confidence, CORE-PO, Policy Optimization

Summary:
Large language models (LLMs) have shown impressive performance in generating reasoning paths, but the high cost of human supervision for learning these paths is a challenge. Recent studies have explored self-training methods to improve reasoning capabilities using pseudo-labels generated by LLMs. However, existing methods focus solely on the quality of final answers and may overlook the quality of reasoning paths. This article proposes a new self-training method, CORE-PO, which prioritizes high-confidence reasoning paths through Policy Optimization. Experimental results demonstrate that CORE-PO enhances the accuracy of outputs on various benchmarks, including in-distribution and out-of-distribution datasets. By considering reasoning-level confidence, CORE-PO improves the overall performance of LLMs in generating high-quality reasoning paths. <div>
arXiv:2505.17454v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive performance by generating reasoning paths before final answers, but learning such a reasoning path requires costly human supervision. To address this issue, recent studies have explored self-training methods that improve reasoning capabilities using pseudo-labels generated by the LLMs themselves. Among these, confidence-based self-training fine-tunes LLMs to prefer reasoning paths with high-confidence answers, where confidence is estimated via majority voting. However, such methods exclusively focus on the quality of the final answer and may ignore the quality of the reasoning paths, as even an incorrect reasoning path leads to a correct answer by chance. Instead, we advocate the use of reasoning-level confidence to identify high-quality reasoning paths for self-training, supported by our empirical observations. We then propose a new self-training method, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths through Policy Optimization. Our experiments show that CORE-PO improves the accuracy of outputs on four in-distribution and two out-of-distribution benchmarks, compared to existing self-training methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.17458</link>
<guid>https://arxiv.org/abs/2505.17458</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, heterogeneous graphs, continual learning, Meta-learning, knowledge distillation 

Summary: 
Machine learning on heterogeneous graphs has advanced rapidly, addressing the challenge of continual learning on evolving graph structures. The Meta-learning based Knowledge Distillation framework (MKD) combines meta-learning for rapid task adaptation with knowledge distillation to balance new information and existing knowledge. MKD uses a novel sampling strategy to efficiently select target-type nodes and retrieve first-order neighbors along metapaths. It incorporates a semantic-level distillation module to align attention distributions between teacher and student models for semantic consistency. Evaluations on benchmark datasets validate MKD's effectiveness in handling continual learning on expanding heterogeneous graphs. <div>
arXiv:2505.17458v1 Announce Type: new 
Abstract: Machine learning on heterogeneous graphs has experienced rapid advancement in recent years, driven by the inherently heterogeneous nature of real-world data. However, existing studies typically assume the graphs to be static, while real-world graphs are continuously expanding. This dynamic nature requires models to adapt to new data while preserving existing knowledge. To this end, this work addresses the challenge of continual learning on heterogeneous graphs by introducing the Meta-learning based Knowledge Distillation framework (MKD), designed to mitigate catastrophic forgetting in evolving heterogeneous graph structures. MKD combines rapid task adaptation through meta-learning on limited samples with knowledge distillation to achieve an optimal balance between incorporating new information and maintaining existing knowledge. To improve the efficiency and effectiveness of sample selection, MKD incorporates a novel sampling strategy that selects a small number of target-type nodes based on node diversity and maintains fixed-size buffers for other types. The strategy retrieves first-order neighbors along metapaths and selects important neighbors based on their structural relevance, enabling the sampled subgraphs to retain key topological and semantic information. In addition, MKD introduces a semantic-level distillation module that aligns the attention distributions over different metapaths between teacher and student models, encouraging semantic consistency beyond the logit level. Comprehensive evaluations across three benchmark datasets validate MKD's effectiveness in handling continual learning scenarios on expanding heterogeneous graphs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient compression of neural networks and datasets</title>
<link>https://arxiv.org/abs/2505.17469</link>
<guid>https://arxiv.org/abs/2505.17469</guid>
<content:encoded><![CDATA[
<div> compression, neural networks, data compression, regularization, optimization

Summary: 
This article discusses methods to reduce the number of parameters in neural networks while maintaining high accuracy. The authors compare and improve existing techniques for data compression and introduce a probabilistic approach to $\ell_0$ regularized optimization. They explore layerwise methods and investigate different architectures and datasets, including image datasets and Wikipedia excerpts. A synthetic teacher-student setup is also implemented for controlled compression analysis. The concept of compression algorithms is related to Solomonoff's theory of inductive inference, and the article confirms that regularized models can exhibit more sample-efficient convergence. <div>
arXiv:2505.17469v1 Announce Type: new 
Abstract: We compare, improve, and contribute methods that substantially decrease the number of parameters of neural networks while maintaining high test accuracy. When applying our methods to minimize description length, we obtain very effective data compression algorithms. In particular, we develop a probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear models that does not require Monte-Carlo sampling and thus improves upon previous methods. We also improve upon methods involving smooth approximations to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods on different architectures and datasets, including convolutional networks trained on image datasets and transformers trained on parts of Wikipedia. We also created a synthetic teacher-student setup to investigate compression in a controlled continuous setting. Finally, we conceptually relate compression algorithms to Solomonoff's theory of inductive inference and empirically verify the prediction that regularized models can exhibit more sample-efficient convergence.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance</title>
<link>https://arxiv.org/abs/2505.17477</link>
<guid>https://arxiv.org/abs/2505.17477</guid>
<content:encoded><![CDATA[
<div> Keywords: Reverse-Speech-Finder, Alzheimer's Disease, neural network, speech analysis, diagnostic models

Summary:
Reverse-Speech-Finder (RSF) is a novel neural network architecture that enhances Alzheimer's Disease (AD) diagnosis through speech analysis by identifying AD-specific speech markers. RSF leverages pre-trained large language models to address the scarcity of real AD speech samples and limited interpretability in existing models. The architecture utilizes a unique approach involving the identification of the most probable speech markers (MPMs) and neurons (MPNs) for AD prediction. By backtracking from MPNs to the input layer, RSF identifies the most probable speech-tokens (MPTs) and corresponding MPMs, uncovering new speech markers for AD detection. Experimental results show RSF outperforms traditional methods, improving accuracy and F1-score. By generating speech data with novel markers, RSF enhances the robustness and accuracy of AD diagnostic models, offering insights into AD-related linguistic deficits and potential for effective early intervention strategies.

<br /><br />Summary: <div>
arXiv:2505.17477v1 Announce Type: new 
Abstract: This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression</title>
<link>https://arxiv.org/abs/2505.17478</link>
<guid>https://arxiv.org/abs/2505.17478</guid>
<content:encoded><![CDATA[
<div> autoregressive model, protein dynamics, molecular dynamics, conformational space, protein folding

Summary:
ConfRover is a new autoregressive model designed to understand protein dynamics by learning from molecular dynamics (MD) data. This model captures temporal dependencies between protein conformations and supports both time-dependent and time-independent sampling. It includes an encoding layer to embed protein-specific information and conformation, a temporal module to capture conformational dynamics, and an SE(3) diffusion model for structure generation. ConfRover is effective in learning conformational dynamics from the ATLAS protein MD dataset and can be used for various downstream tasks. It is the first model to generate protein conformations and trajectories simultaneously, providing a novel and flexible approach to learning from protein MD data.<br /><br />Summary: <div>
arXiv:2505.17478v1 Announce Type: new 
Abstract: Understanding protein dynamics is critical for elucidating their biological functions. The increasing availability of molecular dynamics (MD) data enables the training of deep generative models to efficiently explore the conformational space of proteins. However, existing approaches either fail to explicitly capture the temporal dependencies between conformations or do not support direct generation of time-independent samples. To address these limitations, we introduce ConfRover, an autoregressive model that simultaneously learns protein conformation and dynamics from MD trajectories, supporting both time-dependent and time-independent sampling. At the core of our model is a modular architecture comprising: (i) an encoding layer, adapted from protein folding models, that embeds protein-specific information and conformation at each time frame into a latent space; (ii) a temporal module, a sequence model that captures conformational dynamics across frames; and (iii) an SE(3) diffusion model as the structure decoder, generating conformations in continuous space. Experiments on ATLAS, a large-scale protein MD dataset of diverse structures, demonstrate the effectiveness of our model in learning conformational dynamics and supporting a wide range of downstream tasks. ConfRover is the first model to sample both protein conformations and trajectories within a single framework, offering a novel and flexible approach for learning from protein MD data.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning</title>
<link>https://arxiv.org/abs/2505.17483</link>
<guid>https://arxiv.org/abs/2505.17483</guid>
<content:encoded><![CDATA[
arXiv:2505.17483v1 Announce Type: new 
Abstract: Nitrate ($\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived primarily from anthropogenic sources. The recent increase in river-discharged nitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR) lagoon. Although nitrate is an optically inactive (i.e., colourless) constituent, previous studies have demonstrated there is an indirect, non-causal relationship between water surface nitrate and water-leaving reflectance that is mediated through optically active water quality parameters such as total suspended solids and coloured dissolved organic matter. This work aims to advance our understanding of this relationship with an effort to measure time-series nitrate and simultaneous hyperspectral reflectance at the Fitzroy River estuary, Queensland, Australia. Time-series observations revealed periodic cycles in nitrate loads due to the tidal influence in the estuarine study site. The water surface nitrate loads were predicted from hyperspectral reflectance and water salinity measurements, with hyperspectral reflectance indicating the concentrations of optically active variables and salinity indicating the mixing of river water and seawater proportions. The accuracy assessment of model-predicted nitrate against in-situ measured nitrate values showed that the predicted nitrate values correlated well with the ground-truth data, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work demonstrates the feasibility of predicting water surface nitrate from hyperspectral reflectance and salinity measurements.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics</title>
<link>https://arxiv.org/abs/2505.17488</link>
<guid>https://arxiv.org/abs/2505.17488</guid>
<content:encoded><![CDATA[
arXiv:2505.17488v1 Announce Type: new 
Abstract: Non-stationary power system dynamics, influenced by renewable energy variability, evolving demand patterns, and climate change, are becoming increasingly complex. Accurately capturing these dynamics requires a model capable of adapting to environmental factors. Traditional models, including Recurrent Neural Networks (RNNs), lack efficient mechanisms to encode external factors, such as time or environmental data, for dynamic adaptation. To address this, we propose the External Adaptive RNN (ExARNN), a novel framework that integrates external data (e.g., weather, time) to continuously adjust the parameters of a base RNN. ExARNN achieves this through a hierarchical hypernetwork design, using Neural Controlled Differential Equations (NCDE) to process external data and generate RNN parameters adaptively. This approach enables ExARNN to handle inconsistent timestamps between power and external measurements, ensuring continuous adaptation. Extensive forecasting tests demonstrate ExARNN's superiority over established baseline models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs</title>
<link>https://arxiv.org/abs/2505.17495</link>
<guid>https://arxiv.org/abs/2505.17495</guid>
<content:encoded><![CDATA[
arXiv:2505.17495v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often hierarchical -- higher-order interactions are accompanied by their lower-order subsets -- which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20% over marginal attribution approaches while using $10\times$ fewer inferences than SPEX. By accounting for interactions, ProxySPEX identifies features that influence model output over 20% more than those selected by marginal approaches. Further, we apply ProxySPEX to two interpretability tasks. Data attribution, where we identify interactions among CIFAR-10 training samples that influence test predictions, and mechanistic interpretability, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. ProxySPEX identifies interactions that enable more aggressive pruning of heads than marginal approaches.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.17508</link>
<guid>https://arxiv.org/abs/2505.17508</guid>
<content:encoded><![CDATA[
arXiv:2505.17508v1 Announce Type: new 
Abstract: Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection</title>
<link>https://arxiv.org/abs/2505.17513</link>
<guid>https://arxiv.org/abs/2505.17513</guid>
<content:encoded><![CDATA[
arXiv:2505.17513v1 Announce Type: new 
Abstract: Recent advances in text-to-speech technologies have enabled realistic voice generation, fueling audio-based deepfake attacks such as fraud and impersonation. While audio anti-spoofing systems are critical for detecting such threats, prior work has predominantly focused on acoustic-level perturbations, leaving the impact of linguistic variation largely unexplored. In this paper, we investigate the linguistic sensitivity of both open-source and commercial anti-spoofing detectors by introducing transcript-level adversarial attacks. Our extensive evaluation reveals that even minor linguistic perturbations can significantly degrade detection accuracy: attack success rates surpass 60% on several open-source detector-voice pairs, and notably one commercial detection accuracy drops from 100% on synthetic audio to just 32%. Through a comprehensive feature attribution analysis, we identify that both linguistic complexity and model-level audio embedding similarity contribute strongly to detector vulnerability. We further demonstrate the real-world risk via a case study replicating the Brad Pitt audio deepfake scam, using transcript adversarial attacks to completely bypass commercial detectors. These results highlight the need to move beyond purely acoustic defenses and account for linguistic variation in the design of robust anti-spoofing systems. All source code will be publicly available.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spacetime Geometry of Denoising in Diffusion Models</title>
<link>https://arxiv.org/abs/2505.17517</link>
<guid>https://arxiv.org/abs/2505.17517</guid>
<content:encoded><![CDATA[
arXiv:2505.17517v1 Announce Type: new 
Abstract: We present a novel perspective on diffusion models using the framework of information geometry. We show that the set of noisy samples, taken across all noise levels simultaneously, forms a statistical manifold -- a family of denoising probability distributions. Interpreting the noise level as a temporal parameter, we refer to this manifold as spacetime. This manifold naturally carries a Fisher-Rao metric, which defines geodesics -- shortest paths between noisy points. Notably, this family of distributions is exponential, enabling efficient geodesic computation even in high-dimensional settings without retraining or fine-tuning. We demonstrate the practical value of this geometric viewpoint in transition path sampling, where spacetime geodesics define smooth sequences of Boltzmann distributions, enabling the generation of continuous trajectories between low-energy metastable states. Code is available at: https://github.com/Aalto-QuML/diffusion-spacetime-geometry.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting</title>
<link>https://arxiv.org/abs/2505.17532</link>
<guid>https://arxiv.org/abs/2505.17532</guid>
<content:encoded><![CDATA[
arXiv:2505.17532v1 Announce Type: new 
Abstract: Recent studies have shown that by introducing prior knowledge, multi-scale analysis of complex and non-stationary time series in real environments can achieve good results in the field of long-term forecasting. However, affected by channel-independent methods, models based on multi-scale analysis may produce suboptimal prediction results due to the autocorrelation between time series labels, which in turn affects the generalization ability of the model. To address this challenge, we are inspired by the idea of sharpness-aware minimization and the recently proposed FreDF method and design a deep learning model TimeCF for long-term time series forecasting based on the TimeMixer, combined with our designed adaptive convolution information aggregation module and Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically, TimeCF first decomposes the original time series into sequences of different scales. Next, the same-sized convolution modules are used to adaptively aggregate information of different scales on sequences of different scales. Then, decomposing each sequence into season and trend parts and the two parts are mixed at different scales through bottom-up and top-down methods respectively. Finally, different scales are aggregated through a Feed-Forward Network. What's more, extensive experimental results on different real-world datasets show that our proposed TimeCF has excellent performance in the field of long-term forecasting.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Representational Disparities</title>
<link>https://arxiv.org/abs/2505.17533</link>
<guid>https://arxiv.org/abs/2505.17533</guid>
<content:encoded><![CDATA[
arXiv:2505.17533v1 Announce Type: new 
Abstract: We propose a fair machine learning algorithm to model interpretable differences between observed and desired human decision-making, with the latter aimed at reducing disparity in a downstream outcome impacted by the human decision. Prior work learns fair representations without considering the outcome in the decision-making process. We model the outcome disparities as arising due to the different representations of the input seen by the observed and desired decision-maker, which we term representational disparities. Our goal is to learn interpretable representational disparities which could potentially be corrected by specific nudges to the human decision, mitigating disparities in the downstream outcome; we frame this as a multi-objective optimization problem using a neural network. Under reasonable simplifying assumptions, we prove that our neural network model of the representational disparity learns interpretable weights that fully mitigate the outcome disparity. We validate objectives and interpret results using real-world German Credit, Adult, and Heritage Health datasets.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Style Transfer for Counterfactual Explainability</title>
<link>https://arxiv.org/abs/2505.17542</link>
<guid>https://arxiv.org/abs/2505.17542</guid>
<content:encoded><![CDATA[
arXiv:2505.17542v1 Announce Type: new 
Abstract: Counterfactual explainability seeks to uncover model decisions by identifying minimal changes to the input that alter the predicted outcome. This task becomes particularly challenging for graph data due to preserving structural integrity and semantic meaning. Unlike prior approaches that rely on forward perturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the first framework to re-imagine graph counterfactual generation as a backtracking process, leveraging spectral style transfer. By aligning the global structure with the original input spectrum and preserving local content faithfulness, GIST produces valid counterfactuals as interpolations between the input style and counterfactual content. Tested on 8 binary and multi-class graph classification benchmarks, GIST achieves a remarkable +7.6% improvement in the validity of produced counterfactuals and significant gains (+45.5%) in faithfully explaining the true class distribution. Additionally, GIST's backtracking mechanism effectively mitigates overshooting the underlying predictor's decision boundary, minimizing the spectral differences between the input and the counterfactuals. These results challenge traditional forward perturbation methods, offering a novel perspective that advances graph explainability.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing</title>
<link>https://arxiv.org/abs/2505.17552</link>
<guid>https://arxiv.org/abs/2505.17552</guid>
<content:encoded><![CDATA[
arXiv:2505.17552v1 Announce Type: new 
Abstract: De novo peptide sequencing is a critical task in proteomics. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its base models used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art benchmark. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo sequencing. Our source code is provided on GitHub.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning</title>
<link>https://arxiv.org/abs/2505.17553</link>
<guid>https://arxiv.org/abs/2505.17553</guid>
<content:encoded><![CDATA[
arXiv:2505.17553v1 Announce Type: new 
Abstract: In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves specializing functionalities into different experts and sparsely activating them appropriately, has been widely adopted as a promising approach to trade-off between model capacity and computation overhead. However, current MoE variants fall short on heterogeneous datasets, ignoring the fact that experts may learn similar knowledge, resulting in the underutilization of MoE's capacity. In this paper, we propose Contrastive Representation for MoE (CoMoE), a novel method to promote modularization and specialization in MoE, where the experts are trained along with a contrastive objective by sampling from activated and inactivated experts in top-k routing. We demonstrate that such a contrastive objective recovers the mutual-information gap between inputs and the two types of experts. Experiments on several benchmarks and in multi-task settings demonstrate that CoMoE can consistently enhance MoE's capacity and promote modularization among the experts.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wildfire spread forecasting with Deep Learning</title>
<link>https://arxiv.org/abs/2505.17556</link>
<guid>https://arxiv.org/abs/2505.17556</guid>
<content:encoded><![CDATA[
arXiv:2505.17556v1 Announce Type: new 
Abstract: Accurate prediction of wildfire spread is crucial for effective risk management, emergency response, and strategic resource allocation. In this study, we present a deep learning (DL)-based framework for forecasting the final extent of burned areas, using data available at the time of ignition. We leverage a spatio-temporal dataset that covers the Mediterranean region from 2006 to 2022, incorporating remote sensing data, meteorological observations, vegetation maps, land cover classifications, anthropogenic factors, topography data, and thermal anomalies. To evaluate the influence of temporal context, we conduct an ablation study examining how the inclusion of pre- and post-ignition data affects model performance, benchmarking the temporal-aware DL models against a baseline trained exclusively on ignition-day inputs. Our results indicate that multi-day observational data substantially improve predictive accuracy. Particularly, the best-performing model, incorporating a temporal window of four days before to five days after ignition, improves both the F1 score and the Intersection over Union by almost 5% in comparison to the baseline on the test dataset. We publicly release our dataset and models to enhance research into data-driven approaches for wildfire modeling and response.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs</title>
<link>https://arxiv.org/abs/2505.17575</link>
<guid>https://arxiv.org/abs/2505.17575</guid>
<content:encoded><![CDATA[
arXiv:2505.17575v1 Announce Type: new 
Abstract: Solving partial differential equations (PDEs) with machine learning has recently attracted great attention, as PDEs are fundamental tools for modeling real-world systems that range from fundamental physical science to advanced engineering disciplines. Most real-world physical systems across various disciplines are actually involved in multiple coupled physical fields rather than a single field. However, previous machine learning studies mainly focused on solving single-field problems, but overlooked the importance and characteristics of multiphysics problems in real world. Multiphysics PDEs typically entail multiple strongly coupled variables, thereby introducing additional complexity and challenges, such as inter-field coupling. Both benchmarking and solving multiphysics problems with machine learning remain largely unexamined. To identify and address the emerging challenges in multiphysics problems, we mainly made three contributions in this work. First, we collect the first general multiphysics dataset, the Multiphysics Bench, that focuses on multiphysics PDE solving with machine learning. Multiphysics Bench is also the most comprehensive PDE dataset to date, featuring the broadest range of coupling types, the greatest diversity of PDE formulations, and the largest dataset scale. Second, we conduct the first systematic investigation on multiple representative learning-based PDE solvers, such as PINNs, FNO, DeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately, naively applying these existing solvers usually show very poor performance for solving multiphysics. Third, through extensive experiments and discussions, we report multiple insights and a bag of useful tricks for solving multiphysics with machine learning, motivating future directions in the study and simulation of complex, coupled physical systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation</title>
<link>https://arxiv.org/abs/2505.17579</link>
<guid>https://arxiv.org/abs/2505.17579</guid>
<content:encoded><![CDATA[
arXiv:2505.17579v1 Announce Type: new 
Abstract: In this paper, we propose a novel framework for ownership verification of deep neural network (DNN) models for image classification tasks. It allows verification of model identity by both the rightful owner and third party without presenting the original model. We assume a gray-box scenario where an unauthorized user owns a model that is illegally copied from the original model, provides services in a cloud environment, and the user throws images and receives the classification results as a probability distribution of output classes. The framework applies a white-box adversarial attack to align the output probability of a specific class to a designated value. Due to the knowledge of original model, it enables the owner to generate such adversarial examples. We propose a simple but effective adversarial attack method based on the iterative Fast Gradient Sign Method (FGSM) by introducing control parameters. Experimental results confirm the effectiveness of the identification of DNN models using adversarial attack.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity</title>
<link>https://arxiv.org/abs/2505.17591</link>
<guid>https://arxiv.org/abs/2505.17591</guid>
<content:encoded><![CDATA[
arXiv:2505.17591v1 Announce Type: new 
Abstract: In autonomous navigation systems, the solution of the place recognition problem is crucial for their safe functioning. But this is not a trivial solution, since it must be accurate regardless of any changes in the scene, such as seasonal changes and different weather conditions, and it must be generalizable to other environments. This paper presents our method, MinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input data to obtain its spherical coordinates and intensity values normalized within a range of 0 to 1 for each point, and it produces a robust place recognition descriptor. To that end, a deep learning approach that combines Minkowski convolutions and a U-net architecture with skip connections is used. The results of MinkUNeXt-SI demonstrate that this method reaches and surpasses state-of-the-art performance while it also generalizes satisfactorily to other datasets. Additionally, we showcase the capture of a custom dataset and its use in evaluating our solution, which also achieves outstanding results. Both the code of our solution and the runs of our dataset are publicly available for reproducibility purposes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeUQI: Near-Optimal Uniform Quantization Parameter Initialization</title>
<link>https://arxiv.org/abs/2505.17595</link>
<guid>https://arxiv.org/abs/2505.17595</guid>
<content:encoded><![CDATA[
arXiv:2505.17595v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve impressive performance across domains but face significant challenges when deployed on consumer-grade GPUs or personal devices such as laptops, due to high memory consumption and inference costs. Post-training quantization (PTQ) of LLMs offers a promising solution that reduces their memory footprint and decoding latency. In practice, PTQ with uniform quantization representation is favored for its efficiency and ease of deployment since uniform quantization is widely supported by mainstream hardware and software libraries. Recent studies on $\geq 2$-bit uniform quantization have led to noticeable improvements in post-quantization model performance; however, they primarily focus on quantization methodologies, while the initialization of quantization parameters is underexplored and still relies on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method devoted to efficiently determining near-optimal initial parameters for uniform quantization. NeUQI is orthogonal to prior quantization methodologies and can seamlessly integrate with them. The experiments with the LLaMA and Qwen families on various tasks demonstrate that our NeUQI consistently outperforms existing methods. Furthermore, when combined with a lightweight distillation strategy, NeUQI can achieve superior performance to PV-tuning, a much more resource-intensive approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2505.17599</link>
<guid>https://arxiv.org/abs/2505.17599</guid>
<content:encoded><![CDATA[
arXiv:2505.17599v1 Announce Type: new 
Abstract: Large language models (LLMs) have been used in many zero-shot learning problems, with their strong generalization ability. Recently, adopting LLMs in text-attributed graphs (TAGs) has drawn increasing attention. However, the adoption of LLMs faces two major challenges: limited information on graph structure and unreliable responses. LLMs struggle with text attributes isolated from the graph topology. Worse still, they yield unreliable predictions due to both information insufficiency and the inherent weakness of LLMs (e.g., hallucination). Towards this end, this paper proposes a novel method named Dynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of texts to obtain bundle-level labels and uses these labels to supervise graph neural networks. Specifically, we sample a set of bundles, each containing a set of nodes with corresponding texts of close proximity. We then query LLMs with the bundled texts to obtain the label of each bundle. Subsequently, the bundle labels are used to supervise the optimization of graph neural networks, and the bundles are further refined to exclude noisy items. To justify our design, we also provide theoretical analysis of the proposed method. Extensive experiments across ten datasets validate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Semantic Token Communication for Transformer-based Edge Inference</title>
<link>https://arxiv.org/abs/2505.17604</link>
<guid>https://arxiv.org/abs/2505.17604</guid>
<content:encoded><![CDATA[
arXiv:2505.17604v1 Announce Type: new 
Abstract: This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning</title>
<link>https://arxiv.org/abs/2505.17610</link>
<guid>https://arxiv.org/abs/2505.17610</guid>
<content:encoded><![CDATA[
arXiv:2505.17610v1 Announce Type: new 
Abstract: This paper provides the first expert sample complexity characterization for learning a Nash equilibrium from expert data in Markov Games. We show that a new quantity named the single policy deviation concentrability coefficient is unavoidable in the non-interactive imitation learning setting, and we provide an upper bound for behavioral cloning (BC) featuring such coefficient. BC exhibits substantial regret in games with high concentrability coefficient, leading us to utilize expert queries to develop and introduce two novel solution algorithms: MAIL-BRO and MURMAIL. The former employs a best response oracle and learns an $\varepsilon$-Nash equilibrium with $\mathcal{O}(\varepsilon^{-4})$ expert and oracle queries. The latter bypasses completely the best response oracle at the cost of a worse expert query complexity of order $\mathcal{O}(\varepsilon^{-8})$. Finally, we provide numerical evidence, confirming our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language model as user daily behavior data generator: balancing population diversity and individual personality</title>
<link>https://arxiv.org/abs/2505.17615</link>
<guid>https://arxiv.org/abs/2505.17615</guid>
<content:encoded><![CDATA[
arXiv:2505.17615v1 Announce Type: new 
Abstract: Predicting human daily behavior is challenging due to the complexity of routine patterns and short-term fluctuations. While data-driven models have improved behavior prediction by leveraging empirical data from various platforms and devices, the reliance on sensitive, large-scale user data raises privacy concerns and limits data availability. Synthetic data generation has emerged as a promising solution, though existing methods are often limited to specific applications. In this work, we introduce BehaviorGen, a framework that uses large language models (LLMs) to generate high-quality synthetic behavior data. By simulating user behavior based on profiles and real events, BehaviorGen supports data augmentation and replacement in behavior prediction models. We evaluate its performance in scenarios such as pertaining augmentation, fine-tuning replacement, and fine-tuning augmentation, achieving significant improvements in human mobility and smartphone usage predictions, with gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen to enhance user behavior modeling through flexible and privacy-preserving synthetic data generation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration</title>
<link>https://arxiv.org/abs/2505.17621</link>
<guid>https://arxiv.org/abs/2505.17621</guid>
<content:encoded><![CDATA[
arXiv:2505.17621v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the reasoning capabilities of Large Language Models (LLMs). However, prevalent RL approaches such as Proximal Policy Optimization (PPO) and Group-Regularized Policy Optimization (GRPO) face critical limitations due to their reliance on sparse outcome-based rewards and inadequate mechanisms for incentivizing exploration. These limitations result in inefficient guidance for multi-step reasoning processes. Specifically, sparse reward signals fail to deliver effective or sufficient feedback, particularly for challenging problems. Furthermore, such reward structures induce systematic biases that prioritize exploitation of familiar trajectories over novel solution discovery. These shortcomings critically hinder performance in complex reasoning tasks, which inherently demand iterative refinement across ipntermediate steps. To address these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense rewards and amplify explorations in the RL-based training paradigm. i-MENTOR introduces three key innovations: trajectory-aware exploration rewards that mitigate bias in token-level strategies while maintaining computational efficiency; dynamic reward scaling to stabilize exploration and exploitation in large action spaces; and advantage-preserving reward implementation that maintains advantage distribution integrity while incorporating exploratory guidance. Experiments across three public datasets demonstrate i-MENTOR's effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Stochastic Depth Training for Adaptive Inference</title>
<link>https://arxiv.org/abs/2505.17626</link>
<guid>https://arxiv.org/abs/2505.17626</guid>
<content:encoded><![CDATA[
arXiv:2505.17626v1 Announce Type: new 
Abstract: Dynamic DNN optimization techniques such as layer-skipping offer increased adaptability and efficiency gains but can lead to i) a larger memory footprint as in decision gates, ii) increased training complexity (e.g., with non-differentiable operations), and iii) less control over performance-quality trade-offs due to its inherent input-dependent execution. To approach these issues, we propose a simpler yet effective alternative for adaptive inference with a zero-overhead, single-model, and time-predictable inference. Central to our approach is the observation that models trained with Stochastic Depth -- a method for faster training of residual networks -- become more resilient to arbitrary layer-skipping at inference time. We propose a method to first select near Pareto-optimal skipping configurations from a stochastically-trained model to adapt the inference at runtime later. Compared to original ResNets, our method shows improvements of up to 2X in power efficiency at accuracy drops as low as 0.71%.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis</title>
<link>https://arxiv.org/abs/2505.17636</link>
<guid>https://arxiv.org/abs/2505.17636</guid>
<content:encoded><![CDATA[
arXiv:2505.17636v1 Announce Type: new 
Abstract: Various AI safety datasets have been developed to measure LLMs against evolving interpretations of harm. Our evaluation of five recently published open-source safety benchmarks reveals distinct semantic clusters using UMAP dimensionality reduction and kmeans clustering (silhouette score: 0.470). We identify six primary harm categories with varying benchmark representation. GretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix emphasizes self-harm scenarios. Significant differences in prompt length distribution suggests confounds to data collection and interpretations of harm as well as offer possible context. Our analysis quantifies benchmark orthogonality among AI benchmarks, allowing for transparency in coverage gaps despite topical similarities. Our quantitative framework for analyzing semantic orthogonality across safety benchmarks enables more targeted development of datasets that comprehensively address the evolving landscape of harms in AI use, however that is defined in the future.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach</title>
<link>https://arxiv.org/abs/2505.17637</link>
<guid>https://arxiv.org/abs/2505.17637</guid>
<content:encoded><![CDATA[
arXiv:2505.17637v1 Announce Type: new 
Abstract: Spatio-temporal prediction plays a crucial role in intelligent transportation, weather forecasting, and urban planning. While integrating multi-modal data has shown potential for enhancing prediction accuracy, key challenges persist: (i) inadequate fusion of multi-modal information, (ii) confounding factors that obscure causal relations, and (iii) high computational complexity of prediction models. To address these challenges, we propose E^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal Prediction framework. E^2-CSTP leverages cross-modal attention and gating mechanisms to effectively integrate multi-modal data. Building on this, we design a dual-branch causal inference approach: the primary branch focuses on spatio-temporal prediction, while the auxiliary branch mitigates bias by modeling additional modalities and applying causal interventions to uncover true causal dependencies. To improve model efficiency, we integrate GCN with the Mamba architecture for accelerated spatio-temporal encoding. Extensive experiments on 4 real-world datasets show that E^2-CSTP significantly outperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in accuracy as well as 17.37%-56.11% reductions in computational overhead.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training</title>
<link>https://arxiv.org/abs/2505.17638</link>
<guid>https://arxiv.org/abs/2505.17638</guid>
<content:encoded><![CDATA[
arXiv:2505.17638v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\tau_\mathrm{mem}$ increases linearly with the training set size $n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval</title>
<link>https://arxiv.org/abs/2505.17639</link>
<guid>https://arxiv.org/abs/2505.17639</guid>
<content:encoded><![CDATA[
arXiv:2505.17639v1 Announce Type: new 
Abstract: Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\% accuracy on MATH500 when pruned to 8/128 configuration (50\% expert reduction), and still achieves 72.0\% with aggressive 8/32 pruning (87.5\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\% on MATH500 and 81.3\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Network Science Approach to Granular Time Series Segmentation</title>
<link>https://arxiv.org/abs/2505.17640</link>
<guid>https://arxiv.org/abs/2505.17640</guid>
<content:encoded><![CDATA[
arXiv:2505.17640v1 Announce Type: new 
Abstract: Time series segmentation (TSS) is one of the time series (TS) analysis techniques, that has received considerably less attention compared to other TS related tasks. In recent years, deep learning architectures have been introduced for TSS, however their reliance on sliding windows limits segmentation granularity due to fixed window sizes and strides. To overcome these challenges, we propose a new more granular TSS approach that utilizes the Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines it with a Graph Attention Network (GAT). By transforming TS into graphs, we are able to capture different structural aspects of the data that would otherwise remain hidden. By utilizing the representation learning capabilities of Graph Neural Networks, our method is able to effectively identify meaningful segments within the TS. To better understand the potential of our approach, we also experimented with different TS-to-graph transformations and compared their performance. Our contributions include: a) formulating the TSS as a node classification problem on graphs; b) conducting an extensive analysis of various TS- to-graph transformations applied to TSS using benchmark datasets from the TSSB repository; c) providing the first detailed study on utilizing GNNs for analyzing graph representations of TS in the context of TSS; d) demonstrating the effectiveness of our method, which achieves an average F1 score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the required training data compared to the baseline methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives</title>
<link>https://arxiv.org/abs/2505.17646</link>
<guid>https://arxiv.org/abs/2505.17646</guid>
<content:encoded><![CDATA[
arXiv:2505.17646v1 Announce Type: new 
Abstract: Recent studies have revealed that the loss landscape of large language models resembles a basin, within which the models perform nearly identically, and outside of which they lose all their capabilities. In this work, we conduct further studies on the loss landscape of large language models. We discover that pre-training creates a "basic capability" basin, and subsequent fine-tuning creates "specific capability" basins (e.g., math, safety, coding) within the basic capability basin. We further investigate two types of loss landscapes: the most-case landscape (i.e., the landscape along most directions) and the worst-case landscape (i.e., the landscape along the worst direction). We argue that as long as benign fine-tuning remains within the most-case basin, it will not compromise previous capabilities. Similarly, any fine-tuning (including the adversarial one) that stays within the worst-case basin would not compromise previous capabilities. Finally, we theoretically demonstrate that the size of the most-case basin can bound the size of the worst-case basin and the robustness with respect to input perturbations. We also show that, due to the over-parameterization property of current large language models, one can easily enlarge the basins by five times.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective</title>
<link>https://arxiv.org/abs/2505.17652</link>
<guid>https://arxiv.org/abs/2505.17652</guid>
<content:encoded><![CDATA[
arXiv:2505.17652v1 Announce Type: new 
Abstract: Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces \textbf{C}ompetence-\textbf{D}ifficulty \textbf{A}lignment \textbf{S}ampling (\textbf{CDAS}), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is \textbf{2.33} times slower than CDAS.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification</title>
<link>https://arxiv.org/abs/2505.17660</link>
<guid>https://arxiv.org/abs/2505.17660</guid>
<content:encoded><![CDATA[
arXiv:2505.17660v1 Announce Type: new 
Abstract: Neighborhood-aware tokenized graph Transformers have recently shown great potential for node classification tasks. Despite their effectiveness, our in-depth analysis of neighborhood tokens reveals two critical limitations in the existing paradigm. First, current neighborhood token generation methods fail to adequately capture attribute correlations within a neighborhood. Second, the conventional self-attention mechanism suffers from attention diversion when processing neighborhood tokens, where high-hop neighborhoods receive disproportionate focus, severely disrupting information interactions between the target node and its neighborhood tokens. To address these challenges, we propose DAM-GT, Dual positional encoding-based Attention Masking graph Transformer. DAM-GT introduces a novel dual positional encoding scheme that incorporates attribute-aware encoding via an attribute clustering strategy, effectively preserving node correlations in both topological and attribute spaces. In addition, DAM-GT formulates a new attention mechanism with a simple yet effective masking strategy to guide interactions between target nodes and their neighborhood tokens, overcoming the issue of attention diversion. Extensive experiments on various graphs with different homophily levels as well as different scales demonstrate that DAM-GT consistently outperforms state-of-the-art methods in node classification tasks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated scientific minimization of regret</title>
<link>https://arxiv.org/abs/2505.17661</link>
<guid>https://arxiv.org/abs/2505.17661</guid>
<content:encoded><![CDATA[
arXiv:2505.17661v1 Announce Type: new 
Abstract: We introduce automated scientific minimization of regret (ASMR) -- a framework for automated computational cognitive science. Building on the principles of scientific regret minimization, ASMR leverages Centaur -- a recently proposed foundation model of human cognition -- to identify gaps in an interpretable cognitive model. These gaps are then addressed through automated revisions generated by a language-based reasoning model. We demonstrate the utility of this approach in a multi-attribute decision-making task, showing that ASMR discovers cognitive models that predict human behavior at noise ceiling while retaining interpretability. Taken together, our results highlight the potential of ASMR to automate core components of the cognitive modeling pipeline.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs</title>
<link>https://arxiv.org/abs/2505.17662</link>
<guid>https://arxiv.org/abs/2505.17662</guid>
<content:encoded><![CDATA[
arXiv:2505.17662v1 Announce Type: new 
Abstract: Transformer-based models have shown strong performance across diverse time-series tasks, but their deployment on resource-constrained devices remains challenging due to high memory and computational demand. While prior work targeting Microcontroller Units (MCUs) has explored hardware-specific optimizations, such approaches are often task-specific and limited to 8-bit fixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater flexibility, enabling fine-grained control over data precision and architecture. However, existing FPGA-based deployments of Transformers for time-series analysis typically focus on high-density platforms with manual configuration. This paper presents a unified and fully automated deployment framework for Tiny Transformers on embedded FPGAs. Our framework supports a compact encoder-only Transformer architecture across three representative time-series tasks (forecasting, classification, and anomaly detection). It combines quantization-aware training (down to 4 bits), hardware-aware hyperparameter search using Optuna, and automatic VHDL generation for seamless deployment. We evaluate our framework on six public datasets across two embedded FPGA platforms. Results show that our framework produces integer-only, task-specific Transformer accelerators achieving as low as 0.033 mJ per inference with millisecond latency on AMD Spartan-7, while also providing insights into deployment feasibility on Lattice iCE40. All source code will be released in the GitHub repository (https://github.com/Edwina1030/TinyTransformer4TS).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is the role of memorization in Continual Learning?</title>
<link>https://arxiv.org/abs/2505.17664</link>
<guid>https://arxiv.org/abs/2505.17664</guid>
<content:encoded><![CDATA[
arXiv:2505.17664v1 Announce Type: new 
Abstract: Memorization impacts the performance of deep learning algorithms. Prior works have studied memorization primarily in the context of generalization and privacy. This work studies the memorization effect on incremental learning scenarios. Forgetting prevention and memorization seem similar. However, one should discuss their differences. We designed extensive experiments to evaluate the impact of memorization on continual learning. We clarified that learning examples with high memorization scores are forgotten faster than regular samples. Our findings also indicated that memorization is necessary to achieve the highest performance. However, at low memory regimes, forgetting regular samples is more important. We showed that the importance of a high-memorization score sample rises with an increase in the buffer size. We introduced a memorization proxy and employed it in the buffer policy problem to showcase how memorization could be used during incremental training. We demonstrated that including samples with a higher proxy memorization score is beneficial when the buffer size is large.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards General Continuous Memory for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.17670</link>
<guid>https://arxiv.org/abs/2505.17670</guid>
<content:encoded><![CDATA[
arXiv:2505.17670v1 Announce Type: new 
Abstract: Language models (LMs) and their extension, vision-language models (VLMs), have achieved remarkable performance across various tasks. However, they still struggle with complex reasoning tasks that require multimodal or multilingual real-world knowledge. To support such capabilities, an external memory system that can efficiently provide relevant multimodal information is essential. Existing approaches generally concatenate image and text tokens into a long sequence as memory, which, however, may drastically increase context length and even degrade performance. In contrast, we propose using continuous memory, a compact set of dense embeddings to more effectively and efficiently represent multimodal and multilingual knowledge. Our key insight is that a VLM can serve as its own continuous memory encoder. We empirically show that this design improves performance on complex multimodal reasoning tasks. Building on this, we introduce a data-efficient and parameter-efficient method to fine-tune the VLM into a memory encoder, requiring only 1.2% of the model's parameters and a small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes VLM's original capabilities to encode arbitrary multimodal and multilingual knowledge into just 8 continuous embeddings. Since the inference-time VLM remains frozen, our memory module is plug-and-play and can be flexibly integrated as needed. Extensive experiments across eight multimodal reasoning benchmarks demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding</title>
<link>https://arxiv.org/abs/2505.17694</link>
<guid>https://arxiv.org/abs/2505.17694</guid>
<content:encoded><![CDATA[
arXiv:2505.17694v1 Announce Type: new 
Abstract: Prefix-sharing among multiple prompts presents opportunities to combine the operations of the shared prefix, while attention computation in the decode stage, which becomes a critical bottleneck with increasing context lengths, is a memory-intensive process requiring heavy memory access on the key-value (KV) cache of the prefixes. Therefore, in this paper, we explore the potential of prefix-sharing in the attention computation of the decode stage. However, the tree structure of the prefix-sharing mechanism presents significant challenges for attention computation in efficiently processing shared KV cache access patterns while managing complex dependencies and balancing irregular workloads. To address the above challenges, we propose a dedicated attention kernel to combine the memory access of shared prefixes in the decoding stage, namely FlashForge. FlashForge delivers two key innovations: a novel shared-prefix attention kernel that optimizes memory hierarchy and exploits both intra-block and inter-block parallelism, and a comprehensive workload balancing mechanism that efficiently estimates cost, divides tasks, and schedules execution. Experimental results show that FlashForge achieves an average 1.9x speedup and 120.9x memory access reduction compared to the state-of-the-art FlashDecoding kernel regarding attention computation in the decode stage and 3.8x end-to-end time per output token compared to the vLLM.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data</title>
<link>https://arxiv.org/abs/2505.17695</link>
<guid>https://arxiv.org/abs/2505.17695</guid>
<content:encoded><![CDATA[
arXiv:2505.17695v1 Announce Type: new 
Abstract: Despite the advances in Referring Expression Segmentation (RES) benchmarks, their evaluation protocols remain constrained, primarily focusing on either single targets with short queries (containing minimal attributes) or multiple targets from distinctly different queries on a single domain. This limitation significantly hinders the assessment of more complex reasoning capabilities in RES models. We introduce WildRES, a novel benchmark that incorporates long queries with diverse attributes and non-distinctive queries for multiple targets. This benchmark spans diverse application domains, including autonomous driving environments and robotic manipulation scenarios, thus enabling more rigorous evaluation of complex reasoning capabilities in real-world settings. Our analysis reveals that current RES models demonstrate substantial performance deterioration when evaluated on WildRES. To address this challenge, we introduce SynRES, an automated pipeline generating densely paired compositional synthetic training data through three innovations: (1) a dense caption-driven synthesis for attribute-rich image-mask-expression triplets, (2) reliable semantic alignment mechanisms rectifying caption-pseudo mask inconsistencies via Image-Text Aligned Grouping, and (3) domain-aware augmentations incorporating mosaic composition and superclass replacement to emphasize generalization ability and distinguishing attributes over object categories. Experimental results demonstrate that models trained with SynRES achieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and 3.8% on WildRES-DS. Code and datasets are available at https://github.com/UTLLab/SynRES.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection</title>
<link>https://arxiv.org/abs/2505.17701</link>
<guid>https://arxiv.org/abs/2505.17701</guid>
<content:encoded><![CDATA[
arXiv:2505.17701v1 Announce Type: new 
Abstract: The growing size of large language models has created significant computational inefficiencies. To address this challenge, sparse activation methods selectively deactivates non-essential parameters during inference, reducing computational costs in FFNN layers. While existing methods focus on non-linear gating mechanisms, we hypothesize that the sparsity of the FFNN layer lies globally in the form of a linear combination over its internal down projection matrix. Based on this insight, we propose two methods: M-COUNTDOWN, leveraging indirect coefficients, and D-COUNTDOWN, utilizing direct coefficients of the linear combination. Experimental results demonstrate that D-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5% ideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4% better performance preservation compared to existing methods. Our specialized kernel implementations effectively realize these theoretical gains into substantial real-world acceleration.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations</title>
<link>https://arxiv.org/abs/2505.17708</link>
<guid>https://arxiv.org/abs/2505.17708</guid>
<content:encoded><![CDATA[
arXiv:2505.17708v1 Announce Type: new 
Abstract: Causal reasoning and discovery, two fundamental tasks of causal analysis, often face challenges in applications due to the complexity, noisiness, and high-dimensionality of real-world data. Despite recent progress in identifying latent causal structures using causal representation learning (CRL), what makes learned representations useful for causal downstream tasks and how to evaluate them are still not well understood. In this paper, we reinterpret CRL using a measurement model framework, where the learned representations are viewed as proxy measurements of the latent causal variables. Our approach clarifies the conditions under which learned representations support downstream causal reasoning and provides a principled basis for quantitatively assessing the quality of representations using a new Test-based Measurement EXclusivity (T-MEX) score. We validate T-MEX across diverse causal inference scenarios, including numerical simulations and real-world ecological video analysis, demonstrating that the proposed framework and corresponding score effectively assess the identification of learned representations and their usefulness for causal downstream tasks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization</title>
<link>https://arxiv.org/abs/2505.17714</link>
<guid>https://arxiv.org/abs/2505.17714</guid>
<content:encoded><![CDATA[
arXiv:2505.17714v1 Announce Type: new 
Abstract: Despite Proximal Policy Optimization (PPO) dominating policy gradient methods -- from robotic control to game AI -- its static trust region forces a brittle trade-off: aggressive clipping stifles early exploration, while late-stage updates destabilize convergence. PPO-BR establishes a new paradigm in adaptive RL by fusing exploration and convergence signals into a single bounded trust region -- a theoretically grounded innovation that outperforms five SOTA baselines with less than 2% overhead. This work bridges a critical gap in phase-aware learning, enabling real-world deployment in safety-critical systems like robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1% faster convergence by combining: (1) entropy-driven expansion (epsilon up) for exploration in high-uncertainty states, and (2) reward-guided contraction (epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo, Atari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001), 2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with only five lines of code change. PPO-BR's simplicity and theoretical guarantees make it ready-to-deploy in safety-critical domains -- from surgical robotics to autonomous drones. In contrast to recent methods such as Group Relative Policy Optimization (GRPO), PPO-BR offers a unified entropy-reward mechanism applicable to both language models and general reinforcement learning environments.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Get Experience from Practice: LLM Agents with Record &amp; Replay</title>
<link>https://arxiv.org/abs/2505.17716</link>
<guid>https://arxiv.org/abs/2505.17716</guid>
<content:encoded><![CDATA[
arXiv:2505.17716v1 Announce Type: new 
Abstract: AI agents, empowered by Large Language Models (LLMs) and communication protocols such as MCP and A2A, have rapidly evolved from simple chatbots to autonomous entities capable of executing complex, multi-step tasks, demonstrating great potential. However, the LLMs' inherent uncertainty and heavy computational resource requirements pose four significant challenges to the development of safe and efficient agents: reliability, privacy, cost and performance. Existing approaches, like model alignment, workflow constraints and on-device model deployment, can partially alleviate some issues but often with limitations, failing to fundamentally resolve these challenges.
  This paper proposes a new paradigm called AgentRR (Agent Record & Replay), which introduces the classical record-and-replay mechanism into AI agent frameworks. The core idea is to: 1. Record an agent's interaction trace with its environment and internal decision process during task execution, 2. Summarize this trace into a structured "experience" encapsulating the workflow and constraints, and 3. Replay these experiences in subsequent similar tasks to guide the agent's behavior. We detail a multi-level experience abstraction method and a check function mechanism in AgentRR: the former balances experience specificity and generality, while the latter serves as a trust anchor to ensure completeness and safety during replay. In addition, we explore multiple application modes of AgentRR, including user-recorded task demonstration, large-small model collaboration and privacy-aware agent execution, and envision an experience repository for sharing and reusing knowledge to further reduce deployment cost.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEAR: Equal Area Weather Forecasting on the Sphere</title>
<link>https://arxiv.org/abs/2505.17720</link>
<guid>https://arxiv.org/abs/2505.17720</guid>
<content:encoded><![CDATA[
arXiv:2505.17720v1 Announce Type: new 
Abstract: Machine learning methods for global medium-range weather forecasting have recently received immense attention. Following the publication of the Pangu Weather model, the first deep learning model to outperform traditional numerical simulations of the atmosphere, numerous models have been published in this domain, building on Pangu's success. However, all of these models operate on input data and produce predictions on the Driscoll--Healy discretization of the sphere which suffers from a much finer grid at the poles than around the equator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization (HEALPix) of the sphere, each pixel covers the same surface area, removing unphysical biases. Motivated by a growing support for this grid in meteorology and climate sciences, we propose to perform weather forecasting with deep learning models which natively operate on the HEALPix grid. To this end, we introduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting model which operates directly on HEALPix-features and outperforms the corresponding model on Driscoll--Healy without any computational overhead.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data</title>
<link>https://arxiv.org/abs/2505.17730</link>
<guid>https://arxiv.org/abs/2505.17730</guid>
<content:encoded><![CDATA[
arXiv:2505.17730v1 Announce Type: new 
Abstract: Machine unlearning is studied for a multitude of tasks, but specialization of unlearning methods to particular tasks has made their systematic comparison challenging. To address this issue, we propose a conceptual space to characterize diverse corrupted data unlearning tasks in vision classifiers. This space is described by two dimensions, the discovery rate (the fraction of the corrupted data that are known at unlearning time) and the statistical regularity of the corrupted data (from random exemplars to shared concepts). Methods proposed previously have been targeted at portions of this space and-we show-fail predictably outside these regions. We propose a novel method, Redirection for Erasing Memory (REM), whose key feature is that corrupted data are redirected to dedicated neurons introduced at unlearning time and then discarded or deactivated to suppress the influence of corrupted data. REM performs strongly across the space of tasks, in contrast to prior SOTA methods that fail outside the regions for which they were designed.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2505.17734</link>
<guid>https://arxiv.org/abs/2505.17734</guid>
<content:encoded><![CDATA[
arXiv:2505.17734v1 Announce Type: new 
Abstract: Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future urban networks, potentially by optimizing their routing decisions. Unlike for human drivers, these decisions can be made with collective, data-driven policies, developed by machine learning algorithms. Reinforcement learning (RL) can facilitate the development of such collective routing strategies, yet standardized and realistic benchmarks are missing. To that end, we present \our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles. \our{} is a comprehensive benchmarking environment that unifies evaluation across 29 real-world traffic networks paired with realistic demand patterns. \our{} comes with a catalog of predefined tasks, four state-of-the-art multi-agent RL (MARL) algorithm implementations, three baseline methods, domain-specific performance metrics, and a modular configuration scheme. Our results suggest that, despite the lengthy and costly training, state-of-the-art MARL algorithms rarely outperformed humans. Experimental results reported in this paper initiate the first leaderboard for MARL in large-scale urban routing optimization and reveal that current approaches struggle to scale, emphasizing the urgent need for advancements in this domain.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A tensor network approach for chaotic time series prediction</title>
<link>https://arxiv.org/abs/2505.17740</link>
<guid>https://arxiv.org/abs/2505.17740</guid>
<content:encoded><![CDATA[
arXiv:2505.17740v1 Announce Type: new 
Abstract: Making accurate predictions of chaotic time series is a complex challenge. Reservoir computing, a neuromorphic-inspired approach, has emerged as a powerful tool for this task. It exploits the memory and nonlinearity of dynamical systems without requiring extensive parameter tuning. However, selecting and optimizing reservoir architectures remains an open problem. Next-generation reservoir computing simplifies this problem by employing nonlinear vector autoregression based on truncated Volterra series, thereby reducing hyperparameter complexity. Nevertheless, the latter suffers from exponential parameter growth in terms of the maximum monomial degree. Tensor networks offer a promising solution to this issue by decomposing multidimensional arrays into low-dimensional structures, thus mitigating the curse of dimensionality. This paper explores the application of a previously proposed tensor network model for predicting chaotic time series, demonstrating its advantages in terms of accuracy and computational efficiency compared to conventional echo state networks. Using a state-of-the-art tensor network approach enables us to bridge the gap between the tensor network and reservoir computing communities, fostering advances in both fields.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Neural Flow Samplers with Locally Equivariant Transformer</title>
<link>https://arxiv.org/abs/2505.17741</link>
<guid>https://arxiv.org/abs/2505.17741</guid>
<content:encoded><![CDATA[
arXiv:2505.17741v1 Announce Type: new 
Abstract: Sampling from unnormalised discrete distributions is a fundamental problem across various domains. While Markov chain Monte Carlo offers a principled approach, it often suffers from slow mixing and poor convergence. In this paper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and efficient framework for discrete sampling. DNFS learns the rate matrix of a continuous-time Markov chain such that the resulting dynamics satisfy the Kolmogorov equation. As this objective involves the intractable partition function, we then employ control variates to reduce the variance of its Monte Carlo estimation, leading to a coordinate descent learning algorithm. To further facilitate computational efficiency, we propose locally equivaraint Transformer, a novel parameterisation of the rate matrix that significantly improves training efficiency while preserving powerful network expressiveness. Empirically, we demonstrate the efficacy of DNFS in a wide range of applications, including sampling from unnormalised distributions, training discrete energy-based models, and solving combinatorial optimisation problems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization</title>
<link>https://arxiv.org/abs/2505.17745</link>
<guid>https://arxiv.org/abs/2505.17745</guid>
<content:encoded><![CDATA[
arXiv:2505.17745v1 Announce Type: new 
Abstract: Meta-Black-Box Optimization (MetaBBO) streamlines the automation of optimization algorithm design through meta-learning. It typically employs a bi-level structure: the meta-level policy undergoes meta-training to reduce the manual effort required in developing algorithms for low-level optimization tasks. The original MetaBox (2023) provided the first open-source framework for reinforcement learning-based single-objective MetaBBO. However, its relatively narrow scope no longer keep pace with the swift advancement in this field. In this paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a milestone upgrade with four novel features: 1) a unified architecture supporting RL, evolutionary, and gradient-based approaches, by which we reproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which reduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite of 18 synthetic/realistic tasks (1900+ instances) spanning single-objective, multi-objective, multi-model, and multi-task optimization scenarios; 4) plentiful and extensible interfaces for custom analysis/visualization and integrating to external optimization tools/benchmarks. To show the utility of MetaBox-v2, we carry out a systematic case study that evaluates the built-in baselines in terms of the optimization performance, generalization ability and learning efficiency. Valuable insights are concluded from thorough and detailed analysis for practitioners and those new to the field.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-CAM: Making black box models self-explainable for high-stakes decisions</title>
<link>https://arxiv.org/abs/2505.17748</link>
<guid>https://arxiv.org/abs/2505.17748</guid>
<content:encoded><![CDATA[
arXiv:2505.17748v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) are widely used for high-stakes applications like medicine, often surpassing human performance. However, most explanation methods rely on post-hoc attribution, approximating the decision-making process of already trained black-box models. These methods are often sensitive, unreliable, and fail to reflect true model reasoning, limiting their trustworthiness in critical applications. In this work, we introduce SoftCAM, a straightforward yet effective approach that makes standard CNN architectures inherently interpretable. By removing the global average pooling layer and replacing the fully connected classification layer with a convolution-based class evidence layer, SoftCAM preserves spatial information and produces explicit class activation maps that form the basis of the model's predictions. Evaluated on three medical datasets, SoftCAM maintains classification performance while significantly improving both the qualitative and quantitative explanation compared to existing post-hoc methods. Our results demonstrate that CNNs can be inherently interpretable without compromising performance, advancing the development of self-explainable deep learning for high-stakes decision-making.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17749</link>
<guid>https://arxiv.org/abs/2505.17749</guid>
<content:encoded><![CDATA[
arXiv:2505.17749v1 Announce Type: new 
Abstract: Scaling deep reinforcement learning in pixel-based environments presents a significant challenge, often resulting in diminished performance. While recent works have proposed algorithmic and architectural approaches to address this, the underlying cause of the performance drop remains unclear. In this paper, we identify the connection between the output of the encoder (a stack of convolutional layers) and the ensuing dense layers as the main underlying factor limiting scaling capabilities; we denote this connection as the bottleneck, and we demonstrate that previous approaches implicitly target this bottleneck. As a result of our analyses, we present global average pooling as a simple yet effective way of targeting the bottleneck, thereby avoiding the complexity of earlier approaches.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors</title>
<link>https://arxiv.org/abs/2505.17760</link>
<guid>https://arxiv.org/abs/2505.17760</guid>
<content:encoded><![CDATA[
arXiv:2505.17760v1 Announce Type: new 
Abstract: Recent safety evaluations of Large Language Models (LLMs) show that many models exhibit dishonest behavior, such as sycophancy. However, most honesty benchmarks focus exclusively on factual knowledge or explicitly harmful behavior and rely on external judges, which are often unable to detect less obvious forms of dishonesty. In this work, we introduce a new framework, Judge Using Safety-Steered Alternatives (JUSSA), which utilizes steering vectors trained on a single sample to elicit more honest responses from models, helping LLM-judges in the detection of dishonest behavior. To test our framework, we introduce a new manipulation dataset with prompts specifically designed to elicit deceptive responses. We find that JUSSA enables LLM judges to better differentiate between dishonest and benign responses, and helps them identify subtle instances of manipulative behavior.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models</title>
<link>https://arxiv.org/abs/2505.17761</link>
<guid>https://arxiv.org/abs/2505.17761</guid>
<content:encoded><![CDATA[
arXiv:2505.17761v1 Announce Type: new 
Abstract: Structured Linear Controlled Differential Equations (SLiCEs) provide a unifying framework for sequence models with structured, input-dependent state-transition matrices that retain the maximal expressivity of dense matrices whilst being cheaper to compute. The framework encompasses existing architectures, such as input-dependent block-diagonal linear recurrent neural networks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel variants based on sparsity and the Walsh--Hadamard transform. We prove that, unlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing block-diagonal, sparse, or Walsh--Hadamard matrices match the maximal expressivity of dense matrices. Empirically, SLiCEs solve the $A_5$ state-tracking benchmark with a single layer, achieve best-in-class length generalisation on regular language tasks among parallel-in-time models, and match the state-of-the-art performance of log neural controlled differential equations on six multivariate time-series classification datasets while cutting the average time per training step by a factor of twenty.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals</title>
<link>https://arxiv.org/abs/2505.17763</link>
<guid>https://arxiv.org/abs/2505.17763</guid>
<content:encoded><![CDATA[
arXiv:2505.17763v1 Announce Type: new 
Abstract: The widespread use of sensors in modern power grids has led to the accumulation of large amounts of voltage and current waveform data, especially during fault events. However, the lack of labeled datasets poses a significant challenge for fault classification and analysis. This paper explores the application of unsupervised clustering techniques for fault diagnosis in high-voltage power systems. A dataset provided by the Reseau de Transport d'Electricite (RTE) is analyzed, with frequency domain features extracted using the Fast Fourier Transform (FFT). The K-Means algorithm is then applied to identify underlying patterns in the data, enabling automated fault categorization without the need for labeled training samples. The resulting clusters are evaluated in collaboration with power system experts to assess their alignment with real-world fault characteristics. The results demonstrate the potential of unsupervised learning for scalable and data-driven fault analysis, providing a robust approach to detecting and classifying power system faults with minimal prior assumptions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joker: Joint Optimization Framework for Lightweight Kernel Machines</title>
<link>https://arxiv.org/abs/2505.17765</link>
<guid>https://arxiv.org/abs/2505.17765</guid>
<content:encoded><![CDATA[
arXiv:2505.17765v1 Announce Type: new 
Abstract: Kernel methods are powerful tools for nonlinear learning with well-established theory. The scalability issue has been their long-standing challenge. Despite the existing success, there are two limitations in large-scale kernel methods: (i) The memory overhead is too high for users to afford; (ii) existing efforts mainly focus on kernel ridge regression (KRR), while other models lack study. In this paper, we propose Joker, a joint optimization framework for diverse kernel models, including KRR, logistic regression, and support vector machines. We design a dual block coordinate descent method with trust region (DBCD-TR) and adopt kernel approximation with randomized features, leading to low memory costs and high efficiency in large-scale learning. Experiments show that Joker saves up to 90\% memory but achieves comparable training time and performance (or even better) than the state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models</title>
<link>https://arxiv.org/abs/2505.17769</link>
<guid>https://arxiv.org/abs/2505.17769</guid>
<content:encoded><![CDATA[
arXiv:2505.17769v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage Models (LLM) activations into interpretable latents. However, due to their substantial training cost, most academic research uses open-source SAEs which are only available for a restricted set of models of up to 27B parameters. SAE latents are also learned from a dataset of activations, which means they do not transfer between models. Motivated by relative representation similarity measures, we introduce Inference-Time Decomposition of Activations (ITDA) models, an alternative method for decomposing language model activations. To train an ITDA, we greedily construct a dictionary of language model activations on a dataset of prompts, selecting those activations which were worst approximated by matching pursuit on the existing dictionary. ITDAs can be trained in just 1\% of the time required for SAEs, using 1\% of the data. This allowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU. ITDAs can achieve similar reconstruction performance to SAEs on some target LLMs, but generally incur a performance penalty. However, ITDA dictionaries enable cross-model comparisons, and a simple Jaccard similarity index on ITDA dictionaries outperforms existing methods like CKA, SVCCA, and relative representation similarity metrics. ITDAs provide a cheap alternative to SAEs where computational resources are limited, or when cross model comparisons are necessary. Code available at https://github.com/pleask/itda.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2505.17773</link>
<guid>https://arxiv.org/abs/2505.17773</guid>
<content:encoded><![CDATA[
arXiv:2505.17773v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Shortfall Risk Metric for Learning Regression Models</title>
<link>https://arxiv.org/abs/2505.17777</link>
<guid>https://arxiv.org/abs/2505.17777</guid>
<content:encoded><![CDATA[
arXiv:2505.17777v1 Announce Type: new 
Abstract: We consider the problem of estimating and optimizing utility-based shortfall risk (UBSR) of a loss, say $(Y - \hat Y)^2$, in the context of a regression problem. Empirical risk minimization with a UBSR objective is challenging since UBSR is a non-linear function of the underlying distribution. We first derive a concentration bound for UBSR estimation using independent and identically distributed (i.i.d.) samples. We then frame the UBSR optimization problem as minimization of a pseudo-linear function in the space of achievable distributions $\mathcal D$ of the loss $(Y- \hat Y)^2$. We construct a gradient oracle for the UBSR objective and a linear minimization oracle (LMO) for the set $\mathcal D$. Using these oracles, we devise a bisection-type algorithm, and establish convergence to the UBSR-optimal solution.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised Graph Contrastive Learning for Gene Regulatory Network</title>
<link>https://arxiv.org/abs/2505.17786</link>
<guid>https://arxiv.org/abs/2505.17786</guid>
<content:encoded><![CDATA[
arXiv:2505.17786v1 Announce Type: new 
Abstract: Graph representation learning is effective for obtaining a meaningful latent space utilizing the structure of graph data and is widely applied, including biological networks. In particular, Graph Contrastive Learning (GCL) has emerged as a powerful self-supervised method that relies on applying perturbations to graphs for data augmentation. However, when applying existing GCL methods to biological networks such as Gene Regulatory Networks (GRNs), they overlooked meaningful biologically relevant perturbations, e.g., gene knockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive Learning), a novel GCL method for GRNs that directly incorporates biological perturbations derived from gene knockdown experiments as the supervision. SupGCL mathematically extends existing GCL methods that utilize non-biological perturbations to probabilistic models that introduce actual biological gene perturbation utilizing gene knockdown data. Using the GRN representation obtained by our proposed method, our aim is to improve the performance of biological downstream tasks such as patient hazard prediction and disease subtype classification (graph-level task), and gene function classification (node-level task). We applied SupGCL on real GRN datasets derived from patients with multiple types of cancer, and in all experiments SupGCL achieves better performance than state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2505.17794</link>
<guid>https://arxiv.org/abs/2505.17794</guid>
<content:encoded><![CDATA[
arXiv:2505.17794v1 Announce Type: new 
Abstract: Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped relations between entities. TKG completion involves forecasting missing or future links, requiring models to reason over time-evolving structure. While LLMs show promise for this task, existing approaches often overemphasize supervised fine-tuning and struggle particularly when historical evidence is limited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient framework designed to improve accuracy and generalization in settings with sparse historical context. It combines (1) rule-based multi-hop retrieval for structurally diverse history, (2) contrastive fine-tuning of lightweight adapters to encode relational semantics, and (3) test-time semantic filtering to iteratively refine generations based on embedding similarity. Experiments on four TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based approaches, achieving up to 30.6\% relative improvement in Hits@10. Moreover, our proposed framework produces more semantically coherent predictions, even for the samples with limited historical context.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Mode Decomposition</title>
<link>https://arxiv.org/abs/2505.17797</link>
<guid>https://arxiv.org/abs/2505.17797</guid>
<content:encoded><![CDATA[
arXiv:2505.17797v1 Announce Type: new 
Abstract: We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm for extracting oscillatory modes and associated connectivity structures from multivariate signals. VLMD addresses key limitations of existing Multivariate Mode Decomposition (MMD) techniques -including high computational cost, sensitivity to parameter choices, and weak modeling of interchannel dependencies. Its improved performance is driven by a novel underlying model, Latent Mode Decomposition (LMD), which blends sparse coding and mode decomposition to represent multichannel signals as sparse linear combinations of shared latent components composed of AM-FM oscillatory modes. This formulation enables VLMD to operate in a lower-dimensional latent space, enhancing robustness to noise, scalability, and interpretability. The algorithm solves a constrained variational optimization problem that jointly enforces reconstruction fidelity, sparsity, and frequency regularization. Experiments on synthetic and real-world datasets demonstrate that VLMD outperforms state-of-the-art MMD methods in accuracy, efficiency, and interpretability of extracted structures.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances</title>
<link>https://arxiv.org/abs/2505.17799</link>
<guid>https://arxiv.org/abs/2505.17799</guid>
<content:encoded><![CDATA[
arXiv:2505.17799v1 Announce Type: new 
Abstract: Coreset selection targets the challenge of finding a small, representative subset of a large dataset that preserves essential patterns for effective machine learning. Although several surveys have examined data reduction strategies before, most focus narrowly on either classical geometry-based methods or active learning techniques. In contrast, this survey presents a more comprehensive view by unifying three major lines of coreset research, namely, training-free, training-oriented, and label-free approaches, into a single taxonomy. We present subfields often overlooked by existing work, including submodular formulations, bilevel optimization, and recent progress in pseudo-labeling for unlabeled datasets. Additionally, we examine how pruning strategies influence generalization and neural scaling laws, offering new insights that are absent from prior reviews. Finally, we compare these methods under varying computational, robustness, and performance demands and highlight open challenges, such as robustness, outlier filtering, and adapting coreset selection to foundation models, for future research.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperparameter Optimization via Interacting with Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2505.17804</link>
<guid>https://arxiv.org/abs/2505.17804</guid>
<content:encoded><![CDATA[
arXiv:2505.17804v1 Announce Type: new 
Abstract: Despite the growing interest in designing truly interactive hyperparameter optimization (HPO) methods, to date, only a few allow to include human feedback. Existing interactive Bayesian optimization (BO) methods incorporate human beliefs by weighting the acquisition function with a user-defined prior distribution. However, in light of the non-trivial inner optimization of the acquisition function prevalent in BO, such weighting schemes do not always accurately reflect given user beliefs. We introduce a novel BO approach leveraging tractable probabilistic models named probabilistic circuits (PCs) as a surrogate model. PCs encode a tractable joint distribution over the hybrid hyperparameter space and evaluation scores. They enable exact conditional inference and sampling. Based on conditional sampling, we construct a novel selection policy that enables an acquisition function-free generation of candidate points (thereby eliminating the need for an additional inner-loop optimization) and ensures that user beliefs are reflected accurately in the selection policy. We provide a theoretical analysis and an extensive empirical evaluation, demonstrating that our method achieves state-of-the-art performance in standard HPO and outperforms interactive BO baselines in interactive HPO.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIBE: Vector Index Benchmark for Embeddings</title>
<link>https://arxiv.org/abs/2505.17810</link>
<guid>https://arxiv.org/abs/2505.17810</guid>
<content:encoded><![CDATA[
arXiv:2505.17810v1 Announce Type: new 
Abstract: Approximate nearest neighbor (ANN) search is a performance-critical component of many machine learning pipelines. Rigorous benchmarking is essential for evaluating the performance of vector indexes for ANN search. However, the datasets of the existing benchmarks are no longer representative of the current applications of ANN search. Hence, there is an urgent need for an up-to-date set of benchmarks. To this end, we introduce Vector Index Benchmark for Embeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE contains a pipeline for creating benchmark datasets using dense embedding models characteristic of modern applications, such as retrieval-augmented generation (RAG). To replicate real-world workloads, we also include out-of-distribution (OOD) datasets where the queries and the corpus are drawn from different distributions. We use VIBE to conduct a comprehensive evaluation of SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution and 6 out-of-distribution datasets.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.17826</link>
<guid>https://arxiv.org/abs/2505.17826</guid>
<content:encoded><![CDATA[
arXiv:2505.17826v1 Announce Type: new 
Abstract: Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17830</link>
<guid>https://arxiv.org/abs/2505.17830</guid>
<content:encoded><![CDATA[
arXiv:2505.17830v1 Announce Type: new 
Abstract: Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $\beta$-VAE framework with Distributionally Robust Optimization. DRAG leverages an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without pre-training nor prior environment knowledge.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransDF: Time-Series Forecasting Needs Transformed Label Alignment</title>
<link>https://arxiv.org/abs/2505.17847</link>
<guid>https://arxiv.org/abs/2505.17847</guid>
<content:encoded><![CDATA[
arXiv:2505.17847v1 Announce Type: new 
Abstract: Training time-series forecasting models presents unique challenges in designing effective learning objectives. Existing methods predominantly utilize the temporal mean squared error, which faces two critical challenges: (1) label autocorrelation, which leads to bias from the label sequence likelihood; (2) excessive amount of tasks, which increases with the forecast horizon and complicates optimization. To address these challenges, we propose Transform-enhanced Direct Forecast (TransDF), which transforms the label sequence into decorrelated components with discriminated significance. Models are trained to align the most significant components, thereby effectively mitigating label autocorrelation and reducing task amount. Extensive experiments demonstrate that TransDF achieves state-of-the-art performance and is compatible with various forecasting models. Code is available at https://anonymous.4open.science/r/TransDF-88CF.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization</title>
<link>https://arxiv.org/abs/2505.17852</link>
<guid>https://arxiv.org/abs/2505.17852</guid>
<content:encoded><![CDATA[
arXiv:2505.17852v1 Announce Type: new 
Abstract: During inference, Recurrent Neural Networks (RNNs) scale constant in both FLOPs and GPU memory with increasing context length, as they compress all prior tokens into a fixed-size memory. In contrast, transformers scale linearly in FLOPs and, at best, linearly in memory during generation, since they must attend to all previous tokens explicitly. Despite this inference-time advantage, training large RNNs on long contexts remains impractical because standard optimization methods depend on Backpropagation Through Time (BPTT). BPTT requires retention of all intermediate activations during the forward pass, causing memory usage to scale linearly with both context length and model size. In this paper, we show that Zero-Order Optimization (ZOO) methods such as Random-vector Gradient Estimation (RGE) can successfully replace BPTT to train RNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while using orders of magnitude less memory and cost, as the model remains in inference mode throughout training. We further demonstrate that Central-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate loss, inherently regularizing training and improving generalization. Our method matches or outperforms BPTT across three settings: (1) overfitting, (2) transduction, and (3) language modeling. Across all tasks, with sufficient perturbations, our models generalize as well as or better than those trained with BPTT, often in fewer steps. Despite the need for more forward passes per step, we can surpass BPTT wall-clock time per step using recent advancements such as FlashRNN and distributed inference.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of the Shadows: Exploring a Latent Space for Neural Network Verification</title>
<link>https://arxiv.org/abs/2505.17854</link>
<guid>https://arxiv.org/abs/2505.17854</guid>
<content:encoded><![CDATA[
arXiv:2505.17854v1 Announce Type: new 
Abstract: Neural networks are ubiquitous. However, they are often sensitive to small input changes. Hence, to prevent unexpected behavior in safety-critical applications, their formal verification -- a notoriously hard problem -- is necessary. Many state-of-the-art verification algorithms use reachability analysis or abstract interpretation to enclose the set of possible outputs of a neural network. Often, the verification is inconclusive due to the conservatism of the enclosure. To address this problem, we design a novel latent space for formal verification that enables the transfer of output specifications to the input space for an iterative specification-driven input refinement, i.e., we iteratively reduce the set of possible inputs to only enclose the unsafe ones. The latent space is constructed from a novel view of projection-based set representations, e.g., zonotopes, which are commonly used in reachability analysis of neural networks. A projection-based set representation is a "shadow" of a higher-dimensional set -- a latent space -- that does not change during a set propagation through a neural network. Hence, the input set and the output enclosure are "shadows" of the same latent space that we can use to transfer constraints. We present an efficient verification tool for neural networks that uses our iterative refinement to significantly reduce the number of subproblems in a branch-and-bound procedure. Using zonotopes as a set representation, unlike many other state-of-the-art approaches, our approach can be realized by only using matrix operations, which enables a significant speed-up through efficient GPU acceleration. We demonstrate that our tool achieves competitive performance, which would place it among the top-ranking tools of the last neural network verification competition (VNN-COMP'24).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Weight Sharing for Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2505.17856</link>
<guid>https://arxiv.org/abs/2505.17856</guid>
<content:encoded><![CDATA[
arXiv:2505.17856v1 Announce Type: new 
Abstract: While offering a principled framework for uncertainty quantification in deep learning, the employment of Bayesian Neural Networks (BNNs) is still constrained by their increased computational requirements and the convergence difficulties when training very deep, state-of-the-art architectures. In this work, we reinterpret weight-sharing quantization techniques from a stochastic perspective in the context of training and inference with Bayesian Neural Networks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions, Wasserstein distance estimations, and alpha blending to encode the stochastic behaviour of a BNN in a lower dimensional, soft Gaussian representation. Through extensive empirical investigation, we demonstrate that our approach significantly reduces the computational overhead inherent in Bayesian learning by several orders of magnitude, enabling the efficient Bayesian training of large-scale models, such as ResNet-101 and Vision Transformer (VIT). On various computer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our approach compresses model parameters by approximately 50x and reduces model size by 75, while achieving accuracy and uncertainty estimations comparable to the state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Valuation of Human Feedback through Provably Robust Model Alignment</title>
<link>https://arxiv.org/abs/2505.17859</link>
<guid>https://arxiv.org/abs/2505.17859</guid>
<content:encoded><![CDATA[
arXiv:2505.17859v1 Announce Type: new 
Abstract: Despite the importance of aligning language models with human preferences, crowd-sourced human feedback is often noisy -- for example, preferring less desirable responses -- posing a fundamental challenge to alignment. A truly robust alignment objective should yield identical model parameters even under severe label noise, a property known as redescending. We prove that no existing alignment methods satisfy this property. To address this, we propose H\"older-DPO, the first principled alignment loss with a provable redescending property, enabling estimation of the clean data distribution from noisy feedback. The aligned model estimates the likelihood of clean data, providing a theoretically grounded metric for dataset valuation that identifies the location and fraction of mislabels. This metric is gradient-free, enabling scalable and automated human feedback valuation without costly manual verification or clean validation dataset. H\"older-DPO achieves state-of-the-art robust alignment performance while accurately detecting mislabels in controlled datasets. Finally, we apply H\"older-DPO to widely used alignment datasets, revealing substantial noise levels and demonstrating that removing these mislabels significantly improves alignment performance across methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The emergence of sparse attention: impact of data distribution and benefits of repetition</title>
<link>https://arxiv.org/abs/2505.17863</link>
<guid>https://arxiv.org/abs/2505.17863</guid>
<content:encoded><![CDATA[
arXiv:2505.17863v1 Announce Type: new 
Abstract: Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization</title>
<link>https://arxiv.org/abs/2505.17866</link>
<guid>https://arxiv.org/abs/2505.17866</guid>
<content:encoded><![CDATA[
arXiv:2505.17866v1 Announce Type: new 
Abstract: Designing effective black-box optimizers is hampered by limited problem-specific knowledge and manual control that spans months for almost every detail. In this paper, we present DesignX, the first automated algorithm design framework that generates an effective optimizer specific to a given black-box optimization problem within seconds. Rooted in the first principles, we identify two key sub-tasks: 1) algorithm structure generation and 2) hyperparameter control. To enable systematic construction, a comprehensive modular algorithmic space is first built, embracing hundreds of algorithm components collected from decades of research. We then introduce a dual-agent reinforcement learning system that collaborates on structural and parametric design through a novel cooperative training objective, enabling large-scale meta-training across 10k diverse instances. Remarkably, through days of autonomous learning, the DesignX-generated optimizers continuously surpass human-crafted optimizers by orders of magnitude, either on synthetic testbed or on realistic optimization scenarios such as Protein-docking, AutoML and UAV path planning. Further in-depth analysis reveals DesignX's capability to discover non-trivial algorithm patterns beyond expert intuition, which, conversely, provides valuable design insights for the optimization community. We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpectraLDS: Provable Distillation for Linear Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.17868</link>
<guid>https://arxiv.org/abs/2505.17868</guid>
<content:encoded><![CDATA[
arXiv:2505.17868v1 Announce Type: new 
Abstract: We present the first provable method for identifying symmetric linear dynamical systems (LDS) with accuracy guarantees that are independent of the systems' state dimension or effective memory. Our approach builds upon recent work that represents symmetric LDSs as convolutions learnable via fixed spectral transformations. We show how to invert this representation, thereby recovering an LDS model from its spectral transform and yielding an end-to-end convex optimization procedure. This distillation preserves predictive accuracy while enabling constant-time and constant-space inference per token, independent of sequence length. We evaluate our method, SpectraLDS, as a component in sequence prediction architectures and demonstrate that accuracy is preserved while inference efficiency is improved on tasks such as language modeling.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Best Group Identification in Multi-Objective Bandits</title>
<link>https://arxiv.org/abs/2505.17869</link>
<guid>https://arxiv.org/abs/2505.17869</guid>
<content:encoded><![CDATA[
arXiv:2505.17869v1 Announce Type: new 
Abstract: We introduce the Best Group Identification problem in a multi-objective multi-armed bandit setting, where an agent interacts with groups of arms with vector-valued rewards. The performance of a group is determined by an efficiency vector which represents the group's best attainable rewards across different dimensions. The objective is to identify the set of optimal groups in the fixed-confidence setting. We investigate two key formulations: group Pareto set identification, where efficiency vectors of optimal groups are Pareto optimal and linear best group identification, where each reward dimension has a known weight and the optimal group maximizes the weighted sum of its efficiency vector's entries. For both settings, we propose elimination-based algorithms, establish upper bounds on their sample complexity, and derive lower bounds that apply to any correct algorithm. Through numerical experiments, we demonstrate the strong empirical performance of the proposed algorithms.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models</title>
<link>https://arxiv.org/abs/2505.17871</link>
<guid>https://arxiv.org/abs/2505.17871</guid>
<content:encoded><![CDATA[
arXiv:2505.17871v1 Announce Type: new 
Abstract: The advent of universal time series forecasting models has revolutionized zero-shot forecasting across diverse domains, yet the critical role of data diversity in training these models remains underexplored. Existing large-scale time series datasets often suffer from inherent biases and imbalanced distributions, leading to suboptimal model performance and generalization. To address this gap, we introduce BLAST, a novel pre-training corpus designed to enhance data diversity through a balanced sampling strategy. First, BLAST incorporates 321 billion observations from publicly available datasets and employs a comprehensive suite of statistical metrics to characterize time series patterns. Then, to facilitate pattern-oriented sampling, the data is implicitly clustered using grid-based partitioning. Furthermore, by integrating grid sampling and grid mixup techniques, BLAST ensures a balanced and representative coverage of diverse patterns. Experimental results demonstrate that models pre-trained on BLAST achieve state-of-the-art performance with a fraction of the computational resources and training tokens required by existing methods. Our findings highlight the pivotal role of data diversity in improving both training efficiency and model performance for the universal forecasting task.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.17872</link>
<guid>https://arxiv.org/abs/2505.17872</guid>
<content:encoded><![CDATA[
arXiv:2505.17872v1 Announce Type: new 
Abstract: Multi-task forecasting has become the standard approach for time-series forecasting (TSF). However, we show that it suffers from an Expressiveness Bottleneck, where predictions at different time steps share the same representation, leading to unavoidable errors even with optimal representations. To address this issue, we propose a two-stage framework: first, pre-train a foundation model for one-step-ahead prediction; then, adapt it using step-specific LoRA modules.This design enables the foundation model to handle any number of forecast steps while avoiding the expressiveness bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which employs adaptively weighted LoRA experts to achieve partial parameter sharing across steps. This approach enhances both efficiency and forecasting performance by exploiting interdependencies between forecast steps. Experiments show that MoLA significantly improves model expressiveness and outperforms state-of-the-art time-series forecasting methods. Code is available at https://anonymous.4open.science/r/MoLA-BC92.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning</title>
<link>https://arxiv.org/abs/2505.17875</link>
<guid>https://arxiv.org/abs/2505.17875</guid>
<content:encoded><![CDATA[
arXiv:2505.17875v1 Announce Type: new 
Abstract: In practical domains, high-dimensional data are usually associated with diverse semantic labels, whereas traditional feature selection methods are designed for single-label data. Moreover, existing multi-label methods encounter two main challenges in semi-supervised scenarios: (1). Most semi-supervised methods fail to evaluate the label correlations without enough labeled samples, which are the critical information of multi-label feature selection, making label-specific features discarded. (2). The similarity graph structure directly derived from the original feature space is suboptimal for multi-label problems in existing graph-based methods, leading to unreliable soft labels and degraded feature selection performance. To overcome them, we propose a consistent sparse graph learning method for multi-label semi-supervised feature selection (SGMFS), which can enhance the feature selection performance by maintaining space consistency and learning label correlations in semi-supervised scenarios. Specifically, for Challenge (1), SGMFS learns a low-dimensional and independent label subspace from the projected features, which can compatibly cross multiple labels and effectively achieve the label correlations. For Challenge (2), instead of constructing a fixed similarity graph for semi-supervised learning, SGMFS thoroughly explores the intrinsic structure of the data by performing sparse reconstruction of samples in both the label space and the learned subspace simultaneously. In this way, the similarity graph can be adaptively learned to maintain the consistency between label space and the learned subspace, which can promote propagating proper soft labels for unlabeled samples, facilitating the ultimate feature selection. An effective solution with fast convergence is designed to optimize the objective function. Extensive experiments validate the superiority of SGMFS.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks</title>
<link>https://arxiv.org/abs/2505.17883</link>
<guid>https://arxiv.org/abs/2505.17883</guid>
<content:encoded><![CDATA[
arXiv:2505.17883v1 Announce Type: new 
Abstract: Concepts such as objects, patterns, and shapes are how humans understand the world. Building on this intuition, concept-based explainability methods aim to study representations learned by deep neural networks in relation to human-understandable concepts. Here, Concept Activation Vectors (CAVs) are an important tool and can identify whether a model learned a concept or not. However, the computational cost and time requirements of existing CAV computation pose a significant challenge, particularly in large-scale, high-dimensional architectures. To address this limitation, we introduce FastCAV, a novel approach that accelerates the extraction of CAVs by up to 63.6x (on average 46.4x). We provide a theoretical foundation for our approach and give concrete assumptions under which it is equivalent to established SVM-based methods. Our empirical results demonstrate that CAVs calculated with FastCAV maintain similar performance while being more efficient and stable. In downstream applications, i.e., concept-based explanation methods, we show that FastCAV can act as a replacement leading to equivalent insights. Hence, our approach enables previously infeasible investigations of deep models, which we demonstrate by tracking the evolution of concepts during model training.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Domain Adaptation Benchmark for Time Series Data Representation</title>
<link>https://arxiv.org/abs/2505.17899</link>
<guid>https://arxiv.org/abs/2505.17899</guid>
<content:encoded><![CDATA[
arXiv:2505.17899v1 Announce Type: new 
Abstract: Deep learning models have significantly improved the ability to detect novelties in time series (TS) data. This success is attributed to their strong representation capabilities. However, due to the inherent variability in TS data, these models often struggle with generalization and robustness. To address this, a common approach is to perform Unsupervised Domain Adaptation, particularly Universal Domain Adaptation (UniDA), to handle domain shifts and emerging novel classes. While extensively studied in computer vision, UniDA remains underexplored for TS data. This work provides a comprehensive implementation and comparison of state-of-the-art TS backbones in a UniDA framework. We propose a reliable protocol to evaluate their robustness and generalization across different domains. The goal is to provide practitioners with a framework that can be easily extended to incorporate future advancements in UniDA and TS architectures. Our results highlight the critical influence of backbone selection in UniDA performance and enable a robustness analysis across various datasets and architectures.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving Machine Learning: A Survey</title>
<link>https://arxiv.org/abs/2505.17902</link>
<guid>https://arxiv.org/abs/2505.17902</guid>
<content:encoded><![CDATA[
arXiv:2505.17902v1 Announce Type: new 
Abstract: In an era defined by rapid data evolution, traditional machine learning (ML) models often fall short in adapting to dynamic environments. Evolving Machine Learning (EML) has emerged as a critical paradigm, enabling continuous learning and adaptation in real-time data streams. This survey presents a comprehensive analysis of EML, focusing on five core challenges: data drift, concept drift, catastrophic forgetting, skewed learning, and network adaptation. We systematically review over 120 studies, categorizing state-of-the-art methods across supervised, unsupervised, and semi-supervised approaches. The survey explores diverse evaluation metrics, benchmark datasets, and real-world applications, offering a comparative lens on the effectiveness and limitations of current techniques. Additionally, we highlight the growing role of adaptive neural architectures, meta-learning, and ensemble strategies in addressing evolving data complexities. By synthesizing insights from recent literature, this work not only maps the current landscape of EML but also identifies critical gaps and opportunities for future research. Our findings aim to guide researchers and practitioners in developing robust, ethical, and scalable EML systems for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling</title>
<link>https://arxiv.org/abs/2505.17909</link>
<guid>https://arxiv.org/abs/2505.17909</guid>
<content:encoded><![CDATA[
arXiv:2505.17909v1 Announce Type: new 
Abstract: Model ensembles have long been a cornerstone for improving generalization and robustness in deep learning. However, their effectiveness often comes at the cost of substantial computational overhead. To address this issue, state-of-the-art methods aim to replicate ensemble-class performance without requiring multiple independently trained networks. Unfortunately, these algorithms often still demand considerable compute at inference. In response to these limitations, we introduce $\textbf{NeuroTrails}$, a sparse multi-head architecture with dynamically evolving topology. This unexplored model-agnostic training paradigm improves ensemble performance while reducing the required resources. We analyze the underlying reason for its effectiveness and observe that the various neural trails induced by dynamic sparsity attain a $\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays efficacy with convolutional and transformer-based architectures on computer vision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4, among many others, demonstrate increased accuracy and stronger robustness in zero-shot generalization, while requiring significantly fewer parameters.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Meeting Decision Trees on Tabular Data</title>
<link>https://arxiv.org/abs/2505.17918</link>
<guid>https://arxiv.org/abs/2505.17918</guid>
<content:encoded><![CDATA[
arXiv:2505.17918v1 Announce Type: new 
Abstract: Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data throughough logical decision tree rules as intermediaries, proposes a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of ``errors'' reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches</title>
<link>https://arxiv.org/abs/2505.17919</link>
<guid>https://arxiv.org/abs/2505.17919</guid>
<content:encoded><![CDATA[
arXiv:2505.17919v1 Announce Type: new 
Abstract: Despite the widely recognized success of residual connections in modern neural networks, their design principles remain largely heuristic. This paper introduces KITINet (Kinetics Theory Inspired Network), a novel architecture that reinterprets feature propagation through the lens of non-equilibrium particle dynamics and partial differential equation (PDE) simulation. At its core, we propose a residual module that models feature updates as the stochastic evolution of a particle system, numerically simulated via a discretized solver for the Boltzmann transport equation (BTE). This formulation mimics particle collisions and energy exchange, enabling adaptive feature refinement via physics-informed interactions. Additionally, we reveal that this mechanism induces network parameter condensation during training, where parameters progressively concentrate into a sparse subset of dominant channels. Experiments on scientific computation (PDE operator), image classification (CIFAR-10/100), and text classification (IMDb/SNLI) show consistent improvements over classic network baselines, with negligible increase of FLOPs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV</title>
<link>https://arxiv.org/abs/2505.17929</link>
<guid>https://arxiv.org/abs/2505.17929</guid>
<content:encoded><![CDATA[
arXiv:2505.17929v1 Announce Type: new 
Abstract: Intensive care unit (ICU) is a crucial hospital department that handles life-threatening cases. Nowadays machine learning (ML) is being leveraged in healthcare ubiquitously. In recent years, management of ICU became one of the most significant parts of the hospital functionality (largely but not only due to the worldwide COVID-19 pandemic). This study explores multiple ML approaches for predicting LOS in ICU specifically for the patients with neurological diseases based on the MIMIC-IV dataset. The evaluated models include classic ML algorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and Neural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS prediction is often framed as a classification task, this study categorizes LOS into three groups: less than two days, less than a week, and a week or more. As the first ML-based approach targeting LOS prediction for neurological disorder patients, this study does not aim to outperform existing methods but rather to assess their effectiveness in this specific context. The findings provide insights into the applicability of ML techniques for improving ICU resource management and patient care. According to the results, Random Forest model proved to outperform others on static, achieving an accuracy of 0.68, a precision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model outperformed LSTM model on time-series data with an accuracy of 0.80, a precision of 0.80, a recall of 0.80 and F1-score 0.80.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Gated Neurons in Transformers from Their Input-Output Functionality</title>
<link>https://arxiv.org/abs/2505.17936</link>
<guid>https://arxiv.org/abs/2505.17936</guid>
<content:encoded><![CDATA[
arXiv:2505.17936v1 Announce Type: new 
Abstract: Interpretability researchers have attempted to understand MLP neurons of language models based on both the contexts in which they activate and their output weight vectors. They have paid little attention to a complementary aspect: the interactions between input and output. For example, when neurons detect a direction in the input, they might add much the same direction to the residual stream ("enrichment neurons") or reduce its presence ("depletion neurons"). We address this aspect by examining the cosine similarity between input and output weights of a neuron. We apply our method to 12 models and find that enrichment neurons dominate in early-middle layers whereas later layers tend more towards depletion. To explain this finding, we argue that enrichment neurons are largely responsible for enriching concept representations, one of the first steps of factual recall. Our input-output perspective is a complement to activation-dependent analyses and to approaches that treat input and output separately.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding</title>
<link>https://arxiv.org/abs/2505.17939</link>
<guid>https://arxiv.org/abs/2505.17939</guid>
<content:encoded><![CDATA[
arXiv:2505.17939v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) excel at learning from pairwise interactions but often overlook multi-way and hierarchical relationships. Topological Deep Learning (TDL) addresses this limitation by leveraging combinatorial topological spaces. However, existing TDL models are restricted to undirected settings and fail to capture the higher-order directed patterns prevalent in many complex systems, e.g., brain networks, where such interactions are both abundant and functionally significant. To fill this gap, we introduce Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that operate on semi-simplicial sets -- combinatorial structures that encode directed higher-order motifs and their directional relationships. To enhance scalability, we propose Routing-SSNs, which dynamically select the most informative relations in a learnable manner. We prove that SSNs are strictly more expressive than standard graph and TDL models. We then introduce a new principled framework for brain dynamics representation learning, grounded in the ability of SSNs to provably recover topological descriptors shown to successfully characterize brain activity. Empirically, SSNs achieve state-of-the-art performance on brain dynamics classification tasks, outperforming the second-best model by up to 27%, and message passing GNNs by up to 50% in accuracy. Our results highlight the potential of principled topological models for learning from structured brain data, establishing a unique real-world case study for TDL. We also test SSNs on standard node classification and edge regression tasks, showing competitive performance. We will make the code and data publicly available.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriThinker: Learning to Verify Makes Reasoning Model Efficient</title>
<link>https://arxiv.org/abs/2505.17941</link>
<guid>https://arxiv.org/abs/2505.17941</guid>
<content:encoded><![CDATA[
arXiv:2505.17941v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Principled Bayesian Framework for Training Binary and Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.17962</link>
<guid>https://arxiv.org/abs/2505.17962</guid>
<content:encoded><![CDATA[
arXiv:2505.17962v1 Announce Type: new 
Abstract: We propose a Bayesian framework for training binary and spiking neural networks that achieves state-of-the-art performance without normalisation layers. Unlike commonly used surrogate gradient methods -- often heuristic and sensitive to hyperparameter choices -- our approach is grounded in a probabilistic model of noisy binary networks, enabling fully end-to-end gradient-based optimisation. We introduce importance-weighted straight-through (IW-ST) estimators, a unified class generalising straight-through and relaxation-based estimators. We characterise the bias-variance trade-off in this family and derive a bias-minimising objective implemented via an auxiliary loss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs), a variational inference framework that uses posterior noise to train Binary and Spiking Neural Networks with IW-ST. This Bayesian approach minimises gradient bias, regularises parameters, and introduces dropout-like noise. By linking low-bias conditions, vanishing gradients, and the KL term, we enable training of deep residual networks without normalisation. Experiments on CIFAR-10, DVS Gesture, and SHD show our method matches or exceeds existing approaches without normalisation or hand-tuned gradients.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models</title>
<link>https://arxiv.org/abs/2505.17967</link>
<guid>https://arxiv.org/abs/2505.17967</guid>
<content:encoded><![CDATA[
arXiv:2505.17967v1 Announce Type: new 
Abstract: Low-rank optimization has emerged as a promising direction in training large language models (LLMs) to reduce the memory usage of adaptive optimizers by constraining learning to a lower-dimensional space. Prior work typically projects gradients of linear layers using approaches based on Singular Value Decomposition (SVD). However, applying SVD-based procedures individually to each layer in large models is computationally expensive and incurs additional memory costs due to storing the projection matrices. In this work, we propose a computationally efficient and conceptually simple two-step procedure to approximate SVD-based gradient projections into lower-dimensional spaces. First, we construct a complete orthogonal basis using predefined orthogonal matrices of the Discrete Cosine Transform (DCT). Second, we adaptively select basis columns based on their alignment with the gradient of each layer. Each projection matrix in our method is obtained via a single matrix multiplication followed by a lightweight sorting step to identify the most relevant basis vectors. Due to the predefined nature of the orthogonal bases, they are computed once at the start of training. During training, we store only the indices of the selected columns, avoiding the need to store full projection matrices for each layer. Our numerical experiments on both pre-training and fine-tuning tasks demonstrate the effectiveness of our dual strategy in approximating optimal low-rank projections, matching the performance of costly SVD-based methods while achieving faster runtime and reduced memory usage.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems</title>
<link>https://arxiv.org/abs/2505.17968</link>
<guid>https://arxiv.org/abs/2505.17968</guid>
<content:encoded><![CDATA[
arXiv:2505.17968v1 Announce Type: new 
Abstract: Using AI to create autonomous researchers has the potential to accelerate scientific discovery. A prerequisite for this vision is understanding how well an AI model can identify the underlying structure of a black-box system from its behavior. In this paper, we explore how well a large language model (LLM) learns to identify a black-box function from passively observed versus actively collected data. We investigate the reverse-engineering capabilities of LLMs across three distinct types of black-box systems, each chosen to represent different problem domains where future autonomous AI researchers may have considerable impact: Program, Formal Language, and Math Equation. Through extensive experiments, we show that LLMs fail to extract information from observations, reaching a performance plateau that falls short of the ideal of Bayesian inference. However, we demonstrate that prompting LLMs to not only observe but also intervene -- actively querying the black-box with specific inputs to observe the resulting output -- improves performance by allowing LLMs to test edge cases and refine their beliefs. By providing the intervention data from one LLM to another, we show that this improvement is partly a result of engaging in the process of generating effective interventions, paralleling results in the literature on human learning. Further analysis reveals that engaging in intervention can help LLMs escape from two common failure modes: overcomplication, where the LLM falsely assumes prior knowledge about the black-box, and overlooking, where the LLM fails to incorporate observations. These insights provide practical guidance for helping LLMs more effectively reverse-engineer black-box systems, supporting their use in making new discoveries.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models</title>
<link>https://arxiv.org/abs/2505.17974</link>
<guid>https://arxiv.org/abs/2505.17974</guid>
<content:encoded><![CDATA[
arXiv:2505.17974v1 Announce Type: new 
Abstract: The Fisher information is a fundamental concept for characterizing the sensitivity of parameters in neural networks. However, leveraging the full observed Fisher information is too expensive for large models, so most methods rely on simple diagonal approximations. While efficient, this approach ignores parameter correlations, often resulting in reduced performance on downstream tasks. In this work, we mitigate these limitations and propose Generalized Fisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that accounts for both diagonal and off-diagonal elements of the Fisher information matrix, providing a more accurate reflection of parameter importance. To make the method tractable, we introduce a scalable adaptation of the Kronecker-factored approximation algorithm for the observed Fisher information. We demonstrate the effectiveness of our method on LLM compression, showing improvements over existing compression baselines. For example, at a 20 compression rate on the MMLU benchmark, our method outperforms FWSVD, which is based on a diagonal approximation of the Fisher information, by 5 percent, SVD-LLM by 3 percent, and ASVD by 6 percent compression rate.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling</title>
<link>https://arxiv.org/abs/2505.17987</link>
<guid>https://arxiv.org/abs/2505.17987</guid>
<content:encoded><![CDATA[
arXiv:2505.17987v1 Announce Type: new 
Abstract: Real world collection of Activities of Daily Living data is challenging due to privacy concerns, costly deployment and labeling, and the inherent sparsity and imbalance of human behavior. We present ADLGen, a generative framework specifically designed to synthesize realistic, event triggered, and symbolic sensor sequences for ambient assistive environments. ADLGen integrates a decoder only Transformer with sign based symbolic temporal encoding, and a context and layout aware sampling mechanism to guide generation toward semantically rich and physically plausible sensor event sequences. To enhance semantic fidelity and correct structural inconsistencies, we further incorporate a large language model into an automatic generate evaluate refine loop, which verifies logical, behavioral, and temporal coherence and generates correction rules without manual intervention or environment specific tuning. Through comprehensive experiments with novel evaluation metrics, ADLGen is shown to outperform baseline generators in statistical fidelity, semantic richness, and downstream activity recognition, offering a scalable and privacy-preserving solution for ADL data synthesis.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17988</link>
<guid>https://arxiv.org/abs/2505.17988</guid>
<content:encoded><![CDATA[
arXiv:2505.17988v1 Announce Type: new 
Abstract: R1-style Reinforcement Learning (RL) significantly enhances Large Language Models' reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: https://github.com/on1262/deep-reasoning
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outcome-based Reinforcement Learning to Predict the Future</title>
<link>https://arxiv.org/abs/2505.17989</link>
<guid>https://arxiv.org/abs/2505.17989</guid>
<content:encoded><![CDATA[
arXiv:2505.17989v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has boosted math and coding in large language models, yet there has been little effort to extend RLVR into messier, real-world domains like forecasting. One sticking point is that outcome-based reinforcement learning for forecasting must learn from binary, delayed, and noisy rewards, a regime where standard fine-tuning is brittle. We show that outcome-only online RL on a 14B model can match frontier-scale accuracy and surpass it in calibration and hypothetical prediction market betting by adapting two leading algorithms, Group-Relative Policy Optimisation (GRPO) and ReMax, to the forecasting setting. Our adaptations remove per-question variance scaling in GRPO, apply baseline-subtracted advantages in ReMax, hydrate training with 100k temporally consistent synthetic questions, and introduce lightweight guard-rails that penalise gibberish, non-English responses and missing rationales, enabling a single stable pass over 110k events. Scaling ReMax to 110k questions and ensembling seven predictions yields a 14B model that matches frontier baseline o1 on accuracy on our holdout set (Brier = 0.193, p = 0.23) while beating it in calibration (ECE = 0.042, p < 0.001). A simple trading rule turns this calibration edge into \$127 of hypothetical profit versus \$92 for o1 (p = 0.037). This demonstrates that refined RLVR methods can convert small-scale LLMs into potentially economically valuable forecasting tools, with implications for scaling this to larger models.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2505.17997</link>
<guid>https://arxiv.org/abs/2505.17997</guid>
<content:encoded><![CDATA[
arXiv:2505.17997v1 Announce Type: new 
Abstract: The VAPO framework has demonstrated significant empirical success in enhancing the efficiency and reliability of reinforcement learning for long chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By systematically addressing challenges such as value model bias, heterogeneous sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art performance. While its practical benefits are evident, a deeper theoretical understanding of its underlying mechanisms and potential limitations is crucial for guiding future advancements. This paper aims to initiate such a discussion by exploring VAPO from a theoretical perspective, highlighting areas where its assumptions might be challenged and where further investigation could yield more robust and generalizable reasoning agents. We delve into the intricacies of value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and the enduring challenges of exploration and generalization.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective</title>
<link>https://arxiv.org/abs/2505.18002</link>
<guid>https://arxiv.org/abs/2505.18002</guid>
<content:encoded><![CDATA[
arXiv:2505.18002v1 Announce Type: new 
Abstract: Graph anomaly detection aims to identify unusual patterns in graph-based data, with wide applications in fields such as web security and financial fraud detection. Existing methods typically rely on contrastive learning, assuming that a lower similarity between a node and its local subgraph indicates abnormality. However, these approaches overlook a crucial limitation: the presence of interfering edges invalidates this assumption, since it introduces disruptive noise that compromises the contrastive learning process. Consequently, this limitation impairs the ability to effectively learn meaningful representations of normal patterns, leading to suboptimal detection performance. To address this issue, we propose a Clean-View Enhanced Graph Anomaly Detection framework (CVGAD), which includes a multi-scale anomaly awareness module to identify key sources of interference in the contrastive learning process. Moreover, to mitigate bias from the one-step edge removal process, we introduce a novel progressive purification module. This module incrementally refines the graph by iteratively identifying and removing interfering edges, thereby enhancing model performance. Extensive experiments on five benchmark datasets validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Example Safety Case for Safeguards Against Misuse</title>
<link>https://arxiv.org/abs/2505.18003</link>
<guid>https://arxiv.org/abs/2505.18003</guid>
<content:encoded><![CDATA[
arXiv:2505.18003v1 Announce Type: new 
Abstract: Existing evaluations of AI misuse safeguards provide a patchwork of evidence that is often difficult to connect to real-world decisions. To bridge this gap, we describe an end-to-end argument (a "safety case") that misuse safeguards reduce the risk posed by an AI assistant to low levels. We first describe how a hypothetical developer red teams safeguards, estimating the effort required to evade them. Then, the developer plugs this estimate into a quantitative "uplift model" to determine how much barriers introduced by safeguards dissuade misuse (https://www.aimisusemodel.com/). This procedure provides a continuous signal of risk during deployment that helps the developer rapidly respond to emerging threats. Finally, we describe how to tie these components together into a simple safety case. Our work provides one concrete path -- though not the only path -- to rigorously justifying AI misuse risks are low.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distances for Markov chains from sample streams</title>
<link>https://arxiv.org/abs/2505.18005</link>
<guid>https://arxiv.org/abs/2505.18005</guid>
<content:encoded><![CDATA[
arXiv:2505.18005v1 Announce Type: new 
Abstract: Bisimulation metrics are powerful tools for measuring similarities between stochastic processes, and specifically Markov chains. Recent advances have uncovered that bisimulation metrics are, in fact, optimal-transport distances, which has enabled the development of fast algorithms for computing such metrics with provable accuracy and runtime guarantees. However, these recent methods, as well as all previously known methods, assume full knowledge of the transition dynamics. This is often an impractical assumption in most real-world scenarios, where typically only sample trajectories are available. In this work, we propose a stochastic optimization method that addresses this limitation and estimates bisimulation metrics based on sample access, without requiring explicit transition models. Our approach is derived from a new linear programming (LP) formulation of bisimulation metrics, which we solve using a stochastic primal-dual optimization method. We provide theoretical guarantees on the sample complexity of the algorithm and validate its effectiveness through a series of empirical evaluations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling</title>
<link>https://arxiv.org/abs/2505.18017</link>
<guid>https://arxiv.org/abs/2505.18017</guid>
<content:encoded><![CDATA[
arXiv:2505.18017v1 Announce Type: new 
Abstract: Deep generative models hold great promise for representing complex physical systems, but their deployment is currently limited by the lack of guarantees on the physical plausibility of the generated outputs. Ensuring that known physical constraints are enforced is therefore critical when applying generative models to scientific and engineering problems. We address this limitation by developing a principled framework for sampling from a target distribution while rigorously satisfying physical constraints. Leveraging the variational formulation of Langevin dynamics, we propose Split Augmented Langevin (SAL), a novel primal-dual sampling algorithm that enforces constraints progressively through variable splitting, with convergence guarantees. While the method is developed theoretically for Langevin dynamics, we demonstrate its effective applicability to diffusion models. In particular, we use constrained diffusion models to generate physical fields satisfying energy and mass conservation laws. We apply our method to diffusion-based data assimilation on a complex physical system, where enforcing physical constraints substantially improves both forecast accuracy and the preservation of critical conserved quantities. We also demonstrate the potential of SAL for challenging feasibility problems in optimal control.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time</title>
<link>https://arxiv.org/abs/2505.18023</link>
<guid>https://arxiv.org/abs/2505.18023</guid>
<content:encoded><![CDATA[
arXiv:2505.18023v1 Announce Type: new 
Abstract: Recent years have seen significant progress in developing spiking neural networks (SNNs) as a potential solution to the energy challenges posed by conventional artificial neural networks (ANNs). However, our theoretical understanding of SNNs remains relatively limited compared to the ever-growing body of literature on ANNs. In this paper, we study a discrete-time model of SNNs based on leaky integrate-and-fire (LIF) neurons, referred to as discrete-time LIF-SNNs, a widely used framework that still lacks solid theoretical foundations. We demonstrate that discrete-time LIF-SNNs with static inputs and outputs realize piecewise constant functions defined on polyhedral regions, and more importantly, we quantify the network size required to approximate continuous functions. Moreover, we investigate the impact of latency (number of time steps) and depth (number of layers) on the complexity of the input space partitioning induced by discrete-time LIF-SNNs. Our analysis highlights the importance of latency and contrasts these networks with ANNs employing piecewise linear activation functions. Finally, we present numerical experiments to support our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knot So Simple: A Minimalistic Environment for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2505.18028</link>
<guid>https://arxiv.org/abs/2505.18028</guid>
<content:encoded><![CDATA[
arXiv:2505.18028v1 Announce Type: new 
Abstract: We propose KnotGym, an interactive environment for complex, spatial reasoning and manipulation. KnotGym includes goal-oriented rope manipulation tasks with varying levels of complexity, all requiring acting from pure image observations. Tasks are defined along a clear and quantifiable axis of complexity based on the number of knot crossings, creating a natural generalization test. KnotGym has a simple observation space, allowing for scalable development, yet it highlights core challenges in integrating acute perception, spatial reasoning, and grounded manipulation. We evaluate methods of different classes, including model-based RL, model-predictive control, and chain-of-thought reasoning, and illustrate the challenges KnotGym presents. KnotGym is available at https://github.com/lil-lab/knotgym.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mahalanobis++: Improving OOD Detection via Feature Normalization</title>
<link>https://arxiv.org/abs/2505.18032</link>
<guid>https://arxiv.org/abs/2505.18032</guid>
<content:encoded><![CDATA[
arXiv:2505.18032v1 Announce Type: new 
Abstract: Detecting out-of-distribution (OOD) examples is an important task for deploying reliable machine learning models in safety-critial applications. While post-hoc methods based on the Mahalanobis distance applied to pre-logit features are among the most effective for ImageNet-scale OOD detection, their performance varies significantly across models. We connect this inconsistency to strong variations in feature norms, indicating severe violations of the Gaussian assumption underlying the Mahalanobis distance estimation. We show that simple $\ell_2$-normalization of the features mitigates this problem effectively, aligning better with the premise of normally distributed data with shared covariance matrix. Extensive experiments on 44 models across diverse architectures and pretraining schemes show that $\ell_2$-normalization improves the conventional Mahalanobis distance-based approaches significantly and consistently, and outperforms other recently proposed OOD detection methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach</title>
<link>https://arxiv.org/abs/2505.18043</link>
<guid>https://arxiv.org/abs/2505.18043</guid>
<content:encoded><![CDATA[
arXiv:2505.18043v1 Announce Type: new 
Abstract: Clustering is a fundamental task in both machine learning and data mining. Among various methods, edge-colored clustering (ECC) has emerged as a useful approach for handling categorical data. Given a hypergraph with (hyper)edges labeled by colors, ECC aims to assign vertex colors to minimize the number of edges where the vertex color differs from the edge's color. However, traditional ECC has inherent limitations, as it enforces a nonoverlapping and exhaustive clustering. To tackle these limitations, three versions of ECC have been studied: Local ECC and Global ECC, which allow overlapping clusters, and Robust ECC, which accounts for vertex outliers. For these problems, both linear programming (LP) rounding algorithms and greedy combinatorial algorithms have been proposed. While these LP-rounding algorithms provide high-quality solutions, they demand substantial computation time; the greedy algorithms, on the other hand, run very fast but often compromise solution quality. In this paper, we present an algorithmic framework that combines the strengths of LP with the computational efficiency of combinatorial algorithms. Both experimental and theoretical analyses show that our algorithms efficiently produce high-quality solutions for all three problems: Local, Global, and Robust ECC. We complement our algorithmic contributions with complexity-theoretic inapproximability results and integrality gap bounds, which suggest that significant theoretical improvements are unlikely. Our results also answer two open questions previously raised in the literature.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Mixture Distributionally Robust Markov Decision Processes</title>
<link>https://arxiv.org/abs/2505.18044</link>
<guid>https://arxiv.org/abs/2505.18044</guid>
<content:encoded><![CDATA[
arXiv:2505.18044v1 Announce Type: new 
Abstract: Many real-world decision-making problems face the off-dynamics challenge: the agent learns a policy in a source domain and deploys it in a target domain with different state transitions. The distributionally robust Markov decision process (DRMDP) addresses this challenge by finding a robust policy that performs well under the worst-case environment within a pre-specified uncertainty set of transition dynamics. Its effectiveness heavily hinges on the proper design of these uncertainty sets, based on prior knowledge of the dynamics. In this work, we propose a novel linear mixture DRMDP framework, where the nominal dynamics is assumed to be a linear mixture model. In contrast with existing uncertainty sets directly defined as a ball centered around the nominal kernel, linear mixture DRMDPs define the uncertainty sets based on a ball around the mixture weighting parameter. We show that this new framework provides a more refined representation of uncertainties compared to conventional models based on $(s,a)$-rectangularity and $d$-rectangularity, when prior knowledge about the mixture model is present. We propose a meta algorithm for robust policy learning in linear mixture DRMDPs with general $f$-divergence defined uncertainty sets, and analyze its sample complexities under three divergence metrics instantiations: total variation, Kullback-Leibler, and $\chi^2$ divergences. These results establish the statistical learnability of linear mixture DRMDPs, laying the theoretical foundation for future research on this new setting.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions</title>
<link>https://arxiv.org/abs/2505.18046</link>
<guid>https://arxiv.org/abs/2505.18046</guid>
<content:encoded><![CDATA[
arXiv:2505.18046v1 Announce Type: new 
Abstract: The Restricted Boltzmann Machine (RBM) is one of the simplest generative neural networks capable of learning input distributions. Despite its simplicity, the analysis of its performance in learning from the training data is only well understood in cases that essentially reduce to singular value decomposition of the data. Here, we consider the limit of a large dimension of the input space and a constant number of hidden units. In this limit, we simplify the standard RBM training objective into a form that is equivalent to the multi-index model with non-separable regularization. This opens a path to analyze training of the RBM using methods that are established for multi-index models, such as Approximate Message Passing (AMP) and its state evolution, and the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We then give rigorous asymptotics of the training dynamics of RBM on data generated by the spiked covariance model as a prototype of a structure suitable for unsupervised learning. We show in particular that RBM reaches the optimal computational weak recovery threshold, aligning with the BBP transition, in the spiked covariance model.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymptotically optimal regret in communicating Markov decision processes</title>
<link>https://arxiv.org/abs/2505.18064</link>
<guid>https://arxiv.org/abs/2505.18064</guid>
<content:encoded><![CDATA[
arXiv:2505.18064v1 Announce Type: new 
Abstract: In this paper, we present a learning algorithm that achieves asymptotically optimal regret for Markov decision processes in average reward under a communicating assumption. That is, given a communicating Markov decision process $M$, our algorithm has regret $K(M) \log(T) + \mathrm{o}(\log(T))$ where $T$ is the number of learning steps and $K(M)$ is the best possible constant. This algorithm works by explicitly tracking the constant $K(M)$ to learn optimally, then balances the trade-off between exploration (playing sub-optimally to gain information), co-exploration (playing optimally to gain information) and exploitation (playing optimally to score maximally). We further show that the function $K(M)$ is discontinuous, which is a consequence challenge for our approach. To that end, we describe a regularization mechanism to estimate $K(M)$ with arbitrary precision from empirical data.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Model Generalization for Compute-Aware Test-Time Reasoning</title>
<link>https://arxiv.org/abs/2505.18065</link>
<guid>https://arxiv.org/abs/2505.18065</guid>
<content:encoded><![CDATA[
arXiv:2505.18065v1 Announce Type: new 
Abstract: External test-time reasoning enhances large language models (LLMs) by decoupling generation and selection. At inference time, the model generates multiple reasoning paths, and an auxiliary process reward model (PRM) is used to score and select the best one. A central challenge in this setting is test-time compute optimality (TCO), i.e., how to maximize answer accuracy under a fixed inference budget. In this work, we establish a theoretical framework to analyze how the generalization error of the PRM affects compute efficiency and reasoning performance. Leveraging PAC-Bayes theory, we derive generalization bounds and show that a lower generalization error of PRM leads to fewer samples required to find correct answers. Motivated by this analysis, we propose Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically controls search behavior. The actor outputs sampling hyperparameters based on reward distributions and sparsity statistics, while the critic estimates their utility to guide budget allocation. Experiments on the MATH and AIME benchmarks with various LLMs and PRMs demonstrate that CATS consistently outperforms other external TTS methods, validating our theoretical predictions.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Hebbian Dynamics in Regularized Non-Local Learners</title>
<link>https://arxiv.org/abs/2505.18069</link>
<guid>https://arxiv.org/abs/2505.18069</guid>
<content:encoded><![CDATA[
arXiv:2505.18069v1 Announce Type: new 
Abstract: Stochastic Gradient Descent (SGD) has emerged as a remarkably effective learning algorithm, underpinning nearly all state-of-the-art machine learning models, from large language models to autonomous vehicles. Despite its practical success, SGD appears fundamentally distinct from biological learning mechanisms. It is widely believed that the biological brain can not implement gradient descent because it is nonlocal, and we have found little (if any) experimental evidence for it. In contrast, the brain is widely thought to learn via local Hebbian learning principles, which have been seen as incompatible with gradient descent. In this paper, we establish a theoretical and empirical connection between the learning signals of neural networks trained using SGD with weight decay and those trained with Hebbian learning near convergence. We show that SGD with regularization can appear to learn according to a Hebbian rule, and SGD with injected noise according to an anti-Hebbian rule. We also provide empirical evidence that Hebbian learning properties can emerge in a network with weight decay from virtually any learning rule--even random ones. These results may bridge a long-standing gap between artificial and biological learning, revealing Hebbian properties as an epiphenomenon of deeper optimization principles and cautioning against interpreting their presence in neural data as evidence against more complex hetero-synaptic mechanisms.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction</title>
<link>https://arxiv.org/abs/2505.18080</link>
<guid>https://arxiv.org/abs/2505.18080</guid>
<content:encoded><![CDATA[
arXiv:2505.18080v1 Announce Type: new 
Abstract: This paper presents AFD-STA Net, a neural framework integrating adaptive filtering and spatiotemporal dynamics learning for predicting high-dimensional chaotic systems governed by partial differential equations. The architecture combines: 1) An adaptive exponential smoothing module with position-aware decay coefficients for robust attractor reconstruction, 2) Parallel attention mechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated fusion of multiscale features, and 4) Deep projection networks with dimension-scaling capabilities. Numerical experiments on nonlinear PDE systems demonstrate the model's effectiveness in maintaining prediction accuracy under both smooth and strongly chaotic regimes while exhibiting noise tolerance through adaptive filtering. Component ablation studies confirm critical contributions from each module, particularly highlighting the essential role of spatiotemporal attention in learning complex dynamical interactions. The framework shows promising potential for real-world applications requiring simultaneous handling of measurement uncertainties and high-dimensional nonlinear dynamics.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backpropagation-Free Metropolis-Adjusted Langevin Algorithm</title>
<link>https://arxiv.org/abs/2505.18081</link>
<guid>https://arxiv.org/abs/2505.18081</guid>
<content:encoded><![CDATA[
arXiv:2505.18081v1 Announce Type: new 
Abstract: Recent work on backpropagation-free learning has shown that it is possible to use forward-mode automatic differentiation (AD) to perform optimization on differentiable models. Forward-mode AD requires sampling a tangent vector for each forward pass of a model. The result is the model evaluation with the directional derivative along the tangent. In this paper, we illustrate how the sampling of this tangent vector can be incorporated into the proposal mechanism for the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the first to introduce a backpropagation-free gradient-based Markov chain Monte Carlo (MCMC) algorithm. We also extend to a novel backpropagation-free position-specific preconditioned forward-mode MALA that leverages Hessian information. Overall, we propose four new algorithms: Forward MALA; Line Forward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward MALA. We highlight the reduced computational cost of the forward-mode samplers and show that forward-mode is competitive with the original MALA, while even outperforming it depending on the probabilistic model. We include Bayesian inference results on a range of probabilistic models, including hierarchical distributions and Bayesian neural networks.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Iterative Framework for Generative Backmapping of Coarse Grained Proteins</title>
<link>https://arxiv.org/abs/2505.18082</link>
<guid>https://arxiv.org/abs/2505.18082</guid>
<content:encoded><![CDATA[
arXiv:2505.18082v1 Announce Type: new 
Abstract: The techniques of data-driven backmapping from coarse-grained (CG) to fine-grained (FG) representation often struggle with accuracy, unstable training, and physical realism, especially when applied to complex systems such as proteins. In this work, we introduce a novel iterative framework by using conditional Variational Autoencoders and graph-based neural networks, specifically designed to tackle the challenges associated with such large-scale biomolecules. Our method enables stepwise refinement from CG beads to full atomistic details. We outline the theory of iterative generative backmapping and demonstrate via numerical experiments the advantages of multistep schemes by applying them to proteins of vastly different structures with very coarse representations. This multistep approach not only improves the accuracy of reconstructions but also makes the training process more computationally efficient for proteins with ultra-CG representations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?</title>
<link>https://arxiv.org/abs/2505.18083</link>
<guid>https://arxiv.org/abs/2505.18083</guid>
<content:encoded><![CDATA[
arXiv:2505.18083v1 Announce Type: new 
Abstract: In planning, stitching is an ability of algorithms to piece together sub-trajectories of data they are trained on to generate new and diverse behaviours. While stitching is historically a strength of offline reinforcement learning, recent generative behavioural cloning (BC) methods have also shown proficiency at stitching. However, the main factors behind this are poorly understood, hindering the development of new algorithms that can reliably stitch. Focusing on diffusion planners trained via BC, we find two properties are needed to compose: \emph{positional equivariance} and \emph{local receptiveness}. We use these two properties to explain architecture, data, and inference choices in existing generative BC methods based on diffusion planning, including replanning frequency, data augmentation, and data scaling. Experimental comparisions show that (1) while locality is more important than positional equivariance in creating a diffusion planner capable of composition, both are crucial (2) enabling these properties through relatively simple architecture choices can be competitive with more computationally expensive methods such as replanning or scaling data, and (3) simple inpainting-based guidance can guide architecturally compositional models to enable generalization in goal-conditioned settings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early-Exit Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.18088</link>
<guid>https://arxiv.org/abs/2505.18088</guid>
<content:encoded><![CDATA[
arXiv:2505.18088v1 Announce Type: new 
Abstract: Early-exit mechanisms allow deep neural networks to halt inference as soon as classification confidence is high enough, adaptively trading depth for confidence, and thereby cutting latency and energy on easy inputs while retaining full-depth accuracy for harder ones. Similarly, adding early exit mechanisms to Graph Neural Networks (GNNs), the go-to models for graph-structured data, allows for dynamic trading depth for confidence on simple graphs while maintaining full-depth accuracy on harder and more complex graphs to capture intricate relationships. Although early exits have proven effective across various deep learning domains, their potential within GNNs in scenarios that require deep architectures while resisting over-smoothing and over-squashing remains largely unexplored. We unlock that potential by first introducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose symmetry-based inductive biases mitigate these issues and yield stable intermediate representations that can be useful to allow early exiting in GNNs. Building on this backbone, we present Early-Exit Graph Neural Networks (EEGNNs), which append confidence-aware exit heads that allow on-the-fly termination of propagation based on each node or the entire graph. Experiments show that EEGNNs preserve robust performance as depth grows and deliver competitive accuracy on heterophilic and long-range benchmarks, matching attention-based and asynchronous message-passing models while substantially reducing computation and latency. We plan to release the code to reproduce our experiments.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Mixing Can Induce Phase Transitions in Knowledge Acquisition</title>
<link>https://arxiv.org/abs/2505.18091</link>
<guid>https://arxiv.org/abs/2505.18091</guid>
<content:encoded><![CDATA[
arXiv:2505.18091v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets, unlike training exclusively on knowledge-dense data (arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards more transferable adversarial attack in black-box manner</title>
<link>https://arxiv.org/abs/2505.18097</link>
<guid>https://arxiv.org/abs/2505.18097</guid>
<content:encoded><![CDATA[
arXiv:2505.18097v1 Announce Type: new 
Abstract: Adversarial attacks have become a well-explored domain, frequently serving as evaluation baselines for model robustness. Among these, black-box attacks based on transferability have received significant attention due to their practical applicability in real-world scenarios. Traditional black-box methods have generally focused on improving the optimization framework (e.g., utilizing momentum in MI-FGSM) to enhance transferability, rather than examining the dependency on surrogate white-box model architectures. Recent state-of-the-art approach DiffPGD has demonstrated enhanced transferability by employing diffusion-based adversarial purification models for adaptive attacks. The inductive bias of diffusion-based adversarial purification aligns naturally with the adversarial attack process, where both involving noise addition, reducing dependency on surrogate white-box model selection. However, the denoising process of diffusion models incurs substantial computational costs through chain rule derivation, manifested in excessive VRAM consumption and extended runtime. This progression prompts us to question whether introducing diffusion models is necessary. We hypothesize that a model sharing similar inductive bias to diffusion-based adversarial purification, combined with an appropriate loss function, could achieve comparable or superior transferability while dramatically reducing computational overhead. In this paper, we propose a novel loss function coupled with a unique surrogate model to validate our hypothesis. Our approach leverages the score of the time-dependent classifier from classifier-guided diffusion models, effectively incorporating natural data distribution knowledge into the adversarial optimization process. Experimental results demonstrate significantly improved transferability across diverse model architectures while maintaining robustness against diffusion-based defenses.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning</title>
<link>https://arxiv.org/abs/2505.18101</link>
<guid>https://arxiv.org/abs/2505.18101</guid>
<content:encoded><![CDATA[
arXiv:2505.18101v1 Announce Type: new 
Abstract: Online Continual Learning (OCL) presents a complex learning environment in which new data arrives in a batch-to-batch online format, and the risk of catastrophic forgetting can significantly impair model efficacy. In this study, we address OCL by introducing an innovative memory framework that incorporates a short-term memory system to retain dynamic information and a long-term memory system to archive enduring knowledge. Specifically, the long-term memory system comprises a collection of sub-memory buffers, each linked to a cluster prototype and designed to retain data samples from distinct categories. We propose a novel $K$-means-based sample selection method to identify cluster prototypes for each encountered category. To safeguard essential and critical samples, we introduce a novel memory optimisation strategy that selectively retains samples in the appropriate sub-memory buffer by evaluating each cluster prototype against incoming samples through an optimal transportation mechanism. This approach specifically promotes each sub-memory buffer to retain data samples that exhibit significant discrepancies from the corresponding cluster prototype, thereby ensuring the preservation of semantically rich information. In addition, we propose a novel Divide-and-Conquer (DAC) approach that formulates the memory updating as an optimisation problem and divides it into several subproblems. As a result, the proposed DAC approach can solve these subproblems separately and thus can significantly reduce computations of the proposed memory updating process. We conduct a series of experiments across standard and imbalanced learning settings, and the empirical findings indicate that the proposed memory framework achieves state-of-the-art performance in both learning contexts.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
<link>https://arxiv.org/abs/2505.18102</link>
<guid>https://arxiv.org/abs/2505.18102</guid>
<content:encoded><![CDATA[
arXiv:2505.18102v1 Announce Type: new 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization</title>
<link>https://arxiv.org/abs/2505.18113</link>
<guid>https://arxiv.org/abs/2505.18113</guid>
<content:encoded><![CDATA[
arXiv:2505.18113v1 Announce Type: new 
Abstract: Training quantized neural networks requires addressing the non-differentiable and discrete nature of the underlying optimization problem. To tackle this challenge, the straight-through estimator (STE) has become the most widely adopted heuristic, allowing backpropagation through discrete operations by introducing surrogate gradients. However, its theoretical properties remain largely unexplored, with few existing works simplifying the analysis by assuming an infinite amount of training data. In contrast, this work presents the first finite-sample analysis of STE in the context of neural network quantization. Our theoretical results highlight the critical role of sample size in the success of STE, a key insight absent from existing studies. Specifically, by analyzing the quantization-aware training of a two-layer neural network with binary weights and activations, we derive the sample complexity bound in terms of the data dimensionality that guarantees the convergence of STE-based optimization to the global minimum. Moreover, in the presence of label noises, we uncover an intriguing recurrence property of STE-gradient method, where the iterate repeatedly escape from and return to the optimal binary weights. Our analysis leverages tools from compressed sensing and dynamical systems theory.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Supervised Learning and Reinforcement Learning in Math Reasoning</title>
<link>https://arxiv.org/abs/2505.18116</link>
<guid>https://arxiv.org/abs/2505.18116</guid>
<content:encoded><![CDATA[
arXiv:2505.18116v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of LLMs' math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs' generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like Rejection sampling Fine-Tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations</title>
<link>https://arxiv.org/abs/2505.18125</link>
<guid>https://arxiv.org/abs/2505.18125</guid>
<content:encoded><![CDATA[
arXiv:2505.18125v1 Announce Type: new 
Abstract: While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: a Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Model Overoptimisation in Iterated RLHF</title>
<link>https://arxiv.org/abs/2505.18126</link>
<guid>https://arxiv.org/abs/2505.18126</guid>
<content:encoded><![CDATA[
arXiv:2505.18126v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is a widely used method for aligning large language models with human preferences. However, RLHF often suffers from reward model overoptimisation, in which models overfit to the reward function, resulting in non-generalisable policies that exploit the idiosyncrasies and peculiarities of the reward function. A common mitigation is iterated RLHF, in which reward models are repeatedly retrained with updated human feedback and policies are re-optimised. Despite its increasing adoption, the dynamics of overoptimisation in this setting remain poorly understood. In this work, we present the first comprehensive study of overoptimisation in iterated RLHF. We systematically analyse key design choices - how reward model training data is transferred across iterations, which reward function is used for optimisation, and how policies are initialised. Using the controlled AlpacaFarm benchmark, we observe that overoptimisation tends to decrease over successive iterations, as reward models increasingly approximate ground-truth preferences. However, performance gains diminish over time, and while reinitialising from the base policy is robust, it limits optimisation flexibility. Other initialisation strategies often fail to recover from early overoptimisation. These findings offer actionable insights for building more stable and generalisable RLHF pipelines.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement</title>
<link>https://arxiv.org/abs/2505.18131</link>
<guid>https://arxiv.org/abs/2505.18131</guid>
<content:encoded><![CDATA[
arXiv:2505.18131v1 Announce Type: new 
Abstract: Multilayer perceptrons (MLPs) are a workhorse machine learning architecture, used in a variety of modern deep learning frameworks. However, recently Kolmogorov-Arnold Networks (KANs) have become increasingly popular due to their success on a range of problems, particularly for scientific machine learning tasks. In this paper, we exploit the relationship between KANs and multichannel MLPs to gain structural insight into how to train MLPs faster. We demonstrate the KAN basis (1) provides geometric localized support, and (2) acts as a preconditioned descent in the ReLU basis, overall resulting in expedited training and improved accuracy. Our results show the equivalence between free-knot spline KAN architectures, and a class of MLPs that are refined geometrically along the channel dimension of each weight tensor. We exploit this structural equivalence to define a hierarchical refinement scheme that dramatically accelerates training of the multi-channel MLP architecture. We show further accuracy improvements can be had by allowing the $1$D locations of the spline knots to be trained simultaneously with the weights. These advances are demonstrated on a range of benchmark examples for regression and scientific machine learning.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Distribution Embeddings</title>
<link>https://arxiv.org/abs/2505.18150</link>
<guid>https://arxiv.org/abs/2505.18150</guid>
<content:encoded><![CDATA[
arXiv:2505.18150v1 Announce Type: new 
Abstract: Many real-world problems require reasoning across multiple scales, demanding models which operate not on single data points, but on entire distributions. We introduce generative distribution embeddings (GDE), a framework that lifts autoencoders to the space of distributions. In GDEs, an encoder acts on sets of samples, and the decoder is replaced by a generator which aims to match the input distribution. This framework enables learning representations of distributions by coupling conditional generative models with encoder networks which satisfy a criterion we call distributional invariance. We show that GDEs learn predictive sufficient statistics embedded in the Wasserstein space, such that latent GDE distances approximately recover the $W_2$ distance, and latent interpolation approximately recovers optimal transport trajectories for Gaussian and Gaussian mixture distributions. We systematically benchmark GDEs against existing approaches on synthetic datasets, demonstrating consistently stronger performance. We then apply GDEs to six key problems in computational biology: learning representations of cell populations from lineage-tracing data (150K cells), predicting perturbation effects on single-cell transcriptomes (1M cells), predicting perturbation effects on cellular phenotypes (20M single-cell images), modeling tissue-specific DNA methylation patterns (253M sequences), designing synthetic yeast promoters (34M sequences), and spatiotemporal modeling of viral protein sequences (1M sequences).
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Uncertain Combinatorics through Graphization, Hyperization, and Uncertainization: Fuzzy, Neutrosophic, Soft, Rough, and Beyond</title>
<link>https://arxiv.org/abs/2411.17411</link>
<guid>https://arxiv.org/abs/2411.17411</guid>
<content:encoded><![CDATA[
arXiv:2411.17411v1 Announce Type: cross 
Abstract: To better handle real-world uncertainty, concepts such as fuzzy sets, neutrosophic sets, rough sets, and soft sets have been introduced. For example, neutrosophic sets, which simultaneously represent truth, indeterminacy, and falsehood, have proven to be valuable tools for modeling uncertainty in complex systems. These set concepts are increasingly studied in graphized forms, and generalized graph concepts now encompass well-known structures such as hypergraphs and superhypergraphs. Furthermore, hyperconcepts and superhyperconcepts are being actively researched in areas beyond graph theory.
  Combinatorics, uncertain sets (including fuzzy sets, neutrosophic sets, rough sets, soft sets, and plithogenic sets), uncertain graphs, and hyper and superhyper concepts are active areas of research with significant mathematical and practical implications. Recognizing their importance, this paper explores new graph and set concepts, as well as hyper and superhyper concepts, as detailed in the "Results" section of "The Structure of the Paper." Additionally, this work aims to consolidate recent findings, providing a survey-like resource to inform and engage readers.
  For instance, we extend several graph concepts by introducing Neutrosophic Oversets, Neutrosophic Undersets, Neutrosophic Offsets, and the Nonstandard Real Set. This paper defines a variety of concepts with the goal of inspiring new ideas and serving as a valuable resource for researchers in their academic pursuits.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation-Enabled Knowledge Alignment Protocol for Semantic Communication in AI Agent Networks</title>
<link>https://arxiv.org/abs/2505.17030</link>
<guid>https://arxiv.org/abs/2505.17030</guid>
<content:encoded><![CDATA[
arXiv:2505.17030v1 Announce Type: cross 
Abstract: Future networks are envisioned to connect massive artificial intelligence (AI) agents, enabling their extensive collaboration on diverse tasks. Compared to traditional entities, these agents naturally suit the semantic communication (SC), which can significantly enhance the bandwidth efficiency. Nevertheless, SC requires the knowledge among agents to be aligned, while agents have distinct expert knowledge for their individual tasks in practice. In this paper, we propose a distillation-enabled knowledge alignment protocol (DeKAP), which distills the expert knowledge of each agent into parameter-efficient low-rank matrices, allocates them across the network, and allows agents to simultaneously maintain aligned knowledge for multiple tasks. We formulate the joint minimization of alignment loss, communication overhead, and storage cost as a large-scale integer linear programming problem and develop a highly efficient greedy algorithm. From computer simulation, the DeKAP establishes knowledge alignment with the lowest communication and computation resources compared to conventional approaches.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A brief review of the Deep BSDE method for solving high-dimensional partial differential equations</title>
<link>https://arxiv.org/abs/2505.17032</link>
<guid>https://arxiv.org/abs/2505.17032</guid>
<content:encoded><![CDATA[
arXiv:2505.17032v1 Announce Type: cross 
Abstract: High-dimensional partial differential equations (PDEs) pose significant challenges for numerical computation due to the curse of dimensionality, which limits the applicability of traditional mesh-based methods. Since 2017, the Deep BSDE method has introduced deep learning techniques that enable the effective solution of nonlinear PDEs in very high dimensions. This innovation has sparked considerable interest in using neural networks for high-dimensional PDEs, making it an active area of research. In this short review, we briefly sketch the Deep BSDE method, its subsequent developments, and future directions for the field.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM-KG: Multimodal Radiology Knowledge Graph Generation</title>
<link>https://arxiv.org/abs/2505.17042</link>
<guid>https://arxiv.org/abs/2505.17042</guid>
<content:encoded><![CDATA[
arXiv:2505.17042v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success in natural language generation, excelling at instruction following and structured output generation. Knowledge graphs play a crucial role in radiology, serving as valuable sources of factual information and enhancing various downstream tasks. However, generating radiology-specific knowledge graphs presents significant challenges due to the specialized language of radiology reports and the limited availability of domain-specific data. Existing solutions are predominantly unimodal, meaning they generate knowledge graphs only from radiology reports while excluding radiographic images. Additionally, they struggle with long-form radiology data due to limited context length. To address these limitations, we propose a novel multimodal VLM-based framework for knowledge graph generation in radiology. Our approach outperforms previous methods and introduces the first multimodal solution for radiology knowledge graph generation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Flexible Quantum-Inspired Differential Equation Solvers with Data Integration</title>
<link>https://arxiv.org/abs/2505.17046</link>
<guid>https://arxiv.org/abs/2505.17046</guid>
<content:encoded><![CDATA[
arXiv:2505.17046v1 Announce Type: cross 
Abstract: Accurately solving high-dimensional partial differential equations (PDEs) remains a central challenge in computational mathematics. Traditional numerical methods, while effective in low-dimensional settings or on coarse grids, often struggle to deliver the precision required in practical applications. Recent machine learning-based approaches offer flexibility but frequently fall short in terms of accuracy and reliability, particularly in industrial contexts. In this work, we explore a quantum-inspired method based on quantized tensor trains (QTT), enabling efficient and accurate solutions to PDEs in a variety of challenging scenarios. Through several representative examples, we demonstrate that the QTT approach can achieve logarithmic scaling in both memory and computational cost for linear and nonlinear PDEs. Additionally, we introduce a novel technique for data-driven learning within the quantum-inspired framework, combining the adaptability of neural networks with enhanced accuracy and reduced training time.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METHOD: Modular Efficient Transformer for Health Outcome Discovery</title>
<link>https://arxiv.org/abs/2505.17054</link>
<guid>https://arxiv.org/abs/2505.17054</guid>
<content:encoded><![CDATA[
arXiv:2505.17054v1 Announce Type: cross 
Abstract: Recent advances in transformer architectures have revolutionised natural language processing, but their application to healthcare domains presents unique challenges. Patient timelines are characterised by irregular sampling, variable temporal dependencies, and complex contextual relationships that differ substantially from traditional language tasks. This paper introduces \METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture specifically designed to address the challenges of clinical sequence modelling in electronic health records. \METHOD~integrates three key innovations: (1) a patient-aware attention mechanism that prevents information leakage whilst enabling efficient batch processing; (2) an adaptive sliding window attention scheme that captures multi-scale temporal dependencies; and (3) a U-Net inspired architecture with dynamic skip connections for effective long sequence processing. Evaluations on the MIMIC-IV database demonstrate that \METHOD~consistently outperforms the state-of-the-art \ETHOS~model, particularly in predicting high-severity cases that require urgent clinical intervention. \METHOD~exhibits stable performance across varying inference lengths, a crucial feature for clinical deployment where patient histories vary significantly in length. Analysis of learned embeddings reveals that \METHOD~better preserves clinical hierarchies and relationships between medical concepts. These results suggest that \METHOD~represents a significant advancement in transformer architectures optimised for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Data RL: Task Definition Is All You Need</title>
<link>https://arxiv.org/abs/2505.17063</link>
<guid>https://arxiv.org/abs/2505.17063</guid>
<content:encoded><![CDATA[
arXiv:2505.17063v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases</title>
<link>https://arxiv.org/abs/2505.17065</link>
<guid>https://arxiv.org/abs/2505.17065</guid>
<content:encoded><![CDATA[
arXiv:2505.17065v1 Announce Type: cross 
Abstract: Recent advances in artificial intelligence, particularly large language models LLMs, have shown promising capabilities in transforming rare disease research. This survey paper explores the integration of LLMs in the analysis of rare diseases, highlighting significant strides and pivotal studies that leverage textual data to uncover insights and patterns critical for diagnosis, treatment, and patient care. While current research predominantly employs textual data, the potential for multimodal data integration combining genetic, imaging, and electronic health records stands as a promising frontier. We review foundational papers that demonstrate the application of LLMs in identifying and extracting relevant medical information, simulating intelligent conversational agents for patient interaction, and enabling the formulation of accurate and timely diagnoses. Furthermore, this paper discusses the challenges and ethical considerations inherent in deploying LLMs, including data privacy, model transparency, and the need for robust, inclusive data sets. As part of this exploration, we present a section on experimentation that utilizes multiple LLMs alongside structured questionnaires, specifically designed for diagnostic purposes in the context of different diseases. We conclude with future perspectives on the evolution of LLMs towards truly multimodal platforms, which would integrate diverse data types to provide a more comprehensive understanding of rare diseases, ultimately fostering better outcomes in clinical settings.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.17067</link>
<guid>https://arxiv.org/abs/2505.17067</guid>
<content:encoded><![CDATA[
arXiv:2505.17067v1 Announce Type: cross 
Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning</title>
<link>https://arxiv.org/abs/2505.17068</link>
<guid>https://arxiv.org/abs/2505.17068</guid>
<content:encoded><![CDATA[
arXiv:2505.17068v1 Announce Type: cross 
Abstract: In health-related topics, user toxicity in online discussions frequently becomes a source of social conflict or promotion of dangerous, unscientific behaviour; common approaches for battling it include different forms of detection, flagging and/or removal of existing toxic comments, which is often counterproductive for platforms and users alike. In this work, we propose the alternative of combatting user toxicity predictively, anticipating where a user could interact toxically in health-related online discussions. Applying a Collaborative Filtering-based Machine Learning methodology, we predict the toxicity in COVID-related conversations between any user and subcommunity of Reddit, surpassing 80% predictive performance in relevant metrics, and allowing us to prevent the pairing of conflicting users and subcommunities.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Alignment Can Be Not Superficial With Explicit Safety Signals</title>
<link>https://arxiv.org/abs/2505.17072</link>
<guid>https://arxiv.org/abs/2505.17072</guid>
<content:encoded><![CDATA[
arXiv:2505.17072v1 Announce Type: cross 
Abstract: Recent studies on the safety alignment of large language models (LLMs) have revealed that existing approaches often operate superficially, leaving models vulnerable to various adversarial attacks. Despite their significance, these studies generally fail to offer actionable solutions beyond data augmentation for achieving more robust safety mechanisms. This paper identifies a fundamental cause of this superficiality: existing alignment approaches often presume that models can implicitly learn a safety-related reasoning task during the alignment process, enabling them to refuse harmful requests. However, the learned safety signals are often diluted by other competing objectives, leading models to struggle with drawing a firm safety-conscious decision boundary when confronted with adversarial attacks. Based on this observation, by explicitly introducing a safety-related binary classification task and integrating its signals with our attention and decoding strategies, we eliminate this ambiguity and allow models to respond more responsibly to malicious queries. We emphasize that, with less than 0.2x overhead cost, our approach enables LLMs to assess the safety of both the query and the previously generated tokens at each necessary generating step. Extensive experiments demonstrate that our method significantly improves the resilience of LLMs against various adversarial attacks, offering a promising pathway toward more robust generative AI systems.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Interpretability of GPT-like Models on Summarization Tasks</title>
<link>https://arxiv.org/abs/2505.17073</link>
<guid>https://arxiv.org/abs/2505.17073</guid>
<content:encoded><![CDATA[
arXiv:2505.17073v1 Announce Type: cross 
Abstract: Mechanistic interpretability research seeks to reveal the inner workings of large language models, yet most work focuses on classification or generative tasks rather than summarization. This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks. We conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations. By identifying specific layers and attention heads that undergo significant transformation, we locate the "summarization circuit" within the model architecture. Our findings reveal that middle layers (particularly 2, 3, and 5) exhibit the most dramatic changes, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. We demonstrate that targeted LoRA adaptation of these identified circuits achieves significant performance improvement over standard LoRA fine-tuning while requiring fewer training epochs. This work bridges the gap between black-box evaluation and mechanistic understanding, providing insights into how neural networks perform information selection and compression during summarization.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency</title>
<link>https://arxiv.org/abs/2505.17074</link>
<guid>https://arxiv.org/abs/2505.17074</guid>
<content:encoded><![CDATA[
arXiv:2505.17074v1 Announce Type: cross 
Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by employing a small speculative model (SSM) to generate multiple candidate tokens and verify them using the LLM in parallel. This technique has been widely integrated into LLM inference serving systems. However, inference requests typically exhibit uncertain execution time, which poses a significant challenge of efficiently scheduling requests in these systems. Existing work estimates execution time based solely on predicted output length, which could be inaccurate because execution time depends on both output length and token acceptance rate of verification by the LLM. In this paper, we propose a semi-clairvoyant request scheduling algorithm called Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a number of inference requests, LAPS-SD can effectively minimize average inference latency by adaptively scheduling requests according to their features during decoding. When the token acceptance rate is dynamic and execution time is difficult to estimate, LAPS-SD maintains multiple priority queues and allows request execution preemption across different queues. Once the token acceptance rate becomes stable, LAPS-SD can accurately estimate the execution time and schedule requests accordingly. Extensive experiments show that LAPS-SD reduces inference latency by approximately 39\% compared to state-of-the-art scheduling methods.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streamlining HTTP Flooding Attack Detection through Incremental Feature Selection</title>
<link>https://arxiv.org/abs/2505.17077</link>
<guid>https://arxiv.org/abs/2505.17077</guid>
<content:encoded><![CDATA[
arXiv:2505.17077v1 Announce Type: cross 
Abstract: Applications over the Web primarily rely on the HTTP protocol to transmit web pages to and from systems. There are a variety of application layer protocols, but among all, HTTP is the most targeted because of its versatility and ease of integration with online services. The attackers leverage the fact that by default no detection system blocks any HTTP traffic. Thus, by exploiting such characteristics of the protocol, attacks are launched against web applications. HTTP flooding attacks are one such attack in the application layer of the OSI model. In this paper, a method for the detection of such an attack is proposed. The heart of the detection method is an incremental feature subset selection method based on mutual information and correlation. INFS-MICC helps in identifying a subset of highly relevant and independent feature subset so as to detect HTTP Flooding attacks with best possible classification performance in near-real time.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>