<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>


<item>
<title>Carbon Aware Transformers Through Joint Model-Hardware Optimization</title>
<link>https://arxiv.org/abs/2505.01386</link>
<guid>https://arxiv.org/abs/2505.01386</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, carbon footprint, sustainability, architecture search, environmental impact

Summary: 
CATransformers is a framework that aims to evaluate and optimize the total carbon footprint of machine learning systems, considering both operational and embodied emissions. By integrating carbon metrics into the design process of hardware accelerators, CATransformers can identify design choices that reduce carbon emissions without sacrificing performance. The framework was applied to CLIP-based models, resulting in CarbonCLIP models that achieved up to 17% reduction in total carbon emissions while maintaining accuracy and latency compared to existing models. This work highlights the importance of holistic optimization methods for designing high-performance and environmentally sustainable AI systems. <div>
arXiv:2505.01386v2 Announce Type: replace 
Abstract: The rapid growth of machine learning (ML) systems necessitates a more comprehensive evaluation of their environmental impact, particularly their carbon footprint, which comprises operational carbon from training and inference execution and embodied carbon from hardware manufacturing and its entire life-cycle. Despite the increasing importance of embodied emissions, there is a lack of tools and frameworks to holistically quantify and optimize the total carbon footprint of ML systems. To address this, we propose CATransformers, a carbon-aware architecture search framework that enables sustainability-driven co-optimization of ML models and hardware architectures. By incorporating both operational and embodied carbon metrics into early design space exploration of domain-specific hardware accelerators, CATransformers demonstrates that optimizing for carbon yields design choices distinct from those optimized solely for latency or energy efficiency. We apply our framework to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models achieving up to 17% reduction in total carbon emissions while maintaining accuracy and latency compared to state-of-the-art edge small CLIP baselines. This work underscores the need for holistic optimization methods to design high-performance, environmentally sustainable AI systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Thought Machines</title>
<link>https://arxiv.org/abs/2505.05522</link>
<guid>https://arxiv.org/abs/2505.05522</guid>
<content:encoded><![CDATA[
<div> neural dynamics, temporal processing, neural synchronization, deep learning, artificial intelligence

Summary:
The paper introduces the Continuous Thought Machine (CTM), a model that incorporates neural dynamics to reintroduce neural timing as a key element in deep learning architectures. The CTM utilizes neuron-level temporal processing and neural synchronization as its core innovations, balancing between computational efficiency and biological realism. It demonstrates strong performance across various tasks, including ImageNet-1K classification, sorting, question-answering, and RL tasks. The CTM's internal representations allow for interpretation and complex sequential reasoning abilities. It can adaptively adjust compute resources based on task complexity, showcasing versatility. The goal of the work is to present the CTM and its innovations as a significant step towards developing more biologically plausible and powerful artificial intelligence systems. 

<br /><br />Summary: <div>
arXiv:2505.05522v1 Announce Type: new 
Abstract: Biological brains demonstrate complex neural activity, where the timing and interplay between neurons is critical to how brains process information. Most deep learning architectures simplify neural activity by abstracting away temporal dynamics. In this paper we challenge that paradigm. By incorporating neuron-level processing and synchronization, we can effectively reintroduce neural timing as a foundational element. We present the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation. The CTM aims to strike a balance between oversimplified neuron abstractions that improve computational efficiency, and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable for deep learning. We demonstrate the CTM's strong performance and versatility across a range of challenging tasks, including ImageNet-1K classification, solving 2D mazes, sorting, parity computation, question-answering, and RL tasks. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, we believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows</title>
<link>https://arxiv.org/abs/2505.05525</link>
<guid>https://arxiv.org/abs/2505.05525</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, navigation, fluid mechanics, autonomous robots, PPO

Summary:
In the study, the authors evaluate reinforcement learning methods for navigation in partially observable flows. They introduce a directional navigation problem with a known optimal policy and assess commonly used algorithms like Q-Learning and Advantage Actor Critic in various flow scenarios. The results show poor performance and lack of robustness in these algorithms compared to Proximal Policy Optimization (PPO). Their custom implementation of PPO matches the theoretical optimal performance in turbulent flow and does so robustly. Additional techniques like vectorized environments and hyperparameter optimization were crucial in achieving these results. The study highlights the significance of algorithm selection, implementation details, and fine-tuning in developing efficient autonomous navigation strategies in complex flows.<br /><br />Summary: <div>
arXiv:2505.05525v1 Announce Type: new 
Abstract: Navigating in a fluid flow while being carried by it, using only information accessible from on-board sensors, is a problem commonly faced by small planktonic organisms. It is also directly relevant to autonomous robots deployed in the oceans. In the last ten years, the fluid mechanics community has widely adopted reinforcement learning, often in the form of its simplest implementations, to address this challenge. But it is unclear how good are the strategies learned by these algorithms. In this paper, we perform a quantitative assessment of reinforcement learning methods applied to navigation in partially observable flows. We first introduce a well-posed problem of directional navigation for which a quasi-optimal policy is known analytically. We then report on the poor performance and robustness of commonly used algorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered in the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and two-dimensional turbulence. We show that they are vastly surpassed by PPO (Proximal Policy Optimization), a more advanced algorithm that has established dominance across a wide range of benchmarks in the reinforcement learning community. In particular, our custom implementation of PPO matches the theoretical quasi-optimal performance in turbulent flow and does so in a robust manner. Reaching this result required the use of several additional techniques, such as vectorized environments and generalized advantage estimation, as well as hyperparameter optimization. This study demonstrates the importance of algorithm selection, implementation details, and fine-tuning for discovering truly smart autonomous navigation strategies in complex flows.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADMM-Based Training for Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.05527</link>
<guid>https://arxiv.org/abs/2505.05527</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, Training Algorithm, Alternating Direction Method of Multipliers, Non-differentiability, Convergence Properties 

Summary: 
Spiking neural networks (SNNs) have shown promise in time-series processing with low energy consumption but lack efficient training algorithms. The popular backpropagation method faces scalability and numerical precision issues in SNNs. To address this, a novel training method based on the alternating direction method of multipliers (ADMM) is proposed. This method tackles the problem of non-differentiability in SNN step functions. The formulation of the problem, derivation of closed-form updates, and empirical demonstration of the optimizer's convergence properties highlight the efficiency of the ADMM-based training in SNNs. This simulated proof-of-concept study points towards potential research directions for further improvement and optimization of the training method. <br /><br />Summary: <div>
arXiv:2505.05527v1 Announce Type: new 
Abstract: In recent years, spiking neural networks (SNNs) have gained momentum due to their high potential in time-series processing combined with minimal energy consumption. However, they still lack a dedicated and efficient training algorithm. The popular backpropagation with surrogate gradients, adapted from stochastic gradient descent (SGD)-derived algorithms, has several drawbacks when used as an optimizer for SNNs. Specifically, it suffers from low scalability and numerical imprecision. In this paper, we propose a novel SNN training method based on the alternating direction method of multipliers (ADMM). Our ADMM-based training aims to solve the problem of the SNN step function's non-differentiability. We formulate the problem, derive closed-form updates, and empirically show the optimizer's convergence properties, great potential, and possible new research directions to improve the method in a simulated proof-of-concept.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-bit Model Quantization for Deep Neural Networks: A Survey</title>
<link>https://arxiv.org/abs/2505.05530</link>
<guid>https://arxiv.org/abs/2505.05530</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, model quantization, low-bit quantization, state-of-the-art methods, research opportunities 

Summary:<br /><br />
This article discusses the advancements in low-bit quantization techniques for deep neural networks over the past five years. Model quantization is essential for reducing computation costs and model sizes in real-world deployment. The conversion from floating-point numbers to discrete integers accelerates memory I/O and calculations but may lead to performance degradation due to loss of precision. The article categorizes state-of-the-art quantization methods into 8 main categories and 24 sub-categories based on their core techniques. It highlights the importance of investigating methods to compensate for information loss in the quantization process. The article also presents potential research opportunities in model quantization. To explore further resources on model quantization, a curated list is provided at https://github.com/Kai-Liu001/Awesome-Model-Quantization. <div>
arXiv:2505.05530v1 Announce Type: new 
Abstract: With unprecedented rapid development, deep neural networks (DNNs) have deeply influenced almost all fields. However, their heavy computation costs and model sizes are usually unacceptable in real-world deployment. Model quantization, an effective weight-lighting technique, has become an indispensable procedure in the whole deployment pipeline. The essence of quantization acceleration is the conversion from continuous floating-point numbers to discrete integer ones, which significantly speeds up the memory I/O and calculation, i.e., addition and multiplication. However, performance degradation also comes with the conversion because of the loss of precision. Therefore, it has become increasingly popular and critical to investigate how to perform the conversion and how to compensate for the information loss. This article surveys the recent five-year progress towards low-bit quantization on DNNs. We discuss and compare the state-of-the-art quantization methods and classify them into 8 main categories and 24 sub-categories according to their core techniques. Furthermore, we shed light on the potential research opportunities in the field of model quantization. A curated list of model quantization is provided at https://github.com/Kai-Liu001/Awesome-Model-Quantization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Graph Contrastive Learning through Relative Similarity Preservation</title>
<link>https://arxiv.org/abs/2505.05533</link>
<guid>https://arxiv.org/abs/2505.05533</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph contrastive learning, semantic relationships, homophily-heterophily dichotomy, relative similarity patterns, RELGCL

Summary: 
Graph contrastive learning (GCL) aims to preserve similarity between augmented views in graphs but faces challenges due to their discrete nature. Through analysis of real-world graphs, a universal pattern is discovered where label consistency decreases as structural distance increases. This pattern is proven using random walk theory, showing that structurally closer nodes have stronger semantic relationships. The RELGCL framework is proposed to leverage this insight, with both pairwise and listwise implementations to preserve natural relative similarity. Extensive experiments show that RELGCL outperforms existing approaches on both homophily and heterophily graphs, demonstrating the effectiveness of using relative similarity over artificial absolute similarity.<br /><br />Summary: <div>
arXiv:2505.05533v1 Announce Type: new 
Abstract: Graph contrastive learning (GCL) has achieved remarkable success by following the computer vision paradigm of preserving absolute similarity between augmented views. However, this approach faces fundamental challenges in graphs due to their discrete, non-Euclidean nature -- view generation often breaks semantic validity and similarity verification becomes unreliable. Through analyzing 11 real-world graphs, we discover a universal pattern transcending the homophily-heterophily dichotomy: label consistency systematically diminishes as structural distance increases, manifesting as smooth decay in homophily graphs and oscillatory decay in heterophily graphs. We establish theoretical guarantees for this pattern through random walk theory, proving label distribution convergence and characterizing the mechanisms behind different decay behaviors. This discovery reveals that graphs naturally encode relative similarity patterns, where structurally closer nodes exhibit collectively stronger semantic relationships. Leveraging this insight, we propose RELGCL, a novel GCL framework with complementary pairwise and listwise implementations that preserve these inherent patterns through collective similarity objectives. Extensive experiments demonstrate that our method consistently outperforms 20 existing approaches across both homophily and heterophily graphs, validating the effectiveness of leveraging natural relative similarity over artificial absolute similarity.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching and ResNet</title>
<link>https://arxiv.org/abs/2505.05538</link>
<guid>https://arxiv.org/abs/2505.05538</guid>
<content:encoded><![CDATA[
<div> classification, Electrocardiogram, Cardioformer, self-attention, cardiovascular

Summary:<br /><br />
The paper introduces Cardioformer, a novel model for electrocardiogram (ECG) classification. It addresses challenges of capturing local morphological details and long-range temporal dependencies by integrating cross-channel patching, hierarchical residual learning, and a two-stage self-attention mechanism. The model encodes multi-scale token embeddings to capture fine-grained local features and global contextual information. It selectively fuses these representations through intra- and inter-granularity self-attention. Evaluations on three benchmark ECG datasets show Cardioformer consistently outperforms four state-of-the-art baselines, achieving high AUROC scores. The model also demonstrates strong cross-dataset generalization, highlighting its potential to advance automated ECG analysis for more accurate and robust cardiovascular disease diagnosis. The source code is made available on GitHub. <div>
arXiv:2505.05538v1 Announce Type: new 
Abstract: Electrocardiogram (ECG) classification is crucial for automated cardiac disease diagnosis, yet existing methods often struggle to capture local morphological details and long-range temporal dependencies simultaneously. To address these challenges, we propose Cardioformer, a novel multi-granularity hybrid model that integrates cross-channel patching, hierarchical residual learning, and a two-stage self-attention mechanism. Cardioformer first encodes multi-scale token embeddings to capture fine-grained local features and global contextual information and then selectively fuses these representations through intra- and inter-granularity self-attention. Extensive evaluations on three benchmark ECG datasets under subject-independent settings demonstrate that model consistently outperforms four state-of-the-art baselines. Our Cardioformer model achieves the AUROC of 96.34$\pm$0.11, 89.99$\pm$0.12, and 95.59$\pm$1.66 in MIMIC-IV, PTB-XL and PTB dataset respectively outperforming PatchTST, Reformer, Transformer, and Medformer models. It also demonstrates strong cross-dataset generalization, achieving 49.18% AUROC on PTB and 68.41% on PTB-XL when trained on MIMIC-IV. These findings underscore the potential of Cardioformer to advance automated ECG analysis, paving the way for more accurate and robust cardiovascular disease diagnosis. We release the source code at https://github.com/KMobin555/Cardioformer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Griffin: Towards a Graph-Centric Relational Database Foundation Model</title>
<link>https://arxiv.org/abs/2505.05568</link>
<guid>https://arxiv.org/abs/2505.05568</guid>
<content:encoded><![CDATA[
<div> Keywords: Griffin, relational databases, foundation model, pretraining, RDB tasks 

Summary: 
Griffin is introduced as a foundation model specifically designed for Relational Databases (RDBs), integrating data encoder and task decoder for diverse tasks. It incorporates cross-attention modules and a novel aggregator to enhance its architecture. Griffin undergoes pretraining on single-table and RDB datasets, utilizing advanced encoders for categorical, numerical, and metadata features. The model also incorporates cross-attention modules and enhanced message-passing neural networks (MPNNs) to capture relational data complexities effectively. Evaluated on large-scale, heterogeneous, and temporal RDB graphs comprising over 150 million nodes, Griffin demonstrates superior or comparable performance to individually trained models. It excels in low-data scenarios and exhibits strong transferability, showcasing potential as a universally applicable foundation model for RDBs.Code for Griffin is available at https://github.com/yanxwb/Griffin. 

Summary: <div>
arXiv:2505.05568v1 Announce Type: new 
Abstract: We introduce Griffin, the first foundation model attemptation designed specifically for Relational Databases (RDBs). Unlike previous smaller models focused on single RDB tasks, Griffin unifies the data encoder and task decoder to handle diverse tasks. Additionally, we enhance the architecture by incorporating a cross-attention module and a novel aggregator. Griffin utilizes pretraining on both single-table and RDB datasets, employing advanced encoders for categorical, numerical, and metadata features, along with innovative components such as cross-attention modules and enhanced message-passing neural networks (MPNNs) to capture the complexities of relational data. Evaluated on large-scale, heterogeneous, and temporal graphs extracted from RDBs across various domains (spanning over 150 million nodes), Griffin demonstrates superior or comparable performance to individually trained models, excels in low-data scenarios, and shows strong transferability with similarity and diversity in pretraining across new datasets and tasks, highlighting its potential as a universally applicable foundation model for RDBs. Code available at https://github.com/yanxwb/Griffin.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models</title>
<link>https://arxiv.org/abs/2505.05577</link>
<guid>https://arxiv.org/abs/2505.05577</guid>
<content:encoded><![CDATA[
<div> machine-learning, multimodal biological data, PyTDC, drug-target nomination, single-cell

Summary:
PyTDC is introduced as an open-source machine-learning platform designed for training, evaluation, and inference of models integrating multimodal biological data. It aims to address the lack of infrastructure in existing biomedical benchmarks for diverse machine learning tasks in therapeutics. The architecture of PyTDC is discussed, highlighting its ability to unify distributed data sources and facilitate benchmarking. A case study on single-cell drug-target nomination task is presented, where traditional graph representation and domain-specific methods underperform. A context-aware geometric deep learning model shows promising results but struggles with generalization to unseen cell types and additional modalities. This emphasizes the importance of developing context-aware foundation models using PyTDC for advancing research in biomedical AI. <br /><br />Summary: <div>
arXiv:2505.05577v1 Announce Type: new 
Abstract: Existing biomedical benchmarks do not provide end-to-end infrastructure for training, evaluation, and inference of models that integrate multimodal biological data and a broad range of machine learning tasks in therapeutics. We present PyTDC, an open-source machine-learning platform providing streamlined training, evaluation, and inference software for multimodal biological AI models. PyTDC unifies distributed, heterogeneous, continuously updated data sources and model weights and standardizes benchmarking and inference endpoints. This paper discusses the components of PyTDC's architecture and, to our knowledge, the first-of-its-kind case study on the introduced single-cell drug-target nomination ML task. We find state-of-the-art methods in graph representation learning and domain-specific methods from graph theory perform poorly on this task. Though we find a context-aware geometric deep learning method that outperforms the evaluated SoTA and domain-specific baseline methods, the model is unable to generalize to unseen cell types or incorporate additional modalities, highlighting PyTDC's capacity to facilitate an exciting avenue of research developing multimodal, context-aware, foundation models for open problems in biomedical AI.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anticipating Gaming to Incentivize Improvement: Guiding Agents in (Fair) Strategic Classification</title>
<link>https://arxiv.org/abs/2505.05594</link>
<guid>https://arxiv.org/abs/2505.05594</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, strategic behavior, manipulation, fairness implications, Stackelberg game 

Summary: 
The study focuses on human strategic behavior in response to machine learning algorithms that influence critical decision-making processes. It explores the choice between improving qualifications and manipulating features to deceive the algorithm. The interactions are framed as a Stackelberg game, where a firm deploys a fair classifier and individuals strategically respond to it. The model considers the costs and efficacy of manipulation and improvement. The analysis identifies different classes of agent responses and optimal classifier strategies based on them. Anticipating strategic behavior can help prevent manipulation and incentivize agents to choose improvement. The study sheds light on how algorithm designers can shape strategic responses and its fairness implications. <div>
arXiv:2505.05594v1 Announce Type: new 
Abstract: As machine learning algorithms increasingly influence critical decision making in different application areas, understanding human strategic behavior in response to these systems becomes vital. We explore individuals' choice between genuinely improving their qualifications (``improvement'') vs. attempting to deceive the algorithm by manipulating their features (``manipulation'') in response to an algorithmic decision system. We further investigate an algorithm designer's ability to shape these strategic responses, and its fairness implications. Specifically, we formulate these interactions as a Stackelberg game, where a firm deploys a (fair) classifier, and individuals strategically respond. Our model incorporates both different costs and stochastic efficacy for manipulation and improvement. The analysis reveals different potential classes of agent responses, and characterizes optimal classifiers accordingly. Based on these, we highlight the impact of the firm's anticipation of strategic behavior, identifying when and why a (fair) strategic policy can not only prevent manipulation, but also incentivize agents to opt for improvement.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>This part looks alike this: identifying important parts of explained instances and prototypes</title>
<link>https://arxiv.org/abs/2505.05597</link>
<guid>https://arxiv.org/abs/2505.05597</guid>
<content:encoded><![CDATA[
<div> Keywords: prototype-based explanations, feature importance, alike parts, model predictions, global prototypes diversity

Summary: 
The article introduces a novel approach to improve prototype-based explanations by identifying the most informative features, termed alike parts. These alike parts emphasize the most relevant overlapping features between an instance and its nearest prototype, enhancing user comprehension. By incorporating feature importance scores into prototype selection algorithms, the proposed approach promotes global prototypes diversity, leading to more diverse and informative prototypes. Experimental results on six benchmark datasets show that this approach enhances user understanding while maintaining, and in some cases even increasing, predictive accuracy. The method aims to address the limitations of existing prototype-based explanations by directing user attention to the most relevant features, making model predictions more interpretable and transparent. <br /><br />Summary: <div>
arXiv:2505.05597v1 Announce Type: new 
Abstract: Although prototype-based explanations provide a human-understandable way of representing model predictions they often fail to direct user attention to the most relevant features. We propose a novel approach to identify the most informative features within prototypes, termed alike parts. Using feature importance scores derived from an agnostic explanation method, it emphasizes the most relevant overlapping features between an instance and its nearest prototype. Furthermore, the feature importance score is incorporated into the objective function of the prototype selection algorithms to promote global prototypes diversity. Through experiments on six benchmark datasets, we demonstrate that the proposed approach improves user comprehension while maintaining or even increasing predictive accuracy.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion</title>
<link>https://arxiv.org/abs/2505.05605</link>
<guid>https://arxiv.org/abs/2505.05605</guid>
<content:encoded><![CDATA[
<div> Deep learning, conversion prediction, online advertising, embedding tables, multi-epoch overfitting<br />
Summary:<br />
- Deep learning models for conversion prediction in online advertising have become more complex, predicting multiple objectives like click, add-to-cart, and checkout.
- The use of embedding tables for high cardinality categorical features like advertiser IDs can improve model performance, but training them faces challenges like gradient sparsity and high label sparsity.
- The paper explores techniques to address multi-epoch overfitting in these models, such as feature hashing, ID filtering, and embedding table regularization.
- The authors introduce a Sparse Optimizer that accelerates convergence in training and propose a frequency-adaptive learning rate approach to mitigate multi-epoch overfitting.
- Comparison between the frequency-adaptive learning rate and embedding re-initialization methods is done using an industrial large-scale production dataset.<br /> 
Summary: <div>
arXiv:2505.05605v1 Announce Type: new 
Abstract: Deep learning for conversion prediction has found widespread applications in online advertising. These models have become more complex as they are trained to jointly predict multiple objectives such as click, add-to-cart, checkout and other conversion types. Additionally, the capacity and performance of these models can often be increased with the use of embedding tables that encode high cardinality categorical features such as advertiser, user, campaign, and product identifiers (IDs). These embedding tables can be pre-trained, but also learned end-to-end jointly with the model to directly optimize the model objectives. Training these large tables is challenging due to: gradient sparsity, the high cardinality of the categorical features, the non-uniform distribution of IDs and the very high label sparsity. These issues make training prone to both slow convergence and overfitting after the first epoch. Previous works addressed the multi-epoch overfitting issue by using: stronger feature hashing to reduce cardinality, filtering of low frequency IDs, regularization of the embedding tables, re-initialization of the embedding tables after each epoch, etc. Some of these techniques reduce overfitting at the expense of reduced model performance if used too aggressively. In this paper, we share key learnings from the development of embedding table optimization and multi-epoch training in Pinterest Ads Conversion models. We showcase how our Sparse Optimizer speeds up convergence, and how multi-epoch overfitting varies in severity between different objectives in a multi-task model depending on label sparsity. We propose a new approach to deal with multi-epoch overfitting: the use of a frequency-adaptive learning rate on the embedding tables and compare it to embedding re-initialization. We evaluate both methods offline using an industrial large-scale production dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Corruption-Robustness in Performative Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.05609</link>
<guid>https://arxiv.org/abs/2505.05609</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, performative RL, repeated retraining, convex-concave optimization, corrupted data<br />
<br />
Summary:<br />
The paper focuses on performative Reinforcement Learning (RL) in policy-dependent environments. It extends repeated retraining approaches to handle corrupted data using Huber's $\epsilon$-contamination model. The approach involves solving for a saddle point of a convex-concave objective and utilizing a robust mean estimator for gradients. The method guarantees last-iterate convergence to an approximately stable policy, with the approximation error proportional to $\sqrt{\epsilon}$. Experimental results underline the significance of addressing corruption in performative RL tasks. <div>
arXiv:2505.05609v1 Announce Type: new 
Abstract: In performative Reinforcement Learning (RL), an agent faces a policy-dependent environment: the reward and transition functions depend on the agent's policy. Prior work on performative RL has studied the convergence of repeated retraining approaches to a performatively stable policy. In the finite sample regime, these approaches repeatedly solve for a saddle point of a convex-concave objective, which estimates the Lagrangian of a regularized version of the reinforcement learning problem. In this paper, we aim to extend such repeated retraining approaches, enabling them to operate under corrupted data. More specifically, we consider Huber's $\epsilon$-contamination model, where an $\epsilon$ fraction of data points is corrupted by arbitrary adversarial noise. We propose a repeated retraining approach based on convex-concave optimization under corrupted gradients and a novel problem-specific robust mean estimator for the gradients. We prove that our approach exhibits last-iterate convergence to an approximately stable policy, with the approximation error linear in $\sqrt{\epsilon}$. We experimentally demonstrate the importance of accounting for corruption in performative RL.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation</title>
<link>https://arxiv.org/abs/2505.05625</link>
<guid>https://arxiv.org/abs/2505.05625</guid>
<content:encoded><![CDATA[
<div> latent neural ODE, Chemical Reaction Neural Network, rate coefficient, chemical reaction modelling, real-world datasets  
Summary:
The paper introduces a framework called SPIN-ODE for estimating rate constants in complex chemical reactions. The approach involves a three-stage optimization process: first, a latent neural ODE learns the trajectory between chemical concentrations and their derivatives; second, a Chemical Reaction Neural Network (CRNN) extracts rate coefficients based on learned dynamics; and finally, the CRNN is fine-tuned using a neural ODE solver for improved estimation. The method is demonstrated to be effective and robust through experiments on synthetic and real-world datasets. This work is the first to explore stiff Neural ODEs for discovering chemical rate coefficients and presents a promising direction for integrating neural networks into detailed chemistry.<br /><br />Summary: <div>
arXiv:2505.05625v1 Announce Type: new 
Abstract: Estimating rate constants from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence that hinder effective rate constant estimation using learning-based approaches. To address this, we propose a Stiff Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction modelling. Our method introduces a three-stage optimisation process: first, a latent neural ODE learns the continuous and differentiable trajectory between chemical concentrations and their time derivatives; second, an explicit Chemical Reaction Neural Network (CRNN) extracts the underlying rate coefficients based on the learned dynamics; and third, fine-tune CRNN using a neural ODE solver to further improve rate coefficient estimation. Extensive experiments on both synthetic and newly proposed real-world datasets validate the effectiveness and robustness of our approach. As the first work on stiff Neural ODEs for chemical rate coefficient discovery, our study opens promising directions for integrating neural networks with detailed chemistry.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EquiHGNN: Scalable Rotationally Equivariant Hypergraph Neural Networks</title>
<link>https://arxiv.org/abs/2505.05650</link>
<guid>https://arxiv.org/abs/2505.05650</guid>
<content:encoded><![CDATA[
<div> Keywords: Molecular interactions, Equivariant HyperGraph Neural Network, Symmetry constraints, Geometric features, Molecular learning<br />
<br />
Summary: 
The EquiHGNN framework introduces a novel approach to molecular modeling by leveraging high-order interactions through hypergraphs. By incorporating symmetry-aware representations, EquiHGNN preserves geometric and topological properties, enhancing the robustness and physical interpretability of molecular models. Experimental results showcase the superiority of high-order interactions over traditional graph-based models, particularly on large molecules where geometric features play a crucial role. The integration of symmetry constraints leads to significant performance improvements on large-scale molecular datasets, highlighting the value of symmetry-aware learning in molecular modeling. EquiHGNN demonstrates the benefits of incorporating multi-way interactions and spatial information in molecular learning, offering a promising avenue for advancing the field of molecular representation and prediction. The source code is publicly available for further research and development. <br /><br />Summary: <div>
arXiv:2505.05650v1 Announce Type: new 
Abstract: Molecular interactions often involve high-order relationships that cannot be fully captured by traditional graph-based models limited to pairwise connections. Hypergraphs naturally extend graphs by enabling multi-way interactions, making them well-suited for modeling complex molecular systems. In this work, we introduce EquiHGNN, an Equivariant HyperGraph Neural Network framework that integrates symmetry-aware representations to improve molecular modeling. By enforcing the equivariance under relevant transformation groups, our approach preserves geometric and topological properties, leading to more robust and physically meaningful representations. We examine a range of equivariant architectures and demonstrate that integrating symmetry constraints leads to notable performance gains on large-scale molecular datasets. Experiments on both small and large molecules show that high-order interactions offer limited benefits for small molecules but consistently outperform 2D graphs on larger ones. Adding geometric features to these high-order structures further improves the performance, emphasizing the value of spatial information in molecular learning. Our source code is available at https://github.com/HySonLab/EquiHGNN/
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Front-door Adjustment for Heterogeneous Treatment Assignment Effect Estimation Under Non-adherence</title>
<link>https://arxiv.org/abs/2505.05677</link>
<guid>https://arxiv.org/abs/2505.05677</guid>
<content:encoded><![CDATA[
<div> treatment assignment effects, non-adherence, backdoor adjustment, front-door adjustment, LobsterNet<br />
<br />
Summary:<br />
Estimates of treatment assignment effects are crucial for informing treatment decisions, especially in the presence of non-adherence. This study compares the standard backdoor adjustment (SBD) and conditional front-door adjustment (CFD) methods for estimating treatment effects. The research shows that CFD yields lower-variance estimates than SBD when the true treatment effect is small. To address the challenge of estimating multiple nuisance parameters in CFD, the study introduces LobsterNet, a multi-task neural network that implements CFD with joint modeling of nuisance parameters. Empirical results demonstrate that LobsterNet outperforms baseline methods across various datasets, reducing estimation error in treatment assignment effect estimation under non-adherence. Overall, the findings highlight the potential of CFD with shared nuisance parameter modeling to improve the accuracy of treatment effect estimation in healthcare settings. <br /><br /> <div>
arXiv:2505.05677v1 Announce Type: new 
Abstract: Estimates of heterogeneous treatment assignment effects can inform treatment decisions. Under the presence of non-adherence (e.g., patients do not adhere to their assigned treatment), both the standard backdoor adjustment (SBD) and the conditional front-door adjustment (CFD) can recover unbiased estimates of the treatment assignment effects. However, the estimation variance of these approaches may vary widely across settings, which remains underexplored in the literature. In this work, we demonstrate theoretically and empirically that CFD yields lower-variance estimates than SBD when the true effect of treatment assignment is small (i.e., assigning an intervention leads to small changes in patients' future outcome). Additionally, since CFD requires estimating multiple nuisance parameters, we introduce LobsterNet, a multi-task neural network that implements CFD with joint modeling of the nuisance parameters. Empirically, LobsterNet reduces estimation error across several semi-synthetic and real-world datasets compared to baselines. Our findings suggest CFD with shared nuisance parameter modeling can improve treatment assignment effect estimation under non-adherence.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A Dash-Based Approach with SHAP, LIME, and Comorbidity Insights</title>
<link>https://arxiv.org/abs/2505.05683</link>
<guid>https://arxiv.org/abs/2505.05683</guid>
<content:encoded><![CDATA[
<div> Machine learning models, diabetes risk assessment, Interactive health risk prediction tool, LightGBM, Undersampling

Summary:
This study introduces a web-based tool for predicting diabetes risk using various machine learning models and strategies on the 2015 CDC BRFSS dataset. Models such as Logistic Regression, Random Forest, XGBoost, KNN, and Neural Networks were evaluated, with LightGBM employing undersampling showing the highest recall rate for effective risk detection. The tool incorporates SHAP and LIME for explaining predictions and utilizes Pearson analysis to highlight correlations with comorbidities. Through a Dash-based user interface, the tool provides an interactive platform for users to access personalized suggestions, feature insights, and model predictions, promoting data-driven health awareness and facilitating informed decision-making for individuals concerned about their diabetes risk. <br /><br />Summary: <div>
arXiv:2505.05683v1 Announce Type: new 
Abstract: This study presents a web-based interactive health risk prediction tool designed to assess diabetes risk using machine learning models. Built on the 2015 CDC BRFSS dataset, the study evaluates models including Logistic Regression, Random Forest, XGBoost, LightGBM, KNN, and Neural Networks under original, SMOTE, and undersampling strategies. LightGBM with undersampling achieved the best recall, making it ideal for risk detection. The tool integrates SHAP and LIME to explain predictions and highlights comorbidity correlations using Pearson analysis. A Dash-based UI enables user-friendly interaction with model predictions, personalized suggestions, and feature insights, supporting data-driven health awareness.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning</title>
<link>https://arxiv.org/abs/2505.05702</link>
<guid>https://arxiv.org/abs/2505.05702</guid>
<content:encoded><![CDATA[
<div> sheaf Laplacians, hypergraphs, symmetric simplicial sets, neural sheaf diffusion, experimental evaluation
Summary:
Symmetric simplicial sets are introduced to address the challenges in constructing sheaf Laplacians for hypergraphs by encoding oriented subrelations within hyperedges. The normalized degree zero sheaf Laplacian on symmetric simplicial sets is mathematically consistent with graph sheaf theory and retains all structural information from the original hypergraph. Hypergraph Neural Sheaf Diffusion (HNSD) extends Neural Sheaf Diffusion (NSD) to hypergraphs through normalized sheaf Laplacians, resolving orientation ambiguity and adjacency sparsity in hypergraph learning. Experimental evaluations demonstrate the competitive performance of HNSD across established benchmarks. <div>
arXiv:2505.05702v1 Announce Type: new 
Abstract: The absence of intrinsic adjacency relations and orientation systems in hypergraphs creates fundamental challenges for constructing sheaf Laplacians of arbitrary degrees. We resolve these limitations through symmetric simplicial sets derived directly from hypergraphs, which encode all possible oriented subrelations within each hyperedge as ordered tuples. This construction canonically defines adjacency via facet maps while inherently preserving hyperedge provenance. We establish that the normalized degree zero sheaf Laplacian on our induced symmetric simplicial set reduces exactly to the traditional graph normalized sheaf Laplacian when restricted to graphs, validating its mathematical consistency with prior graph-based sheaf theory. Furthermore, the induced structure preserves all structural information from the original hypergraph, ensuring that every multi-way relational detail is faithfully retained. Leveraging this framework, we introduce Hypergraph Neural Sheaf Diffusion (HNSD), the first principled extension of Neural Sheaf Diffusion (NSD) to hypergraphs. HNSD operates via normalized degree zero sheaf Laplacians over symmetric simplicial sets, resolving orientation ambiguity and adjacency sparsity inherent to hypergraph learning. Experimental evaluations demonstrate HNSD's competitive performance across established benchmarks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowding Out The Noise: Algorithmic Collective Action Under Differential Privacy</title>
<link>https://arxiv.org/abs/2505.05707</link>
<guid>https://arxiv.org/abs/2505.05707</guid>
<content:encoded><![CDATA[
<div> privacy, AI, algorithmic collective action, trustworthy, data protection

Summary: 
This paper explores the intersection of AI integration into daily life and concerns regarding algorithmic harms and social inequities. It discusses the concept of Algorithmic Collective Action as a way for individuals to influence AI behavior by modifying the data they share with platforms. The focus is on differential privacy in AI models, which aims to protect individual data but presents challenges for collective action. The study investigates the impact of Differentially Private Stochastic Gradient Descent (DPSGD) on the ability of collectives to influence the learning process. Lower bounds on the success of algorithmic collective action under differential privacy are characterized, showing trends based on collective size and privacy parameters. Experimental simulations confirm these findings across various datasets. <div>
arXiv:2505.05707v1 Announce Type: new 
Abstract: The integration of AI into daily life has generated considerable attention and excitement, while also raising concerns about automating algorithmic harms and re-entrenching existing social inequities. While the responsible deployment of trustworthy AI systems is a worthy goal, there are many possible ways to realize it, from policy and regulation to improved algorithm design and evaluation. In fact, since AI trains on social data, there is even a possibility for everyday users, citizens, or workers to directly steer its behavior through Algorithmic Collective Action, by deliberately modifying the data they share with a platform to drive its learning process in their favor. This paper considers how these grassroots efforts to influence AI interact with methods already used by AI firms and governments to improve model trustworthiness. In particular, we focus on the setting where the AI firm deploys a differentially private model, motivated by the growing regulatory focus on privacy and data protection. We investigate how the use of Differentially Private Stochastic Gradient Descent (DPSGD) affects the collective's ability to influence the learning process. Our findings show that while differential privacy contributes to the protection of individual data, it introduces challenges for effective algorithmic collective action. We characterize lower bounds on the success of algorithmic collective action under differential privacy as a function of the collective's size and the firm's privacy parameters, and verify these trends experimentally by simulating collective action during the training of deep neural network classifiers across several datasets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Learning of Semantic Embedding Representations for Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05732</link>
<guid>https://arxiv.org/abs/2505.05732</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative models, Denoising diffusion models, Autoencoder, Diffusion Transformers, Representation learning 

Summary:
This article introduces a method to enhance the representation capacity of denoising diffusion models (DDMs) using a multi-level denoising autoencoder framework. The framework includes Diffusion Transformers and a timestep-dependent encoder to learn embedding representations. The encoder compresses high-dimensional data into directional vectors, aiding in learning image embeddings across all timesteps. Experiment results show that the embeddings generated by DDMs outperform state-of-the-art self-supervised methods, demonstrating high-quality discriminative semantic representations. This work suggests that DDMs are effective not only for generative tasks but also for general deep learning applications. <div>
arXiv:2505.05732v1 Announce Type: new 
Abstract: Generative models capture the true distribution of data, yielding semantically rich representations. Denoising diffusion models (DDMs) exhibit superior generative capabilities, though efficient representation learning for them are lacking. In this work, we employ a multi-level denoising autoencoder framework to expand the representation capacity of DDMs, which introduces sequentially consistent Diffusion Transformers and an additional timestep-dependent encoder to acquire embedding representations on the denoising Markov chain through self-conditional diffusion learning. Intuitively, the encoder, conditioned on the entire diffusion process, compresses high-dimensional data into directional vectors in latent under different noise levels, facilitating the learning of image embeddings across all timesteps. To verify the semantic adequacy of embeddings generated through this approach, extensive experiments are conducted on various datasets, demonstrating that optimally learned embeddings by DDMs surpass state-of-the-art self-supervised representation learning methods in most cases, achieving remarkable discriminative semantic representation quality. Our work justifies that DDMs are not only suitable for generative tasks, but also potentially advantageous for general-purpose deep learning applications.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering</title>
<link>https://arxiv.org/abs/2505.05738</link>
<guid>https://arxiv.org/abs/2505.05738</guid>
<content:encoded><![CDATA[
<div> clustering, multivariate time series forecasting, long-range dependencies, prototypes, FOCUS

Summary: 
The paper introduces FOCUS, a novel approach to multivariate time series forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes capture high-level events in the underlying real-world system and summarize key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input, capturing dependencies between the input segment and high-level events for accurate and efficient forecasting. By identifying prototypes during offline clustering, FOCUS reduces the computational complexity of modeling long-range dependencies during the online phase to linear scaling. Extensive experiments across diverse benchmarks show that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs. 

Summary: <div>
arXiv:2505.05738v1 Announce Type: new 
Abstract: Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep-ICE: The first globally optimal algorithm for empirical risk minimization of two-layer maxout and ReLU networks</title>
<link>https://arxiv.org/abs/2505.05740</link>
<guid>https://arxiv.org/abs/2505.05740</guid>
<content:encoded><![CDATA[
<div> Algorithm, Empirical Risk Minimization, Two-layer networks, Coreset selection method, Misclassifications

Summary:
The paper introduces a globally optimal algorithm for minimizing misclassifications in two-layer maxout and ReLU networks. The algorithm has a worst-case time complexity of $O\left(N^{DK+1}\right)$ and can handle arbitrary loss functions. Experiments show exact solutions for small datasets and a novel coreset selection method reduces data size for larger datasets. This allows efficient processing of large-scale datasets with a 20-30% reduction in misclassifications compared to neural networks trained with gradient descent and support vector machines. The approach applies to two-layer networks with fixed hidden nodes and linear models, proving its superiority in achieving improved performance for both training and prediction. <br /><br />Summary: <div>
arXiv:2505.05740v1 Announce Type: new 
Abstract: This paper introduces the first globally optimal algorithm for the empirical risk minimization problem of two-layer maxout and ReLU networks, i.e., minimizing the number of misclassifications. The algorithm has a worst-case time complexity of $O\left(N^{DK+1}\right)$, where $K$ denotes the number of hidden neurons and $D$ represents the number of features. It can be can be generalized to accommodate arbitrary computable loss functions without affecting its computational complexity. Our experiments demonstrate that the proposed algorithm provides provably exact solutions for small-scale datasets. To handle larger datasets, we introduce a novel coreset selection method that reduces the data size to a manageable scale, making it feasible for our algorithm. This extension enables efficient processing of large-scale datasets and achieves significantly improved performance, with a 20-30\% reduction in misclassifications for both training and prediction, compared to state-of-the-art approaches (neural networks trained using gradient descent and support vector machines), when applied to the same models (two-layer networks with fixed hidden nodes and linear models).
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification</title>
<link>https://arxiv.org/abs/2505.05744</link>
<guid>https://arxiv.org/abs/2505.05744</guid>
<content:encoded><![CDATA[
<div> explanations, large language models, tabular prediction, interpretable outputs, in-context learning

Summary: 
The article introduces a novel in-context learning framework for tabular prediction using Large Language Models (LLMs) and Surrogate Language Models (SLMs). This framework addresses issues such as high resource requirements, suboptimal demonstration selection, and limited interpretability in existing LLM-based methods. It comprises three stages: Post Hoc Explanation Generation, Post Hoc Explanation-Guided Demonstrations Selection, and Post Hoc Explanation-Guided Interpretable SLM Prediction. LLMs generate explanations for question-answer pairs in candidate demonstrations to provide insights into the reasoning behind answers. These explanations guide the selection of demonstrations and enhance the performance of the SLM by merging explanations as rationales. Experimental results demonstrate an average accuracy improvement of 5.31% across various tabular datasets in diverse domains. The framework effectively leverages LLM explanations to improve the interpretability and prediction performance of tabular data. 

<br /><br />Summary: <div>
arXiv:2505.05744v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable ability in solving complex tasks, making them a promising tool for enhancing tabular learning. However, existing LLM-based methods suffer from high resource requirements, suboptimal demonstration selection, and limited interpretability, which largely hinder their prediction performance and application in the real world. To overcome these problems, we propose a novel in-context learning framework for tabular prediction. The core idea is to leverage the explanations generated by LLMs to guide a smaller, locally deployable Surrogate Language Model (SLM) to make interpretable tabular predictions. Specifically, our framework mainly involves three stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to generate explanations for question-answer pairs in candidate demonstrations, providing insights into the reasoning behind the answer. (ii) Post Hoc Explanation-Guided Demonstrations Selection, which utilizes explanations generated by LLMs to guide the process of demonstration selection from candidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM Prediction, which utilizes the demonstrations obtained in step (ii) as in-context and merges corresponding explanations as rationales to improve the performance of SLM and guide the model to generate interpretable outputs. Experimental results highlight the framework's effectiveness, with an average accuracy improvement of 5.31% across various tabular datasets in diverse domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection</title>
<link>https://arxiv.org/abs/2505.05763</link>
<guid>https://arxiv.org/abs/2505.05763</guid>
<content:encoded><![CDATA[
<div> Keywords: biomedical research, academic misconduct detection, multimodal deep learning, journal metadata, textual anomalies<br />
<br />
Summary: 
The article introduces BMMDetect, a new multimodal deep learning framework designed to detect academic misconduct in biomedical research. By integrating journal metadata, semantic embeddings, and GPT-4o-mined textual attributes, BMMDetect offers a holistic approach to evaluating manuscripts. Key innovations include the fusion of domain-specific features to reduce bias, the identification of dominant predictors such as journal authority metrics and textual anomalies, and the development of the BioMCD dataset for benchmarking. BMMDetect achieves a high AUC of 74.33%, outperforming single-modality baselines by 8.6%, and shows transferability across different biomedical subfields. This work represents a significant advancement in scalable and interpretable tools for safeguarding the integrity of research in the biomedical field. 
<br /><br />Summary: <div>
arXiv:2505.05763v1 Announce Type: new 
Abstract: Academic misconduct detection in biomedical research remains challenging due to algorithmic narrowness in existing methods and fragmented analytical pipelines. We present BMMDetect, a multimodal deep learning framework that integrates journal metadata (SJR, institutional data), semantic embeddings (PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics, data anomalies) for holistic manuscript evaluation. Key innovations include: (1) multimodal fusion of domain-specific features to reduce detection bias; (2) quantitative evaluation of feature importance, identifying journal authority metrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as dominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with 13,160 retracted articles and 53,411 controls. BMMDetect achieves 74.33% AUC, outperforming single-modality baselines by 8.6%, and demonstrates transferability across biomedical subfields. This work advances scalable, interpretable tools for safeguarding research integrity.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective</title>
<link>https://arxiv.org/abs/2505.05785</link>
<guid>https://arxiv.org/abs/2505.05785</guid>
<content:encoded><![CDATA[
<div> random walk, graph neural networks, out-of-distribution generalization, kernel density estimation, mutual information loss

Summary:
The paper introduces LRW-OOD, a new approach for improving out-of-distribution (OOD) generalization in graph neural networks. It proposes the learnable random walk (LRW) perspective as a way to capture invariant knowledge across datasets. By parameterizing the transition matrix with an LRW sampler and a path encoder, LRW-OOD generates random walk sequences that follow OOD principles. The model incorporates a kernel density estimation (KDE)-based mutual information (MI) loss to ensure the generated random walks adhere to OOD principles. Experimental results show that LRW-OOD significantly enhances graph OOD generalization under various distribution shifts, outperforming state-of-the-art graph OOD generalization methods by an accuracy improvement of 3.87%. <div>
arXiv:2505.05785v1 Announce Type: new 
Abstract: Out-Of-Distribution (OOD) generalization has gained increasing attentions for machine learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation under distribution shifts. Existing graph OOD methods tend to follow the basic ideas of invariant risk minimization and structural causal models, interpreting the invariant knowledge across datasets under various distribution shifts as graph topology or graph spectrum. However, these interpretations may be inconsistent with real-world scenarios, as neither invariant topology nor spectrum is assured. In this paper, we advocate the learnable random walk (LRW) perspective as the instantiation of invariant knowledge, and propose LRW-OOD to realize graph OOD generalization learning. Instead of employing fixed probability transition matrix (i.e., degree-normalized adjacency matrix), we parameterize the transition matrix with an LRW-sampler and a path encoder. Furthermore, we propose the kernel density estimation (KDE)-based mutual information (MI) loss to generate random walk sequences that adhere to OOD principles. Extensive experiment demonstrates that our model can effectively enhance graph OOD generalization under various types of distribution shifts and yield a significant accuracy improvement of 3.87% over state-of-the-art graph OOD generalization baselines.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes</title>
<link>https://arxiv.org/abs/2505.05798</link>
<guid>https://arxiv.org/abs/2505.05798</guid>
<content:encoded><![CDATA[
<div> Kolmogorov-Arnold Networks, KAN, universal function approximation, univariate spline compositions, Error-Correcting Output Codes, ECOC integration, multi-class classification, Hamming-distance decoding, blood cell classification dataset, higher accuracy, robustness improvement, healthcare AI applications<br />
<br />
Summary:<br />
This work introduces a novel integration of Error-Correcting Output Codes (ECOC) into the Kolmogorov-Arnold Networks (KAN) framework for multi-class classification tasks. By transforming the problem into multiple binary tasks and utilizing Hamming-distance decoding, the proposed KAN with ECOC method demonstrates superior performance compared to the standard KAN model. Specifically, experiments conducted on a challenging blood cell classification dataset showcase higher accuracy and improved robustness under various hyperparameter settings. Ablation studies further confirm the consistent enhancement of performance across different KAN variants, highlighting the effectiveness of ECOC integration in boosting generalizability in critical healthcare AI applications. This study represents the first integration of ECOC with KAN for enhancing multi-class medical image classification performance. <div>
arXiv:2505.05798v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using univariate spline compositions without nonlinear activations. In this work, we integrate Error-Correcting Output Codes (ECOC) into the KAN framework to transform multi-class classification into multiple binary tasks, improving robustness via Hamming-distance decoding. Our proposed KAN with ECOC method outperforms vanilla KAN on a challenging blood cell classification dataset, achieving higher accuracy under diverse hyperparameter settings. Ablation studies further confirm that ECOC consistently enhances performance across FastKAN and FasterKAN variants. These results demonstrate that ECOC integration significantly boosts KAN generalizability in critical healthcare AI applications. To the best of our knowledge, this is the first integration of ECOC with KAN for enhancing multi-class medical image classification performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design</title>
<link>https://arxiv.org/abs/2505.05799</link>
<guid>https://arxiv.org/abs/2505.05799</guid>
<content:encoded><![CDATA[
<div> quantization, MxMoE, mixed-precision optimization, MoE models, GroupGEMM kernels  
Summary:  
Mixture-of-Experts (MoE) models face challenges in deployment due to their large parameter counts and computational demands. The research explores quantization for MoE models, highlighting the varying sensitivity of linear blocks and the heterogeneous computational characteristics caused by divergent expert activation frequencies. MxMoE, a mixed-precision optimization framework, considers algorithmic and system perspectives to derive efficient configurations. It navigates the design space effectively by optimizing mixed-precision GroupGEMM kernels for parallel execution of GEMMs with different precisions. Evaluations demonstrate superior performance compared to existing methods, achieving lower perplexity and delivering significant speedups over full precision and uniform quantization at equivalent accuracy. The code for MxMoE is available on GitHub for implementation. <br /><br />Summary: <div>
arXiv:2505.05799v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models face deployment challenges due to their large parameter counts and computational demands. We explore quantization for MoE models and highlight two key insights: 1) linear blocks exhibit varying quantization sensitivity, and 2) divergent expert activation frequencies create heterogeneous computational characteristics. Based on these observations, we introduce MxMoE, a mixed-precision optimization framework for MoE models that considers both algorithmic and system perspectives. MxMoE navigates the design space defined by parameter sensitivity, expert activation dynamics, and hardware resources to derive efficient mixed-precision configurations. Additionally, MxMoE automatically generates optimized mixed-precision GroupGEMM kernels, enabling parallel execution of GEMMs with different precisions. Evaluations show that MxMoE outperforms existing methods, achieving 2.4 lower Wikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup over full precision, as well as up to 29.4% speedup over uniform quantization at equivalent accuracy with 5-bit weight-activation quantization. Our code is available at https://github.com/cat538/MxMoE.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel Neural-ODE model for the state of health estimation of lithium-ion battery using charging curve</title>
<link>https://arxiv.org/abs/2505.05803</link>
<guid>https://arxiv.org/abs/2505.05803</guid>
<content:encoded><![CDATA[
<div> Keywords: lithium-ion batteries, state of health estimation, data-driven approach, ACLA model, attention mechanism

Summary:<br />
- The article introduces a new data-driven approach, the ACLA model, for estimating the state of health (SOH) of lithium-ion batteries in electric vehicles.
- The ACLA model integrates the attention mechanism, convolutional neural network (CNN), and long short-term memory network (LSTM) into the augmented neural ordinary differential equation (ANODE) framework.
- Normalized charging time at specific voltages during constant current charging is used as input to predict SOH and remaining useful life.
- The ACLA model is trained on NASA and Oxford datasets and validated on TJU and HUST datasets, demonstrating higher accuracy compared to benchmark models NODE and ANODE.
- Results show that the ACLA model achieves low root mean square errors (RMSE) for SOH estimation, indicating its effectiveness in improving generalization in lithium-ion battery health monitoring systems.

<br /><br />Summary: <div>
arXiv:2505.05803v1 Announce Type: new 
Abstract: The state of health (SOH) of lithium-ion batteries (LIBs) is crucial for ensuring the safe and reliable operation of electric vehicles. Nevertheless, the prevailing SOH estimation methods often have limited generalizability. This paper introduces a data-driven approach for estimating the SOH of LIBs, which is designed to improve generalization. We construct a hybrid model named ACLA, which integrates the attention mechanism, convolutional neural network (CNN), and long short-term memory network (LSTM) into the augmented neural ordinary differential equation (ANODE) framework. This model employs normalized charging time corresponding to specific voltages in the constant current charging phase as input and outputs the SOH as well as remaining useful of life. The model is trained on NASA and Oxford datasets and validated on the TJU and HUST datasets. Compared to the benchmark models NODE and ANODE, ACLA exhibits higher accuracy with root mean square errors (RMSE) for SOH estimation as low as 1.01% and 2.24% on the TJU and HUST datasets, respectively.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BCE vs. CE in Deep Feature Learning</title>
<link>https://arxiv.org/abs/2505.05813</link>
<guid>https://arxiv.org/abs/2505.05813</guid>
<content:encoded><![CDATA[
<div> Feature Learning, Binary Cross-Entropy, Classification, Neural Collapse, Decision Scores 
<br />
Summary: 
Binary cross-entropy (BCE) and cross-entropy (CE) are compared for deep feature learning in classification models. It is found that both BCE and CE can achieve neural collapse (NC), maximizing intra-class compactness and inter-class distinctiveness. BCE adjusts decision scores uniformly across all samples, enhancing feature properties explicitly through classifier biases. In contrast, CE measures relative decision score values and enhances features implicitly by classifying samples individually. Experimental results support these findings, showing that BCE improves classification performance and enhances compactness and distinctiveness of sample features. The study provides insights into the different ways BCE and CE optimize feature learning in classification models. <div>
arXiv:2505.05813v1 Announce Type: new 
Abstract: When training classification models, it expects that the learned features are compact within classes, and can well separate different classes. As the dominant loss function for training classification models, minimizing cross-entropy (CE) loss maximizes the compactness and distinctiveness, i.e., reaching neural collapse (NC). The recent works show that binary CE (BCE) performs also well in multi-class tasks. In this paper, we compare BCE and CE in deep feature learning. For the first time, we prove that BCE can also maximize the intra-class compactness and inter-class distinctiveness when reaching its minimum, i.e., leading to NC. We point out that CE measures the relative values of decision scores in the model training, implicitly enhancing the feature properties by classifying samples one-by-one. In contrast, BCE measures the absolute values of decision scores and adjust the positive/negative decision scores across all samples to uniformly high/low levels. Meanwhile, the classifier biases in BCE present a substantial constraint on the decision scores to explicitly enhance the feature properties in the training. The experimental results are aligned with above analysis, and show that BCE could improve the classification and leads to better compactness and distinctiveness among sample features. The codes will be released.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Statistical and Computational Results for Learning Junta Distributions</title>
<link>https://arxiv.org/abs/2505.05819</link>
<guid>https://arxiv.org/abs/2505.05819</guid>
<content:encoded><![CDATA[
<div> junta distributions, computational equivalence, learning, k-parity functions, statistical complexity
<br />
<br />
Summary: 
This study delves into the learning of junta distributions on binary sequences, where a distribution is termed a k-junta if its probability function is dependent on a subset of at most k variables. The research uncovers that the endeavor of learning k-junta distributions aligns computationally with the learning of k-parity functions with noise (LPN), a pivotal problem in computational learning theory. Additionally, an algorithm is formulated for learning junta distributions, boasting optimal statistical complexity, nearly matching the computational complexity of prior algorithms. This suggests potential limitations for improvement in the algorithm's performance, both statistically and computationally, unless a breakthrough in solving LPN is achieved. <div>
arXiv:2505.05819v1 Announce Type: new 
Abstract: We study the problem of learning junta distributions on $\{0, 1\}^n$, where a distribution is a $k$-junta if its probability mass function depends on a subset of at most $k$ variables. We make two main contributions:
  - We show that learning $k$-junta distributions is \emph{computationally} equivalent to learning $k$-parity functions with noise (LPN), a landmark problem in computational learning theory.
  - We design an algorithm for learning junta distributions whose statistical complexity is optimal, up to polylogarithmic factors. Computationally, our algorithm matches the complexity of previous (non-sample-optimal) algorithms.
  Combined, our two contributions imply that our algorithm cannot be significantly improved, statistically or computationally, barring a breakthrough for LPN.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed-Integer Optimization for Responsible Machine Learning</title>
<link>https://arxiv.org/abs/2505.05857</link>
<guid>https://arxiv.org/abs/2505.05857</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Learning, responsible ML, fairness, transparency, mixed-integer optimization 

Summary: 
Machine Learning has been successful in various domains but raises concerns around fairness, transparency, robustness, and privacy. The complexity of ML systems calls for responsible ML methods to address these challenges. Mixed-integer optimization (MIO) allows embedding responsible ML considerations into the learning process while maintaining performance. This tutorial paper provides an introduction to responsible ML, discussing theoretical and practical aspects. It highlights the importance of responsible ML in applications and the utility of MIO for building transparent models with domain-specific constraints. Practical strategies and tools for solving MIO problems efficiently are illustrated through examples and mathematical formulations. The paper concludes with a discussion on current limitations and research questions, providing suggestions for future work.

<br /><br />Summary: <div>
arXiv:2505.05857v1 Announce Type: new 
Abstract: In the last few decades, Machine Learning (ML) has achieved significant success across domains ranging from healthcare, sustainability, and the social sciences, to criminal justice and finance. But its deployment in increasingly sophisticated, critical, and sensitive areas affecting individuals, the groups they belong to, and society as a whole raises critical concerns around fairness, transparency, robustness, and privacy, among others. As the complexity and scale of ML systems and of the settings in which they are deployed grow, so does the need for responsible ML methods that address these challenges while providing guaranteed performance in deployment.
  Mixed-integer optimization (MIO) offers a powerful framework for embedding responsible ML considerations directly into the learning process while maintaining performance. For example, it enables learning of inherently transparent models that can conveniently incorporate fairness or other domain specific constraints. This tutorial paper provides an accessible and comprehensive introduction to this topic discussing both theoretical and practical aspects. It outlines some of the core principles of responsible ML, their importance in applications, and the practical utility of MIO for building ML models that align with these principles. Through examples and mathematical formulations, it illustrates practical strategies and available tools for efficiently solving MIO problems for responsible ML. It concludes with a discussion on current limitations and open research questions, providing suggestions for future work.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open Set Label Shift with Test Time Out-of-Distribution Reference</title>
<link>https://arxiv.org/abs/2505.05868</link>
<guid>https://arxiv.org/abs/2505.05868</guid>
<content:encoded><![CDATA[
<div> estimators, label shift, distribution, classifier, open set 

Summary: 
The paper addresses the issue of open set label shift, where the label distributions change between a source and target distribution, with an additional out-of-distribution (OOD) class in the target. The authors propose estimators for both source and target open set label distributions using an in-distribution (ID) classifier and an ID/OOD classifier. These estimators are organized into three stages: estimating the source label distribution of the OOD class, using an EM algorithm for Maximum Likelihood estimates (MLE) for the target label distribution, and estimating the target label distribution of the OOD class. Sampling errors of the estimates are quantified with a concentration inequality. The resulting estimation allows for correcting the ID classifier from the source to the target distribution without retraining. Experimental results across various open set label shift settings validate the effectiveness of the model. <div>
arXiv:2505.05868v1 Announce Type: new 
Abstract: Open set label shift (OSLS) occurs when label distributions change from a source to a target distribution, and the target distribution has an additional out-of-distribution (OOD) class. In this work, we build estimators for both source and target open set label distributions using a source domain in-distribution (ID) classifier and an ID/OOD classifier. With reasonable assumptions on the ID/OOD classifier, the estimators are assembled into a sequence of three stages: 1) an estimate of the source label distribution of the OOD class, 2) an EM algorithm for Maximum Likelihood estimates (MLE) of the target label distribution, and 3) an estimate of the target label distribution of OOD class under relaxed assumptions on the OOD classifier. The sampling errors of estimates in 1) and 3) are quantified with a concentration inequality. The estimation result allows us to correct the ID classifier trained on the source distribution to the target distribution without retraining. Experiments on a variety of open set label shift settings demonstrate the effectiveness of our model. Our code is available at https://github.com/ChangkunYe/OpenSetLabelShift.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Discovery of Partial Differential Equations by Learning from Math Handbooks</title>
<link>https://arxiv.org/abs/2505.05869</link>
<guid>https://arxiv.org/abs/2505.05869</guid>
<content:encoded><![CDATA[
<div> discovery, partial differential equations, data driven, generative model, scientific discovery
<br />
Data driven discovery of partial differential equations is enhanced by incorporating existing PDEs into a generative model, EqGPT. This approach uses a loop of generation, evaluation, and optimization to identify suitable PDEs efficiently. The framework successfully recovers various PDE forms accurately, even in cases with complex temporal derivatives or intricate spatial terms. It shows generalizability to irregular spatial domains and higher dimensional settings, and can uncover previously unreported PDEs from real-world data. The discovery of a new PDE governing strongly nonlinear surface gravity waves demonstrates the approach's practical applicability and potential for supporting scientific advancements.
<br /><br />Summary: <div>
arXiv:2505.05869v1 Announce Type: new 
Abstract: Data driven discovery of partial differential equations (PDEs) is a promising approach for uncovering the underlying laws governing complex systems. However, purely data driven techniques face the dilemma of balancing search space with optimization efficiency. This study introduces a knowledge guided approach that incorporates existing PDEs documented in a mathematical handbook to facilitate the discovery process. These PDEs are encoded as sentence like structures composed of operators and basic terms, and used to train a generative model, called EqGPT, which enables the generation of free form PDEs. A loop of generation evaluation optimization is constructed to autonomously identify the most suitable PDE. Experimental results demonstrate that this framework can recover a variety of PDE forms with high accuracy and computational efficiency, particularly in cases involving complex temporal derivatives or intricate spatial terms, which are often beyond the reach of conventional methods. The approach also exhibits generalizability to irregular spatial domains and higher dimensional settings. Notably, it succeeds in discovering a previously unreported PDE governing strongly nonlinear surface gravity waves propagating toward breaking, based on real world experimental data, highlighting its applicability to practical scenarios and its potential to support scientific discovery.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A 3D pocket-aware and evolutionary conserved interaction guided diffusion model for molecular optimization</title>
<link>https://arxiv.org/abs/2505.05874</link>
<guid>https://arxiv.org/abs/2505.05874</guid>
<content:encoded><![CDATA[
<div> diffusion models, protein-ligand binding interactions, molecule optimization, protein residues, evolutionary conservation<br />
<br />Summary: <br />
The article introduces a new 3D target-aware diffusion model called DiffDecip for molecule optimization in drug design. This model incorporates protein-ligand binding interactions and evolutionary conservation information of protein residues to enhance molecule generation and affinity. DiffDecip outperforms baseline models by forming more non-covalent interactions with highly conserved residues in the protein pocket. This allows for the identification of molecules that can target specific protein binding sites more effectively. By considering the interactions with highly conserved residues, DiffDecip enhances the bioactivity of ligands and improves the overall efficiency of structure-based drug design. The model's performance highlights the importance of incorporating protein evolutionary information into diffusion models for enhanced molecule optimization. <div>
arXiv:2505.05874v1 Announce Type: new 
Abstract: Generating molecules that bind to specific protein targets via diffusion models has shown good promise for structure-based drug design and molecule optimization. Especially, the diffusion models with binding interaction guidance enables molecule generation with high affinity through forming favorable interaction within protein pocket. However, the generated molecules may not form interactions with the highly conserved residues, which are important for protein functions and bioactivities of the ligands. Herein, we developed a new 3D target-aware diffusion model DiffDecip, which explicitly incorporates the protein-ligand binding interactions and evolutionary conservation information of protein residues into both diffusion and sampling process, for molecule optimization through scaffold decoration. The model performance revealed that DiffDecip outperforms baseline model DiffDec on molecule optimization towards higher affinity through forming more non-covalent interactions with highly conserved residues in the protein pocket.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Molecular Representation Learning via Structure Awareness</title>
<link>https://arxiv.org/abs/2505.05877</link>
<guid>https://arxiv.org/abs/2505.05877</guid>
<content:encoded><![CDATA[
<div> representation learning, molecular, multi-modal, structure-awareness, pre-training

Summary: 
The article introduces a new structure-awareness-based multi-modal self-supervised molecular representation pre-training framework (MMSA) for accurate extraction of molecular representations in drug discovery. MMSA addresses the limitations of existing multi-modal approaches by leveraging intermodal interactions and higher-order relationships between molecules. The framework consists of two modules: a multi-modal molecular representation learning module that processes information from different modalities to generate a unified molecular embedding, and a structure-awareness module that constructs a hypergraph structure to model correlations between molecules and integrates invariant knowledge using a memory mechanism. Extensive experiments show that MMSA achieves state-of-the-art performance on the MoleculeNet benchmark, with significant improvements in ROC-AUC ranging from 1.8% to 9.6% over baseline methods. <div>
arXiv:2505.05877v1 Announce Type: new 
Abstract: Accurate extraction of molecular representations is a critical step in the drug discovery process. In recent years, significant progress has been made in molecular representation learning methods, among which multi-modal molecular representation methods based on images, and 2D/3D topologies have become increasingly mainstream. However, existing these multi-modal approaches often directly fuse information from different modalities, overlooking the potential of intermodal interactions and failing to adequately capture the complex higher-order relationships and invariant features between molecules. To overcome these challenges, we propose a structure-awareness-based multi-modal self-supervised molecular representation pre-training framework (MMSA) designed to enhance molecular graph representations by leveraging invariant knowledge between molecules. The framework consists of two main modules: the multi-modal molecular representation learning module and the structure-awareness module. The multi-modal molecular representation learning module collaboratively processes information from different modalities of the same molecule to overcome intermodal differences and generate a unified molecular embedding. Subsequently, the structure-awareness module enhances the molecular representation by constructing a hypergraph structure to model higher-order correlations between molecules. This module also introduces a memory mechanism for storing typical molecular representations, aligning them with memory anchors in the memory bank to integrate invariant knowledge, thereby improving the model generalization ability. Extensive experiments have demonstrated the effectiveness of MMSA, which achieves state-of-the-art performance on the MoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to 9.6% over baseline methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRNN: Innovation-driven Recurrent Neural Network for Time-Series Data Modeling and Prediction</title>
<link>https://arxiv.org/abs/2505.05916</link>
<guid>https://arxiv.org/abs/2505.05916</guid>
<content:encoded><![CDATA[
<div> RNN, time series data, prediction, innovation, IU-BPTT <br />
Summary: <br />
The paper introduces Innovation-driven RNN (IRNN), a novel architecture for modeling and predicting time series data. Inspired by the Kalman filter, IRNN incorporates the concept of "innovation" by using past prediction errors as additional input signals to update hidden states and improve prediction accuracy. As traditional RNN training algorithms are not directly applicable to IRNN, a custom training algorithm called Input Updating-based Back-propagation Through Time (IU-BPTT) is proposed. This algorithm alternates between updating innovations and optimizing network parameters using gradient descent. Experimental results on real-world datasets demonstrate that incorporating innovations into different forms of RNN significantly enhances prediction accuracy without significantly increasing training costs. <div>
arXiv:2505.05916v1 Announce Type: new 
Abstract: Many real-world datasets are time series that are sequentially collected and contain rich temporal information. Thus, a common interest in practice is to capture dynamics of time series and predict their future evolutions. To this end, the recurrent neural network (RNN) has been a prevalent and effective machine learning option, which admits a nonlinear state-space model representation. Motivated by the resemblance between RNN and Kalman filter (KF) for linear state-space models, we propose in this paper Innovation-driven RNN (IRNN), a novel RNN architecture tailored to time-series data modeling and prediction tasks. By adapting the concept of "innovation" from KF to RNN, past prediction errors are adopted as additional input signals to update hidden states of RNN and boost prediction performance. Since innovation data depend on network parameters, existing training algorithms for RNN do not apply to IRNN straightforwardly. Thus, a tailored training algorithm dubbed input updating-based back-propagation through time (IU-BPTT) is further proposed, which alternates between updating innovations and optimizing network parameters via gradient descent. Experiments on real-world benchmark datasets show that the integration of innovations into various forms of RNN leads to remarkably improved prediction accuracy of IRNN without increasing the training cost substantially.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoder-Based Hybrid Replay for Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2505.05926</link>
<guid>https://arxiv.org/abs/2505.05926</guid>
<content:encoded><![CDATA[
<div> autoencoder, hybrid replay, class-incremental learning, catastrophic forgetting, memory/compute complexities 

Summary:
Hybrid replay strategy introduced for class-incremental learning (CIL) using a new hybrid autoencoder (HAE) to compress data and reduce memory requirements. Achieves $\mathcal{O}(0.1 t)$ memory complexity with $\mathcal{O}(t)$ computing complexity. HAE enables discriminative and generative modeling for classification and replay capabilities. Incorporates charged particle system energy minimization equations for embedding of new class centroids. Outperforms recent baselines on multiple benchmarks while maintaining the same memory/compute budgets. Source code included in supplementary material and will be open-sourced upon publication. <div>
arXiv:2505.05926v1 Announce Type: new 
Abstract: In class-incremental learning (CIL), effective incremental learning strategies are essential to mitigate task confusion and catastrophic forgetting, especially as the number of tasks $t$ increases. Current exemplar replay strategies impose $\mathcal{O}(t)$ memory/compute complexities. We propose an autoencoder-based hybrid replay (AHR) strategy that leverages our new hybrid autoencoder (HAE) to function as a compressor to alleviate the requirement for large memory, achieving $\mathcal{O}(0.1 t)$ at the worst case with the computing complexity of $\mathcal{O}(t)$ while accomplishing state-of-the-art performance. The decoder later recovers the exemplar data stored in the latent space, rather than in raw format. Additionally, HAE is designed for both discriminative and generative modeling, enabling classification and replay capabilities, respectively. HAE adopts the charged particle system energy minimization equations and repulsive force algorithm for the incremental embedding and distribution of new class centroids in its latent space. Our results demonstrate that AHR consistently outperforms recent baselines across multiple benchmarks while operating with the same memory/compute budgets. The source code is included in the supplementary material and will be open-sourced upon publication.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FloE: On-the-Fly MoE Inference</title>
<link>https://arxiv.org/abs/2505.05950</link>
<guid>https://arxiv.org/abs/2505.05950</guid>
<content:encoded><![CDATA[
<div> compression, Mixture-of-Experts, FloE, GPU, latency-sensitive 

Summary:<br />
The paper introduces FloE, an on-the-fly MoE inference system designed for memory-constrained GPUs. It addresses the issue of large PCIe bandwidth usage when activating experts by utilizing compression techniques on expert parameter matrices and implementing sparse prediction. FloE achieves a significant 9.3x compression of parameters per expert in Mixtral-8x7B, making it suitable for deployment on GPUs with limited VRAM. Additionally, it offers up to an 8.5x reduction in memory footprint and a substantial 48.7x speedup in inference compared to DeepSpeed-MII on a GeForce RTX 3090. This efficient approach to inference acceleration proves beneficial for devices with resource constraints like memory limitations. <br /><br />Summary: <div>
arXiv:2505.05950v1 Announce Type: new 
Abstract: With the widespread adoption of Mixture-of-Experts (MoE) models, there is a growing demand for efficient inference on memory-constrained devices. While offloading expert parameters to CPU memory and loading activated experts on demand has emerged as a potential solution, the large size of activated experts overburdens the limited PCIe bandwidth, hindering the effectiveness in latency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly MoE inference system on memory-constrained GPUs. FloE is built on the insight that there exists substantial untapped redundancy within sparsely activated experts. It employs various compression techniques on the expert's internal parameter matrices to reduce the data movement load, combined with low-cost sparse prediction, achieving perceptible inference acceleration in wall-clock time on resource-constrained devices. Empirically, FloE achieves a 9.3x compression of parameters per expert in Mixtral-8x7B; enables deployment on a GPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and delivers a 48.7x inference speedup compared to DeepSpeed-MII on a single GeForce RTX 3090.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Power Control Protocol for In-Factory 6G Subnetworks</title>
<link>https://arxiv.org/abs/2505.05967</link>
<guid>https://arxiv.org/abs/2505.05967</guid>
<content:encoded><![CDATA[
<div> Keywords: In-X Subnetworks, power control, reinforcement learning, multi-agent, signaling overhead<br />
Summary:<br />
In the context of In-Factory scenarios for 6G communication, effective power control is essential to address interference issues caused by high subnetwork density. Existing approaches have focused on data plane optimization, neglecting signaling overhead. This paper presents a novel multi-agent reinforcement learning framework for access points in In-Factory Subnetwork environments. By treating the problem as a partially observable Markov decision process and using multi-agent proximal policy optimization, the proposed approach reduces signaling overhead by 8 times while maintaining buffer flush rate close to the ideal "Genie" approach. The simulation results highlight the significant advantages of the learning-based method in autonomous protocol learning for power and signaling control in In-Factory Subnetworks. <br /><br />Summary: <div>
arXiv:2505.05967v1 Announce Type: new 
Abstract: In-X Subnetworks are envisioned to meet the stringent demands of short-range communication in diverse 6G use cases. In the context of In-Factory scenarios, effective power control is critical to mitigating the impact of interference resulting from potentially high subnetwork density. Existing approaches to power control in this domain have predominantly emphasized the data plane, often overlooking the impact of signaling overhead. Furthermore, prior work has typically adopted a network-centric perspective, relying on the assumption of complete and up-to-date channel state information (CSI) being readily available at the central controller. This paper introduces a novel multi-agent reinforcement learning (MARL) framework designed to enable access points to autonomously learn both signaling and power control protocols in an In-Factory Subnetwork environment. By formulating the problem as a partially observable Markov decision process (POMDP) and leveraging multi-agent proximal policy optimization (MAPPO), the proposed approach achieves significant advantages. The simulation results demonstrate that the learning-based method reduces signaling overhead by a factor of 8 while maintaining a buffer flush rate that lags the ideal "Genie" approach by only 5%.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Multi-agent Reinforcement Learning via Score Decomposition</title>
<link>https://arxiv.org/abs/2505.05968</link>
<guid>https://arxiv.org/abs/2505.05968</guid>
<content:encoded><![CDATA[
<div> Keywords: offline multi-agent reinforcement learning, distributional shifts, joint action spaces, coordination strategies, generative model

Summary:<br /><br />Offline multi-agent reinforcement learning (MARL) faces challenges due to distributional shifts and high-dimensional joint action spaces. Conventional approaches struggle with out-of-distribution actions, leading to suboptimal performance. To combat these issues, a novel two-stage framework is proposed. Firstly, a diffusion-based generative model captures complex behavior policies, facilitating accurate modeling of diverse coordination patterns. Secondly, a sequential score function decomposition mechanism regulates individual policies for decentralized execution. Extensive experiments on continuous control tasks showcase superior performance, outperforming existing methods by 26.3% in normalized returns. This approach offers valuable insights into offline coordination and equilibrium selection in cooperative multi-agent systems. <div>
arXiv:2505.05968v1 Announce Type: new 
Abstract: Offline multi-agent reinforcement learning (MARL) faces critical challenges due to distributional shifts, further exacerbated by the high dimensionality of joint action spaces and the diversity in coordination strategies and quality among agents. Conventional approaches, including independent learning frameworks and value decomposition methods based on pessimistic principles, remain susceptible to out-of-distribution (OOD) joint actions and often yield suboptimal performance. Through systematic analysis of prevalent offline MARL benchmarks, we identify that this limitation primarily stems from the inherently multimodal nature of joint collaborative policies induced by offline data collection. To address these challenges, we propose a novel two-stage framework: First, we employ a diffusion-based generative model to explicitly capture the complex behavior policy, enabling accurate modeling of diverse multi-agent coordination patterns. Second, we introduce a sequential score function decomposition mechanism to regularize individual policies and enable decentralized execution. Extensive experiments on continuous control tasks demonstrate state-of-the-art performance across multiple standard offline MARL benchmarks, outperforming existing methods by 26.3\% in normalized returns. Our approach provides new insights into offline coordination and equilibrium selection in cooperative multi-agent systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Architectural Exploration of Hybrid Neural Decoders for Neuromorphic Implantable BMI</title>
<link>https://arxiv.org/abs/2505.05983</link>
<guid>https://arxiv.org/abs/2505.05983</guid>
<content:encoded><![CDATA[
<div> Efficient Decoding Pipeline, Neuromorphic Implantable Brain-Machine Interfaces, Sparse Neural Event Data, Tunable Event Filter, Spike Detector<br />
<br />
Summary:<br />
This work introduces an efficient decoding pipeline for Neu-iBMI, utilizing sparse neural event data. A tunable event filter (EvFilter) reduces the processed events by up to 554X, achieving high decoding performance of R^2=0.73 with ANN- and SNN-based decoders. The pipeline eliminates the need for signal recovery, spike detection, or sorting typically done in traditional iBMI systems. The SNN-Decoder reduces computations and memory requirements significantly compared to NN- and LSTM-Decoders. The ST-NN-Decoder performs similarly to an LSTM-Decoder but with 2.5X fewer resources. This streamlined approach reduces computational and memory demands, making it suitable for low-power, on-implant, or wearable iBMIs. <div>
arXiv:2505.05983v1 Announce Type: new 
Abstract: This work presents an efficient decoding pipeline for neuromorphic implantable brain-machine interfaces (Neu-iBMI), leveraging sparse neural event data from an event-based neural sensing scheme. We introduce a tunable event filter (EvFilter), which also functions as a spike detector (EvFilter-SPD), significantly reducing the number of events processed for decoding by 192X and 554X, respectively. The proposed pipeline achieves high decoding performance, up to R^2=0.73, with ANN- and SNN-based decoders, eliminating the need for signal recovery, spike detection, or sorting, commonly performed in conventional iBMI systems. The SNN-Decoder reduces computations and memory required by 5-23X compared to NN-, and LSTM-Decoders, while the ST-NN-Decoder delivers similar performance to an LSTM-Decoder requiring 2.5X fewer resources. This streamlined approach significantly reduces computational and memory demands, making it ideal for low-power, on-implant, or wearable iBMIs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Fuzzy Neural Networks for Recommender Systems</title>
<link>https://arxiv.org/abs/2505.06000</link>
<guid>https://arxiv.org/abs/2505.06000</guid>
<content:encoded><![CDATA[
<div> Neuro-symbolic, Fuzzy neural networks, Transparent recommender systems, User-centric, Hybrid integration <br />
<br />
Summary: 
This work-in-progress explores the use of fuzzy neural networks (FNNs) in neuro-symbolic recommender systems to enhance transparency. By learning logic-based rules over human-readable atoms, the recommender system's decision-making process becomes transparent and understandable. Compared to black-box machine learning methods, the FNN approach maintains competitive performance while revealing the reasoning behind recommendations. The evaluation on synthetic and MovieLens 1M datasets shows that this approach accurately captures user behavior. Additionally, the differentiable nature of FNNs enables integration with other neural models, paving the way for the development of hybrid, transparent recommender systems. <div>
arXiv:2505.06000v1 Announce Type: new 
Abstract: As recommender systems become increasingly complex, transparency is essential to increase user trust, accountability, and regulatory compliance. Neuro-symbolic approaches that integrate symbolic reasoning with sub-symbolic learning offer a promising approach toward transparent and user-centric systems. In this work-in-progress, we investigate using fuzzy neural networks (FNNs) as a neuro-symbolic approach for recommendations that learn logic-based rules over predefined, human-readable atoms. Each rule corresponds to a fuzzy logic expression, making the recommender's decision process inherently transparent. In contrast to black-box machine learning methods, our approach reveals the reasoning behind a recommendation while maintaining competitive performance. We evaluate our method on a synthetic and MovieLens 1M datasets and compare it to state-of-the-art recommendation algorithms. Our results demonstrate that our approach accurately captures user behavior while providing a transparent decision-making process. Finally, the differentiable nature of this approach facilitates an integration with other neural models, enabling the development of hybrid, transparent recommender systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy-UCS Revisited: Self-Adaptation of Rule Representations in Michigan-Style Learning Fuzzy-Classifier Systems</title>
<link>https://arxiv.org/abs/2505.06017</link>
<guid>https://arxiv.org/abs/2505.06017</guid>
<content:encoded><![CDATA[
<div> Learning Fuzzy-Classifier Systems, Rule representation, Adaptive-UCS, Evolutionary operators, Classification accuracy

Summary:
Adaptive-UCS, a supervised Learning Fuzzy-Classifier System, introduces a self-adaptive rule representation mechanism utilizing a fuzzy indicator to determine the membership function shape of rules. This novel approach enhances classification performance by optimizing rule representation through evolutionary operators. Experimental results on continuous space problems show that Adaptive-UCS surpasses conventional UCSs with crisp or fuzzy rule representations in terms of accuracy. The system demonstrates robustness in handling noisy inputs and real-world problems with uncertainty, such as missing data. By adapting rule representation based on data characteristics, Adaptive-UCS provides stable and improved classification performance, making it a valuable tool for addressing classification challenges in various scenarios. <div>
arXiv:2505.06017v1 Announce Type: new 
Abstract: This paper focuses on the impact of rule representation in Michigan-style Learning Fuzzy-Classifier Systems (LFCSs) on its classification performance. A well-representation of the rules in an LFCS is crucial for improving its performance. However, conventional rule representations frequently need help addressing problems with unknown data characteristics. To address this issue, this paper proposes a supervised LFCS (i.e., Fuzzy-UCS) with a self-adaptive rule representation mechanism, entitled Adaptive-UCS. Adaptive-UCS incorporates a fuzzy indicator as a new rule parameter that sets the membership function of a rule as either rectangular (i.e., crisp) or triangular (i.e., fuzzy) shapes. The fuzzy indicator is optimized with evolutionary operators, allowing the system to search for an optimal rule representation. Results from extensive experiments conducted on continuous space problems demonstrate that Adaptive-UCS outperforms other UCSs with conventional crisp-hyperrectangular and fuzzy-hypertrapezoidal rule representations in classification accuracy. Additionally, Adaptive-UCS exhibits robustness in the case of noisy inputs and real-world problems with inherent uncertainty, such as missing values, leading to stable classification performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Approximation Theorem for Deep Q-Learning via FBSDE System</title>
<link>https://arxiv.org/abs/2505.06023</link>
<guid>https://arxiv.org/abs/2505.06023</guid>
<content:encoded><![CDATA[
<div> Deep Q-Networks, Approximation Capabilities, Universal Approximation Theorems, Bellman Equation, Control Problem's Structure<br />
<br />
Summary:
This paper introduces a Universal Approximation Theorem (UAT) specifically tailored for Deep Q-Networks (DQNs) designed to mimic the iterative refinement process found in Bellman updates. It highlights the propagation of regularity in Bellman operator applications and demonstrates how the transformation induced by the Bellman operator can be approximated by layers of a deep residual network. The analysis focuses on the uniform regularity of the sequence of value iteration iterates, showing their uniform Lipschitz continuity on compact domains. By viewing the layers of the network as neural operators acting on function spaces, the approximation theorem is closely linked to the structure of the control problem. This perspective offers a dynamic systems view of the network's operation on a space of value functions, showcasing how network depth corresponds to iterations of value function refinement with controlled error propagation. <div>
arXiv:2505.06023v1 Announce Type: new 
Abstract: The approximation capabilities of Deep Q-Networks (DQNs) are commonly justified by general Universal Approximation Theorems (UATs) that do not leverage the intrinsic structural properties of the optimal Q-function, the solution to a Bellman equation. This paper establishes a UAT for a class of DQNs whose architecture is designed to emulate the iterative refinement process inherent in Bellman updates. A central element of our analysis is the propagation of regularity: while the transformation induced by a single Bellman operator application exhibits regularity, for which Backward Stochastic Differential Equations (BSDEs) theory provides analytical tools, the uniform regularity of the entire sequence of value iteration iterates--specifically, their uniform Lipschitz continuity on compact domains under standard Lipschitz assumptions on the problem data--is derived from finite-horizon dynamic programming principles. We demonstrate that layers of a deep residual network, conceived as neural operators acting on function spaces, can approximate the action of the Bellman operator. The resulting approximation theorem is thus intrinsically linked to the control problem's structure, offering a proof technique wherein network depth directly corresponds to iterations of value function refinement, accompanied by controlled error propagation. This perspective reveals a dynamic systems view of the network's operation on a space of value functions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification</title>
<link>https://arxiv.org/abs/2505.06032</link>
<guid>https://arxiv.org/abs/2505.06032</guid>
<content:encoded><![CDATA[
<div> shortcuts, language models, actor names, attention heads, mechanistic interpretability <br />
Summary: <br />
The study delves into the role of shortcuts in language models, focusing on how these shortcuts are processed within the decision-making mechanism of the model. By using actor names in movie reviews as controllable shortcuts, the researchers identify specific attention heads that prioritize shortcuts, leading to premature decisions that skip contextual analysis. The introduction of Head-based Token Attribution (HTA) allows for tracing intermediate decisions back to input tokens, effectively detecting shortcuts and enabling targeted mitigation by deactivating shortcut-related attention heads. This novel approach sheds light on the inner workings of language models and provides a method for addressing the reliance on spurious correlations in these models. <div>
arXiv:2505.06032v1 Announce Type: new 
Abstract: Reliance on spurious correlations (shortcuts) has been shown to underlie many of the successes of language models. Previous work focused on identifying the input elements that impact prediction. We investigate how shortcuts are actually processed within the model's decision-making mechanism. We use actor names in movie reviews as controllable shortcuts with known impact on the outcome. We use mechanistic interpretability methods and identify specific attention heads that focus on shortcuts. These heads gear the model towards a label before processing the complete input, effectively making premature decisions that bypass contextual analysis. Based on these findings, we introduce Head-based Token Attribution (HTA), which traces intermediate decisions back to input tokens. We show that HTA is effective in detecting shortcuts in LLMs and enables targeted mitigation by selectively deactivating shortcut-related attention heads.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PYRREGULAR: A Unified Framework for Irregular Time Series, with Classification Benchmarks</title>
<link>https://arxiv.org/abs/2505.06047</link>
<guid>https://arxiv.org/abs/2505.06047</guid>
<content:encoded><![CDATA[
<div> Keywords: Irregular temporal data, classification, dataset repository, interoperability, evaluation

Summary:
Irregular temporal data, with varying recording frequencies and missing values, poses challenges in fields like mobility and healthcare. Existing research approaches often overlook these challenges, leading to fragmented methods. To address this gap, a unified framework and dataset repository have been introduced for irregular time series classification. The repository contains 34 datasets and benchmarks 12 classifier models from different domains. This initiative aims to centralize research efforts and enable a more robust evaluation of irregular temporal data analysis methods. By standardizing the dataset format and promoting interoperability, this work seeks to enhance the efficiency and effectiveness of analyzing irregular time series data. <div>
arXiv:2505.06047v1 Announce Type: new 
Abstract: Irregular temporal data, characterized by varying recording frequencies, differing observation durations, and missing values, presents significant challenges across fields like mobility, healthcare, and environmental science. Existing research communities often overlook or address these challenges in isolation, leading to fragmented tools and methods. To bridge this gap, we introduce a unified framework, and the first standardized dataset repository for irregular time series classification, built on a common array format to enhance interoperability. This repository comprises 34 datasets on which we benchmark 12 classifier models from diverse domains and communities. This work aims to centralize research efforts and enable a more robust evaluation of irregular temporal data analysis methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safe-EF: Error Feedback for Nonsmooth Constrained Optimization</title>
<link>https://arxiv.org/abs/2505.06053</link>
<guid>https://arxiv.org/abs/2505.06053</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, communication compression, error feedback, safety constraints, reinforcement learning<br />
Summary: <br />
This article discusses the challenges faced by federated learning due to communication bottlenecks caused by high-dimensional model updates. It explores the use of contractive compressors and error feedback to mitigate performance degradation but notes limitations in handling non-smooth objectives and safety constraints. The authors establish lower complexity bounds for first-order algorithms with contractive compression in a non-smooth convex setting. They introduce Safe-EF, an algorithm that matches the lower bound while enforcing safety constraints, essential for practical applications. The extension to the stochastic setting bridges theory and practical implementation. Experimental validation in a distributed humanoid robot training scenario demonstrates the effectiveness of Safe-EF in ensuring safety and reducing communication complexity. <div>
arXiv:2505.06053v1 Announce Type: new 
Abstract: Federated learning faces severe communication bottlenecks due to the high dimensionality of model updates. Communication compression with contractive compressors (e.g., Top-K) is often preferable in practice but can degrade performance without proper handling. Error feedback (EF) mitigates such issues but has been largely restricted for smooth, unconstrained problems, limiting its real-world applicability where non-smooth objectives and safety constraints are critical. We advance our understanding of EF in the canonical non-smooth convex setting by establishing new lower complexity bounds for first-order algorithms with contractive compression. Next, we propose Safe-EF, a novel algorithm that matches our lower bound (up to a constant) while enforcing safety constraints essential for practical applications. Extending our approach to the stochastic setting, we bridge the gap between theory and practical implementation. Extensive experiments in a reinforcement learning setup, simulating distributed humanoid robot training, validate the effectiveness of Safe-EF in ensuring safety and reducing communication complexity.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fault Diagnosis of 3D-Printed Scaled Wind Turbine Blades</title>
<link>https://arxiv.org/abs/2505.06080</link>
<guid>https://arxiv.org/abs/2505.06080</guid>
<content:encoded><![CDATA[
<div> 3D-printed scaled models, finite element simulations, experimental modal analysis, machine learning techniques, fault detection<br />
<br />Summary:
This study demonstrates an integrated methodology for fault detection in wind turbine blades by utilizing 3D-printed scaled models, finite element simulations, experimental modal analysis, and machine learning techniques. The research involved fabricating a scaled model of the NREL 5MW blade through 3D printing and introducing crack-type damages at critical locations. Finite Element Analysis predicted the impact of damages on natural frequencies, with validation through hammer impact tests. Vibration data analysis extracted time-domain and frequency-domain features, with key variables identified through statistics. Machine learning classifiers like Support Vector Machine and K-Nearest Neighbors achieved high classification accuracies. The study pinpointed vibration modes 3, 4, and 6 as sensitive to structural anomalies in the blade, affirming the feasibility of combining numerical simulations and experimental validations for wind energy health monitoring systems. <div>
arXiv:2505.06080v1 Announce Type: new 
Abstract: This study presents an integrated methodology for fault detection in wind turbine blades using 3D-printed scaled models, finite element simulations, experimental modal analysis, and machine learning techniques. A scaled model of the NREL 5MW blade was fabricated using 3D printing, and crack-type damages were introduced at critical locations. Finite Element Analysis was employed to predict the impact of these damages on the natural frequencies, with the results validated through controlled hammer impact tests. Vibration data was processed to extract both time-domain and frequency-domain features, and key discriminative variables were identified using statistical analyses (ANOVA). Machine learning classifiers, including Support Vector Machine and K-Nearest Neighbors, achieved classification accuracies exceeding 94%. The results revealed that vibration modes 3, 4, and 6 are particularly sensitive to structural anomalies for this blade. This integrated approach confirms the feasibility of combining numerical simulations with experimental validations and paves the way for structural health monitoring systems in wind energy applications.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Diffusion Maps</title>
<link>https://arxiv.org/abs/2505.06087</link>
<guid>https://arxiv.org/abs/2505.06087</guid>
<content:encoded><![CDATA[
<div> Keywords: dimensionality reduction, Diffusion Maps, manifold learning, deep learning, spectral decomposition

Summary: 
Dimensionality reduction is essential in machine learning to combat the curse of dimensionality. Diffusion Maps is a popular nonlinear method, but it has limitations like inability to generalize to new data and high computational complexity. This study introduces a novel approach that combines deep learning with Diffusion Maps to address these issues. By formulating Diffusion Maps embedding as an unconstrained minimization problem, a neural network can be trained to efficiently compute Diffusion Maps embedding both within and outside the training set without spectral decomposition. The proposed method is compared to traditional Diffusion Maps and the Nystrom method on various datasets, demonstrating its effectiveness in reducing dimensionality and improving efficiency in handling large datasets. Overall, this new approach shows promise in overcoming the limitations of existing manifold learning methods. 

<br /><br />Summary: <div>
arXiv:2505.06087v1 Announce Type: new 
Abstract: One of the fundamental problems within the field of machine learning is dimensionality reduction. Dimensionality reduction methods make it possible to combat the so-called curse of dimensionality, visualize high-dimensional data and, in general, improve the efficiency of storing and processing large data sets. One of the best-known nonlinear dimensionality reduction methods is Diffusion Maps. However, despite their virtues, both Diffusion Maps and many other manifold learning methods based on the spectral decomposition of kernel matrices have drawbacks such as the inability to apply them to data outside the initial set, their computational complexity, and high memory costs for large data sets. In this work, we propose to alleviate these problems by resorting to deep learning. Specifically, a new formulation of Diffusion Maps embedding is offered as a solution to a certain unconstrained minimization problem and, based on it, a cost function to train a neural network which computes Diffusion Maps embedding -- both inside and outside the training sample -- without the need to perform any spectral decomposition. The capabilities of this approach are compared on different data sets, both real and synthetic, with those of Diffusion Maps and the Nystrom method.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniSymNet: A Unified Symbolic Network Guided by Transformer</title>
<link>https://arxiv.org/abs/2505.06091</link>
<guid>https://arxiv.org/abs/2505.06091</guid>
<content:encoded><![CDATA[
<div> Keywords: Symbolic Regression, Symbolic Networks, Transformer model, Label encoding, Complexity reduction 

Summary: 
The paper introduces a new approach called Unified Symbolic Network (UniSymNet) for Symbolic Regression (SR), which aims to automatically discover mathematical expressions from input data. It addresses the limitations of traditional SR algorithms by unifying binary operators into nested unary operators and reducing complexity. The proposed UniSymNet utilizes a pre-trained Transformer model with a novel label encoding method to guide structural selection and adopts objective-specific optimization strategies for parameter learning. This approach achieves high fitting accuracy, excellent symbolic solution rate, and relatively low expression complexity. Experimental results demonstrate competitive performance on both low-dimensional Standard Benchmarks and high-dimensional SRBench datasets. The UniSymNet offers a promising solution to the challenges faced by existing symbolic networks in symbolic regression tasks. 

<br /><br />Summary: <div>
arXiv:2505.06091v1 Announce Type: new 
Abstract: Symbolic Regression (SR) is a powerful technique for automatically discovering mathematical expressions from input data. Mainstream SR algorithms search for the optimal symbolic tree in a vast function space, but the increasing complexity of the tree structure limits their performance. Inspired by neural networks, symbolic networks have emerged as a promising new paradigm. However, most existing symbolic networks still face certain challenges: binary nonlinear operators $\{\times, \div\}$ cannot be naturally extended to multivariate operators, and training with fixed architecture often leads to higher complexity and overfitting. In this work, we propose a Unified Symbolic Network that unifies nonlinear binary operators into nested unary operators and define the conditions under which UniSymNet can reduce complexity. Moreover, we pre-train a Transformer model with a novel label encoding method to guide structural selection, and adopt objective-specific optimization strategies to learn the parameters of the symbolic network. UniSymNet shows high fitting accuracy, excellent symbolic solution rate, and relatively low expression complexity, achieving competitive performance on low-dimensional Standard Benchmarks and high-dimensional SRBench.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Outperform Experts on Challenging Biology Benchmarks</title>
<link>https://arxiv.org/abs/2505.06108</link>
<guid>https://arxiv.org/abs/2505.06108</guid>
<content:encoded><![CDATA[
<div> AI, Large Language Models, Biology, Benchmark, Evaluation

Summary:
The study evaluates 27 Large Language Models on various biology benchmarks, showing significant improvements in biological capabilities over time. Top models outperformed expert virologists in certain benchmarks and matched or exceeded expert-level performance in others. Chain-of-thought did not notably enhance performance, while extended reasoning features in specific models improved as expected. Some benchmarks displayed performance plateaus below 100%, revealing potential saturation and errors in the data. The findings underscore the need for more advanced evaluation methodologies as AI technology progresses. 

<br /><br />Summary: <div>
arXiv:2505.06108v1 Announce Type: new 
Abstract: This study systematically evaluates 27 frontier Large Language Models on eight diverse biology benchmarks spanning molecular biology, genetics, cloning, virology, and biosecurity. Models from major AI developers released between November 2022 and April 2025 were assessed through ten independent runs per benchmark. The findings reveal dramatic improvements in biological capabilities. Top model performance increased more than 4-fold on the challenging text-only subset of the Virology Capabilities Test over the study period, with the top model now performing twice as well as expert virologists. Several models now match or exceed expert-level performance on other challenging benchmarks, including LAB-Bench CloningScenarios and the biology subsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not substantially improve performance over zero-shot evaluation, while extended reasoning features in o3-mini and Claude 3.7 Sonnet typically improved performance as predicted by inference scaling. Benchmarks such as PubMedQA and the MMLU and WMDP biology subsets exhibited performance plateaus well below 100%, suggesting benchmark saturation and errors in the underlying benchmark data. The analysis highlights the need for more sophisticated evaluation methodologies as AI systems continue to advance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FIC-TSC: Learning Time Series Classification with Fisher Information Constraint</title>
<link>https://arxiv.org/abs/2505.06114</link>
<guid>https://arxiv.org/abs/2505.06114</guid>
<content:encoded><![CDATA[
arXiv:2505.06114v1 Announce Type: new 
Abstract: Analyzing time series data is crucial to a wide spectrum of applications, including economics, online marketplaces, and human healthcare. In particular, time series classification plays an indispensable role in segmenting different phases in stock markets, predicting customer behavior, and classifying worker actions and engagement levels. These aspects contribute significantly to the advancement of automated decision-making and system optimization in real-world applications. However, there is a large consensus that time series data often suffers from domain shifts between training and test sets, which dramatically degrades the classification performance. Despite the success of (reversible) instance normalization in handling the domain shifts for time series regression tasks, its performance in classification is unsatisfactory. In this paper, we propose \textit{FIC-TSC}, a training framework for time series classification that leverages Fisher information as the constraint. We theoretically and empirically show this is an efficient and effective solution to guide the model converge toward flatter minima, which enhances its generalizability to distribution shifts. We rigorously evaluate our method on 30 UEA multivariate and 85 UCR univariate datasets. Our empirical results demonstrate the superiority of the proposed method over 14 recent state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena</title>
<link>https://arxiv.org/abs/2505.06123</link>
<guid>https://arxiv.org/abs/2505.06123</guid>
<content:encoded><![CDATA[
arXiv:2505.06123v1 Announce Type: new 
Abstract: Wasserstein distances provide a powerful framework for comparing data distributions. They can be used to analyze processes over time or to detect inhomogeneities within data. However, simply calculating the Wasserstein distance or analyzing the corresponding transport map (or coupling) may not be sufficient for understanding what factors contribute to a high or low Wasserstein distance. In this work, we propose a novel solution based on Explainable AI that allows us to efficiently and accurately attribute Wasserstein distances to various data components, including data subgroups, input features, or interpretable subspaces. Our method achieves high accuracy across diverse datasets and Wasserstein distance specifications, and its practical utility is demonstrated in two use cases.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation</title>
<link>https://arxiv.org/abs/2505.06134</link>
<guid>https://arxiv.org/abs/2505.06134</guid>
<content:encoded><![CDATA[
arXiv:2505.06134v1 Announce Type: new 
Abstract: Trajectory prediction is a key element of autonomous vehicle systems, enabling them to anticipate and react to the movements of other road users. Evaluating the robustness of prediction models against adversarial attacks is essential to ensure their reliability in real-world traffic. However, current approaches tend to focus on perturbing the past positions of surrounding agents, which can generate unrealistic scenarios and overlook critical vulnerabilities. This limitation may result in overly optimistic assessments of model performance in real-world conditions.
  In this work, we demonstrate that perturbing not just past but also future states of adversarial agents can uncover previously undetected weaknesses and thereby provide a more rigorous evaluation of model robustness. Our novel approach incorporates dynamic constraints and preserves tactical behaviors, enabling more effective and realistic adversarial attacks. We introduce new performance measures to assess the realism and impact of these adversarial trajectories. Testing our method on a state-of-the-art prediction model revealed significant increases in prediction errors and collision rates under adversarial conditions. Qualitative analysis further showed that our attacks can expose critical weaknesses, such as the inability of the model to detect potential collisions in what appear to be safe predictions. These results underscore the need for more comprehensive adversarial testing to better evaluate and improve the reliability of trajectory prediction models for autonomous vehicles.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Depth of Monotone ReLU Neural Networks and ICNNs</title>
<link>https://arxiv.org/abs/2505.06169</link>
<guid>https://arxiv.org/abs/2505.06169</guid>
<content:encoded><![CDATA[
arXiv:2505.06169v1 Announce Type: new 
Abstract: We study two models of ReLU neural networks: monotone networks (ReLU$^+$) and input convex neural networks (ICNN). Our focus is on expressivity, mostly in terms of depth, and we prove the following lower bounds. For the maximum function MAX$_n$ computing the maximum of $n$ real numbers, we show that ReLU$^+$ networks cannot compute MAX$_n$, or even approximate it. We prove a sharp $n$ lower bound on the ICNN depth complexity of MAX$_n$. We also prove depth separations between ReLU networks and ICNNs; for every $k$, there is a depth-2 ReLU network of size $O(k^2)$ that cannot be simulated by a depth-$k$ ICNN. The proofs are based on deep connections between neural networks and polyhedral geometry, and also use isoperimetric properties of triangulations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model-Enhanced Q-learning for Capacitated Vehicle Routing Problem with Time Windows</title>
<link>https://arxiv.org/abs/2505.06178</link>
<guid>https://arxiv.org/abs/2505.06178</guid>
<content:encoded><![CDATA[
arXiv:2505.06178v1 Announce Type: new 
Abstract: The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a classic NP-hard combinatorial optimization problem widely applied in logistics distribution and transportation management. Its complexity stems from the constraints of vehicle capacity and time windows, which pose significant challenges to traditional approaches. Advances in Large Language Models (LLMs) provide new possibilities for finding approximate solutions to CVRPTW. This paper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW with real-time emergency constraints. Our solution introduces an adaptive two-phase training mechanism that transitions from the LLM-guided exploration phase to the autonomous optimization phase of Q-network. To ensure reliability, we design a three-tier self-correction mechanism based on the Chain-of-Thought (CoT) for LLMs: syntactic validation, semantic verification, and physical constraint enforcement. In addition, we also prioritized replay of the experience generated by LLMs to amplify the regulatory role of LLMs in the architecture. Experimental results demonstrate that our framework achieves a 7.3\% average reduction in cost compared to traditional Q-learning, with fewer training steps required for convergence.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet</title>
<link>https://arxiv.org/abs/2505.06185</link>
<guid>https://arxiv.org/abs/2505.06185</guid>
<content:encoded><![CDATA[
arXiv:2505.06185v1 Announce Type: new 
Abstract: This paper proposes a method MTL-Swin-Unet which is multi-task learning using transformers for classification and semantic segmentation. For spurious-correlation problems, this method allows us to enhance the image representation with two other image representations: representation obtained by semantic segmentation and representation obtained by image reconstruction. In our experiments, the proposed method outperformed in F-value measure than other classifiers when the test data included slices from the same patient (no covariate shift). Similarly, when the test data did not include slices from the same patient (covariate shift setting), the proposed method outperformed in AUC measure.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto Tensor Singular Value Thresholding: A Non-Iterative and Rank-Free Framework for Tensor Denoising</title>
<link>https://arxiv.org/abs/2505.06203</link>
<guid>https://arxiv.org/abs/2505.06203</guid>
<content:encoded><![CDATA[
arXiv:2505.06203v1 Announce Type: new 
Abstract: In modern data-driven tasks such as classification, optimization, and forecasting, mitigating the effects of intrinsic noise is crucial for improving predictive accuracy. While numerous denoising techniques have been developed, the rising dimensionality of real-world datasets limits conventional matrix-based methods in preserving data structure and accuracy. This challenge has led to increasing interest in tensor-based approaches, which naturally capture multi-way data relationships. However, classical tensor decomposition methods (e.g., HOSVD, HOOI) typically require pre-specified ranks and iterative optimization, making them computationally expensive and less practical. In this work, we propose a novel low-rank approximation method for tensor data that avoids these limitations. Our approach applies statistically grounded singular value thresholding to mode-wise matricizations, enabling automatic extraction of significant components without requiring prior rank specification or iterative refinement. Experiments on synthetic and real-world tensors show that our method consistently outperforms existing techniques in terms of estimation accuracy and computational efficiency, especially in noisy high-dimensional settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks</title>
<link>https://arxiv.org/abs/2505.06224</link>
<guid>https://arxiv.org/abs/2505.06224</guid>
<content:encoded><![CDATA[
arXiv:2505.06224v1 Announce Type: new 
Abstract: Downstream probing has been the dominant method for evaluating model representations, an important process given the increasing prominence of self-supervised learning and foundation models. However, downstream probing primarily assesses the availability of task-relevant information in the model's latent space, overlooking attributes such as equivariance, invariance, and disentanglement, which contribute to the interpretability, adaptability, and utility of representations in real-world applications. While some attempts have been made to measure these qualities in representations, no unified evaluation framework with modular, generalizable, and interpretable metrics exists.
  In this paper, we argue for the importance of representation evaluation beyond downstream probing. We introduce a standardized protocol to quantify informativeness, equivariance, invariance, and disentanglement of factors of variation in model representations. We use it to evaluate representations from a variety of models in the image and speech domains using different architectures and pretraining approaches on identified controllable factors of variation. We find that representations from models with similar downstream performance can behave substantially differently with regard to these attributes. This hints that the respective mechanisms underlying their downstream performance are functionally different, prompting new research directions to understand and improve representations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L2R: Learning to Reduce Search Space for Generalizable Neural Routing Solver</title>
<link>https://arxiv.org/abs/2503.03137</link>
<guid>https://arxiv.org/abs/2503.03137</guid>
<content:encoded><![CDATA[
arXiv:2503.03137v1 Announce Type: cross 
Abstract: Constructive neural combinatorial optimization (NCO) has attracted growing research attention due to its ability to solve complex routing problems without relying on handcrafted rules. However, existing NCO methods face significant challenges in generalizing to large-scale problems due to high computational complexity and inefficient capture of structural patterns. To address this issue, we propose a novel learning-based search space reduction method that adaptively selects a small set of promising candidate nodes at each step of the constructive NCO process. Unlike traditional methods that rely on fixed heuristics, our selection model dynamically prioritizes nodes based on learned patterns, significantly reducing the search space while maintaining solution quality. Experimental results demonstrate that our method, trained solely on 100-node instances from uniform distribution, generalizes remarkably well to large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) instances with up to 1 million nodes from the uniform distribution and over 80K nodes from other distributions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OccuEMBED: Occupancy Extraction Merged with Building Energy Disaggregation for Occupant-Responsive Operation at Scale</title>
<link>https://arxiv.org/abs/2505.05478</link>
<guid>https://arxiv.org/abs/2505.05478</guid>
<content:encoded><![CDATA[
arXiv:2505.05478v1 Announce Type: cross 
Abstract: Buildings account for a significant share of global energy consumption and emissions, making it critical to operate them efficiently. As electricity grids become more volatile with renewable penetration, buildings must provide flexibility to support grid stability. Building automation plays a key role in enhancing efficiency and flexibility via centralized operations, but it must prioritize occupant-centric strategies to balance energy and comfort targets. However, incorporating occupant information into large-scale, centralized building operations remains challenging due to data limitations. We investigate the potential of using whole-building smart meter data to infer both occupancy and system operations. Integrating these insights into data-driven building energy analysis allows more occupant-centric energy-saving and flexibility at scale. Specifically, we propose OccuEMBED, a unified framework for occupancy inference and system-level load analysis. It combines two key components: a probabilistic occupancy profile generator, and a controllable and interpretable load disaggregator supported by Kolmogorov-Arnold Networks (KAN). This design embeds knowledge of occupancy patterns and load-occupancy-weather relationships into deep learning models. We conducted comprehensive evaluations to demonstrate its effectiveness across synthetic and real-world datasets compared to various occupancy inference baselines. OccuEMBED always achieved average F1 scores above 0.8 in discrete occupancy inference and RMSE within 0.1-0.2 for continuous occupancy ratios. We further demonstrate how OccuEMBED integrates with building load monitoring platforms to display occupancy profiles, analyze system-level operations, and inform occupant-responsive strategies. Our model lays a robust foundation in scaling occupant-centric building management systems to meet the challenges of an evolving energy system.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Local Air Quality Predictions Using Transfer Learning on Satellite Data and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.05479</link>
<guid>https://arxiv.org/abs/2505.05479</guid>
<content:encoded><![CDATA[
arXiv:2505.05479v1 Announce Type: cross 
Abstract: Air pollution is a significant global health risk, contributing to millions of premature deaths annually. Nitrogen dioxide (NO2), a harmful pollutant, disproportionately affects urban areas where monitoring networks are often sparse. We propose a novel method for predicting NO2 concentrations at unmonitored locations using transfer learning with satellite and meteorological data. Leveraging the GraphSAGE framework, our approach integrates autoregression and transfer learning to enhance predictive accuracy in data-scarce regions like Bristol. Pre-trained on data from London, UK, our model achieves a 8.6% reduction in Normalised Root Mean Squared Error (NRMSE) and a 32.6% reduction in Gradient RMSE compared to a baseline model. This work demonstrates the potential of virtual sensors for cost-effective air quality monitoring, contributing to actionable insights for climate and health interventions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Optimization for the Classification of Small Molecules Regulating the Circadian Rhythm Period: A Reliable Assessment</title>
<link>https://arxiv.org/abs/2505.05485</link>
<guid>https://arxiv.org/abs/2505.05485</guid>
<content:encoded><![CDATA[
arXiv:2505.05485v1 Announce Type: cross 
Abstract: The circadian rhythm plays a crucial role in regulating biological processes, and its disruption is linked to various health issues. Identifying small molecules that influence the circadian period is essential for developing targeted therapies. This study explores the use of evolutionary optimization techniques to enhance the classification of these molecules. We applied an evolutionary algorithm to optimize feature selection and classification performance. Several machine learning classifiers were employed, and performance was evaluated using accuracy and generalization ability. The findings demonstrate that the proposed evolutionary optimization method improves classification accuracy and reduces overfitting compared to baseline models. Additionally, the use of variance in accuracy as a penalty factor may enhance the model's reliability for real-world applications. Our study confirms that evolutionary optimization is an effective strategy for classifying small molecules regulating the circadian rhythm. The proposed approach not only improves predictive performance but also ensures a more robust model.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedAvgen: Metadata for Model Aggregation In Communication Systems</title>
<link>https://arxiv.org/abs/2505.05486</link>
<guid>https://arxiv.org/abs/2505.05486</guid>
<content:encoded><![CDATA[
arXiv:2505.05486v1 Announce Type: cross 
Abstract: To improve business efficiency and minimize costs, Artificial Intelligence (AI) practitioners have adopted a shift from formulating models from scratch towards sharing pretrained models. The pretrained models are then aggregated into a global model with higher generalization capabilities, which is afterwards distributed to the client devices. This approach is known as federated learning and inherently utilizes different techniques to select the candidate client models averaged to obtain the global model. This approach, in the case of communication systems, faces challenges arising from the existential diversity in device profiles. The multiplicity in profiles motivates our conceptual assessment of a metaheuristic algorithm (FedAvgen), which relates each pretrained model with its weight space as metadata, to a phenotype and genotype, respectively. This parent-child genetic evolution characterizes the global averaging step in federated learning. We then compare the results of our approach to two widely adopted baseline federated learning algorithms like Federated Averaging (FedAvg) and Federated Stochastic Gradient Descent (FedSGD).
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Akkumula: Evidence accumulation driver models with Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.05489</link>
<guid>https://arxiv.org/abs/2505.05489</guid>
<content:encoded><![CDATA[
arXiv:2505.05489v1 Announce Type: cross 
Abstract: Processes of evidence accumulation for motor control contribute to the ecological validity of driver models. According to established theories of cognition, drivers make control adjustments when a process of accumulation of perceptual inputs reaches a decision boundary. Unfortunately, there is not a standard way for building such models, limiting their use. Current implementations are hand-crafted, lack adaptability, and rely on inefficient optimization techniques that do not scale well with large datasets. This paper introduces Akkumula, an evidence accumulation modelling framework built using deep learning techniques to leverage established coding libraries, gradient optimization, and large batch training. The core of the library is based on Spiking Neural Networks, whose operation mimic the evidence accumulation process in the biological brain. The model was tested on data collected during a test-track experiment. Results are promising. The model fits well the time course of vehicle control (brake, accelerate, steering) based on vehicle sensor data. The perceptual inputs are extracted by a dedicated neural network, increasing the context-awareness of the model in dynamic scenarios. Akkumula integrates with existing machine learning architectures, benefits from continuous advancements in deep learning, efficiently processes large datasets, adapts to diverse driving scenarios, and maintains a degree of transparency in its core mechanisms.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision</title>
<link>https://arxiv.org/abs/2505.05492</link>
<guid>https://arxiv.org/abs/2505.05492</guid>
<content:encoded><![CDATA[
arXiv:2505.05492v1 Announce Type: cross 
Abstract: While machine learning fairness has made significant progress in recent years, most existing solutions focus on tabular data and are poorly suited for vision-based classification tasks, which rely heavily on deep learning. To bridge this gap, we introduce DetoxAI, an open-source Python library for improving fairness in deep learning vision classifiers through post-hoc debiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness metrics, and visualization tools. It supports debiasing via interventions in internal representations and includes attribution-based visualization tools and quantitative algorithmic fairness metrics to show how bias is mitigated. This paper presents the motivation, design, and use cases of DetoxAI, demonstrating its tangible value to engineers and researchers.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Automated LLM-based Pipeline for Asset-Level Database Creation to Assess Deforestation Impact</title>
<link>https://arxiv.org/abs/2505.05494</link>
<guid>https://arxiv.org/abs/2505.05494</guid>
<content:encoded><![CDATA[
arXiv:2505.05494v1 Announce Type: cross 
Abstract: The European Union Deforestation Regulation (EUDR) requires companies to prove their products do not contribute to deforestation, creating a critical demand for precise, asset-level environmental impact data. Current databases lack the necessary detail, relying heavily on broad financial metrics and manual data collection, which limits regulatory compliance and accurate environmental modeling. This study presents an automated, end-to-end data extraction pipeline that uses LLMs to create, clean, and validate structured databases, specifically targeting sectors with a high risk of deforestation. The pipeline introduces Instructional, Role-Based, Zero-Shot Chain-of-Thought (IRZ-CoT) prompting to enhance data extraction accuracy and a Retrieval-Augmented Validation (RAV) process that integrates real-time web searches for improved data reliability. Applied to SEC EDGAR filings in the Mining, Oil & Gas, and Utilities sectors, the pipeline demonstrates significant improvements over traditional zero-shot prompting approaches, particularly in extraction accuracy and validation coverage. This work advances NLP-driven automation for regulatory compliance, CSR (Corporate Social Responsibility), and ESG, with broad sectoral applicability.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train Your Metamorphic Deep Neural Network</title>
<link>https://arxiv.org/abs/2505.05510</link>
<guid>https://arxiv.org/abs/2505.05510</guid>
<content:encoded><![CDATA[
arXiv:2505.05510v1 Announce Type: cross 
Abstract: Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural networks of varying width and depth. Based on Implicit Neural Representation (INR), NeuMeta learns a continuous weight manifold, enabling the direct generation of compressed models, including those with configurations not seen during training. While promising, the original formulation of NeuMeta proves effective only for the final layers of the undelying model, limiting its broader applicability. In this work, we propose a training algorithm that extends the capabilities of NeuMeta to enable full-network metamorphosis with minimal accuracy degradation. Our approach follows a structured recipe comprising block-wise incremental training, INR initialization, and strategies for replacing batch normalization. The resulting metamorphic networks maintain competitive accuracy across a wide range of compression ratios, offering a scalable solution for adaptable and efficient deployment of deep models. The code is available at: https://github.com/TSommariva/HTTY_NeuMeta.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Economic Analysis and Optimization of Energy Storage Configuration for Park Power Systems Based on Random Forest and Genetic Algorithm</title>
<link>https://arxiv.org/abs/2505.05511</link>
<guid>https://arxiv.org/abs/2505.05511</guid>
<content:encoded><![CDATA[
arXiv:2505.05511v1 Announce Type: cross 
Abstract: This study aims to analyze the economic performance of various parks under different conditions, particularly focusing on the operational costs and power load balancing before and after the deployment of energy storage systems. Firstly, the economic performance of the parks without energy storage was analyzed using a random forest model. Taking Park A as an example, it was found that the cost had the greatest correlation with electricity purchase, followed by photovoltaic output, indicating that solar and wind power output are key factors affecting economic performance. Subsequently, the operation of the parks after the configuration of a 50kW/100kWh energy storage system was simulated, and the total cost and operation strategy of the energy storage system were calculated. The results showed that after the deployment of energy storage, the amount of wind and solar power curtailment in each park decreased, and the operational costs were reduced. Finally, a genetic algorithm was used to optimize the energy storage configuration of each park. The energy storage operation strategy was optimized through fitness functions, crossover operations, and mutation operations. After optimization, the economic indicators of Parks A, B, and C all improved. The research results indicate that by optimizing energy storage configuration, each park can reduce costs, enhance economic benefits, and achieve sustainable development of the power system.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience</title>
<link>https://arxiv.org/abs/2505.05515</link>
<guid>https://arxiv.org/abs/2505.05515</guid>
<content:encoded><![CDATA[
arXiv:2505.05515v1 Announce Type: cross 
Abstract: Autonomous AI is no longer a hard-to-reach concept, it enables the agents to move beyond executing tasks to independently addressing complex problems, adapting to change while handling the uncertainty of the environment. However, what makes the agents truly autonomous? It is agentic reasoning, that is crucial for foundation models to develop symbolic logic, statistical correlations, or large-scale pattern recognition to process information, draw inferences, and make decisions. However, it remains unclear why and how existing agentic reasoning approaches work, in comparison to biological reasoning, which instead is deeply rooted in neural mechanisms involving hierarchical cognition, multimodal integration, and dynamic interactions. In this work, we propose a novel neuroscience-inspired framework for agentic reasoning. Grounded in three neuroscience-based definitions and supported by mathematical and biological foundations, we propose a unified framework modeling reasoning from perception to action, encompassing four core types, perceptual, dimensional, logical, and interactive, inspired by distinct functional roles observed in the human brain. We apply this framework to systematically classify and analyze existing AI reasoning methods, evaluating their theoretical foundations, computational designs, and practical limitations. We also explore its implications for building more generalizable, cognitively aligned agents in physical and virtual environments. Finally, building on our framework, we outline future directions and propose new neural-inspired reasoning methods, analogous to chain-of-thought prompting. By bridging cognitive neuroscience and AI, this work offers a theoretical foundation and practical roadmap for advancing agentic reasoning in intelligent systems. The associated project can be found at: https://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions</title>
<link>https://arxiv.org/abs/2505.05517</link>
<guid>https://arxiv.org/abs/2505.05517</guid>
<content:encoded><![CDATA[
arXiv:2505.05517v1 Announce Type: cross 
Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands to manipulate objects effectively. However, most prior work either focuses on power grasping, which simply involves holding an object still, or relies on costly teleoperated robot demonstrations to teach robots how to grasp each object functionally. Instead, we propose extracting human grasp information from web images since they depict natural and functional object interactions, thereby bypassing the need for curated demonstrations. We reconstruct human hand-object interaction (HOI) 3D meshes from RGB images, retarget the human hand to multi-finger robot hands, and align the noisy object mesh with its accurate 3D shape. We show that these relatively low-quality HOI data from inexpensive web sources can effectively train a functional grasping model. To further expand the grasp dataset for seen and unseen objects, we use the initially-trained grasping policy with web data in the IsaacGym simulator to generate physically feasible grasps while preserving functionality. We train the grasping model on 10 object categories and evaluate it on 9 unseen objects, including challenging items such as syringes, pens, spray bottles, and tongs, which are underrepresented in existing datasets. The model trained on the web HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with a 6.7% improvement in success rate and a 1.8x increase in functionality ratings over baselines. Simulator-augmented data further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the LEAP Hand achieves a 85% success rate. Project website is at: https://webgrasp.github.io/.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP</title>
<link>https://arxiv.org/abs/2505.05528</link>
<guid>https://arxiv.org/abs/2505.05528</guid>
<content:encoded><![CDATA[
arXiv:2505.05528v1 Announce Type: cross 
Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Vision, Language, &amp; Action Models in Procedurally Generated, Open Ended Action Environments</title>
<link>https://arxiv.org/abs/2505.05540</link>
<guid>https://arxiv.org/abs/2505.05540</guid>
<content:encoded><![CDATA[
arXiv:2505.05540v1 Announce Type: cross 
Abstract: Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLM and VLA models-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexit; (2) VLAs generally outperform other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Common Interface for Automatic Differentiation</title>
<link>https://arxiv.org/abs/2505.05542</link>
<guid>https://arxiv.org/abs/2505.05542</guid>
<content:encoded><![CDATA[
arXiv:2505.05542v1 Announce Type: cross 
Abstract: For scientific machine learning tasks with a lot of custom code, picking the right Automatic Differentiation (AD) system matters. Our Julia package DifferentiationInterface.jl provides a common frontend to a dozen AD backends, unlocking easy comparison and modular development. In particular, its built-in preparation mechanism leverages the strengths of each backend by amortizing one-time computations. This is key to enabling sophisticated features like sparsity handling without putting additional burdens on the user.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine learning automorphic forms for black holes</title>
<link>https://arxiv.org/abs/2505.05549</link>
<guid>https://arxiv.org/abs/2505.05549</guid>
<content:encoded><![CDATA[
arXiv:2505.05549v1 Announce Type: cross 
Abstract: Modular, Jacobi, and mock-modular forms serve as generating functions for BPS black hole degeneracies. By training feed-forward neural networks on Fourier coefficients of automorphic forms derived from the Dedekind eta function, Eisenstein series, and Jacobi theta functions, we demonstrate that machine learning techniques can accurately predict modular weights from truncated expansions. Our results reveal strong performance for negative weight modular and quasi-modular forms, particularly those arising in exact black hole counting formulae, with lower accuracy for positive weights and more complicated combinations of Jacobi theta functions. This study establishes a proof of concept for using machine learning to identify how data is organized in terms of modular symmetries in gravitational systems and suggests a pathway toward automated detection and verification of symmetries in quantum gravity.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRIMG : Efficient LLM-driven Test Generation Using Mutant Prioritization</title>
<link>https://arxiv.org/abs/2505.05584</link>
<guid>https://arxiv.org/abs/2505.05584</guid>
<content:encoded><![CDATA[
arXiv:2505.05584v1 Announce Type: cross 
Abstract: Mutation testing is a widely recognized technique for assessing and enhancing the effectiveness of software test suites by introducing deliberate code mutations. However, its application often results in overly large test suites, as developers generate numerous tests to kill specific mutants, increasing computational overhead. This paper introduces PRIMG (Prioritization and Refinement Integrated Mutation-driven Generation), a novel framework for incremental and adaptive test case generation for Solidity smart contracts. PRIMG integrates two core components: a mutation prioritization module, which employs a machine learning model trained on mutant subsumption graphs to predict the usefulness of surviving mutants, and a test case generation module, which utilizes Large Language Models (LLMs) to generate and iteratively refine test cases to achieve syntactic and behavioral correctness.
  We evaluated PRIMG on real-world Solidity projects from Code4Arena to assess its effectiveness in improving mutation scores and generating high-quality test cases. The experimental results demonstrate that PRIMG significantly reduces test suite size while maintaining high mutation coverage. The prioritization module consistently outperformed random mutant selection, enabling the generation of high-impact tests with reduced computational effort. Furthermore, the refining process enhanced the correctness and utility of LLM-generated tests, addressing their inherent limitations in handling edge cases and complex program logic.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flight Validation of Learning-Based Trajectory Optimization for the Astrobee Free-Flyer</title>
<link>https://arxiv.org/abs/2505.05588</link>
<guid>https://arxiv.org/abs/2505.05588</guid>
<content:encoded><![CDATA[
arXiv:2505.05588v1 Announce Type: cross 
Abstract: Although widely used in commercial and industrial robotics, trajectory optimization has seen limited use in space applications due to its high computational demands. In this work, we present flight results from experiments with the Astrobee free-flying robot on board the International Space Station (ISS), that demonstrate how machine learning can accelerate on-board trajectory optimization while preserving theoretical solver guarantees. To the best of the authors' knowledge, this is the first-ever demonstration of learning-based control on the ISS. Our approach leverages the GuSTO sequential convex programming framework and uses a neural network, trained offline, to map problem parameters to effective initial ``warm-start'' trajectories, paving the way for faster real-time optimization on resource-constrained space platforms.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation</title>
<link>https://arxiv.org/abs/2505.05589</link>
<guid>https://arxiv.org/abs/2505.05589</guid>
<content:encoded><![CDATA[
arXiv:2505.05589v1 Announce Type: cross 
Abstract: Reactive dance generation (RDG) produces follower movements conditioned on guiding dancer and music while ensuring spatial coordination and temporal coherence. However, existing methods overemphasize global constraints and optimization, overlooking local information, such as fine-grained spatial interactions and localized temporal context. Therefore, we present ReactDance, a novel diffusion-based framework for high-fidelity RDG with long-term coherence and multi-scale controllability. Unlike existing methods that struggle with interaction fidelity, synchronization, and temporal consistency in duet synthesis, our approach introduces two key innovations: 1)Group Residual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion representation that captures interaction semantics from coarse body rhythms to fine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling strategy eliminating error accumulation in long sequence generation via local block causal masking and periodic positional encoding. Built on the decoupled multi-scale GRFSQ representation, we implement a diffusion model withLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control over motion semantics across scales. Extensive experiments on standard benchmarks demonstrate that ReactDance surpasses existing methods, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Drive Anywhere with Model-Based Reannotation11</title>
<link>https://arxiv.org/abs/2505.05592</link>
<guid>https://arxiv.org/abs/2505.05592</guid>
<content:encoded><![CDATA[
arXiv:2505.05592v1 Announce Type: cross 
Abstract: Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading Under Uncertainty: A Distribution-Based Strategy for Futures Markets Using FutureQuant Transformer</title>
<link>https://arxiv.org/abs/2505.05595</link>
<guid>https://arxiv.org/abs/2505.05595</guid>
<content:encoded><![CDATA[
arXiv:2505.05595v1 Announce Type: cross 
Abstract: In the complex landscape of traditional futures trading, where vast data and variables like real-time Limit Order Books (LOB) complicate price predictions, we introduce the FutureQuant Transformer model, leveraging attention mechanisms to navigate these challenges. Unlike conventional models focused on point predictions, the FutureQuant model excels in forecasting the range and volatility of future prices, thus offering richer insights for trading strategies. Its ability to parse and learn from intricate market patterns allows for enhanced decision-making, significantly improving risk management and achieving a notable average gain of 0.1193% per 30-minute trade over state-of-the-art models with a simple algorithm using factors such as RSI, ATR, and Bollinger Bands. This innovation marks a substantial leap forward in predictive analytics within the volatile domain of futures trading.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Language Models with Faster Code Preprocessing for Vulnerability Detection</title>
<link>https://arxiv.org/abs/2505.05600</link>
<guid>https://arxiv.org/abs/2505.05600</guid>
<content:encoded><![CDATA[
arXiv:2505.05600v1 Announce Type: cross 
Abstract: The application of Artificial Intelligence has become a powerful approach to detecting software vulnerabilities. However, effective vulnerability detection relies on accurately capturing the semantic structure of code and its contextual relationships. Given that the same functionality can be implemented in various forms, a preprocessing tool that standardizes code representation is important. This tool must be efficient, adaptable across programming languages, and capable of supporting new transformations. To address this challenge, we build on the existing SCoPE framework and introduce SCoPE2, an enhanced version with improved performance. We compare both versions in terms of processing time and memory usage and evaluate their impact on a Large Language Model (LLM) for vulnerability detection. Our results show a 97.3\% reduction in processing time with SCoPE2, along with an improved F1-score for the LLM, solely due to the refined preprocessing approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction</title>
<link>https://arxiv.org/abs/2505.05612</link>
<guid>https://arxiv.org/abs/2505.05612</guid>
<content:encoded><![CDATA[
arXiv:2505.05612v1 Announce Type: cross 
Abstract: Drug resistance presents a major challenge in cancer therapy. Single cell profiling offers insights into cellular heterogeneity, yet the application of large-scale foundation models for predicting drug response in single cell data remains underexplored. To address this, we developed scDrugMap, an integrated framework featuring both a Python command-line interface and a web server for drug response prediction. scDrugMap evaluates a wide range of foundation models, including eight single-cell models and two large language models, using a curated dataset of over 326,000 cells in the primary collection and 18,800 cells in the validation set, spanning 36 datasets and diverse tissue and cancer types. We benchmarked model performance under pooled-data and cross-data evaluation settings, employing both layer freezing and Low-Rank Adaptation (LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation achieved the best performance, with mean F1 scores of 0.971 (layer freezing) and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%. In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774), while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap provides the first large-scale benchmark of foundation models for drug response prediction in single-cell data and serves as a user-friendly, flexible platform for advancing drug discovery and translational research.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Regret of Bernoulli Bandits under Global Differential Privacy</title>
<link>https://arxiv.org/abs/2505.05613</link>
<guid>https://arxiv.org/abs/2505.05613</guid>
<content:encoded><![CDATA[
arXiv:2505.05613v1 Announce Type: cross 
Abstract: As sequential learning algorithms are increasingly applied to real life, ensuring data privacy while maintaining their utilities emerges as a timely question. In this context, regret minimisation in stochastic bandits under $\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike bandits without DP, there is a significant gap between the best-known regret lower and upper bound in this setting, though they "match" in order. Thus, we revisit the regret lower and upper bounds of $\epsilon$-global DP algorithms for Bernoulli bandits and improve both. First, we prove a tighter regret lower bound involving a novel information-theoretic quantity characterising the hardness of $\epsilon$-global DP in stochastic bandits. Our lower bound strictly improves on the existing ones across all $\epsilon$ values. Then, we choose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED, and propose their DP versions using a unified blueprint, i.e., (a) running in arm-dependent phases, and (b) adding Laplace noise to achieve privacy. For Bernoulli bandits, we analyse the regrets of these algorithms and show that their regrets asymptotically match our lower bound up to a constant arbitrary close to 1. This refutes the conjecture that forgetting past rewards is necessary to design optimal bandit algorithms under global DP. At the core of our algorithms lies a new concentration inequality for sums of Bernoulli variables under Laplace mechanism, which is a new DP version of the Chernoff bound. This result is universally useful as the DP literature commonly treats the concentrations of Laplace noise and random variables separately, while we couple them to yield a tighter bound.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for enzymatic reaction prediction and characterization</title>
<link>https://arxiv.org/abs/2505.05616</link>
<guid>https://arxiv.org/abs/2505.05616</guid>
<content:encoded><![CDATA[
arXiv:2505.05616v1 Announce Type: cross 
Abstract: Predicting enzymatic reactions is crucial for applications in biocatalysis, metabolic engineering, and drug discovery, yet it remains a complex and resource-intensive task. Large Language Models (LLMs) have recently demonstrated remarkable success in various scientific domains, e.g., through their ability to generalize knowledge, reason over complex structures, and leverage in-context learning strategies. In this study, we systematically evaluate the capability of LLMs, particularly the Llama-3.1 family (8B and 70B), across three core biochemical tasks: Enzyme Commission number prediction, forward synthesis, and retrosynthesis. We compare single-task and multitask learning strategies, employing parameter-efficient fine-tuning via LoRA adapters. Additionally, we assess performance across different data regimes to explore their adaptability in low-data settings. Our results demonstrate that fine-tuned LLMs capture biochemical knowledge, with multitask learning enhancing forward- and retrosynthesis predictions by leveraging shared enzymatic information. We also identify key limitations, for example challenges in hierarchical EC classification schemes, highlighting areas for further improvement in LLM-driven biochemical modeling.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for Safeguarding Small Language Models against Quantization-induced Risks and Vulnerabilities</title>
<link>https://arxiv.org/abs/2505.05619</link>
<guid>https://arxiv.org/abs/2505.05619</guid>
<content:encoded><![CDATA[
arXiv:2505.05619v1 Announce Type: cross 
Abstract: The growing adoption of Large Language Models (LLMs) has influenced the development of their lighter counterparts-Small Language Models (SLMs)-to enable on-device deployment across smartphones and edge devices. These SLMs offer enhanced privacy, reduced latency, server-free functionality, and improved user experience. However, due to resource constraints of on-device environment, SLMs undergo size optimization through compression techniques like quantization, which can inadvertently introduce fairness, ethical and privacy risks. Critically, quantized SLMs may respond to harmful queries directly, without requiring adversarial manipulation, raising significant safety and trust concerns.
  To address this, we propose LiteLMGuard (LLMG), an on-device prompt guard that provides real-time, prompt-level defense for quantized SLMs. Additionally, our prompt guard is designed to be model-agnostic such that it can be seamlessly integrated with any SLM, operating independently of underlying architectures. Our LLMG formalizes prompt filtering as a deep learning (DL)-based prompt answerability classification task, leveraging semantic understanding to determine whether a query should be answered by any SLM. Using our curated dataset, Answerable-or-Not, we trained and fine-tuned several DL models and selected ELECTRA as the candidate, with 97.75% answerability classification accuracy.
  Our safety effectiveness evaluations demonstrate that LLMG defends against over 87% of harmful prompts, including both direct instruction and jailbreak attack strategies. We further showcase its ability to mitigate the Open Knowledge Attacks, where compromised SLMs provide unsafe responses without adversarial prompting. In terms of prompt filtering effectiveness, LLMG achieves near state-of-the-art filtering accuracy of 94%, with an average latency of 135 ms, incurring negligible overhead for users.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation</title>
<link>https://arxiv.org/abs/2505.05648</link>
<guid>https://arxiv.org/abs/2505.05648</guid>
<content:encoded><![CDATA[
arXiv:2505.05648v1 Announce Type: cross 
Abstract: In this paper we train a transformer using differential privacy (DP) for language modeling in SwiftKey. We run multiple experiments to balance the trade-off between the model size, run-time speed and accuracy. We show that we get small and consistent gains in the next-word-prediction and accuracy with graceful increase in memory and speed compared to the production GRU. This is obtained by scaling down a GPT2 architecture to fit the required size and a two stage training process that builds a seed model on general data and DP finetunes it on typing data. The transformer is integrated using ONNX offering both flexibility and efficiency.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Blind Speech Separation with a Diffusion Prior</title>
<link>https://arxiv.org/abs/2505.05657</link>
<guid>https://arxiv.org/abs/2505.05657</guid>
<content:encoded><![CDATA[
arXiv:2505.05657v1 Announce Type: cross 
Abstract: Blind Speech Separation (BSS) aims to separate multiple speech sources from audio mixtures recorded by a microphone array. The problem is challenging because it is a blind inverse problem, i.e., the microphone array geometry, the room impulse response (RIR), and the speech sources, are all unknown. We propose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic, and generative manner. The core idea builds on diffusion posterior sampling (DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must approximate the likelihood by formulating a separate optimization problem. The solution to the optimization approximates room acoustics and the relative transfer functions between microphones. These approximations, along with the diffusion priors, iterate through the ArrayDPS sampling process and ultimately yield separated voice sources. We only need a simple single-speaker speech diffusion model as a prior along with the mixtures recorded at the microphones; no microphone array information is necessary. Evaluation results show that ArrayDPS outperforms all baseline unsupervised methods while being comparable to supervised methods in terms of SDR. Audio demos are provided at: https://arraydps.github.io/ArrayDPSDemo/.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models</title>
<link>https://arxiv.org/abs/2505.05659</link>
<guid>https://arxiv.org/abs/2505.05659</guid>
<content:encoded><![CDATA[
arXiv:2505.05659v1 Announce Type: cross 
Abstract: EfficientNet models are convolutional neural networks optimized for parameter allocation by jointly balancing network width, depth, and resolution. Renowned for their exceptional accuracy, these models have become a standard for image classification tasks across diverse computer vision benchmarks. While traditional neural networks learn correlations between feature channels during training, vector-valued neural networks inherently treat multidimensional data as coherent entities, taking for granted the inter-channel relationships. This paper introduces vector-valued EfficientNets (V-EfficientNets), a novel extension of EfficientNet designed to process arbitrary vector-valued data. The proposed models are evaluated on a medical image classification task, achieving an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency, significantly reducing parameters while outperforming state-of-the-art models, including the original EfficientNet. The source code is available at https://github.com/mevalle/v-nets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompted Meta-Learning for Few-shot Knowledge Graph Completion</title>
<link>https://arxiv.org/abs/2505.05684</link>
<guid>https://arxiv.org/abs/2505.05684</guid>
<content:encoded><![CDATA[
arXiv:2505.05684v1 Announce Type: cross 
Abstract: Few-shot knowledge graph completion (KGC) has obtained significant attention due to its practical applications in real-world scenarios, where new knowledge often emerges with limited available data. While most existing methods for few-shot KGC have predominantly focused on leveraging relational information, rich semantics inherent in KGs have been largely overlooked. To address this gap, we propose a novel prompted meta-learning (PromptMeta) framework that seamlessly integrates meta-semantics with relational information for few-shot KGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that captures and consolidates high-level meta-semantics, enabling effective knowledge transfer and adaptation to rare and newly emerging relations. (2) a learnable fusion prompt that dynamically combines meta-semantic information with task-specific relational information tailored to different few-shot tasks. Both components are optimized together with model parameters within a meta-learning framework. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology</title>
<link>https://arxiv.org/abs/2505.05689</link>
<guid>https://arxiv.org/abs/2505.05689</guid>
<content:encoded><![CDATA[
arXiv:2505.05689v1 Announce Type: cross 
Abstract: Histopathology evaluation of tissue specimens through microscopic examination is essential for accurate disease diagnosis and prognosis. However, traditional manual analysis by specially trained pathologists is time-consuming, labor-intensive, cost-inefficient, and prone to inter-rater variability, potentially affecting diagnostic consistency and accuracy. As digital pathology images continue to proliferate, there is a pressing need for automated analysis to address these challenges. Recent advancements in artificial intelligence-based tools such as machine learning (ML) models, have significantly enhanced the precision and efficiency of analyzing histopathological slides. However, despite their impressive performance, ML models are invariant only to translation, lacking invariance to rotation and reflection. This limitation restricts their ability to generalize effectively, particularly in histopathology, where images intrinsically lack meaningful orientation. In this study, we develop robust, equivariant histopathological biomarkers through a novel symmetric convolutional kernel via unsupervised segmentation. The approach is validated using prostate tissue micro-array (TMA) images from 50 patients in the Gleason 2019 Challenge public dataset. The biomarkers extracted through this approach demonstrate enhanced robustness and generalizability against rotation compared to models using standard convolution kernels, holding promise for enhancing the accuracy, consistency, and robustness of ML models in digital pathology. Ultimately, this work aims to improve diagnostic and prognostic capabilities of histopathology beyond prostate cancer through equivariant imaging.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed Temporal Difference Metric Learning for Robot Motion Planning</title>
<link>https://arxiv.org/abs/2505.05691</link>
<guid>https://arxiv.org/abs/2505.05691</guid>
<content:encoded><![CDATA[
arXiv:2505.05691v1 Announce Type: cross 
Abstract: The motion planning problem involves finding a collision-free path from a robot's starting to its target configuration. Recently, self-supervised learning methods have emerged to tackle motion planning problems without requiring expensive expert demonstrations. They solve the Eikonal equation for training neural networks and lead to efficient solutions. However, these methods struggle in complex environments because they fail to maintain key properties of the Eikonal equation, such as optimal value functions and geodesic distances. To overcome these limitations, we propose a novel self-supervised temporal difference metric learning approach that solves the Eikonal equation more accurately and enhances performance in solving complex and unseen planning tasks. Our method enforces Bellman's principle of optimality over finite regions, using temporal difference learning to avoid spurious local minima while incorporating metric learning to preserve the Eikonal equation's essential geodesic properties. We demonstrate that our approach significantly outperforms existing self-supervised learning methods in handling complex environments and generalizing to unseen environments, with robot configurations ranging from 2 to 12 degrees of freedom (DOF).
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Stress Detection Reproducibility to Consumer Wearable Sensors</title>
<link>https://arxiv.org/abs/2505.05694</link>
<guid>https://arxiv.org/abs/2505.05694</guid>
<content:encoded><![CDATA[
arXiv:2505.05694v1 Announce Type: cross 
Abstract: Wearable sensors are widely used to collect physiological data and develop stress detection models. However, most studies focus on a single dataset, rarely evaluating model reproducibility across devices, populations, or study conditions. We previously assessed the reproducibility of stress detection models across multiple studies, testing models trained on one dataset against others using heart rate (with R-R interval) and electrodermal activity (EDA). In this study, we extended our stress detection reproducibility to consumer wearable sensors. We compared validated research-grade devices, to consumer wearables - Biopac MP160, Polar H10, Empatica E4, to the Garmin Forerunner 55s, assessing device-specific stress detection performance by conducting a new stress study on undergraduate students. Thirty-five students completed three standardized stress-induction tasks in a lab setting. Biopac MP160 performed the best, being consistent with our expectations of it as the gold standard, though performance varied across devices and models. Combining heart rate variability (HRV) and EDA enhanced stress prediction across most scenarios. However, Empatica E4 showed variability; while HRV and EDA improved stress detection in leave-one-subject-out (LOSO) evaluations (AUROC up to 0.953), device-specific limitations led to underperformance when tested with our pre-trained stress detection tool (AUROC 0.723), highlighting generalizability challenges related to hardware-model compatibility. Garmin Forerunner 55s demonstrated strong potential for real-world stress monitoring, achieving the best mental arithmetic stress detection performance in LOSO (AUROC up to 0.961) comparable to research-grade devices like Polar H10 (AUROC 0.954), and Empatica E4 (AUROC 0.905 with HRV-only model and AUROC 0.953 with HRV+EDA model), with the added advantage of consumer-friendly wearability for free-living contexts.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.05701</link>
<guid>https://arxiv.org/abs/2505.05701</guid>
<content:encoded><![CDATA[
arXiv:2505.05701v1 Announce Type: cross 
Abstract: Offline reinforcement learning (RL) aims to learn a policy from a static dataset without further interactions with the environment. Collecting sufficiently large datasets for offline RL is exhausting since this data collection requires colossus interactions with environments and becomes tricky when the interaction with the environment is restricted. Hence, how an agent learns the best policy with a minimal static dataset is a crucial issue in offline RL, similar to the sample efficiency problem in online RL. In this paper, we propose a simple yet effective plug-and-play pretraining method to initialize a feature of a $Q$-network to enhance data efficiency in offline RL. Specifically, we introduce a shared $Q$-network structure that outputs predictions of the next state and $Q$-value. We pretrain the shared $Q$-network through a supervised regression task that predicts a next state and trains the shared $Q$-network using diverse offline RL methods. Through extensive experiments, we empirically demonstrate that our method enhances the performance of existing popular offline RL methods on the D4RL, Robomimic and V-D4RL benchmarks. Furthermore, we show that our method significantly boosts data-efficient offline RL across various data qualities and data distributions trough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of the dataset outperforms standard algorithms even with full datasets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Stragglers in Large Model Training Using What-if Analysis</title>
<link>https://arxiv.org/abs/2505.05713</link>
<guid>https://arxiv.org/abs/2505.05713</guid>
<content:encoded><![CDATA[
arXiv:2505.05713v1 Announce Type: cross 
Abstract: Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
<link>https://arxiv.org/abs/2505.05736</link>
<guid>https://arxiv.org/abs/2505.05736</guid>
<content:encoded><![CDATA[
arXiv:2505.05736v1 Announce Type: cross 
Abstract: The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate its effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite relying on text input only, the MINT-derived model outperforms models trained with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</title>
<link>https://arxiv.org/abs/2505.05752</link>
<guid>https://arxiv.org/abs/2505.05752</guid>
<content:encoded><![CDATA[
arXiv:2505.05752v1 Announce Type: cross 
Abstract: Automation can play a prominent role in improving efficiency, accuracy, and scalability in infrastructure surveying and assessing construction and compliance standards. This paper presents a framework for automation of geometric measurements and compliance assessment using point cloud data. The proposed approach integrates deep learning-based detection and segmentation, in conjunction with geometric and signal processing techniques, to automate surveying tasks. As a proof of concept, we apply this framework to automatically evaluate the compliance of curb ramps with the Americans with Disabilities Act (ADA), demonstrating the utility of point cloud data in survey automation. The method leverages a newly collected, large annotated dataset of curb ramps, made publicly available as part of this work, to facilitate robust model training and evaluation. Experimental results, including comparison with manual field measurements of several ramps, validate the accuracy and reliability of the proposed method, highlighting its potential to significantly reduce manual effort and improve consistency in infrastructure assessment. Beyond ADA compliance, the proposed framework lays the groundwork for broader applications in infrastructure surveying and automated construction evaluation, promoting wider adoption of point cloud data in these domains. The annotated database, manual ramp survey data, and developed algorithms are publicly available on the project's GitHub page: https://github.com/Soltanilara/SurveyAutomation.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Embodiment Scaling Laws in Robot Locomotion</title>
<link>https://arxiv.org/abs/2505.05753</link>
<guid>https://arxiv.org/abs/2505.05753</guid>
<content:encoded><![CDATA[
arXiv:2505.05753v1 Announce Type: cross 
Abstract: Developing generalist agents that can operate across diverse tasks, environments, and physical embodiments is a grand challenge in robotics and artificial intelligence. In this work, we focus on the axis of embodiment and investigate embodiment scaling laws$\unicode{x2013}$the hypothesis that increasing the number of training embodiments improves generalization to unseen ones. Using robot locomotion as a test bed, we procedurally generate a dataset of $\sim$1,000 varied embodiments, spanning humanoids, quadrupeds, and hexapods, and train generalist policies capable of handling diverse observation and action spaces on random subsets. We find that increasing the number of training embodiments improves generalization to unseen ones, and scaling embodiments is more effective in enabling embodiment-level generalization than scaling data on small, fixed sets of embodiments. Notably, our best policy, trained on the full dataset, zero-shot transfers to novel embodiments in the real world, such as Unitree Go2 and H1. These results represent a step toward general embodied intelligence, with potential relevance to adaptive control for configurable robots, co-design of morphology and control, and beyond.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions</title>
<link>https://arxiv.org/abs/2505.05755</link>
<guid>https://arxiv.org/abs/2505.05755</guid>
<content:encoded><![CDATA[
arXiv:2505.05755v1 Announce Type: cross 
Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one ``from left to right,'' have achieved significant success across a wide range of sequence generation tasks. However, they struggle to accurately represent sequences that require satisfying sophisticated constraints or whose sequential dependencies are better addressed by out-of-order generation. Masked Diffusion Models (MDMs) address some of these limitations, but the process of unmasking multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs cannot handle arbitrary infilling constraints when the number of tokens to be filled in is not known in advance. In this work, we introduce Insertion Language Models (ILMs), which learn to insert tokens at arbitrary positions in a sequence -- that is, they select jointly both the position and the vocabulary element to be inserted. By inserting tokens one at a time, ILMs can represent strong dependencies between tokens, and their ability to generate sequences in arbitrary order allows them to accurately model sequences where token dependencies do not follow a left-to-right sequential structure. To train ILMs, we propose a tailored network parameterization and use a simple denoising objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs and MDMs on common planning tasks. Furthermore, we show that ILMs outperform MDMs and perform on par with ARMs in an unconditional text generation task while offering greater flexibility than MDMs in arbitrary-length text infilling.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM</title>
<link>https://arxiv.org/abs/2505.05772</link>
<guid>https://arxiv.org/abs/2505.05772</guid>
<content:encoded><![CDATA[
arXiv:2505.05772v1 Announce Type: cross 
Abstract: Transformer-based models are the foundation of modern machine learning, but their execution, particularly during autoregressive decoding in large language models (LLMs), places significant pressure on memory systems due to frequent memory accesses and growing key-value (KV) caches. This creates a bottleneck in memory bandwidth, especially as context lengths increase. Processing-in-memory (PIM) architectures are a promising solution, offering high internal bandwidth and compute parallelism near memory. However, current PIM designs are primarily optimized for dense attention and struggle with the dynamic, irregular access patterns introduced by modern KV cache sparsity techniques. Consequently, they suffer from workload imbalance, reducing throughput and resource utilization. In this work, we propose STARC, a novel sparsity-optimized data mapping scheme tailored specifically for efficient LLM decoding on PIM architectures. STARC clusters KV pairs by semantic similarity and maps them to contiguous memory regions aligned with PIM bank structures. During decoding, queries retrieve relevant tokens at cluster granularity by matching against precomputed centroids, enabling selective attention and parallel processing without frequent reclustering or data movement overhead. Experiments on the HBM-PIM system show that, compared to common token-wise sparsity methods, STARC reduces attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a KV cache budget of 1024, it achieves up to 54%--74% latency reduction and 45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC maintains model accuracy comparable to state-of-the-art sparse attention methods, demonstrating its effectiveness in enabling efficient and hardware-friendly long-context LLM inference on PIM architectures.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Price of Differential Privacy for Spectral Clustering over Stochastic Block Models</title>
<link>https://arxiv.org/abs/2505.05816</link>
<guid>https://arxiv.org/abs/2505.05816</guid>
<content:encoded><![CDATA[
arXiv:2505.05816v1 Announce Type: cross 
Abstract: We investigate privacy-preserving spectral clustering for community detection within stochastic block models (SBMs). Specifically, we focus on edge differential privacy (DP) and propose private algorithms for community recovery. Our work explores the fundamental trade-offs between the privacy budget and the accurate recovery of community labels. Furthermore, we establish information-theoretic conditions that guarantee the accuracy of our methods, providing theoretical assurances for successful community recovery under edge DP.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition</title>
<link>https://arxiv.org/abs/2505.05829</link>
<guid>https://arxiv.org/abs/2505.05829</guid>
<content:encoded><![CDATA[
arXiv:2505.05829v1 Announce Type: cross 
Abstract: Diffusion transformer (DiT) models have achieved remarkable success in image generation, thanks for their exceptional generative capabilities and scalability. Nonetheless, the iterative nature of diffusion models (DMs) results in high computation complexity, posing challenges for deployment. Although existing cache-based acceleration methods try to utilize the inherent temporal similarity to skip redundant computations of DiT, the lack of correction may induce potential quality degradation. In this paper, we propose increment-calibrated caching, a training-free method for DiT acceleration, where the calibration parameters are generated from the pre-trained model itself with low-rank approximation. To deal with the possible correction failure arising from outlier activations, we introduce channel-aware Singular Value Decomposition (SVD), which further strengthens the calibration effect. Experimental results show that our method always achieve better performance than existing naive caching methods with a similar computation resource budget. When compared with 35-step DDIM, our method eliminates more than 45% computation and improves IS by 12 at the cost of less than 0.06 FID increase. Code is available at https://github.com/ccccczzy/icc.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DaringFed: A Dynamic Bayesian Persuasion Pricing for Online Federated Learning under Two-sided Incomplete Information</title>
<link>https://arxiv.org/abs/2505.05842</link>
<guid>https://arxiv.org/abs/2505.05842</guid>
<content:encoded><![CDATA[
arXiv:2505.05842v1 Announce Type: cross 
Abstract: Online Federated Learning (OFL) is a real-time learning paradigm that sequentially executes parameter aggregation immediately for each random arriving client. To motivate clients to participate in OFL, it is crucial to offer appropriate incentives to offset the training resource consumption. However, the design of incentive mechanisms in OFL is constrained by the dynamic variability of Two-sided Incomplete Information (TII) concerning resources, where the server is unaware of the clients' dynamically changing computational resources, while clients lack knowledge of the real-time communication resources allocated by the server. To incentivize clients to participate in training by offering dynamic rewards to each arriving client, we design a novel Dynamic Bayesian persuasion pricing for online Federated learning (DaringFed) under TII. Specifically, we begin by formulating the interaction between the server and clients as a dynamic signaling and pricing allocation problem within a Bayesian persuasion game, and then demonstrate the existence of a unique Bayesian persuasion Nash equilibrium. By deriving the optimal design of DaringFed under one-sided incomplete information, we further analyze the approximate optimal design of DaringFed with a specific bound under TII. Finally, extensive evaluation conducted on real datasets demonstrate that DaringFed optimizes accuracy and converges speed by 16.99%, while experiments with synthetic datasets validate the convergence of estimate unknown values and the effectiveness of DaringFed in improving the server's utility by up to 12.6%.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry</title>
<link>https://arxiv.org/abs/2505.05845</link>
<guid>https://arxiv.org/abs/2505.05845</guid>
<content:encoded><![CDATA[
arXiv:2505.05845v1 Announce Type: cross 
Abstract: Knots in wood are critical to both aesthetics and structural integrity, making their detection and pairing essential in timber processing. However, traditional manual annotation was labor-intensive and inefficient, necessitating automation. This paper proposes a lightweight and fully automated pipeline for knot detection and pairing based on machine learning techniques. In the detection stage, high-resolution surface images of wooden boards were collected using industrial-grade cameras, and a large-scale dataset was manually annotated and preprocessed. After the transfer learning, the YOLOv8l achieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were analyzed and paired based on multidimensional feature extraction. A triplet neural network was used to map the features into a latent space, enabling clustering algorithms to identify and pair corresponding knots. The triplet network with learnable weights achieved a pairing accuracy of 0.85. Further analysis revealed that he distances from the knot's start and end points to the bottom of the wooden board, and the longitudinal coordinates play crucial roles in achieving high pairing accuracy. Our experiments validate the effectiveness of the proposed solution, demonstrating the potential of AI in advancing wood science and industry.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization</title>
<link>https://arxiv.org/abs/2505.05851</link>
<guid>https://arxiv.org/abs/2505.05851</guid>
<content:encoded><![CDATA[
arXiv:2505.05851v1 Announce Type: cross 
Abstract: With robots increasingly integrating into human environments, understanding and predicting human motion is essential for safe and efficient interactions. Modern human motion and activity prediction approaches require high quality and quantity of data for training and evaluation, usually collected from motion capture systems, onboard or stationary sensors. Setting up these systems is challenging due to the intricate setup of hardware components, extensive calibration procedures, occlusions, and substantial costs. These constraints make deploying such systems in new and large environments difficult and limit their usability for in-the-wild measurements. In this paper we investigate the possibility to apply the novel Ultra-Wideband (UWB) localization technology as a scalable alternative for human motion capture in crowded and occlusion-prone environments. We include additional sensing modalities such as eye-tracking, onboard robot LiDAR and radar sensors, and record motion capture data as ground truth for evaluation and comparison. The environment imitates a museum setup, with up to four active participants navigating toward random goals in a natural way, and offers more than 130 minutes of multi-modal data. Our investigation provides a step toward scalable and accurate motion data collection beyond vision-based systems, laying a foundation for evaluating sensing modalities like UWB in larger and complex environments like warehouses, airports, or convention centers.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Taxonomy of Attacks and Defenses in Split Learning</title>
<link>https://arxiv.org/abs/2505.05872</link>
<guid>https://arxiv.org/abs/2505.05872</guid>
<content:encoded><![CDATA[
arXiv:2505.05872v1 Announce Type: cross 
Abstract: Split Learning (SL) has emerged as a promising paradigm for distributed deep learning, allowing resource-constrained clients to offload portions of their model computation to servers while maintaining collaborative learning. However, recent research has demonstrated that SL remains vulnerable to a range of privacy and security threats, including information leakage, model inversion, and adversarial attacks. While various defense mechanisms have been proposed, a systematic understanding of the attack landscape and corresponding countermeasures is still lacking. In this study, we present a comprehensive taxonomy of attacks and defenses in SL, categorizing them along three key dimensions: employed strategies, constraints, and effectiveness. Furthermore, we identify key open challenges and research gaps in SL based on our systematization, highlighting potential future directions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Register and CLS tokens yield a decoupling of local and global features in large ViTs</title>
<link>https://arxiv.org/abs/2505.05892</link>
<guid>https://arxiv.org/abs/2505.05892</guid>
<content:encoded><![CDATA[
arXiv:2505.05892v1 Announce Type: cross 
Abstract: Recent work has shown that the attention maps of the widely popular DINOv2 model exhibit artifacts, which hurt both model interpretability and performance on dense image tasks. These artifacts emerge due to the model repurposing patch tokens with redundant local information for the storage of global image information. To address this problem, additional register tokens have been incorporated in which the model can store such information instead. We carefully examine the influence of these register tokens on the relationship between global and local image features, showing that while register tokens yield cleaner attention maps, these maps do not accurately reflect the integration of local image information in large models. Instead, global information is dominated by information extracted from register tokens, leading to a disconnect between local and global features. Inspired by these findings, we show that the CLS token itself, which can be interpreted as a register, leads to a very similar phenomenon in models without explicit register tokens. Our work shows that care must be taken when interpreting attention maps of large ViTs. Further, by clearly attributing the faulty behaviour to register and CLS tokens, we show a path towards more interpretable vision models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization</title>
<link>https://arxiv.org/abs/2505.05893</link>
<guid>https://arxiv.org/abs/2505.05893</guid>
<content:encoded><![CDATA[
arXiv:2505.05893v1 Announce Type: cross 
Abstract: Recent advances in Protein Structure Prediction Models (PPMs), such as AlphaFold2 and ESMFold, have revolutionized computational biology by achieving unprecedented accuracy in predicting three-dimensional protein folding structures. However, these models face significant scalability challenges, particularly when processing proteins with long amino acid sequences (e.g., sequence length > 1,000). The primary bottleneck that arises from the exponential growth in activation sizes is driven by the unique data structure in PPM, which introduces an additional dimension that leads to substantial memory and computational demands. These limitations have hindered the effective scaling of PPM for real-world applications, such as analyzing large proteins or complex multimers with critical biological and pharmaceutical relevance.
  In this paper, we present LightNobel, the first hardware-software co-designed accelerator developed to overcome scalability limitations on the sequence length in PPM. At the software level, we propose Token-wise Adaptive Activation Quantization (AAQ), which leverages unique token-wise characteristics, such as distogram patterns in PPM activations, to enable fine-grained quantization techniques without compromising accuracy. At the hardware level, LightNobel integrates the multi-precision reconfigurable matrix processing unit (RMPU) and versatile vector processing unit (VVPU) to enable the efficient execution of AAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup and 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100 GPUs, respectively, while maintaining negligible accuracy loss. It also reduces the peak memory requirement up to 120.05x in PPM, enabling scalable processing for proteins with long sequences.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPE: Context-Aware Prompt Perturbation Mechanism with Differential Privacy</title>
<link>https://arxiv.org/abs/2505.05922</link>
<guid>https://arxiv.org/abs/2505.05922</guid>
<content:encoded><![CDATA[
arXiv:2505.05922v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have gained significant popularity due to their remarkable capabilities in text understanding and generation. However, despite their widespread deployment in inference services such as ChatGPT, concerns about the potential leakage of sensitive user data have arisen. Existing solutions primarily rely on privacy-enhancing technologies to mitigate such risks, facing the trade-off among efficiency, privacy, and utility. To narrow this gap, we propose Cape, a context-aware prompt perturbation mechanism based on differential privacy, to enable efficient inference with an improved privacy-utility trade-off. Concretely, we introduce a hybrid utility function that better captures the token similarity. Additionally, we propose a bucketized sampling mechanism to handle large sampling space, which might lead to long-tail phenomenons. Extensive experiments across multiple datasets, along with ablation studies, demonstrate that Cape achieves a better privacy-utility trade-off compared to prior state-of-the-art works.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates</title>
<link>https://arxiv.org/abs/2505.05940</link>
<guid>https://arxiv.org/abs/2505.05940</guid>
<content:encoded><![CDATA[
arXiv:2505.05940v1 Announce Type: cross 
Abstract: Modal methods for simulating vibrations of strings, membranes, and plates are widely used in acoustics and physically informed audio synthesis. However, traditional implementations, particularly for non-linear models like the von K\'arm\'an plate, are computationally demanding and lack differentiability, limiting inverse modelling and real-time applications. We introduce a fast, differentiable, GPU-accelerated modal framework built with the JAX library, providing efficient simulations and enabling gradient-based inverse modelling. Benchmarks show that our approach significantly outperforms CPU and GPU-based implementations, particularly for simulations with many modes. Inverse modelling experiments demonstrate that our approach can recover physical parameters, including tension, stiffness, and geometry, from both synthetic and experimental data. Although fitting physical parameters is more sensitive to initialisation compared to other methods, it provides greater interpretability and more compact parameterisation. The code is released as open source to support future research and applications in differentiable physical modelling and sound synthesis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving 3D Attention via Triplet Squeeze and Excitation Block</title>
<link>https://arxiv.org/abs/2505.05943</link>
<guid>https://arxiv.org/abs/2505.05943</guid>
<content:encoded><![CDATA[
arXiv:2505.05943v1 Announce Type: cross 
Abstract: The emergence of ConvNeXt and its variants has reaffirmed the conceptual and structural suitability of CNN-based models for vision tasks, re-establishing them as key players in image classification in general, and in facial expression recognition (FER) in particular. In this paper, we propose a new set of models that build on these advancements by incorporating a new set of attention mechanisms that combines Triplet attention with Squeeze-and-Excitation (TripSE) in four different variants. We demonstrate the effectiveness of these variants by applying them to the ResNet18, DenseNet and ConvNext architectures to validate their versatility and impact. Our study shows that incorporating a TripSE block in these CNN models boosts their performances, particularly for the ConvNeXt architecture, indicating its utility. We evaluate the proposed mechanisms and associated models across four datasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where ConvNext with TripSE achieves state-of-the-art results with an accuracy of \textbf{78.27\%} on the popular FER2013 dataset, a new feat for this dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2</title>
<link>https://arxiv.org/abs/2505.05946</link>
<guid>https://arxiv.org/abs/2505.05946</guid>
<content:encoded><![CDATA[
arXiv:2505.05946v1 Announce Type: cross 
Abstract: This technical report describes an experiment on autoregressive pre-training of Gemma2 2 billion parameter large language model (LLM) with 10\% on the Lithuanian language component of CulturaX from the point of view of continual learning. We apply elastic weight consolidation (EWC) to the full set of the model's parameters and investigate language understanding benchmarks, consisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande sets (both in English and Lithuanian versions), and perplexity benchmarks. We empirically demonstrate that EWC regularisation allows us not only to mitigate catastrophic forgetting effects but also that it is potentially beneficial for learning of the new task with LLMs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-User Beamforming with Deep Reinforcement Learning in Sensing-Aided Communication</title>
<link>https://arxiv.org/abs/2505.05956</link>
<guid>https://arxiv.org/abs/2505.05956</guid>
<content:encoded><![CDATA[
arXiv:2505.05956v1 Announce Type: cross 
Abstract: Mobile users are prone to experience beam failure due to beam drifting in millimeter wave (mmWave) communications. Sensing can help alleviate beam drifting with timely beam changes and low overhead since it does not need user feedback. This work studies the problem of optimizing sensing-aided communication by dynamically managing beams allocated to mobile users. A multi-beam scheme is introduced, which allocates multiple beams to the users that need an update on the angle of departure (AoD) estimates and a single beam to the users that have satisfied AoD estimation precision. A deep reinforcement learning (DRL) assisted method is developed to optimize the beam allocation policy, relying only upon the sensing echoes. For comparison, a heuristic AoD-based method using approximated Cram\'er-Rao lower bound (CRLB) for allocation is also presented. Both methods require neither user feedback nor prior state evolution information. Results show that the DRL-assisted method achieves a considerable gain in throughput than the conventional beam sweeping method and the AoD-based method, and it is robust to different user speeds.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints</title>
<link>https://arxiv.org/abs/2505.05957</link>
<guid>https://arxiv.org/abs/2505.05957</guid>
<content:encoded><![CDATA[
arXiv:2505.05957v1 Announce Type: cross 
Abstract: While classical convolutional neural networks (CNNs) have revolutionized image classification, the emergence of quantum computing presents new opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs) leverage quantum mechanical properties and hold potential to outperform classical approaches. However, their implementation on current noisy intermediate-scale quantum (NISQ) devices remains challenging due to hardware limitations. In our research, we address this challenge by introducing an encoding scheme that significantly reduces the input dimensionality. We demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to directly process $28\times 28$ pixel MNIST images, eliminating the need for classical dimensionality reduction pre-processing. Additionally, we propose an automated framework based on expressibility, entanglement, and complexity characteristics to identify the building blocks of QCNNs, parameterized quantum circuits (PQCs). Our approach demonstrates advantages in accuracy and convergence speed with a similar parameter count compared to both hybrid QCNNs and classical CNNs. We validated our experiments on IBM's Heron r2 quantum processor, achieving $96.08\%$ classification accuracy, surpassing the $71.74\%$ benchmark of traditional approaches under identical training conditions. These results represent one of the first implementations of image classifications on real quantum hardware and validate the potential of quantum computing in this area.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous Information Networks</title>
<link>https://arxiv.org/abs/2505.05989</link>
<guid>https://arxiv.org/abs/2505.05989</guid>
<content:encoded><![CDATA[
arXiv:2505.05989v1 Announce Type: cross 
Abstract: This study focuses on the problem of path modeling in heterogeneous information networks and proposes a multi-hop path-aware recommendation framework. The method centers on multi-hop paths composed of various types of entities and relations. It models user preferences through three stages: path selection, semantic representation, and attention-based fusion. In the path selection stage, a path filtering mechanism is introduced to remove redundant and noisy information. In the representation learning stage, a sequential modeling structure is used to jointly encode entities and relations, preserving the semantic dependencies within paths. In the fusion stage, an attention mechanism assigns different weights to each path to generate a global user interest representation. Experiments conducted on real-world datasets such as Amazon-Book show that the proposed method significantly outperforms existing recommendation models across multiple evaluation metrics, including HR@10, Recall@10, and Precision@10. The results confirm the effectiveness of multi-hop paths in capturing high-order interaction semantics and demonstrate the expressive modeling capabilities of the framework in heterogeneous recommendation scenarios. This method provides both theoretical and practical value by integrating structural information modeling in heterogeneous networks with recommendation algorithm design. It offers a more expressive and flexible paradigm for learning user preferences in complex data environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection</title>
<link>https://arxiv.org/abs/2505.06003</link>
<guid>https://arxiv.org/abs/2505.06003</guid>
<content:encoded><![CDATA[
arXiv:2505.06003v1 Announce Type: cross 
Abstract: Understanding the decision-making process of machine learning models provides valuable insights into the task, the data, and the reasons behind a model's failures. In this work, we propose a method that performs inherently interpretable predictions through the instance-wise sparsification of input images. To align the sparsification with human perception, we learn the masking in the space of semantically meaningful pixel regions rather than on pixel-level. Additionally, we introduce an explicit way to dynamically determine the required level of sparsity for each instance. We show empirically on semi-synthetic and natural image datasets that our inherently interpretable classifier produces more meaningful, human-understandable predictions than state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation</title>
<link>https://arxiv.org/abs/2505.06027</link>
<guid>https://arxiv.org/abs/2505.06027</guid>
<content:encoded><![CDATA[
arXiv:2505.06027v1 Announce Type: cross 
Abstract: This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects</title>
<link>https://arxiv.org/abs/2505.06030</link>
<guid>https://arxiv.org/abs/2505.06030</guid>
<content:encoded><![CDATA[
arXiv:2505.06030v1 Announce Type: cross 
Abstract: Combining natural language and geometric shapes is an emerging research area with multiple applications in robotics and language-assisted design. A crucial task in this domain is object referent identification, which involves selecting a 3D object given a textual description of the target. Variability in language descriptions and spatial relationships of 3D objects makes this a complex task, increasing the need to better understand the behavior of neural network models in this domain. However, limited research has been conducted in this area. Specifically, when a model makes an incorrect prediction despite being provided with a seemingly correct object description, practitioners are left wondering: "Why is the model wrong?". In this work, we present a method answering this question by generating counterfactual examples. Our method takes a misclassified sample, which includes two objects and a text description, and generates an alternative yet similar formulation that would have resulted in a correct prediction by the model. We have evaluated our approach with data from the ShapeTalk dataset along with three distinct models. Our counterfactual examples maintain the structure of the original description, are semantically similar and meaningful. They reveal weaknesses in the description, model bias and enhance the understanding of the models behavior. Theses insights help practitioners to better interact with systems as well as engineers to improve models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Music Audio Representations With Limited Data</title>
<link>https://arxiv.org/abs/2505.06042</link>
<guid>https://arxiv.org/abs/2505.06042</guid>
<content:encoded><![CDATA[
arXiv:2505.06042v1 Announce Type: cross 
Abstract: Large deep-learning models for music, including those focused on learning general-purpose music audio representations, are often assumed to require substantial training data to achieve high performance. If true, this would pose challenges in scenarios where audio data or annotations are scarce, such as for underrepresented music traditions, non-popular genres, and personalized music creation and listening. Understanding how these models behave in limited-data scenarios could be crucial for developing techniques to tackle them.
  In this work, we investigate the behavior of several music audio representation models under limited-data learning regimes. We consider music models with various architectures, training paradigms, and input durations, and train them on data collections ranging from 5 to 8,000 minutes long. We evaluate the learned representations on various music information retrieval tasks and analyze their robustness to noise. We show that, under certain conditions, representations from limited-data and even random models perform comparably to ones from large-dataset models, though handcrafted features outperform all learned representations in some tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information</title>
<link>https://arxiv.org/abs/2505.06046</link>
<guid>https://arxiv.org/abs/2505.06046</guid>
<content:encoded><![CDATA[
arXiv:2505.06046v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniVLA: Learning to Act Anywhere with Task-centric Latent Actions</title>
<link>https://arxiv.org/abs/2505.06111</link>
<guid>https://arxiv.org/abs/2505.06111</guid>
<content:encoded><![CDATA[
arXiv:2505.06111v1 Announce Type: cross 
Abstract: A generalist robot should perform effectively across various environments. However, most existing approaches heavily rely on scaling action-annotated data to enhance their capabilities. Consequently, they are often limited to single physical specification and struggle to learn transferable knowledge across different embodiments and environments. To confront these limitations, we propose UniVLA, a new framework for learning cross-embodiment vision-language-action (VLA) policies. Our key innovation is to derive task-centric action representations from videos with a latent action model. This enables us to exploit extensive data across a wide spectrum of embodiments and perspectives. To mitigate the effect of task-irrelevant dynamics, we incorporate language instructions and establish a latent action model within the DINO feature space. Learned from internet-scale videos, the generalist policy can be deployed to various robots through efficient latent action decoding. We obtain state-of-the-art results across multiple manipulation and navigation benchmarks, as well as real-robot deployments. UniVLA achieves superior performance over OpenVLA with less than 1/20 of pretraining compute and 1/10 of downstream data. Continuous performance improvements are observed as heterogeneous data, even including human videos, are incorporated into the training pipeline. The results underscore UniVLA's potential to facilitate scalable and efficient robot policy learning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies</title>
<link>https://arxiv.org/abs/2505.06145</link>
<guid>https://arxiv.org/abs/2505.06145</guid>
<content:encoded><![CDATA[
arXiv:2505.06145v1 Announce Type: cross 
Abstract: Few-shot text classification has important application value in low-resource environments. This paper proposes a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to improve the classification performance of Transformer-based models. Experiments on the FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform well in few-shot tasks, especially in the 5-shot setting, which can more effectively capture text features and improve classification accuracy. The experiment also found that there are significant differences in the classification difficulty of different relationship categories. Some categories have fuzzy semantic boundaries or complex feature distributions, making it difficult for the standard cross entropy loss to learn the discriminative information required to distinguish categories. By introducing contrastive loss and regularization loss, the generalization ability of the model is enhanced, effectively alleviating the overfitting problem in few-shot environments. In addition, the research results show that the use of Transformer models or generative architectures with stronger self-attention mechanisms can help improve the stability and accuracy of few-shot classification.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-Augmented Algorithms for Boolean Satisfiability</title>
<link>https://arxiv.org/abs/2505.06146</link>
<guid>https://arxiv.org/abs/2505.06146</guid>
<content:encoded><![CDATA[
arXiv:2505.06146v1 Announce Type: cross 
Abstract: Learning-augmented algorithms are a prominent recent development in beyond worst-case analysis. In this framework, a problem instance is provided with a prediction (``advice'') from a machine-learning oracle, which provides partial information about an optimal solution, and the goal is to design algorithms that leverage this advice to improve worst-case performance. We study the classic Boolean satisfiability (SAT) decision and optimization problems within this framework using two forms of advice. ``Subset advice" provides a random $\epsilon$ fraction of the variables from an optimal assignment, whereas ``label advice" provides noisy predictions for all variables in an optimal assignment.
  For the decision problem $k$-SAT, by using the subset advice we accelerate the exponential running time of the PPSZ family of algorithms due to Paturi, Pudlak, Saks and Zane, which currently represent the state of the art in the worst case. We accelerate the running time by a multiplicative factor of $2^{-c}$ in the base of the exponent, where $c$ is a function of $\epsilon$ and $k$. For the optimization problem, we show how to incorporate subset advice in a black-box fashion with any $\alpha$-approximation algorithm, improving the approximation ratio to $\alpha + (1 - \alpha)\epsilon$. Specifically, we achieve approximations of $0.94 + \Omega(\epsilon)$ for MAX-$2$-SAT, $7/8 + \Omega(\epsilon)$ for MAX-$3$-SAT, and $0.79 + \Omega(\epsilon)$ for MAX-SAT. Moreover, for label advice, we obtain near-optimal approximation for instances with large average degree, thereby generalizing recent results on MAX-CUT and MAX-$2$-LIN.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills</title>
<link>https://arxiv.org/abs/2505.06176</link>
<guid>https://arxiv.org/abs/2505.06176</guid>
<content:encoded><![CDATA[
arXiv:2505.06176v1 Announce Type: cross 
Abstract: Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based Approach</title>
<link>https://arxiv.org/abs/2505.06182</link>
<guid>https://arxiv.org/abs/2505.06182</guid>
<content:encoded><![CDATA[
arXiv:2505.06182v1 Announce Type: cross 
Abstract: Humans make extensive use of haptic exploration to map and identify the properties of the objects that we touch. In robotics, active tactile perception has emerged as an important research domain that complements vision for tasks such as object classification, shape reconstruction, and manipulation. This work introduces TAP (Task-agnostic Active Perception) -- a novel framework that leverages reinforcement learning (RL) and transformer-based architectures to address the challenges posed by partially observable environments. TAP integrates Soft Actor-Critic (SAC) and CrossQ algorithms within a unified optimization objective, jointly training a perception module and decision-making policy. By design, TAP is completely task-agnostic and can, in principle, generalize to any active perception problem. We evaluate TAP across diverse tasks, including toy examples and realistic applications involving haptic exploration of 3D models from the Tactile MNIST benchmark. Experiments demonstrate the efficacy of TAP, achieving high accuracies on the Tactile MNIST haptic digit recognition task and a tactile pose estimation task. These findings underscore the potential of TAP as a versatile and generalizable framework for advancing active tactile perception in robotics.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Concepts</title>
<link>https://arxiv.org/abs/2505.06191</link>
<guid>https://arxiv.org/abs/2505.06191</guid>
<content:encoded><![CDATA[
arXiv:2505.06191v1 Announce Type: cross 
Abstract: This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multi-Task Learning for Multi-Label Power System Security Assessment</title>
<link>https://arxiv.org/abs/2505.06207</link>
<guid>https://arxiv.org/abs/2505.06207</guid>
<content:encoded><![CDATA[
arXiv:2505.06207v1 Announce Type: cross 
Abstract: This paper introduces a novel approach to the power system security assessment using Multi-Task Learning (MTL), and reformulating the problem as a multi-label classification task. The proposed MTL framework simultaneously assesses static, voltage, transient, and small-signal stability, improving both accuracy and interpretability with respect to the most state of the art machine learning methods. It consists of a shared encoder and multiple decoders, enabling knowledge transfer between stability tasks. Experiments on the IEEE 68-bus system demonstrate a measurable superior performance of the proposed method compared to the extant state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine-Learning Compositional Study of Exoplanetary Material Accreted Onto Five Helium-Atmosphere White Dwarfs with $\texttt{cecilia}$</title>
<link>https://arxiv.org/abs/2505.06228</link>
<guid>https://arxiv.org/abs/2505.06228</guid>
<content:encoded><![CDATA[
arXiv:2505.06228v1 Announce Type: cross 
Abstract: We present the first application of the Machine Learning (ML) pipeline $\texttt{cecilia}$ to determine the physical parameters and photospheric composition of five metal-polluted He-atmosphere white dwarfs without well-characterised elemental abundances. To achieve this, we perform a joint and iterative Bayesian fit to their $\textit{SDSS}$ (R=2,000) and $\textit{Keck/ESI}$ (R=4,500) optical spectra, covering the wavelength range from about 3,800\r{A} to 9,000\r{A}. Our analysis measures the abundances of at least two $-$and up to six$-$ chemical elements in their atmospheres with a predictive accuracy similar to that of conventional WD analysis techniques ($\approx$0.20 dex). The white dwarfs with the largest number of detected heavy elements are SDSS J0859$+$5732 and SDSS J2311$-$0041, which simultaneously exhibit O, Mg, Si, Ca, and Fe in their $\textit{Keck/ESI}$ spectra. For all systems, we find that the bulk composition of their pollutants is largely consistent with those of primitive CI chondrites to within 1-2$\sigma$. We also find evidence of statistically significant ($>2\sigma$) oxygen excesses for SDSS J0859$+$5732 and SDSS J2311$-$0041, which could point to the accretion of oxygen-rich exoplanetary material. In the future, as wide-field astronomical surveys deliver millions of public WD spectra to the scientific community, $\texttt{cecilia}$ aspires to unlock population-wide studies of polluted WDs, therefore helping to improve our statistical knowledge of extrasolar compositions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Model Processing Strategies with Non-Negative Per-Example Fisher Factorization</title>
<link>https://arxiv.org/abs/2310.04649</link>
<guid>https://arxiv.org/abs/2310.04649</guid>
<content:encoded><![CDATA[
arXiv:2310.04649v2 Announce Type: replace 
Abstract: We introduce NPEFF (Non-Negative Per-Example Fisher Factorization), an interpretability method that aims to uncover strategies used by a model to generate its predictions. NPEFF decomposes per-example Fisher matrices using a novel decomposition algorithm that learns a set of components represented by learned rank-1 positive semi-definite matrices. Through a combination of human evaluation and automated analysis, we demonstrate that these NPEFF components correspond to model processing strategies for a variety of language models and text processing tasks. We further show how to construct parameter perturbations from NPEFF components to selectively disrupt a given component's role in the model's processing. Along with conducting extensive ablation studies, we include experiments to show how NPEFF can be used to analyze and mitigate collateral effects of unlearning and use NPEFF to study in-context learning. Furthermore, we demonstrate the advantages of NPEFF over baselines such as gradient clustering and using sparse autoencoders for dictionary learning over model activations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner Structure of Networks</title>
<link>https://arxiv.org/abs/2310.19470</link>
<guid>https://arxiv.org/abs/2310.19470</guid>
<content:encoded><![CDATA[
arXiv:2310.19470v3 Announce Type: replace 
Abstract: Grokking is an intriguing phenomenon of delayed generalization, where neural networks initially memorize training data with perfect accuracy but exhibit poor generalization, subsequently transitioning to a generalizing solution with continued training. While factors such as weight norms and sparsity have been proposed to explain this delayed generalization, the influence of network structure remains underexplored. In this work, we link the grokking phenomenon to the lottery ticket hypothesis to investigate the impact of internal network structures. We demonstrate that utilizing lottery tickets obtained during the generalizing phase (termed grokked tickets) significantly reduces delayed generalization across various tasks, including multiple modular arithmetic operations, polynomial regression, sparse parity, and MNIST classification. Through controlled experiments, we show that the mitigation of delayed generalization is not due solely to reduced weight norms or increased sparsity, but rather to the discovery of good subnetworks. Furthermore, we find that grokked tickets exhibit periodic weight patterns, beneficial graph properties such as increased average path lengths and reduced clustering coefficients, and undergo rapid structural changes that coincide with improvements in generalization. Additionally, pruning techniques like the edge-popup algorithm can identify these effective structures without modifying the weights, thereby transforming memorizing networks into generalizing ones. These results underscore the novel insight that structural exploration plays a pivotal role in understanding grokking. The implementation code can be accessed via this link: https://github.com/gouki510/Grokking-Tickets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Multi-modal Feature Alignment for Time Series Representation Learning</title>
<link>https://arxiv.org/abs/2312.05698</link>
<guid>https://arxiv.org/abs/2312.05698</guid>
<content:encoded><![CDATA[
arXiv:2312.05698v2 Announce Type: replace 
Abstract: In recent times, the field of unsupervised representation learning (URL) for time series data has garnered significant interest due to its remarkable adaptability across diverse downstream applications. Unsupervised learning goals differ from downstream tasks, making it tricky to ensure downstream task utility by focusing only on temporal feature characterization. Researchers have proposed multiple transformations to extract discriminative patterns implied in informative time series, trying to fill the gap. Despite the introduction of a variety of feature engineering techniques, e.g. spectral domain, wavelet transformed features, features in image form and symbolic features etc. the utilization of intricate feature fusion methods and dependence on heterogeneous features during inference hampers the scalability of the solutions. To address this, our study introduces an innovative approach that focuses on aligning and binding time series representations encoded from different modalities, inspired by spectral graph theory, thereby guiding the neural encoder to uncover latent pattern associations among these multi-modal features. In contrast to conventional methods that fuse features from multiple modalities, our proposed approach simplifies the neural architecture by retaining a single time series encoder, consequently leading to preserved scalability. We further demonstrate and prove mechanisms for the encoder to maintain better inductive bias. In our experimental evaluation, we validated the proposed method on a diverse set of time series datasets from various domains. Our approach outperforms existing state-of-the-art URL methods across diverse downstream tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Invitation to Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2312.08365</link>
<guid>https://arxiv.org/abs/2312.08365</guid>
<content:encoded><![CDATA[
arXiv:2312.08365v3 Announce Type: replace 
Abstract: Training a deep neural network to maximize a target objective has become the standard recipe for successful machine learning over the last decade. These networks can be optimized with supervised learning, if the target objective is differentiable. For many interesting problems, this is however not the case. Common objectives like intersection over union (IoU), bilingual evaluation understudy (BLEU) score or rewards cannot be optimized with supervised learning. A common workaround is to define differentiable surrogate losses, leading to suboptimal solutions with respect to the actual objective. Reinforcement learning (RL) has emerged as a promising alternative for optimizing deep neural networks to maximize non-differentiable objectives in recent years. Examples include aligning large language models via human feedback, code generation, object detection or control problems. This makes RL techniques relevant to the larger machine learning audience. The subject is, however, time intensive to approach due to the large range of methods, as well as the often very theoretical presentation. In this introduction, we take an alternative approach, different from classic reinforcement learning textbooks. Rather than focusing on tabular problems, we introduce reinforcement learning as a generalization of supervised learning, which we first apply to non-differentiable objectives and later to temporal problems. Assuming only basic knowledge of supervised learning, the reader will be able to understand state-of-the-art deep RL algorithms like proximal policy optimization (PPO) after reading this tutorial.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MotherNet: Fast Training and Inference via Hyper-Network Transformers</title>
<link>https://arxiv.org/abs/2312.08598</link>
<guid>https://arxiv.org/abs/2312.08598</guid>
<content:encoded><![CDATA[
arXiv:2312.08598v2 Announce Type: replace 
Abstract: Foundation models are transforming machine learning across many modalities, with in-context learning replacing classical model training. Recent work on tabular data hints at a similar opportunity to build foundation models for classification for numerical data. However, existing meta-learning approaches can not compete with tree-based methods in terms of inference time. In this paper, we propose MotherNet, a hypernetwork architecture trained on synthetic classification tasks that, once prompted with a never-seen-before training set generates the weights of a trained ``child'' neural-network by in-context learning using a single forward pass. In contrast to most existing hypernetworks that are usually trained for relatively constrained multi-task settings, MotherNet can create models for multiclass classification on arbitrary tabular datasets without any dataset specific gradient descent. The child network generated by MotherNet outperforms neural networks trained using gradient descent on small datasets, and is comparable to predictions by TabPFN and standard ML methods like Gradient Boosting. Unlike a direct application of TabPFN, MotherNet generated networks are highly efficient at inference time. We also demonstrate that HyperFast is unable to perform effective in-context learning on small datasets, and heavily relies on dataset specific fine-tuning and hyper-parameter tuning, while MotherNet requires no fine-tuning or per-dataset hyper-parameters.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Gradient Clipping for Robust Federated Learning</title>
<link>https://arxiv.org/abs/2405.14432</link>
<guid>https://arxiv.org/abs/2405.14432</guid>
<content:encoded><![CDATA[
arXiv:2405.14432v5 Announce Type: replace 
Abstract: Robust federated learning aims to maintain reliable performance despite the presence of adversarial or misbehaving workers. While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods were proven theoretically optimal, their empirical success has often relied on pre-aggregation gradient clipping. However, existing static clipping strategies yield inconsistent results: enhancing robustness against some attacks while being ineffective or even detrimental against others. To address this limitation, we propose a principled adaptive clipping strategy, Adaptive Robust Clipping (ARC), which dynamically adjusts clipping thresholds based on the input gradients. We prove that ARC not only preserves the theoretical robustness guarantees of SOTA Robust-DGD methods but also provably improves asymptotic convergence when the model is well-initialized. Extensive experiments on benchmark image classification tasks confirm these theoretical insights, demonstrating that ARC significantly enhances robustness, particularly in highly heterogeneous and adversarial settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification</title>
<link>https://arxiv.org/abs/2405.15047</link>
<guid>https://arxiv.org/abs/2405.15047</guid>
<content:encoded><![CDATA[
arXiv:2405.15047v2 Announce Type: replace 
Abstract: This paper presents an innovative approach, called credal wrapper, to formulating a credal set representation of model averaging for Bayesian neural networks (BNNs) and deep ensembles (DEs), capable of improving uncertainty estimation in classification tasks. Given a finite collection of single predictive distributions derived from BNNs or DEs, the proposed credal wrapper approach extracts an upper and a lower probability bound per class, acknowledging the epistemic uncertainty due to the availability of a limited amount of distributions. Such probability intervals over classes can be mapped on a convex set of probabilities (a credal set) from which, in turn, a unique prediction can be obtained using a transformation called intersection probability transformation. In this article, we conduct extensive experiments on several out-of-distribution (OOD) detection benchmarks, encompassing various dataset pairs (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C, CIFAR100 vs CIFAR100-C and ImageNet vs ImageNet-O) and using different network architectures (such as VGG16, ResNet-18/50, EfficientNet B2, and ViT Base). Compared to the BNN and DE baselines, the proposed credal wrapper method exhibits superior performance in uncertainty estimation and achieves a lower expected calibration error on corrupted data.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pretraining with Random Noise for Fast and Robust Learning without Weight Transport</title>
<link>https://arxiv.org/abs/2405.16731</link>
<guid>https://arxiv.org/abs/2405.16731</guid>
<content:encoded><![CDATA[
arXiv:2405.16731v2 Announce Type: replace 
Abstract: The brain prepares for learning even before interacting with the environment, by refining and optimizing its structures through spontaneous neural activity that resembles random noise. However, the mechanism of such a process has yet to be thoroughly understood, and it is unclear whether this process can benefit the algorithm of machine learning. Here, we study this issue using a neural network with a feedback alignment algorithm, demonstrating that pretraining neural networks with random noise increases the learning efficiency as well as generalization abilities without weight transport. First, we found that random noise training modifies forward weights to match backward synaptic feedback, which is necessary for teaching errors by feedback alignment. As a result, a network with pre-aligned weights learns notably faster than a network without random noise training, even reaching a convergence speed comparable to that of a backpropagation algorithm. Sequential training with both random noise and data brings weights closer to synaptic feedback than training solely with data, enabling more precise credit assignment and faster learning. We also found that each readout probability approaches the chance level and that the effective dimensionality of weights decreases in a network pretrained with random noise. This pre-regularization allows the network to learn simple solutions of a low rank, reducing the generalization loss during subsequent training. This also enables the network robustly to generalize a novel, out-of-distribution dataset. Lastly, we confirmed that random noise pretraining reduces the amount of meta-loss, enhancing the network ability to adapt to various tasks. Overall, our results suggest that random noise training with feedback alignment offers a straightforward yet effective method of pretraining that facilitates quick and reliable learning without weight transport.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security</title>
<link>https://arxiv.org/abs/2406.09831</link>
<guid>https://arxiv.org/abs/2406.09831</guid>
<content:encoded><![CDATA[
arXiv:2406.09831v2 Announce Type: replace 
Abstract: Federated Learning (FL) offers a promising paradigm for training Large Language Models (LLMs) in a decentralized manner while preserving data privacy and minimizing communication overhead. This survey examines recent advancements in FL-driven LLMs, with a particular emphasis on architectural designs, performance optimization, and security concerns, including the emerging area of machine unlearning. In this context, machine unlearning refers to the systematic removal of specific data contributions from trained models to comply with privacy regulations such as the Right to be Forgotten. We review a range of strategies enabling unlearning in federated LLMs, including perturbation-based methods, model decomposition, and incremental retraining, while evaluating their trade-offs in terms of efficiency, privacy guarantees, and model utility. Through selected case studies and empirical evaluations, we analyze how these methods perform in practical FL scenarios. This survey identifies critical research directions toward developing secure, adaptable, and high-performing federated LLM systems for real-world deployment.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations</title>
<link>https://arxiv.org/abs/2409.17264</link>
<guid>https://arxiv.org/abs/2409.17264</guid>
<content:encoded><![CDATA[
arXiv:2409.17264v3 Announce Type: replace 
Abstract: As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.
  We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).
  Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation of Discrete Diffusion through Dimensional Correlations</title>
<link>https://arxiv.org/abs/2410.08709</link>
<guid>https://arxiv.org/abs/2410.08709</guid>
<content:encoded><![CDATA[
arXiv:2410.08709v4 Announce Type: replace 
Abstract: Diffusion models have demonstrated exceptional performances in various fields of generative modeling, but suffer from slow sampling speed due to their iterative nature. While this issue is being addressed in continuous domains, discrete diffusion models face unique challenges, particularly in capturing dependencies between elements (e.g., pixel relationships in image, sequential dependencies in language) mainly due to the computational cost of processing high-dimensional joint distributions. In this paper, (i) we propose "mixture" models for discrete diffusion that are capable of treating dimensional correlations while remaining scalable, and (ii) we provide a set of loss functions for distilling the iterations of existing models. Two primary theoretical insights underpin our approach: First, conventional models with element-wise independence can well approximate the data distribution, but essentially require {\it many sampling steps}. Second, our loss functions enable the mixture models to distill such many-step conventional models into just a few steps by learning the dimensional correlations. Our experimental results show the effectiveness of the proposed method in distilling pretrained discrete diffusion models across image and language domains. The code used in the paper is available at https://github.com/sony/di4c .
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Algorithms Made Simple</title>
<link>https://arxiv.org/abs/2410.09186</link>
<guid>https://arxiv.org/abs/2410.09186</guid>
<content:encoded><![CDATA[
arXiv:2410.09186v2 Announce Type: replace 
Abstract: In this paper, we discuss learning algorithms and their importance in different types of applications which includes training to identify important patterns and features in a straightforward, easy-to-understand manner. We will review the main concepts of artificial intelligence (AI), machine learning (ML), deep learning (DL), and hybrid models. Some important subsets of Machine Learning algorithms such as supervised, unsupervised, and reinforcement learning are also discussed in this paper. These techniques can be used for some important tasks like prediction, classification, and segmentation. Convolutional Neural Networks (CNNs) are used for image and video processing and many more applications. We dive into the architecture of CNNs and how to integrate CNNs with ML algorithms to build hybrid models. This paper explores the vulnerability of learning algorithms to noise, leading to misclassification. We further discuss the integration of learning algorithms with Large Language Models (LLM) to generate coherent responses applicable to many domains such as healthcare, marketing, and finance by learning important patterns from large volumes of data. Furthermore, we discuss the next generation of learning algorithms and how we may have an unified Adaptive and Dynamic Network to perform important tasks. Overall, this article provides brief overview of learning algorithms, exploring their current state, applications and future direction.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO-Learn: Label-Efficient Graph Open-Set Learning</title>
<link>https://arxiv.org/abs/2410.16386</link>
<guid>https://arxiv.org/abs/2410.16386</guid>
<content:encoded><![CDATA[
arXiv:2410.16386v2 Announce Type: replace 
Abstract: How can we train graph-based models to recognize unseen classes while keeping labeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD) detection aim to address this challenge by training models that can accurately classify known, in-distribution (ID) classes while identifying and handling previously unseen classes during inference. It is critical for high-stakes, real-world applications where models frequently encounter unexpected data, including finance, security, and healthcare. However, current GOL methods assume access to many labeled ID samples, which is unrealistic for large-scale graphs due to high annotation costs. In this paper, we propose LEGO-Learn (Label-Efficient Graph Open-set Learning), a novel framework that tackles open-set node classification on graphs within a given label budget by selecting the most informative ID nodes. LEGO-Learn employs a GNN-based filter to identify and exclude potential OOD nodes and then select highly informative ID nodes for labeling using the K-Medoids algorithm. To prevent the filter from discarding valuable ID examples, we introduce a classifier that differentiates between the C known ID classes and an additional class representing OOD nodes (hence, a C+1 classifier). This classifier uses a weighted cross-entropy loss to balance the removal of OOD nodes while retaining informative ID nodes. Experimental results on four real-world datasets demonstrate that LEGO-Learn significantly outperforms leading methods, with up to a 6.62% improvement in ID classification accuracy and a 7.49% increase in AUROC for OOD detection.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prioritized Generative Replay</title>
<link>https://arxiv.org/abs/2410.18082</link>
<guid>https://arxiv.org/abs/2410.18082</guid>
<content:encoded><![CDATA[
arXiv:2410.18082v2 Announce Type: replace 
Abstract: Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of "relevance functions" that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crash Severity Risk Modeling Strategies under Data Imbalance</title>
<link>https://arxiv.org/abs/2412.02094</link>
<guid>https://arxiv.org/abs/2412.02094</guid>
<content:encoded><![CDATA[
arXiv:2412.02094v2 Announce Type: replace 
Abstract: This study investigates crash severity risk modeling strategies for work zones involving large vehicles (i.e., trucks, buses, and vans) under crash data imbalance between low-severity (LS) and high-severity (HS) crashes. We utilized crash data involving large vehicles in South Carolina work zones from 2014 to 2018, which included four times more LS crashes than HS crashes. The objective of this study is to evaluate the crash severity prediction performance of various statistical, machine learning, and deep learning models under different feature selection and data balancing techniques. Findings highlight a disparity in LS and HS predictions, with lower accuracy for HS crashes due to class imbalance and feature overlap. Discriminative Mutual Information (DMI) yields the most effective feature set for predicting HS crashes without requiring data balancing, particularly when paired with gradient boosting models and deep neural networks such as CatBoost, NeuralNetTorch, XGBoost, and LightGBM. Data balancing techniques such as NearMiss-1 maximize HS recall when combined with DMI-selected features and certain models such as LightGBM, making them well-suited for HS crash prediction. Conversely, RandomUnderSampler, HS Class Weighting, and RandomOverSampler achieve more balanced performance, which is defined as an equitable trade-off between LS and HS metrics, especially when applied to NeuralNetTorch, NeuralNetFastAI, CatBoost, LightGBM, and Bayesian Mixed Logit (BML) using merged feature sets or models without feature selection. The insights from this study offer safety analysts guidance on selecting models, feature selection, and data balancing techniques aligned with specific safety goals, providing a robust foundation for enhancing work-zone crash severity prediction.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation</title>
<link>https://arxiv.org/abs/2501.02704</link>
<guid>https://arxiv.org/abs/2501.02704</guid>
<content:encoded><![CDATA[
arXiv:2501.02704v3 Announce Type: replace 
Abstract: Deep Neural Networks (DNNs) have gained considerable traction in recent years due to the unparalleled results they gathered. However, the cost behind training such sophisticated models is resource intensive, resulting in many to consider DNNs to be intellectual property (IP) to model owners. In this era of cloud computing, high-performance DNNs are often deployed all over the internet so that people can access them publicly. As such, DNN watermarking schemes, especially backdoor-based watermarks, have been actively developed in recent years to preserve proprietary rights. Nonetheless, there lies much uncertainty on the robustness of existing backdoor watermark schemes, towards both adversarial attacks and unintended means such as fine-tuning neural network models. One reason for this is that no complete guarantee of robustness can be assured in the context of backdoor-based watermark. In this paper, we extensively evaluate the persistence of recent backdoor-based watermarks within neural networks in the scenario of fine-tuning, we propose/develop a novel data-driven idea to restore watermark after fine-tuning without exposing the trigger set. Our empirical results show that by solely introducing training data after fine-tuning, the watermark can be restored if model parameters do not shift dramatically during fine-tuning. Depending on the types of trigger samples used, trigger accuracy can be reinstated to up to 100%. Our study further explores how the restoration process works using loss landscape visualization, as well as the idea of introducing training data in fine-tuning stage to alleviate watermark vanishing.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2501.03119</link>
<guid>https://arxiv.org/abs/2501.03119</guid>
<content:encoded><![CDATA[
arXiv:2501.03119v2 Announce Type: replace 
Abstract: Federated Learning (FL) is widely recognized as a privacy-preserving machine learning paradigm due to its model-sharing mechanism that avoids direct data exchange. Nevertheless, model training leaves exploitable traces that can be used to infer sensitive information. In Decentralized FL (DFL), the topology, defining how participants are connected, plays a crucial role in shaping the model's privacy, robustness, and convergence. However, the topology introduces an unexplored vulnerability: attackers can exploit it to infer participant relationships and launch targeted attacks. This work uncovers the hidden risks of DFL topologies by proposing a novel Topology Inference Attack that infers the topology solely from model behavior. A taxonomy of topology inference attacks is introduced, categorizing them by the attacker's capabilities and knowledge. Practical attack strategies are designed for various scenarios, and experiments are conducted to identify key factors influencing attack success. The results demonstrate that analyzing only the model of each node can accurately infer the DFL topology, highlighting a critical privacy risk in DFL systems. These findings offer valuable insights for improving privacy preservation in DFL environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Sparse Kernel Generator for O(3)-Equivariant Deep Networks</title>
<link>https://arxiv.org/abs/2501.13986</link>
<guid>https://arxiv.org/abs/2501.13986</guid>
<content:encoded><![CDATA[
arXiv:2501.13986v4 Announce Type: replace 
Abstract: Rotation equivariant graph neural networks, i.e. networks designed to guarantee certain geometric relations between their inputs and outputs, yield state of the art performance on spatial deep learning tasks. They exhibit high data efficiency during training and significantly reduced inference time for interatomic potential calculations compared to classical approaches. Key to these models is the Clebsch-Gordon (CG) tensor product, a kernel that contracts two dense feature vectors with a highly-structured sparse tensor to produce a dense output vector. The operation, which may be repeated millions of times for typical equivariant models, is a costly and inefficient bottleneck. We introduce a GPU sparse kernel generator for the CG tensor product that provides significant speedups over the best existing open and closed-source implementations. Our implementation achieves high performance by carefully managing the limited GPU shared memory through static analysis at model compile-time, minimizing reads and writes to global memory. We break the tensor product into a series of smaller kernels with operands that fit entirely into registers, enabling us to emit long arithmetic instruction streams that maximize instruction-level parallelism. By fusing the CG tensor product with a subsequent graph convolution, we reduce both intermediate storage and global memory traffic over naive approaches that duplicate input data. We also provide optimized kernels for the gradient of the CG tensor product and a novel identity for the higher partial derivatives required to predict interatomic forces. Our kernels offer up to 1.3x speedup over NVIDIA's closed-source cuEquivariance package, as well as 10x speedup over the widely-used e3nn package. In FP64 precision, we offer up to 6.2x inference-time speedup for the MACE chemistry foundation model over the original unoptimized version.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GDformer: Going Beyond Subsequence Isolation for Multivariate Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2501.18196</link>
<guid>https://arxiv.org/abs/2501.18196</guid>
<content:encoded><![CDATA[
arXiv:2501.18196v2 Announce Type: replace 
Abstract: Unsupervised anomaly detection of multivariate time series is a challenging task, given the requirements of deriving a compact detection criterion without accessing the anomaly points. The existing methods are mainly based on reconstruction error or association divergence, which are both confined to isolated subsequences with limited horizons, hardly promising unified series-level criterion. In this paper, we propose the Global Dictionary-enhanced Transformer (GDformer) with a renovated dictionary-based cross attention mechanism to cultivate the global representations shared by all normal points in the entire series. Accordingly, the cross-attention maps reflect the correlation weights between the point and global representations, which naturally leads to the representation-wise similarity-based detection criterion. To foster more compact detection boundary, prototypes are introduced to capture the distribution of normal point-global correlation weights. GDformer consistently achieves state-of-the-art unsupervised anomaly detection performance on five real-world benchmark datasets. Further experiments validate the global dictionary has great transferability among various datasets. The code is available at https://github.com/yuppielqx/GDformer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments</title>
<link>https://arxiv.org/abs/2502.01778</link>
<guid>https://arxiv.org/abs/2502.01778</guid>
<content:encoded><![CDATA[
arXiv:2502.01778v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient exploration of the solution space. This study introduces GNN-DT, a novel Decision Transformer (DT) architecture that integrates Graph Neural Network (GNN) embedders with a novel residual connection between input and output tokens crucial for handling dynamic environments. By learning from previously collected trajectories, GNN-DT tackles the sparse rewards limitations of online RL algorithms and delivers high-quality solutions in real-time. We evaluate GNN-DT on the complex electric vehicle (EV) charging optimization problem and prove that its performance is superior and requires significantly fewer training trajectories, thus improving sample efficiency compared to existing DT and offline RL baselines. Furthermore, GNN-DT exhibits robust generalization to unseen environments and larger action spaces, addressing a critical gap in prior offline and online RL approaches.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIE-SenseNet: Riemannian Manifold Embedding of Multi-Source Industrial Sensor Signals for Robust Pattern Recognition</title>
<link>https://arxiv.org/abs/2502.02428</link>
<guid>https://arxiv.org/abs/2502.02428</guid>
<content:encoded><![CDATA[
arXiv:2502.02428v2 Announce Type: replace 
Abstract: Industrial sensor networks produce complex signals with nonlinear structure and shifting distributions. We propose RIE-SenseNet, a novel geometry-aware Transformer model that embeds sensor data in a Riemannian manifold to tackle these challenges. By leveraging hyperbolic geometry for sequence modeling and introducing a manifold-based augmentation technique, RIE-SenseNet preserves sensor signal structure and generates realistic synthetic samples. Experiments show RIE-SenseNet achieves >90% F1-score, far surpassing CNN and Transformer baselines. These results illustrate the benefit of combining non-Euclidean feature representations with geometry-consistent data augmentation for robust pattern recognition in industrial sensing.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-preserving contrastive learning for spatial time series</title>
<link>https://arxiv.org/abs/2502.06380</link>
<guid>https://arxiv.org/abs/2502.06380</guid>
<content:encoded><![CDATA[
arXiv:2502.06380v3 Announce Type: replace 
Abstract: The effectiveness of neural network models largely relies on learning meaningful latent patterns from data, where self-supervised learning of informative representations can enhance model performance and generalisability. However, self-supervised representation learning for spatially characterised time series, which are ubiquitous in transportation domain, poses unique challenges due to the necessity of maintaining fine-grained spatio-temporal similarities in the latent space. In this study, we introduce two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. To balance the contrastive learning objective and the need for structure preservation, we propose a dynamic weighting mechanism that adaptively manages this trade-off and stabilises training. We validate the proposed method through extensive experiments, including multivariate time series classification to demonstrate its general applicability, as well as macroscopic and microscopic traffic prediction to highlight its particular usefulness in encoding traffic interactions. Across all tasks, our method preserves the similarity structures more effectively and improves state-of-the-art task performances. This method can be integrated with an arbitrary neural network model and is particularly beneficial for time series data with spatial or geographical features. Furthermore, our findings suggest that well-preserved similarity structures in the latent space indicate more informative and useful representations. This provides insights to design more effective neural networks for data-driven transportation research. Our code is made openly accessible with all resulting data at https://github.com/yiru-jiao/spclt
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient-Guided Annealing for Domain Generalization</title>
<link>https://arxiv.org/abs/2502.20162</link>
<guid>https://arxiv.org/abs/2502.20162</guid>
<content:encoded><![CDATA[
arXiv:2502.20162v5 Announce Type: replace 
Abstract: Domain Generalization (DG) research has gained considerable traction as of late, since the ability to generalize to unseen data distributions is a requirement that eludes even state-of-the-art training algorithms. In this paper we observe that the initial iterations of model training play a key role in domain generalization effectiveness, since the loss landscape may be significantly different across the training and test distributions, contrary to the case of i.i.d. data. Conflicts between gradients of the loss components of each domain lead the optimization procedure to undesirable local minima that do not capture the domain-invariant features of the target classes. We propose alleviating domain conflicts in model optimization, by iteratively annealing the parameters of a model in the early stages of training and searching for points where gradients align between domains. By discovering a set of parameter values where gradients are updated towards the same direction for each data distribution present in the training set, the proposed Gradient-Guided Annealing (GGA) algorithm encourages models to seek out minima that exhibit improved robustness against domain shifts. The efficacy of GGA is evaluated on five widely accepted and challenging image classification domain generalization benchmarks, where its use alone is able to establish highly competitive or even state-of-the-art performance. Moreover, when combined with previously proposed domain-generalization algorithms it is able to consistently improve their effectiveness by significant margins.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Preserved Automated Scoring using Federated Learning for Educational Research</title>
<link>https://arxiv.org/abs/2503.11711</link>
<guid>https://arxiv.org/abs/2503.11711</guid>
<content:encoded><![CDATA[
arXiv:2503.11711v2 Announce Type: replace 
Abstract: Data privacy remains a critical concern in educational research, requiring strict adherence to ethical standards and regulatory protocols. While traditional approaches rely on anonymization and centralized data collection, they often expose raw student data to security vulnerabilities and impose substantial logistical overhead. In this study, we propose a federated learning (FL) framework for automated scoring of educational assessments that eliminates the need to share sensitive data across institutions. Our approach leverages parameter-efficient fine-tuning of large language models (LLMs) with Low-Rank Adaptation (LoRA), enabling each client (school) to train locally while sharing only optimized model updates. To address data heterogeneity, we implement an adaptive weighted aggregation strategy that considers both client performance and data volume. We benchmark our model against two state-of-the-art FL methods and a centralized learning baseline using NGSS-aligned multi-label science assessment data from nine middle schools. Results show that our model achieves the highest accuracy (94.5%) among FL approaches, and performs within 0.5-1.0 percentage points of the centralized model on these metrics. Additionally, it achieves comparable rubric-level scoring accuracy, with only a 1.3% difference in rubric match and a lower score deviation (MAE), highlighting its effectiveness in preserving both prediction quality and interpretability.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Graphon Neural Networks: Approximation and Cut Distance</title>
<link>https://arxiv.org/abs/2503.14338</link>
<guid>https://arxiv.org/abs/2503.14338</guid>
<content:encoded><![CDATA[
arXiv:2503.14338v2 Announce Type: replace 
Abstract: Graph limit models, like graphons for limits of dense graphs, have recently been used to study size transferability of graph neural networks (GNNs). While most literature focuses on message passing GNNs (MPNNs), in this work we attend to the more powerful higher-order GNNs. First, we extend the $k$-WL test for graphons (B\"oker, 2023) to the graphon-signal space and introduce signal-weighted homomorphism densities as a key tool. As an exemplary focus, we generalize Invariant Graph Networks (IGNs) to graphons, proposing Invariant Graphon Networks (IWNs) defined via a subset of the IGN basis corresponding to bounded linear operators. Even with this restricted basis, we show that IWNs of order $k$ are at least as powerful as the $k$-WL test, and we establish universal approximation results for graphon-signals in $L^p$ distances. This significantly extends the prior work of Cai & Wang (2022), showing that IWNs--a subset of their IGN-small--retain effectively the same expressivity as the full IGN basis in the limit. In contrast to their approach, our blueprint of IWNs also aligns better with the geometry of graphon space, for example facilitating comparability to MPNNs. We highlight that, while typical higher-order GNNs are discontinuous w.r.t. cut distance--which causes their lack of convergence and is inherently tied to the definition of $k$-WL--transferability remains achievable.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Graph Structure Learning in the Era of LLMs</title>
<link>https://arxiv.org/abs/2503.21223</link>
<guid>https://arxiv.org/abs/2503.21223</guid>
<content:encoded><![CDATA[
arXiv:2503.21223v3 Announce Type: replace 
Abstract: Recently, the emergence of LLMs has prompted researchers to integrate language descriptions into graphs, aiming to enhance model encoding capabilities from a data-centric perspective. This graph representation is called text-attributed graphs (TAGs). A review of prior advancements highlights that graph structure learning (GSL) is a pivotal technique for improving data utility, making it highly relevant to efficient TAG learning. However, most GSL methods are tailored for traditional graphs without textual information, underscoring the necessity of developing a new GSL paradigm. Despite clear motivations, it remains challenging: (1) How can we define a reasonable optimization objective for GSL in the era of LLMs, considering the massive parameters in LLM? (2) How can we design an efficient model architecture that enables seamless integration of LLM for this optimization objective? For Question 1, we reformulate existing GSL optimization objectives as a tree optimization framework, shifting the focus from obtaining a well-trained edge predictor to a language-aware tree sampler. For Question 2, we propose decoupled and training-free model design principles for LLM integration, shifting the focus from computation-intensive fine-tuning to more efficient inference. Based on this, we propose Large Language and Tree Assistant (LLaTA), which leverages tree-based LLM in-context learning to enhance the understanding of topology and text, enabling reliable inference and generating improved graph structure. Extensive experiments on 10 datasets demonstrate that LLaTA enjoys flexibility-incorporated with any backbone; scalability-outperforms other LLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Ultra-Low-Power $\mu$NPUs</title>
<link>https://arxiv.org/abs/2503.22567</link>
<guid>https://arxiv.org/abs/2503.22567</guid>
<content:encoded><![CDATA[
arXiv:2503.22567v2 Announce Type: replace 
Abstract: Efficient on-device neural network (NN) inference has various advantages over cloud-based processing, including predictable latency, enhanced privacy, greater reliability, and reduced operating costs for vendors. This has sparked the recent rapid development of microcontroller-scale NN accelerators, often referred to as neural processing units ($\mu$NPUs), designed specifically for ultra-low-power applications.
  In this paper we present the first comparative evaluation of a number of commercially-available $\mu$NPUs, as well as the first independent benchmarks for several of these platforms. We develop and open-source a model compilation framework to enable consistent benchmarking of quantized models across diverse $\mu$NPU hardware. Our benchmark targets end-to-end performance and includes model inference latency, power consumption, and memory overhead, alongside other factors. The resulting analysis uncovers both expected performance trends as well as surprising disparities between hardware specifications and actual performance, including $\mu$NPUs exhibiting unexpected scaling behaviors with increasing model complexity. Our framework provides a foundation for further evaluation of $\mu$NPU platforms alongside valuable insights for both hardware designers and software developers in this rapidly evolving space.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Observation to Orientation: an Adaptive Integer Programming Approach to Intervention Design</title>
<link>https://arxiv.org/abs/2504.03122</link>
<guid>https://arxiv.org/abs/2504.03122</guid>
<content:encoded><![CDATA[
arXiv:2504.03122v3 Announce Type: replace 
Abstract: Using both observational and experimental data, a causal discovery process can identify the causal relationships between variables. A unique adaptive intervention design paradigm is presented in this work, where causal directed acyclic graphs (DAGs) are for effectively recovered with practical budgetary considerations. In order to choose treatments that optimize information gain under these considerations, an iterative integer programming (IP) approach is proposed, which drastically reduces the number of experiments required. Simulations over a broad range of graph sizes and edge densities are used to assess the effectiveness of the suggested approach. Results show that the proposed adaptive IP approach achieves full causal graph recovery with fewer intervention iterations and variable manipulations than random intervention baselines, and it is also flexible enough to accommodate a variety of practical constraints.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directional Sign Loss: A Topology-Preserving Loss Function that Approximates the Sign of Finite Differences</title>
<link>https://arxiv.org/abs/2504.04202</link>
<guid>https://arxiv.org/abs/2504.04202</guid>
<content:encoded><![CDATA[
arXiv:2504.04202v2 Announce Type: replace 
Abstract: Preserving critical topological features in learned latent spaces is a fundamental challenge in representation learning, particularly for topology-sensitive data. This paper introduces directional sign loss (DSL), a novel loss function that approximates the number of mismatches in the signs of finite differences between corresponding elements of two arrays. By penalizing discrepancies in critical points between input and reconstructed data, DSL encourages autoencoders and other learnable compressors to retain the topological features of the original data. We present the mathematical formulation, complexity analysis, and practical implementation of DSL, comparing its behavior to its non-differentiable counterpart and to other topological measures. Experiments on one-, two-, and three-dimensional data show that combining DSL with traditional loss functions preserves topological features more effectively than traditional losses alone. Moreover, DSL serves as a differentiable, efficient proxy for common topology-based metrics, enabling its use in gradient-based optimization frameworks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable and Efficient Inverse Analysis using Physics-Informed Neural Networks with Distance Functions and Adaptive Weight Tuning</title>
<link>https://arxiv.org/abs/2504.18091</link>
<guid>https://arxiv.org/abs/2504.18091</guid>
<content:encoded><![CDATA[
arXiv:2504.18091v2 Announce Type: replace 
Abstract: Physics-informed neural networks have attracted significant attention in scientific machine learning for their capability to solve forward and inverse problems governed by partial differential equations. However, the accuracy of PINN solutions is often limited by the treatment of boundary conditions. Conventional penalty-based methods, which incorporate boundary conditions as penalty terms in the loss function, cannot guarantee exact satisfaction of the given boundary conditions and are highly sensitive to the choice of penalty parameters. This paper demonstrates that distance functions, specifically R-functions, can be leveraged to enforce boundary conditions, overcoming these limitations. R-functions provide normalized distance fields, enabling accurate representation of boundary geometries, including non-convex domains, and facilitating various types of boundary conditions. We extend this distance function-based boundary condition imposition method to inverse problems using PINNs and introduce an adaptive weight tuning technique to ensure reliable and efficient inverse analysis. We demonstrate the efficacy of the method through several numerical experiments. Numerical results show that the proposed method solves inverse problems more accurately and efficiently than penalty-based methods, even in the presence of complex non-convex geometries. This approach offers a reliable and efficient framework for inverse analysis using PINNs, with potential applications across a wide range of engineering problems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced semi-supervised stamping process monitoring with physically-informed feature extraction</title>
<link>https://arxiv.org/abs/2504.21389</link>
<guid>https://arxiv.org/abs/2504.21389</guid>
<content:encoded><![CDATA[
arXiv:2504.21389v2 Announce Type: replace 
Abstract: In tackling frequent batch anomalies in high-speed stamping processes, this study introduces a novel semi-supervised in-process anomaly monitoring framework, utilizing accelerometer signals and physics information, to capture the process anomaly effectively. The proposed framework facilitates the construction of a monitoring model with imbalanced sample distribution, which enables in-process condition monitoring in real-time to prevent batch anomalies, which helps to reduce batch defects risk and enhance production yield. Firstly, to effectively capture key features from raw data containing redundant information, a hybrid feature extraction algorithm is proposed to utilize data-driven methods and physical mechanisms simultaneously. Secondly, to address the challenge brought by imbalanced sample distribution, a semi-supervised anomaly detection model is established, which merely employs normal samples to build a golden baseline model, and a novel deviation score is proposed to quantify the anomaly level of each online stamping stroke. The effectiveness of the proposed feature extraction method is validated with various classification algorithms. A real-world in-process dataset from stamping manufacturing workshop is employed to illustrate the superiority of proposed semi-supervised framework with enhance performance for process anomaly monitoring.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations</title>
<link>https://arxiv.org/abs/2505.00307</link>
<guid>https://arxiv.org/abs/2505.00307</guid>
<content:encoded><![CDATA[
arXiv:2505.00307v2 Announce Type: replace 
Abstract: There has been a recent surge of interest in time series modeling using the Transformer architecture. However, forecasting multivariate time series with Transformer presents a unique challenge as it requires modeling both temporal (cross-time) and variate (cross-variate) dependencies. While Transformer-based models have gained popularity for their flexibility in capturing both sequential and cross-variate relationships, it is unclear how to best integrate these two sources of information in the context of the Transformer architecture while optimizing for both performance and efficiency. We re-purpose the Transformer architecture to effectively model both cross-time and cross-variate dependencies. Our approach begins by embedding each variate independently into a variate-wise representation that captures its cross-time dynamics, and then models cross-variate dependencies through attention mechanisms on these learned embeddings. Gating operations in both cross-time and cross-variate modeling phases regulate information flow, allowing the model to focus on the most relevant features for accurate predictions. Our method achieves state-of-the-art performance across 13 real-world datasets and can be seamlessly integrated into other Transformer-based and LLM-based forecasters, delivering performance improvements up to 20.7\% over original models. Code is available at this repository: https://github.com/nyuolab/Gateformer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis</title>
<link>https://arxiv.org/abs/2505.00410</link>
<guid>https://arxiv.org/abs/2505.00410</guid>
<content:encoded><![CDATA[
arXiv:2505.00410v2 Announce Type: replace 
Abstract: The present research tackles the difficulty of predicting osteoporosis risk via machine learning (ML) approaches, emphasizing the use of explainable artificial intelligence (XAI) to improve model transparency. Osteoporosis is a significant public health concern, sometimes remaining untreated owing to its asymptomatic characteristics, and early identification is essential to avert fractures. The research assesses six machine learning classifiers: Random Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting and utilizes a dataset based on clinical, demographic, and lifestyle variables. The models are refined using GridSearchCV to calibrate hyperparameters, with the objective of enhancing predictive efficacy. XGBoost had the greatest accuracy (91%) among the evaluated models, surpassing others in precision (0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI approaches, such as SHAP, LIME, and Permutation Feature Importance, to elucidate the decision-making process of the optimal model. The study indicates that age is the primary determinant in forecasting osteoporosis risk, followed by hormonal alterations and familial history. These results corroborate clinical knowledge and affirm the models' therapeutic significance. The research underscores the significance of explainability in machine learning models for healthcare applications, guaranteeing that physicians can rely on the system's predictions. The report ultimately proposes directions for further research, such as validation across varied populations and the integration of supplementary biomarkers for enhanced predictive accuracy.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A vector quantized masked autoencoder for audiovisual speech emotion recognition</title>
<link>https://arxiv.org/abs/2305.03568</link>
<guid>https://arxiv.org/abs/2305.03568</guid>
<content:encoded><![CDATA[
arXiv:2305.03568v3 Announce Type: replace-cross 
Abstract: An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning-based adaption of robotic friction models</title>
<link>https://arxiv.org/abs/2310.16688</link>
<guid>https://arxiv.org/abs/2310.16688</guid>
<content:encoded><![CDATA[
arXiv:2310.16688v2 Announce Type: replace-cross 
Abstract: In the Fourth Industrial Revolution, wherein artificial intelligence and the automation of machines occupy a central role, the deployment of robots is indispensable. However, the manufacturing process using robots, especially in collaboration with humans, is highly intricate. In particular, modeling the friction torque in robotic joints is a longstanding problem due to the lack of a good mathematical description. This motivates the usage of data-driven methods in recent works. However, model-based and data-driven models often exhibit limitations in their ability to generalize beyond the specific dynamics they were trained on, as we demonstrate in this paper. To address this challenge, we introduce a novel approach based on residual learning, which aims to adapt an existing friction model to new dynamics using as little data as possible. We validate our approach by training a base neural network on a symmetric friction data set to learn an accurate relation between the velocity and the friction torque. Subsequently, to adapt to more complex asymmetric settings, we train a second network on a small dataset, focusing on predicting the residual of the initial network's output. By combining the output of both networks in a suitable manner, our proposed estimator outperforms the conventional model-based approach, an extended LuGre model, and the base neural network significantly. Furthermore, we evaluate our method on trajectories involving external loads and still observe a substantial improvement, approximately 60-70%, over the conventional approach. Our method does not rely on data with external load during training, eliminating the need for external torque sensors. This demonstrates the generalization capability of our approach, even with a small amount of data--less than a minute--enabling adaptation to diverse scenarios based on prior knowledge about friction in different settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital-analog quantum learning on Rydberg atom arrays</title>
<link>https://arxiv.org/abs/2401.02940</link>
<guid>https://arxiv.org/abs/2401.02940</guid>
<content:encoded><![CDATA[
arXiv:2401.02940v2 Announce Type: replace-cross 
Abstract: We propose hybrid digital-analog learning algorithms on Rydberg atom arrays, combining the potentially practical utility and near-term realizability of quantum learning with the rapidly scaling architectures of neutral atoms. Our construction requires only single-qubit operations in the digital setting and global driving according to the Rydberg Hamiltonian in the analog setting. We perform a comprehensive numerical study of our algorithm on both classical and quantum data, given respectively by handwritten digit classification and unsupervised quantum phase boundary learning. We show in the two representative problems that digital-analog learning is not only feasible in the near term, but also requires shorter circuit depths and is more robust to realistic error models as compared to digital learning schemes. Our results suggest that digital-analog learning opens a promising path towards improved variational quantum learning experiments in the near term.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Sleep Staging via Multi-Level Domain Alignment</title>
<link>https://arxiv.org/abs/2401.05363</link>
<guid>https://arxiv.org/abs/2401.05363</guid>
<content:encoded><![CDATA[
arXiv:2401.05363v5 Announce Type: replace-cross 
Abstract: Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different domains, and a Sequence-level Feature Alignment to minimize the discrepancy of sequential features among different domains. SleepDG is validated on five public datasets, achieving the state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Multimedia Generated by Large AI Models: A Survey</title>
<link>https://arxiv.org/abs/2402.00045</link>
<guid>https://arxiv.org/abs/2402.00045</guid>
<content:encoded><![CDATA[
arXiv:2402.00045v4 Announce Type: replace-cross 
Abstract: The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, online detection tools, and evaluation metrics to provide a valuable resource for researchers and practitioners in this field. Most importantly, we offer a focused analysis from a social media perspective to highlight their broader societal impact. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep hybrid models: infer and plan in a dynamic world</title>
<link>https://arxiv.org/abs/2402.10088</link>
<guid>https://arxiv.org/abs/2402.10088</guid>
<content:encoded><![CDATA[
arXiv:2402.10088v4 Announce Type: replace-cross 
Abstract: To determine an optimal plan for complex tasks, one often deals with dynamic and hierarchical relationships between several entities. Traditionally, such problems are tackled with optimal control, which relies on the optimization of cost functions; instead, a recent biologically-motivated proposal casts planning and control as an inference process. Active inference assumes that action and perception are two complementary aspects of life whereby the role of the former is to fulfill the predictions inferred by the latter. Here, we present an active inference approach that exploits discrete and continuous processing, based on three features: the representation of potential body configurations in relation to the objects of interest; the use of hierarchical relationships that enable the agent to easily interpret and flexibly expand its body schema for tool use; the definition of potential trajectories related to the agent's intentions, used to infer and plan with dynamic elements at different temporal scales. We evaluate this deep hybrid model on a habitual task: reaching a moving object after having picked a moving tool. We show that the model can tackle the presented task under different conditions. This study extends past work on planning as inference and advances an alternative direction to optimal control.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image space formalism of convolutional neural networks for k-space interpolation</title>
<link>https://arxiv.org/abs/2402.17410</link>
<guid>https://arxiv.org/abs/2402.17410</guid>
<content:encoded><![CDATA[
arXiv:2402.17410v2 Announce Type: replace-cross 
Abstract: Purpose: Noise resilience in image reconstructions by scan-specific robust artificial neural networks for k-space interpolation (RAKI) is linked to nonlinear activations in k-space. To gain a deeper understanding of this relationship, an image space formalism of RAKI is introduced for analyzing noise propagation analytically, identifying and characterizing image reconstruction features and to describe the role of nonlinear activations in a human readable manner. Methods: The image space formalism for RAKI inference is employed by expressing nonlinear activations in k-space as element-wise multiplications with activation masks, which transform into convolutions in image space. Jacobians of the de-aliased, coil-combined image relative to the aliased coil images can be expressed algebraically, and thus, the noise amplification is quantified analytically (g-factor maps). We analyze the role of nonlinearity for noise resilience by controlling the degree of nonlinearity in the reconstruction model via the negative slope parameter in leaky ReLU. Results: The analytical g-factor maps correspond with those obtained from Monte Carlo simulations and from an auto differentiation approach for in vivo brain images. Apparent blurring and contrast loss artifacts are identified as implications of enhanced noise resilience. These residual artifacts can be traded against noise resilience by adjusting the degree of nonlinearity in the model (Tikhonov-like regularization) in case of limited training data. The inspection of image space activations reveals an autocorrelation pattern leading to a potential center artifact. Conclusion: The image space formalism of RAKI provides the means for analytical quantitative noisepropagation analysis and human-readable visualization of the effects of the nonlinear activation functions in k-space.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-Reinforcement Learning Hierarchical Motion Planning in Multi-agent Adversarial Games</title>
<link>https://arxiv.org/abs/2403.10794</link>
<guid>https://arxiv.org/abs/2403.10794</guid>
<content:encoded><![CDATA[
arXiv:2403.10794v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL)-based motion planning has recently shown the potential to outperform traditional approaches from autonomous navigation to robot manipulation. In this work, we focus on a motion planning task for an evasive target in a partially observable multi-agent adversarial pursuit-evasion game (PEG). Pursuit-evasion problems are relevant to various applications, such as search and rescue operations and surveillance robots, where robots must effectively plan their actions to gather intelligence or accomplish mission tasks while avoiding detection or capture. We propose a hierarchical architecture that integrates a high-level diffusion model to plan global paths responsive to environment data, while a low-level RL policy reasons about evasive versus global path-following behavior. The benchmark results across different domains and different observability show that our approach outperforms baselines by 77.18% and 47.38% on detection and goal reaching rate, which leads to 51.4% increasing of the performance score on average. Additionally, our method improves interpretability, flexibility and efficiency of the learned policy.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of Decentralized Stochastic Subgradient-based Methods for Nonsmooth Nonconvex functions</title>
<link>https://arxiv.org/abs/2403.11565</link>
<guid>https://arxiv.org/abs/2403.11565</guid>
<content:encoded><![CDATA[
arXiv:2403.11565v3 Announce Type: replace-cross 
Abstract: In this paper, we focus on the decentralized stochastic subgradient-based methods in minimizing nonsmooth nonconvex functions without Clarke regularity, especially in the decentralized training of nonsmooth neural networks. We propose a general framework that unifies various decentralized subgradient-based methods, such as decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGD-M). To establish the convergence properties of our proposed framework, we relate the discrete iterates to the trajectories of a continuous-time differential inclusion, which is assumed to have a coercive Lyapunov function with a stable set $\mathcal{A}$. We prove the asymptotic convergence of the iterates to the stable set $\mathcal{A}$ with sufficiently small and diminishing step-sizes. These results provide first convergence guarantees for some well-recognized of decentralized stochastic subgradient-based methods without Clarke regularity of the objective function. Preliminary numerical experiments demonstrate that our proposed framework yields highly efficient decentralized stochastic subgradient-based methods with convergence guarantees in the training of nonsmooth neural networks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoverUp: Effective High Coverage Test Generation for Python</title>
<link>https://arxiv.org/abs/2403.16218</link>
<guid>https://arxiv.org/abs/2403.16218</guid>
<content:encoded><![CDATA[
arXiv:2403.16218v4 Announce Type: replace-cross 
Abstract: Testing is an essential part of software development. Test generation tools attempt to automate the otherwise labor-intensive task of test creation, but generating high-coverage tests remains challenging. This paper proposes CoverUp, a novel approach to driving the generation of high-coverage Python regression tests. CoverUp combines coverage analysis, code context, and feedback in prompts that iteratively guide the LLM to generate tests that improve line and branch coverage. We evaluate our prototype CoverUp implementation across a benchmark of challenging code derived from open-source Python projects and show that CoverUp substantially improves on the state of the art. Compared to CodaMosa, a hybrid search/LLM-based test generator, CoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%). Compared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves an overall line+branch coverage of 89% (vs. 77%). We also demonstrate that CoverUp's performance stems not only from the LLM used but from the combined effectiveness of its components.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model</title>
<link>https://arxiv.org/abs/2404.09957</link>
<guid>https://arxiv.org/abs/2404.09957</guid>
<content:encoded><![CDATA[
arXiv:2404.09957v3 Announce Type: replace-cross 
Abstract: Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or "best-practice" guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at https://github.com/mazurowski-lab/finetune-SAM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles</title>
<link>https://arxiv.org/abs/2405.21027</link>
<guid>https://arxiv.org/abs/2405.21027</guid>
<content:encoded><![CDATA[
arXiv:2405.21027v5 Announce Type: replace-cross 
Abstract: For solving zero-sum games involving non-transitivity, a useful approach is to maintain a policy population to approximate the Nash Equilibrium (NE). Previous studies have shown that the Policy Space Response Oracles (PSRO) algorithm is an effective framework for solving such games. However, current methods initialize a new policy from scratch or inherit a single historical policy in Best Response (BR), missing the opportunity to leverage past policies to generate a better BR. In this paper, we propose Fusion-PSRO, which employs Nash Policy Fusion to initialize a new policy for BR training. Nash Policy Fusion serves as an implicit guiding policy that starts exploration on the current Meta-NE, thus providing a closer approximation to BR. Moreover, it insightfully captures a weighted moving average of past policies, dynamically adjusting these weights based on the Meta-NE in each iteration. This cumulative process further enhances the policy population. Empirical results on classic benchmarks show that Fusion-PSRO achieves lower exploitability, thereby mitigating the shortcomings of previous research on policy initialization in BR.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speed-accuracy relations for diffusion models: Wisdom from nonequilibrium thermodynamics and optimal transport</title>
<link>https://arxiv.org/abs/2407.04495</link>
<guid>https://arxiv.org/abs/2407.04495</guid>
<content:encoded><![CDATA[
arXiv:2407.04495v5 Announce Type: replace-cross 
Abstract: We discuss a connection between a generative model, called the diffusion model, and nonequilibrium thermodynamics for the Fokker-Planck equation, called stochastic thermodynamics. Using techniques from stochastic thermodynamics, we derive the speed-accuracy relations for diffusion models, which are inequalities that relate the accuracy of data generation to the entropy production rate. This relation can be interpreted as the speed of the diffusion dynamics in the absence of the non-conservative force. From a stochastic thermodynamic perspective, our results provide quantitative insight into how best to generate data in diffusion models. The optimal learning protocol is introduced by the geodesic of space of the 2-Wasserstein distance in optimal transport theory. We numerically illustrate the validity of the speed-accuracy relations for diffusion models with different noise schedules and different data. We numerically discuss our results for optimal and suboptimal learning protocols. We also demonstrate the applicability of our results to data generation from the real-world image datasets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrackFormers: In Search of Transformer-Based Particle Tracking for the High-Luminosity LHC Era</title>
<link>https://arxiv.org/abs/2407.07179</link>
<guid>https://arxiv.org/abs/2407.07179</guid>
<content:encoded><![CDATA[
arXiv:2407.07179v3 Announce Type: replace-cross 
Abstract: High-Energy Physics experiments are facing a multi-fold data increase with every new iteration. This is certainly the case for the upcoming High-Luminosity LHC upgrade. Such increased data processing requirements forces revisions to almost every step of the data processing pipeline. One such step in need of an overhaul is the task of particle track reconstruction, a.k.a., tracking. A Machine Learning-assisted solution is expected to provide significant improvements, since the most time-consuming step in tracking is the assignment of hits to particles or track candidates. This is the topic of this paper.
  We take inspiration from large language models. As such, we consider two approaches: the prediction of the next word in a sentence (next hit point in a track), as well as the one-shot prediction of all hits within an event. In an extensive design effort, we have experimented with three models based on the Transformer architecture and one model based on the U-Net architecture, performing track association predictions for collision event hit points. In our evaluation, we consider a spectrum of simple to complex representations of the problem, eliminating designs with lower metrics early on. We report extensive results, covering both prediction accuracy (score) and computational performance. We have made use of the REDVID simulation framework, as well as reductions applied to the TrackML data set, to compose five data sets from simple to complex, for our experiments. The results highlight distinct advantages among different designs in terms of prediction accuracy and computational performance, demonstrating the efficiency of our methodology. Most importantly, the results show the viability of a one-shot encoder-classifier based Transformer solution as a practical approach for the task of tracking.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Drift Detection in Medical Imaging with Sketching and Fine-Tuned Transformer</title>
<link>https://arxiv.org/abs/2408.08456</link>
<guid>https://arxiv.org/abs/2408.08456</guid>
<content:encoded><![CDATA[
arXiv:2408.08456v2 Announce Type: replace-cross 
Abstract: Distributional drift detection is important in medical applications as it helps ensure the accuracy and reliability of models by identifying changes in the underlying data distribution that could affect the prediction results of machine learning models. However, current methods have limitations in detecting drift, for example, the inclusion of abnormal datasets can lead to unfair comparisons. This paper presents an accurate and sensitive approach to detect distributional drift in CT-scan medical images by leveraging data-sketching and fine-tuning techniques. We developed a robust baseline library model for real-time anomaly detection, allowing for efficient comparison of incoming images and identification of anomalies. Additionally, we fine-tuned a pre-trained Vision Transformer model to extract relevant features, using mammography as a case study, significantly enhancing model accuracy to 99.11%. Combining with data-sketches and fine-tuning, our feature extraction evaluation demonstrated that cosine similarity scores between similar datasets provide greater improvements, from around 50% increased to 99.1%. Finally, the sensitivity evaluation shows that our solutions are highly sensitive to even 1% salt-and-pepper and speckle noise, and it is not sensitive to lighting noise (e.g., lighting conditions have no impact on data drift). The proposed methods offer a scalable and reliable solution for maintaining the accuracy of diagnostic models in dynamic clinical environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreenLight-Gym: Reinforcement learning benchmark environment for control of greenhouse production systems</title>
<link>https://arxiv.org/abs/2410.05336</link>
<guid>https://arxiv.org/abs/2410.05336</guid>
<content:encoded><![CDATA[
arXiv:2410.05336v2 Announce Type: replace-cross 
Abstract: This study presents GreenLight-Gym, a new, fast, open-source benchmark environment for developing reinforcement learning (RL) methods in greenhouse crop production control. Built on the state-of-the-art GreenLight model, it features a differentiable C++ implementation leveraging the CasADi framework for efficient numerical integration. GreenLight-Gym improves simulation speed by a factor of 17 over the original GreenLight implementation. A modular Python environment wrapper enables flexible configuration of control tasks and RL-based controllers. This flexibility is demonstrated by learning controllers under parametric uncertainty using two well-known RL algorithms. GreenLight-Gym provides a standardized benchmark for advancing RL methodologies and evaluating greenhouse control solutions under diverse conditions. The greenhouse control community is encouraged to use and extend this benchmark to accelerate innovation in greenhouse crop production.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Source-Channel Coding for Semantic Communication</title>
<link>https://arxiv.org/abs/2410.08222</link>
<guid>https://arxiv.org/abs/2410.08222</guid>
<content:encoded><![CDATA[
arXiv:2410.08222v3 Announce Type: replace-cross 
Abstract: Semantic communication technology emerges as a pivotal bridge connecting AI with classical communication. The current semantic communication systems are generally modeled as an Auto-Encoder (AE). AE lacks a deep integration of AI principles with communication strategies due to its inability to effectively capture channel dynamics. This gap makes it difficult to justify the need for joint source-channel coding (JSCC) and to explain why performance improves. This paper begins by exploring lossless and lossy communication, highlighting that the inclusion of data distortion distinguishes semantic communication from classical communication. It breaks the conditions for the separation theorem to hold and explains why the amount of data transferred by semantic communication is less. Therefore, employing JSCC becomes imperative for achieving optimal semantic communication. Moreover, a Variational Source-Channel Coding (VSCC) method is proposed for constructing semantic communication systems based on data distortion theory, integrating variational inference and channel characteristics. Using a deep learning network, we develop a semantic communication system employing the VSCC method and demonstrate its capability for semantic transmission. We also establish semantic communication systems of equivalent complexity employing the AE method and the VAE method. Experimental results reveal that the VSCC model offers superior interpretability compared to AE model, as it clearly captures the semantic features of the transmitted data, represented as the variance of latent variables in our experiments. In addition, VSCC model exhibits superior semantic transmission capabilities compared to VAE model. At the same level of data distortion evaluated by PSNR, VSCC model exhibits stronger human interpretability, which can be partially assessed by SSIM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits</title>
<link>https://arxiv.org/abs/2410.18234</link>
<guid>https://arxiv.org/abs/2410.18234</guid>
<content:encoded><![CDATA[
arXiv:2410.18234v2 Announce Type: replace-cross 
Abstract: We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection schemes based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Neutrino-Nucleus Cross Sections</title>
<link>https://arxiv.org/abs/2412.16303</link>
<guid>https://arxiv.org/abs/2412.16303</guid>
<content:encoded><![CDATA[
arXiv:2412.16303v2 Announce Type: replace-cross 
Abstract: Neutrino-nucleus scattering cross sections are critical theoretical inputs for long-baseline neutrino oscillation experiments. However, robust modeling of these cross sections remains challenging. For a simple but physically motivated toy model of the DUNE experiment, we demonstrate that an accurate neural-network model of the cross section -- leveraging Standard Model symmetries -- can be learned from near-detector data. We then perform a neutrino oscillation analysis with simulated far-detector events, finding that the modeled cross section achieves results consistent with what could be obtained if the true cross section were known exactly. This proof-of-principle study highlights the potential of future neutrino near-detector datasets and data-driven cross-section models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2501.14851</link>
<guid>https://arxiv.org/abs/2501.14851</guid>
<content:encoded><![CDATA[
arXiv:2501.14851v2 Announce Type: replace-cross 
Abstract: Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis. To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy. Our experimental results on JustLogic reveal that (i) state-of-the-art (SOTA) reasoning LLMs perform on par or better than the human average but significantly worse than the human ceiling, and (ii) SOTA non-reasoning models still underperform the human average. All code and data are available at https://github.com/michaelchen-lab/JustLogic
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via LLM-based Centroids</title>
<link>https://arxiv.org/abs/2502.09667</link>
<guid>https://arxiv.org/abs/2502.09667</guid>
<content:encoded><![CDATA[
arXiv:2502.09667v2 Announce Type: replace-cross 
Abstract: We introduce k-LLMmeans, a novel modification of the k-means algorithm for text clustering that leverages LLM-generated summaries as cluster centroids, capturing semantic nuances often missed by purely numerical averages. This design preserves the core optimization properties of k-means while enhancing semantic interpretability and avoiding the scalability and instability issues typical of modern LLM-based clustering. Unlike existing methods, our approach does not increase LLM usage with dataset size and produces transparent intermediate outputs. We further extend it with a mini-batch variant for efficient, real-time clustering of streaming text. Extensive experiments across multiple datasets, embeddings, and LLMs show that k-LLMmeans consistently outperforms k-means and other traditional baselines and achieves results comparable to state-of-the-art LLM-based clustering, with a fraction of the LLM calls. Finally, we present a case study on sequential text streams and introduce a new benchmark dataset constructed from StackExchange to evaluate text-stream clustering methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MERGE$^3$: Efficient Evolutionary Merging on Consumer-grade GPUs</title>
<link>https://arxiv.org/abs/2502.10436</link>
<guid>https://arxiv.org/abs/2502.10436</guid>
<content:encoded><![CDATA[
arXiv:2502.10436v4 Announce Type: replace-cross 
Abstract: Evolutionary model merging enables the creation of high-performing multi-task models but remains computationally prohibitive for consumer hardware. We introduce MERGE$^3$, an efficient framework that makes evolutionary merging feasible on a single GPU by reducing fitness computation costs 50$\times$ while preserving performance. MERGE$^3$ achieves this by Extracting a reduced dataset for evaluation, Estimating model abilities using Item Response Theory (IRT), and Evolving optimal merges via IRT-based performance estimators. Our method enables state-of-the-art multilingual and cross-lingual merging, transferring knowledge across languages with significantly lower computational overhead. We provide theoretical guarantees and an open-source library, democratizing high-quality model merging.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting fermionic densities using a Projected Quantum Kernel method</title>
<link>https://arxiv.org/abs/2504.14002</link>
<guid>https://arxiv.org/abs/2504.14002</guid>
<content:encoded><![CDATA[
arXiv:2504.14002v2 Announce Type: replace-cross 
Abstract: We use a support vector regressor based on a projected quantum kernel method to predict the density structure of 1D fermionic systems of interest in quantum chemistry and quantum matter. The kernel is built on with the observables of a quantum reservoir implementable with interacting Rydberg atoms. Training and test data of the fermionic system are generated using a Density Functional Theory approach. We test the performance of the method for several Hamiltonian parameters, finding a general common behavior of the error as a function of measurement time. At sufficiently large measurement times, we find that the method outperforms the classical linear kernel method and can be competitive with the radial basis function method.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting</title>
<link>https://arxiv.org/abs/2504.16098</link>
<guid>https://arxiv.org/abs/2504.16098</guid>
<content:encoded><![CDATA[
arXiv:2504.16098v3 Announce Type: replace-cross 
Abstract: We present SeizureFormer, a Transformer-based model for long-term seizure risk forecasting using interictal epileptiform activity (IEA) surrogate biomarkers and long episode (LE) biomarkers from responsive neurostimulation (RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages structured, clinically relevant features and integrates CNN-based patch embedding, multi-head self-attention, and squeeze-and-excitation blocks to model both short-term dynamics and long-term seizure cycles. Tested across five patients and multiple prediction windows (1 to 14 days), SeizureFormer achieved state-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC of 76.29 percent. Compared to statistical, machine learning, and deep learning baselines, it demonstrates enhanced generalizability and seizure risk forecasting performance under class imbalance. This work supports future clinical integration of interpretable and robust seizure forecasting tools for personalized epilepsy management.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws For Scalable Oversight</title>
<link>https://arxiv.org/abs/2504.18530</link>
<guid>https://arxiv.org/abs/2504.18530</guid>
<content:encoded><![CDATA[
arXiv:2504.18530v2 Announce Type: replace-cross 
Abstract: Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, our framework models oversight as a game between capability-mismatched players; the players have oversight-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. We validate our framework with a modified version of the game Nim and then apply it to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each game, we find scaling laws that approximate how domain performance depends on general AI system capability. We then build on our findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. We identify conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. We also apply our theory to our four oversight games, where we find that NSO success rates at a general Elo gap of 400 are 13.5% for Mafia, 51.7% for Debate, 10.0% for Backdoor Code, and 9.4% for Wargames; these rates decline further when overseeing stronger systems.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatMMFuse: Multi-Modal Fusion model for Material Property Prediction</title>
<link>https://arxiv.org/abs/2505.04634</link>
<guid>https://arxiv.org/abs/2505.04634</guid>
<content:encoded><![CDATA[
<div> Keywords: graph-based encoding, multi-modal fusion, Crystal Graph Convolution Network, SciBERT, zero-shot performance

<br /><br />Summary: The study presents a new model called Material Multi-Modal Fusion (MatMMFuse), which combines different representations for predicting material properties using a multi-head attention mechanism. By integrating structure-aware embeddings from the Crystal Graph Convolution Network (CGCNN) and text embeddings from the SciBERT model, this approach takes advantage of enhanced feature spaces. The model is trained in an end-to-end framework using data from the Materials Project Dataset. Results show significant improvements compared to vanilla CGCNN and SciBERT models, with up to 40% better performance for predicting formation energy per atom. Additionally, the proposed model demonstrates impressive zero-shot performance on curated datasets of Perovskites, Chalcogenides, and the Jarvis Dataset. This ability to perform well without extensive training data makes MatMMFuse suitable for specialized industrial applications where data collection can be prohibitively expensive. Overall, the research highlights the effectiveness of multi-modal approaches in advancing material property prediction while showcasing the advantages of leveraging both graph-based and text-based representations. <div>
arXiv:2505.04634v1 Announce Type: new 
Abstract: The recent progress of using graph based encoding of crystal structures for high throughput material property prediction has been quite successful. However, using a single modality model prevents us from exploiting the advantages of an enhanced features space by combining different representations. Specifically, pre-trained Large language models(LLMs) can encode a large amount of knowledge which is beneficial for training of models. Moreover, the graph encoder is able to learn the local features while the text encoder is able to learn global information such as space group and crystal symmetry. In this work, we propose Material Multi-Modal Fusion(MatMMFuse), a fusion based model which uses a multi-head attention mechanism for the combination of structure aware embedding from the Crystal Graph Convolution Network (CGCNN) and text embeddings from the SciBERT model. We train our model in an end-to-end framework using data from the Materials Project Dataset. We show that our proposed model shows an improvement compared to the vanilla CGCNN and SciBERT model for all four key properties: formation energy, band gap, energy above hull and fermi energy. Specifically, we observe an improvement of 40% compared to the vanilla CGCNN model and 68% compared to the SciBERT model for predicting the formation energy per atom. Importantly, we demonstrate the zero shot performance of the trained model on small curated datasets of Perovskites, Chalcogenides and the Jarvis Dataset. The results show that the proposed model exhibits better zero shot performance than the individual plain vanilla CGCNN and SciBERT model. This enables researchers to deploy the model for specialized industrial applications where collection of training data is prohibitively expensive.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction with Corrupted Labels: Uncertain Imputation and Robust Re-weighting</title>
<link>https://arxiv.org/abs/2505.04733</link>
<guid>https://arxiv.org/abs/2505.04733</guid>
<content:encoded><![CDATA[
<div> Keywords: uncertainty quantification, conformal prediction, privileged information, robust methods, label imputation

<br /><br />Summary: The article presents a new framework for robust uncertainty quantification in contexts where training data labels are corrupted, either through noise or missing information. It builds on conformal prediction (CP), which traditionally assumes independent and identically distributed (i.i.d) data, a condition not met due to label corruptions. To address this issue, the privileged conformal prediction (PCP) method is introduced, which utilizes privileged information (PI) to re-weight the data distribution and produce valid prediction sets. Analysis demonstrates that PCP remains robust against inaccuracies in weight estimation. Additionally, the paper introduces uncertain imputation (UI), a conformal approach that imputes corrupted labels while preserving uncertainty, thus avoiding reliance on weight estimates. Both strategies are supported by theoretical guarantees and empirical validation across synthetic and real datasets. Lastly, these techniques can be incorporated into a triply robust framework, ensuring statistically valid predictions as long as at least one of the underlying methods is valid. <div>
arXiv:2505.04733v1 Announce Type: new 
Abstract: We introduce a framework for robust uncertainty quantification in situations where labeled training data are corrupted, through noisy or missing labels. We build on conformal prediction, a statistical tool for generating prediction sets that cover the test label with a pre-specified probability. The validity of conformal prediction, however, holds under the i.i.d assumption, which does not hold in our setting due to the corruptions in the data. To account for this distribution shift, the privileged conformal prediction (PCP) method proposed leveraging privileged information (PI) -- additional features available only during training -- to re-weight the data distribution, yielding valid prediction sets under the assumption that the weights are accurate. In this work, we analyze the robustness of PCP to inaccuracies in the weights. Our analysis indicates that PCP can still yield valid uncertainty estimates even when the weights are poorly estimated. Furthermore, we introduce uncertain imputation (UI), a new conformal method that does not rely on weight estimation. Instead, we impute corrupted labels in a way that preserves their uncertainty. Our approach is supported by theoretical guarantees and validated empirically on both synthetic and real benchmarks. Finally, we show that these techniques can be integrated into a triply robust framework, ensuring statistically valid predictions as long as at least one underlying method is valid.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SetONet: A Deep Set-based Operator Network for Solving PDEs with permutation invariant variable input sampling</title>
<link>https://arxiv.org/abs/2505.04738</link>
<guid>https://arxiv.org/abs/2505.04738</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Operators, DeepONet, SetONet, Function Spaces, Sensor Configurations  

<br /><br />Summary: This article presents the Set Operator Network (SetONet), an innovative architecture designed to enhance the Deep Operator Network (DeepONet) framework. Standard DeepONet requires fixed input function samples, which limits its use in scenarios with varying sensor configurations and missing data. SetONet utilizes principles from Deep Sets to allow input functions to be treated as unordered sets of location-value pairs, ensuring permutation invariance and enhancing robustness against variations in sensor locations and numbers. The architecture processes spatial coordinates and function values to create richer, spatially-aware input representations. The effectiveness of SetONet is demonstrated on several benchmark problems, including derivative operators, 1D Darcy flow, and 2D elasticity, showcasing its ability to learn operators under variable sampling conditions—where standard DeepONet struggles. Additionally, SetONet performs well in situations with sensor drop-off without requiring interpolation methods. It achieves comparable or improved accuracy over DeepONet, particularly for nonlinear problems, due to its enhanced input representation. Overall, SetONet significantly expands the capabilities of operator learning methods, allowing for greater flexibility and robustness in handling variable or incomplete input data. <div>
arXiv:2505.04738v1 Announce Type: new 
Abstract: Neural operators, particularly the Deep Operator Network (DeepONet), have shown promise in learning mappings between function spaces for solving differential equations. However, standard DeepONet requires input functions to be sampled at fixed locations, limiting its applicability in scenarios with variable sensor configurations, missing data, or irregular grids. We introduce the Set Operator Network (SetONet), a novel architecture that integrates Deep Sets principles into the DeepONet framework to address this limitation. The core innovation lies in the SetONet branch network, which processes the input function as an unordered \emph{set} of location-value pairs. This design ensures permutation invariance with respect to the input points, making SetONet inherently robust to variations in the number and locations of sensors. SetONet learns richer, spatially-aware input representations by explicitly processing spatial coordinates and function values. We demonstrate SetONet's effectiveness on several benchmark problems, including derivative/anti-derivative operators, 1D Darcy flow, and 2D elasticity. Results show that SetONet successfully learns operators under variable input sampling conditions where standard DeepONet fails. Furthermore, SetONet is architecturally robust to sensor drop-off; unlike standard DeepONet, which requires methods like interpolation to function with missing data. Notably, SetONet can achieve comparable or improved accuracy over DeepONet on fixed grids, particularly for nonlinear problems, likely due to its enhanced input representation. SetONet provides a flexible and robust extension to the neural operator toolkit, significantly broadening the applicability of operator learning to problems with variable or incomplete input data.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Bad Data Leads to Good Models</title>
<link>https://arxiv.org/abs/2505.04741</link>
<guid>https://arxiv.org/abs/2505.04741</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, data quality, toxicity, pre-training, detoxifying techniques

<br /><br />Summary: This paper investigates the role of data quality in large language model (LLM) pretraining, specifically examining how pre-training on toxic data can potentially improve post-training control over model outputs. The authors conduct a toy experiment to analyze how varying data composition affects feature representation in the model space. Through experiments with the Olmo-1B models, they find that an increased proportion of toxic data results in representations of toxicity that is less entangled linearly, despite being associated with higher generational toxicity. Notably, models trained on toxic data demonstrate enhanced detoxification capabilities when subjected to intervention techniques at inference time. Evaluations using Toxigen and Real Toxicity Prompts reveal that these models strike a better balance between maintaining general capabilities and successfully reducing generational toxicity. Ultimately, the findings challenge traditional notions surrounding data quality by suggesting that, when considering post-training strategies, the introduction of toxic data may contribute positively to the overall effectiveness of LLMs. <div>
arXiv:2505.04741v1 Announce Type: new 
Abstract: In large language model (LLM) pretraining, data quality is believed to determine model quality. In this paper, we re-examine the notion of "quality" from the perspective of pre- and post-training co-design. Specifically, we explore the possibility that pre-training on more toxic data can lead to better control in post-training, ultimately decreasing a model's output toxicity. First, we use a toy experiment to study how data composition affects the geometry of features in the representation space. Next, through controlled experiments with Olmo-1B models trained on varying ratios of clean and toxic data, we find that the concept of toxicity enjoys a less entangled linear representation as the proportion of toxic data increases. Furthermore, we show that although toxic data increases the generational toxicity of the base model, it also makes the toxicity easier to remove. Evaluations on Toxigen and Real Toxicity Prompts demonstrate that models trained on toxic data achieve a better trade-off between reducing generational toxicity and preserving general capabilities when detoxifying techniques such as inference-time intervention (ITI) are applied. Our findings suggest that, with post-training taken into account, bad data may lead to good models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Primal-dual algorithm for contextual stochastic combinatorial optimization</title>
<link>https://arxiv.org/abs/2505.04757</link>
<guid>https://arxiv.org/abs/2505.04757</guid>
<content:encoded><![CDATA[
<div> Keywords: stochastic optimization, neural networks, empirical risk, combinatorial settings, linear convergence  

<br /><br />Summary: This paper presents a new method for contextual stochastic optimization by merging operations research with machine learning to improve decision-making in uncertain environments. Traditional optimization methods often overlook contextual factors, necessitating the development of novel algorithms. The authors leverage neural networks equipped with combinatorial optimization layers to encapsulate policies aimed at minimizing empirical risk derived from historical data on uncertain parameters and contexts. They introduce a surrogate learning problem alongside a versatile primal-dual algorithm applicable across various combinatorial optimization scenarios. The approach expands existing Fenchel-Young loss principles and proposes a fresh regularization technique utilizing sparse perturbations on the distribution simplex, enabling manageable updates in the original space while supporting different objective functions. Notably, they establish the linear convergence of their algorithm under specified conditions and provide a quantifiable bound on the non-optimality of the derived policy concerning empirical risk. Experiments using a contextual stochastic minimum weight spanning tree problem illustrate the efficiency and scalability of the proposed algorithm, which achieves performance levels similar to those obtained through imitation learning of solutions developed with a costly Lagrangian-based heuristic. <div>
arXiv:2505.04757v1 Announce Type: new 
Abstract: This paper introduces a novel approach to contextual stochastic optimization, integrating operations research and machine learning to address decision-making under uncertainty. Traditional methods often fail to leverage contextual information, which underscores the necessity for new algorithms. In this study, we utilize neural networks with combinatorial optimization layers to encode policies. Our goal is to minimize the empirical risk, which is estimated from past data on uncertain parameters and contexts. To that end, we present a surrogate learning problem and a generic primal-dual algorithm that is applicable to various combinatorial settings in stochastic optimization. Our approach extends classic Fenchel-Young loss results and introduces a new regularization method using sparse perturbations on the distribution simplex. This allows for tractable updates in the original space and can accommodate diverse objective functions. We demonstrate the linear convergence of our algorithm under certain conditions and provide a bound on the non-optimality of the resulting policy in terms of the empirical risk. Experiments on a contextual stochastic minimum weight spanning tree problem show that our algorithm is efficient and scalable, achieving performance comparable to imitation learning of solutions computed using an expensive Lagrangian-based heuristic.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction via Shapley Value Regression</title>
<link>https://arxiv.org/abs/2505.04775</link>
<guid>https://arxiv.org/abs/2505.04775</guid>
<content:encoded><![CDATA[
<div> Keywords: Shapley values, black-box models, ViaSHAP, Kolmogorov-Arnold, tabular data  

<br /><br />Summary: Shapley values are a powerful tool for explaining predictions from black-box models, but traditional computation methods can be computationally expensive during inference. To address this challenge, a new method called ViaSHAP is introduced, which learns to compute Shapley values directly, allowing predictions to be derived through simple summation. The paper explores two implementation approaches for ViaSHAP: one based on the universal approximation theorem and another utilizing the Kolmogorov-Arnold representation theorem. Results from extensive empirical investigations demonstrate that the ViaSHAP method, particularly when employing Kolmogorov-Arnold Networks, performs comparably to state-of-the-art algorithms designed for tabular data. Moreover, it is highlighted that the explanations generated by ViaSHAP are significantly more accurate than those produced by the widely-used FastSHAP method, both in the context of tabular data and image data. The findings underscore the potential of ViaSHAP as a more efficient and effective means of providing accurate explanations for model predictions in challenging environments. <div>
arXiv:2505.04775v1 Announce Type: new 
Abstract: Shapley values have several desirable, theoretically well-supported, properties for explaining black-box model predictions. Traditionally, Shapley values are computed post-hoc, leading to additional computational cost at inference time. To overcome this, a novel method, called ViaSHAP, is proposed, that learns a function to compute Shapley values, from which the predictions can be derived directly by summation. Two approaches to implement the proposed method are explored; one based on the universal approximation theorem and the other on the Kolmogorov-Arnold representation theorem. Results from a large-scale empirical investigation are presented, showing that ViaSHAP using Kolmogorov-Arnold Networks performs on par with state-of-the-art algorithms for tabular data. It is also shown that the explanations of ViaSHAP are significantly more accurate than the popular approximator FastSHAP on both tabular data and images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust ML Auditing using Prior Knowledge</title>
<link>https://arxiv.org/abs/2505.04796</link>
<guid>https://arxiv.org/abs/2505.04796</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, auditing, fairness, manipulation, regulations  

<br /><br />Summary: The widespread use of machine learning (ML) decision-making systems has prompted the establishment of regulations governing their behavior and development. A critical yet under-explored issue in this context is the risk of manipulation during fairness audits. This manipulation can occur when a platform intentionally alters its responses to regulators while maintaining different answers for other users. The paper presents a new approach to manipulation-proof auditing by considering the auditor's prior knowledge of the task performed by the platform. It stresses that auditors should not depend on public priors, such as datasets, as platforms can easily deceive them in these situations. The authors outline conditions under which auditors can thwart manipulative tactics by leveraging their understanding of the ground truth. Furthermore, experimental results using two standard datasets illustrate the extent of unfairness that platforms can conceal before being flagged as malicious. Overall, the formalization and expansion of manipulation-proof auditing with a prior knowledge framework pave the way for future research aimed at enhancing the robustness of fairness audits in ML systems. <div>
arXiv:2505.04796v1 Announce Type: new 
Abstract: The rapid adoption of ML decision-making systems across products and services has led to a set of regulations on how such systems should behave and be built. Among all the technical challenges to enforcing these regulations, one crucial, yet under-explored problem is the risk of manipulation while these systems are being audited for fairness. This manipulation occurs when a platform deliberately alters its answers to a regulator to pass an audit without modifying its answers to other users. In this paper, we introduce a novel approach to manipulation-proof auditing by taking into account the auditor's prior knowledge of the task solved by the platform. We first demonstrate that regulators must not rely on public priors (e.g. a public dataset), as platforms could easily fool the auditor in such cases. We then formally establish the conditions under which an auditor can prevent audit manipulations using prior knowledge about the ground truth. Finally, our experiments with two standard datasets exemplify the maximum level of unfairness a platform can hide before being detected as malicious. Our formalization and generalization of manipulation-proof auditing with a prior opens up new research directions for more robust fairness audits.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORBIT-2: Scaling Exascale Vision Foundation Models for Weather and Climate Downscaling</title>
<link>https://arxiv.org/abs/2505.04802</link>
<guid>https://arxiv.org/abs/2505.04802</guid>
<content:encoded><![CDATA[
<div> Keywords: climate downscaling, ORBIT-2, Residual Slim ViT, TILES, AI methods

<br /><br />Summary: Sparse observations and coarse-resolution climate models hinder regional decision-making, necessitating effective downscaling techniques. Existing AI methods face challenges in generalizing across different variables and geographical areas while also being limited by the quadratic complexity associated with Vision Transformer (ViT) self-attention mechanisms. To address these issues, we introduce ORBIT-2, a scalable foundation model designed for hyper-resolution climate downscaling. 

Key innovations of ORBIT-2 include: (1) Residual Slim ViT (Reslim), which offers a lightweight architecture coupled with residual learning and Bayesian regularization, enhancing prediction efficiency and robustness; and (2) TILES, a novel tile-wise sequence scaling algorithm that minimizes the complexity of self-attention from quadratic to linear, allowing for the processing of long sequences and facilitating massive parallelism. 

ORBIT-2 is capable of scaling up to 10 billion parameters across 32,768 GPUs, achieving a remarkable sustained throughput of up to 1.8 ExaFLOPS and demonstrating strong scaling efficiency of 92-98%. It can downscale data to a global resolution of 0.9 km while managing sequences of up to 4.2 billion tokens. On benchmarks with 7 km resolution, ORBIT-2 displays exceptional accuracy, with R^2 scores ranging from 0.98 to 0.99 compared to observational data. <div>
arXiv:2505.04802v1 Announce Type: new 
Abstract: Sparse observations and coarse-resolution climate models limit effective regional decision-making, underscoring the need for robust downscaling. However, existing AI methods struggle with generalization across variables and geographies and are constrained by the quadratic complexity of Vision Transformer (ViT) self-attention. We introduce ORBIT-2, a scalable foundation model for global, hyper-resolution climate downscaling. ORBIT-2 incorporates two key innovations: (1) Residual Slim ViT (Reslim), a lightweight architecture with residual learning and Bayesian regularization for efficient, robust prediction; and (2) TILES, a tile-wise sequence scaling algorithm that reduces self-attention complexity from quadratic to linear, enabling long-sequence processing and massive parallelism. ORBIT-2 scales to 10 billion parameters across 32,768 GPUs, achieving up to 1.8 ExaFLOPS sustained throughput and 92-98% strong scaling efficiency. It supports downscaling to 0.9 km global resolution and processes sequences up to 4.2 billion tokens. On 7 km resolution benchmarks, ORBIT-2 achieves high accuracy with R^2 scores in the range of 0.98 to 0.99 against observation data.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Piecewise Constant Spectral Graph Neural Network</title>
<link>https://arxiv.org/abs/2505.04808</link>
<guid>https://arxiv.org/abs/2505.04808</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, spectral properties, polynomial filters, PieCoN, heterophilic datasets  

<br /><br />Summary:  
Graph Neural Networks (GNNs) have seen impressive advancements by utilizing graph structures in various fields. However, existing spectral GNNs often rely on low-degree polynomial filters. This limitation can prevent the full capture of graph spectral characteristics, and increasing the polynomial degree can lead to computational inefficiencies, performance plateaus, or degradation. To tackle these issues, this paper introduces the Piecewise Constant Spectral Graph Neural Network (PieCoN). PieCoN innovatively combines constant spectral filters with polynomial filters, allowing for a more adaptable approach to exploiting graph structures. The framework achieves this by partitioning the spectrum into intervals, thus enhancing the range of spectral properties that can be learned effectively. Extensive experiments were conducted across nine benchmark datasets, representing both homophilic and heterophilic graphs. The results reveal that PieCoN excels particularly in the context of heterophilic datasets, suggesting its promising applicability in diverse scenarios. Overall, the proposed method stands out by providing a more flexible and efficient means to capture complex graph spectral information, potentially broadening the scope of GNN applications in future research. <div>
arXiv:2505.04808v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved significant success across various domains by leveraging graph structures in data. Existing spectral GNNs, which use low-degree polynomial filters to capture graph spectral properties, may not fully identify the graph's spectral characteristics because of the polynomial's small degree. However, increasing the polynomial degree is computationally expensive and beyond certain thresholds leads to performance plateaus or degradation. In this paper, we introduce the Piecewise Constant Spectral Graph Neural Network(PieCoN) to address these challenges. PieCoN combines constant spectral filters with polynomial filters to provide a more flexible way to leverage the graph structure. By adaptively partitioning the spectrum into intervals, our approach increases the range of spectral properties that can be effectively learned. Experiments on nine benchmark datasets, including both homophilic and heterophilic graphs, demonstrate that PieCoN is particularly effective on heterophilic datasets, highlighting its potential for a wide range of applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guide your favorite protein sequence generative model</title>
<link>https://arxiv.org/abs/2505.04823</link>
<guid>https://arxiv.org/abs/2505.04823</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative models, Protein engineering, Conditioning, ProteinMPNN, ESM3  

<br /><br />Summary: Generative machine learning models are making significant strides in protein engineering; however, there is a lack of a systematic method to condition these models with supplementary information effectively. This paper introduces ProteinGuide, a comprehensive framework designed to address this need. The framework allows for the integration of experimental feedback and existing classifiers, such as those predicting enzyme commission numbers, to enhance the output of generative models. ProteinGuide unifies various protein generative models, including masked language models, autoregressive models, diffusion models, and flow-matching models, enabling statistical conditioning of pre-trained models. The efficacy of the proposed approach is illustrated by applying it to two prevalent protein generative models: ProteinMPNN and ESM3. The study showcases how to guide these models in generating amino acid sequences and structural tokens that meet user-defined requirements, particularly focusing on enhancing stability and generating protein folds labeled by the CATH classification system. This framework represents a significant advancement in protein design by allowing iterative refinement based on specific desired properties, thus improving the overall efficiency and applicability of generative protein engineering. <div>
arXiv:2505.04823v1 Announce Type: new 
Abstract: Generative machine learning models have begun to transform protein engineering, yet no principled framework for conditioning on auxiliary information in a plug-and-play manner exists; one may want to iteratively incorporate experimental feedback, or make use of an existing classifier -- such as for predicting enzyme commission number -- in order to guide the sampling of the generative model to generate sequences with desired properties. Herein, we present ProteinGuide, a rigorous and general framework to achieve just that: through unifying a broad class of protein generative models that includes masked language, (order-agnostic) autoregressive, diffusion and flow-matching models, we provide an approach to statistically condition pre-trained protein generative models. We demonstrate applicability of our approach by guiding each of two commonly used protein generative models, ProteinMPNN and ESM3, to generate amino acid and structure token sequences conditioned on several user-specified properties, namely, enhanced stability and CATH-labeled fold generation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers</title>
<link>https://arxiv.org/abs/2505.04842</link>
<guid>https://arxiv.org/abs/2505.04842</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, value function, LLM, test-time compute, generalization  

<br /><br />Summary:  
The article presents a novel reinforcement learning method named RL$^V$ designed to enhance the fine-tuning of large language models (LLMs) like GRPO and Leave-one-out PPO. Traditional methods often discard the learned value function, relying instead on empirically estimated returns, which complicates test-time compute scaling. RL$^V$ addresses this issue by allowing the LLM to function as both a reasoner and a generative verifier, generating data through reinforcement learning while maintaining verification capabilities. This approach significantly improves MATH accuracy by over 20% with parallel sampling and boosts test-time compute efficiency by 8-32 times compared to existing methods. Furthermore, RL$^V$ demonstrates strong generalization skills across various tasks, ranging from easy to hard and including out-of-domain challenges. The method also achieves a 1.2-1.6 times higher performance when scaling both parallel and sequential test-time computations using a long reasoning R1 model. Overall, RL$^V$ represents a promising advancement in reinforcement learning for LLMs, offering crucial enhancements in both accuracy and computational efficiency. <div>
arXiv:2505.04842v1 Announce Type: new 
Abstract: Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL$^V$ boosts MATH accuracy by over 20\% with parallel sampling and enables $8-32\times$ efficient test-time compute scaling compared to the base RL method. RL$^V$ also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves $1.2-1.6\times$ higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning for Cyber Physical Systems: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.04873</link>
<guid>https://arxiv.org/abs/2505.04873</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, cyber physical systems, federated learning, intelligent transportation, smart cities

<br /><br />Summary: The integration of machine learning (ML) into cyber-physical systems (CPS) presents numerous challenges related to real-time decision-making, safety, reliability, device diversity, and data privacy. In recent years, federated learning (FL) has emerged as a prominent ML approach, allowing model training from decentralized data sources. This work aims to analyze recent developments in FL-CPS, discussing their integration in detail. It highlights various application areas including intelligent transportation systems, cybersecurity services, smart cities, and smart healthcare. The paper compares FL applications in CPS with those in the Internet of Things (IoT) to underscore their connections and distinctions. Additionally, critical insights and lessons from various FL-CPS implementations are examined. The conclusion addresses significant concerns and outlines potential research avenues necessary to harness the full potential of FL in CPS. This study serves as a comprehensive overview of how federated learning can be effectively applied within cyber-physical systems, pointing to its growing importance in modern technological ecosystems. <div>
arXiv:2505.04873v1 Announce Type: new 
Abstract: The integration of machine learning (ML) in cyber physical systems (CPS) is a complex task due to the challenges that arise in terms of real-time decision making, safety, reliability, device heterogeneity, and data privacy. There are also open research questions that must be addressed in order to fully realize the potential of ML in CPS. Federated learning (FL), a distributed approach to ML, has become increasingly popular in recent years. It allows models to be trained using data from decentralized sources. This approach has been gaining popularity in the CPS field, as it integrates computer, communication, and physical processes. Therefore, the purpose of this work is to provide a comprehensive analysis of the most recent developments of FL-CPS, including the numerous application areas, system topologies, and algorithms developed in recent years. The paper starts by discussing recent advances in both FL and CPS, followed by their integration. Then, the paper compares the application of FL in CPS with its applications in the internet of things (IoT) in further depth to show their connections and distinctions. Furthermore, the article scrutinizes how FL is utilized in critical CPS applications, e.g., intelligent transportation systems, cybersecurity services, smart cities, and smart healthcare solutions. The study also includes critical insights and lessons learned from various FL-CPS implementations. The paper's concluding section delves into significant concerns and suggests avenues for further research in this fast-paced and dynamic era.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning</title>
<link>https://arxiv.org/abs/2505.04881</link>
<guid>https://arxiv.org/abs/2505.04881</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, Chain-of-Thought, confidence-guided compression, redundant reflection, Early Stopping  

<br /><br />Summary:  
Large Reasoning Models (LRMs) excel in complex reasoning tasks through Chain-of-Thought (CoT) prompting but often produce verbose outputs due to redundant content. This redundancy increases computational overhead and negatively affects user experience. Existing approaches like post-hoc pruning and sampling-based selection struggle with maintaining reasoning coherence. The authors introduce a confidence-guided perspective that identifies two main patterns responsible for redundancy: Confidence Deficit, where the model second-guesses correct steps due to low confidence, and Termination Delay, where reasoning continues despite achieving a confident answer. To address these issues, they propose ConCISE (Confidence-guided Compression In Step-by-step Efficient Reasoning), which enhances reasoning by reinforcing model confidence during inference. This framework incorporates Confidence Injection to stabilize intermediate reasoning steps and Early Stopping to conclude reasoning when sufficient confidence is reached. Experimental results show that fine-tuning LRMs on ConCISE-generated data leads to output reductions of approximately 50% in length under SimPO while preserving high task accuracy. ConCISE also consistently outperforms other existing methods across various reasoning benchmarks, making it a promising approach for improving efficiency in LRM outputs. <div>
arXiv:2505.04881v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via Chain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused by redundant content, increasing computational overhead, and degrading user experience. Existing compression methods either operate post-hoc pruning, risking disruption to reasoning coherence, or rely on sampling-based selection, which fails to intervene effectively during generation. In this work, we introduce a confidence-guided perspective to explain the emergence of redundant reflection in LRMs, identifying two key patterns: Confidence Deficit, where the model reconsiders correct steps due to low internal confidence, and Termination Delay, where reasoning continues even after reaching a confident answer. Based on this analysis, we propose ConCISE (Confidence-guided Compression In Step-by-step Efficient Reasoning), a framework that simplifies reasoning chains by reinforcing the model's confidence during inference, thus preventing the generation of redundant reflection steps. It integrates Confidence Injection to stabilize intermediate steps and Early Stopping to terminate reasoning when confidence is sufficient. Extensive experiments demonstrate that fine-tuning LRMs on ConCISE-generated data yields significantly shorter outputs, reducing length by up to approximately 50% under SimPO, while maintaining high task accuracy. ConCISE consistently outperforms existing baselines across multiple reasoning benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedRE: Robust and Effective Federated Learning with Privacy Preference</title>
<link>https://arxiv.org/abs/2505.04889</link>
<guid>https://arxiv.org/abs/2505.04889</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, privacy protection, local differential privacy, privacy-sensitive information, parameter aggregation

<br /><br />Summary: This article addresses the challenge of privacy leakage in Federated Learning (FL), where uploaded gradients can reveal private information. While local differential privacy (LDP) has been integrated into FL to enhance privacy, current methods are not tailored to individual clients’ privacy preferences, often applying a uniform perturbation approach. This can lead to excessive noise for private-insensitive information, negatively impacting model performance. The authors propose a novel approach called FedRE, which defines privacy-sensitive information (PSI) based on individual client preferences. The study optimizes LDP by applying a layered privacy budget allocation strategy, allowing for stricter privacy guarantees for data deemed more sensitive. Additionally, a parameter aggregation mechanism is introduced to minimize performance degradation resulting from LDP-induced noise, focusing on the distribution of the perturbed data. Experimental results on text tamper detection using T-SROIE and DocTamper datasets demonstrate that FedRE maintains competitive performance compared to existing state-of-the-art methods, effectively balancing privacy protection with model efficacy. Overall, FedRE presents a refined approach to FL that respects client-specific privacy concerns while improving the robustness and effectiveness of the learning process. <div>
arXiv:2505.04889v1 Announce Type: new 
Abstract: Despite Federated Learning (FL) employing gradient aggregation at the server for distributed training to prevent the privacy leakage of raw data, private information can still be divulged through the analysis of uploaded gradients from clients. Substantial efforts have been made to integrate local differential privacy (LDP) into the system to achieve a strict privacy guarantee. However, existing methods fail to take practical issues into account by merely perturbing each sample with the same mechanism while each client may have their own privacy preferences on privacy-sensitive information (PSI), which is not uniformly distributed across the raw data. In such a case, excessive privacy protection from private-insensitive information can additionally introduce unnecessary noise, which may degrade the model performance. In this work, we study the PSI within data and develop FedRE, that can simultaneously achieve robustness and effectiveness benefits with LDP protection. More specifically, we first define PSI with regard to the privacy preferences of each client. Then, we optimize the LDP by allocating less privacy budget to gradients with higher PSI in a layer-wise manner, thus providing a stricter privacy guarantee for PSI. Furthermore, to mitigate the performance degradation caused by LDP, we design a parameter aggregation mechanism based on the distribution of the perturbed information. We conducted experiments with text tamper detection on T-SROIE and DocTamper datasets, and FedRE achieves competitive performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clustering with Communication: A Variational Framework for Single Cell Representation Learning</title>
<link>https://arxiv.org/abs/2505.04891</link>
<guid>https://arxiv.org/abs/2505.04891</guid>
<content:encoded><![CDATA[
<div> Keywords: scRNA-seq, cell-cell communication, variational autoencoder, CCCVAE, clustering performance  

<br /><br />Summary: Single-cell RNA sequencing (scRNA-seq) has unveiled significant cellular heterogeneity, but insights into biological functions necessitate understanding cell-cell communication (CCC), the signaling interactions driven by ligand-receptor pairs. Tools such as CellChat highlight CCC's vital role in processes like cell differentiation and immune responses, showing that transcriptomic data contains critical intercellular signaling information. To address this, the authors introduce CCCVAE, a new variational autoencoder framework that integrates CCC signals into single-cell representation learning. CCCVAE employs a communication-aware kernel based on ligand-receptor interactions and a sparse Gaussian process, embedding biologically informed priors within the latent space. This contrasts with traditional VAEs that treat cells independently, as CCCVAE promotes latent embeddings that capture both transcriptional similarity and intercellular signaling context. Empirical results from four scRNA-seq datasets indicate that CCCVAE enhances clustering performance, achieving improved evaluation scores compared to standard VAE models. The findings emphasize the importance of incorporating biological priors into deep generative models for effective unsupervised analysis of single-cell data. <div>
arXiv:2505.04891v1 Announce Type: new 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular heterogeneity, but recent studies emphasize that understanding biological function also requires modeling cell-cell communication (CCC), the signaling interactions mediated by ligand-receptor pairs that coordinate cellular behavior. Tools like CellChat have demonstrated that CCC plays a critical role in processes such as cell differentiation, tissue regeneration, and immune response, and that transcriptomic data inherently encodes rich information about intercellular signaling. We propose CCCVAE, a novel variational autoencoder framework that incorporates CCC signals into single-cell representation learning. By leveraging a communication-aware kernel derived from ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes biologically informed priors into the latent space. Unlike conventional VAEs that treat each cell independently, CCCVAE encourages latent embeddings to reflect both transcriptional similarity and intercellular signaling context. Empirical results across four scRNA-seq datasets show that CCCVAE improves clustering performance, achieving higher evaluation scores than standard VAE baselines. This work demonstrates the value of embedding biological priors into deep generative models for unsupervised single-cell analysis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GCN-Based Throughput-Oriented Handover Management in Dense 5G Vehicular Networks</title>
<link>https://arxiv.org/abs/2505.04894</link>
<guid>https://arxiv.org/abs/2505.04894</guid>
<content:encoded><![CDATA[
<div> Keywords: 5G, vehicular networks, handover management, graph neural networks, TH-GCN

<br /><br />Summary: The advancement of 5G technology significantly benefits vehicular networks by providing high bandwidth, low latency, and fast data rates, essential for real-time applications in smart cities and vehicles. These enhancements contribute to better traffic safety and entertainment services. However, challenges like limited coverage and frequent handovers lead to network instability in high-mobility environments, particularly due to the ping-pong effect. This paper introduces TH-GCN (Throughput-oriented Graph Convolutional Network), a novel method aimed at optimizing handover management in dense 5G networks. The approach utilizes graph neural networks (GNNs) to represent vehicles and base stations as nodes within a dynamic graph, incorporating features such as signal quality, throughput, vehicle speed, and base station load. By adopting a dual-centric perspective that considers both user equipment and base stations, TH-GCN enables adaptive, real-time handover decisions that enhance network stability. Simulation results demonstrate that TH-GCN is effective, achieving a reduction in handovers by up to 78 percent and a 10 percent improvement in signal quality, significantly outperforming existing handover management methods. <div>
arXiv:2505.04894v1 Announce Type: new 
Abstract: The rapid advancement of 5G has transformed vehicular networks, offering high bandwidth, low latency, and fast data rates essential for real-time applications in smart cities and vehicles. These improvements enhance traffic safety and entertainment services. However, the limited coverage and frequent handovers in 5G networks cause network instability, especially in high-mobility environments due to the ping-pong effect. This paper presents TH-GCN (Throughput-oriented Graph Convolutional Network), a novel approach for optimizing handover management in dense 5G networks. Using graph neural networks (GNNs), TH-GCN models vehicles and base stations as nodes in a dynamic graph enriched with features such as signal quality, throughput, vehicle speed, and base station load. By integrating both user equipment and base station perspectives, this dual-centric approach enables adaptive, real-time handover decisions that improve network stability. Simulation results show that TH-GCN reduces handovers by up to 78 percent and improves signal quality by 10 percent, outperforming existing methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precise gradient descent training dynamics for finite-width multi-layer neural networks</title>
<link>https://arxiv.org/abs/2505.04898</link>
<guid>https://arxiv.org/abs/2505.04898</guid>
<content:encoded><![CDATA[
<div> Keywords: gradient descent, neural networks, distributional characterization, generalization error, finite-width regime

<br /><br />Summary: This paper introduces a precise distributional characterization of gradient descent iterates for general multi-layer neural networks, focusing on the finite-width proportional regime, where sample size and feature dimension grow proportionally. Unlike existing theories such as neural tangent kernel, mean-field, and tensor program, the authors operate within the finite-width framework, allowing weight evolution beyond lazy training and independent of special initialization schemes. The proposed non-asymptotic state evolution theory captures Gaussian fluctuations in first-layer weights and concentration in deeper layers, applicable even to non-Gaussian features. Additionally, the framework characterizes training and generalization errors in a manner that surpasses uniform convergence, which is predominantly focused on two-layer settings in previous works. A practical application of this theory demonstrates that vanilla gradient descent can be enhanced to provide consistent estimates of the generalization error at each iteration, aiding in early stopping and hyperparameter tuning. The authors further elucidate that, despite potential model misspecification, the gradient descent algorithm retains the essence of a single-index function, with the effective signal influenced by both the true signal and the initialization process. <div>
arXiv:2505.04898v1 Announce Type: new 
Abstract: In this paper, we provide the first precise distributional characterization of gradient descent iterates for general multi-layer neural networks under the canonical single-index regression model, in the `finite-width proportional regime' where the sample size and feature dimension grow proportionally while the network width and depth remain bounded. Our non-asymptotic state evolution theory captures Gaussian fluctuations in first-layer weights and concentration in deeper-layer weights, and remains valid for non-Gaussian features.
  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF) theories and tensor program (TP) in several key aspects. First, our theory operates in the finite-width regime whereas these existing theories are fundamentally infinite-width. Second, our theory allows weights to evolve from individual initializations beyond the lazy training regime, whereas NTK and MF are either frozen at or only weakly sensitive to initialization, and TP relies on special initialization schemes. Third, our theory characterizes both training and generalization errors for general multi-layer neural networks beyond the uniform convergence regime, whereas existing theories study generalization almost exclusively in two-layer settings.
  As a statistical application, we show that vanilla gradient descent can be augmented to yield consistent estimates of the generalization error at each iteration, which can be used to guide early stopping and hyperparameter tuning. As a further theoretical implication, we show that despite model misspecification, the model learned by gradient descent retains the structure of a single-index function with an effective signal determined by a linear combination of the true signal and the initialization.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VaCDA: Variational Contrastive Alignment-based Scalable Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.04907</link>
<guid>https://arxiv.org/abs/2505.04907</guid>
<content:encoded><![CDATA[
<div> Keywords: wearable devices, variational autoencoder, contrastive learning, domain adaptation, heterogeneous data

<br /><br />Summary: 
Technological advancements in wearable devices have led to continuous monitoring of user activities, resulting in large amounts of unlabeled data that are difficult to interpret. The manual annotation of this data is labor-intensive and often error-prone, compounded by the heterogeneous nature of data due to differences in device placement, type, and user behavior. Traditional transfer learning methods struggle under these conditions, making daily activity recognition challenging. To tackle these issues, the authors propose a method using a variational autoencoder (VAE) to create a shared, low-dimensional latent space that generalizes across diverse sensor data. This approach aims to reduce data heterogeneity and enhance adaptation to the target domain. Furthermore, they incorporate contrastive learning to improve feature representation by aligning instances of the same class across different domains while ensuring separation between different classes. The proposed framework, Variational Contrastive Domain Adaptation (VaCDA), combines VAEs and contrastive learning for effective multi-source domain adaptation. Evaluations on several publicly available datasets across cross-person, cross-position, and cross-device scenarios demonstrate that VaCDA outperforms baseline methods, particularly in cross-position and cross-device situations. <div>
arXiv:2505.04907v1 Announce Type: new 
Abstract: Technological advancements have led to the rise of wearable devices with sensors that continuously monitor user activities, generating vast amounts of unlabeled data. This data is challenging to interpret, and manual annotation is labor-intensive and error-prone. Additionally, data distribution is often heterogeneous due to device placement, type, and user behavior variations. As a result, traditional transfer learning methods perform suboptimally, making it difficult to recognize daily activities. To address these challenges, we use a variational autoencoder (VAE) to learn a shared, low-dimensional latent space from available sensor data. This space generalizes data across diverse sensors, mitigating heterogeneity and aiding robust adaptation to the target domain. We integrate contrastive learning to enhance feature representation by aligning instances of the same class across domains while separating different classes. We propose Variational Contrastive Domain Adaptation (VaCDA), a multi-source domain adaptation framework combining VAEs and contrastive learning to improve feature representation and reduce heterogeneity between source and target domains. We evaluate VaCDA on multiple publicly available datasets across three heterogeneity scenarios: cross-person, cross-position, and cross-device. VaCDA outperforms the baselines in cross-position and cross-device scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction</title>
<link>https://arxiv.org/abs/2505.04918</link>
<guid>https://arxiv.org/abs/2505.04918</guid>
<content:encoded><![CDATA[
<div> Keywords: weather prediction, deep learning, Physics-ASSisted, topology, PASSAT

<br /><br />Summary: The article presents PASSAT, a novel deep learning model specifically designed for weather prediction that integrates essential physics and the topology of the Earth's surface. Unlike traditional models, PASSAT emphasizes two main factors influencing weather evolution: the advection process, modeled through the advection and Navier-Stokes equations, and the complex interactions between the Earth and atmosphere, which are often challenging to quantify. This model numerically addresses the advection and Navier-Stokes equations on a spherical manifold, which is a significant improvement over modeling the Earth as a flat surface. Additionally, PASSAT employs a spherical graph neural network to effectively capture the dynamics of Earth-atmosphere interactions. This network also generates the initial velocity fields necessary for solving the advection equation. Upon testing with the ERA5 data set at a resolution of $5.625^\circ$, PASSAT demonstrated superior performance compared to leading deep learning-based weather prediction models and the established operational model, IFS T42. The authors have made the code and model checkpoints accessible on GitHub, promoting further research and development in this domain. <div>
arXiv:2505.04918v1 Announce Type: new 
Abstract: Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the \textbf{physics} of the underlying weather evolution or the \textbf{topology} of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the $5.625^\circ$-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Uncertainty Quantification for Depression Prediction</title>
<link>https://arxiv.org/abs/2505.04931</link>
<guid>https://arxiv.org/abs/2505.04931</guid>
<content:encoded><![CDATA[
<div> Keywords: depression prediction, deep learning, algorithmic fairness, uncertainty quantification, Fair Uncertainty Quantification

<br /><br />Summary: The paper addresses the critical need for trustworthy depression prediction using deep learning by emphasizing both predictive reliability and algorithmic fairness across diverse demographic groups. While recent studies have explored uncertainty quantification (UQ) for reliable predictions, few have examined its fairness implications within depression prediction. This work focuses on the Equal Opportunity Coverage (EOC) fairness framework and introduces Fair Uncertainty Quantification (FUQ) to develop fair and reliable predictions. The authors group participants by sensitive attributes and utilize conformal prediction to quantify uncertainty within each demographic, which enables robust UQ assessment and fairness evaluation. Additionally, a fairness-aware optimization strategy is proposed, framing fairness as a constrained optimization problem under EOC criteria. This approach allows the model to maintain predictive reliability while adjusting to varied uncertainty levels across different groups. Comprehensive evaluations on visual and audio datasets related to depression indicate that the proposed methodology effectively balances reliability and fairness in predictions, resulting in improved outcomes across demographic categories. Overall, this research contributes valuable insights into integrating fairness considerations into UQ for critical clinical applications. <div>
arXiv:2505.04931v1 Announce Type: new 
Abstract: Trustworthy depression prediction based on deep learning, incorporating both predictive reliability and algorithmic fairness across diverse demographic groups, is crucial for clinical application. Recently, achieving reliable depression predictions through uncertainty quantification has attracted increasing attention. However, few studies have focused on the fairness of uncertainty quantification (UQ) in depression prediction. In this work, we investigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage (EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for depression prediction. FUQ pursues reliable and fair depression predictions through group-based analysis. Specifically, we first group all the participants by different sensitive attributes and leverage conformal prediction to quantify uncertainty within each demographic group, which provides a theoretically guaranteed and valid way to quantify uncertainty for depression prediction and facilitates the investigation of fairness across different demographic groups. Furthermore, we propose a fairness-aware optimization strategy that formulates fairness as a constrained optimization problem under EOC constraints. This enables the model to preserve predictive reliability while adapting to the heterogeneous uncertainty levels across demographic groups, thereby achieving optimal fairness. Through extensive evaluations on several visual and audio depression datasets, our approach demonstrates its effectiveness.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structural Alignment in Link Prediction</title>
<link>https://arxiv.org/abs/2505.04939</link>
<guid>https://arxiv.org/abs/2505.04939</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Graphs, Link Prediction, structure-first perspective, embeddings, Structural Alignment Hypothesis  

<br /><br />Summary:  
This thesis investigates Knowledge Graphs (KGs), which are essential for modeling and linking large amounts of data. It highlights the incompleteness of real-world KGs and the rise of machine learning tools aimed at predicting missing information through the Link Prediction Task. Most current methods rely on an embedding-based approach that focuses on vector representations of individual nodes and edges. This work proposes a novel viewpoint, emphasizing a graph-structure-first perspective that considers the entire triple (node-edge-node) as the fundamental unit of analysis for KG information content. Through a thorough literature review and two primary experiment sets, the thesis concludes that this alternative approach offers valuable insights for understanding KG learning and can enhance cross-KG transfer learning for link prediction. It introduces the Structural Alignment Hypothesis, asserting that link prediction is inherently a structural task. Additionally, the thesis is noteworthy for being bilingual, featuring a main document in English and an extended summary in Irish, alongside an open-sourced translation dictionary of machine learning terminology termed the Foclóir Tráchtáis. All associated code and data are also made publicly accessible. <div>
arXiv:2505.04939v1 Announce Type: new 
Abstract: While Knowledge Graphs (KGs) have become increasingly popular across various scientific disciplines for their ability to model and interlink huge quantities of data, essentially all real-world KGs are known to be incomplete. As such, with the growth of KG use has been a concurrent development of machine learning tools designed to predict missing information in KGs, which is referred to as the Link Prediction Task. The majority of state-of-the-art link predictors to date have followed an embedding-based paradigm. In this paradigm, it is assumed that the information content of a KG is best represented by the (individual) vector representations of its nodes and edges, and that therefore node and edge embeddings are particularly well-suited to performing link prediction.
  This thesis proposes an alternative perspective on the field's approach to link prediction and KG data modelling. Specifically, this work re-analyses KGs and state-of-the-art link predictors from a graph-structure-first perspective that models the information content of a KG in terms of whole triples, rather than individual nodes and edges.
  Following a literature review and two core sets of experiments, this thesis concludes that a structure-first perspective on KGs and link prediction is both viable and useful for understanding KG learning and for enabling cross-KG transfer learning for the link prediction task. This observation is used to create and propose the Structural Alignment Hypothesis, which postulates that link prediction can be understood and modelled as a structural task.
  All code and data used for this thesis are open-sourced. This thesis was written bilingually, with the main document in English and an informal extended summary in Irish. An Irish-language translation dictionary of machine learning terms (the Focl\'oir Tr\'achtais) created for this work is open-sourced as well.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graffe: Graph Representation Learning via Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2505.04956</link>
<guid>https://arxiv.org/abs/2505.04956</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, graph representation, self-supervised learning, conditional mutual information, Graffe  

<br /><br />Summary: The paper introduces Graffe, a self-supervised diffusion probabilistic model tailored for graph representation learning. Despite the success of diffusion models in sample generation, their application in representation learning, especially for graphs, is still in early stages. Graffe includes a graph encoder that converts a source graph into a compact representation, which conditions the diffusion decoder's denoising process. The authors explore the theoretical aspects of using diffusion models in this context, demonstrating that the denoising objective implicitly maximizes the conditional mutual information between the data and its representation. They prove that the negative logarithm of the denoising score matching loss functions as a tractable lower bound for this mutual information. To validate their theoretical contributions, a series of empirical case studies were conducted. The results from these studies show that Graffe performs competitively under linear probing across node and graph classification tasks, achieving state-of-the-art results on 9 out of 11 real-world datasets. These insights suggest that generative models, particularly diffusion models, are effective tools for advancing graph representation learning. <div>
arXiv:2505.04956v1 Announce Type: new 
Abstract: Diffusion probabilistic models (DPMs), widely recognized for their potential to generate high-quality samples, tend to go unnoticed in representation learning. While recent progress has highlighted their potential for capturing visual semantics, adapting DPMs to graph representation learning remains in its infancy. In this paper, we introduce Graffe, a self-supervised diffusion model proposed for graph representation learning. It features a graph encoder that distills a source graph into a compact representation, which, in turn, serves as the condition to guide the denoising process of the diffusion decoder. To evaluate the effectiveness of our model, we first explore the theoretical foundations of applying diffusion models to representation learning, proving that the denoising objective implicitly maximizes the conditional mutual information between data and its representation. Specifically, we prove that the negative logarithm of the denoising score matching loss is a tractable lower bound for the conditional mutual information. Empirically, we conduct a series of case studies to validate our theoretical insights. In addition, Graffe delivers competitive results under the linear probing setting on node and graph classification tasks, achieving state-of-the-art performance on 9 of the 11 real-world datasets. These findings indicate that powerful generative models, especially diffusion models, serve as an effective tool for graph representation learning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Transform: A Unified Framework for Adaptive Transform to Enhance Representations</title>
<link>https://arxiv.org/abs/2505.04969</link>
<guid>https://arxiv.org/abs/2505.04969</guid>
<content:encoded><![CDATA[
<div> Keywords: General Transform, machine learning, discrete transforms, adaptive representation, dataset properties

<br /><br />Summary: This article discusses the limitations of traditional discrete transforms, like the discrete Fourier transform, in machine learning, particularly when dataset properties are not well understood. Conventional transforms are selected based on prior knowledge about the dataset, which can hinder performance when such information is lacking. To address this issue, the authors introduce the General Transform (GT), an adaptive representation that learns data-driven mappings specifically tailored to the dataset and the intended task. The key innovation of GT is its ability to adapt to the characteristics of the data, making it suitable for a wide range of machine learning applications. The authors present empirical evidence demonstrating that models utilizing GT consistently outperform those that rely on traditional transform-based approaches. This performance enhancement is observed across various domains, including computer vision and natural language processing, indicating GT's versatility and effectiveness in different learning scenarios. By leveraging a more adaptive and data-driven approach to feature extraction, GT provides a promising advancement in model performance and adaptability in machine learning practices. <div>
arXiv:2505.04969v1 Announce Type: new 
Abstract: Discrete transforms, such as the discrete Fourier transform, are widely used in machine learning to improve model performance by extracting meaningful features. However, with numerous transforms available, selecting an appropriate one often depends on understanding the dataset's properties, making the approach less effective when such knowledge is unavailable. In this work, we propose General Transform (GT), an adaptive transform-based representation designed for machine learning applications. Unlike conventional transforms, GT learns data-driven mapping tailored to the dataset and task of interest. Here, we demonstrate that models incorporating GT outperform conventional transform-based approaches across computer vision and natural language processing tasks, highlighting its effectiveness in diverse learning scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Aided Deep Reinforcement Learning for Resource Allocation in Dynamic Terahertz UAV Networks</title>
<link>https://arxiv.org/abs/2505.04981</link>
<guid>https://arxiv.org/abs/2505.04981</guid>
<content:encoded><![CDATA[
<div> Keywords: Terahertz, UAV networks, resource allocation, deep reinforcement learning, graph neural network  

<br /><br />Summary: This paper discusses the challenges and advancements in Terahertz (THz) unmanned aerial vehicle (UAV) networks. The expected flexibility in topologies and ultra-high data rates have a wide range of applications, including security surveillance and disaster response. However, the dynamic nature of these topologies complicates the long-term joint power and antenna array resource allocation among UAVs, resulting in a mixed-integer nonlinear programming (MINLP) problem characterized by non-convexity and NP-hardness. To address this, the authors propose GLOVE, a graph neural network (GNN)-aided deep reinforcement learning (DRL) algorithm that focuses on maximizing resource efficiency (RE). GLOVE learns the relationships between a UAV and its neighbors while emphasizing its self-node features. The algorithm employs a multi-task structure to enable cooperative training for power and sub-array resource allocations across all UAVs. Experimental results demonstrate that GLOVE significantly outperforms benchmark schemes, achieving the highest RE, the lowest latency, and maintaining zero packet loss throughout the training process, thereby showcasing its robustness in dynamic THz UAV networks. <div>
arXiv:2505.04981v1 Announce Type: new 
Abstract: Terahertz (THz) unmanned aerial vehicle (UAV) networks with flexible topologies and ultra-high data rates are expected to empower numerous applications in security surveillance, disaster response, and environmental monitoring, among others. However, the dynamic topologies hinder the efficient long-term joint power and antenna array resource allocation for THz links among UAVs. Furthermore, the continuous nature of power and the discrete nature of antennas cause this joint resource allocation problem to be a mixed-integer nonlinear programming (MINLP) problem with non-convexity and NP-hardness. Inspired by recent rapid advancements in deep reinforcement learning (DRL), a graph neural network (GNN) aided DRL algorithm for resource allocation in the dynamic THz UAV network with an emphasis on self-node features (GLOVE) is proposed in this paper, with the aim of resource efficiency (RE) maximization. When training the allocation policy for each UAV, GLOVE learns the relationship between this UAV and its neighboring UAVs via GNN, while also emphasizing the important self-node features of this UAV. In addition, a multi-task structure is leveraged by GLOVE to cooperatively train resource allocation decisions for the power and sub-arrays of all UAVs. Experimental results illustrate that GLOVE outperforms benchmark schemes in terms of the highest RE and the lowest latency. Moreover, unlike the benchmark methods with severe packet loss, GLOVE maintains zero packet loss during the entire training process, demonstrating its better robustness under the highly dynamic THz UAV network.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for Continuous Authentication</title>
<link>https://arxiv.org/abs/2505.05015</link>
<guid>https://arxiv.org/abs/2505.05015</guid>
<content:encoded><![CDATA[
<div> Keywords: continuous authentication, keyboard dynamics, behavioral biometrics, machine learning, Random Forest

<br /><br />Summary: This study explores continuous authentication systems utilizing free-text keyboard dynamics as an additional security layer in multifactor authentication setups. Employing an Agent-Based Model (ABM), researchers generated synthetic keystroke data from five unique agents, analyzing variables such as dwell time, flight time, and error rates over 5-second sliding windows. The performance of two machine learning approaches, One-Class Support Vector Machine (OC-SVM) and Random Forest (RF), was compared for user verification purposes. Results indicated a significant performance discrepancy; OC-SVM was ineffective in distinguishing individual users within groups, whereas RF maintained robust intra-keyboard user recognition, achieving over 70% accuracy. However, RF encountered challenges in generalizing user profiles across different keyboard types, underscoring the influence of hardware on typing behavior. The study concludes that reliable authentication may necessitate keyboard-specific user profiles and emphasizes the superiority of ensemble methods like Random Forest in capturing the nuanced patterns of individual users compared to One-Class SVM. Overall, the research highlights the potential of leveraging keyboard dynamics in secure authentication systems while acknowledging the hardware's significant role in typing behavior variability. <div>
arXiv:2505.05015v1 Announce Type: new 
Abstract: Continuous authentication systems leveraging free-text keyboard dynamics offer a promising additional layer of security in a multifactor authentication setup that can be used in a transparent way with no impact on user experience. This study investigates the efficacy of behavioral biometrics by employing an Agent-Based Model (ABM) to simulate diverse typing profiles across mechanical and membrane keyboards. Specifically, we generated synthetic keystroke data from five unique agents, capturing features related to dwell time, flight time, and error rates within sliding 5-second windows updated every second. Two machine learning approaches, One-Class Support Vector Machine (OC-SVM) and Random Forest (RF), were evaluated for user verification. Results revealed a stark contrast in performance: while One-Class SVM failed to differentiate individual users within each group, Random Forest achieved robust intra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize across keyboards for the same user, highlighting the significant impact of keyboard hardware on typing behavior. These findings suggest that: (1) keyboard-specific user profiles may be necessary for reliable authentication, and (2) ensemble methods like RF outperform One-Class SVM in capturing fine-grained user-specific patterns.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Reliable Synthetic Clinical Trial Data: The Role of Hyperparameter Optimization and Domain Constraints</title>
<link>https://arxiv.org/abs/2505.05019</link>
<guid>https://arxiv.org/abs/2505.05019</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic clinical data, hyperparameter optimization, generative models, data quality, domain knowledge

<br /><br />Summary: The generation of synthetic clinical trial data addresses privacy and accessibility concerns in medical research but faces challenges in maintaining fidelity and utility. This study evaluates four hyperparameter optimization (HPO) strategies across eight generative models, comparing single-metric versus compound metric optimization. Results indicate that HPO enhances synthetic data quality significantly, with improvements of up to 60% for TVAE, 39% for CTGAN, and 38% for CTAB-GAN+. Compound metric optimization yields more balanced and generalizable datasets compared to single metrics. However, HPO alone cannot guarantee clinically valid synthetic data. All models showed violations of essential survival constraints, with preprocessing and postprocessing being crucial in minimizing these issues. Models without robust processing steps produced invalid data in up to 61% of cases. The findings emphasize the importance of incorporating domain knowledge alongside HPO for generating high-quality synthetic datasets. The study offers actionable recommendations for refining synthetic data generation techniques and highlights the need for future research to optimize metric selection and validate results using larger datasets for enhanced clinical relevance. <div>
arXiv:2505.05019v1 Announce Type: new 
Abstract: The generation of synthetic clinical trial data offers a promising approach to mitigating privacy concerns and data accessibility limitations in medical research. However, ensuring that synthetic datasets maintain high fidelity, utility, and adherence to domain-specific constraints remains a key challenge. While hyperparameter optimization (HPO) has been shown to improve generative model performance, the effectiveness of different optimization strategies for synthetic clinical data remains unclear. This study systematically evaluates four HPO strategies across eight generative models, comparing single-metric optimization against compound metric optimization approaches. Our results demonstrate that HPO consistently improves synthetic data quality, with TVAE, CTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%, respectively. Compound metric optimization outperformed single-metric strategies, producing more balanced and generalizable synthetic datasets. Interestingly, HPO alone is insufficient to ensure clinically valid synthetic data, as all models exhibited violations of fundamental survival constraints. Preprocessing and postprocessing played a crucial role in reducing these violations, as models lacking robust processing steps produced invalid data in up to 61% of cases. These findings underscore the necessity of integrating explicit domain knowledge alongside HPO to create high quality synthetic datasets. Our study provides actionable recommendations for improving synthetic data generation, with future research needed to refine metric selection and validate these findings on larger datasets to enhance clinical applicability.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Models for Long Time Series: Approximately Equivariant Recurrent Network Structures for an Adjusted Training Scheme</title>
<link>https://arxiv.org/abs/2505.05020</link>
<guid>https://arxiv.org/abs/2505.05020</guid>
<content:encoded><![CDATA[
<div> Keywords: Variational Autoencoder, time series, generative model, recurrent layers, temporal dependencies  

<br /><br />Summary: This paper introduces a novel generative model for time series data, called the Recurrent Variational Autoencoder with Subsequent Training (RVAE-ST), built on the framework of a Variational Autoencoder (VAE) that utilizes recurrent layers. The RVAE-ST model features an innovative adaptive training scheme that incrementally increases sequence lengths, overcoming the limitations often encountered in recurrent layers when dealing with long sequences. A key advantage of this architecture is its ability to maintain a consistent number of parameters irrespective of the sequence length, which promotes approximate time-shift equivariance and enhances the modeling of long-range temporal dependencies. Importantly, the authors emphasize that their approach does not rely on a new architecture but rather on a strategic combination of existing components, which can either match or exceed the performance of state-of-the-art generative models across various benchmark datasets. The model demonstrates exceptional results on quasi-periodic time series while remaining competitive with datasets characterized by irregular or partially non-stationary patterns. The evaluation metrics applied include ELBO, Fréchet Distance, discriminative scores, and visualizations of learned embeddings. <div>
arXiv:2505.05020v1 Announce Type: new 
Abstract: We present a simple yet effective generative model for time series data based on a Variational Autoencoder (VAE) with recurrent layers, referred to as the Recurrent Variational Autoencoder with Subsequent Training (RVAE-ST). Our method introduces an adapted training scheme that progressively increases the sequence length, addressing the challenge recurrent layers typically face when modeling long sequences. By leveraging the recurrent architecture, the model maintains a constant number of parameters regardless of sequence length. This design encourages approximate time-shift equivariance and enables efficient modeling of long-range temporal dependencies. Rather than introducing a fundamentally new architecture, we show that a carefully composed combination of known components can match or outperform state-of-the-art generative models on several benchmark datasets. Our model performs particularly well on time series that exhibit quasi-periodic structure,while remaining competitive on datasets with more irregular or partially non-stationary behavior. We evaluate its performance using ELBO, Fr\'echet Distance, discriminative scores, and visualizations of the learned embeddings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dequantified Diffusion Schr\"odinger Bridge for Density Ratio Estimation</title>
<link>https://arxiv.org/abs/2505.05034</link>
<guid>https://arxiv.org/abs/2505.05034</guid>
<content:encoded><![CDATA[
<div> Keywords: density ratio estimation, f-divergences, support coverage, optimal transport, empirical performance

<br /><br />Summary: Density ratio estimation is crucial for tasks involving $f$-divergences; however, traditional methods often struggle when distributions differ significantly or lack adequate support overlap, leading to the density-chasm and support-chasm issues. Additionally, previous techniques may experience unstable, divergent time scores near the boundaries. In response to these challenges, we introduce $\text{D}^3\text{RE}$, a comprehensive framework designed for robust and efficient density ratio estimation. Our approach features the Dequantified Diffusion-Bridge Interpolant (DDBI), which aims to enhance support coverage and stabilize time scores by leveraging diffusion bridges and Gaussian dequantization techniques. Building on this foundation, we further develop the Dequantified Schr\"odinger-Bridge Interpolant (DSBI), which utilizes optimal transport methods to address the Schr\"odinger bridge problem, thus improving accuracy and efficiency. The theoretical framework of our method guarantees uniform approximation and bounded time scores. Empirically, $\text{D}^3\text{RE}$ demonstrates superior performance compared to baseline methods in tasks involving mutual information and density estimation, providing significant advancements over existing approaches. <div>
arXiv:2505.05034v1 Announce Type: new 
Abstract: Density ratio estimation is fundamental to tasks involving $f$-divergences, yet existing methods often fail under significantly different distributions or inadequately overlap supports, suffering from the \textit{density-chasm} and the \textit{support-chasm} problems. Additionally, prior approaches yield divergent time scores near boundaries, leading to instability. We propose $\text{D}^3\text{RE}$, a unified framework for robust and efficient density ratio estimation. It introduces the Dequantified Diffusion-Bridge Interpolant (DDBI), which expands support coverage and stabilizes time scores via diffusion bridges and Gaussian dequantization. Building on DDBI, the Dequantified Schr\"odinger-Bridge Interpolant (DSBI) incorporates optimal transport to solve the Schr\"odinger bridge problem, enhancing accuracy and efficiency. Our method offers uniform approximation and bounded time scores in theory, and outperforms baselines empirically in mutual information and density estimation tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Pathways to Program Success: Hopfield Networks for PERT Analysis</title>
<link>https://arxiv.org/abs/2505.05047</link>
<guid>https://arxiv.org/abs/2505.05047</guid>
<content:encoded><![CDATA[
<div> Keywords: project scheduling, uncertainty, PERT, Hopfield neural network, optimization

<br /><br />Summary: The paper addresses the complexities of project and task scheduling under uncertainty, emphasizing the importance of accurate task duration and dependency estimations in program management. It introduces a novel approach to Program Evaluation and Review Technique (PERT) scheduling by formulating the problem as an energy minimization task within a Hopfield neural network framework. This innovative mapping of task start times and precedence constraints allows the network's optimization dynamics to derive globally consistent schedules. The author delves into theoretical aspects, including the differentiability of energy functions, encoding of constraints, and convergence of solutions, while also extending the Hopfield model to accommodate structured precedence graphs. Numerical experiments on synthetic networks with up to 1000 tasks illustrate the effectiveness of this method, with results showcasing near-optimal makespans and minimal constraint violations. The findings indicate that leveraging neural optimization models represents a promising avenue for scalable and adaptable scheduling of project tasks amid uncertainty, which is essential for modern applications, such as AI workflows and microservice architectures that underpin contemporary AI systems. <div>
arXiv:2505.05047v1 Announce Type: new 
Abstract: Project and task scheduling under uncertainty remains a fundamental challenge in program and project management, where accurate estimation of task durations and dependencies is critical for delivering complex, multi project systems. The Program Evaluation and Review Technique provides a probabilistic framework to model task variability and critical paths. In this paper, the author presents a novel formulation of PERT scheduling as an energy minimization problem within a Hopfield neural network architecture. By mapping task start times and precedence constraints into a neural computation framework, the networks inherent optimization dynamics is exploited to approximate globally consistent schedules. The author addresses key theoretical issues related to energy function differentiability, constraint encoding, and convergence, and extends the Hopfield model for structured precedence graphs. Numerical simulations on synthetic project networks comprising up to 1000 tasks demonstrate the viability of this approach, achieving near optimal makespans with minimal constraint violations. The findings suggest that neural optimization models offer a promising direction for scalable and adaptive project tasks scheduling under uncertainty in areas such as the agentic AI workflows, microservice based applications that the modern AI systems are being built upon.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts</title>
<link>https://arxiv.org/abs/2505.05063</link>
<guid>https://arxiv.org/abs/2505.05063</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, CodeMixBench, Multilingual, Code Generation, Code-Mixed Prompts  

<br /><br />Summary: Large Language Models (LLMs) have excelled in code generation tasks, supporting applications like code completion and debugging. However, current benchmarks like HumanEval and MBPP evaluate LLMs strictly on English prompts, neglecting the practical use of code-mixed languages by multilingual developers. To bridge this gap, the authors introduce CodeMixBench, a new benchmark assessing LLM performance with code-mixed prompts. It is based on BigCodeBench and incorporates controlled code-mixing (CMD) for three language combinations: Hinglish (Hindi-English), Spanish-English, and Chinese Pinyin-English. A thorough evaluation was conducted on various open-source code generation models, ranging from 1.5B to 15B parameters. The findings indicate that code-mixed prompts lead to a notable decrease in Pass@1 performance compared to English-only prompts, especially in smaller models where performance drops intensify under higher levels of CMD. CodeMixBench establishes a realistic evaluation system for multilingual code generation, revealing significant challenges and pointing toward future research directions for developing resilient code generation models that can effectively operate in diverse linguistic contexts. <div>
arXiv:2505.05063v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success in code generation tasks, powering various applications like code completion, debugging, and programming assistance. However, existing benchmarks such as HumanEval, MBPP, and BigCodeBench primarily evaluate LLMs on English-only prompts, overlooking the real-world scenario where multilingual developers often use code-mixed language while interacting with LLMs. To address this gap, we introduce CodeMixBench, a novel benchmark designed to evaluate the robustness of LLMs on code generation from code-mixed prompts. Built upon BigCodeBench, CodeMixBench introduces controlled code-mixing (CMD) into the natural language parts of prompts across three language pairs: Hinglish (Hindi-English), Spanish-English, and Chinese Pinyin-English. We comprehensively evaluate a diverse set of open-source code generation models ranging from 1.5B to 15B parameters. Our results show that code-mixed prompts consistently degrade Pass@1 performance compared to their English-only counterparts, with performance drops increasing under higher CMD levels for smaller models. CodeMixBench provides a realistic evaluation framework for studying multilingual code generation and highlights new challenges and directions for building robust code generation models that generalize well across diverse linguistic settings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaterDrum: Watermarking for Data-centric Unlearning Metric</title>
<link>https://arxiv.org/abs/2505.05064</link>
<guid>https://arxiv.org/abs/2505.05064</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, unlearning, watermarking, datasets, evaluation  

<br /><br />Summary: Large language model (LLM) unlearning is essential for real-world applications that require the removal of private, copyrighted, or harmful data from users. However, traditional utility-focused unlearning metrics may not accurately assess the effectiveness of unlearning, particularly in situations where the forget and retain sets contain semantically similar content, where retraining the model from scratch on the retained data is not feasible, or where the model owner can artificially enhance the unlearning metric without actual unlearning. This paper introduces WaterDrum, the first data-centric unlearning metric for LLMs, which utilizes robust text watermarking techniques to address these challenges. Additionally, the authors present new benchmark datasets specifically designed for LLM unlearning; these datasets feature varying levels of similar data points which facilitate rigorous evaluation of unlearning algorithms using the WaterDrum metric. The code for implementing this metric is made available at https://github.com/lululu008/WaterDrum, and the newly released benchmark datasets can be found at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax. <div>
arXiv:2505.05064v1 Announce Type: new 
Abstract: Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. However, existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when (a) the forget and retain set have semantically similar content, (b) retraining the model from scratch on the retain set is impractical, and/or (c) the model owner can improve the unlearning metric without directly performing unlearning on the LLM. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking for overcoming these limitations. We also introduce new benchmark datasets for LLM unlearning that contain varying levels of similar data points and can be used to rigorously evaluate unlearning algorithms using WaterDrum. Our code is available at https://github.com/lululu008/WaterDrum and our new benchmark datasets are released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model</title>
<link>https://arxiv.org/abs/2505.05082</link>
<guid>https://arxiv.org/abs/2505.05082</guid>
<content:encoded><![CDATA[
<div> Keywords: generative modeling, discrete data, Poisson diffusion, music dataset, negative log-likelihood  

<br /><br />Summary: Existing generative modeling methods for discrete data, like symbolic music tokens, face challenges by either embedding inputs in continuous spaces or employing variational losses that approximate negative log-likelihood. Recent attempts have addressed these issues separately. Information-theoretic Gaussian diffusion models reduce variational loss suboptimality but still operate within continuous domains. This work presents the Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which overcomes both limitations by directly functioning in a discrete state-space using a Poisson diffusion process inspired by photon arrival in camera sensors. A new Poisson Reconstruction Loss (PRL) is proposed, establishing an exact relationship with true negative log-likelihood, thus eliminating the need for approximate evidence lower bounds. Experiments on the Lakh MIDI symbolic music dataset and CIFAR-10 image benchmark show that ItDPDM significantly enhances performance, reducing test negative log-likelihood by up to 80% compared to previous methods and achieving faster convergence rates. The findings suggest that ItDPDM is a robust alternative for generative modeling of discrete data, with substantial improvements in efficiency and performance metrics. <div>
arXiv:2505.05082v1 Announce Type: new 
Abstract: Existing methods for generative modeling of discrete data, such as symbolic music tokens, face two primary challenges: (1) they either embed discrete inputs into continuous state-spaces or (2) rely on variational losses that only approximate the true negative log-likelihood. Previous efforts have individually targeted these limitations. While information-theoretic Gaussian diffusion models alleviate the suboptimality of variational losses, they still perform modeling in continuous domains. In this work, we introduce the Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which simultaneously addresses both limitations by directly operating in a discrete state-space via a Poisson diffusion process inspired by photon arrival processes in camera sensors. We introduce a novel Poisson Reconstruction Loss (PRL) and derive an exact relationship between PRL and the true negative log-likelihood, thereby eliminating the need for approximate evidence lower bounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the CIFAR-10 image benchmark demonstrate that ItDPDM delivers significant improvements, reducing test NLL by up to 80% compared to prior baselines, while also achieving faster convergence.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning</title>
<link>https://arxiv.org/abs/2505.05086</link>
<guid>https://arxiv.org/abs/2505.05086</guid>
<content:encoded><![CDATA[
<div> Keywords: on-device learning, activation memory, low-rank decomposition, training efficiency, privacy risks  

<br /><br />Summary: 
On-device learning is gaining traction in AI development due to its ability to alleviate latency issues and enhance privacy by minimizing device-server communication. It also aims to improve energy efficiency, but challenges remain due to significant memory and computational constraints that hinder its deployment. To address these issues, the authors leverage insights from previous research on low-rank decomposition methods, which aim to mitigate activation memory bottlenecks encountered during backpropagation. They introduce a novel shortcut approach as an alternative strategy. Through theoretical analysis and experimental evaluations, the proposed method demonstrates substantial improvements, achieving a remarkable reduction in activation memory usage of up to 120.09 times compared to conventional training methods. Additionally, the approach effectively decreases overall training floating-point operations per second (FLOPs) by as much as 1.86 times when assessed on standard benchmarks. These findings suggest that the method could significantly enhance the practicality of on-device learning, making it a viable option for AI applications that require efficient resource utilization while maintaining performance. <div>
arXiv:2505.05086v1 Announce Type: new 
Abstract: On-device learning has emerged as a promising direction for AI development, particularly because of its potential to reduce latency issues and mitigate privacy risks associated with device-server communication, while improving energy efficiency. Despite these advantages, significant memory and computational constraints still represent major challenges for its deployment. Drawing on previous studies on low-rank decomposition methods that address activation memory bottlenecks in backpropagation, we propose a novel shortcut approach as an alternative. Our analysis and experiments demonstrate that our method can reduce activation memory usage, even up to $120.09\times$ compared to vanilla training, while also reducing overall training FLOPs up to $1.86\times$ when evaluated on traditional benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Conjoint Graph Representation Learning Framework for Hypertension Comorbidity Risk Prediction</title>
<link>https://arxiv.org/abs/2505.05094</link>
<guid>https://arxiv.org/abs/2505.05094</guid>
<content:encoded><![CDATA[
<div> Keywords: hypertension, comorbidities, graph learning, diabetes, coronary heart disease  

<br /><br />Summary: This study focuses on the significant burden that hypertension and its comorbidities place on patients and society, emphasizing the need for early identification to enable timely intervention. To tackle this challenge, the authors propose a Conjoint Graph Representation Learning (CGRL) framework, which integrates joint graph learning and network analysis. The framework constructs two distinct networks based on disease coding: a patient network and a disease difference network. Through this construction, three comorbidity network features are developed to analyze the relationships between comorbidities and risk diseases. Additionally, the CGRL framework employs computational structure intervention and learning feature representation to predict the risks of diabetes and coronary heart disease among patients. This enables a deeper understanding of comorbidity patterns and offers insights into the pathways of disease progression, potentially illuminating the pathological mechanisms behind diabetes and coronary heart disease. Results indicate that the network features derived from the difference network are crucial for prediction accuracy. Ultimately, the proposed framework outperforms other robust models, providing more accurate predictions regarding patient risk for these diseases. <div>
arXiv:2505.05094v1 Announce Type: new 
Abstract: The comorbidities of hypertension impose a heavy burden on patients and society. Early identification is necessary to prompt intervention, but it remains a challenging task. This study aims to address this challenge by combining joint graph learning with network analysis. Motivated by this discovery, we develop a Conjoint Graph Representation Learning (CGRL) framework that: a) constructs two networks based on disease coding, including the patient network and the disease difference network. Three comorbidity network features were generated based on the basic difference network to capture the potential relationship between comorbidities and risk diseases; b) incorporates computational structure intervention and learning feature representation, CGRL was developed to predict the risks of diabetes and coronary heart disease in patients; and c) analysis the comorbidity patterns and exploring the pathways of disease progression, the pathological pathogenesis of diabetes and coronary heart disease may be revealed. The results show that the network features extracted based on the difference network are important, and the framework we proposed provides more accurate predictions than other strong models in terms of accuracy.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Client Participation in Federated Learning Using AoI</title>
<link>https://arxiv.org/abs/2505.05099</link>
<guid>https://arxiv.org/abs/2505.05099</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Age of Information, client selection, convergence stability, decentralized scheduling  

<br /><br />Summary: 
This paper addresses the challenges faced by Federated Learning (FL), such as limited communication resources, statistical heterogeneity, and the necessity for balanced client participation. The authors propose a client selection policy based on the Age of Information (AoI) to mitigate these issues by minimizing load imbalance through controlled selection intervals. The strategy involves a decentralized Markov scheduling policy, enabling clients to independently manage their participation using age-dependent selection probabilities. This formulation aims to balance client updates across multiple training rounds while maintaining minimal central oversight. The paper presents a convergence proof for the proposed method, reinforcing the idea of stable and efficient model convergence. Additionally, it derives optimal parameters for the Markov selection model to ensure balanced and consistent client participation. Through extensive simulations, the results indicate that the AoI-based method, especially its optimal Markov variant, enhances convergence performance compared to the FedAvg selection approach, demonstrating improvements of 7.5% in IID settings and up to 20% in non-IID conditions. The findings highlight the potential of AoI-based scheduling schemes to foster scalable, fair, and efficient FL systems across various learning environments. <div>
arXiv:2505.05099v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a decentralized framework that preserves data privacy while enabling collaborative model training across distributed clients. However, FL faces significant challenges due to limited communication resources, statistical heterogeneity, and the need for balanced client participation. This paper proposes an Age of Information (AoI)-based client selection policy that addresses these challenges by minimizing load imbalance through controlled selection intervals. Our method employs a decentralized Markov scheduling policy, allowing clients to independently manage participation based on age-dependent selection probabilities, which balances client updates across training rounds with minimal central oversight. We provide a convergence proof for our method, demonstrating that it ensures stable and efficient model convergence. Specifically, we derive optimal parameters for the Markov selection model to achieve balanced and consistent client participation, highlighting the benefits of AoI in enhancing convergence stability. Through extensive simulations, we demonstrate that our AoI-based method, particularly the optimal Markov variant, improves convergence over the FedAvg selection approach across both IID and non-IID data settings by $7.5\%$ and up to $20\%$. Our findings underscore the effectiveness of AoI-based scheduling for scalable, fair, and efficient FL systems across diverse learning environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USPR: Learning a Unified Solver for Profiled Routing</title>
<link>https://arxiv.org/abs/2505.05119</link>
<guid>https://arxiv.org/abs/2505.05119</guid>
<content:encoded><![CDATA[
<div> Keywords: Profiled Vehicle Routing Problem, USPR, Profile Embeddings, Multi-Head Profiled Attention, Profile-aware Score Reshaping

<br /><br />Summary: The Profiled Vehicle Routing Problem (PVRP) expands upon the classical Vehicle Routing Problem (VRP) by integrating vehicle-client-specific preferences and constraints that mirror real-world scenarios. Despite advancements in reinforcement learning (RL) solvers, challenges remain as they necessitate retraining for each distinct profile distribution, exhibit inadequate representation capabilities, and face difficulties with out-of-distribution generalization. To tackle these issues, this paper presents USPR (Unified Solver for Profiled Routing), a cutting-edge framework designed to accommodate diverse profile types without the need for retraining. USPR incorporates three groundbreaking innovations: firstly, Profile Embeddings (PE), which encode combinations of a variety of profile types; secondly, Multi-Head Profiled Attention (MHPA), an attention mechanism that captures complex interactions between vehicles and clients; and thirdly, Profile-aware Score Reshaping (PSR), which dynamically modifies decoder logits incorporating profile scores to enhance generalization. The empirical results from various PVRP benchmarks indicate that USPR not only outperforms existing learning-based methods but also provides significant improvements in flexibility and computational efficiency. The authors have also made their source code publicly available to encourage future research in this area. <div>
arXiv:2505.05119v1 Announce Type: new 
Abstract: The Profiled Vehicle Routing Problem (PVRP) extends the classical VRP by incorporating vehicle-client-specific preferences and constraints, reflecting real-world requirements such as zone restrictions and service-level preferences. While recent reinforcement learning (RL) solvers have shown promise, they require retraining for each new profile distribution, suffer from poor representation ability, and struggle to generalize to out-of-distribution instances. In this paper, we address these limitations by introducing USPR (Unified Solver for Profiled Routing), a novel framework that natively handles arbitrary profile types. USPR introduces three key innovations: (i) Profile Embeddings (PE) to encode any combination of profile types; (ii) Multi-Head Profiled Attention (MHPA), an attention mechanism that models rich interactions between vehicles and clients; (iii) Profile-aware Score Reshaping (PSR), which dynamically adjusts decoder logits using profile scores to improve generalization. Empirical results on diverse PVRP benchmarks demonstrate that USPR achieves state-of-the-art results among learning-based methods while offering significant gains in flexibility and computational efficiency. We make our source code publicly available to foster future research at https://github.com/ai4co/uspr.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming OOD Actions for Offline Reinforcement Learning: An Advantage-Based Approach</title>
<link>https://arxiv.org/abs/2505.05126</link>
<guid>https://arxiv.org/abs/2505.05126</guid>
<content:encoded><![CDATA[
<div> Keywords: Offline reinforcement learning, distribution shift, OOD actions, Advantage-based Diffusion Actor-Critic, D4RL benchmark

<br /><br />Summary: Offline reinforcement learning (RL) enables the learning of decision-making policies from fixed datasets without requiring online interactions, which is useful in scenarios where data collection can be costly or risky. A common challenge in offline RL is the distribution shift, leading to inaccurate evaluations and overestimations of out-of-distribution (OOD) actions. Existing methods often employ a conservative approach that discourages all OOD actions, limiting the agent's capacity to generalize effectively and exploit potentially advantageous actions. This paper introduces Advantage-based Diffusion Actor-Critic (ADAC), a novel method that assesses OOD actions using batch-optimal value functions. By employing this evaluation, ADAC creates an advantage function that refines the Q-function updates, allowing for a more nuanced assessment of the quality of OOD actions. The authors develop a custom PointMaze environment and collect datasets to demonstrate that advantage modulation can successfully identify and select superior OOD actions. Results from extensive experiments indicate that ADAC achieves state-of-the-art performance on nearly all tasks within the D4RL benchmark, with particularly significant improvements in more challenging scenarios. <div>
arXiv:2505.05126v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) aims to learn decision-making policies from fixed datasets without online interactions, providing a practical solution where online data collection is expensive or risky. However, offline RL often suffers from distribution shift, resulting in inaccurate evaluation and substantial overestimation on out-of-distribution (OOD) actions. To address this, existing approaches incorporate conservatism by indiscriminately discouraging all OOD actions, thereby hindering the agent's ability to generalize and exploit beneficial ones. In this paper, we propose Advantage-based Diffusion Actor-Critic (ADAC), a novel method that systematically evaluates OOD actions using the batch-optimal value function. Based on this evaluation, ADAC defines an advantage function to modulate the Q-function update, enabling more precise assessment of OOD action quality. We design a custom PointMaze environment and collect datasets to visually reveal that advantage modulation can effectively identify and select superior OOD actions. Extensive experiments show that ADAC achieves state-of-the-art performance on almost all tasks in the D4RL benchmark, with particularly clear margins on the more challenging tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Anomaly Detection Methods Based on Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05137</link>
<guid>https://arxiv.org/abs/2505.05137</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, diffusion models, reconstruction errors, multi-scale feature extraction, benchmark datasets

<br /><br />Summary: Anomaly detection is essential in various fields, but traditional techniques struggle with complex, high-dimensional data. This study introduces a novel framework using diffusion probabilistic models (DPMs) for effectively identifying anomalies in image and audio data. The method models normal data distribution through a diffusion process and reconstructs data via reverse diffusion, utilizing reconstruction errors and semantic discrepancies as indicators of anomalies. To improve performance, the framework incorporates multi-scale feature extraction, attention mechanisms, and wavelet-domain representations, which help capture both fine-grained structures and global dependencies. The proposed approach is evaluated on benchmark datasets, including MVTec AD for images and UrbanSound8K for audio, showcasing superior accuracy and robustness compared to existing state-of-the-art anomaly detection methods. The experiments highlight the strengths of diffusion models in anomaly detection tasks, demonstrating their capability as a robust and efficient solution for real-world applications across diverse data modalities. This research emphasizes the potential of diffusion models as novel tools in addressing the challenges of anomaly detection in complex datasets. <div>
arXiv:2505.05137v1 Announce Type: new 
Abstract: Anomaly detection is a fundamental task in machine learning and data mining, with significant applications in cybersecurity, industrial fault diagnosis, and clinical disease monitoring. Traditional methods, such as statistical modeling and machine learning-based approaches, often face challenges in handling complex, high-dimensional data distributions. In this study, we explore the potential of diffusion models for anomaly detection, proposing a novel framework that leverages the strengths of diffusion probabilistic models (DPMs) to effectively identify anomalies in both image and audio data. The proposed method models the distribution of normal data through a diffusion process and reconstructs input data via reverse diffusion, using a combination of reconstruction errors and semantic discrepancies as anomaly indicators. To enhance the framework's performance, we introduce multi-scale feature extraction, attention mechanisms, and wavelet-domain representations, enabling the model to capture fine-grained structures and global dependencies in the data. Extensive experiments on benchmark datasets, including MVTec AD and UrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly detection techniques, achieving superior accuracy and robustness across diverse data modalities. This research highlights the effectiveness of diffusion models in anomaly detection and provides a robust and efficient solution for real-world applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry</title>
<link>https://arxiv.org/abs/2505.05143</link>
<guid>https://arxiv.org/abs/2505.05143</guid>
<content:encoded><![CDATA[
<div> Keywords: Lottery Ticket Hypothesis, sparsity mask, generalization performance, optimization basin, aligned training

<br /><br />Summary: The Lottery Ticket Hypothesis (LTH) posits that a sparse mask and weight configuration exist that can match the generalization performance of a dense model while utilizing significantly fewer parameters. However, the process of discovering a suitable LTH solution is computationally intensive, and the masks often do not transfer well to new random weight initializations. Recent studies have indicated that neural networks, when trained from random initialization, often converge to solutions within the same loss basin modulo permutation. To address the issue of LTH masks not generalizing effectively, this paper hypothesizes that misalignment of these basins is a contributing factor. The authors propose permuting the LTH mask to align it with the new optimization basin during sparse training from a different random initialization. Empirical results showcase a marked improvement in generalization performance when using the permuted mask in sparse training compared to employing the original non-permuted LTH mask. These findings are substantiated across various datasets, including CIFAR-10, CIFAR-100, and ImageNet, as well as different models like VGG11, ResNet20, and ResNet50. <div>
arXiv:2505.05143v1 Announce Type: new 
Abstract: The Lottery Ticket Hypothesis (LTH) suggests there exists a sparse LTH mask and weights that achieve the same generalization performance as the dense model while using significantly fewer parameters. However, finding a LTH solution is computationally expensive, and a LTH sparsity mask does not generalize to other random weight initializations. Recent work has suggested that neural networks trained from random initialization find solutions within the same basin modulo permutation, and proposes a method to align trained models within the same loss basin. We hypothesize that misalignment of basins is the reason why LTH masks do not generalize to new random initializations and propose permuting the LTH mask to align with the new optimization basin when performing sparse training from a different random init. We empirically show a significant increase in generalization when sparse training from random initialization with the permuted mask as compared to using the non-permuted LTH mask, on multiple datasets (CIFAR-10, CIFAR-100 and ImageNet) and models (VGG11, ResNet20 and ResNet50).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding In-context Learning of Addition via Activation Subspaces</title>
<link>https://arxiv.org/abs/2505.05145</link>
<guid>https://arxiv.org/abs/2505.05145</guid>
<content:encoded><![CDATA[
<div> Keywords: few-shot learning, transformer models, attention heads, self-correction, low-dimensional subspaces

<br /><br />Summary: The paper investigates how language models, specifically Llama-3-8B, implement in-context learning during the forward pass by extracting signals from few-shot examples. By focusing on a structured family of tasks where the prediction rule is to add an integer \(k\) to the input, the authors discover that the model achieves high accuracy across various \(k\) values. They utilize a novel optimization technique to pinpoint that the few-shot learning capability is localized to just three attention heads. Further analysis reveals that the extracted signals reside within a six-dimensional subspace: four dimensions pertaining to the unit digit and two dimensions related to overall magnitude. Additionally, the research uncovers a self-correction mechanism whereby errors from earlier examples are mitigated by subsequent examples. These findings illustrate how examining low-dimensional subspaces throughout a forward pass can enhance the understanding of intricate computational structures within transformer models, shedding light on the mechanisms behind few-shot learning effectiveness. <div>
arXiv:2505.05145v1 Announce Type: new 
Abstract: To perform in-context learning, language models must extract signals from individual few-shot examples, aggregate these into a learned prediction rule, and then apply this rule to new examples. How is this implemented in the forward pass of modern transformer models? To study this, we consider a structured family of few-shot learning tasks for which the true prediction rule is to add an integer $k$ to the input. We find that Llama-3-8B attains high accuracy on this task for a range of $k$, and localize its few-shot ability to just three attention heads via a novel optimization approach. We further show the extracted signals lie in a six-dimensional subspace, where four of the dimensions track the unit digit and the other two dimensions track overall magnitude. We finally examine how these heads extract information from individual few-shot examples, identifying a self-correction mechanism in which mistakes from earlier examples are suppressed by later examples. Our results demonstrate how tracking low-dimensional subspaces across a forward pass can provide insight into fine-grained computational structures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedTDP: A Privacy-Preserving and Unified Framework for Trajectory Data Preparation via Federated Learning</title>
<link>https://arxiv.org/abs/2505.05155</link>
<guid>https://arxiv.org/abs/2505.05155</guid>
<content:encoded><![CDATA[
<div> Keywords: trajectory data, privacy, federated learning, Large Language Models, data quality

<br /><br />Summary: Trajectory data, essential for traffic optimization and urban planning, often faces quality issues due to noise and incompleteness, which hinders accurate analyses. Current Trajectory Data Preparation (TDP) methods fall short in two areas: they overlook privacy concerns in federated settings where data sharing is restricted, and they tend to be task-specific, lacking generalizability across various TDP scenarios. To address these challenges, the authors introduce FedTDP, a privacy-preserving and unified framework that employs Large Language Models (LLMs) for TDP in federated environments. The framework includes (i) a trajectory privacy autoencoder that secures data transmission and protects users' privacy, (ii) a trajectory knowledge enhancer that improves the model's understanding of TDP-related knowledge, allowing for the development of TDP-oriented LLMs, and (iii) a federated parallel optimization approach that boosts training efficiency by reducing the need for extensive data transmission and facilitating parallel model training. Comprehensive experiments conducted on six real datasets and ten mainstream TDP tasks indicate that FedTDP consistently outperforms 13 state-of-the-art baselines, showcasing its effectiveness and innovation in the field. <div>
arXiv:2505.05155v1 Announce Type: new 
Abstract: Trajectory data, which capture the movement patterns of people and vehicles over time and space, are crucial for applications like traffic optimization and urban planning. However, issues such as noise and incompleteness often compromise data quality, leading to inaccurate trajectory analyses and limiting the potential of these applications. While Trajectory Data Preparation (TDP) can enhance data quality, existing methods suffer from two key limitations: (i) they do not address data privacy concerns, particularly in federated settings where trajectory data sharing is prohibited, and (ii) they typically design task-specific models that lack generalizability across diverse TDP scenarios. To overcome these challenges, we propose FedTDP, a privacy-preserving and unified framework that leverages the capabilities of Large Language Models (LLMs) for TDP in federated environments. Specifically, we: (i) design a trajectory privacy autoencoder to secure data transmission and protect privacy, (ii) introduce a trajectory knowledge enhancer to improve model learning of TDP-related knowledge, enabling the development of TDP-oriented LLMs, and (iii) propose federated parallel optimization to enhance training efficiency by reducing data transmission and enabling parallel model training. Experiments on 6 real datasets and 10 mainstream TDP tasks demonstrate that FedTDP consistently outperforms 13 state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bandit Max-Min Fair Allocation</title>
<link>https://arxiv.org/abs/2505.05169</link>
<guid>https://arxiv.org/abs/2505.05169</guid>
<content:encoded><![CDATA[
<div> Keywords: bandit max-min fair allocation, semi-bandit feedback, regret bound, additive valuations, competitive analysis  

<br /><br />Summary: This paper introduces the bandit max-min fair allocation (BMMFA) problem, which aims to maximize the minimum utility among agents with additive valuations through the repeated assignment of indivisible goods. A distinctive feature of this problem is that agents' valuations are observed via semi-bandit feedback, contrasting with prior work that assumed complete knowledge of item values at the start of each round. Additionally, the algorithm's reward function differs, as it is not additive across rounds. The authors present an algorithm achieving an asymptotic regret bound of $O(m\sqrt{T}\ln T/n + m\sqrt{T \ln(mnT)})$, with $n$ representing the number of agents, $m$ the number of items, and $T$ the time horizon. This result stems from an innovative integration of bandit techniques and resource allocation methods found in competitive analysis literature. Furthermore, they establish a regret lower bound of $\Omega(m\sqrt{T}/n)$, demonstrating that when $T$ significantly exceeds $n$, the difference between the upper and lower bounds is logarithmic in $T$. <div>
arXiv:2505.05169v1 Announce Type: new 
Abstract: In this paper, we study a new decision-making problem called the bandit max-min fair allocation (BMMFA) problem. The goal of this problem is to maximize the minimum utility among agents with additive valuations by repeatedly assigning indivisible goods to them. One key feature of this problem is that each agent's valuation for each item can only be observed through the semi-bandit feedback, while existing work supposes that the item values are provided at the beginning of each round. Another key feature is that the algorithm's reward function is not additive with respect to rounds, unlike most bandit-setting problems.
  Our first contribution is to propose an algorithm that has an asymptotic regret bound of $O(m\sqrt{T}\ln T/n + m\sqrt{T \ln(mnT)})$, where $n$ is the number of agents, $m$ is the number of items, and $T$ is the time horizon. This is based on a novel combination of bandit techniques and a resource allocation algorithm studied in the literature on competitive analysis. Our second contribution is to provide the regret lower bound of $\Omega(m\sqrt{T}/n)$. When $T$ is sufficiently larger than $n$, the gap between the upper and lower bounds is a logarithmic factor of $T$.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning</title>
<link>https://arxiv.org/abs/2505.05180</link>
<guid>https://arxiv.org/abs/2505.05180</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt tuning, open-world tasks, detection, classification, OpenworldAUC

<br /><br />Summary: This paper introduces a method to adapt Vision-Language Models, like CLIP, to open-world tasks using prompt tuning with minimal training costs. It discusses the conventional approach that evaluates model performance on known and unseen classes separately. However, real-world scenarios necessitate models to process inputs without having prior domain knowledge. The authors identify a challenge that requires unified evaluation in two stages: 1) detecting if an input belongs to the base or new domain, and 2) classifying the input correctly. They note that existing metrics, such as HM, overall accuracy, and AUROC, do not adequately fulfill the requirements of handling unknown domain distributions and varying sample ratios. To address this, the authors propose OpenworldAUC, a new metric for simultaneous detection and classification via pairwise instance comparisons. Additionally, they introduce Gated Mixture-of-Prompts (GMoP), which utilizes domain-specific prompts and a gating mechanism to effectively balance the detection and classification tasks. Theoretical guarantees are provided for GMoP's generalization under realistic conditions. Experimental results from 15 benchmarks in open-world settings demonstrate that GMoP achieves state-of-the-art performance on OpenworldAUC and other metrics. The code is available at https://github.com/huacong/OpenworldAUC. <div>
arXiv:2505.05180v1 Announce Type: new 
Abstract: Prompt tuning adapts Vision-Language Models like CLIP to open-world tasks with minimal training costs. In this direction, one typical paradigm evaluates model performance separately on known classes (i.e., base domain) and unseen classes (i.e., new domain). However, real-world scenarios require models to handle inputs without prior domain knowledge. This practical challenge has spurred the development of open-world prompt tuning, which demands a unified evaluation of two stages: 1) detecting whether an input belongs to the base or new domain (P1), and 2) classifying the sample into its correct class (P2). What's more, as domain distributions are generally unknown, a proper metric should be insensitive to varying base/new sample ratios (P3). However, we find that current metrics, including HM, overall accuracy, and AUROC, fail to satisfy these three properties simultaneously. To bridge this gap, we propose OpenworldAUC, a unified metric that jointly assesses detection and classification through pairwise instance comparisons. To optimize OpenworldAUC effectively, we introduce Gated Mixture-of-Prompts (GMoP), which employs domain-specific prompts and a gating mechanism to dynamically balance detection and classification. Theoretical guarantees ensure generalization of GMoP under practical conditions. Experiments on 15 benchmarks in open-world scenarios show GMoP achieves SOTA performance on OpenworldAUC and other metrics. We release the code at https://github.com/huacong/OpenworldAUC
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation</title>
<link>https://arxiv.org/abs/2505.05181</link>
<guid>https://arxiv.org/abs/2505.05181</guid>
<content:encoded><![CDATA[
<div> Keywords: Backpropagation, Stochastic Variational Propagation, Evidence Lower Bounds, memory reduction, neural network design.

<br /><br />Summary:  
Backpropagation (BP) is essential for deep learning but faces scalability issues and high memory usage due to global gradient synchronization. The authors propose a new method called Stochastic Variational Propagation (SVP), which reframes neural network training as hierarchical variational inference. In SVP, layer activations are treated as latent variables, optimizing local Evidence Lower Bounds (ELBOs) for independent updates while maintaining global coherence. A challenge with this approach is the potential collapse of inter-layer representations because of excessive compression from KL divergence. To address this, SVP uses fixed random matrices to project activations into low-dimensional spaces, which helps retain information and foster representational diversity. The introduction of a feature alignment loss further ensures consistency across layers. SVP demonstrates competitive performance compared to BP across various architectures, including Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Transformers, as well as different datasets from MNIST to ImageNet. Additionally, it achieves up to 4x reduction in memory usage, significantly enhancing scalability. Overall, SVP adds a probabilistic framework to deep representation learning, paving the way for more modular and interpretable designs in neural networks. <div>
arXiv:2505.05181v1 Announce Type: new 
Abstract: Backpropagation (BP) is the cornerstone of deep learning, but its reliance on global gradient synchronization limits scalability and imposes significant memory overhead. We propose Stochastic Variational Propagation (SVP), a scalable alternative that reframes training as hierarchical variational inference. SVP treats layer activations as latent variables and optimizes local Evidence Lower Bounds (ELBOs), enabling independent, local updates while preserving global coherence. However, directly applying KL divergence in layer-wise ELBOs risks inter-layer's representation collapse due to excessive compression. To prevent this, SVP projects activations into low-dimensional spaces via fixed random matrices, ensuring information preservation and representational diversity. Combined with a feature alignment loss for inter-layer consistency, SVP achieves competitive accuracy with BP across diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to ImageNet), reduces memory usage by up to 4x, and significantly improves scalability. More broadly, SVP introduces a probabilistic perspective to deep representation learning, opening pathways toward more modular and interpretable neural network design.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks</title>
<link>https://arxiv.org/abs/2505.05190</link>
<guid>https://arxiv.org/abs/2505.05190</guid>
<content:encoded><![CDATA[
<div> Keywords: text watermarking, robustness, Self-Information Rewrite Attack, vulnerability, large language models  

<br /><br />Summary:  
Text watermarking is a technique using statistical signals embedded into text through the sampling process of Large Language Models (LLMs) to allow verification by detectors. The effectiveness of these algorithms hinges on their robustness. Current text watermarking methods utilize high-entropy tokens to maintain text quality. However, researchers have discovered that this design can be exploited, exposing a critical vulnerability. They present a novel attack called the Self-Information Rewrite Attack (SIRA), which targets the watermark by calculating the self-information of each token, identifying at-risk pattern tokens for a focused attack. The study reveals a widespread vulnerability present in multiple watermarking approaches. Experimental results indicate that SIRA achieves nearly 100% success rates on seven different watermarking methods at a minimal cost of 0.88 USD per million tokens. Notably, this attack does not require access to the watermark algorithms or the watermarked LLM and can be effectively applied across any LLM, including mobile-level models. The findings underscore an urgent need for enhanced robustness in watermarking techniques to safeguard against such vulnerabilities. <div>
arXiv:2505.05190v1 Announce Type: new 
Abstract: Text watermarking aims to subtly embed statistical signals into text by controlling the Large Language Model (LLM)'s sampling process, enabling watermark detectors to verify that the output was generated by the specified model. The robustness of these watermarking algorithms has become a key factor in evaluating their effectiveness. Current text watermarking algorithms embed watermarks in high-entropy tokens to ensure text quality. In this paper, we reveal that this seemingly benign design can be exploited by attackers, posing a significant risk to the robustness of the watermark. We introduce a generic efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA), which leverages the vulnerability by calculating the self-information of each token to identify potential pattern tokens and perform targeted attack. Our work exposes a widely prevalent vulnerability in current watermarking algorithms. The experimental results show SIRA achieves nearly 100% attack success rates on seven recent watermarking methods with only 0.88 USD per million tokens cost. Our approach does not require any access to the watermark algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the attack model, even mobile-level models. Our findings highlight the urgent need for more robust watermarking.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Term Individual Causal Effect Estimation via Identifiable Latent Representation Learning</title>
<link>https://arxiv.org/abs/2505.05192</link>
<guid>https://arxiv.org/abs/2505.05192</guid>
<content:encoded><![CDATA[
<div> Keywords: long-term causal effects, observational data, latent confounders, representation learning, experimental data

<br /><br />Summary: This paper addresses the challenge of estimating long-term causal effects by integrating long-term observational data with short-term experimental data. Traditional methods rely on ideal assumptions such as latent unconfoundedness, which often do not hold in practical scenarios, limiting their effectiveness. The authors propose a novel approach that utilizes the natural heterogeneity of multi-source data to identify latent confounders without depending on idealized assumptions. They introduce a latent representation learning-based estimator for long-term causal effects, enhancing accuracy in real-world applications. The paper establishes the theoretical framework for identifying latent confounders, thereby facilitating long-term effect identification. The proposed method demonstrates significant improvements through extensive experimental studies on various synthetic and semi-synthetic datasets, showcasing its effectiveness and robustness. Overall, this work provides a new direction for causal inference in complex environments where traditional assumptions are violated, offering a more reliable tool for researchers and practitioners in fields requiring causal estimation. <div>
arXiv:2505.05192v1 Announce Type: new 
Abstract: Estimating long-term causal effects by combining long-term observational and short-term experimental data is a crucial but challenging problem in many real-world scenarios. In existing methods, several ideal assumptions, e.g. latent unconfoundedness assumption or additive equi-confounding bias assumption, are proposed to address the latent confounder problem raised by the observational data. However, in real-world applications, these assumptions are typically violated which limits their practical effectiveness. In this paper, we tackle the problem of estimating the long-term individual causal effects without the aforementioned assumptions. Specifically, we propose to utilize the natural heterogeneity of data, such as data from multiple sources, to identify latent confounders, thereby significantly avoiding reliance on idealized assumptions. Practically, we devise a latent representation learning-based estimator of long-term causal effects. Theoretically, we establish the identifiability of latent confounders, with which we further achieve long-term effect identification. Extensive experimental studies, conducted on multiple synthetic and semi-synthetic datasets, demonstrate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept-Based Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.05195</link>
<guid>https://arxiv.org/abs/2505.05195</guid>
<content:encoded><![CDATA[
<div> Keywords: Concept Bottleneck Models, domain adaptation, adversarial training, interpretability, unsupervised learning  

<br /><br />Summary: Concept Bottleneck Models (CBMs) enhance interpretability by relying on human-understandable concepts to explain predictions. However, they usually assume consistent data distributions between training and testing, which is a limitation in cases of domain shifts that can harm performance. To tackle these challenges, the authors propose the Concept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA aims to: (1) align concept representations across varying domains through adversarial training; (2) introduce a relaxation threshold to accommodate minor domain-specific variations in concept distributions, reducing over-constraints that lead to performance drops; (3) enable direct inference of concepts in the target domain without needing labeled concept data, allowing flexible adaptation to different domains; and (4) integrate concept learning with traditional domain adaptation methods, providing theoretical guarantees to enhance interpretability. The experiments conducted demonstrate that CUDA significantly surpasses the existing state-of-the-art approaches for both CBMs and domain adaptation methods when tested on real-world datasets, establishing new performance benchmarks while simultaneously improving the robustness and interpretability of the models. <div>
arXiv:2505.05195v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by explaining predictions through human-understandable concepts but typically assume that training and test data share the same distribution. This assumption often fails under domain shifts, leading to degraded performance and poor generalization. To address these limitations and improve the robustness of CBMs, we propose the Concept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed to: (1) align concept representations across domains using adversarial training, (2) introduce a relaxation threshold to allow minor domain-specific differences in concept distributions, thereby preventing performance drop due to over-constraints of these distributions, (3) infer concepts directly in the target domain without requiring labeled concept data, enabling CBMs to adapt to diverse domains, and (4) integrate concept learning into conventional domain adaptation (DA) with theoretical guarantees, improving interpretability and establishing new benchmarks for DA. Experiments demonstrate that our approach significantly outperforms the state-of-the-art CBM and DA methods on real-world datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GFlowNets for Active Learning Based Resource Allocation in Next Generation Wireless Networks</title>
<link>https://arxiv.org/abs/2505.05224</link>
<guid>https://arxiv.org/abs/2505.05224</guid>
<content:encoded><![CDATA[
<div> Keywords: radio resource allocation, wireless systems, active learning, GFlowNet, performance improvement

<br /><br />Summary: In this work, the authors address the radio resource allocation problem within a multifunctional wireless system that incorporates communication, sensing, and computing capabilities. They propose innovative resource management techniques tailored to meet diverse requirements while effectively handling the high-dimensional and discrete nature of the allocation challenges. The study introduces a novel active learning framework wherein resource allocation patterns are sequentially selected, assessed in their operating environment, and subsequently used to iteratively refine a surrogate model representing that environment. A key component of their approach is the application of a generative flow network (GFlowNet). This network samples promising solutions based on their training reward, ensuring a comprehensive exploration of potential outcomes. This methodology enables GFlowNet to produce varied and high-yield resource management designs that enhance the surrogate model and expedite the identification of effective solutions. Simulation results showcase the superiority of the proposed method, demonstrating performance improvements of 20% over existing benchmarks while utilizing less than half the number of acquisition rounds typically required in similar circumstances. <div>
arXiv:2505.05224v1 Announce Type: new 
Abstract: In this work, we consider the radio resource allocation problem in a wireless system with various integrated functionalities, such as communication, sensing and computing. We design suitable resource management techniques that can simultaneously cater to those heterogeneous requirements, and scale appropriately with the high-dimensional and discrete nature of the problem. We propose a novel active learning framework where resource allocation patterns are drawn sequentially, evaluated in the environment, and then used to iteratively update a surrogate model of the environment. Our method leverages a generative flow network (GFlowNet) to sample favorable solutions, as such models are trained to generate compositional objects proportionally to their training reward, hence providing an appropriate coverage of its modes. As such, GFlowNet generates diverse and high return resource management designs that update the surrogate model and swiftly discover suitable solutions. We provide simulation results showing that our method can allocate radio resources achieving 20% performance gains against benchmarks, while requiring less than half of the number of acquisition rounds.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning</title>
<link>https://arxiv.org/abs/2505.05226</link>
<guid>https://arxiv.org/abs/2505.05226</guid>
<content:encoded><![CDATA[
<div> Keywords: AUTO-MML, MaxUCB, algorithm selection, hyperparameter optimization, bandit methods

<br /><br />Summary: 
The paper introduces MaxUCB, a novel max $k$-armed bandit method that addresses the Combined Algorithm Selection and Hyperparameter optimization (CASH) problem in AutoML. CASH is considered a challenging resource allocation problem due to the need for balancing the exploration of various model classes and the optimization of hyperparameters. MaxUCB is specifically tailored for scenarios involving light-tailed and bounded reward distributions, which are common in this context. Unlike traditional max $k$-armed bandit methods that generally assume heavy-tailed rewards, MaxUCB provides a more efficient and effective alternative. The authors conduct both theoretical analyses and empirical evaluations of their method across four established AutoML benchmarks. The results demonstrate that MaxUCB outperforms previous approaches in terms of performance, showing its potential as a robust option for tackling CASH problems in AutoML. This research contributes to the field by enhancing the understanding and methodologies available for automated machine learning, thereby serving both practitioners and researchers interested in improving algorithm selection and hyperparameter tuning processes. <div>
arXiv:2505.05226v1 Announce Type: new 
Abstract: The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a challenging resource allocation problem in the field of AutoML. We propose MaxUCB, a max $k$-armed bandit method to trade off exploring different model classes and conducting hyperparameter optimization. MaxUCB is specifically designed for the light-tailed and bounded reward distributions arising in this setting and, thus, provides an efficient alternative compared to classic max $k$-armed bandit methods assuming heavy-tailed reward distributions. We theoretically and empirically evaluate our method on four standard AutoML benchmarks, demonstrating superior performance over prior approaches.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular Learning</title>
<link>https://arxiv.org/abs/2505.05237</link>
<guid>https://arxiv.org/abs/2505.05237</guid>
<content:encoded><![CDATA[
<div> Keywords: Few-shot learning, tabular data, Large Language Models, Latte, knowledge extraction  

<br /><br />Summary:  
The paper introduces Latte, a novel framework designed for few-shot tabular learning, which involves training machine learning models with limited labeled data. This approach aims to be cost-effective for addressing real-world challenges. The emergence of Large Language Models (LLMs) has led to interest in utilizing their pre-trained knowledge for enhancing tabular learning. However, existing methods face issues with test-time knowledge extraction, resulting in latency, and reliance on text-level knowledge that can lead to unreliable feature engineering. Latte addresses these limitations by providing a training-time knowledge extraction mechanism that optimizes downstream models by transferring latent knowledge from LLMs. It facilitates general knowledge-guided learning and allows for weighted fusion of information across different feature values, helping to mitigate overfitting to small labeled datasets. Additionally, Latte is designed to work well with current unsupervised pre-training methods, effectively leveraging available unlabeled samples to improve performance constraints imposed by having very few labeled instances. Comprehensive experiments across several few-shot tabular learning benchmarks exhibit Latte's superior performance, establishing it as a leading approach in the field. <div>
arXiv:2505.05237v1 Announce Type: new 
Abstract: Few-shot tabular learning, in which machine learning models are trained with a limited amount of labeled data, provides a cost-effective approach to addressing real-world challenges. The advent of Large Language Models (LLMs) has sparked interest in leveraging their pre-trained knowledge for few-shot tabular learning. Despite promising results, existing approaches either rely on test-time knowledge extraction, which introduces undesirable latency, or text-level knowledge, which leads to unreliable feature engineering. To overcome these limitations, we propose Latte, a training-time knowledge extraction framework that transfers the latent prior knowledge within LLMs to optimize a more generalized downstream model. Latte enables general knowledge-guided downstream tabular learning, facilitating the weighted fusion of information across different feature values while reducing the risk of overfitting to limited labeled data. Furthermore, Latte is compatible with existing unsupervised pre-training paradigms and effectively utilizes available unlabeled samples to overcome the performance limitations imposed by an extremely small labeled dataset. Extensive experiments on various few-shot tabular learning benchmarks demonstrate the superior performance of Latte, establishing it as a state-of-the-art approach in this domain
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Treatment Effect Estimation via Active Learning: A Counterfactual Covering Perspective</title>
<link>https://arxiv.org/abs/2505.05242</link>
<guid>https://arxiv.org/abs/2505.05242</guid>
<content:encoded><![CDATA[
arXiv:2505.05242v1 Announce Type: new 
Abstract: Although numerous complex algorithms for treatment effect estimation have been developed in recent years, their effectiveness remains limited when handling insufficiently labeled training sets due to the high cost of labeling the effect after treatment, e.g., expensive tumor imaging or biopsy procedures needed to evaluate treatment effects. Therefore, it becomes essential to actively incorporate more high-quality labeled data, all while adhering to a constrained labeling budget. To enable data-efficient treatment effect estimation, we formalize the problem through rigorous theoretical analysis within the active learning context, where the derived key measures -- \textit{factual} and \textit{counterfactual covering radius} determine the risk upper bound. To reduce the bound, we propose a greedy radius reduction algorithm, which excels under an idealized, balanced data distribution. To generalize to more realistic data distributions, we further propose FCCM, which transforms the optimization objective into the \textit{Factual} and \textit{Counterfactual Coverage Maximization} to ensure effective radius reduction during data acquisition. Furthermore, benchmarking FCCM against other baselines demonstrates its superiority across both fully synthetic and semi-synthetic datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Cooperative Multi-Agent Reinforcement Learning with State Modelling and Adversarial Exploration</title>
<link>https://arxiv.org/abs/2505.05262</link>
<guid>https://arxiv.org/abs/2505.05262</guid>
<content:encoded><![CDATA[
arXiv:2505.05262v1 Announce Type: new 
Abstract: Learning to cooperate in distributed partially observable environments with no communication abilities poses significant challenges for multi-agent deep reinforcement learning (MARL). This paper addresses key concerns in this domain, focusing on inferring state representations from individual agent observations and leveraging these representations to enhance agents' exploration and collaborative task execution policies. To this end, we propose a novel state modelling framework for cooperative MARL, where agents infer meaningful belief representations of the non-observable state, with respect to optimizing their own policies, while filtering redundant and less informative joint state information. Building upon this framework, we propose the MARL SMPE algorithm. In SMPE, agents enhance their own policy's discriminative abilities under partial observability, explicitly by incorporating their beliefs into the policy network, and implicitly by adopting an adversarial type of exploration policies which encourages agents to discover novel, high-value states while improving the discriminative abilities of others. Experimentally, we show that SMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative tasks from the MPE, LBF, and RWARE benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTL-UE: Learning to Learn Nothing for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2505.05279</link>
<guid>https://arxiv.org/abs/2505.05279</guid>
<content:encoded><![CDATA[
arXiv:2505.05279v1 Announce Type: new 
Abstract: Most existing unlearnable strategies focus on preventing unauthorized users from training single-task learning (STL) models with personal data. Nevertheless, the paradigm has recently shifted towards multi-task data and multi-task learning (MTL), targeting generalist and foundation models that can handle multiple tasks simultaneously. Despite their growing importance, MTL data and models have been largely neglected while pursuing unlearnable strategies. This paper presents MTL-UE, the first unified framework for generating unlearnable examples for multi-task data and MTL models. Instead of optimizing perturbations for each sample, we design a generator-based structure that introduces label priors and class-wise feature embeddings which leads to much better attacking performance. In addition, MTL-UE incorporates intra-task and inter-task embedding regularization to increase inter-class separation and suppress intra-class variance which enhances the attack robustness greatly. Furthermore, MTL-UE is versatile with good supports for dense prediction tasks in MTL. It is also plug-and-play allowing integrating existing surrogate-dependent unlearnable methods with little adaptation. Extensive experiments show that MTL-UE achieves superior attacking performance consistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5 MTL task-weighting strategies.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Estimation in Binary Classification Using Calibrated Confidence</title>
<link>https://arxiv.org/abs/2505.05295</link>
<guid>https://arxiv.org/abs/2505.05295</guid>
<content:encoded><![CDATA[
arXiv:2505.05295v1 Announce Type: new 
Abstract: Model monitoring is a critical component of the machine learning lifecycle, safeguarding against undetected drops in the model's performance after deployment. Traditionally, performance monitoring has required access to ground truth labels, which are not always readily available. This can result in unacceptable latency or render performance monitoring altogether impossible. Recently, methods designed to estimate the accuracy of classifier models without access to labels have shown promising results. However, there are various other metrics that might be more suitable for assessing model performance in many cases. Until now, none of these important metrics has received similar interest from the scientific community. In this work, we address this gap by presenting CBPE, a novel method that can estimate any binary classification metric defined using the confusion matrix. In particular, we choose four metrics from this large family: accuracy, precision, recall, and F$_1$, to demonstrate our method. CBPE treats the elements of the confusion matrix as random variables and leverages calibrated confidence scores of the model to estimate their distributions. The desired metric is then also treated as a random variable, whose full probability distribution can be derived from the estimated confusion matrix. CBPE is shown to produce estimates that come with strong theoretical guarantees and valid confidence intervals.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Chain of Thoughts via Elastic Reasoning</title>
<link>https://arxiv.org/abs/2505.05315</link>
<guid>https://arxiv.org/abs/2505.05315</guid>
<content:encoded><![CDATA[
arXiv:2505.05315v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases--thinking and solution--with independently allocated budgets. At test time, Elastic Reasoning prioritize that completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearly Optimal Sample Complexity for Learning with Label Proportions</title>
<link>https://arxiv.org/abs/2505.05355</link>
<guid>https://arxiv.org/abs/2505.05355</guid>
<content:encoded><![CDATA[
arXiv:2505.05355v1 Announce Type: new 
Abstract: We investigate Learning from Label Proportions (LLP), a partial information setting where examples in a training set are grouped into bags, and only aggregate label values in each bag are available. Despite the partial observability, the goal is still to achieve small regret at the level of individual examples. We give results on the sample complexity of LLP under square loss, showing that our sample complexity is essentially optimal. From an algorithmic viewpoint, we rely on carefully designed variants of Empirical Risk Minimization, and Stochastic Gradient Descent algorithms, combined with ad hoc variance reduction techniques. On one hand, our theoretical results improve in important ways on the existing literature on LLP, specifically in the way the sample complexity depends on the bag size. On the other hand, we validate our algorithmic solutions on several datasets, demonstrating improved empirical performance (better accuracy for less samples) against recent baselines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising Diffusion Probabilistic Models for Coastal Inundation Forecasting</title>
<link>https://arxiv.org/abs/2505.05381</link>
<guid>https://arxiv.org/abs/2505.05381</guid>
<content:encoded><![CDATA[
arXiv:2505.05381v1 Announce Type: new 
Abstract: Coastal flooding poses significant risks to communities, necessitating fast and accurate forecasting methods to mitigate potential damage. To approach this problem, we present DIFF-FLOOD, a probabilistic spatiotemporal forecasting method designed based on denoising diffusion models. DIFF-FLOOD predicts inundation level at a location by taking both spatial and temporal context into account. It utilizes inundation levels at neighboring locations and digital elevation data as spatial context. Inundation history from a context time window, together with additional co-variates are used as temporal context. Convolutional neural networks and cross-attention mechanism are then employed to capture the spatiotemporal dynamics in the data. We trained and tested DIFF-FLOOD on coastal inundation data from the Eastern Shore of Virginia, a region highly impacted by coastal flooding. Our results show that, DIFF-FLOOD outperforms existing forecasting methods in terms of prediction performance (6% to 64% improvement in terms of two performance metrics) and scalability.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CART-ELC: Oblique Decision Tree Induction via Exhaustive Search</title>
<link>https://arxiv.org/abs/2505.05402</link>
<guid>https://arxiv.org/abs/2505.05402</guid>
<content:encoded><![CDATA[
arXiv:2505.05402v1 Announce Type: new 
Abstract: Oblique decision trees have attracted attention due to their potential for improved classification performance over traditional axis-aligned decision trees. However, methods that rely on exhaustive search to find oblique splits face computational challenges. As a result, they have not been widely explored. We introduce a novel algorithm, Classification and Regression Tree - Exhaustive Linear Combinations (CART-ELC), for inducing oblique decision trees that performs an exhaustive search on a restricted set of hyperplanes. We then investigate the algorithm's computational complexity and its predictive capabilities. Our results demonstrate that CART-ELC consistently achieves competitive performance on small datasets, often yielding statistically significant improvements in classification accuracy relative to existing decision tree induction algorithms, while frequently producing shallower, simpler, and thus more interpretable trees.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hide &amp; Seek: Transformer Symmetries Obscure Sharpness &amp; Riemannian Geometry Finds It</title>
<link>https://arxiv.org/abs/2505.05409</link>
<guid>https://arxiv.org/abs/2505.05409</guid>
<content:encoded><![CDATA[
arXiv:2505.05409v1 Announce Type: new 
Abstract: The concept of sharpness has been successfully applied to traditional architectures like MLPs and CNNs to predict their generalization. For transformers, however, recent work reported weak correlation between flatness and generalization. We argue that existing sharpness measures fail for transformers, because they have much richer symmetries in their attention mechanism that induce directions in parameter space along which the network or its loss remain identical. We posit that sharpness must account fully for these symmetries, and thus we redefine it on a quotient manifold that results from quotienting out the transformer symmetries, thereby removing their ambiguities. Leveraging tools from Riemannian geometry, we propose a fully general notion of sharpness, in terms of a geodesic ball on the symmetry-corrected quotient manifold. In practice, we need to resort to approximating the geodesics. Doing so up to first order yields existing adaptive sharpness measures, and we demonstrate that including higher-order terms is crucial to recover correlation with generalization. We present results on diagonal networks with synthetic data, and show that our geodesic sharpness reveals strong correlation for real-world transformers on both text and image classification tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPQ-HD: Post-Training Compression for Ultra-Low Power Hyperdimensional Computing</title>
<link>https://arxiv.org/abs/2505.05413</link>
<guid>https://arxiv.org/abs/2505.05413</guid>
<content:encoded><![CDATA[
arXiv:2505.05413v1 Announce Type: new 
Abstract: Hyperdimensional Computing (HDC) is emerging as a promising approach for edge AI, offering a balance between accuracy and efficiency. However, current HDC-based applications often rely on high-precision models and/or encoding matrices to achieve competitive performance, which imposes significant computational and memory demands, especially for ultra-low power devices. While recent efforts use techniques like precision reduction and pruning to increase the efficiency, most require retraining to maintain performance, making them expensive and impractical. To address this issue, we propose a novel Post Training Compression algorithm, Decomposition-Pruning-Quantization (DPQ-HD), which aims at compressing the end-to-end HDC system, achieving near floating point performance without the need of retraining. DPQ-HD reduces computational and memory overhead by uniquely combining the above three compression techniques and efficiently adapts to hardware constraints. Additionally, we introduce an energy-efficient inference approach that progressively evaluates similarity scores such as cosine similarity and performs early exit to reduce the computation, accelerating prediction inference while maintaining accuracy. We demonstrate that DPQ-HD achieves up to 20-100x reduction in memory for image and graph classification tasks with only a 1-2% drop in accuracy compared to uncompressed workloads. Lastly, we show that DPQ-HD outperforms the existing post-training compression methods and performs better or at par with retraining-based state-of-the-art techniques, requiring significantly less overall optimization time (up to 100x) and faster inference (up to 56x) on a microcontroller
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with Uncertainty-Aware Constrained Ensembles</title>
<link>https://arxiv.org/abs/2505.05452</link>
<guid>https://arxiv.org/abs/2505.05452</guid>
<content:encoded><![CDATA[
arXiv:2505.05452v1 Announce Type: new 
Abstract: Machine learning has become a powerful tool for enhancing data assimilation. While supervised learning remains the standard method, reinforcement learning (RL) offers unique advantages through its sequential decision-making framework, which naturally fits the iterative nature of data assimilation by dynamically balancing model forecasts with observations. We develop RL-DAUNCE, a new RL-based method that enhances data assimilation with physical constraints through three key aspects. First, RL-DAUNCE inherits the computational efficiency of machine learning while it uniquely structures its agents to mirror ensemble members in conventional data assimilation methods. Second, RL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble members, moving beyond simple mean-state optimization. Third, RL-DAUNCE's ensemble-as-agents design facilitates the enforcement of physical constraints during the assimilation process, which is crucial to improving the state estimation and subsequent forecasting. A primal-dual optimization strategy is developed to enforce constraints, which dynamically penalizes the reward function to ensure constraint satisfaction throughout the learning process. Also, state variable bounds are respected by constraining the RL action space. Together, these features ensure physical consistency without sacrificing efficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an intermittent atmospheric phenomenon characterized by strongly non-Gaussian features and multiple physical constraints. RL-DAUNCE outperforms the standard ensemble Kalman filter (EnKF), which fails catastrophically due to the violation of physical constraints. Notably, RL-DAUNCE matches the performance of constrained EnKF, particularly in recovering intermittent signals, capturing extreme events, and quantifying uncertainties, while requiring substantially less computational effort.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BitHEP -- The Limits of Low-Precision ML in HEP</title>
<link>https://arxiv.org/abs/2504.03387</link>
<guid>https://arxiv.org/abs/2504.03387</guid>
<content:encoded><![CDATA[
arXiv:2504.03387v1 Announce Type: cross 
Abstract: The increasing complexity of modern neural network architectures demands fast and memory-efficient implementations to mitigate computational bottlenecks. In this work, we evaluate the recently proposed BitNet architecture in HEP applications, assessing its performance in classification, regression, and generative modeling tasks. Specifically, we investigate its suitability for quark-gluon discrimination, SMEFT parameter estimation, and detector simulation, comparing its efficiency and accuracy to state-of-the-art methods. Our results show that while BitNet consistently performs competitively in classification tasks, its performance in regression and generation varies with the size and type of the network, highlighting key limitations and potential areas for improvement.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cryptogenic stroke and migraine: using probabilistic independence and machine learning to uncover latent sources of disease from the electronic health record</title>
<link>https://arxiv.org/abs/2505.04631</link>
<guid>https://arxiv.org/abs/2505.04631</guid>
<content:encoded><![CDATA[
arXiv:2505.04631v1 Announce Type: cross 
Abstract: Migraine is a common but complex neurological disorder that doubles the lifetime risk of cryptogenic stroke (CS). However, this relationship remains poorly characterized, and few clinical guidelines exist to reduce this associated risk. We therefore propose a data-driven approach to extract probabilistically-independent sources from electronic health record (EHR) data and create a 10-year risk-predictive model for CS in migraine patients. These sources represent external latent variables acting on the causal graph constructed from the EHR data and approximate root causes of CS in our population. A random forest model trained on patient expressions of these sources demonstrated good accuracy (ROC 0.771) and identified the top 10 most predictive sources of CS in migraine patients. These sources revealed that pharmacologic interventions were the most important factor in minimizing CS risk in our population and identified a factor related to allergic rhinitis as a potential causative source of CS in migraine patients.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation</title>
<link>https://arxiv.org/abs/2505.04643</link>
<guid>https://arxiv.org/abs/2505.04643</guid>
<content:encoded><![CDATA[
arXiv:2505.04643v1 Announce Type: cross 
Abstract: Estimating population parameters in finite populations of text documents can be challenging when obtaining the labels for the target variable requires manual annotation. To address this problem, we combine predictions from a transformer encoder neural network with well-established survey sampling estimators using the model predictions as an auxiliary variable. The applicability is demonstrated in Swedish hate crime statistics based on Swedish police reports. Estimates of the yearly number of hate crimes and the police's under-reporting are derived using the Hansen-Hurwitz estimator, difference estimation, and stratified random sampling estimation. We conclude that if labeled training data is available, the proposed method can provide very efficient estimates with reduced time spent on manual annotation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT for automated grading of short answer questions in mechanical ventilation</title>
<link>https://arxiv.org/abs/2505.04645</link>
<guid>https://arxiv.org/abs/2505.04645</guid>
<content:encoded><![CDATA[
arXiv:2505.04645v1 Announce Type: cross 
Abstract: Standardised tests using short answer questions (SAQs) are common in postgraduate education. Large language models (LLMs) simulate conversational language and interpret unstructured free-text responses in ways aligning with applying SAQ grading rubrics, making them attractive for automated grading. We evaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data from 215 students (557 short-answer responses) enrolled in an online course on mechanical ventilation (2020--2024). Deidentified responses to three case-based scenarios were presented to ChatGPT with a standardised grading prompt and rubric. Outputs were analysed using mixed-effects modelling, variance component analysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's W, and Bland--Altman statistics. ChatGPT awarded systematically lower marks than human graders with a mean difference (bias) of -1.34 on a 10-point scale. ICC values indicated poor individual-level agreement (ICC1 = 0.086), and Cohen's kappa (-0.0786) suggested no meaningful agreement. Variance component analysis showed minimal variability among the five ChatGPT sessions (G-value = 0.87), indicating internal consistency but divergence from the human grader. The poorest agreement was observed for evaluative and analytic items, whereas checklist and prescriptive rubric items had less disagreement. We caution against the use of LLMs in grading postgraduate coursework. Over 60% of ChatGPT-assigned grades differed from human grades by more than acceptable boundaries for high-stakes assessments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChannelExplorer: Exploring Class Separability Through Activation Channel Visualization</title>
<link>https://arxiv.org/abs/2505.04647</link>
<guid>https://arxiv.org/abs/2505.04647</guid>
<content:encoded><![CDATA[
arXiv:2505.04647v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) achieve state-of-the-art performance in many vision tasks, yet understanding their internal behavior remains challenging, particularly how different layers and activation channels contribute to class separability. We introduce ChannelExplorer, an interactive visual analytics tool for analyzing image-based outputs across model layers, emphasizing data-driven insights over architecture analysis for exploring class separability. ChannelExplorer summarizes activations across layers and visualizes them using three primary coordinated views: a Scatterplot View to reveal inter- and intra-class confusion, a Jaccard Similarity View to quantify activation overlap, and a Heatmap View to inspect activation channel patterns. Our technique supports diverse model architectures, including CNNs, GANs, ResNet and Stable Diffusion models. We demonstrate the capabilities of ChannelExplorer through four use-case scenarios: (1) generating class hierarchy in ImageNet, (2) finding mislabeled images, (3) identifying activation channel contributions, and(4) locating latent states' position in Stable Diffusion model. Finally, we evaluate the tool with expert users.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum QSAR for drug discovery</title>
<link>https://arxiv.org/abs/2505.04648</link>
<guid>https://arxiv.org/abs/2505.04648</guid>
<content:encoded><![CDATA[
arXiv:2505.04648v1 Announce Type: cross 
Abstract: Quantitative Structure-Activity Relationship (QSAR) modeling is key in drug discovery, but classical methods face limitations when handling high-dimensional data and capturing complex molecular interactions. This research proposes enhancing QSAR techniques through Quantum Support Vector Machines (QSVMs), which leverage quantum computing principles to process information Hilbert spaces. By using quantum data encoding and quantum kernel functions, we aim to develop more accurate and efficient predictive models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Benchmarking and Recommendation of Text-to-Image Generation Models</title>
<link>https://arxiv.org/abs/2505.04650</link>
<guid>https://arxiv.org/abs/2505.04650</guid>
<content:encoded><![CDATA[
arXiv:2505.04650v1 Announce Type: cross 
Abstract: This work presents an open-source unified benchmarking and evaluation framework for text-to-image generation models, with a particular focus on the impact of metadata augmented prompts. Leveraging the DeepFashion-MultiModal dataset, we assess generated outputs through a comprehensive set of quantitative metrics, including Weighted Score, CLIP (Contrastive Language Image Pre-training)-based similarity, LPIPS (Learned Perceptual Image Patch Similarity), FID (Frechet Inception Distance), and retrieval-based measures, as well as qualitative analysis. Our results demonstrate that structured metadata enrichments greatly enhance visual realism, semantic fidelity, and model robustness across diverse text-to-image architectures. While not a traditional recommender system, our framework enables task-specific recommendations for model selection and prompt design based on evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions</title>
<link>https://arxiv.org/abs/2505.04651</link>
<guid>https://arxiv.org/abs/2505.04651</guid>
<content:encoded><![CDATA[
arXiv:2505.04651v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are transforming scientific hypothesis generation and validation by enabling information synthesis, latent relationship discovery, and reasoning augmentation. This survey provides a structured overview of LLM-driven approaches, including symbolic frameworks, generative models, hybrid systems, and multi-agent architectures. We examine techniques such as retrieval-augmented generation, knowledge-graph completion, simulation, causal inference, and tool-assisted reasoning, highlighting trade-offs in interpretability, novelty, and domain alignment. We contrast early symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM pipelines that leverage in-context learning and domain adaptation via fine-tuning, retrieval, and symbolic grounding. For validation, we review simulation, human-AI collaboration, causal modeling, and uncertainty quantification, emphasizing iterative assessment in open-world contexts. The survey maps datasets across biomedicine, materials science, environmental science, and social science, introducing new resources like AHTech and CSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation, multimodal-symbolic integration, human-in-the-loop systems, and ethical safeguards, positioning LLMs as agents for principled, scalable scientific discovery.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Conversational Diagnostic AI with Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.04653</link>
<guid>https://arxiv.org/abs/2505.04653</guid>
<content:encoded><![CDATA[
arXiv:2505.04653v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. Instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of LLMs to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. Here we advance the conversational diagnosis and management performance of the Articulate Medical Intelligence Explorer (AMIE) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. Leveraging Gemini 2.0 Flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. Follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. We compared AMIE to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of chat-based consultations with patient actors. We constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of clinical documents across diverse conditions and demographics. Our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The results show clear progress in multimodal conversational diagnostic AI, but real-world translation needs further research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes</title>
<link>https://arxiv.org/abs/2505.04666</link>
<guid>https://arxiv.org/abs/2505.04666</guid>
<content:encoded><![CDATA[
arXiv:2505.04666v1 Announce Type: cross 
Abstract: Building codes are regulations that establish standards for the design, construction, and safety of buildings to ensure structural integrity, fire protection, and accessibility. They are often extensive, complex, and subject to frequent updates, making manual querying challenging and time-consuming. Key difficulties include navigating large volumes of text, interpreting technical language, and identifying relevant clauses across different sections. A potential solution is to build a Question-Answering (QA) system that answers user queries based on building codes. Among the various methods for building a QA system, Retrieval-Augmented Generation (RAG) stands out in performance. RAG consists of two components: a retriever and a language model. This study focuses on identifying a suitable retriever method for building codes and optimizing the generational capability of the language model using fine-tuning techniques. We conducted a detailed evaluation of various retrieval methods by performing the retrieval on the National Building Code of Canada (NBCC) and explored the impact of domain-specific fine-tuning on several language models using the dataset derived from NBCC. Our analysis included a comparative assessment of different retrievers and the performance of both pre-trained and fine-tuned models to determine the efficacy and domain-specific adaptation of language models using fine-tuning on the NBCC dataset. Experimental results showed that Elasticsearch proved to be the most robust retriever among all. The findings also indicate that fine-tuning language models on an NBCC-specific dataset can enhance their ability to generate contextually relevant responses. When combined with context retrieved by a powerful retriever like Elasticsearch, this improvement in LLM performance can optimize the RAG system, enabling it to better navigate the complexities of the NBCC.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards</title>
<link>https://arxiv.org/abs/2505.04671</link>
<guid>https://arxiv.org/abs/2505.04671</guid>
<content:encoded><![CDATA[
arXiv:2505.04671v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly improved performance on the Text-to-SQL task by leveraging their powerful reasoning capabilities. To enhance accuracy during the reasoning process, external Process Reward Models (PRMs) can be introduced during training and inference to provide fine-grained supervision. However, if misused, PRMs may distort the reasoning trajectory and lead to suboptimal or incorrect SQL generation.To address this challenge, we propose Reward-SQL, a framework that systematically explores how to incorporate PRMs into the Text-to-SQL reasoning process effectively. Our approach follows a "cold start, then PRM supervision" paradigm. Specifically, we first train the model to decompose SQL queries into structured stepwise reasoning chains using common table expressions (Chain-of-CTEs), establishing a strong and interpretable reasoning baseline. Then, we investigate four strategies for integrating PRMs, and find that combining PRM as an online training signal (GRPO) with PRM-guided inference (e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD benchmark, Reward-SQL enables models supervised by a 7B PRM to achieve a 13.1% performance gain across various guidance strategies. Notably, our GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the BIRD development set, outperforming all baseline methods under the same model size. These results demonstrate the effectiveness of Reward-SQL in leveraging reward-based supervision for Text-to-SQL reasoning. Our code is publicly available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proceedings The 13th International Workshop on Theorem proving components for Educational software</title>
<link>https://arxiv.org/abs/2505.04677</link>
<guid>https://arxiv.org/abs/2505.04677</guid>
<content:encoded><![CDATA[
arXiv:2505.04677v1 Announce Type: cross 
Abstract: The ThEdu series pursues the smooth transition from an intuitive way of doing mathematics at secondary school to a more formal approach to the subject in STEM education while favoring software support for this transition by exploiting the power of theorem-proving technologies.  What follows is a brief description of how the present volume contributes to this enterprise.  The 13th International Workshop on Theorem Proving Components for Educational Software (ThEdu'24), was a satellite event of the CADE29, part of IJCAR 2024, Nancy, France. ThEdu'24 was a vibrant workshop, with one invited talk by Jeremy Avigad (Carnegie Mellon University) and 14 submitted talks. An open call for papers was then issued and attracted 9 submissions. Eight of those submissions have been accepted by our reviewers. The resulting revised papers are collected in the present volume. The contributions in this volume are a faithful representation of the wide spectrum of ThEdu, ranging from those more focused on the automated deduction research, not losing track of the possible applications in an educational setting, to those focused on the applications, in educational settings, of automated deduction tools and methods. We, the volume editors, hope that this collection of papers will further promote the development of theorem-proving-based software and that it will allow to improve the mutual understanding between computer scientists, mathematicians, and stakeholders in education. While this volume goes to press, the next edition of the ThEdu workshop is being prepared: ThEdu'25 will be a satellite event of the 30th international Conference on Automated DEduction (CADE-30), July 28th - August 2nd, 2025, Stuttgart, Germany.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Visual Trackers for Biomechanical Analysis of Running</title>
<link>https://arxiv.org/abs/2505.04713</link>
<guid>https://arxiv.org/abs/2505.04713</guid>
<content:encoded><![CDATA[
arXiv:2505.04713v1 Announce Type: cross 
Abstract: Human pose estimation has witnessed significant advancements in recent years, mainly due to the integration of deep learning models, the availability of a vast amount of data, and large computational resources. These developments have led to highly accurate body tracking systems, which have direct applications in sports analysis and performance evaluation.
  This work analyzes the performance of six trackers: two point trackers and four joint trackers for biomechanical analysis in sprints. The proposed framework compares the results obtained from these pose trackers with the manual annotations of biomechanical experts for more than 5870 frames. The experimental framework employs forty sprints from five professional runners, focusing on three key angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. We propose a post-processing module for outlier detection and fusion prediction in the joint angles.
  The experimental results demonstrate that using joint-based models yields root mean squared errors ranging from 11.41{\deg} to 4.37{\deg}. When integrated with the post-processing modules, these errors can be reduced to 6.99{\deg} and 3.88{\deg}, respectively. The experimental findings suggest that human pose tracking approaches can be valuable resources for the biomechanical analysis of running. However, there is still room for improvement in applications where high accuracy is required.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.04718</link>
<guid>https://arxiv.org/abs/2505.04718</guid>
<content:encoded><![CDATA[
arXiv:2505.04718v1 Announce Type: cross 
Abstract: We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay</title>
<link>https://arxiv.org/abs/2505.04787</link>
<guid>https://arxiv.org/abs/2505.04787</guid>
<content:encoded><![CDATA[
arXiv:2505.04787v1 Announce Type: cross 
Abstract: Continual Learning entails progressively acquiring knowledge from new data while retaining previously acquired knowledge, thereby mitigating ``Catastrophic Forgetting'' in neural networks. Our work presents a novel uncertainty-driven Unsupervised Continual Learning framework using Generative Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture efficiently uses unlabelled and synthetic labelled data in a balanced proportion using a cluster-level uncertainty-driven feedback mechanism and a VLM-powered generative replay module. Unlike traditional memory-buffer methods that depend on pretrained models and pseudo-labels, our R2R framework operates without any prior training. It leverages visual features from unlabeled data and adapts continuously using clustering-based uncertainty estimation coupled with dynamic thresholding. Concurrently, a generative replay mechanism along with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data representative of past experiences, resembling biological visual thinking that replays memory to remember and act in new, unseen tasks. Extensive experimental analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and TinyImageNet datasets. Our proposed R2R approach improves knowledge retention, achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%, 59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confabulation dynamics in a reservoir computer: Filling in the gaps with untrained attractors</title>
<link>https://arxiv.org/abs/2505.04792</link>
<guid>https://arxiv.org/abs/2505.04792</guid>
<content:encoded><![CDATA[
arXiv:2505.04792v1 Announce Type: cross 
Abstract: Artificial Intelligence has advanced significantly in recent years thanks to innovations in the design and training of artificial neural networks (ANNs). Despite these advancements, we still understand relatively little about how elementary forms of ANNs learn, fail to learn, and generate false information without the intent to deceive, a phenomenon known as `confabulation'. To provide some foundational insight, in this paper we analyse how confabulation occurs in reservoir computers (RCs): a dynamical system in the form of an ANN. RCs are particularly useful to study as they are known to confabulate in a well-defined way: when RCs are trained to reconstruct the dynamics of a given attractor, they sometimes construct an attractor that they were not trained to construct, a so-called `untrained attractor' (UA). This paper sheds light on the role played by UAs when reconstruction fails and their influence when modelling transitions between reconstructed attractors. Based on our results, we conclude that UAs are an intrinsic feature of learning systems whose state spaces are bounded, and that this means of confabulation may be present in systems beyond RCs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is there Value in Reinforcement Learning?</title>
<link>https://arxiv.org/abs/2505.04822</link>
<guid>https://arxiv.org/abs/2505.04822</guid>
<content:encoded><![CDATA[
arXiv:2505.04822v1 Announce Type: cross 
Abstract: Action-values play a central role in popular Reinforcement Learing (RL) models of behavior. Yet, the idea that action-values are explicitly represented has been extensively debated. Critics had therefore repeatedly suggested that policy-gradient (PG) models should be favored over value-based (VB) ones, as a potential solution for this dilemma. Here we argue that this solution is unsatisfying. This is because PG methods are not, in fact, "Value-free" -- while they do not rely on an explicit representation of Value for acting (stimulus-response mapping), they do require it for learning. Hence, switching to PG models is, per se, insufficient for eliminating Value from models of behavior. More broadly, the requirement for a representation of Value stems from the underlying assumptions regarding the optimization objective posed by the standard RL framework, not from the particular algorithm chosen to solve it. Previous studies mostly took these standard RL assumptions for granted, as part of their conceptualization or problem modeling, while debating the different methods used to optimize it (i.e., PG or VB). We propose that, instead, the focus of the debate should shift to critically evaluating the underlying modeling assumptions. Such evaluation is particularly important from an experimental perspective. Indeed, the very notion of Value must be reconsidered when standard assumptions (e.g., risk neutrality, full-observability, Markovian environment, exponential discounting) are relaxed, as is likely in natural settings. Finally, we use the Value debate as a case study to argue in favor of a more nuanced, algorithmic rather than statistical, view of what constitutes "a model" in cognitive sciences. Our analysis suggests that besides "parametric" statistical complexity, additional aspects such as computational complexity must also be taken into account when evaluating model complexity.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steerable Scene Generation with Post Training and Inference-Time Search</title>
<link>https://arxiv.org/abs/2505.04831</link>
<guid>https://arxiv.org/abs/2505.04831</guid>
<content:encoded><![CDATA[
arXiv:2505.04831v1 Announce Type: cross 
Abstract: Training robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals. We do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution. Our method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments. Website with videos, code, data, and model weights: https://steerable-scene-generation.github.io/
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Study of Generative Models for Early Detection of Failures in Medical Devices</title>
<link>https://arxiv.org/abs/2505.04845</link>
<guid>https://arxiv.org/abs/2505.04845</guid>
<content:encoded><![CDATA[
arXiv:2505.04845v1 Announce Type: cross 
Abstract: The medical device industry has significantly advanced by integrating sophisticated electronics like microchips and field-programmable gate arrays (FPGAs) to enhance the safety and usability of life-saving devices. These complex electro-mechanical systems, however, introduce challenging failure modes that are not easily detectable with conventional methods. Effective fault detection and mitigation become vital as reliance on such electronics grows. This paper explores three generative machine learning-based approaches for fault detection in medical devices, leveraging sensor data from surgical staplers,a class 2 medical device. Historically considered low-risk, these devices have recently been linked to an increasing number of injuries and fatalities. The study evaluates the performance and data requirements of these machine-learning approaches, highlighting their potential to enhance device safety.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
<link>https://arxiv.org/abs/2505.04846</link>
<guid>https://arxiv.org/abs/2505.04846</guid>
<content:encoded><![CDATA[
arXiv:2505.04846v1 Announce Type: cross 
Abstract: The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.04851</link>
<guid>https://arxiv.org/abs/2505.04851</guid>
<content:encoded><![CDATA[
arXiv:2505.04851v1 Announce Type: cross 
Abstract: Despite the fact that popular text-to-image generation models cope well with international and general cultural queries, they have a significant knowledge gap regarding individual cultures. This is due to the content of existing large training datasets collected on the Internet, which are predominantly based on Western European or American popular culture. Meanwhile, the lack of cultural adaptation of the model can lead to incorrect results, a decrease in the generation quality, and the spread of stereotypes and offensive content. In an effort to address this issue, we examine the concept of cultural code and recognize the critical importance of its understanding by modern image generation models, an issue that has not been sufficiently addressed in the research community to date. We propose the methodology for collecting and processing the data necessary to form a dataset based on the cultural code, in particular the Russian one. We explore how the collected data affects the quality of generations in the national domain and analyze the effectiveness of our approach using the Kandinsky 3.1 text-to-image model. Human evaluation results demonstrate an increase in the level of awareness of Russian culture in the model.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation</title>
<link>https://arxiv.org/abs/2505.04860</link>
<guid>https://arxiv.org/abs/2505.04860</guid>
<content:encoded><![CDATA[
arXiv:2505.04860v1 Announce Type: cross 
Abstract: Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 300 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our project website is at: https://dcodaaug.github.io/D-CODA/.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed solution reconstruction in elasticity and heat transfer using the explicit constraint force method</title>
<link>https://arxiv.org/abs/2505.04875</link>
<guid>https://arxiv.org/abs/2505.04875</guid>
<content:encoded><![CDATA[
arXiv:2505.04875v1 Announce Type: cross 
Abstract: One use case of ``physics-informed neural networks'' (PINNs) is solution reconstruction, which aims to estimate the full-field state of a physical system from sparse measurements. Parameterized governing equations of the system are used in tandem with the measurements to regularize the regression problem. However, in real-world solution reconstruction problems, the parameterized governing equation may be inconsistent with the physical phenomena that give rise to the measurement data. We show that due to assuming consistency between the true and parameterized physics, PINNs-based approaches may fail to satisfy three basic criteria of interpretability, robustness, and data consistency. As we argue, these criteria ensure that (i) the quality of the reconstruction can be assessed, (ii) the reconstruction does not depend strongly on the choice of physics loss, and (iii) that in certain situations, the physics parameters can be uniquely recovered. In the context of elasticity and heat transfer, we demonstrate how standard formulations of the physics loss and techniques for constraining the solution to respect the measurement data lead to different ``constraint forces" -- which we define as additional source terms arising from the constraints -- and that these constraint forces can significantly influence the reconstructed solution. To avoid the potentially substantial influence of the choice of physics loss and method of constraint enforcement on the reconstructed solution, we propose the ``explicit constraint force method'' (ECFM) to gain control of the source term introduced by the constraint. We then show that by satisfying the criteria of interpretability, robustness, and data consistency, this approach leads to more predictable and customizable reconstructions from noisy measurement data, even when the parameterization of the missing physics is inconsistent with the measured system.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought Reasoning and Quantum-Native Tokenization</title>
<link>https://arxiv.org/abs/2505.04880</link>
<guid>https://arxiv.org/abs/2505.04880</guid>
<content:encoded><![CDATA[
arXiv:2505.04880v1 Announce Type: cross 
Abstract: Quantum computing offers theoretical advantages over classical computing for specific tasks, yet the boundary of practical quantum advantage remains an open question. To investigate this boundary, it is crucial to understand whether, and how, classical machines can learn and simulate quantum algorithms. Recent progress in large language models (LLMs) has demonstrated strong reasoning abilities, prompting exploration into their potential for this challenge. In this work, we introduce GroverGPT-2, an LLM-based method for simulating Grover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-native tokenization. Building on its predecessor, GroverGPT-2 performs simulation directly from quantum circuit representations while producing logically structured and interpretable outputs. Our results show that GroverGPT-2 can learn and internalize quantum circuit logic through efficient processing of quantum-native tokens, providing direct evidence that classical models like LLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2 outputs interleave circuit data with natural language, embedding explicit reasoning into the simulation. This dual capability positions GroverGPT-2 as a prototype for advancing machine understanding of quantum algorithms and modeling quantum circuit logic. We also identify an empirical scaling law for GroverGPT-2 with increasing qubit numbers, suggesting a path toward scalable classical simulation. These findings open new directions for exploring the limits of classical simulatability, enhancing quantum education and research, and laying groundwork for future foundation models in quantum computing.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness Perceptions in Regression-based Predictive Models</title>
<link>https://arxiv.org/abs/2505.04886</link>
<guid>https://arxiv.org/abs/2505.04886</guid>
<content:encoded><![CDATA[
arXiv:2505.04886v1 Announce Type: cross 
Abstract: Regression-based predictive analytics used in modern kidney transplantation is known to inherit biases from training data. This leads to social discrimination and inefficient organ utilization, particularly in the context of a few social groups. Despite this concern, there is limited research on fairness in regression and its impact on organ utilization and placement. This paper introduces three novel divergence-based group fairness notions: (i) independence, (ii) separation, and (iii) sufficiency to assess the fairness of regression-based analytics tools. In addition, fairness preferences are investigated from crowd feedback, in order to identify a socially accepted group fairness criterion for evaluating these tools. A total of 85 participants were recruited from the Prolific crowdsourcing platform, and a Mixed-Logit discrete choice model was used to model fairness feedback and estimate social fairness preferences. The findings clearly depict a strong preference towards the separation and sufficiency fairness notions, and that the predictive analytics is deemed fair with respect to gender and race groups, but unfair in terms of age groups.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability</title>
<link>https://arxiv.org/abs/2505.04897</link>
<guid>https://arxiv.org/abs/2505.04897</guid>
<content:encoded><![CDATA[
arXiv:2505.04897v1 Announce Type: cross 
Abstract: Interactive imitation learning makes an agent's control policy robust by stepwise supervisions from an expert. The recent algorithms mostly employ expert-agent switching systems to reduce the expert's burden by limitedly selecting the supervision timing. However, the precise selection is difficult and such a switching causes abrupt changes in actions, damaging the dynamic stability. This paper therefore proposes a novel method, so-called CubeDAgger, which improves robustness while reducing dynamic stability violations by making three improvements to a baseline method, EnsembleDAgger. The first improvement adds a regularization to explicitly activate the threshold for deciding the supervision timing. The second transforms the expert-agent switching system to an optimal consensus system of multiple action candidates. Third, autoregressive colored noise to the actions is introduced to make the stochastic exploration consistent over time. These improvements are verified by simulations, showing that the learned policies are sufficiently robust while maintaining dynamic stability during interaction.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Analysis for Contrastive Representation Learning under Non-IID Settings</title>
<link>https://arxiv.org/abs/2505.04937</link>
<guid>https://arxiv.org/abs/2505.04937</guid>
<content:encoded><![CDATA[
arXiv:2505.04937v1 Announce Type: cross 
Abstract: Contrastive Representation Learning (CRL) has achieved impressive success in various domains in recent years. Nevertheless, the theoretical understanding of the generalization behavior of CRL is limited. Moreover, to the best of our knowledge, the current literature only analyzes generalization bounds under the assumption that the data tuples used for contrastive learning are independently and identically distributed. However, in practice, we are often limited to a fixed pool of reusable labeled data points, making it inevitable to recycle data across tuples to create sufficiently large datasets. Therefore, the tuple-wise independence condition imposed by previous works is invalidated. In this paper, we provide a generalization analysis for the CRL framework under non-$i.i.d.$ settings that adheres to practice more realistically. Drawing inspiration from the literature on U-statistics, we derive generalization bounds which indicate the required number of samples in each class scales as the logarithm of the covering number of the class of learnable feature representations associated to each class. Next, we apply our main results to derive excess risk bounds for common function classes such as linear maps and neural networks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models</title>
<link>https://arxiv.org/abs/2505.04946</link>
<guid>https://arxiv.org/abs/2505.04946</guid>
<content:encoded><![CDATA[
arXiv:2505.04946v1 Announce Type: cross 
Abstract: Thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. However, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. In this work, we introduce T2VTextBench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. Our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. We evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. These results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Linearized Models from Nonlinear Systems under Initialization Constraints with Finite Data</title>
<link>https://arxiv.org/abs/2505.04954</link>
<guid>https://arxiv.org/abs/2505.04954</guid>
<content:encoded><![CDATA[
arXiv:2505.04954v1 Announce Type: cross 
Abstract: The identification of a linear system model from data has wide applications in control theory. The existing work that provides finite sample guarantees for linear system identification typically uses data from a single long system trajectory under i.i.d. random inputs, and assumes that the underlying dynamics is truly linear. In contrast, we consider the problem of identifying a linearized model when the true underlying dynamics is nonlinear, given that there is a certain constraint on the region where one can initialize the experiments. We provide a multiple trajectories-based deterministic data acquisition algorithm followed by a regularized least squares algorithm, and provide a finite sample error bound on the learned linearized dynamics. Our error bound shows that one can consistently learn the linearized dynamics, and demonstrates a trade-off between the error due to nonlinearity and the error due to noise. We validate our results through numerical experiments, where we also show the potential insufficiency of linear system identification using a single trajectory with i.i.d. random inputs, when nonlinearity does exist.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Community and hyperedge inference in multiple hypergraphs</title>
<link>https://arxiv.org/abs/2505.04967</link>
<guid>https://arxiv.org/abs/2505.04967</guid>
<content:encoded><![CDATA[
arXiv:2505.04967v1 Announce Type: cross 
Abstract: Hypergraphs, capable of representing high-order interactions via hyperedges, have become a powerful tool for modeling real-world biological and social systems. Inherent relationships within these real-world systems, such as the encoding relationship between genes and their protein products, drive the establishment of interconnections between multiple hypergraphs. Here, we demonstrate how to utilize those interconnections between multiple hypergraphs to synthesize integrated information from multiple higher-order systems, thereby enhancing understanding of underlying structures. We propose a model based on the stochastic block model, which integrates information from multiple hypergraphs to reveal latent high-order structures. Real-world hyperedges exhibit preferential attachment, where certain nodes dominate hyperedge formation. To characterize this phenomenon, our model introduces hyperedge internal degree to quantify nodes' contributions to hyperedge formation. This model is capable of mining communities, predicting missing hyperedges of arbitrary sizes within hypergraphs, and inferring inter-hypergraph edges between hypergraphs. We apply our model to high-order datasets to evaluate its performance. Experimental results demonstrate strong performance of our model in community detection, hyperedge prediction, and inter-hypergraph edge prediction tasks. Moreover, we show that our model enables analysis of multiple hypergraphs of different types and supports the analysis of a single hypergraph in the absence of inter-hypergraph edges. Our work provides a practical and flexible tool for analyzing multiple hypergraphs, greatly advancing the understanding of the organization in real-world high-order systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments</title>
<link>https://arxiv.org/abs/2505.04972</link>
<guid>https://arxiv.org/abs/2505.04972</guid>
<content:encoded><![CDATA[
arXiv:2505.04972v1 Announce Type: cross 
Abstract: The miniaturisation of sensors and processors, the advancements in connected edge intelligence, and the exponential interest in Artificial Intelligence are boosting the affirmation of autonomous nano-size drones in the Internet of Robotic Things ecosystem. However, achieving safe autonomous navigation and high-level tasks such as exploration and surveillance with these tiny platforms is extremely challenging due to their limited resources. This work focuses on enabling the safe and autonomous flight of a pocket-size, 30-gram platform called Crazyflie 2.1 in a partially known environment. We propose a novel AI-aided, vision-based reactive planning method for obstacle avoidance under the ambit of Integrated Sensing, Computing and Communication paradigm. We deal with the constraints of the nano-drone by splitting the navigation task into two parts: a deep learning-based object detector runs on the edge (external hardware) while the planning algorithm is executed onboard. The results show the ability to command the drone at $\sim8$ frames-per-second and a model performance reaching a COCO mean-average-precision of $60.8$. Field experiments demonstrate the feasibility of the solution with the drone flying at a top speed of $1$ m/s while steering away from an obstacle placed in an unknown position and reaching the target destination. The outcome highlights the compatibility of the communication delay and the model performance with the requirements of the real-time navigation task. We provide a feasible alternative to a fully onboard implementation that can be extended to autonomous exploration with nano-drones.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction with Cellwise Outliers: A Detect-then-Impute Approach</title>
<link>https://arxiv.org/abs/2505.04986</link>
<guid>https://arxiv.org/abs/2505.04986</guid>
<content:encoded><![CDATA[
arXiv:2505.04986v1 Announce Type: cross 
Abstract: Conformal prediction is a powerful tool for constructing prediction intervals for black-box models, providing a finite sample coverage guarantee for exchangeable data. However, this exchangeability is compromised when some entries of the test feature are contaminated, such as in the case of cellwise outliers. To address this issue, this paper introduces a novel framework called detect-then-impute conformal prediction. This framework first employs an outlier detection procedure on the test feature and then utilizes an imputation method to fill in those cells identified as outliers. To quantify the uncertainty in the processed test feature, we adaptively apply the detection and imputation procedures to the calibration set, thereby constructing exchangeable features for the conformal prediction interval of the test label. We develop two practical algorithms, PDI-CP and JDI-CP, and provide a distribution-free coverage analysis under some commonly used detection and imputation procedures. Notably, JDI-CP achieves a finite sample $1-2\alpha$ coverage guarantee. Numerical experiments on both synthetic and real datasets demonstrate that our proposed algorithms exhibit robust coverage properties and comparable efficiency to the oracle baseline.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Statistic Learning with Synthetic Data from Pretrained Large Models</title>
<link>https://arxiv.org/abs/2505.04992</link>
<guid>https://arxiv.org/abs/2505.04992</guid>
<content:encoded><![CDATA[
arXiv:2505.04992v1 Announce Type: cross 
Abstract: The rapid advancement of generative models, such as Stable Diffusion, raises a key question: how can synthetic data from these models enhance predictive modeling? While they can generate vast amounts of datasets, only a subset meaningfully improves performance. We propose a novel end-to-end framework that generates and systematically filters synthetic data through domain-specific statistical methods, selectively integrating high-quality samples for effective augmentation. Our experiments demonstrate consistent improvements in predictive performance across various settings, highlighting the potential of our framework while underscoring the inherent limitations of generative models for data augmentation. Despite the ability to produce large volumes of synthetic data, the proportion that effectively improves model performance is limited.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations</title>
<link>https://arxiv.org/abs/2505.04999</link>
<guid>https://arxiv.org/abs/2505.04999</guid>
<content:encoded><![CDATA[
arXiv:2505.04999v1 Announce Type: cross 
Abstract: Learning robot policies using imitation learning requires collecting large amounts of costly action-labeled expert demonstrations, which fundamentally limits the scale of training data. A promising approach to address this bottleneck is to harness the abundance of unlabeled observations-e.g., from video demonstrations-to learn latent action labels in an unsupervised way. However, we find that existing methods struggle when applied to complex robot tasks requiring fine-grained motions. We design continuous latent action models (CLAM) which incorporate two key ingredients we find necessary for learning to solve complex continuous control tasks from unlabeled observation data: (a) using continuous latent action labels instead of discrete representations, and (b) jointly training an action decoder to ensure that the latent action space can be easily grounded to real actions with relatively few labeled examples. Importantly, the labeled examples can be collected from non-optimal play data, enabling CLAM to learn performant policies without access to any action-labeled expert data. We demonstrate on continuous control benchmarks in DMControl (locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot arm that CLAM significantly outperforms prior state-of-the-art methods, remarkably with a 2-3x improvement in task success rate compared to the best baseline. Videos and code can be found at clamrobot.github.io.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort</title>
<link>https://arxiv.org/abs/2505.05004</link>
<guid>https://arxiv.org/abs/2505.05004</guid>
<content:encoded><![CDATA[
arXiv:2505.05004v1 Announce Type: cross 
Abstract: Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar transitional vertebrae or enumeration anomalies. While some studies manually assess these anomalies and describe the ribs qualitatively, this study aims to automate thoracolumbar stump rib detection and analyze their morphology quantitatively. To this end, we train a high-resolution deep-learning model for rib segmentation and show significant improvements compared to existing models (Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative algorithm and piece-wise linear interpolation to assess the length of the ribs, showing a success rate of 98.2%. When analyzing morphological features, we show that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs -13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1, p-value < 0.01), and are oriented more downwards and sideways within the first centimeters in contrast to full-length ribs. We show that with partially visible ribs, these features can achieve an F1-score of 0.84 in differentiating stump ribs from regular ones. We publish the model weights and masks for public use.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness</title>
<link>https://arxiv.org/abs/2505.05026</link>
<guid>https://arxiv.org/abs/2505.05026</guid>
<content:encoded><![CDATA[
arXiv:2505.05026v1 Announce Type: cross 
Abstract: Evaluating user interface (UI) design effectiveness extends beyond aesthetics to influencing user behavior, a principle central to Design Persuasiveness. A/B testing is the predominant method for determining which UI variations drive higher user engagement, but it is costly and time-consuming. While recent Vision-Language Models (VLMs) can process automated UI analysis, current approaches focus on isolated design attributes rather than comparative persuasiveness-the key factor in optimizing user interactions. To address this, we introduce WiserUI-Bench, a benchmark designed for Pairwise UI Design Persuasiveness Assessment task, featuring 300 real-world UI image pairs labeled with A/B test results and expert rationales. Additionally, we propose G-FOCUS, a novel inference-time reasoning strategy that enhances VLM-based persuasiveness assessment by reducing position bias and improving evaluation accuracy. Experimental results show that G-FOCUS surpasses existing inference strategies in consistency and accuracy for pairwise UI evaluation. Through promoting VLM-driven evaluation of UI persuasiveness, our work offers an approach to complement A/B testing, propelling progress in scalable UI preference modeling and design optimization. Code and data will be released publicly.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Reinforcement Learning for the Floorplanning of Analog ICs with Beam Search</title>
<link>https://arxiv.org/abs/2505.05059</link>
<guid>https://arxiv.org/abs/2505.05059</guid>
<content:encoded><![CDATA[
arXiv:2505.05059v1 Announce Type: cross 
Abstract: The layout of analog ICs requires making complex trade-offs, while addressing device physics and variability of the circuits. This makes full automation with learning-based solutions hard to achieve. However, reinforcement learning (RL) has recently reached significant results, particularly in solving the floorplanning problem. This paper presents a hybrid method that combines RL with a beam (BS) strategy. The BS algorithm enhances the agent's inference process, allowing for the generation of flexible floorplans by accomodating various objective weightings, and addressing congestion without without the need for policy retraining or fine-tuning. Moreover, the RL agent's generalization ability stays intact, along with its efficient handling of circuit features and constraints. Experimental results show approx. 5-85% improvement in area, dead space and half-perimeter wire length compared to a standard RL application, along with higher rewards for the agent. Moreover, performance and efficiency align closely with those of existing state-of-the-art techniques.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning dynamically inspired invariant subspaces for Koopman and transfer operator approximation</title>
<link>https://arxiv.org/abs/2505.05085</link>
<guid>https://arxiv.org/abs/2505.05085</guid>
<content:encoded><![CDATA[
arXiv:2505.05085v1 Announce Type: cross 
Abstract: Transfer and Koopman operator methods offer a framework for representing complex, nonlinear dynamical systems via linear transformations, enabling for a deeper understanding of the underlying dynamics. The spectrum of these operators provide important insights into system predictability and emergent behaviour, although efficiently estimating them from data can be challenging. We tackle this issue through the lens of general operator and representational learning, in which we approximate these linear operators using efficient finite-dimensional representations. Specifically, we machine-learn orthonormal, locally supported basis functions that are dynamically tailored to the system. This learned basis provides a particularly accurate approximation of the operator's action as well as a nearly invariant finite-dimensional subspace. We illustrate our approach with examples that showcase the retrieval of spectral properties from the estimated operator, and emphasise the dynamically adaptive quality of the machine-learned basis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions</title>
<link>https://arxiv.org/abs/2505.05091</link>
<guid>https://arxiv.org/abs/2505.05091</guid>
<content:encoded><![CDATA[
arXiv:2505.05091v1 Announce Type: cross 
Abstract: Deep learning (DL) has surpassed human performance on standard benchmarks, driving its widespread adoption in computer vision tasks. One such task is disparity estimation, estimating the disparity between matching pixels in stereo image pairs, which is crucial for safety-critical applications like medical surgeries and autonomous navigation. However, DL-based disparity estimation methods are highly susceptible to distribution shifts and adversarial attacks, raising concerns about their reliability and generalization. Despite these concerns, a standardized benchmark for evaluating the robustness of disparity estimation methods remains absent, hindering progress in the field.
  To address this gap, we introduce DispBench, a comprehensive benchmarking tool for systematically assessing the reliability of disparity estimation methods. DispBench evaluates robustness against synthetic image corruptions such as adversarial attacks and out-of-distribution shifts caused by 2D Common Corruptions across multiple datasets and diverse corruption scenarios. We conduct the most extensive performance and robustness analysis of disparity estimation methods to date, uncovering key correlations between accuracy, reliability, and generalization. Open-source code for DispBench: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Text2Cypher with Schema Filtering</title>
<link>https://arxiv.org/abs/2505.05118</link>
<guid>https://arxiv.org/abs/2505.05118</guid>
<content:encoded><![CDATA[
arXiv:2505.05118v1 Announce Type: cross 
Abstract: Knowledge graphs represent complex data using nodes, relationships, and properties. Cypher, a powerful query language for graph databases, enables efficient modeling and querying. Recent advancements in large language models allow translation of natural language questions into Cypher queries - Text2Cypher. A common approach is incorporating database schema into prompts. However, complex schemas can introduce noise, increase hallucinations, and raise computational costs. Schema filtering addresses these challenges by including only relevant schema elements, improving query generation while reducing token costs. This work explores various schema filtering methods for Text2Cypher task and analyzes their impact on token length, performance, and cost. Results show that schema filtering effectively optimizes Text2Cypher, especially for smaller models. Consistent with prior research, we find that larger models benefit less from schema filtering due to their longer context capabilities. However, schema filtering remains valuable for both larger and smaller models in cost reduction.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Analysis of Deep PDE Solvers for Option Pricing</title>
<link>https://arxiv.org/abs/2505.05121</link>
<guid>https://arxiv.org/abs/2505.05121</guid>
<content:encoded><![CDATA[
arXiv:2505.05121v1 Announce Type: cross 
Abstract: Option pricing often requires solving partial differential equations (PDEs). Although deep learning-based PDE solvers have recently emerged as quick solutions to this problem, their empirical and quantitative accuracy remain not well understood, hindering their real-world applicability. In this research, our aim is to offer actionable insights into the utility of deep PDE solvers for practical option pricing implementation. Through comparative experiments in both the Black--Scholes and the Heston model, we assess the empirical performance of two neural network algorithms to solve PDEs: the Deep Galerkin Method and the Time Deep Gradient Flow method (TDGF). We determine their empirical convergence rates and training time as functions of (i) the number of sampling stages, (ii) the number of samples, (iii) the number of layers, and (iv) the number of nodes per layer. For the TDGF, we also consider the order of the discretization scheme and the number of time steps.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2Cypher: Data Pruning using Hard Example Selection</title>
<link>https://arxiv.org/abs/2505.05122</link>
<guid>https://arxiv.org/abs/2505.05122</guid>
<content:encoded><![CDATA[
arXiv:2505.05122v1 Announce Type: cross 
Abstract: Database query languages such as SQL for relational databases and Cypher for graph databases have been widely adopted. Recent advancements in large language models (LLMs) enable natural language interactions with databases through models like Text2SQL and Text2Cypher. Fine-tuning these models typically requires large, diverse datasets containing non-trivial examples. However, as dataset size increases, the cost of fine-tuning also rises. This makes smaller, high-quality datasets essential for reducing costs for the same or better performance. In this paper, we propose five hard-example selection techniques for pruning the Text2Cypher dataset, aiming to preserve or improve performance while reducing resource usage. Our results show that these hard-example selection approaches can halve training time and costs with minimal impact on performance, and demonstrates that hard-example selection provides a cost-effective solution.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Dimensional Factorization Limits in Discrete Diffusion Models through Quantum Joint Distribution Learning</title>
<link>https://arxiv.org/abs/2505.05151</link>
<guid>https://arxiv.org/abs/2505.05151</guid>
<content:encoded><![CDATA[
arXiv:2505.05151v1 Announce Type: cross 
Abstract: This study explores quantum-enhanced discrete diffusion models to overcome classical limitations in learning high-dimensional distributions. We rigorously prove that classical discrete diffusion models, which calculate per-dimension transition probabilities to avoid exponential computational cost, exhibit worst-case linear scaling of Kullback-Leibler (KL) divergence with data dimension. To address this, we propose a Quantum Discrete Denoising Diffusion Probabilistic Model (QD3PM), which enables joint probability learning through diffusion and denoising in exponentially large Hilbert spaces. By deriving posterior states through quantum Bayes' theorem, similar to the crucial role of posterior probabilities in classical diffusion models, and by learning the joint probability, we establish a solid theoretical foundation for quantum-enhanced diffusion models. For denoising, we design a quantum circuit using temporal information for parameter sharing and learnable classical-data-controlled rotations for encoding. Exploiting joint distribution learning, our approach enables single-step sampling from pure noise, eliminating iterative requirements of existing models. Simulations demonstrate the proposed model's superior accuracy in modeling complex distributions compared to factorization methods. Hence, this paper establishes a new theoretical paradigm in generative models by leveraging the quantum advantage in joint distribution learning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models</title>
<link>https://arxiv.org/abs/2505.05163</link>
<guid>https://arxiv.org/abs/2505.05163</guid>
<content:encoded><![CDATA[
arXiv:2505.05163v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) learn joint representations by mapping images and text into a shared latent space. However, recent research highlights that deterministic embeddings from standard VLMs often struggle to capture the uncertainties arising from the ambiguities in visual and textual descriptions and the multiple possible correspondences between images and texts. Existing approaches tackle this by learning probabilistic embeddings during VLM training, which demands large datasets and does not leverage the powerful representations already learned by large-scale VLMs like CLIP. In this paper, we propose GroVE, a post-hoc approach to obtaining probabilistic embeddings from frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model (GPLVM) to learn a shared low-dimensional latent space where image and text inputs are mapped to a unified representation, optimized through single-modal embedding reconstruction and cross-modal alignment objectives. Once trained, the Gaussian Process model generates uncertainty-aware probabilistic embeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty calibration across multiple downstream tasks, including cross-modal retrieval, visual question answering, and active learning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local linear Fr\'echet curve regression in manifolds</title>
<link>https://arxiv.org/abs/2505.05168</link>
<guid>https://arxiv.org/abs/2505.05168</guid>
<content:encoded><![CDATA[
arXiv:2505.05168v1 Announce Type: cross 
Abstract: Global Fr\'echet functional regression has been recently addressed from time correlated bivariate curve data evaluated in a manifold (see Torres et al. 2025). For this type of curve data sets, the present paper solves the problem of local linear approximation of the Fr\'echet conditional mean in an extrinsic and intrinsic way. The extrinsic local linear Fr\'echet functional regression predictor is obtained in the time varying tangent space by projection into an orthornormal basis of the ambient Hilbert space. The conditions assumed ensure the existence and uniqueness of this predictor, and its computation via exponential and logarithmic maps. A weighted Fr\'echet mean approach is adopted in the computation of an intrinsic local linear Fr\'echet functional regression predictor. The asymptotic optimality of this intrinsic local approximation is also proved. The performance of the empirical version of both, extrinsic and intrinsic functional predictors, and of a Nadaraya-Watson type Fr\'echet curve predictor is illustrated in the simulation study undertaken. The finite-sample size properties are also tested in a real-data application via cross-validation. Specifically, functional prediction of the magnetic vector field from the time-varying geocentric latitude and longitude of the satellite NASA's MAGSAT spacecraft is addressed.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARK: Memory Augmented Refinement of Knowledge</title>
<link>https://arxiv.org/abs/2505.05177</link>
<guid>https://arxiv.org/abs/2505.05177</guid>
<content:encoded><![CDATA[
arXiv:2505.05177v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) assist in specialized tasks but struggle to align with evolving domain knowledge without costly fine-tuning. Domain knowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid') and generally accepted principles (e.g., ethical standards); Refined Memory: Evolving insights shaped by business needs and real-world changes. However, a significant gap often exists between a domain expert's deep, nuanced understanding and the system's domain knowledge, which can hinder accurate information retrieval and application. Our Memory-Augmented Refinement of Knowledge (MARK) framework enables LLMs to continuously learn without retraining by leveraging structured refined memory, inspired by the Society of Mind. MARK operates through specialized agents, each serving a distinct role: Residual Refined Memory Agent: Stores and retrieves domain-specific insights to maintain context over time; User Question Refined Memory Agent: Captures user-provided facts, abbreviations, and terminology for better comprehension; LLM Response Refined Memory Agent: Extracts key elements from responses for refinement and personalization. These agents analyse stored refined memory, detect patterns, resolve contradictions, and improve response accuracy. Temporal factors like recency and frequency prioritize relevant information while discarding outdated insights. MARK enhances LLMs in multiple ways: Ground Truth Strategy: Reduces hallucinations by establishing a structured reference; Domain-Specific Adaptation: Essential for fields like healthcare, law, and manufacturing, where proprietary insights are absent from public datasets; Personalized AI Assistants: Improves virtual assistants by remembering user preferences, ensuring coherent responses over time.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting</title>
<link>https://arxiv.org/abs/2505.05183</link>
<guid>https://arxiv.org/abs/2505.05183</guid>
<content:encoded><![CDATA[
arXiv:2505.05183v1 Announce Type: cross 
Abstract: The safety of autonomous cars has come under scrutiny in recent years, especially after 16 documented incidents involving Teslas (with autopilot engaged) crashing into parked emergency vehicles (police cars, ambulances, and firetrucks). While previous studies have revealed that strong light sources often introduce flare artifacts in the captured image, which degrade the image quality, the impact of flare on object detection performance remains unclear. In this research, we unveil PaniCar, a digital phenomenon that causes an object detector's confidence score to fluctuate below detection thresholds when exposed to activated emergency vehicle lighting. This vulnerability poses a significant safety risk, and can cause autonomous vehicles to fail to detect objects near emergency vehicles. In addition, this vulnerability could be exploited by adversaries to compromise the security of advanced driving assistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3, "manufacturer C", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors (YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle lighting to understand the influence of various technical and environmental factors. We also evaluate four SOTA flare removal methods and show that their performance and latency are insufficient for real-time driving constraints. To mitigate this risk, we propose Caracetamol, a robust framework designed to enhance the resilience of object detectors against the effects of activated emergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster RCNN, Caracetamol improves the models' average confidence of car detection by 0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by 0.33. In addition, Caracetamol is capable of processing frames at a rate of between 30-50 FPS, enabling real-time ADAS car detection.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.05223</link>
<guid>https://arxiv.org/abs/2505.05223</guid>
<content:encoded><![CDATA[
arXiv:2505.05223v1 Announce Type: cross 
Abstract: Human drivers exhibit individual preferences regarding driving style. Adapting autonomous vehicles to these preferences is essential for user trust and satisfaction. However, existing end-to-end driving approaches often rely on predefined driving styles or require continuous user feedback for adaptation, limiting their ability to support dynamic, context-dependent preferences. We propose a novel approach using multi-objective reinforcement learning (MORL) with preference-driven optimization for end-to-end autonomous driving that enables runtime adaptation to driving style preferences. Preferences are encoded as continuous weight vectors to modulate behavior along interpretable style objectives$\unicode{x2013}$including efficiency, comfort, speed, and aggressiveness$\unicode{x2013}$without requiring policy retraining. Our single-policy agent integrates vision-based perception in complex mixed-traffic scenarios and is evaluated in diverse urban environments using the CARLA simulator. Experimental results demonstrate that the agent dynamically adapts its driving behavior according to changing preferences while maintaining performance in terms of collision avoidance and route completion.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming</title>
<link>https://arxiv.org/abs/2505.05261</link>
<guid>https://arxiv.org/abs/2505.05261</guid>
<content:encoded><![CDATA[
arXiv:2505.05261v1 Announce Type: cross 
Abstract: Two-stage stochastic programming (2SP) offers a basic framework for modelling decision-making under uncertainty, yet scalability remains a challenge due to the computational complexity of recourse function evaluation. Existing learning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP) employ neural networks (NNs) as recourse function surrogates but rely on computationally intensive mixed-integer programming (MIP) formulations. We propose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks (ICNNs) to exploit linear programming (LP) representability in convex 2SP problems. By architecturally enforcing convexity and enabling exact inference through LP, our approach eliminates the need for integer variables inherent to the conventional MIP-based formulation while retaining an exact embedding of the ICNN surrogate within the 2SP framework. This results in a more computationally efficient alternative that maintains solution quality. Comprehensive experiments reveal that ICNNs incur only marginally longer training times while achieving validation accuracy on par with their MIP-based counterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits considerably faster solution times than the MIP-based formulations while preserving solution quality, with these advantages becoming significantly more pronounced as problem scale increases. For the most challenging instances, the method achieves speedups of up to 100$\times$ and solution quality superior to MIP-based formulations.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Sample Test of Text Generation Similarity</title>
<link>https://arxiv.org/abs/2505.05269</link>
<guid>https://arxiv.org/abs/2505.05269</guid>
<content:encoded><![CDATA[
arXiv:2505.05269v1 Announce Type: cross 
Abstract: The surge in digitized text data requires reliable inferential methods on observed textual patterns. This article proposes a novel two-sample text test for comparing similarity between two groups of documents. The hypothesis is whether the probabilistic mapping generating the textual data is identical across two groups of documents. The proposed test aims to assess text similarity by comparing the entropy of the documents. Entropy is estimated using neural network-based language models. The test statistic is derived from an estimation-and-inference framework, where the entropy is first approximated using an estimation set, followed by inference on the remaining data set. We showed theoretically that under mild conditions, the test statistic asymptotically follows a normal distribution. A multiple data-splitting strategy is proposed to enhance test power, which combines p-values into a unified decision. Various simulation studies and a real data example demonstrated that the proposed two-sample text test maintains the nominal Type one error rate while offering greater power compared to existing methods. The proposed method provides a novel solution to assert differences in document classes, particularly in fields where large-scale textual information is crucial.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Connection Between Learning to Reject and Bhattacharyya Divergences</title>
<link>https://arxiv.org/abs/2505.05273</link>
<guid>https://arxiv.org/abs/2505.05273</guid>
<content:encoded><![CDATA[
arXiv:2505.05273v1 Announce Type: cross 
Abstract: Learning to reject provide a learning paradigm which allows for our models to abstain from making predictions. One way to learn the rejector is to learn an ideal marginal distribution (w.r.t. the input domain) - which characterizes a hypothetical best marginal distribution - and compares it to the true marginal distribution via a density ratio. In this paper, we consider learning a joint ideal distribution over both inputs and labels; and develop a link between rejection and thresholding different statistical divergences. We further find that when one considers a variant of the log-loss, the rejector obtained by considering the joint ideal distribution corresponds to the thresholding of the skewed Bhattacharyya divergence between class-probabilities. This is in contrast to the marginal case - that is equivalent to a typical characterization of optimal rejection, Chow's Rule - which corresponds to a thresholding of the Kullback-Leibler divergence. In general, we find that rejecting via a Bhattacharyya divergence is less aggressive than Chow's Rule.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation</title>
<link>https://arxiv.org/abs/2505.05287</link>
<guid>https://arxiv.org/abs/2505.05287</guid>
<content:encoded><![CDATA[
arXiv:2505.05287v1 Announce Type: cross 
Abstract: Humans naturally exhibit bilateral symmetry in their gross manipulation skills, effortlessly mirroring simple actions between left and right hands. Bimanual robots-which also feature bilateral symmetry-should similarly exploit this property to perform tasks with either hand. Unlike humans, who often favor a dominant hand for fine dexterous skills, robots should ideally execute ambidextrous manipulation with equal proficiency. To this end, we introduce SYMDEX (SYMmetric DEXterity), a reinforcement learning framework for ambidextrous bi-manipulation that leverages the robot's inherent bilateral symmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation tasks into per-hand subtasks and trains dedicated policies for each. By exploiting bilateral symmetry via equivariant neural networks, experience from one arm is inherently leveraged by the opposite arm. We then distill the subtask policies into a global ambidextrous policy that is independent of the hand-task assignment. We evaluate SYMDEX on six challenging simulated manipulation tasks and demonstrate successful real-world deployment on two of them. Our approach strongly outperforms baselines on complex task in which the left and right hands perform different roles. We further demonstrate SYMDEX's scalability by extending it to a four-arm manipulation setup, where our symmetry-aware policies enable effective multi-arm collaboration and coordination. Our results highlight how structural symmetry as inductive bias in policy learning enhances sample efficiency, robustness, and generalization across diverse dexterous manipulation tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operator-Level Quantum Acceleration of Non-Logconcave Sampling</title>
<link>https://arxiv.org/abs/2505.05301</link>
<guid>https://arxiv.org/abs/2505.05301</guid>
<content:encoded><![CDATA[
arXiv:2505.05301v1 Announce Type: cross 
Abstract: Sampling from probability distributions of the form $\sigma \propto e^{-\beta V}$, where $V$ is a continuous potential, is a fundamental task across physics, chemistry, biology, computer science, and statistics. However, when $V$ is non-convex, the resulting distribution becomes non-logconcave, and classical methods such as Langevin dynamics often exhibit poor performance. We introduce the first quantum algorithm that provably accelerates a broad class of continuous-time sampling dynamics. For Langevin dynamics, our method encodes the target Gibbs measure into the amplitudes of a quantum state, identified as the kernel of a block matrix derived from a factorization of the Witten Laplacian operator. This connection enables Gibbs sampling via singular value thresholding and yields the first provable quantum advantage with respect to the Poincar\'e constant in the non-logconcave setting. Building on this framework, we further develop the first quantum algorithm that accelerates replica exchange Langevin diffusion, a widely used method for sampling from complex, rugged energy landscapes.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Sleep Staging to Spindle Detection: Evaluating End-to-End Automated Sleep Analysis</title>
<link>https://arxiv.org/abs/2505.05371</link>
<guid>https://arxiv.org/abs/2505.05371</guid>
<content:encoded><![CDATA[
arXiv:2505.05371v1 Announce Type: cross 
Abstract: Automation of sleep analysis, including both macrostructural (sleep stages) and microstructural (e.g., sleep spindles) elements, promises to enable large-scale sleep studies and to reduce variance due to inter-rater incongruencies. While individual steps, such as sleep staging and spindle detection, have been studied separately, the feasibility of automating multi-step sleep analysis remains unclear. Here, we evaluate whether a fully automated analysis using state-of-the-art machine learning models for sleep staging (RobustSleepNet) and subsequent spindle detection (SUMOv2) can replicate findings from an expert-based study of bipolar disorder. The automated analysis qualitatively reproduced key findings from the expert-based study, including significant differences in fast spindle densities between bipolar patients and healthy controls, accomplishing in minutes what previously took months to complete manually. While the results of the automated analysis differed quantitatively from the expert-based study, possibly due to biases between expert raters or between raters and the models, the models individually performed at or above inter-rater agreement for both sleep staging and spindle detection. Our results demonstrate that fully automated approaches have the potential to facilitate large-scale sleep research. We are providing public access to the tools used in our automated analysis by sharing our code and introducing SomnoBot, a privacy-preserving sleep analysis platform.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.05375</link>
<guid>https://arxiv.org/abs/2505.05375</guid>
<content:encoded><![CDATA[
arXiv:2505.05375v1 Announce Type: cross 
Abstract: Recently, spiking neural networks (SNNs), deployed on neuromorphic chips, provide highly efficient solutions on edge devices in different scenarios. However, their ability to adapt to distribution shifts after deployment has become a crucial challenge. Online test-time adaptation (OTTA) offers a promising solution by enabling models to dynamically adjust to new data distributions without requiring source data or labeled target samples. Nevertheless, existing OTTA methods are largely designed for traditional artificial neural networks and are not well-suited for SNNs. To address this gap, we propose a low-power, neuromorphic chip-friendly online test-time adaptation framework, aiming to enhance model generalization under distribution shifts. The proposed approach is called Threshold Modulation (TM), which dynamically adjusts the firing threshold through neuronal dynamics-inspired normalization, being more compatible with neuromorphic hardware. Experimental results on benchmark datasets demonstrate the effectiveness of this method in improving the robustness of SNNs against distribution shifts while maintaining low computational cost. The proposed method offers a practical solution for online test-time adaptation of SNNs, providing inspiration for the design of future neuromorphic chips. The demo code is available at github.com/NneurotransmitterR/TM-OTTA-SNN.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods</title>
<link>https://arxiv.org/abs/2505.05396</link>
<guid>https://arxiv.org/abs/2505.05396</guid>
<content:encoded><![CDATA[
arXiv:2505.05396v1 Announce Type: cross 
Abstract: From the original abstract:
  This thesis initially aims to study the pain assessment process from a clinical-theoretical perspective while exploring and examining existing automatic approaches. Building on this foundation, the primary objective of this Ph.D. project is to develop innovative computational methods for automatic pain assessment that achieve high performance and are applicable in real clinical settings. A primary goal is to thoroughly investigate and assess significant factors, including demographic elements that impact pain perception, as recognized in pain research, through a computational standpoint. Within the limits of the available data in this research area, our goal was to design, develop, propose, and offer automatic pain assessment pipelines for unimodal and multimodal configurations that are applicable to the specific requirements of different scenarios. The studies published in this Ph.D. thesis showcased the effectiveness of the proposed methods, achieving state-of-the-art results. Additionally, they paved the way for exploring new approaches in artificial intelligence, foundation models, and generative artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representing spherical tensors with scalar-based machine-learning models</title>
<link>https://arxiv.org/abs/2505.05404</link>
<guid>https://arxiv.org/abs/2505.05404</guid>
<content:encoded><![CDATA[
arXiv:2505.05404v1 Announce Type: cross 
Abstract: Rotational symmetry plays a central role in physics, providing an elegant framework to describe how the properties of 3D objects -- from atoms to the macroscopic scale -- transform under the action of rigid rotations. Equivariant models of 3D point clouds are able to approximate structure-property relations in a way that is fully consistent with the structure of the rotation group, by combining intermediate representations that are themselves spherical tensors. The symmetry constraints however make this approach computationally demanding and cumbersome to implement, which motivates increasingly popular unconstrained architectures that learn approximate symmetries as part of the training process. In this work, we explore a third route to tackle this learning problem, where equivariant functions are expressed as the product of a scalar function of the point cloud coordinates and a small basis of tensors with the appropriate symmetry. We also propose approximations of the general expressions that, while lacking universal approximation properties, are fast, simple to implement, and accurate in practical settings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crosslingual Reasoning through Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.05408</link>
<guid>https://arxiv.org/abs/2505.05408</guid>
<content:encoded><![CDATA[
arXiv:2505.05408v1 Announce Type: cross 
Abstract: Reasoning capabilities of large language models are primarily studied for English, even when pretrained models are multilingual. In this work, we investigate to what extent English reasoning finetuning with long chain-of-thoughts (CoTs) can generalize across languages. First, we find that scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size. Second, we reveal that while English-centric RLM's CoTs are naturally predominantly English, they consistently follow a quote-and-think pattern to reason about quoted non-English inputs. Third, we discover an effective strategy to control the language of long CoT reasoning, and we observe that models reason better and more efficiently in high-resource languages. Finally, we observe poor out-of-domain reasoning generalization, in particular from STEM to cultural commonsense knowledge, even for English. Overall, we demonstrate the potentials, study the mechanisms and outline the limitations of crosslingual generalization of English reasoning test-time scaling. We conclude that practitioners should let English-centric RLMs reason in high-resource languages, while further work is needed to improve reasoning in low-resource languages and out-of-domain contexts.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Models Don't Always Say What They Think</title>
<link>https://arxiv.org/abs/2505.05410</link>
<guid>https://arxiv.org/abs/2505.05410</guid>
<content:encoded><![CDATA[
arXiv:2505.05410v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustly optimal dynamics for active matter reservoir computing</title>
<link>https://arxiv.org/abs/2505.05420</link>
<guid>https://arxiv.org/abs/2505.05420</guid>
<content:encoded><![CDATA[
arXiv:2505.05420v1 Announce Type: cross 
Abstract: We study the information processing abilities of active matter in the reservoir computing (RC) paradigm, using a model that is externally driven to infer the future state of a chaotic signal. The simulated system closely follows a previously reported model. We uncover an exceptional dynamical regime of agent dynamics that has been overlooked heretofore. It appears robustly optimal across varying physical parameters and inference tasks, thus providing valuable insights into computation and inference with physical systems more generally. The ability to form effective mechanisms for information processing are primarily determined by the system's own intrinsic relaxation abilities. These are identifiable when probing the system without a specific inference goal and manifest when testing minimalistic single-particle reservoirs. The regime that achieves optimal computation is situated just below the critical damping threshold, involving a microscopic dynamical relaxation with multiple stages. The optimal system is adaptable under chaotic external driving, due to a diversity in response mechanisms that emerge like rapid alternations between quasi-stationary and highly nonlinear dynamical states. Both coherent and incoherent dynamics contribute to their operation, partly at dissimilar scales of space and delay time. Correlations on agent dynamics can indicate the best-performing regimes and onsets of tight relationships between the responding system and the fluctuating driver. As this model of computation is interpretable in physical terms, it facilitates re-framing inquiries regarding learning and unconventional computing with a fresh rationale for many-body physics out of equilibrium.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComPO: Preference Alignment via Comparison Oracles</title>
<link>https://arxiv.org/abs/2505.05465</link>
<guid>https://arxiv.org/abs/2505.05465</guid>
<content:encoded><![CDATA[
arXiv:2505.05465v1 Announce Type: cross 
Abstract: Direct alignment methods are increasingly used for aligning large language models (LLMs) with human preferences. However, these methods suffer from the issues of verbosity and likelihood displacement, which can be driven by the noisy preference pairs that induce similar likelihood for preferred and dispreferred responses. The contributions of this paper are two-fold. First, we propose a new preference alignment method based on comparison oracles and provide the convergence guarantee for its basic scheme. Second, we improve our method using some heuristics and conduct the experiments to demonstrate the flexibility and compatibility of practical scheme in improving the performance of LLMs using noisy preference pairs. Evaluations are conducted across multiple base and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with benchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show the effectiveness of our method as an alternative to addressing the limitations of existing direct alignment methods. A highlight of our work is that we evidence the importance of designing specialized methods for preference pairs with distinct likelihood margin, which complements the recent findings in \citet{Razin-2025-Unintentional}.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facets of Disparate Impact: Evaluating Legally Consistent Bias in Machine Learning</title>
<link>https://arxiv.org/abs/2505.05471</link>
<guid>https://arxiv.org/abs/2505.05471</guid>
<content:encoded><![CDATA[
arXiv:2505.05471v1 Announce Type: cross 
Abstract: Leveraging current legal standards, we define bias through the lens of marginal benefits and objective testing with the novel metric "Objective Fairness Index". This index combines the contextual nuances of objective testing with metric stability, providing a legally consistent and reliable measure. Utilizing the Objective Fairness Index, we provide fresh insights into sensitive machine learning applications, such as COMPAS (recidivism prediction), highlighting the metric's practical and theoretical significance. The Objective Fairness Index allows one to differentiate between discriminatory tests and systemic disparities.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointBA: Towards Backdoor Attacks in 3D Point Cloud</title>
<link>https://arxiv.org/abs/2103.16074</link>
<guid>https://arxiv.org/abs/2103.16074</guid>
<content:encoded><![CDATA[
arXiv:2103.16074v4 Announce Type: replace 
Abstract: 3D deep learning has been increasingly more popular for a variety of tasks including many safety-critical applications. However, recently several works raise the security issues of 3D deep models. Although most of them consider adversarial attacks, we identify that backdoor attack is indeed a more serious threat to 3D deep learning systems but remains unexplored. We present the backdoor attacks in 3D point cloud with a unified framework that exploits the unique properties of 3D data and networks. In particular, we design two attack approaches on point cloud: the poison-label backdoor attack (PointPBA) and the clean-label backdoor attack (PointCBA). The first one is straightforward and effective in practice, while the latter is more sophisticated assuming there are certain data inspections. The attack algorithms are mainly motivated and developed by 1) the recent discovery of 3D adversarial samples suggesting the vulnerability of deep models under spatial transformation; 2) the proposed feature disentanglement technique that manipulates the feature of the data through optimization methods and its potential to embed a new task. Extensive experiments show the efficacy of the PointPBA with over 95% success rate across various 3D datasets and models, and the more stealthy PointCBA with around 50% success rate. Our proposed backdoor attack in 3D point cloud is expected to perform as a baseline for improving the robustness of 3D deep models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connecting NTK and NNGP: A Unified Theoretical Framework for Wide Neural Network Learning Dynamics</title>
<link>https://arxiv.org/abs/2309.04522</link>
<guid>https://arxiv.org/abs/2309.04522</guid>
<content:encoded><![CDATA[
arXiv:2309.04522v3 Announce Type: replace 
Abstract: Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial advances were achieved for wide networks, within two disparate theoretical frameworks: the Neural Tangent Kernel (NTK), which assumes linearized gradient descent dynamics, and the Bayesian Neural Network Gaussian Process (NNGP). We unify these two theories using gradient descent learning with an additional noise in an ensemble of wide deep networks. We construct an analytical theory for the network input-output function and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP kernels are derived. We identify two learning phases: a gradient-driven learning phase, dominated by loss minimization, in which the time scale is governed by the initialization variance. It is followed by a slow diffusive learning stage, where the parameters sample the solution space, with a time constant decided by the noise and the Bayesian prior variance. The two variance parameters strongly affect the performance in the two regimes, especially in sigmoidal neurons. In contrast to the exponential convergence of the mean predictor in the initial phase, the convergence to the equilibrium is more complex and may behave nonmonotonically. By characterizing the diffusive phase, our work sheds light on representational drift in the brain, explaining how neural activity changes continuously without degrading performance, either by ongoing gradient signals that synchronize the drifts of different synapses or by architectural biases that generate task-relevant information that is robust against the drift process. This work closes the gap between the NTK and NNGP theories, providing a comprehensive framework for the learning process of deep wide neural networks and for analyzing dynamics in biological circuits.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When the Universe is Too Big: Bounding Consideration Probabilities for Plackett-Luce Rankings</title>
<link>https://arxiv.org/abs/2401.11016</link>
<guid>https://arxiv.org/abs/2401.11016</guid>
<content:encoded><![CDATA[
arXiv:2401.11016v2 Announce Type: replace 
Abstract: The widely used Plackett-Luce ranking model assumes that individuals rank items by making repeated choices from a universe of items. But in many cases the universe is too big for people to plausibly consider all options. In the choice literature, this issue has been addressed by supposing that individuals first sample a small consideration set and then choose among the considered items. However, inferring unobserved consideration sets (or item consideration probabilities) in this "consider then choose" setting poses significant challenges, because even simple models of consideration with strong independence assumptions are not identifiable, even if item utilities are known. We apply the consider-then-choose framework to top-$k$ rankings, where we assume rankings are constructed according to a Plackett-Luce model after sampling a consideration set. While item consideration probabilities remain non-identified in this setting, we prove that we can infer bounds on the relative values of consideration probabilities. Additionally, given a condition on the expected consideration set size and known item utilities, we derive absolute upper and lower bounds on item consideration probabilities. We also provide algorithms to tighten those bounds on consideration probabilities by propagating inferred constraints. Thus, we show that we can learn useful information about consideration probabilities despite not being able to identify them precisely. We demonstrate our methods on a ranking dataset from a psychology experiment with two different ranking tasks (one with fixed consideration sets and one with unknown consideration sets). This combination of data allows us to estimate utilities and then learn about unknown consideration probabilities using our bounds.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Learning Complexity for Efficient Downstream Dataset Pruning</title>
<link>https://arxiv.org/abs/2402.05356</link>
<guid>https://arxiv.org/abs/2402.05356</guid>
<content:encoded><![CDATA[
arXiv:2402.05356v3 Announce Type: replace 
Abstract: The ever-increasing fine-tuning cost of large-scale pre-trained models gives rise to the importance of dataset pruning, which aims to reduce dataset size while maintaining task performance. However, existing dataset pruning methods require training on the entire dataset, which is impractical for large-scale pre-trained models. In this paper, we propose a straightforward, novel, and training-free hardness score named Distorting-based Learning Complexity (DLC), to identify informative images and instructions from the downstream dataset efficiently. Our method is motivated by the observation that easy samples learned faster can also be learned with fewer parameters. Specifically, we define the Learning Complexity to quantify sample hardness and utilize a lightweight weights masking process for fast estimation, instead of the costly SGD optimization. Based on DLC, we further design a flexible under-sampling with randomness (dubbed FlexRand), replacing the top-K strategy, to alleviate the severe subset distribution shift. Extensive experiments with downstream image and instructions dataset pruning benchmarks demonstrate the effectiveness and efficiency of the proposed approach. In the images pruning benchmark, DLC significantly reduces the pruning time by 35x while establishing state-of-the-art performance with FlexRand.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyCE: Dynamically Configurable Exiting for Deep Learning Compression and Real-time Scaling</title>
<link>https://arxiv.org/abs/2403.01695</link>
<guid>https://arxiv.org/abs/2403.01695</guid>
<content:encoded><![CDATA[
arXiv:2403.01695v3 Announce Type: replace 
Abstract: Conventional deep learning (DL) model compression and scaling methods focus on altering the model's components, impacting the results across all samples uniformly. However, since samples vary in difficulty, a dynamic model that adapts computation based on sample complexity offers a novel perspective for compression and scaling. Despite this potential, existing dynamic models are typically monolithic and model-specific, limiting their generalizability as broad compression and scaling methods. Additionally, most deployed DL systems are fixed, unable to adjust their scale once deployed and, therefore, cannot adapt to the varying real-time demands. This paper introduces DyCE, a dynamically configurable system that can adjust the performance-complexity trade-off of a DL model at runtime without requiring re-initialization or redeployment on inference hardware. DyCE achieves this by adding small exit networks to intermediate layers of the original model, allowing computation to terminate early if acceptable results are obtained. DyCE also decouples the design of an efficient dynamic model, facilitating easy adaptation to new base models and potential general use in compression and scaling. We also propose methods for generating optimized configurations and determining the types and positions of exit networks to achieve desired performance and complexity trade-offs. By enabling simple configuration switching, DyCE provides fine-grained performance tuning in real-time. We demonstrate the effectiveness of DyCE through image classification tasks using deep convolutional neural networks (CNNs). DyCE significantly reduces computational complexity by 23.5% for ResNet152 and 25.9% for ConvNextv2-tiny on ImageNet, with accuracy reductions of less than 0.5%.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Hyper-optimized Machine Learning Models for Predicting Efficiency Degradation in Organic Solar Cells</title>
<link>https://arxiv.org/abs/2404.00173</link>
<guid>https://arxiv.org/abs/2404.00173</guid>
<content:encoded><![CDATA[
arXiv:2404.00173v3 Announce Type: replace 
Abstract: This work presents a set of optimal machine learning (ML) models to represent the temporal degradation suffered by the power conversion efficiency (PCE) of polymeric organic solar cells (OSCs) with a multilayer structure ITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996 entries, which includes up to 7 variables regarding both the manufacturing process and environmental conditions for more than 180 days. Then, we relied on a software framework that brings together a conglomeration of automated ML protocols that execute sequentially against our database by simply command-line interface. This easily permits hyper-optimizing and randomizing seeds of the ML models through exhaustive benchmarking so that optimal models are obtained. The accuracy achieved reaches values of the coefficient determination (R2) widely exceeding 0.90, whereas the root mean squared error (RMSE), sum of squared error (SSE), and mean absolute error (MAE)>1% of the target value, the PCE. Additionally, we contribute with validated models able to screen the behavior of OSCs never seen in the database. In that case, R2~0.96-0.97 and RMSE~1%, thus confirming the reliability of the proposal to predict. For comparative purposes, classical Bayesian regression fitting based on non-linear mean squares (LMS) are also presented, which only perform sufficiently for univariate cases of single OSCs. Hence they fail to outperform the breadth of the capabilities shown by the ML models. Finally, thanks to the standardized results offered by the ML framework, we study the dependencies between the variables of the dataset and their implications for the optimal performance and stability of the OSCs. Reproducibility is ensured by a standardized report altogether with the dataset, which are publicly available at Github.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Theoretic Generalization Bounds for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2404.03176</link>
<guid>https://arxiv.org/abs/2404.03176</guid>
<content:encoded><![CDATA[
arXiv:2404.03176v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) exhibit an exceptional capacity for generalization in practical applications. This work aims to capture the effect and benefits of depth for supervised learning via information-theoretic generalization bounds. We first derive two hierarchical bounds on the generalization error in terms of the Kullback-Leibler (KL) divergence or the 1-Wasserstein distance between the train and test distributions of the network internal representations. The KL divergence bound shrinks as the layer index increases, while the Wasserstein bound implies the existence of a layer that serves as a generalization funnel, which attains a minimal 1-Wasserstein distance. Analytic expressions for both bounds are derived under the setting of binary Gaussian classification with linear DNNs. To quantify the contraction of the relevant information measures when moving deeper into the network, we analyze the strong data processing inequality (SDPI) coefficient between consecutive layers of three regularized DNN models: $\mathsf{Dropout}$, $\mathsf{DropConnect}$, and Gaussian noise injection. This enables refining our generalization bounds to capture the contraction as a function of the network architecture parameters. Specializing our results to DNNs with a finite parameter space and the Gibbs algorithm reveals that deeper yet narrower network architectures generalize better in those examples, although how broadly this statement applies remains a question.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Probabilistic Approach to Learning the Degree of Equivariance in Steerable CNNs</title>
<link>https://arxiv.org/abs/2406.03946</link>
<guid>https://arxiv.org/abs/2406.03946</guid>
<content:encoded><![CDATA[
arXiv:2406.03946v3 Announce Type: replace 
Abstract: Steerable convolutional neural networks (SCNNs) enhance task performance by modelling geometric symmetries through equivariance constraints on weights. Yet, unknown or varying symmetries can lead to overconstrained weights and decreased performance. To address this, this paper introduces a probabilistic method to learn the degree of equivariance in SCNNs. We parameterise the degree of equivariance as a likelihood distribution over the transformation group using Fourier coefficients, offering the option to model layer-wise and shared equivariance. These likelihood distributions are regularised to ensure an interpretable degree of equivariance across the network. Advantages include the applicability to many types of equivariant networks through the flexible framework of SCNNs and the ability to learn equivariance with respect to any subgroup of any compact group without requiring additional layers. Our experiments reveal competitive performance on datasets with mixed symmetries, with learnt likelihood distributions that are representative of the underlying degree of equivariance.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HORAE: A Domain-Agnostic Language for Automated Service Regulation</title>
<link>https://arxiv.org/abs/2406.06600</link>
<guid>https://arxiv.org/abs/2406.06600</guid>
<content:encoded><![CDATA[
arXiv:2406.06600v4 Announce Type: replace 
Abstract: Artificial intelligence is rapidly encroaching on the field of service regulation. However, existing AI-based regulation techniques are often tailored to specific application domains and thus are difficult to generalize in an automated manner. This paper presents Horae, a unified specification language for modeling (multimodal) regulation rules across a diverse set of domains. We showcase how Horae facilitates an intelligent service regulation pipeline by further exploiting a fine-tuned large language model named RuleGPT that automates the Horae modeling process, thereby yielding an end-to-end framework for fully automated intelligent service regulation. The feasibility and effectiveness of our framework are demonstrated over a benchmark of various real-world regulation domains. In particular, we show that our open-sourced, fine-tuned RuleGPT with 7B parameters suffices to outperform GPT-3.5 and perform on par with GPT-4o.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Aware Differentially Private Regression via Meta-Learning</title>
<link>https://arxiv.org/abs/2406.08569</link>
<guid>https://arxiv.org/abs/2406.08569</guid>
<content:encoded><![CDATA[
arXiv:2406.08569v2 Announce Type: replace 
Abstract: Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions. While Differential Privacy (DP) is the gold standard for protecting user privacy, standard DP mechanisms typically significantly impair performance. One approach to mitigating this issue is pre-training models on simulated data before DP learning on the private data. In this work we go a step further, using simulated data to train a meta-learning model that combines the Convolutional Conditional Neural Process (ConvCNP) with an improved functional DP mechanism of Hall et al. [2013] yielding the DPConvCNP. DPConvCNP learns from simulated data how to map private data to a DP predictive model in one forward pass, and then provides accurate, well-calibrated predictions. We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is much faster at test time and requires less tuning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retraining with Predicted Hard Labels Provably Increases Model Accuracy</title>
<link>https://arxiv.org/abs/2406.11206</link>
<guid>https://arxiv.org/abs/2406.11206</guid>
<content:encoded><![CDATA[
arXiv:2406.11206v3 Announce Type: replace 
Abstract: The performance of a model trained with noisy labels is often improved by simply \textit{retraining} the model with its \textit{own predicted hard labels} (i.e., 1/0 labels). Yet, a detailed theoretical characterization of this phenomenon is lacking. In this paper, we theoretically analyze retraining in a linearly separable binary classification setting with randomly corrupted labels given to us and prove that retraining can improve the population accuracy obtained by initially training with the given (noisy) labels. To the best of our knowledge, this is the first such theoretical result. Retraining finds application in improving training with local label differential privacy (DP) which involves training with noisy labels. We empirically show that retraining selectively on the samples for which the predicted label matches the given label significantly improves label DP training at no extra privacy cost; we call this consensus-based retraining. As an example, when training ResNet-18 on CIFAR-100 with $\epsilon=3$ label DP, we obtain more than 6% improvement in accuracy with consensus-based retraining.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Another Imputation Method: A Transformer-based Model for Missing Values in Tabular Datasets</title>
<link>https://arxiv.org/abs/2407.11540</link>
<guid>https://arxiv.org/abs/2407.11540</guid>
<content:encoded><![CDATA[
arXiv:2407.11540v2 Announce Type: replace 
Abstract: Handling missing values in tabular datasets presents a significant challenge in training and testing artificial intelligence models, an issue usually addressed using imputation techniques. Here we introduce "Not Another Imputation Method" (NAIM), a novel transformer-based model specifically designed to address this issue without the need for traditional imputation techniques. NAIM's ability to avoid the necessity of imputing missing values and to effectively learn from available data relies on two main techniques: the use of feature-specific embeddings to encode both categorical and numerical features also handling missing inputs; the modification of the masked self-attention mechanism to completely mask out the contributions of missing data. Additionally, a novel regularization technique is introduced to enhance the model's generalization capability from incomplete data. We extensively evaluated NAIM on 5 publicly available tabular datasets, demonstrating its superior performance over 6 state-of-the-art machine learning models and 5 deep learning models, each paired with 3 different imputation techniques when necessary. The results highlight the efficacy of NAIM in improving predictive performance and resilience in the presence of missing data. To facilitate further research and practical application in handling missing data without traditional imputation methods, we made the code for NAIM available at https://github.com/cosbidev/NAIM.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Certified Unlearning for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2408.00920</link>
<guid>https://arxiv.org/abs/2408.00920</guid>
<content:encoded><![CDATA[
arXiv:2408.00920v3 Announce Type: replace 
Abstract: In the field of machine unlearning, certified unlearning has been extensively studied in convex machine learning models due to its high efficiency and strong theoretical guarantees. However, its application to deep neural networks (DNNs), known for their highly nonconvex nature, still poses challenges. To bridge the gap between certified unlearning and DNNs, we propose several simple techniques to extend certified unlearning methods to nonconvex objectives. To reduce the time complexity, we develop an efficient computation method by inverse Hessian approximation without compromising certification guarantees. In addition, we extend our discussion of certification to nonconvergence training and sequential unlearning, considering that real-world users can send unlearning requests at different time points. Extensive experiments on three real-world datasets demonstrate the efficacy of our method and the advantages of certified unlearning in DNNs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Bandits for Unbounded Context Distributions</title>
<link>https://arxiv.org/abs/2408.09655</link>
<guid>https://arxiv.org/abs/2408.09655</guid>
<content:encoded><![CDATA[
arXiv:2408.09655v2 Announce Type: replace 
Abstract: Nonparametric contextual bandit is an important model of sequential decision making problems. Under $\alpha$-Tsybakov margin condition, existing research has established a regret bound of $\tilde{O}\left(T^{1-\frac{\alpha+1}{d+2}}\right)$ for bounded supports. However, the optimal regret with unbounded contexts has not been analyzed. The challenge of solving contextual bandit problems with unbounded support is to achieve both exploration-exploitation tradeoff and bias-variance tradeoff simultaneously. In this paper, we solve the nonparametric contextual bandit problem with unbounded contexts. We propose two nearest neighbor methods combined with UCB exploration. The first method uses a fixed $k$. Our analysis shows that this method achieves minimax optimal regret under a weak margin condition and relatively light-tailed context distributions. The second method uses adaptive $k$. By a proper data-driven selection of $k$, this method achieves an expected regret of $\tilde{O}\left(T^{1-\frac{(\alpha+1)\beta}{\alpha+(d+2)\beta}}+T^{1-\beta}\right)$, in which $\beta$ is a parameter describing the tail strength. This bound matches the minimax lower bound up to logarithm factors, indicating that the second method is approximately optimal.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HESSO: Towards Automatic Efficient and User Friendly Any Neural Network Training and Pruning</title>
<link>https://arxiv.org/abs/2409.09085</link>
<guid>https://arxiv.org/abs/2409.09085</guid>
<content:encoded><![CDATA[
arXiv:2409.09085v2 Announce Type: replace 
Abstract: Structured pruning is one of the most popular approaches to effectively compress the heavy deep neural networks (DNNs) into compact sub-networks while retaining performance. The existing methods suffer from multi-stage procedures along with significant engineering efforts and human expertise. The Only-Train-Once (OTO) series has been recently proposed to resolve the many pain points by streamlining the workflow by automatically conducting (i) search space generation, (ii) structured sparse optimization, and (iii) sub-network construction. However, the built-in sparse optimizers in the OTO series, i.e., the Half-Space Projected Gradient (HSPG) family, have limitations that require hyper-parameter tuning and the implicit controls of the sparsity exploration, consequently requires intervening by human expertise. To address such limitations, we propose a Hybrid Efficient Structured Sparse Optimizer (HESSO). HESSO could automatically and efficiently train a DNN to produce a high-performing subnetwork. Meanwhile, it is almost tuning-free and enjoys user-friendly integration for generic training applications. To address another common issue of irreversible performance collapse observed in pruning DNNs, we further propose a Corrective Redundant Identification Cycle (CRIC) for reliably identifying indispensable structures. We numerically demonstrate the efficacy of HESSO and its enhanced version HESSO-CRIC on a variety of applications ranging from computer vision to natural language processing, including large language model. The numerical results showcase that HESSO can achieve competitive even superior performance to varying state-of-the-arts and support most DNN architectures. Meanwhile, CRIC can effectively prevent the irreversible performance collapse and further enhance the performance of HESSO on certain applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Compare Hardware Designs for High-Level Synthesis</title>
<link>https://arxiv.org/abs/2409.13138</link>
<guid>https://arxiv.org/abs/2409.13138</guid>
<content:encoded><![CDATA[
arXiv:2409.13138v2 Announce Type: replace 
Abstract: High-level synthesis (HLS) is an automated design process that transforms high-level code into hardware designs, enabling the rapid development of hardware accelerators. HLS relies on pragmas, which are directives inserted into the source code to guide the synthesis process, and pragmas have various settings and values that significantly impact the resulting hardware design. State-of-the-art ML-based HLS methods, such as HARP, first train a deep learning model, typically based on graph neural networks (GNNs) applied to graph-based representations of the source code and pragmas. They then perform design space exploration (DSE) to explore the pragma design space, rank candidate designs using the model, and return the top designs. However, traditional DSE methods face challenges due to the highly nonlinear relationship between pragma settings and performance metrics, along with complex interactions between pragmas that affect performance in non-obvious ways.
  To address these challenges, we propose compareXplore, a novel approach that learns to compare hardware designs for effective HLS optimization. CompareXplore introduces a hybrid loss function that combines pairwise preference learning with pointwise performance prediction, enabling the model to capture both relative preferences and absolute performance. Moreover, we introduce a novel node difference attention module that focuses on the most informative differences between designs, enabling the model to identify critical pragmas impacting performance. CompareXplore adopts a two-stage DSE, where a pointwise prediction model is used for the initial design pruning, followed by a pairwise comparison stage for precise performance verification. In extensive experiments, compareXplore achieves significant improvements in ranking metrics and generates high-quality HLS results for the selected designs, outperforming the existing SOTA method.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing and Efficiently Accelerating Multimodal Generation Model Inference</title>
<link>https://arxiv.org/abs/2410.00215</link>
<guid>https://arxiv.org/abs/2410.00215</guid>
<content:encoded><![CDATA[
arXiv:2410.00215v2 Announce Type: replace 
Abstract: Generative artificial intelligence (AI) technology is revolutionizing the computing industry. Not only its applications have broadened to various sectors but also poses new system design and optimization opportunities. The technology is capable of understanding and responding in multiple modalities. However, the advanced capability currently comes with significant system resource demands. To sustainably scale generative AI capabilities to billions of users in the world, inference must be fast and efficient. This paper pinpoints key system design and optimization opportunities by characterizing a family of emerging multi-modal generation models on real systems. Auto-regressive token generation is a critical latency performance bottleneck, typically dominated by GPU idle time. In addition to memory-intensive attention across the generative AI models, linear operations constitute significant inference latency due to the feed forward networks in Transformer-based models. We demonstrate that state-of-the-art optimization levers, spanning from applications to system software and hardware, set a 3.88x better baseline.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-MORL: Multi-Objective Reinforcement Learning through Efficient Discovery of Pareto Front</title>
<link>https://arxiv.org/abs/2410.02236</link>
<guid>https://arxiv.org/abs/2410.02236</guid>
<content:encoded><![CDATA[
arXiv:2410.02236v2 Announce Type: replace 
Abstract: Multi-objective reinforcement learning (MORL) excels at handling rapidly changing preferences in tasks that involve multiple criteria, even for unseen preferences. However, previous dominating MORL methods typically generate a fixed policy set or preference-conditioned policy through multiple training iterations exclusively for sampled preference vectors, and cannot ensure the efficient discovery of the Pareto front. Furthermore, integrating preferences into the input of policy or value functions presents scalability challenges, in particular as the dimension of the state and preference space grow, which can complicate the learning process and hinder the algorithm's performance on more complex tasks. To address these issues, we propose a two-stage Pareto front discovery algorithm called Constrained MORL (C-MORL), which serves as a seamless bridge between constrained policy optimization and MORL. Concretely, a set of policies is trained in parallel in the initialization stage, with each optimized towards its individual preference over the multiple objectives. Then, to fill the remaining vacancies in the Pareto front, the constrained optimization steps are employed to maximize one objective while constraining the other objectives to exceed a predefined threshold. Empirically, compared to recent advancements in MORL methods, our algorithm achieves more consistent and superior performances in terms of hypervolume, expected utility, and sparsity on both discrete and continuous control tasks, especially with numerous objectives (up to nine objectives in our experiments).
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Ask GNN: Empowering Large Language Model for Graph In-Context Learning</title>
<link>https://arxiv.org/abs/2410.07074</link>
<guid>https://arxiv.org/abs/2410.07074</guid>
<content:encoded><![CDATA[
arXiv:2410.07074v3 Announce Type: replace 
Abstract: Textual Attributed Graphs (TAGs) are crucial for modeling complex real-world systems, yet leveraging large language models (LLMs) for TAGs presents unique challenges due to the gap between sequential text processing and graph-structured data. We introduce AskGNN, a novel approach that bridges this gap by leveraging In-Context Learning (ICL) to integrate graph data and task-specific information into LLMs. AskGNN employs a Graph Neural Network (GNN)-powered structure-enhanced retriever to select labeled nodes across graphs, incorporating complex graph structures and their supervision signals. Our learning-to-retrieve algorithm optimizes the retriever to select example nodes that maximize LLM performance on graph. Experiments across three tasks and seven LLMs demonstrate AskGNN's superior effectiveness in graph task performance, opening new avenues for applying LLMs to graph-structured data without extensive fine-tuning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularized Robustly Reliable Learners and Instance Targeted Attacks</title>
<link>https://arxiv.org/abs/2410.10572</link>
<guid>https://arxiv.org/abs/2410.10572</guid>
<content:encoded><![CDATA[
arXiv:2410.10572v4 Announce Type: replace 
Abstract: Instance-targeted data poisoning attacks, where an adversary corrupts a training set to induce errors on specific test points, have raised significant concerns. Balcan et al (2022) proposed an approach to addressing this challenge by defining a notion of robustly-reliable learners that provide per-instance guarantees of correctness under well-defined assumptions, even in the presence of data poisoning attacks. They then give a generic optimal (but computationally inefficient) robustly reliable learner as well as a computationally efficient algorithm for the case of linear separators over log-concave distributions.
  In this work, we address two challenges left open by Balcan et al (2022). The first is that the definition of robustly-reliable learners in Balcan et al (2022) becomes vacuous for highly-flexible hypothesis classes: if there are two classifiers h_0, h_1 \in H both with zero error on the training set such that h_0(x) \neq h_1(x), then a robustly-reliable learner must abstain on x. We address this problem by defining a modified notion of regularized robustly-reliable learners that allows for nontrivial statements in this case. The second is that the generic algorithm of Balcan et al (2022) requires re-running an ERM oracle (essentially, retraining the classifier) on each test point x, which is generally impractical even if ERM can be implemented efficiently. To tackle this problem, we show that at least in certain interesting cases we can design algorithms that can produce their outputs in time sublinear in training time, by using techniques from dynamic algorithm design.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CATCH: Channel-Aware multivariate Time Series Anomaly Detection via Frequency Patching</title>
<link>https://arxiv.org/abs/2410.12261</link>
<guid>https://arxiv.org/abs/2410.12261</guid>
<content:encoded><![CDATA[
arXiv:2410.12261v4 Announce Type: replace 
Abstract: Anomaly detection in multivariate time series is challenging as heterogeneous subsequence anomalies may occur. Reconstruction-based methods, which focus on learning normal patterns in the frequency domain to detect diverse abnormal subsequences, achieve promising results, while still falling short on capturing fine-grained frequency characteristics and channel correlations. To contend with the limitations, we introduce CATCH, a framework based on frequency patching. We propose to patchify the frequency domain into frequency bands, which enhances its ability to capture fine-grained frequency characteristics. To perceive appropriate channel correlations, we propose a Channel Fusion Module (CFM), which features a patch-wise mask generator and a masked-attention mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM is encouraged to iteratively discover appropriate patch-wise channel correlations, and to cluster relevant channels while isolating adverse effects from irrelevant channels. Extensive experiments on 10 real-world datasets and 12 synthetic datasets demonstrate that CATCH achieves state-of-the-art performance. We make our code and datasets available at https://github.com/decisionintelligence/CATCH.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning</title>
<link>https://arxiv.org/abs/2410.14464</link>
<guid>https://arxiv.org/abs/2410.14464</guid>
<content:encoded><![CDATA[
arXiv:2410.14464v2 Announce Type: replace 
Abstract: Electrocardiogram (ECG) interpretation requires specialized expertise, often involving synthesizing insights from ECG signals with complex clinical queries posed in natural language. The scarcity of labeled ECG data coupled with the diverse nature of clinical inquiries presents a significant challenge for developing robust and adaptable ECG diagnostic systems. This work introduces a novel multimodal meta-learning method for few-shot ECG question answering, addressing the challenge of limited labeled data while leveraging the rich knowledge encoded within large language models (LLMs). Our LLM-agnostic approach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA and Gemma) via a trainable fusion module, enabling the language model to reason about ECG data and generate clinically meaningful answers. Extensive experiments demonstrate superior generalization to unseen diagnostic tasks compared to supervised baselines, achieving notable performance even with limited ECG leads. For instance, in a 5-way 5-shot setting, our method using LLaMA-3.1-8B achieves an accuracy of 84.6%, 77.3%, and 69.6% on single verify, choose and query question types, respectively. These results highlight the potential of our method to enhance clinical ECG interpretation by combining signal processing with the nuanced language understanding capabilities of LLMs, particularly in data-constrained scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Convolution-based Unlearnable Datasets</title>
<link>https://arxiv.org/abs/2411.01742</link>
<guid>https://arxiv.org/abs/2411.01742</guid>
<content:encoded><![CDATA[
arXiv:2411.01742v2 Announce Type: replace 
Abstract: The construction of large datasets for deep learning has raised concerns regarding unauthorized use of online data, leading to increased interest in protecting data from third-parties who want to use it for training. The Convolution-based Unlearnable DAtaset (CUDA) method aims to make data unlearnable by applying class-wise blurs to every image in the dataset so that neural networks learn relations between blur kernels and labels, as opposed to informative features for classifying clean data. In this work, we evaluate whether CUDA data remains unlearnable after image sharpening and frequency filtering, finding that this combination of simple transforms improves the utility of CUDA data for training. In particular, we observe a substantial increase in test accuracy over adversarial training for models trained with CUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training models to high accuracy using unlearnable data, we underscore the need for ongoing refinement in data poisoning techniques to ensure data privacy. Our method opens new avenues for enhancing the robustness of unlearnable datasets by highlighting that simple methods such as sharpening and frequency filtering are capable of breaking convolution-based unlearnable datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-device Anomaly Detection in Conveyor Belt Operations</title>
<link>https://arxiv.org/abs/2411.10729</link>
<guid>https://arxiv.org/abs/2411.10729</guid>
<content:encoded><![CDATA[
arXiv:2411.10729v2 Announce Type: replace 
Abstract: Conveyor belts are crucial in mining operations by enabling the continuous and efficient movement of bulk materials over long distances, which directly impacts productivity. While detecting anomalies in specific conveyor belt components has been widely studied, identifying the root causes of these failures, such as changing production conditions and operator errors, remains critical. Continuous monitoring of mining conveyor belt work cycles is still at an early stage and requires robust solutions. Recently, an anomaly detection method for duty cycle operations of a mining conveyor belt has been proposed. Based on its limited performance and unevaluated long-term proper operation, this study proposes two novel methods for classifying normal and abnormal duty cycles. The proposed approaches are pattern recognition systems that make use of threshold-based duty-cycle detection mechanisms, manually extracted features, pattern-matching, and supervised tiny machine learning models. The explored low-computational models include decision tree, random forest, extra trees, extreme gradient boosting, Gaussian naive Bayes, and multi-layer perceptron. A comprehensive evaluation of the former and proposed approaches is carried out on two datasets. Both proposed methods outperform the former method, with the best-performing approach being dataset-dependent. The heuristic rule-based approach achieves the highest performance in the same dataset used for algorithm training, with 97.3% for normal cycles and 80.2% for abnormal cycles. The ML-based approach performs better on a dataset including the effects of machine aging, scoring 91.3% for normal cycles and 67.9% for abnormal cycles. Implemented on two low-power microcontrollers, the methods demonstrate efficient, real-time operation with energy consumption of 13.3 and 20.6 ${\mu}$J during inference. These results offer valuable insights for detecting ...
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Data Representation Learning for Non-parametric Two-sample Testing</title>
<link>https://arxiv.org/abs/2412.00613</link>
<guid>https://arxiv.org/abs/2412.00613</guid>
<content:encoded><![CDATA[
arXiv:2412.00613v2 Announce Type: replace 
Abstract: Learning effective data representations has been crucial in non-parametric two-sample testing. Common approaches will first split data into training and test sets and then learn data representations purely on the training set. However, recent theoretical studies have shown that, as long as the sample indexes are not used during the learning process, the whole data can be used to learn data representations, meanwhile ensuring control of Type-I errors. The above fact motivates us to use the test set (but without sample indexes) to facilitate the data representation learning in the testing. To this end, we propose a representation-learning two-sample testing (RL-TST) framework. RL-TST first performs purely self-supervised representation learning on the entire dataset to capture inherent representations (IRs) that reflect the underlying data manifold. A discriminative model is then trained on these IRs to learn discriminative representations (DRs), enabling the framework to leverage both the rich structural information from IRs and the discriminative power of DRs. Extensive experiments demonstrate that RL-TST outperforms representative approaches by simultaneously using data manifold information in the test set and enhancing test power via finding the DRs with the training set.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Attention is Not Always Beneficial: A Theoretical Analysis of Graph Attention Mechanisms via Contextual Stochastic Block Models</title>
<link>https://arxiv.org/abs/2412.15496</link>
<guid>https://arxiv.org/abs/2412.15496</guid>
<content:encoded><![CDATA[
arXiv:2412.15496v2 Announce Type: replace 
Abstract: Despite the growing popularity of graph attention mechanisms, their theoretical understanding remains limited. This paper aims to explore the conditions under which these mechanisms are effective in node classification tasks through the lens of Contextual Stochastic Block Models (CSBMs). Our theoretical analysis reveals that incorporating graph attention mechanisms is \emph{not universally beneficial}. Specifically, by appropriately defining \emph{structure noise} and \emph{feature noise} in graphs, we show that graph attention mechanisms can enhance classification performance when structure noise exceeds feature noise. Conversely, when feature noise predominates, simpler graph convolution operations are more effective. Furthermore, we examine the over-smoothing phenomenon and show that, in the high signal-to-noise ratio (SNR) regime, graph convolutional networks suffer from over-smoothing, whereas graph attention mechanisms can effectively resolve this issue. Building on these insights, we propose a novel multi-layer Graph Attention Network (GAT) architecture that significantly outperforms single-layer GATs in achieving \emph{perfect node classification} in CSBMs, relaxing the SNR requirement from $ \omega(\sqrt{\log n}) $ to $ \omega(\sqrt{\log n} / \sqrt[3]{n}) $. To our knowledge, this is the first study to delineate the conditions for perfect node classification using multi-layer GATs. Our theoretical contributions are corroborated by extensive experiments on both synthetic and real-world datasets, highlighting the practical implications of our findings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Worst-case Robustness of Large Language Models</title>
<link>https://arxiv.org/abs/2501.19040</link>
<guid>https://arxiv.org/abs/2501.19040</guid>
<content:encoded><![CDATA[
arXiv:2501.19040v2 Announce Type: replace 
Abstract: Recent studies have revealed the vulnerability of large language models to adversarial attacks, where adversaries craft specific input sequences to induce harmful, violent, private, or incorrect outputs. In this work, we study their worst-case robustness, i.e., whether an adversarial example exists that leads to such undesirable outputs. We upper bound the worst-case robustness using stronger white-box attacks, indicating that most current deterministic defenses achieve nearly 0\% worst-case robustness. We propose a general tight lower bound for randomized smoothing using fractional knapsack solvers or 0-1 knapsack solvers, and using them to bound the worst-case robustness of all stochastic defenses. Based on these solvers, we provide theoretical lower bounds for several previous empirical defenses. For example, we certify the robustness of a specific case, smoothing using a uniform kernel, against \textit{any possible attack} with an average $\ell_0$ perturbation of 2.02 or an average suffix length of 6.41.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Task Generalization via Memory Augmentation in Meta-Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.01521</link>
<guid>https://arxiv.org/abs/2502.01521</guid>
<content:encoded><![CDATA[
arXiv:2502.01521v2 Announce Type: replace 
Abstract: Agents trained via reinforcement learning (RL) often struggle to perform well on tasks that differ from those encountered during training. This limitation presents a challenge to the broader deployment of RL in diverse and dynamic task settings. In this work, we introduce memory augmentation, a memory-based RL approach to improve task generalization. Our approach leverages task-structured augmentations to simulate plausible out-of-distribution scenarios and incorporates memory mechanisms to enable context-aware policy adaptation. Trained on a predefined set of tasks, our policy demonstrates the ability to generalize to unseen tasks through memory augmentation without requiring additional interactions with the environment. Through extensive simulation experiments and real-world hardware evaluations on legged locomotion tasks, we demonstrate that our approach achieves zero-shot generalization to unseen tasks while maintaining robust in-distribution performance and high sample efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts</title>
<link>https://arxiv.org/abs/2502.14281</link>
<guid>https://arxiv.org/abs/2502.14281</guid>
<content:encoded><![CDATA[
arXiv:2502.14281v3 Announce Type: replace 
Abstract: Noise in data appears to be inevitable in most real-world machine learning applications and would cause severe overfitting problems. Not only can data features contain noise, but labels are also prone to be noisy due to human input. In this paper, rather than noisy label learning in multiclass classifications, we instead focus on the less explored area of noisy label learning for multilabel classifications. Specifically, we investigate the post-correction of predictions generated from classifiers learned with noisy labels. The reasons are two-fold. Firstly, this approach can directly work with the trained models to save computational resources. Secondly, it could be applied on top of other noisy label correction techniques to achieve further improvements. To handle this problem, we appeal to deep generative approaches that are possible for uncertainty estimation. Our model posits that label noise arises from a stochastic shift in the latent variable, providing a more robust and beneficial means for noisy learning. We develop both unsupervised and semi-supervised learning methods for our model. The extensive empirical study presents solid evidence to that our approach is able to consistently improve the independent models and performs better than a number of existing methods across various noisy label settings. Moreover, a comprehensive empirical analysis of the proposed method is carried out to validate its robustness, including sensitivity analysis and an ablation study, among other elements.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems</title>
<link>https://arxiv.org/abs/2502.18635</link>
<guid>https://arxiv.org/abs/2502.18635</guid>
<content:encoded><![CDATA[
arXiv:2502.18635v2 Announce Type: replace 
Abstract: While Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving Large Language Model (LLM) systems, it introduces a large number of choices, parameters and hyperparameters that must be made or tuned. This includes the LLM, embedding, and ranker models themselves, as well as hyperparameters governing individual RAG components. Yet, collectively optimizing the entire configuration in a RAG or LLM system remains under-explored - especially in multi-objective settings - due to intractably large solution spaces, noisy objective evaluations, and the high cost of evaluations. In this work, we introduce the first approach for multi-objective parameter optimization of cost, latency, safety and alignment over entire LLM and RAG systems. We find that Bayesian optimization methods significantly outperform baseline approaches, obtaining a superior Pareto front on two new RAG benchmark tasks. We conclude our work with important considerations for practitioners who are designing multi-objective RAG systems, highlighting nuances such as how optimal configurations may not generalize across tasks and objectives.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion Models</title>
<link>https://arxiv.org/abs/2503.01876</link>
<guid>https://arxiv.org/abs/2503.01876</guid>
<content:encoded><![CDATA[
arXiv:2503.01876v2 Announce Type: replace 
Abstract: Human-in-the-loop (HitL) robot deployment has gained significant attention in both academia and industry as a semi-autonomous paradigm that enables human operators to intervene and adjust robot behaviors at deployment time, improving success rates. However, continuous human monitoring and intervention can be highly labor-intensive and impractical when deploying a large number of robots. To address this limitation, we propose a method that allows diffusion policies to actively seek human assistance only when necessary, reducing reliance on constant human oversight. To achieve this, we leverage the generative process of diffusion policies to compute an uncertainty-based metric based on which the autonomous agent can decide to request operator assistance at deployment time, without requiring any operator interaction during training. Additionally, we show that the same method can be used for efficient data collection for fine-tuning diffusion policies in order to improve their autonomous performance. Experimental results from simulated and real-world environments demonstrate that our approach enhances policy performance during deployment for a variety of scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Dimension Aware Fractional-Order Stochastic Gradient Descent for Convex Optimization Problems</title>
<link>https://arxiv.org/abs/2503.13764</link>
<guid>https://arxiv.org/abs/2503.13764</guid>
<content:encoded><![CDATA[
arXiv:2503.13764v2 Announce Type: replace 
Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional exponents to capture long-memory effects in optimization. However, its utility is often limited by the difficulty of tuning and stabilizing these exponents. We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to adapt the fractional exponent in a data-driven manner. By tracking model sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the exponent to mitigate oscillations and hasten convergence. Theoretically, this approach preserves the advantages of fractional memory without the sluggish or unstable behavior observed in na\"ive fractional SGD. Empirical evaluations in Gaussian and $\alpha$-stable noise scenarios using an autoregressive (AR) model\textcolor{red}{, as well as on the MNIST and CIFAR-100 datasets for image classification,} highlight faster convergence and more robust parameter estimates compared to baseline methods, underscoring the potential of dimension-aware fractional techniques for advanced modeling and estimation tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Protein Representation Learning: Methods, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2503.16659</link>
<guid>https://arxiv.org/abs/2503.16659</guid>
<content:encoded><![CDATA[
arXiv:2503.16659v2 Announce Type: replace 
Abstract: Proteins are complex biomolecules that play a central role in various biological processes, making them critical targets for breakthroughs in molecular biology, medical research, and drug discovery. Deciphering their intricate, hierarchical structures, and diverse functions is essential for advancing our understanding of life at the molecular level. Protein Representation Learning (PRL) has emerged as a transformative approach, enabling the extraction of meaningful computational representations from protein data to address these challenges. In this paper, we provide a comprehensive review of PRL research, categorizing methodologies into five key areas: feature-based, sequence-based, structure-based, multimodal, and complex-based approaches. To support researchers in this rapidly evolving field, we introduce widely used databases for protein sequences, structures, and functions, which serve as essential resources for model development and evaluation. We also explore the diverse applications of these approaches in multiple domains, demonstrating their broad impact. Finally, we discuss pressing technical challenges and outline future directions to advance PRL, offering insights to inspire continued innovation in this foundational field.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Uncertain Reward Model</title>
<link>https://arxiv.org/abs/2503.22480</link>
<guid>https://arxiv.org/abs/2503.22480</guid>
<content:encoded><![CDATA[
arXiv:2503.22480v5 Announce Type: replace 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical technique for training large language models. However, reward hacking-a phenomenon where models exploit flaws in the reward model-remains a significant barrier to achieving robust and scalable intelligence through long-term training. Existing studies have proposed the uncertain reward models to address reward hacking, however, they often lack systematic or theoretical foundations, failing to model the uncertainty intrinsically emerging from preference data, and thus cannot sufficiently mitigate reward hacking to sustain prolonged RLHF training and exploration. In this paper, we propose a Probabilistic Uncertain Reward Model (PURM), a natural generalization of the classical Bradley-Terry reward model, that can directly learn the reward distribution emerged from the preference data. We theoretically derived PURM's loss function and the uncertainty of the reward distribution. To mitigate reward hacking with PURM, we further introduce an uncertainty-aware penalty into Proximal Policy Optimization (PPO), which leverages the learned uncertainty to dynamically balance reward optimization and exploration. Experimental results demonstrate that PURM significantly delays the onset of reward hacking while improving final performance compared with existing methods. We also find that PURM genuinely produce sound reward and uncertainty estimations. The data and code of this paper can be found at https://anonymous.4open.science/r/Probabilistic-Uncertain-Reward-Model/
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining</title>
<link>https://arxiv.org/abs/2504.02107</link>
<guid>https://arxiv.org/abs/2504.02107</guid>
<content:encoded><![CDATA[
arXiv:2504.02107v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) trained on historical web data inevitably become outdated. We investigate evaluation strategies and update methods for LLMs as new data becomes available. We introduce a web-scale dataset for time-continual pretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of magnitude larger than previous continual language modeling benchmarks. We also design time-stratified evaluations across both general CC data and specific domains (Wikipedia, StackExchange, and code documentation) to assess how well various continual learning methods adapt to new data while retaining past knowledge. Our findings demonstrate that, on general CC data, autoregressive meta-schedules combined with a fixed-ratio replay of older data can achieve comparable held-out loss to re-training from scratch, while requiring significantly less computation (2.6x). However, the optimal balance between incorporating new data and replaying old data differs as replay is crucial to avoid forgetting on generic web data but less so on specific domains.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest</title>
<link>https://arxiv.org/abs/2504.04243</link>
<guid>https://arxiv.org/abs/2504.04243</guid>
<content:encoded><![CDATA[
arXiv:2504.04243v2 Announce Type: replace 
Abstract: The design of AI systems to assist human decision-making typically requires the availability of labels to train and evaluate supervised models. Frequently, however, these labels are unknown, and different ways of estimating them involve unverifiable assumptions or arbitrary choices. In this work, we introduce the concept of label indeterminacy and derive important implications in high-stakes AI-assisted decision-making. We present an empirical study in a healthcare context, focusing specifically on predicting the recovery of comatose patients after resuscitation from cardiac arrest. Our study shows that label indeterminacy can result in models that perform similarly when evaluated on patients with known labels, but vary drastically in their predictions for patients where labels are unknown. After demonstrating crucial ethical implications of label indeterminacy in this high-stakes context, we discuss takeaways for evaluation, reporting, and design.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Type-Constrained Code Generation with Language Models</title>
<link>https://arxiv.org/abs/2504.09246</link>
<guid>https://arxiv.org/abs/2504.09246</guid>
<content:encoded><![CDATA[
arXiv:2504.09246v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic features of general-purpose programming languages. However, LLMs frequently generate code with typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. For this purpose, we develop novel prefix automata and a search over inhabitable types, forming a sound approach to enforce well-typedness on LLM-generated code. We formalize our approach on a foundational simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on the HumanEval and MBPP datasets shows that our approach reduces compilation errors by more than half and significantly increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including state-of-the-art open-weight models with more than 30B parameters. The results demonstrate the generality and effectiveness of our approach in constraining LLM code generation with formal rules of type systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions</title>
<link>https://arxiv.org/abs/2504.10146</link>
<guid>https://arxiv.org/abs/2504.10146</guid>
<content:encoded><![CDATA[
arXiv:2504.10146v2 Announce Type: replace 
Abstract: We propose GeoUni, the first unified geometry expert model capable of generating problem solutions and diagrams within a single framework in a way that enables the creation of unique and individualized geometry problems. Traditionally, solving geometry problems and generating diagrams have been treated as separate tasks in machine learning, with no models successfully integrating both to support problem creation. However, we believe that mastery in geometry requires frictionless integration of all of these skills, from solving problems to visualizing geometric relationships, and finally, crafting tailored problems. Our extensive experiments demonstrate that GeoUni, with only 1.5B parameters, achieves performance comparable to larger models such as DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also excels in generating precise geometric diagrams, surpassing both text-to-image models and unified models, including the GPT-4o image generation. Most importantly, GeoUni is the only model capable of successfully generating textual problems with matching diagrams based on specific knowledge points, thus offering a wider range of capabilities that extend beyond current models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Multivariate Financial Time Series Classification</title>
<link>https://arxiv.org/abs/2504.17664</link>
<guid>https://arxiv.org/abs/2504.17664</guid>
<content:encoded><![CDATA[
arXiv:2504.17664v2 Announce Type: replace 
Abstract: This article investigates the use of Machine Learning and Deep Learning models in multivariate time series analysis within financial markets. It compares small and big data approaches, focusing on their distinct challenges and the benefits of scaling. Traditional methods such as SVMs are contrasted with modern architectures like ConvTimeNet. The results show the importance of using and understanding Big Data in depth in the analysis and prediction of financial time series.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets</title>
<link>https://arxiv.org/abs/2504.19981</link>
<guid>https://arxiv.org/abs/2504.19981</guid>
<content:encoded><![CDATA[
arXiv:2504.19981v2 Announce Type: replace 
Abstract: Achieving both accuracy and diverse reasoning remains challenging for Large Language Models (LLMs) in complex domains like mathematics. A key bottleneck is evaluating intermediate reasoning steps to guide generation without costly human annotations. To address this, we first introduce a novel Process Reward Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a similarity-based data augmentation technique, effectively capturing step-level reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks (GFlowNets) to operate at the reasoning step level. Unlike traditional reinforcement learning focused on maximizing a single reward, GFlowNets naturally sample diverse, high-quality solutions proportional to their rewards, as measured by our PRM. Empirical evaluation shows strong improvements in both accuracy and solution diversity on challenging mathematical benchmarks (e.g., +2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work demonstrates the potential of PRM-guided, step-level GFlowNets for developing more robust and versatile mathematical reasoning in LLMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-objective optimisation via the R2 utilities</title>
<link>https://arxiv.org/abs/2305.11774</link>
<guid>https://arxiv.org/abs/2305.11774</guid>
<content:encoded><![CDATA[
arXiv:2305.11774v4 Announce Type: replace-cross 
Abstract: The goal of multi-objective optimisation is to identify a collection of points which describe the best possible trade-offs between the multiple objectives. In order to solve this vector-valued optimisation problem, practitioners often appeal to the use of scalarisation functions in order to transform the multi-objective problem into a collection of single-objective problems. This set of scalarised problems can then be solved using traditional single-objective optimisation techniques. In this work, we formalise this convention into a general mathematical framework. We show how this strategy effectively recasts the original multi-objective optimisation problem into a single-objective optimisation problem defined over sets. An appropriate class of objective functions for this new problem are the R2 utilities, which are utility functions that are defined as a weighted integral over the scalarised optimisation problems. As part of our work, we show that these utilities are monotone and submodular set functions which can be optimised effectively using greedy optimisation algorithms. We then analyse the performance of these greedy algorithms both theoretically and empirically. Our analysis largely focusses on Bayesian optimisation, which is a popular probabilistic framework for black-box optimisation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment</title>
<link>https://arxiv.org/abs/2307.02075</link>
<guid>https://arxiv.org/abs/2307.02075</guid>
<content:encoded><![CDATA[
arXiv:2307.02075v3 Announce Type: replace-cross 
Abstract: Entity alignment (EA) aims at identifying equivalent entity pairs across different knowledge graphs (KGs) that refer to the same real-world identity. To circumvent the shortage of seed alignments provided for training, recent EA models utilize pseudo-labeling strategies to iteratively add unaligned entity pairs predicted with high confidence to the seed alignments for model training. However, the adverse impact of confirmation bias during pseudo-labeling has been largely overlooked, thus hindering entity alignment performance. To systematically combat confirmation bias for pseudo-labeling-based entity alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment (UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the accuracy of entity alignment. UPL-EA consists of two complementary components: (1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as an effective means to determine entity correspondences and reduce erroneous matches across two KGs. An effective criterion is derived to infer pseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel pseudo-label ensembling refines pseudo-labeled alignments by combining predictions over multiple models independently trained in parallel. The ensembled pseudo-labeled alignments are thereafter used to augment seed alignments to reinforce subsequent model training for alignment inference. The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both theoretically supported and experimentally validated. Our extensive results and in-depth analyses demonstrate the superiority of UPL-EA over 15 competitive baselines and its utility as a general pseudo-labeling framework for entity alignment.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact nonlinear state estimation</title>
<link>https://arxiv.org/abs/2310.10976</link>
<guid>https://arxiv.org/abs/2310.10976</guid>
<content:encoded><![CDATA[
arXiv:2310.10976v2 Announce Type: replace-cross 
Abstract: The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and validated using idealized statistical experiments that feature bounded quantities with non-Gaussian distributions, a prevalent challenge in Earth system models. Results from these experiments indicate that the greatest benefits from ECTF occur when observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Ultimately, the new filtering theory offers exciting avenues for improving conventional DA algorithms through their principled integration with AI techniques.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Inadequacy of Similarity-based Privacy Metrics: Privacy Attacks against "Truly Anonymous" Synthetic Datasets</title>
<link>https://arxiv.org/abs/2312.05114</link>
<guid>https://arxiv.org/abs/2312.05114</guid>
<content:encoded><![CDATA[
arXiv:2312.05114v5 Announce Type: replace-cross 
Abstract: Generative models producing synthetic data are meant to provide a privacy-friendly approach to releasing data. However, their privacy guarantees are only considered robust when models satisfy Differential Privacy (DP). Alas, this is not a ubiquitous standard, as many leading companies (and, in fact, research papers) use ad-hoc privacy metrics based on testing the statistical similarity between synthetic and real data.
  In this paper, we examine the privacy metrics used in real-world synthetic data deployments and demonstrate their unreliability in several ways. First, we provide counter-examples where severe privacy violations occur even if the privacy tests pass and instantiate accurate membership and attribute inference attacks with minimal cost. We then introduce ReconSyn, a reconstruction attack that generates multiple synthetic datasets that are considered private by the metrics but actually leak information unique to individual records. We show that ReconSyn recovers 78-100% of the outliers in the train data with only black-box access to a single fitted generative model and the privacy metrics. In the process, we show that applying DP only to the model does not mitigate this attack, as using privacy metrics breaks the end-to-end DP pipeline.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Merton's Strategies via Policy Randomization</title>
<link>https://arxiv.org/abs/2312.11797</link>
<guid>https://arxiv.org/abs/2312.11797</guid>
<content:encoded><![CDATA[
arXiv:2312.11797v2 Announce Type: replace-cross 
Abstract: We study Merton's expected utility maximization problem in an incomplete market, characterized by a factor process in addition to the stock price process, where all the model primitives are unknown. The agent under consideration is a price taker who has access only to the stock and factor value processes and the instantaneous volatility. We propose an auxiliary problem in which the agent can invoke policy randomization according to a specific class of Gaussian distributions, and prove that the mean of its optimal Gaussian policy solves the original Merton problem. With randomized policies, we are in the realm of continuous-time reinforcement learning (RL) recently developed in Wang et al. (2020) and Jia and Zhou (2022a, 2022b, 2023), enabling us to solve the auxiliary problem in a data-driven way without having to estimate the model primitives. Specifically, we establish a policy improvement theorem based on which we design both online and offline actor-critic RL algorithms for learning Merton's strategies. A key insight from this study is that RL in general and policy randomization in particular are useful beyond the purpose for exploration -- they can be employed as a technical tool to solve a problem that cannot be otherwise solved by mere deterministic policies. At last, we carry out both simulation and empirical studies in a stochastic volatility environment to demonstrate the decisive outperformance of the devised RL algorithms in comparison to the conventional model-based, plug-in method.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a Comprehensive Survey</title>
<link>https://arxiv.org/abs/2403.16149</link>
<guid>https://arxiv.org/abs/2403.16149</guid>
<content:encoded><![CDATA[
arXiv:2403.16149v5 Announce Type: replace-cross 
Abstract: The Consumer Internet of Things (CIoT), a notable segment within the IoT domain, involves the integration of IoT technology into consumer electronics and devices, such as smart homes and smart wearables. Compared to traditional IoT fields, CIoT differs notably in target users, product types, and design approaches. While offering convenience to users, it also raises new security and privacy concerns. Network traffic analysis, a widely used technique in the security community, has been extensively applied to investigate these concerns about CIoT. Compared to traditional network traffic analysis in fields like mobile apps and websites, CIoT introduces unique characteristics that pose new challenges and research opportunities. Researchers have made significant contributions in this area. To aid researchers in understanding the application of traffic analysis tools for assessing CIoT security and privacy risks, this survey reviews 310 publications on traffic analysis within the CIoT security and privacy domain from January 2018 to June 2024, focusing on three research questions. Our work: 1) outlines the CIoT traffic analysis process and highlights its differences from general network traffic analysis. 2) summarizes and classifies existing research into four categories according to its application objectives: device fingerprinting, user activity inference, malicious traffic detection, and measurement. 3) explores emerging challenges and potential future research directions based on each step of the CIoT traffic analysis process. This will provide new insights to the community and guide the industry towards safer product designs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaveSleepNet: An Interpretable Network for Expert-like Sleep Staging</title>
<link>https://arxiv.org/abs/2404.15342</link>
<guid>https://arxiv.org/abs/2404.15342</guid>
<content:encoded><![CDATA[
arXiv:2404.15342v2 Announce Type: replace-cross 
Abstract: Although deep learning algorithms have proven their efficiency in automatic sleep staging, the widespread skepticism about their "black-box" nature has limited its clinical acceptance. In this study, we propose WaveSleepNet, an interpretable neural network for sleep staging that reasons in a similar way to sleep experts. In this network, we utilize the latent space representations generated during training to identify characteristic wave prototypes corresponding to different sleep stages. The feature representation of an input signal is segmented into patches within the latent space, each of which is compared against the learned wave prototypes. The proximity between these patches and the wave prototypes is quantified through scores, indicating the prototypes' presence and relative proportion within the signal. The scores are served as the decision-making criteria for final sleep staging. During training, an ensemble of loss functions is employed for the prototypes' diversity and robustness. Furthermore, the learned wave prototypes are visualized by analysing occlusion sensitivity. The efficacy of WaveSleepNet is validated across three public datasets, achieving sleep staging performance that are on par with the state-of-the-art models when several WaveSleepNets are combine into a larger network. A detailed case study examined the decision-making process of the WaveSleepNet which aligns closely with American Academy of Sleep Medicine (AASM) manual guidelines. Another case study systematically explained the misidentified reason behind each sleep stage. WaveSleepNet's transparent process provides specialists with direct access to the physiological significance of its criteria, allowing for future adaptation or enrichment by sleep experts.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Barren Plateaus in Variational Quantum Computing</title>
<link>https://arxiv.org/abs/2405.00781</link>
<guid>https://arxiv.org/abs/2405.00781</guid>
<content:encoded><![CDATA[
arXiv:2405.00781v2 Announce Type: replace-cross 
Abstract: Variational quantum computing offers a flexible computational paradigm with applications in diverse areas. However, a key obstacle to realizing their potential is the Barren Plateau (BP) phenomenon. When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases. Importantly, all the moving pieces of an algorithm -- choices of ansatz, initial state, observable, loss function and hardware noise -- can lead to BPs when ill-suited. Due to the significant impact of BPs on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects. As a result, the study of BPs has become a thriving area of research, influencing and cross-fertilizing other fields such as quantum optimal control, tensor networks, and learning theory. This article provides a comprehensive review of the current understanding of the BP phenomenon.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment</title>
<link>https://arxiv.org/abs/2405.12258</link>
<guid>https://arxiv.org/abs/2405.12258</guid>
<content:encoded><![CDATA[
arXiv:2405.12258v3 Announce Type: replace-cross 
Abstract: Large language models LLMs have transformed AI and achieved breakthrough performance on a wide range of tasks In science the most interesting application of LLMs is for hypothesis formation A feature of LLMs which results from their probabilistic structure is that the output text is not necessarily a valid inference from the training text These are termed hallucinations and are harmful in many applications In science some hallucinations may be useful novel hypotheses whose validity may be tested by laboratory experiments Here we experimentally test the application of LLMs as a source of scientific hypotheses using the domain of breast cancer treatment We applied the LLM GPT4 to hypothesize novel synergistic pairs of FDA-approved noncancer drugs that target the MCF7 breast cancer cell line relative to the nontumorigenic breast cell line MCF10A In the first round of laboratory experiments GPT4 succeeded in discovering three drug combinations out of twelve tested with synergy scores above the positive controls GPT4 then generated new combinations based on its initial results this generated three more combinations with positive synergy scores out of four tested We conclude that LLMs are a valuable source of scientific hypotheses.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rejection via Learning Density Ratios</title>
<link>https://arxiv.org/abs/2405.18686</link>
<guid>https://arxiv.org/abs/2405.18686</guid>
<content:encoded><![CDATA[
arXiv:2405.18686v2 Announce Type: replace-cross 
Abstract: Classification with rejection emerges as a learning paradigm which allows models to abstain from making predictions. The predominant approach is to alter the supervised learning pipeline by augmenting typical loss functions, letting model rejection incur a lower loss than an incorrect prediction. Instead, we propose a different distributional perspective, where we seek to find an idealized data distribution which maximizes a pretrained model's performance. This can be formalized via the optimization of a loss's risk with a $\varphi$-divergence regularization term. Through this idealized distribution, a rejection decision can be made by utilizing the density ratio between this distribution and the data distribution. We focus on the setting where our $\varphi$-divergences are specified by the family of $\alpha$-divergence. Our framework is tested empirically over clean and noisy datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Synthetic Data Creation with 1,000,000,000 Personas</title>
<link>https://arxiv.org/abs/2406.20094</link>
<guid>https://arxiv.org/abs/2406.20094</guid>
<content:encoded><![CDATA[
arXiv:2406.20094v3 Announce Type: replace-cross 
Abstract: We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas (~13% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous Graph Neural Network and Large Language Model</title>
<link>https://arxiv.org/abs/2408.16021</link>
<guid>https://arxiv.org/abs/2408.16021</guid>
<content:encoded><![CDATA[
arXiv:2408.16021v2 Announce Type: replace-cross 
Abstract: In the rapidly evolving field of cybersecurity, the integration of flow-level and packet-level information for real-time intrusion detection remains a largely untapped area of research. This paper introduces "XG-NID," a novel framework that, to the best of our knowledge, is the first to fuse flow-level and packet-level data within a heterogeneous graph structure, offering a comprehensive analysis of network traffic. Leveraging a heterogeneous graph neural network (GNN) with graph-level classification, XG-NID uniquely enables real-time inference while effectively capturing the intricate relationships between flow and packet payload data. Unlike traditional GNN-based methodologies that predominantly analyze historical data, XG-NID is designed to accommodate the heterogeneous nature of network traffic, providing a robust and real-time defense mechanism. Our framework extends beyond mere classification; it integrates Large Language Models (LLMs) to generate detailed, human-readable explanations and suggest potential remedial actions, ensuring that the insights produced are both actionable and comprehensible. Additionally, we introduce a new set of flow features based on temporal information, further enhancing the contextual and explainable inferences provided by our model. To facilitate practical application and accessibility, we developed "GNN4ID," an open-source tool that enables the extraction and transformation of raw network traffic into the proposed heterogeneous graph structure, seamlessly integrating flow and packet-level data. Our comprehensive quantitative comparative analysis demonstrates that XG-NID achieves an F1 score of 97\% in multi-class classification, outperforming existing baseline and state-of-the-art methods. This sets a new standard in Network Intrusion Detection Systems by combining innovative data fusion with enhanced interpretability and real-time capabilities.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2409.03757</link>
<guid>https://arxiv.org/abs/2409.03757</guid>
<content:encoded><![CDATA[
arXiv:2409.03757v3 Announce Type: replace-cross 
Abstract: Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks. Code: https://github.com/YunzeMan/Lexicon3D
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Theory of Generalization in Selectivity Learning</title>
<link>https://arxiv.org/abs/2409.07014</link>
<guid>https://arxiv.org/abs/2409.07014</guid>
<content:encoded><![CDATA[
arXiv:2409.07014v3 Announce Type: replace-cross 
Abstract: Query-driven machine learning models have emerged as a promising estimation technique for query selectivities. Yet, surprisingly little is known about the efficacy of these techniques from a theoretical perspective, as there exist substantial gaps between practical solutions and state-of-the-art (SOTA) theory based on the Probably Approximately Correct (PAC) learning framework. In this paper, we aim to bridge the gaps between theory and practice. First, we demonstrate that selectivity predictors induced by signed measures are learnable, which relaxes the reliance on probability measures in SOTA theory. More importantly, beyond the PAC learning framework (which only allows us to characterize how the model behaves when both training and test workloads are drawn from the same distribution), we establish, under mild assumptions, that selectivity predictors from this class exhibit favorable out-of-distribution (OOD) generalization error bounds.
  These theoretical advances provide us with a better understanding of both the in-distribution and OOD generalization capabilities of query-driven selectivity learning, and facilitate the design of two general strategies to improve OOD generalization for existing query-driven selectivity models. We empirically verify that our techniques help query-driven selectivity models generalize significantly better to OOD queries both in terms of prediction accuracy and query latency performance, while maintaining their superior in-distribution generalization performance.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of underdiagnosed medical conditions via opportunistic imaging</title>
<link>https://arxiv.org/abs/2409.11686</link>
<guid>https://arxiv.org/abs/2409.11686</guid>
<content:encoded><![CDATA[
arXiv:2409.11686v3 Announce Type: replace-cross 
Abstract: Abdominal computed tomography (CT) scans are frequently performed in clinical settings. Opportunistic CT involves repurposing routine CT images to extract diagnostic information and is an emerging tool for detecting underdiagnosed conditions such as sarcopenia, hepatic steatosis, and ascites. This study utilizes deep learning methods to promote accurate diagnosis and clinical documentation. We analyze 2,674 inpatient CT scans to identify discrepancies between imaging phenotypes (characteristics derived from opportunistic CT scans) and their corresponding documentation in radiology reports and ICD coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively) through either opportunistic imaging or radiology reports were ICD-coded. Our findings demonstrate opportunistic CT's potential to enhance diagnostic precision and accuracy of risk adjustment models, offering advancements in precision medicine.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning</title>
<link>https://arxiv.org/abs/2409.12183</link>
<guid>https://arxiv.org/abs/2409.12183</guid>
<content:encoded><![CDATA[
arXiv:2409.12183v3 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking and Mitigation of Vulnerabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2410.15236</link>
<guid>https://arxiv.org/abs/2410.15236</guid>
<content:encoded><![CDATA[
arXiv:2410.15236v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pairwise Markov Chains for Volatility Forecasting</title>
<link>https://arxiv.org/abs/2411.11838</link>
<guid>https://arxiv.org/abs/2411.11838</guid>
<content:encoded><![CDATA[
arXiv:2411.11838v2 Announce Type: replace-cross 
Abstract: The Pairwise Markov Chain (PMC) is a probabilistic graphical model extending the well-known Hidden Markov Model. This model, although highly effective for many tasks, has been scarcely utilized for continuous value prediction. This is mainly due to the issue of modeling observations inherent in generative probabilistic models. In this paper, we introduce a new algorithm for prediction with the PMC. On the one hand, this algorithm allows circumventing the feature problem, thus fully exploiting the capabilities of the PMC. On the other hand, it enables the PMC to extend any predictive model by introducing hidden states, updated at each time step, and allowing the introduction of non-stationarity for any model. We apply the PMC with its new algorithm for volatility forecasting, which we compare to the highly popular GARCH(1,1) and feedforward neural models across numerous pairs. This is particularly relevant given the regime changes that we can observe in volatility. For each scenario, our algorithm enhances the performance of the extended model, demonstrating the value of our approach.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Operators for Predictor Feedback Control of Nonlinear Delay Systems</title>
<link>https://arxiv.org/abs/2411.18964</link>
<guid>https://arxiv.org/abs/2411.18964</guid>
<content:encoded><![CDATA[
arXiv:2411.18964v2 Announce Type: replace-cross 
Abstract: Predictor feedback designs are critical for delay-compensating controllers in nonlinear systems. However, these designs are limited in practical applications as predictors cannot be directly implemented, but require numerical approximation schemes, which become computationally prohibitive when system dynamics are expensive to compute. To address this challenge, we recast the predictor design as an operator learning problem, and learn the predictor mapping via a neural operator. We prove the existence of an arbitrarily accurate neural operator approximation of the predictor operator. Under the approximated predictor, we achieve semiglobal practical stability of the closed-loop nonlinear delay system. The estimate is semiglobal in a unique sense - one can enlarge the set of initial states as desired, though this increases the difficulty of training a neural operator, which appears practically in the stability estimate. Furthermore, our analysis holds for any black-box predictor satisfying the universal approximation error bound. We demonstrate the approach by controlling a 5-link robotic manipulator with different neural operator models, achieving significant speedups compared to classic predictor feedback schemes while maintaining closed-loop stability.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active learning of neural population dynamics using two-photon holographic optogenetics</title>
<link>https://arxiv.org/abs/2412.02529</link>
<guid>https://arxiv.org/abs/2412.02529</guid>
<content:encoded><![CDATA[
arXiv:2412.02529v4 Announce Type: replace-cross 
Abstract: Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guaranteed Recovery of Unambiguous Clusters</title>
<link>https://arxiv.org/abs/2501.13093</link>
<guid>https://arxiv.org/abs/2501.13093</guid>
<content:encoded><![CDATA[
arXiv:2501.13093v3 Announce Type: replace-cross 
Abstract: Clustering is often a challenging problem because of the inherent ambiguity in what the "correct" clustering should be. Even when the number of clusters $K$ is known, this ambiguity often still exists, particularly when there is variation in density among different clusters, and clusters have multiple relatively separated regions of high density. In this paper we propose an information-theoretic characterization of when a $K$-clustering is ambiguous, and design an algorithm that recovers the clustering whenever it is unambiguous. This characterization formalizes the situation when two high density regions within a cluster are separable enough that they look more like two distinct clusters than two truly distinct clusters in the $K$-clustering. The algorithm first identifies $K$ partial clusters (or "seeds") using a density-based approach, and then adds unclustered points to the initial $K$ partial clusters in a greedy manner to form a complete clustering. We implement and test a version of the algorithm that is modified to effectively handle overlapping clusters, and observe that it requires little parameter selection and displays improved performance on many datasets compared to widely used algorithms for non-convex cluster recovery.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communicating Activations Between Language Model Agents</title>
<link>https://arxiv.org/abs/2501.14082</link>
<guid>https://arxiv.org/abs/2501.14082</guid>
<content:encoded><![CDATA[
arXiv:2501.14082v2 Announce Type: replace-cross 
Abstract: Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\textit{B}$'s computation at an intermediate layer, combine its current activation with another LM $\textit{A}$'s intermediate activation via some function $\textit{f}$, then pass $\textit{f}$'s output into the next layer of $\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\%$ improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative "language" for communication between LMs.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Conic Proxy for Semidefinite Relaxation of AC Optimal Power Flow</title>
<link>https://arxiv.org/abs/2502.06978</link>
<guid>https://arxiv.org/abs/2502.06978</guid>
<content:encoded><![CDATA[
arXiv:2502.06978v2 Announce Type: replace-cross 
Abstract: The nonlinear, non-convex AC Optimal Power Flow (AC-OPF) problem is fundamental for power systems operations. The intrinsic complexity of AC-OPF has fueled a growing interest in the development of optimization proxies for the problem, i.e., machine learning models that predict high-quality, close-to-optimal solutions. More recently, dual conic proxy architectures have been proposed, which combine machine learning and convex relaxations of AC-OPF, to provide valid certificates of optimality using learning-based methods. Building on this methodology, this paper proposes, for the first time, a dual conic proxy architecture for the semidefinite (SDP) relaxation of AC-OPF problems. Although the SDP relaxation is stronger than the second-order cone relaxation considered in previous work, its practical use has been hindered by its computational cost. The proposed method combines a neural network with a differentiable dual completion strategy that leverages the structure of the dual SDP problem. This approach guarantees dual feasibility, and therefore valid dual bounds, while providing orders of magnitude of speedups compared to interior-point algorithms. The paper also leverages self-supervised learning, which alleviates the need for time-consuming data generation and allows to train the proposed models efficiently. Numerical experiments are presented on several power grid benchmarks with up to 500 buses. The results demonstrate that the proposed SDP-based proxies can outperform weaker conic relaxations, while providing several orders of magnitude speedups compared to a state-of-the-art interior-point SDP solver.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with Saliency Maps</title>
<link>https://arxiv.org/abs/2502.08821</link>
<guid>https://arxiv.org/abs/2502.08821</guid>
<content:encoded><![CDATA[
arXiv:2502.08821v2 Announce Type: replace-cross 
Abstract: The recent surge in advanced generative models, such as diffusion models and generative adversarial networks (GANs), has led to an alarming rise in AI-generated images across various domains on the web. While such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. Additionally, the uncredited use of AI-generated images in media and marketing has sparked significant backlash from online communities. In response to this, we introduce DejAIvu, a Chrome Web extension that combines real-time AI-generated image detection with saliency-based explainability while users browse the web. Using an ONNX-optimized deep learning model, DejAIvu automatically analyzes images on websites such as Google Images, identifies AI-generated content using model inference, and overlays a saliency heatmap to highlight AI-related artifacts. Our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that AI detection is both transparent and interpretable. We also evaluate DejAIvu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing AI image accountability. The code for this system can be found at https://github.com/Noodulz/dejAIvu.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning</title>
<link>https://arxiv.org/abs/2502.12674</link>
<guid>https://arxiv.org/abs/2502.12674</guid>
<content:encoded><![CDATA[
arXiv:2502.12674v2 Announce Type: replace-cross 
Abstract: Despite recent advances in learning-based controllers for legged robots, deployments in human-centric environments remain limited by safety concerns. Most of these approaches use position-based control, where policies output target joint angles that must be processed by a low-level controller (e.g., PD or impedance controllers) to compute joint torques. Although impressive results have been achieved in controlled real-world scenarios, these methods often struggle with compliance and adaptability when encountering environments or disturbances unseen during training, potentially resulting in extreme or unsafe behaviors. Inspired by how animals achieve smooth and adaptive movements by controlling muscle extension and contraction, torque-based policies offer a promising alternative by enabling precise and direct control of the actuators in torque space. In principle, this approach facilitates more effective interactions with the environment, resulting in safer and more adaptable behaviors. However, challenges such as a highly nonlinear state space and inefficient exploration during training have hindered their broader adoption. To address these limitations, we propose SATA, a bio-inspired framework that mimics key biomechanical principles and adaptive learning mechanisms observed in animal locomotion. Our approach effectively addresses the inherent challenges of learning torque-based policies by significantly improving early-stage exploration, leading to high-performance final policies. Remarkably, our method achieves zero-shot sim-to-real transfer. Our experimental results indicate that SATA demonstrates remarkable compliance and safety, even in challenging environments such as soft/slippery terrain or narrow passages, and under significant external disturbances, highlighting its potential for practical deployments in human-centric and safety-critical scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TLOB: A Novel Transformer Model with Dual Attention for Price Trend Prediction with Limit Order Book Data</title>
<link>https://arxiv.org/abs/2502.15757</link>
<guid>https://arxiv.org/abs/2502.15757</guid>
<content:encoded><![CDATA[
arXiv:2502.15757v3 Announce Type: replace-cross 
Abstract: Price Trend Prediction (PTP) based on Limit Order Book (LOB) data is a fundamental challenge in financial markets. Despite advances in deep learning, existing models fail to generalize across different market conditions and assets. Surprisingly, by adapting a simple MLP-based architecture to LOB, we show that we surpass SoTA performance; thus, challenging the necessity of complex architectures. Unlike past work that shows robustness issues, we propose TLOB, a transformer-based model that uses a dual attention mechanism to capture spatial and temporal dependencies in LOB data. This allows it to adaptively focus on the market microstructure, making it particularly effective for longer-horizon predictions and volatile market conditions. We also introduce a new labeling method that improves on previous ones, removing the horizon bias. We evaluate TLOB's effectiveness across four horizons, using the established FI-2010 benchmark, a NASDAQ and a Bitcoin dataset. TLOB outperforms SoTA methods in every dataset and horizon. Additionally, we empirically show how stock price predictability has declined over time, -6.68 in F1-score, highlighting the growing market efficiency. Predictability must be considered in relation to transaction costs, so we experimented with defining trends using an average spread, reflecting the primary transaction cost. The resulting performance deterioration underscores the complexity of translating trend classification into profitable trading strategies. We argue that our work provides new insights into the evolving landscape of stock price trend prediction and sets a strong foundation for future advancements in financial AI. We release the code at https://github.com/LeonardoBerti00/TLOB.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-evaluating Open-ended Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2502.20170</link>
<guid>https://arxiv.org/abs/2502.20170</guid>
<content:encoded><![CDATA[
arXiv:2502.20170v2 Announce Type: replace-cross 
Abstract: Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular solution. Despite their many advantages, we show that the current Elo-based rating systems can be susceptible to and even reinforce biases in data, intentional or accidental, due to their sensitivity to redundancies. To address this issue, we propose evaluation as a 3-player game, and introduce novel game-theoretic solution concepts to ensure robustness to redundancy. We show that our method leads to intuitive ratings and provide insights into the competitive landscape of LLM development.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large AI Model for Delay-Doppler Domain Channel Prediction in 6G OTFS-Based Vehicular Networks</title>
<link>https://arxiv.org/abs/2503.01116</link>
<guid>https://arxiv.org/abs/2503.01116</guid>
<content:encoded><![CDATA[
arXiv:2503.01116v2 Announce Type: replace-cross 
Abstract: Channel prediction is crucial for high-mobility vehicular networks, as it enables the anticipation of future channel conditions and the proactive adjustment of communication strategies. However, achieving accurate vehicular channel prediction is challenging due to significant Doppler effects and rapid channel variations resulting from high-speed vehicle movement and complex propagation environments. In this paper, we propose a novel delay-Doppler (DD) domain channel prediction framework tailored for high-mobility vehicular networks. By transforming the channel representation into the DD domain, we obtain an intuitive, sparse, and stable depiction that closely aligns with the underlying physical propagation processes, effectively reducing the complex vehicular channel to a set of time-series parameters with enhanced predictability. Furthermore, we leverage the large artificial intelligence (AI) model to predict these DD-domain time-series parameters, capitalizing on their advanced ability to model temporal correlations. The zero-shot capability of the pre-trained large AI model facilitates accurate channel predictions without requiring task-specific training, while subsequent fine-tuning on specific vehicular channel data further improves prediction accuracy. Extensive simulation results demonstrate the effectiveness of our DD-domain channel prediction framework and the superior accuracy of the large AI model in predicting time-series channel parameters, thereby highlighting the potential of our approach for robust vehicular communication systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment</title>
<link>https://arxiv.org/abs/2503.03355</link>
<guid>https://arxiv.org/abs/2503.03355</guid>
<content:encoded><![CDATA[
arXiv:2503.03355v4 Announce Type: replace-cross 
Abstract: In this work, we rethink the approach to video super-resolution by introducing a method based on the Diffusion Posterior Sampling framework, combined with an unconditional video diffusion transformer operating in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Empirical results on synthetic and real-world datasets illustrate the feasibility of diffusion-based, alignment-free video super-resolution.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2503.05423</link>
<guid>https://arxiv.org/abs/2503.05423</guid>
<content:encoded><![CDATA[
arXiv:2503.05423v2 Announce Type: replace-cross 
Abstract: Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn from distinct categories without retaining exemplars but easily suffers from catastrophic forgetting of learned knowledge. While existing EFCIL methods leverage knowledge distillation to alleviate forgetting, they still face two critical challenges: semantic shift and decision bias. Specifically, the embeddings of old tasks shift in the embedding space after learning new tasks, and the classifier becomes biased towards new tasks due to training solely with new data, thereby hindering the balance between old and new knowledge. To address these issues, we propose the Dual-Projection Shift Estimation and Classifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates semantic shift through a dual-projection, which combines a learnable transformation with a row-space projection to capture both task-wise and category-wise shifts. Furthermore, to mitigate decision bias, DPCR employs ridge regression to reformulate classifier training as a reconstruction process. This reconstruction exploits previous information encoded in covariance and prototype of each class after calibration with estimated shift, thereby reducing decision bias. Extensive experiments demonstrate that, across various datasets, DPCR effectively balances old and new tasks, outperforming state-of-the-art EFCIL methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaskAttn-UNet: A Mask Attention-Driven Framework for Universal Low-Resolution Image Segmentation</title>
<link>https://arxiv.org/abs/2503.10686</link>
<guid>https://arxiv.org/abs/2503.10686</guid>
<content:encoded><![CDATA[
arXiv:2503.10686v2 Announce Type: replace-cross 
Abstract: Low-resolution image segmentation is crucial in real-world applications such as robotics, augmented reality, and large-scale scene understanding, where high-resolution data is often unavailable due to computational constraints. To address this challenge, we propose MaskAttn-UNet, a novel segmentation framework that enhances the traditional U-Net architecture via a mask attention mechanism. Our model selectively emphasizes important regions while suppressing irrelevant backgrounds, thereby improving segmentation accuracy in cluttered and complex scenes. Unlike conventional U-Net variants, MaskAttn-UNet effectively balances local feature extraction with broader contextual awareness, making it particularly well-suited for low-resolution inputs. We evaluate our approach on three benchmark datasets with input images rescaled to 128x128 and demonstrate competitive performance across semantic, instance, and panoptic segmentation tasks. Our results show that MaskAttn-UNet achieves accuracy comparable to state-of-the-art methods at significantly lower computational cost than transformer-based models, making it an efficient and scalable solution for low-resolution segmentation in resource-constrained scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization</title>
<link>https://arxiv.org/abs/2503.13690</link>
<guid>https://arxiv.org/abs/2503.13690</guid>
<content:encoded><![CDATA[
arXiv:2503.13690v2 Announce Type: replace-cross 
Abstract: We present a submission to the SemEval 2025 shared task on unlearning sensitive content from LLMs. Our approach employs negative preference optimization using low-rank adaptation. We show that we can utilize this combination to efficiently compute additional regularization terms, which help with unlearning stabilization. The results of our approach significantly exceed the shared task baselines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NaFM: Pre-training a Foundation Model for Small-Molecule Natural Products</title>
<link>https://arxiv.org/abs/2503.17656</link>
<guid>https://arxiv.org/abs/2503.17656</guid>
<content:encoded><![CDATA[
arXiv:2503.17656v2 Announce Type: replace-cross 
Abstract: Natural products, as metabolites from microorganisms, animals, or plants, exhibit diverse biological activities, making them crucial for drug discovery. Nowadays, existing deep learning methods for natural products research primarily rely on supervised learning approaches designed for specific downstream tasks. However, such one-model-for-a-task paradigm often lacks generalizability and leaves significant room for performance improvement. Additionally, existing molecular characterization methods are not well-suited for the unique tasks associated with natural products. To address these limitations, we have pre-trained a foundation model for natural products based on their unique properties. Our approach employs a novel pretraining strategy that is especially tailored to natural products. By incorporating contrastive learning and masked graph learning objectives, we emphasize evolutional information from molecular scaffolds while capturing side-chain information. Our framework achieves state-of-the-art (SOTA) results in various downstream tasks related to natural product mining and drug discovery. We first compare taxonomy classification with synthesized molecule-focused baselines to demonstrate that current models are inadequate for understanding natural synthesis. Furthermore, by diving into a fine-grained analysis at both the gene and microbial levels, NaFM demonstrates the ability to capture evolutionary information. Eventually, our method is experimented with virtual screening, illustrating informative natural product representations that can lead to more effective identification of potential drug candidates.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Attention-Based Model for 5G Radio-based Outdoor Localization</title>
<link>https://arxiv.org/abs/2503.23810</link>
<guid>https://arxiv.org/abs/2503.23810</guid>
<content:encoded><![CDATA[
arXiv:2503.23810v2 Announce Type: replace-cross 
Abstract: Radio-based localization in dynamic environments, such as urban and vehicular settings, requires systems that efficiently adapt to varying signal conditions and environmental changes. Factors like multipath interference and obstructions introduce different levels of complexity that affect the accuracy of the localization. Although generalized models offer broad applicability, they often struggle to capture the nuances of specific environments, leading to suboptimal performance in real-world deployments. In contrast, specialized models can be tailored to particular conditions, enabling more precise localization by effectively handling domain-specific variations, which also results in reduced execution time and smaller model size. However, deploying multiple specialized models requires an efficient mechanism to select the most appropriate one for a given scenario. In this work, we develop an adaptive localization framework that combines shallow attention-based models with a router/switching mechanism based on a single-layer perceptron. This enables seamless transitions between specialized localization models optimized for different conditions, balancing accuracy and computational complexity. We design three low-complex models tailored for distinct scenarios, and a router that dynamically selects the most suitable model based on real-time input characteristics. The proposed framework is validated using real-world vehicle localization data collected from a massive MIMO base station and compared to more general models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shallow AutoEncoding Recommender with Cold Start Handling via Side Features</title>
<link>https://arxiv.org/abs/2504.02288</link>
<guid>https://arxiv.org/abs/2504.02288</guid>
<content:encoded><![CDATA[
arXiv:2504.02288v3 Announce Type: replace-cross 
Abstract: User and item cold starts present significant challenges in industrial applications of recommendation systems. Supplementing user-item interaction data with metadata is a common solution-but often at the cost of introducing additional biases. In this work, we introduce an augmented EASE model, i.e. FEASE, that seamlessly integrates both user and item side information to address these cold start issues. Our straightforward, autoencoder-based method produces a closed-form solution that leverages rich content signals for cold items while refining user representations in data-sparse environments. Importantly, our method strikes a balance by effectively recommending cold start items and handling cold start users without incurring extra bias, and it maintains strong performance in warm settings. Experimental results demonstrate improved recommendation accuracy and robustness compared to previous collaborative filtering approaches. Moreover, our model serves as a strong baseline for future comparative studies.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouterKT: Mixture-of-Experts for Knowledge Tracing</title>
<link>https://arxiv.org/abs/2504.08989</link>
<guid>https://arxiv.org/abs/2504.08989</guid>
<content:encoded><![CDATA[
arXiv:2504.08989v3 Announce Type: replace-cross 
Abstract: Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems (ITS), which aims to model the dynamic knowledge states of students based on their interaction histories. However, existing KT models often rely on a global forgetting decay mechanism for capturing learning patterns, assuming that students' performance is predominantly influenced by their most recent interactions. Such approaches fail to account for the diverse and complex learning patterns arising from individual differences and varying learning stages. To address this limitation, we propose RouterKT, a novel Mixture-of-Experts (MoE) architecture designed to capture heterogeneous learning patterns by enabling experts to specialize in different patterns without any handcrafted learning pattern bias such as forgetting decay. Specifically, RouterKT introduces a \textbf{person-wise routing mechanism} to effectively model individual-specific learning behaviors and employs \textbf{multi-heads as experts} to enhance the modeling of complex and diverse patterns. Comprehensive experiments on ten benchmark datasets demonstrate that RouterKT exhibits significant flexibility and improves the performance of various KT backbone models, with a maximum average AUC improvement of 3.29\% across different backbones and datasets, outperforming other state-of-the-art models. Moreover, RouterKT demonstrates consistently superior inference efficiency compared to existing approaches based on handcrafted learning pattern bias, highlighting its usability for real-world educational applications. The source code is available at https://github.com/ringotc/RouterKT.git.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Quantum of Learning: Using Quaternion Algebra to Model Learning on Quantum Devices</title>
<link>https://arxiv.org/abs/2504.13232</link>
<guid>https://arxiv.org/abs/2504.13232</guid>
<content:encoded><![CDATA[
arXiv:2504.13232v2 Announce Type: replace-cross 
Abstract: This article considers the problem of designing adaption and optimisation techniques for training quantum learning machines. To this end, the division algebra of quaternions is used to derive an effective model for representing computation and measurement operations on qubits. In turn, the derived model, serves as the foundation for formulating an adaptive learning problem on principal quantum learning units, thereby establishing quantum information processing units akin to that of neurons in classical approaches. Then, leveraging the modern HR-calculus, a comprehensive training framework for learning on quantum machines is developed. The quaternion-valued model accommodates mathematical tractability and establishment of performance criteria, such as convergence conditions.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections</title>
<link>https://arxiv.org/abs/2504.16612</link>
<guid>https://arxiv.org/abs/2504.16612</guid>
<content:encoded><![CDATA[
arXiv:2504.16612v2 Announce Type: replace-cross 
Abstract: Purpose: In this study, we investigate the training of foundation models using federated learning to address data-sharing limitations and enable collaborative model training without data transfer for minimally invasive surgery. Methods: Inspired by the EndoViT study, we adapt the Masked Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is pretrained on the Endo700k dataset collection and later fine-tuned and evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition, and Surgical Phase Recognition. Results: Our findings demonstrate that integrating adaptive FedSAM into the federated MAE approach improves pretraining, leading to a reduction in reconstruction loss per patch. The application of FL-EndoViT in surgical downstream tasks results in performance comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over CEN-EndoViT in surgical scene segmentation when data is limited and in action triplet recognition when large datasets are used. Conclusion: These findings highlight the potential of federated learning for privacy-preserving training of surgical foundation models, offering a robust and generalizable solution for surgical data science. Effective collaboration requires adapting federated learning methods, such as the integration of FedSAM, which can accommodate the inherent data heterogeneity across institutions. In future, exploring FL in video-based models may enhance these capabilities by incorporating spatiotemporal dynamics crucial for real-world surgical environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approximate Lifted Model Construction</title>
<link>https://arxiv.org/abs/2504.20784</link>
<guid>https://arxiv.org/abs/2504.20784</guid>
<content:encoded><![CDATA[
arXiv:2504.20784v2 Announce Type: replace-cross 
Abstract: Probabilistic relational models such as parametric factor graphs enable efficient (lifted) inference by exploiting the indistinguishability of objects. In lifted inference, a representative of indistinguishable objects is used for computations. To obtain a relational (i.e., lifted) representation, the Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP algorithm, however, requires underlying distributions, encoded as potential-based factorisations, to exactly match to identify and exploit indistinguishabilities. Hence, ACP is unsuitable for practical applications where potentials learned from data inevitably deviate even if associated objects are indistinguishable. To mitigate this problem, we introduce the $\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which allows for a deviation of potentials depending on a hyperparameter $\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits indistinguishabilities that are not exact. We prove that the approximation error induced by $\varepsilon$-ACP is strictly bounded and our experiments show that the approximation error is close to zero in practice.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the generalization of language models from in-context learning and finetuning: a controlled study</title>
<link>https://arxiv.org/abs/2505.00661</link>
<guid>https://arxiv.org/abs/2505.00661</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, generalization, fine-tuning, in-context learning, datasets  

<br /><br />Summary: Large language models (LLMs) have demonstrated remarkable capabilities, yet they often struggle with generalization following fine-tuning, exhibiting issues like failing to reverse learned relations or make simple logical deductions. These limitations can significantly impede practical applications of LLMs. Conversely, in-context learning appears to showcase different inductive biases, allowing for potentially better generalization in specific scenarios. To investigate these distinctions, the authors created novel datasets that facilitate controlled evaluations of model generalization. By exposing pre-trained LLMs to subsets of these datasets through in-context or fine-tuning methods, they assessed performance on tests requiring varied generalization types. The findings suggest that in-context learning generally outperforms fine-tuning in data-matched settings, while also noting conditions where fine-tuning successfully generalizes to complex knowledge structures. Building on these insights, the authors proposed a new method to enhance fine-tuning generalization by integrating in-context inferences into the training data. This approach yielded improved generalization across multiple dataset splits and benchmarks. The results provide valuable insights into the differing inductive biases present in learning modes of language models and offer practical strategies for enhancing their performance. <div>
arXiv:2505.00661v2 Announce Type: replace-cross 
Abstract: Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning. E.g. they can fail to generalize to simple reversals of relations they are trained on, or fail to make simple logical deductions based on trained information. These failures to generalize from fine-tuning can hinder practical application of these models. On the other hand, language models' in-context learning shows different inductive biases, and can generalize better in some cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' abilities to generalize from finetuning data. The datasets are designed to create clean tests of generalization, by isolating the knowledge in the dataset from that in pretraining. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation</title>
<link>https://arxiv.org/abs/2505.03774</link>
<guid>https://arxiv.org/abs/2505.03774</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, out-of-distribution detection, heterogeneous graphs, node classification, energy propagation<br />
Summary:<br />
1. Graph neural networks (GNNs) excel at extracting complex node and structural information from graph data, primarily in in-distribution (ID) settings.
2. Real-world scenarios often involve distribution shifts, leading to the presence of out-of-distribution (OOD) nodes, a crucial and challenging task.
3. Existing research mainly focuses on homogeneous graphs, while real-world graphs are often heterogeneous, containing diverse node and edge types.
4. OOD detection in heterogeneous graphs is an underexplored area, prompting the proposal of a novel methodology for OOD detection in heterogeneous graphs (OODHG).
5. OODHG aims to detect OOD nodes and classify ID nodes by leveraging representations, energy values calculation, and energy constraint mechanisms, showing superiority over baseline models in OOD detection tasks and ID node classification. <br /> <div>
arXiv:2505.03774v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node and structural information from graph data. While current GNNs perform well in node classification tasks within in-distribution (ID) settings, real-world scenarios often present distribution shifts, leading to the presence of out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and challenging task. Most existing research focuses on homogeneous graphs, but real-world graphs are often heterogeneous, consisting of diverse node and edge types. This heterogeneity adds complexity and enriches the informational content. To the best of our knowledge, OOD detection in heterogeneous graphs remains an underexplored area. In this context, we propose a novel methodology for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the first task's results. Specifically, we learn representations for each node in the heterogeneous graph, calculate energy values to determine whether nodes are OOD, and then classify ID nodes. To leverage the structural information of heterogeneous graphs, we introduce a meta-path-based energy propagation mechanism and an energy constraint to enhance the distinction between ID and OOD nodes. Extensive experimental findings substantiate the simplicity and effectiveness of OODHG, demonstrating its superiority over baseline models in OOD detection tasks and its accuracy in ID node classification.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Multi-Label Generation with Probabilistic Level-Constraint</title>
<link>https://arxiv.org/abs/2505.03775</link>
<guid>https://arxiv.org/abs/2505.03775</guid>
<content:encoded><![CDATA[
<div> Hierarchical Multi-Label Generation, generative framework, Probabilistic Level Constraints, taxonomy, SOTA performance
Summary:
Hierarchical Extreme Multi-Label Classification is challenging due to complex hierarchical label connections and a large number of labels. Traditional methods like clustering and multistage classification have limitations in controlling generative model outputs. This paper introduces a Hierarchical Multi-Label Generation (HMG) approach using a generative framework with Probabilistic Level Constraints (PLC) to accurately generate hierarchical labels within a taxonomy. By generating all relevant labels across levels for each document without clustering, the model can precisely control output count, length, and level. Experimental results show that the proposed approach achieves state-of-the-art performance in HMG tasks and significantly improves model output control compared to previous research.  
Summary: <div>
arXiv:2505.03775v1 Announce Type: new 
Abstract: Hierarchical Extreme Multi-Label Classification poses greater difficulties compared to traditional multi-label classification because of the intricate hierarchical connections of labels within a domain-specific taxonomy and the substantial number of labels. Some of the prior research endeavors centered on classifying text through several ancillary stages such as the cluster algorithm and multiphase classification. Others made attempts to leverage the assistance of generative methods yet were unable to properly control the output of the generative model. We redefine the task from hierarchical multi-Label classification to Hierarchical Multi-Label Generation (HMG) and employ a generative framework with Probabilistic Level Constraints (PLC) to generate hierarchical labels within a specific taxonomy that have complex hierarchical relationships. The approach we proposed in this paper enables the framework to generate all relevant labels across levels for each document without relying on preliminary operations like clustering. Meanwhile, it can control the model output precisely in terms of count, length, and level aspects. Experiments demonstrate that our approach not only achieves a new SOTA performance in the HMG task, but also has a much better performance in constrained the output of model than previous research work.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction</title>
<link>https://arxiv.org/abs/2505.03776</link>
<guid>https://arxiv.org/abs/2505.03776</guid>
<content:encoded><![CDATA[
<div> Attention mechanism, encoder-decoder architecture, Pointer Network, route prediction, last-mile delivery<br />
<br />
Summary: <br />
This article introduces a novel Proximity Attention mechanism in an encoder-decoder architecture (PAPN) for route prediction in last-mile delivery and first-mile pickup logistics. The model combines local and global attention via a multi-head transformer encoder. Proximity attention is used to prioritize locations during decoding for improved prediction accuracy. Tested on a real-world dataset, LaDE, the PAPN model outperforms existing supervised systems and is on par with a reinforcement learning method, DRL4Route. This approach shows promise in enhancing the efficiency and accuracy of last-mile delivery optimization processes. <div>
arXiv:2505.03776v1 Announce Type: new 
Abstract: Optimization of the last-mile delivery and first-mile pickup of parcels is an integral part of the broader logistics optimization pipeline as it entails both cost and resource efficiency as well as a heightened service quality. Such optimization requires accurate route and time prediction systems to adapt to different scenarios in advance. This work tackles the first building block, namely route prediction. This is done by introducing a novel Proximity Attention mechanism in an encoder-decoder architecture utilizing a Pointer Network in the decoding process (Proximity Attention Encoder and Pointer Network decoder: PAPN) to leverage the underlying connections between the different visitable pickup positions at each timestep. To this local attention process is coupled global context computing via a multi-head attention transformer encoder. The obtained global context is then mixed to an aggregated version of the local embedding thus achieving a mix of global and local attention for complete modeling of the problems. Proximity attention is also used in the decoding process to skew predictions towards the locations with the highest attention scores and thus using inter-connectivity of locations as a base for next-location prediction. This method is trained, validated and tested on a large industry-level dataset of real-world, large-scale last-mile delivery and first-mile pickup named LaDE[1]. This approach shows noticeable promise, outperforming all state-of-the-art supervised systems in terms of most metrics used for benchmarking methods on this dataset while still being competitive with the best-performing reinforcement learning method named DRL4Route[2].
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolMole: Molecule Mining from Scientific Literature</title>
<link>https://arxiv.org/abs/2505.03777</link>
<guid>https://arxiv.org/abs/2505.03777</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular structures, reaction data, deep learning, chemical data extraction, document analysis

Summary: 
MolMole is a vision-based deep learning framework designed to extract molecular structures and reaction data from scientific documents. It combines molecule detection, reaction diagram parsing, and optical chemical structure recognition (OCSR) into a single pipeline. The framework addresses the challenges posed by the unstructured chemical formats and complex layouts of such documents. A benchmark test set of 550 pages annotated with molecule bounding boxes, reaction labels, and MOLfiles has been created, along with a novel evaluation metric. Experimental results show MolMole outperforms existing toolkits on both the benchmark and public datasets. The MolMole toolkit will soon be available through an interactive demo on the LG AI Research website. For commercial inquiries, contact may be made via email to contact_ddu@lgresearch.ai.

<br /><br />Summary: <div>
arXiv:2505.03777v1 Announce Type: new 
Abstract: The extraction of molecular structures and reaction data from scientific documents is challenging due to their varied, unstructured chemical formats and complex document layouts. To address this, we introduce MolMole, a vision-based deep learning framework that unifies molecule detection, reaction diagram parsing, and optical chemical structure recognition (OCSR) into a single pipeline for automating the extraction of chemical data directly from page-level documents. Recognizing the lack of a standard page-level benchmark and evaluation metric, we also present a testset of 550 pages annotated with molecule bounding boxes, reaction labels, and MOLfiles, along with a novel evaluation metric. Experimental results demonstrate that MolMole outperforms existing toolkits on both our benchmark and public datasets. The benchmark testset will be publicly available, and the MolMole toolkit will be accessible soon through an interactive demo on the LG AI Research website. For commercial inquiries, please contact us at \href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dragonfly: a modular deep reinforcement learning library</title>
<link>https://arxiv.org/abs/2505.03778</link>
<guid>https://arxiv.org/abs/2505.03778</guid>
<content:encoded><![CDATA[
<div> Keywords: deep reinforcement learning, modularity, json serialization, CPU-intensive environments, benchmarks

Summary: Dragonfly is a new deep reinforcement learning library that focuses on modularity for ease of experimentation and development. It utilizes a json serialization method that allows for swapping building blocks and conducting parameter sweeps while minimizing the need for code maintenance. The library is optimized for CPU-intensive environments, making it suitable for numerical simulations. Dragonfly's performance on standard agents in common benchmarks is comparable to existing literature. Its features, designed for flexibility and efficiency, make it a promising tool for researchers and developers in the field of deep reinforcement learning.<br /><br />Summary: <div>
arXiv:2505.03778v1 Announce Type: new 
Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in order to ease experimentation and developments. It relies on a json serialization that allows to swap building blocks and perform parameter sweep, while minimizing code maintenance. Some of its features are specifically designed for CPU-intensive environments, such as numerical simulations. Its performance on standard agents using common benchmarks compares favorably with the literature.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites</title>
<link>https://arxiv.org/abs/2505.03779</link>
<guid>https://arxiv.org/abs/2505.03779</guid>
<content:encoded><![CDATA[
<div> neural network, computational framework, structural topology, anisotropic strength, fiber-reinforced composites

Summary:
The paper presents a neural network-based computational framework for optimizing structural topology, curved layers, and path orientations in fiber-reinforced thermoplastic composites. Three implicit neural fields represent geometric shape, layer sequence, and fiber orientation, allowing for the integration of design and manufacturability objectives into an optimized process. The framework incorporates objectives such as anisotropic strength, structural volume, machine motion control, layer curvature, and layer thickness through loss functions. Physical experiments demonstrate that composites generated using this co-optimization method exhibit up to a 33.1% improvement in failure loads compared to sequentially optimized structures and manufacturing sequences. This approach ensures strong mechanical strength while maintaining manufacturability for filament-based multi-axis 3D printing on different hardware platforms. <br /><br />Summary: <div>
arXiv:2505.03779v1 Announce Type: new 
Abstract: We propose a neural network-based computational framework for the simultaneous optimization of structural topology, curved layers, and path orientations to achieve strong anisotropic strength in fiber-reinforced thermoplastic composites while ensuring manufacturability. Our framework employs three implicit neural fields to represent geometric shape, layer sequence, and fiber orientation. This enables the direct formulation of both design and manufacturability objectives - such as anisotropic strength, structural volume, machine motion control, layer curvature, and layer thickness - into an integrated and differentiable optimization process. By incorporating these objectives as loss functions, the framework ensures that the resultant composites exhibit optimized mechanical strength while remaining its manufacturability for filament-based multi-axis 3D printing across diverse hardware platforms. Physical experiments demonstrate that the composites generated by our co-optimization method can achieve an improvement of up to 33.1% in failure loads compared to composites with sequentially optimized structures and manufacturing sequences.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALFRED: Ask a Large-language model For Reliable ECG Diagnosis</title>
<link>https://arxiv.org/abs/2505.03781</link>
<guid>https://arxiv.org/abs/2505.03781</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, ECG analysis, Zero-shot diagnosis framework, expert-curated knowledge

Summary: 
A new framework utilizing Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for analyzing medical data, specifically Electrocardiogram (ECG), has been proposed. This framework addresses the challenge of generating reliable and evidence-based results in healthcare by incorporating expert-curated knowledge. The Zero-shot ECG diagnosis framework based on RAG aims to enhance diagnostic accuracy and explainability, particularly in automated ECG interpretation. Evaluation on the PTB-XL dataset has shown the effectiveness of this approach, highlighting the importance of structured domain expertise in ECG analysis. The framework is designed to support comprehensive ECG analysis and cater to diverse diagnostic needs, with potential applications beyond the tested dataset. <div>
arXiv:2505.03781v1 Announce Type: new 
Abstract: Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers high accuracy and convenience. However, generating reliable, evidence-based results in specialized fields like healthcare remains a challenge, as RAG alone may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG for ECG analysis that incorporates expert-curated knowledge to enhance diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset demonstrates the framework's effectiveness, highlighting the value of structured domain expertise in automated ECG interpretation. Our framework is designed to support comprehensive ECG analysis, addressing diverse diagnostic needs with potential applications beyond the tested dataset.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A general physics-constrained method for the modelling of equation's closure terms with sparse data</title>
<link>https://arxiv.org/abs/2505.03783</link>
<guid>https://arxiv.org/abs/2505.03783</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Closure Terms, Physics-Informed, Partial Differential Equation, Predictive Simulations <br />
Summary: 
This study introduces a novel approach for constructing closure models in scenarios with sparse data. The proposed Series-Parallel Multi-Network Architecture combines Physics-Informed Neural Networks (PINNs) with heterogeneous data to create robust models. Dedicated subnetworks are used to model unknown closure terms independently, enhancing generalizability. These models are integrated into a PDE solver for accurate predictive simulations in engineering applications. <div>
arXiv:2505.03783v1 Announce Type: new 
Abstract: Accurate modeling of closure terms is a critical challenge in engineering and scientific research, particularly when data is sparse (scarse or incomplete), making widely applicable models difficult to develop. This study proposes a novel approach for constructing closure models in such challenging scenarios. We introduce a Series-Parallel Multi-Network Architecture that integrates Physics-Informed Neural Networks (PINNs) to incorporate physical constraints and heterogeneous data from multiple initial and boundary conditions, while employing dedicated subnetworks to independently model unknown closure terms, enhancing generalizability across diverse problems. These closure models are integrated into an accurate Partial Differential Equation (PDE) solver, enabling robust solutions to complex predictive simulations in engineering applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers</title>
<link>https://arxiv.org/abs/2505.03784</link>
<guid>https://arxiv.org/abs/2505.03784</guid>
<content:encoded><![CDATA[
<div> wearable device, blood biomarkers, insulin resistance, deep neural network model, early intervention <br />
Summary: 
This study aimed to develop a method for predicting insulin resistance, a precursor to type 2 diabetes, using wearable device data and blood biomarkers. The researchers used a large dataset of 1,165 participants to train deep neural network models that can predict insulin resistance better when combining wearable data and blood biomarkers. The models showed promising results in predicting insulin resistance, with high sensitivity and specificity, especially in obese and sedentary individuals. The performance of the models was validated on an independent cohort, demonstrating their generalizability. Additionally, the study integrated the predicted insulin resistance values into a large language model agent to aid in interpretation and personalized recommendations. This research opens up possibilities for early detection of individuals at risk of type 2 diabetes, allowing for timely interventions to prevent the disease. <br /> <div>
arXiv:2505.03784v1 Announce Type: new 
Abstract: Insulin resistance, a precursor to type 2 diabetes, is characterized by impaired insulin action in tissues. Current methods for measuring insulin resistance, while effective, are expensive, inaccessible, not widely available and hinder opportunities for early intervention. In this study, we remotely recruited the largest dataset to date across the US to study insulin resistance (N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%), incorporating wearable device time series data and blood biomarkers, including the ground-truth measure of insulin resistance, homeostatic model assessment for insulin resistance (HOMA-IR). We developed deep neural network models to predict insulin resistance based on readily available digital and blood biomarkers. Our results show that our models can predict insulin resistance by combining both wearable data and readily available blood biomarkers better than either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%, and specificity 84%). The model showed 93% sensitivity and 95% adjusted specificity in obese and sedentary participants, a subpopulation most vulnerable to developing type 2 diabetes and who could benefit most from early intervention. Rigorous evaluation of model performance, including interpretability, and robustness, facilitates generalizability across larger cohorts, which is demonstrated by reproducing the prediction performance on an independent validation cohort (N=72 participants). Additionally, we demonstrated how the predicted insulin resistance can be integrated into a large language model agent to help understand and contextualize HOMA-IR values, facilitating interpretation and safe personalized recommendations. This work offers the potential for early detection of people at risk of type 2 diabetes and thereby facilitate earlier implementation of preventative strategies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging</title>
<link>https://arxiv.org/abs/2505.03785</link>
<guid>https://arxiv.org/abs/2505.03785</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic systems, large language models, medical AI, autonomous framework, open-source <br />
Summary: 
mAIstro is introduced as an autonomous multi-agentic framework for healthcare AI development, leveraging large language models for automation. The system facilitates end-to-end processing of medical data and tasks such as data analysis, feature extraction, classification, and regression through a user-friendly natural language interface, eliminating the need for coding. With a modular architecture supporting both open and closed-source LLMs, mAIstro was evaluated on diverse datasets and successfully executed various tasks, generating interpretable outputs and validated models. This innovative framework integrates data analysis, AI model development, and inference in healthcare applications, providing a reproducible and extensible platform for clinical and research AI integration. The code for mAIstro is openly available on GitHub, enabling further development and collaboration in the medical AI community. <br /><br />Summary: <div>
arXiv:2505.03785v1 Announce Type: new 
Abstract: Agentic systems built on large language models (LLMs) offer promising capabilities for automating complex workflows in healthcare AI. We introduce mAIstro, an open-source, autonomous multi-agentic framework for end-to-end development and deployment of medical AI models. The system orchestrates exploratory data analysis, radiomic feature extraction, image segmentation, classification, and regression through a natural language interface, requiring no coding from the user. Built on a modular architecture, mAIstro supports both open- and closed-source LLMs, and was evaluated using a large and diverse set of prompts across 16 open-source datasets, covering a wide range of imaging modalities, anatomical regions, and data types. The agents successfully executed all tasks, producing interpretable outputs and validated models. This work presents the first agentic framework capable of unifying data analysis, AI model development, and inference across varied healthcare applications, offering a reproducible and extensible foundation for clinical and research AI integration. The code is available at: https://github.com/eltzanis/mAIstro
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator</title>
<link>https://arxiv.org/abs/2505.03786</link>
<guid>https://arxiv.org/abs/2505.03786</guid>
<content:encoded><![CDATA[
<div> reasoning models, LLM, candidate evaluation, planning frameworks, text-to-SQL task

Summary:
- The study compares a reasoning model, DeepSeek-R1, with non-reasoning LLMs for candidate evaluation in a text-to-SQL task.
- DeepSeek-R1 outperforms CodeLlama-7B and CodeLlama-13B in discrimination accuracy and execution accuracy despite having fewer parameters.
- Providing more context or compute budget for reasoning does not significantly improve discrimination performance.
- Reasoning models struggle more with generation compared to discrimination, unlike non-reasoning LLMs.
- The study suggests that reasoning models excel as discriminators in planning frameworks, offering insights into their optimal role within LLM infrastructures.<br /><br />Summary: <div>
arXiv:2505.03786v1 Announce Type: new 
Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising path for improving candidate evaluation in planning frameworks, but their relative performance against traditional non-reasoning models remains largely underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator LLM planning framework for the text-to-SQL task. For this, we introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking of candidates. Our central hypothesis is that reasoning models are more effective discriminators than non-reasoning LLMs. Our results show that distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution accuracy than CodeLlama-13B, despite having significantly fewer parameters. Furthermore, we find that there is a limit to the logical capabilities of reasoning models, and only providing more context or allowing more compute budget for reasoning is not enough to improve their discrimination performance. Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find generation more challenging than discrimination and may underperform as generators compared to smaller non-reasoning LLMs. Our work highlights the potential of reasoning models as discriminators in agentic frameworks, far outweighing their capabilities as generators, offering insights into their optimal role within LLM planning infrastructures.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification</title>
<link>https://arxiv.org/abs/2505.03787</link>
<guid>https://arxiv.org/abs/2505.03787</guid>
<content:encoded><![CDATA[
<div> Convolutional Neural Networks, Lightweight, Arrhythmia Classification, ECG Analysis, Interpretability <br />
Summary: 
The article introduces two novel lightweight 1D convolutional neural networks, ArrhythmiNet V1 and V2, designed for real-time arrhythmia classification on edge devices. These models have small memory footprints and high classification accuracies across five classes of cardiac arrhythmias. They incorporate interpretability techniques to highlight physiologically meaningful patterns in ECG signals. The study demonstrates the feasibility of combining interpretability, predictive accuracy, and computational efficiency in wearable and embedded ECG monitoring systems. The models offer efficient automated analysis of ECG signals, addressing the need for accurate and timely detection of cardiac arrhythmias. However, there are limitations related to dataset diversity and generalizability that need to be addressed in future research. <div>
arXiv:2505.03787v1 Announce Type: new 
Abstract: Cardiac arrhythmias are a leading cause of life-threatening cardiac events, highlighting the urgent need for accurate and timely detection. Electrocardiography (ECG) remains the clinical gold standard for arrhythmia diagnosis; however, manual interpretation is time-consuming, dependent on clinical expertise, and prone to human error. Although deep learning has advanced automated ECG analysis, many existing models abstract away the signal's intrinsic temporal and morphological features, lack interpretability, and are computationally intensive-hindering their deployment on resource-constrained platforms. In this work, we propose two novel lightweight 1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for efficient, real-time arrhythmia classification on edge devices. Inspired by MobileNet's depthwise separable convolutional design, these models maintain memory footprints of just 302.18 KB and 157.76 KB, respectively, while achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature Ventricular Contraction. In order to ensure clinical transparency and relevance, we integrate Shapley Additive Explanations and Gradient-weighted Class Activation Mapping, enabling both local and global interpretability. These techniques highlight physiologically meaningful patterns such as the QRS complex and T-wave that contribute to the model's predictions. We also discuss performance-efficiency trade-offs and address current limitations related to dataset diversity and generalizability. Overall, our findings demonstrate the feasibility of combining interpretability, predictive accuracy, and computational efficiency in practical, wearable, and embedded ECG monitoring systems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new architecture of high-order deep neural networks that learn martingales</title>
<link>https://arxiv.org/abs/2505.03789</link>
<guid>https://arxiv.org/abs/2505.03789</guid>
<content:encoded><![CDATA[
<div> neural network, deep learning, stochastic differential equations, financial derivatives, Runge-Kutta

Summary:
The article introduces a novel deep-learning neural network architecture for stochastic differential equations (SDEs), specifically designed to efficiently learn martingales in deep learning models. By leveraging high-order weak approximation algorithms, the architecture allows for the accurate pricing of financial derivatives. Central to this new approach is the use of explicit Runge-Kutta type algorithms, which approximate the SDEs through iterative compositions and linear combinations of vector fields. The study explores the behavior of deep neural networks based on this architecture, shedding light on their effectiveness in tackling complex financial problems. <div>
arXiv:2505.03789v1 Announce Type: new 
Abstract: A new deep-learning neural network architecture based on high-order weak approximation algorithms for stochastic differential equations (SDEs) is proposed. The architecture enables the efficient learning of martingales by deep learning models. The behaviour of deep neural networks based on this architecture, when applied to the problem of pricing financial derivatives, is also examined. The core of this new architecture lies in the high-order weak approximation algorithms of the explicit Runge--Kutta type, wherein the approximation is realised solely through iterative compositions and linear combinations of vector fields of the target SDEs.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Time-Series Data Augmentation Model through Diffusion and Transformer Integration</title>
<link>https://arxiv.org/abs/2505.03790</link>
<guid>https://arxiv.org/abs/2505.03790</guid>
<content:encoded><![CDATA[
<div> Diffusion model, Transformer model, data augmentation, time-series data, deep neural networks
Summary:
- The article introduces a method that combines Diffusion and Transformer models to augment time-series data efficiently.
- Deep neural networks often require large volumes of training data for optimal performance, but there has been a lack of focus on augmenting time-series data.
- The proposed method utilizes a diffusion denoising model to generate initial time-step action data and a Transformer model to predict subsequent actions.
- A weighted loss function is incorporated to achieve convergence and improve the model's performance after applying augmented data.
- Comparisons with traditional data augmentation methods demonstrate the effectiveness of the approach in producing high-quality augmented time-series data. 

<br /><br />Summary: <div>
arXiv:2505.03790v1 Announce Type: new 
Abstract: With the development of Artificial Intelligence, numerous real-world tasks have been accomplished using technology integrated with deep learning. To achieve optimal performance, deep neural networks typically require large volumes of data for training. Although advances in data augmentation have facilitated the acquisition of vast datasets, most of this data is concentrated in domains like images and speech. However, there has been relatively less focus on augmenting time-series data. To address this gap and generate a substantial amount of time-series data, we propose a simple and effective method that combines the Diffusion and Transformer models. By utilizing an adjusted diffusion denoising model to generate a large volume of initial time-step action data, followed by employing a Transformer model to predict subsequent actions, and incorporating a weighted loss function to achieve convergence, the method demonstrates its effectiveness. Using the performance improvement of the model after applying augmented data as a benchmark, and comparing the results with those obtained without data augmentation or using traditional data augmentation methods, this approach shows its capability to produce high-quality augmented data.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Boolean Backpropagation</title>
<link>https://arxiv.org/abs/2505.03791</link>
<guid>https://arxiv.org/abs/2505.03791</guid>
<content:encoded><![CDATA[
<div> Boolean neural networks, hardware-efficient, real-valued models, quantization, purely Boolean training, backpropagation, specific gate, Boolean algebra, feasibility <br />
<br />
Boolean neural networks offer hardware-efficient alternatives to real-valued models, with the use of quantization commonly seen. This study introduces a practical method for purely Boolean backpropagation for networks based on a specific chosen gate, operating directly in Boolean algebra without any numerical computations. Initial experiments confirm the feasibility of this approach. The benefits include the potential for more efficient hardware implementation and reduced resource usage. By focusing on Boolean operations, the training process becomes less complex and may lead to improved performance in specific tasks. This research contributes to the exploration of purely Boolean approaches in neural network training and highlights the potential for development in hardware-efficient models. <br /><br />Summary: <div>
arXiv:2505.03791v1 Announce Type: new 
Abstract: Boolean neural networks offer hardware-efficient alternatives to real-valued models. While quantization is common, purely Boolean training remains underexplored. We present a practical method for purely Boolean backpropagation for networks based on a single specific gate we chose, operating directly in Boolean algebra involving no numerics. Initial experiments confirm its feasibility.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.03792</link>
<guid>https://arxiv.org/abs/2505.03792</guid>
<content:encoded><![CDATA[
<div> Keywords: online fine-tuning, vision-language model, reinforcement learning, exploration efficiency, counterfactual reasoning

Summary:
Counterfactual Soft Reinforcement Learning (CoSo) is a novel online fine-tuning method that addresses the challenges of exploration in reinforcement learning for vision-language model agents. By dynamically assessing the causal influence of individual tokens on actions and prioritizing critical tokens, CoSo enables more targeted and efficient online exploration. The method has theoretical guarantees of convergence and policy improvement and has been shown to consistently enhance exploration efficiency and performance across diverse agent tasks such as Android device control, card gaming, and embodied AI. The empirical evaluations demonstrate the effectiveness of CoSo in improving exploration efficiency and delivering performance gains. The code for CoSo is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2505.03792v1 Announce Type: new 
Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement learning (RL) has shown promise for equipping agents with multi-step, goal-oriented capabilities in dynamic environments. However, their open-ended textual action space and non-end-to-end nature of action generation present significant challenges to effective online exploration in RL, e.g., explosion of the exploration space. We propose a novel online fine-tuning method, Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual output space of VLM agents. Compared to prior methods that assign uniform uncertainty to all tokens, CoSo leverages counterfactual reasoning to dynamically assess the causal influence of individual tokens on post-processed actions. By prioritizing the exploration of action-critical tokens while reducing the impact of semantically redundant or low-impact tokens, CoSo enables a more targeted and efficient online rollout process. We provide theoretical analysis proving CoSo's convergence and policy improvement guarantees, and extensive empirical evaluations supporting CoSo's effectiveness. Our results across a diverse set of agent tasks, including Android device control, card gaming, and embodied AI, highlight its remarkable ability to enhance exploration efficiency and deliver consistent performance gains. The code is available at https://github.com/langfengQ/CoSo.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection</title>
<link>https://arxiv.org/abs/2505.03793</link>
<guid>https://arxiv.org/abs/2505.03793</guid>
<content:encoded><![CDATA[
<div> derive, generalization, Large Language Models, selection, Neural Tangent Kernel

Summary:
- The article addresses the challenge of efficiently selecting Large Language Models (LLMs) for downstream tasks due to computational constraints.
- A novel theoretical framework is proposed to assess the generalization capabilities of LLMs, allowing for accurate and efficient model selection.
- A Hessian-based PAC-Bayes generalization bound is derived to uncover the fine-tuning dynamics of LLMs.
- The introduction of LENSLLM, a Neural Tangent Kernel (NTK)-based Rectified Scaling Model, enables accurate performance predictions across diverse tasks while maintaining computational efficiency.
- Empirical results on 3 large-scale benchmarks demonstrate that the LENSLLM model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods.<br /><br />Summary: <div>
arXiv:2505.03793v1 Announce Type: new 
Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a Hessian-based PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at the Github link: https://github.com/Susan571/LENSLLM.git.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems</title>
<link>https://arxiv.org/abs/2505.03794</link>
<guid>https://arxiv.org/abs/2505.03794</guid>
<content:encoded><![CDATA[
<div> Keywords: forward-backward splitting, inertial parameters, co-coercive operator, maximal monotone operator, experimental results

Summary:
The paper introduces an enhanced forward-backward splitting algorithm with two inertial parameters for finding a point in real Hilbert space where the sum of a co-coercive operator and a maximal monotone operator vanishes. The algorithm shows weak convergence under standard assumptions. Experimental results showcase its efficiency compared to existing algorithms in regression and data classification tasks. The proposed algorithm outperforms other methods in the literature, indicating its superior performance. The experiments provide evidence of the algorithm's effectiveness in various scenarios, cementing its potential for practical applications. Overall, the research contributes a valuable algorithmic advancement in optimization problems, demonstrating promising results in real-world applications.<br /><br />Summary: <div>
arXiv:2505.03794v1 Announce Type: new 
Abstract: This paper presents an improved forward-backward splitting algorithm with two inertial parameters. It aims to find a point in the real Hilbert space at which the sum of a co-coercive operator and a maximal monotone operator vanishes. Under standard assumptions, our proposed algorithm demonstrates weak convergence. We present numerous experimental results to demonstrate the behavior of the developed algorithm by comparing it with existing algorithms in the literature for regression and data classification problems. Furthermore, these implementations suggest our proposed algorithm yields superior outcomes when benchmarked against other relevant algorithms in existing literature.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2505.03797</link>
<guid>https://arxiv.org/abs/2505.03797</guid>
<content:encoded><![CDATA[
<div> Bayesian neural networks, Partial, Sequential Monte Carlo, Scalability, High-dimensional <br />
Summary: 
Partial Bayesian neural networks (pBNNs) have shown promising performance by incorporating stochastic parameters within neural networks. A new training method using Sequential Monte Carlo (SMC) samplers with guided proposals and gradient-based Markov kernels has been introduced. This method allows for non-parametric probabilistic estimation of stochastic parameters, leading to improved predictive performance and optimal loss compared to parametric methods. pBNNs also exhibit scalability on high-dimensional problems, outperforming state-of-the-art techniques. Furthermore, pBNNs show efficiency in handling larger batch sizes, resulting in reduced training times and enhanced performance. <div>
arXiv:2505.03797v1 Announce Type: new 
Abstract: Partial Bayesian neural networks (pBNNs) have been shown to perform competitively with fully Bayesian neural networks while only having a subset of the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as the inference method for pBNNs gives a non-parametric probabilistic estimation of the stochastic parameters, and has shown improved performance over parametric methods. In this paper we introduce a new SMC-based training method for pBNNs by utilising a guided proposal and incorporating gradient-based Markov kernels, which gives us better scalability on high dimensional problems. We show that our new method outperforms the state-of-the-art in terms of predictive performance and optimal loss. We also show that pBNNs scale well with larger batch sizes, resulting in significantly reduced training times and often better performance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Foundation Models Need Digital Twin Representations</title>
<link>https://arxiv.org/abs/2505.03798</link>
<guid>https://arxiv.org/abs/2505.03798</guid>
<content:encoded><![CDATA[
<div> digital twin, foundation models, multimodal data, domain knowledge, spatial-temporal dynamics

Summary:
This position paper critiques current foundation models (FMs) for their reliance on token representations that fragment continuous real-world multimodal data. It argues that FMs should consider digital twin (DT) representations, which are outcome-driven digital replicas of physical processes, as an alternative. The limitations of current FMs include struggles in maintaining semantic coherence across modalities, capturing fine-grained spatial-temporal dynamics, and performing causal reasoning. Scaling up model size or expanding datasets cannot address these challenges. The machine learning community should adopt DT representations that provide physically grounded representations and preserve the continuous nature of real-world processes. DT representations explicitly encode domain knowledge, offering a solution to the limitations of token representations in FMs. <div>
arXiv:2505.03798v1 Announce Type: new 
Abstract: Current foundation models (FMs) rely on token representations that directly fragment continuous real-world multimodal data into discrete tokens. They limit FMs to learning real-world knowledge and relationships purely through statistical correlation rather than leveraging explicit domain knowledge. Consequently, current FMs struggle with maintaining semantic coherence across modalities, capturing fine-grained spatial-temporal dynamics, and performing causal reasoning. These limitations cannot be overcome by simply scaling up model size or expanding datasets. This position paper argues that the machine learning community should consider digital twin (DT) representations, which are outcome-driven digital representations that serve as building blocks for creating virtual replicas of physical processes, as an alternative to the token representation for building FMs. Finally, we discuss how DT representations can address these challenges by providing physically grounded representations that explicitly encode domain knowledge and preserve the continuous nature of real-world processes.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling</title>
<link>https://arxiv.org/abs/2505.03799</link>
<guid>https://arxiv.org/abs/2505.03799</guid>
<content:encoded><![CDATA[
<div> Graph Language Models, Large Language Models, Graph Neural Networks, token efficiency, node classification <br />
<br />
Summary: 
The article introduces SDM-InstructGLM, a novel framework for Graph Language Models (GLMs) that enhances scalability and efficiency without using Graph Neural Networks (GNNs). It proposes a similarity-degree-based biased random walk mechanism that selectively samples and encodes graph information within Large Language Models (LLMs) based on node-feature similarity and degree centrality. This approach improves token efficiency, reduces information loss, and enhances performance on graph-based tasks like node classification and link prediction. The results demonstrate the feasibility of processing graphs with LLMs alone, allowing for scalable and interpretable GLMs through instruction-based fine-tuning. This work opens up possibilities for GNN-free approaches to graph learning, utilizing LLMs as standalone graph reasoning models. The source code for this framework is available on GitHub. <br /> <br />Summary: <div>
arXiv:2505.03799v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Compression with Global Rank and Sparsity Optimization</title>
<link>https://arxiv.org/abs/2505.03801</link>
<guid>https://arxiv.org/abs/2505.03801</guid>
<content:encoded><![CDATA[
<div> Keywords: Low-rank approximation, Sparse approximation, Large Language Models, Compression, Principal Component Analysis

Summary:
This article introduces a novel two-stage method for compressing Large Language Models (LLMs) through low-rank and sparse composite approximation. The method addresses challenges related to the interaction between low-rank and sparse matrices, as well as weight allocation across different layers with varying redundancy levels. In the first stage, robust principal component analysis decomposes weight matrices into low-rank and sparse components. The second stage utilizes a probabilistic global optimization technique to identify low-rank and sparse structures within the optimized spaces. The method automatically detects redundancy across layers and manages the interaction between sparse and low-rank components, leading to superior performance compared to existing techniques for sparsification and composite approximation. Extensive experiments demonstrate the effectiveness of the proposed approach. 

<br /><br />Summary: <div>
arXiv:2505.03801v1 Announce Type: new 
Abstract: Low-rank and sparse composite approximation is a natural idea to compress Large Language Models (LLMs). However, such an idea faces two primary challenges that adversely affect the performance of existing methods. The first challenge relates to the interaction and cooperation between low-rank and sparse matrices, while the second involves determining weight allocation across different layers, as redundancy varies considerably among them. To address these challenges, we propose a novel two-stage LLM compression method with the capability of global rank and sparsity optimization. It is noteworthy that the overall optimization space is vast, making comprehensive optimization computationally prohibitive. Therefore, to reduce the optimization space, our first stage utilizes robust principal component analysis to decompose the weight matrices of LLMs into low-rank and sparse components, which span the low dimensional and sparse spaces containing the resultant low-rank and sparse matrices, respectively. In the second stage, we propose a probabilistic global optimization technique to jointly identify the low-rank and sparse structures within the above two spaces. The appealing feature of our approach is its ability to automatically detect the redundancy across different layers and to manage the interaction between the sparse and low-rank components. Extensive experimental results indicate that our method significantly surpasses state-of-the-art techniques for sparsification and composite approximation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth</title>
<link>https://arxiv.org/abs/2505.03802</link>
<guid>https://arxiv.org/abs/2505.03802</guid>
<content:encoded><![CDATA[
<div> quantization, LoRA, fine-tuning, large language models, memory-friendly
Summary: 
The article introduces QR-Adaptor, a novel approach that combines low-bit quantization and LoRA for efficient fine-tuning of large language models. Unlike previous methods based on SVD, QR-Adaptor leverages dynamic mixed precision to continuously improve model performance by jointly optimizing quantization components and low-rank spaces for each layer. By treating precision and rank allocation as a discrete optimization problem guided by downstream performance and memory usage, QR-Adaptor achieves a 4.89% accuracy improvement on GSM8K compared to state-of-the-art methods. Furthermore, in some cases, it even outperforms the 16-bit fine-tuned model while maintaining the memory footprint of the 4-bit setting. This unified, gradient-free strategy offers a promising solution for memory-efficient fine-tuning of quantized large language models. 
<br /><br />Summary: <div>
arXiv:2505.03802v1 Announce Type: new 
Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve memory-friendly fine-tuning for large language models (LLM). Recently, methods based on SVD for continuous update iterations to initialize LoRA matrices to accommodate quantization errors have generally failed to consistently improve performance. Dynamic mixed precision is a natural idea for continuously improving the fine-tuning performance of quantized models, but previous methods often optimize low-rank subspaces or quantization components separately, without considering their synergy. To address this, we propose \textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial calibration data to jointly search the quantization components and the rank of low-rank spaces for each layer, thereby continuously improving model performance. QR-Adaptor does not minimize quantization error but treats precision and rank allocation as a discrete optimization problem guided by actual downstream performance and memory usage. Compared to state-of-the-art (SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\% accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit fine-tuned model while maintaining the memory footprint of the 4-bit setting.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization</title>
<link>https://arxiv.org/abs/2505.03803</link>
<guid>https://arxiv.org/abs/2505.03803</guid>
<content:encoded><![CDATA[
<div> RWKV, RNN architecture, Transformer, Post Training Quantization, RWKVQuant<br />
Summary:<br />
- RWKV is a modern RNN architecture comparable to Transformer but faces challenges on resource-constrained devices. 
- Post Training Quantization (PTQ) is essential for reducing model size and latency but degrades RWKV's performance due to non-linear operators and uniformly distributed weights. 
- RWKVQuant, a PTQ framework for RWKV models, introduces a coarse-to-fine proxy for adaptive quantization selection and a codebook optimization algorithm for cluster-based quantization, achieving a 3-bit quantization with less than 1% accuracy loss and 2.14x speedup. <div>
arXiv:2505.03803v1 Announce Type: new 
Abstract: RWKV is a modern RNN architecture with comparable performance to Transformer, but still faces challenges when deployed to resource-constrained devices. Post Training Quantization (PTQ), which is a an essential technique to reduce model size and inference latency, has been widely used in Transformer models. However, it suffers significant degradation of performance when applied to RWKV. This paper investigates and identifies two key constraints inherent in the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of both smooth- and rotation-based quantization, introducing extra computation overhead. (2) The larger amount of uniformly distributed weights poses challenges for cluster-based quantization, leading to reduced accuracy. To this end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively selecting different quantization approaches by assessing the uniformity and identifying outliers in the weights, and (2) a codebook optimization algorithm that enhances the performance of cluster-based quantization methods for element-wise multiplication in RWKV. Experiments show that RWKVQuant can quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x speed up.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance</title>
<link>https://arxiv.org/abs/2505.03804</link>
<guid>https://arxiv.org/abs/2505.03804</guid>
<content:encoded><![CDATA[
<div> quantization, Mixture-of-Experts, large language models, efficiency, scalability

Summary:
Expert-Balanced Self-Sampling (EBSS) and Affinity-Guided Quantization (AGQ) are proposed in MoEQuant to address challenges faced by Mixture-of-Experts (MoE) large language models (LLMs) during post-training quantization (PTQ). MoE models have sparse and dynamic characteristics that pose difficulties for traditional quantization methods, leading to accuracy degradation and limited generalization performance. EBSS creates a calibration set with balanced expert distributions by considering token probabilities and expert balance metrics. AGQ incorporates affinities between experts and samples, accurately evaluating the impact of samples on different experts within the MoE layer. Through experiments, MoEQuant achieves significant performance gains, with more than a 10-point accuracy improvement in HumanEval for DeepSeekMoE-16B under 4-bit quantization. This novel quantization framework enhances efficiency and scalability of MoE LLMs, overcoming challenges related to inter-expert and intra-expert imbalances in sample distribution and aggregation mechanisms. 

<br /><br />Summary: <div>
arXiv:2505.03804v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic routing and sparse activation to enhance efficiency and scalability, have achieved higher performance while reducing computational costs. However, these models face significant memory overheads, limiting their practical deployment and broader adoption. Post-training quantization (PTQ), a widely used method for compressing LLMs, encounters severe accuracy degradation and diminished generalization performance when applied to MoE models. This paper investigates the impact of MoE's sparse and dynamic characteristics on quantization and identifies two primary challenges: (1) Inter-expert imbalance, referring to the uneven distribution of samples across experts, which leads to insufficient and biased calibration for less frequently utilized experts; (2) Intra-expert imbalance, arising from MoE's unique aggregation mechanism, which leads to varying degrees of correlation between different samples and their assigned experts. To address these challenges, we propose MoEQuant, a novel quantization framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1) Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that efficiently constructs a calibration set with balanced expert distributions by leveraging the cumulative probabilities of tokens and expert balance metrics as guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates affinities between experts and samples into the quantization process, thereby accurately assessing the impact of individual samples on different experts within the MoE layer. Experiments demonstrate that MoEQuant achieves substantial performance gains (more than 10 points accuracy gain in the HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing</title>
<link>https://arxiv.org/abs/2505.03805</link>
<guid>https://arxiv.org/abs/2505.03805</guid>
<content:encoded><![CDATA[
<div> Keywords: Randomized Uphill Climbing, feature optimization, multivariate time series forecasting, surrogate models, interpretability 

Summary: 
Randomized Uphill Climbing (RUC) has been successful in generating state-of-the-art equity alpha factors for hedge funds. The proposal aims to expand RUC into a versatile feature optimization framework for multivariate time series forecasting. By randomly combining operators from a specific grammar, candidate feature programs are synthesized and assessed using surrogate models on rolling windows. The method leverages nested cross-validation and information theoretic shrinkage to ensure stability and filter out unreliable features. By separating feature discovery from GPU-intensive deep learning, the approach offers faster iteration cycles, reduced energy consumption, and improved interpretability. This methodology not only allows for accurate forecasting but also promotes transparency, enabling various organizations such as energy regulators and climate risk NGOs to make informed decisions without relying on black-box models.

<br /><br />Summary: <div>
arXiv:2505.03805v1 Announce Type: new 
Abstract: Randomized Uphill Climbing is a lightweight, stochastic search heuristic that has delivered state of the art equity alpha factors for quantitative hedge funds. I propose to generalize RUC into a model agnostic feature optimization framework for multivariate time series forecasting. The core idea is to synthesize candidate feature programs by randomly composing operators from a domain specific grammar, score candidates rapidly with inexpensive surrogate models on rolling windows, and filter instability via nested cross validation and information theoretic shrinkage. By decoupling feature discovery from GPU heavy deep learning, the method promises faster iteration cycles, lower energy consumption, and greater interpretability. Societal relevance: accurate, transparent forecasting tools empower resource constrained institutions, energy regulators, climate risk NGOs to make data driven decisions without proprietary black box models.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.03806</link>
<guid>https://arxiv.org/abs/2505.03806</guid>
<content:encoded><![CDATA[
<div> Keywords: Perception-Informed Neural Networks, Physics-Informed Neural Networks, Mixture of Experts Informed Neural Networks, Transformed-Knowledge Informed Neural Networks, Fuzzy-Informed Neural Networks<br />
Summary: <br />
Perception-Informed Neural Networks (PrINNs) integrate perception-based information into neural networks, accommodating known and unknown physics laws. This framework extends Physics-Informed Neural Networks (PINNs) by incorporating diverse perception precisation forms like probability distribution and fuzzy graph. PrINNs enable neural networks to model complex systems by combining expert knowledge and perception through loss functions. Mixture of Experts Informed Neural Networks (MOEINNs) fuses heterogeneous expert information, while Transformed-Knowledge Informed Neural Networks (TKINNs) enhance model performance by incorporating meta-information. Fuzzy-Informed Neural Networks (FINNs) leverage fuzzy logic constraints in deep learning, enabling online training without defuzzification. PrINNs bridge traditional physics-based modeling with modern data-driven approaches, allowing neural networks to learn from structured laws and flexible rules. This advancement empowers neural networks to operate in uncertain environments, model complex systems, and uncover new differential equations, making PrINNs a valuable tool for computational science and engineering. <br /><br />Summary: <div>
arXiv:2505.03806v1 Announce Type: new 
Abstract: This article introduces Perception-Informed Neural Networks (PrINNs), a framework designed to incorporate perception-based information into neural networks, addressing both systems with known and unknown physics laws or differential equations. Moreover, PrINNs extend the concept of Physics-Informed Neural Networks (PINNs) and their variants, offering a platform for the integration of diverse forms of perception precisiation, including singular, probability distribution, possibility distribution, interval, and fuzzy graph. In fact, PrINNs allow neural networks to model dynamical systems by integrating expert knowledge and perception-based information through loss functions, enabling the creation of modern data-driven models. Some of the key contributions include Mixture of Experts Informed Neural Networks (MOEINNs), which combine heterogeneous expert knowledge into the network, and Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the incorporation of meta-information for enhanced model performance. Additionally, Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural networks leverage fuzzy logic constraints within a deep learning architecture, allowing online training without pre-training and eliminating the need for defuzzification. PrINNs represent a significant step forward in bridging the gap between traditional physics-based modeling and modern data-driven approaches, enabling neural networks to learn from both structured physics laws and flexible perception-based rules. This approach empowers neural networks to operate in uncertain environments, model complex systems, and discover new forms of differential equations, making PrINNs a powerful tool for advancing computational science and engineering.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data</title>
<link>https://arxiv.org/abs/2505.03808</link>
<guid>https://arxiv.org/abs/2505.03808</guid>
<content:encoded><![CDATA[
<div> Keywords: harmful algal blooms, remote sensing, artificial intelligence, machine learning, environmental monitoring

Summary: 
This research presents a methodology for detecting harmful algal blooms using a combination of remote sensing data and artificial intelligence models. The approach integrates Copernicus Sentinel-2 optical imagery, the Copernicus Digital Elevation Model, and NOAA's High-Resolution Rapid Refresh climate data obtained through platforms like Google Earth Engine and Microsoft Planetary Computer. Key features for classification include Sentinel-2 bands, altitude, temperature, wind, longitude, and latitude. The methodology involves a combination of tree-based models and a neural network ensemble to classify algal bloom severity. While the tree models show strong performance individually, the neural network adds robustness, highlighting the effectiveness of deep learning models in utilizing diverse remote sensing inputs. The code is available for adaptation and practical implementation, demonstrating the potential for global application in monitoring harmful algal blooms. 

<br /><br />Summary: <div>
arXiv:2505.03808v1 Announce Type: new 
Abstract: Harmful algal blooms are a growing threat to inland water quality and public health worldwide, creating an urgent need for efficient, accurate, and cost-effective detection methods. This research introduces a high-performing methodology that integrates multiple open-source remote sensing data with advanced artificial intelligence models. Key data sources include Copernicus Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently retrieved using platforms like Google Earth Engine (GEE) and Microsoft Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the altitude from the elevation model, the temperature and wind from NOAA as well as the longitude and latitude were the most important features. The approach combines two types of machine learning models, tree-based models and a neural network, into an ensemble for classifying algal bloom severity. While the tree models performed strongly on their own, incorporating a neural network added robustness and demonstrated how deep learning models can effectively use diverse remote sensing inputs. The method leverages high-resolution satellite imagery and AI-driven analysis to monitor algal blooms dynamically, and although initially developed for a NASA competition in the U.S., it shows potential for global application. The complete code is available for further adaptation and practical implementation, illustrating the convergence of remote sensing data and AI to address critical environmental challenges (https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dynamic Data Selection Meets Data Augmentation</title>
<link>https://arxiv.org/abs/2505.03809</link>
<guid>https://arxiv.org/abs/2505.03809</guid>
<content:encoded><![CDATA[
<div> Joint distribution, local density, multimodal semantic consistency, dynamic data selection, data augmentation

Summary:
The article introduces a novel online data training framework that combines dynamic data selection and data augmentation for improved training efficiency and performance. By estimating each sample's joint distribution of local density and multimodal semantic consistency, the method can select augmentation-suitable samples while filtering out noisy or ambiguous data. This results in a significant reduction in dataset size without compromising model generalization. Experimental results show that the proposed approach outperforms existing state-of-the-art methods on various benchmark datasets and architectures, such as achieving a 50% reduction in training costs on ImageNet-1k with no loss in performance. Additionally, the method enhances noise resistance and improves model robustness, showcasing its practical utility in real-world scenarios. 

<br /><br />Summary: <div>
arXiv:2505.03809v1 Announce Type: new 
Abstract: Dynamic data selection aims to accelerate training with lossless performance. However, reducing training data inherently limits data diversity, potentially hindering generalization. While data augmentation is widely used to enhance diversity, it is typically not optimized in conjunction with selection. As a result, directly combining these techniques fails to fully exploit their synergies. To tackle the challenge, we propose a novel online data training framework that, for the first time, unifies dynamic data selection and augmentation, achieving both training efficiency and enhanced performance. Our method estimates each sample's joint distribution of local density and multimodal semantic consistency, allowing for the targeted selection of augmentation-suitable samples while suppressing the inclusion of noisy or ambiguous data. This enables a more significant reduction in dataset size without sacrificing model generalization. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches on various benchmark datasets and architectures, e.g., reducing 50\% training costs on ImageNet-1k with lossless performance. Furthermore, our approach enhances noise resistance and improves model robustness, reinforcing its practical utility in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free</title>
<link>https://arxiv.org/abs/2505.03810</link>
<guid>https://arxiv.org/abs/2505.03810</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Post-Training Quantization, Walsh-Hadamard transform, Grouped Sequency-arranged Rotation, performance improvement

Summary:<br />
- The article discusses the deployment challenges faced by Large Language Models (LLMs) due to high computational costs and the potential solution offered by Post-Training Quantization (PTQ).
- Existing rotation-based methods struggle at very low bit-widths like 2-bit, prompting the need for a novel approach to construct an improved rotation matrix without the requirement for additional training.
- The novel approach leverages the Walsh-Hadamard transform with sequency ordering to cluster similar frequency components and reduce quantization error compared to standard Hadamard matrices, thereby significantly enhancing performance.
- The proposed Grouped Sequency-arranged Rotation (GSR) utilizes block-diagonal matrices with smaller Walsh blocks to isolate outlier impacts and achieve performance comparable to optimization-based methods.
- The method demonstrates robust performance on reasoning tasks and improves Perplexity (PPL) scores on WikiText-2, showcasing its versatility and effectiveness in enhancing existing learned rotation techniques.<br /> 
Summary: <div>
arXiv:2505.03810v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face deployment challenges due to high computational costs, and while Post-Training Quantization (PTQ) offers a solution, existing rotation-based methods struggle at very low bit-widths like 2-bit. We introduce a novel, training-free approach to construct an improved rotation matrix, addressing the limitations of current methods. The key contributions include leveraging the Walsh-Hadamard transform with sequency ordering, which clusters similar frequency components to reduce quantization error compared to standard Hadamard matrices, significantly improving performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices with smaller Walsh blocks, effectively isolating outlier impacts and achieving performance comparable to optimization-based methods without requiring any training. Our method demonstrates robust performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our method also enhances results even when applied over existing learned rotation techniques.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior</title>
<link>https://arxiv.org/abs/2505.03811</link>
<guid>https://arxiv.org/abs/2505.03811</guid>
<content:encoded><![CDATA[
<div> ScarceGAN, extremely rare samples, longitudinal telemetry data, small label prior, severe scarcity in positive class, multi-class negative samples, unlabelled data, semi-supervised GAN, weakly labelled multi-class negative samples, risky players identification, skill gaming, recall benchmark, positive imbalanced class identification, rare attack class identification, KDDCUP99 challenge
<br />
<br />
Summary: ScarceGAN addresses the identification of extremely rare samples in multi-dimensional longitudinal telemetry data with small label prior. It deals with severe scarcity in the positive class, multi-class negative samples, and unlabelled data. By re-formulating semi-supervised GAN to accommodate weakly labelled multi-class negative samples, ScarceGAN improves recall to over 85% for the scarce class, surpassing vanilla GAN by 60%. It outperforms benchmarks in identifying positive imbalanced classes and achieves a new benchmark in identifying rare attack classes in the KDDCUP99 dataset. <div>
arXiv:2505.03811v1 Announce Type: new 
Abstract: This paper introduces ScarceGAN which focuses on identification of extremely rare or scarce samples from multi-dimensional longitudinal telemetry data with small and weak label prior. We specifically address: (i) severe scarcity in positive class, stemming from both underlying organic skew in the data, as well as extremely limited labels; (ii) multi-class nature of the negative samples, with uneven density distributions and partially overlapping feature distributions; and (iii) massively unlabelled data leading to tiny and weak prior on both positive and negative classes, and possibility of unseen or unknown behavior in the unlabelled set, especially in the negative class. Although related to PU learning problems, we contend that knowledge (or lack of it) on the negative class can be leveraged to learn the compliment of it (i.e., the positive class) better in a semi-supervised manner. To this effect, ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled multi-class negative samples and the available positive samples. It relaxes the supervised discriminator's constraint on exact differentiation between negative samples by introducing a 'leeway' term for samples with noisy prior. We propose modifications to the cost objectives of discriminator, in supervised and unsupervised path as well as that of the generator. For identifying risky players in skill gaming, this formulation in whole gives us a recall of over 85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very minimal verbosity in the unknown space. Further ScarceGAN outperforms the recall benchmarks established by recent GAN based specialized models for the positive imbalanced class identification and establishes a new benchmark in identifying one of rare attack classes (0.09%) in the intrusion dataset from the KDDCUP99 challenge.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications</title>
<link>https://arxiv.org/abs/2505.03812</link>
<guid>https://arxiv.org/abs/2505.03812</guid>
<content:encoded><![CDATA[
<div> Keywords: Information Filtering Networks, Higher-order networks, Triangulated Maximally Filtered Graph, Maximally Filtered Clique Forest, Graphical modeling<br />
Summary: <br />
The article presents a review of Information Filtering Networks (IFNs), emphasizing their theoretical foundations, construction methodologies, and applications in fields like finance, biology, psychology, and artificial intelligence. IFNs are unique higher-order networks that generate simplicial complexes, offering sparse yet locally dense and interpretable structures. They address challenges in high-dimensional data-driven modeling and are particularly useful in graphical modeling, enabling accurate estimation of sparse inverse covariance matrices. IFNs also improve interpretability, computational efficiency, and predictive performance. The review highlights advanced formulations such as the Triangulated Maximally Filtered Graph and the Maximally Filtered Clique Forest, showcasing how IFNs bridge classical network theory with contemporary data-driven paradigms. Additionally, recent developments integrate IFNs with machine learning and deep learning, suggesting their potential to shape deep learning model architectures. <div>
arXiv:2505.03812v1 Announce Type: new 
Abstract: Information Filtering Networks (IFNs) provide a powerful framework for modeling complex systems through globally sparse yet locally dense and interpretable structures that capture multivariate dependencies. This review offers a comprehensive account of IFNs, covering their theoretical foundations, construction methodologies, and diverse applications. Tracing their origins from early network-based models to advanced formulations such as the Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique Forest (MFCF), the paper highlights how IFNs address key challenges in high-dimensional data-driven modeling. IFNs and their construction methodologies are intrinsically higher-order networks that generate simplicial complexes-structures that are only now becoming popular in the broader literature. Applications span fields including finance, biology, psychology, and artificial intelligence, where IFNs improve interpretability, computational efficiency, and predictive performance. Special attention is given to their role in graphical modeling, where IFNs enable the estimation of sparse inverse covariance matrices with greater accuracy and scalability than traditional approaches like Graphical LASSO. Finally, the review discusses recent developments that integrate IFNs with machine learning and deep learning, underscoring their potential not only to bridge classical network theory with contemporary data-driven paradigms, but also to shape the architectures of deep learning models themselves.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Program Semantic Inequivalence Game with Large Language Models</title>
<link>https://arxiv.org/abs/2505.03818</link>
<guid>https://arxiv.org/abs/2505.03818</guid>
<content:encoded><![CDATA[
<div> training data, large language models, code reasoning, synthetic data generation, vulnerability detection 

Summary:
Large Language Models (LLMs) excel in everyday coding tasks but struggle with more complex tasks requiring deep understanding of program semantics. To address this, a method is proposed to synthetically generate code reasoning training data using a semantic inequivalence game (SInQ). In this game, a generator agent creates program variants that differ semantically, while an evaluator agent identifies input examples causing divergence in behavior between the original programs and variants. Through semi-adversarial training, this approach allows for unlimited improvement through self-play with infinite computational resources. Evaluation on various benchmarks, including cross-language vulnerability detection and Python identifier swap, demonstrates the effectiveness of the method in enhancing LLM performance. The code and synthetic data generated are released for replication and fine-tuning of LLMs. 

<br /><br />Summary: <div>
arXiv:2505.03818v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can achieve strong performance on everyday coding tasks, but they can fail on complex tasks that require non-trivial reasoning about program semantics. Finding training examples to teach LLMs to solve these tasks can be challenging.
  In this work, we explore a method to synthetically generate code reasoning training data based on a semantic inequivalence game SInQ: a generator agent creates program variants that are semantically distinct, derived from a dataset of real-world programming tasks, while an evaluator agent has to identify input examples that cause the original programs and the generated variants to diverge in their behaviour, with the agents training each other semi-adversarially. We prove that this setup enables theoretically unlimited improvement through self-play in the limit of infinite computational resources.
  We evaluated our approach on multiple code generation and understanding benchmarks, including cross-language vulnerability detection (Lu et al., 2021), where our method improves vulnerability detection in C/C++ code despite being trained exclusively on Python code, and the challenging Python builtin identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas modern LLMs still struggle with this benchmark, our approach yields substantial improvements.
  We release the code needed to replicate the experiments, as well as the generated synthetic data, which can be used to fine-tune LLMs.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focus on the Likely: Test-time Instance-based Uncertainty Removal</title>
<link>https://arxiv.org/abs/2505.03819</link>
<guid>https://arxiv.org/abs/2505.03819</guid>
<content:encoded><![CDATA[
<div> test-time fine-tuning, model predictions, uncertain, gradient descent, experimental evaluation
<br />
Summary: 
The article introduces two new test-time fine-tuning methods to enhance uncertain model predictions without needing additional data. Instead of simply choosing the most likely class during inference, the methods incorporate a focus on likely classes and use single-step gradient descent to adjust predictions for high uncertainty cases. This helps align predictions with the goal of assigning lower probability to less plausible outcomes. The theoretical discussion explores the impact of shared and non-shared features among focus classes. Experimental results demonstrate improved accuracy on samples with high decision uncertainty across a variety of models in text and image domains, using the same hyperparameters. <div>
arXiv:2505.03819v1 Announce Type: new 
Abstract: We propose two novel test-time fine-tuning methods to improve uncertain model predictions. Our methods require no auxiliary data and use the given test instance only. Instead of performing a greedy selection of the most likely class to make a prediction, we introduce an additional focus on the likely classes step during inference. By applying a single-step gradient descent, we refine predictions when an initial forward pass indicates high uncertainty. This aligns predictions more closely with the ideal of assigning zero probability to less plausible outcomes. Our theoretical discussion provides a deeper understanding highlighting the impact on shared and non-shared features among (focus) classes. The experimental evaluation highlights accuracy gains on samples exhibiting high decision uncertainty for a diverse set of models from both the text and image domain using the same hyperparameters.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction</title>
<link>https://arxiv.org/abs/2505.03822</link>
<guid>https://arxiv.org/abs/2505.03822</guid>
<content:encoded><![CDATA[
<div> Regularized second-order latent factor, QoS data, cloud service selection, low-rank representation, Hessian-vector product
Summary: 
The paper introduces the double regularized second-order latent factor (DRSLF) model as a novel approach for improving Quality-of-Service (QoS) prediction accuracy in cloud service selection. By integrating L1-norm and L2-norm regularization terms, the DRSLF model enhances the low-rank representation performance of high-dimensional and incomplete QoS matrices. Additionally, the model incorporates second-order information by calculating the Hessian-vector product in each conjugate gradient step. Experimental results on real-world response-time QoS datasets show that the DRSLF model outperforms two baseline models in terms of low-rank representation capability, highlighting its efficacy in addressing the challenges of QoS data analysis in cloud service selection.<br /><br />Summary: <div>
arXiv:2505.03822v1 Announce Type: new 
Abstract: Quality-of-Service (QoS) data plays a crucial role in cloud service selection. Since users cannot access all services, QoS can be represented by a high-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA) models have been proven effective as low-rank representation techniques for addressing this issue. However, most LFA models rely on first-order optimizers and use L2-norm regularization, which can lead to lower QoS prediction accuracy. To address this issue, this paper proposes a double regularized second-order latent factor (DRSLF) model with two key ideas: a) integrating L1-norm and L2-norm regularization terms to enhance the low-rank representation performance; b) incorporating second-order information by calculating the Hessian-vector product in each conjugate gradient step. Experimental results on two real-world response-time QoS datasets demonstrate that DRSLF has a higher low-rank representation capability than two baselines.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments</title>
<link>https://arxiv.org/abs/2505.03825</link>
<guid>https://arxiv.org/abs/2505.03825</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-dimensional time series, deep learning, contrastive tensor factorization, data-efficient, classification

Summary: 
The article introduces a new framework called Intelligently Augmented Contrastive Tensor Factorization (ITA-CTF) for learning representations from multi-dimensional time series data with low training data availability. This framework incorporates a contrastive loss optimization that induces similarity learning and class-awareness into the representations learned by the tensor factorization module. It also utilizes an ITA module to generate targeted augmentations that highlight intra-class patterns in the data, enabling the recognition of complex intra-class variations. By intelligently mixing patterns between class prototypes and query samples, the ITA-CTF method improves classification performance by up to 18.7% compared to standard tensor factorization and deep learning benchmarks. The proposed approach shows promise in effectively capturing cross-dimensional dependencies and intra-class variations in real-world systems with limited training data. 

<br /><br />Summary: <div>
arXiv:2505.03825v1 Announce Type: new 
Abstract: Classification of multi-dimensional time series from real-world systems require fine-grained learning of complex features such as cross-dimensional dependencies and intra-class variations-all under the practical challenge of low training data availability. However, standard deep learning (DL) struggles to learn generalizable features in low-data environments due to model overfitting. We propose a versatile yet data-efficient framework, Intelligently Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective representations from multi-dimensional time series. The CTF module learns core explanatory components of the time series (e.g., sensor factors, temporal factors), and importantly, their joint dependencies. Notably, unlike standard tensor factorization (TF), the CTF module incorporates a new contrastive loss optimization to induce similarity learning and class-awareness into the learnt representations for better classification performance. To strengthen this contrastive learning, the preceding ITA module generates targeted but informative augmentations that highlight realistic intra-class patterns in the original data, while preserving class-wise properties. This is achieved by dynamically sampling a "soft" class prototype to guide the warping of each query data sample, which results in an augmentation that is intelligently pattern-mixed between the "soft" class prototype and the query sample. These augmentations enable the CTF module to recognize complex intra-class variations despite the limited original training data, and seek out invariant class-wise properties for accurate classification performance. The proposed method is comprehensively evaluated on five different classification tasks. Compared to standard TF and several DL benchmarks, notable performance improvements up to 18.7% were achieved.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation</title>
<link>https://arxiv.org/abs/2505.03827</link>
<guid>https://arxiv.org/abs/2505.03827</guid>
<content:encoded><![CDATA[
<div> keywords:
social media, stress, stressors, meta-learning, dataset

Summary:
This study introduces a new task in social media analysis aimed at estimating specific stressors from users' posts. Traditional classification methods have limitations due to the diversity and evolving nature of stressors. The proposed meta-learning based framework utilizes a meta-knowledge inheritance mechanism to effectively estimate stressors with few examples. This approach not only generalizes well to new stressors but also prevents catastrophic forgetting when adapting to them. The experimental results demonstrate that the model outperforms existing baselines. Additionally, a social media-based stressor estimation dataset has been constructed and made public to train AI models for improving human well-being. The dataset is available on Kaggle and Hugging Face platforms.  <br /><br />Summary: <div>
arXiv:2505.03827v1 Announce Type: new 
Abstract: Stress haunts people in modern society, which may cause severe health issues if left unattended. With social media becoming an integral part of daily life, leveraging social media to detect stress has gained increasing attention. While the majority of the work focuses on classifying stress states and stress categories, this study introduce a new task aimed at estimating more specific stressors (like exam, writing paper, etc.) through users' posts on social media. Unfortunately, the diversity of stressors with many different classes but a few examples per class, combined with the consistent arising of new stressors over time, hinders the machine understanding of stressors. To this end, we cast the stressor estimation problem within a practical scenario few-shot learning setting, and propose a novel meta-learning based stressor estimation framework that is enhanced by a meta-knowledge inheritance mechanism. This model can not only learn generic stressor context through meta-learning, but also has a good generalization ability to estimate new stressors with little labeled data. A fundamental breakthrough in our approach lies in the inclusion of the meta-knowledge inheritance mechanism, which equips our model with the ability to prevent catastrophic forgetting when adapting to new stressors. The experimental results show that our model achieves state-of-the-art performance compared with the baselines. Additionally, we construct a social media-based stressor estimation dataset that can help train artificial intelligence models to facilitate human well-being. The dataset is now public at \href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\underline{Kaggle}} and \href{https://huggingface.co/datasets/XinWangcs/Stressor}{\underline{Hugging Face}}.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics</title>
<link>https://arxiv.org/abs/2505.03849</link>
<guid>https://arxiv.org/abs/2505.03849</guid>
<content:encoded><![CDATA[
<div> Monte Carlo sampling, high-dimensional parameter scans, non-linear dimensionality reduction, formal verification methods, numerical algorithms <br />
Summary: Inverse problems in nuclear fusion and high-energy astrophysics involve large uncertainties and high-dimensional parameter spaces. This paper suggests using Monte Carlo sampling with dimensionality reduction techniques to analyze these problems but highlights the need for ensuring the resulting parameter combinations are physically valid. The proposed hybrid approach combines formal verification methods for numerical algorithms to restrict parameter spaces with provably correct mathematical and physical properties. By considering experimental uncertainties and uncertainties in physical models, this approach aims to optimize tokamak reactor geometries and infer black hole parameters accurately. This strategy offers a way to balance mathematical consistency with respect to physical reality in solving complex inverse problems. <br /><br /> <div>
arXiv:2505.03849v1 Announce Type: new 
Abstract: Many inverse problems in nuclear fusion and high-energy astrophysics research, such as the optimization of tokamak reactor geometries or the inference of black hole parameters from interferometric images, necessitate high-dimensional parameter scans and large ensembles of simulations to be performed. Such inverse problems typically involve large uncertainties, both in the measurement parameters being inverted and in the underlying physics models themselves. Monte Carlo sampling, when combined with modern non-linear dimensionality reduction techniques such as autoencoders and manifold learning, can be used to reduce the size of the parameter spaces considerably. However, there is no guarantee that the resulting combinations of parameters will be physically valid, or even mathematically consistent. In this position paper, we advocate adopting a hybrid approach that leverages our recent advances in the development of formal verification methods for numerical algorithms, with the goal of constructing parameter space restrictions with provable mathematical and physical correctness properties, whilst nevertheless respecting both experimental uncertainties and uncertainties in the underlying physical processes.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning: a Lecture Note</title>
<link>https://arxiv.org/abs/2505.03861</link>
<guid>https://arxiv.org/abs/2505.03861</guid>
<content:encoded><![CDATA[
<div> classification, machine learning, neural networks, unsupervised learning, reinforcement learning

Summary:
This lecture note aims to provide foundational concepts in machine learning for early-year master's and PhD students in data science. It covers basic ideas such as loss formulation, backpropagation, and stochastic gradient descent. The note explores the probabilistic approach to unsupervised learning, including directed latent variable models and generative adversarial networks. It also discusses additional topics like reinforcement learning, ensemble methods, and meta-learning. By studying this material, students will have a solid understanding of machine learning principles and be prepared for advanced topics in artificial intelligence. 

<br /><br />Summary: <div>
arXiv:2505.03861v1 Announce Type: new 
Abstract: This lecture note is intended to prepare early-year master's and PhD students in data science or a related discipline with foundational ideas in machine learning. It starts with basic ideas in modern machine learning with classification as a main target task. These basic ideas include loss formulation, backpropagation, stochastic gradient descent, generalization, model selection as well as fundamental blocks of artificial neural networks. Based on these basic ideas, the lecture note explores in depth the probablistic approach to unsupervised learning, covering directed latent variable models, product of experts, generative adversarial networks and autoregressive models. Finally, the note ends by covering a diverse set of further topics, such as reinforcement learning, ensemble methods and meta-learning. After reading this lecture note, a student should be ready to embark on studying and researching more advanced topics in machine learning and more broadly artificial intelligence.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Anomalies with Tensor Networks</title>
<link>https://arxiv.org/abs/2505.03911</link>
<guid>https://arxiv.org/abs/2505.03911</guid>
<content:encoded><![CDATA[
<div> Tensor networks, anomaly detection, explainable, real-valued data, tree tensor networks <br />
<br />
Summary: 
This study explores the application of tensor networks in anomaly detection, extending the framework to real-valued data domains and introducing tree tensor networks for explainable anomaly detection. The research demonstrates the effectiveness of these methods through three benchmark problems, showcasing their predictive performance against baseline models and their ability to explain anomalies within the data. By successfully applying tensor networks to a broader range of potential problems, this study paves the way for future advancements in utilizing more complex tensor network architectures for various applications. <div>
arXiv:2505.03911v1 Announce Type: new 
Abstract: Tensor networks, a class of variational quantum many-body wave functions have attracted considerable research interest across many disciplines, including classical machine learning. Recently, Aizpurua et al. demonstrated explainable anomaly detection with matrix product states on a discrete-valued cyber-security task, using quantum-inspired methods to gain insight into the learned model and detected anomalies. Here, we extend this framework to real-valued data domains. We furthermore introduce tree tensor networks for the task of explainable anomaly detection. We demonstrate these methods with three benchmark problems, show adequate predictive performance compared to several baseline models and both tensor network architectures' ability to explain anomalous samples. We thereby extend the application of tensor networks to a broader class of potential problems and open a pathway for future extensions to more complex tensor network architectures.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAND: One-Shot Feature Selection with Additive Noise Distortion</title>
<link>https://arxiv.org/abs/2505.03923</link>
<guid>https://arxiv.org/abs/2505.03923</guid>
<content:encoded><![CDATA[
<div> feature selection, neural network training, automatic clustering, Gaussian noise, machine learning

Summary:
This article introduces a novel feature selection layer for neural networks that automatically identifies and selects the most informative features during training. The layer operates by applying a simple mathematical formula that induces a clustering effect, driving some gains to select informative features and discarding redundant ones through noise distortion and gain normalization. Despite its simplicity, the method achieves state-of-the-art performance on benchmark and real-world datasets without the need for hyperparameter search or retraining. Theoretical analysis in linear regression further supports its effectiveness, demonstrating that simplicity and performance can coexist in feature selection for machine learning. <div>
arXiv:2505.03923v1 Announce Type: new 
Abstract: Feature selection is a critical step in data-driven applications, reducing input dimensionality to enhance learning accuracy, computational efficiency, and interpretability. Existing state-of-the-art methods often require post-selection retraining and extensive hyperparameter tuning, complicating their adoption. We introduce a novel, non-intrusive feature selection layer that, given a target feature count $k$, automatically identifies and selects the $k$ most informative features during neural network training. Our method is uniquely simple, requiring no alterations to the loss function, network architecture, or post-selection retraining. The layer is mathematically elegant and can be fully described by: \begin{align} \nonumber \tilde{x}_i = a_i x_i + (1-a_i)z_i \end{align} where $x_i$ is the input feature, $\tilde{x}_i$ the output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that $\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect, driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the rest to $0$ (discarding redundant ones) via weighted noise distortion and gain normalization. Despite its extreme simplicity, our method delivers state-of-the-art performance on standard benchmark datasets and a novel real-world dataset, outperforming or matching existing approaches without requiring hyperparameter search for $k$ or retraining. Theoretical analysis in the context of linear regression further validates its efficacy. Our work demonstrates that simplicity and performance are not mutually exclusive, offering a powerful yet straightforward tool for feature selection in machine learning.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading</title>
<link>https://arxiv.org/abs/2505.03949</link>
<guid>https://arxiv.org/abs/2505.03949</guid>
<content:encoded><![CDATA[
<div> Keywords: automated stock trading, deep learning framework, Convolutional Neural Network, Long Short-Term Memory, Deep Q-Network<br />
Summary: 
This project proposes an integrated deep learning framework for automated stock trading that addresses challenges faced by traditional methods and direct reinforcement learning. The framework combines a Convolutional Neural Network (CNN) to identify patterns in technical indicators, a Long Short-Term Memory (LSTM) network to capture temporal dependencies, and a Deep Q-Network (DQN) agent for learning the optimal trading policy. The CNN processes technical indicators as images, while the LSTM captures temporal dependencies in price history and indicators. The DQN agent uses features extracted by the CNN and LSTM to determine the optimal trading actions of buy, sell, or hold. By combining these components, the framework aims to overcome market noise, complexity, and generalization issues in automated stock trading.<br /><br />Summary: <div>
arXiv:2505.03949v1 Announce Type: new 
Abstract: This project addresses the challenge of automated stock trading, where traditional methods and direct reinforcement learning (RL) struggle with market noise, complexity, and generalization. Our proposed solution is an integrated deep learning framework combining a Convolutional Neural Network (CNN) to identify patterns in technical indicators formatted as images, a Long Short-Term Memory (LSTM) network to capture temporal dependencies across both price history and technical indicators, and a Deep Q-Network (DQN) agent which learns the optimal trading policy (buy, sell, hold) based on the features extracted by the CNN and LSTM.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sufficient Decision Proxies for Decision-Focused Learning</title>
<link>https://arxiv.org/abs/2505.03953</link>
<guid>https://arxiv.org/abs/2505.03953</guid>
<content:encoded><![CDATA[
<div> machine learning, optimization, decision-focused learning, uncertainty, predictive model

Summary:
This paper explores the application of decision-focused learning in optimization problems under uncertainty with contextual data. It examines the validity of assumptions regarding the prediction of uncertain parameters, either as single values or as distributions, based on problem properties. The study presents effective decision proxies for decision-focused learning, balancing decision quality and the complexity of the learning task. Experimental results demonstrate the effectiveness of the proposed approaches in problems involving continuous and discrete variables, as well as uncertainty in the objective function and constraints.<br /><br />Summary: <div>
arXiv:2505.03953v1 Announce Type: new 
Abstract: When solving optimization problems under uncertainty with contextual data, utilizing machine learning to predict the uncertain parameters is a popular and effective approach. Decision-focused learning (DFL) aims at learning a predictive model such that decision quality, instead of prediction accuracy, is maximized. Common practice here is to predict a single value for each uncertain parameter, implicitly assuming that there exists a (single-scenario) deterministic problem approximation (proxy) that is sufficient to obtain an optimal decision. Other work assumes the opposite, where the underlying distribution needs to be estimated. However, little is known about when either choice is valid. This paper investigates for the first time problem properties that justify using either assumption. Using this, we present effective decision proxies for DFL, with very limited compromise on the complexity of the learning task. We show the effectiveness of presented approaches in experiments on problems with continuous and discrete variables, as well as uncertainty in the objective function and in the constraints.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation</title>
<link>https://arxiv.org/abs/2505.03955</link>
<guid>https://arxiv.org/abs/2505.03955</guid>
<content:encoded><![CDATA[
<div> FlowRec, hierarchical forecasting, reconciliation, network flow optimization, structured relationships
<br />
Summary: FlowRec introduces a new method for hierarchical forecasting reconciliation, addressing the issue of ensuring structural relationships between forecast values in a hierarchy. It reformulates reconciliation as a network flow optimization, allowing forecasting on generalized network structures. It proves polynomial-time solvability for certain loss functions and achieves significantly improved complexity over existing methods. FlowRec extends previous approaches to handle general networks and dynamic scenarios, providing efficient updates with optimality guarantees. It also offers error-bounded approximate reconciliation for time-critical applications. Experimental results demonstrate that FlowRec improves accuracy, runtime, and memory usage, making it a powerful tool for large-scale hierarchical forecasting applications. 
<br /><br />Summary: <div>
arXiv:2505.03955v1 Announce Type: new 
Abstract: Hierarchical forecasting with reconciliation requires forecasting values of a hierarchy (e.g.~customer demand in a state and district), such that forecast values are linked (e.g.~ district forecasts should add up to the state forecast). Basic forecasting provides no guarantee for these desired structural relationships. Reconciliation addresses this problem, which is crucial for organizations requiring coherent predictions across multiple aggregation levels. Current methods like minimum trace (MinT) are mostly limited to tree structures and are computationally expensive. We introduce FlowRec, which reformulates hierarchical forecast reconciliation as a network flow optimization, enabling forecasting on generalized network structures. While reconciliation under the $\ell_0$ norm is NP-hard, we prove polynomial-time solvability for all $\ell_{p > 0}$ norms and , for any strictly convex and continuously differentiable loss function. For sparse networks, FlowRec achieves $O(n^2\log n)$ complexity, significantly improving upon MinT's $O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general networks, replacing MinT's error-covariance estimation step with direct network structural information. A key novelty of our approach is its handling of dynamic scenarios: while traditional methods recompute both base forecasts and reconciliation, FlowRec provides efficient localised updates with optimality guarantees. Monotonicity ensures that when forecasts improve incrementally, the initial reconciliation remains optimal. We also establish efficient, error-bounded approximate reconciliation, enabling fast updates in time-critical applications. Experiments on both simulated and real benchmarks demonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage by 5-7x. These results establish FlowRec as a powerful tool for large-scale hierarchical forecasting applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Call for Action: towards the next generation of symbolic regression benchmark</title>
<link>https://arxiv.org/abs/2505.03977</link>
<guid>https://arxiv.org/abs/2505.03977</guid>
<content:encoded><![CDATA[
<div> benchmarking, symbolic regression, evaluation metrics, model complexity, energy consumption

Summary:
Symbolic Regression (SR) is a powerful technique for discovering interpretable mathematical expressions. The updated version of SRBench expands the benchmark by including more methods, refining evaluation metrics, and using improved visualizations. Trade-offs between model complexity, accuracy, and energy consumption are analyzed, showing that no single algorithm dominates across all datasets. A call for action is made to maintain and evolve SRBench as a living benchmark for symbolic regression. Standardizing hyperparameter tuning, execution constraints, and resource allocation is proposed, along with deprecation criteria and best practices for improving SR algorithms such as adaptive hyperparameter tuning and energy-efficient implementations. <div>
arXiv:2505.03977v1 Announce Type: new 
Abstract: Symbolic Regression (SR) is a powerful technique for discovering interpretable mathematical expressions. However, benchmarking SR methods remains challenging due to the diversity of algorithms, datasets, and evaluation criteria. In this work, we present an updated version of SRBench. Our benchmark expands the previous one by nearly doubling the number of evaluated methods, refining evaluation metrics, and using improved visualizations of the results to understand the performances. Additionally, we analyze trade-offs between model complexity, accuracy, and energy consumption. Our results show that no single algorithm dominates across all datasets. We propose a call for action from SR community in maintaining and evolving SRBench as a living benchmark that reflects the state-of-the-art in symbolic regression, by standardizing hyperparameter tuning, execution constraints, and computational resource allocation. We also propose deprecation criteria to maintain the benchmark's relevance and discuss best practices for improving SR algorithms, such as adaptive hyperparameter tuning and energy-efficient implementations.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations</title>
<link>https://arxiv.org/abs/2505.03980</link>
<guid>https://arxiv.org/abs/2505.03980</guid>
<content:encoded><![CDATA[
<div> Maximum Likelihood Estimation, Kalman Filtering, Inverse Variable Method, Recurrent Neural Network, Ornstein-Uhlenbeck process
<br />
Summary:
Statistical methods like Maximum Likelihood Estimation (MLE) have traditionally been used to estimate parameters of stochastic differential equations. However, the rise of deep learning technology has led to the exploration of alternative models like Recurrent Neural Networks (RNN) for parameter estimation. This study compares the accuracy and computational efficiency of MLE and RNN in estimating parameters of the Ornstein-Uhlenbeck process, a commonly used model for probabilistic events like stock prices and temperature fluctuations. Through a series of experiments, the researchers evaluate the performance of both approaches. The results aim to shed light on the potential of leveraging deep learning models in improving parameter estimation in stochastic differential equations. <div>
arXiv:2505.03980v1 Announce Type: new 
Abstract: Stochastic differential equations such as the Ornstein-Uhlenbeck process have long been used to model realworld probablistic events such as stock prices and temperature fluctuations. While statistical methods such as Maximum Likelihood Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have historically been used to estimate the parameters of stochastic differential equations, the recent explosion of deep learning technology suggests that models such as a Recurrent Neural Network (RNN) could produce more precise estimators. We present a series of experiments that compare the estimation accuracy and computational expensiveness of a statistical method (MLE) with a deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation</title>
<link>https://arxiv.org/abs/2505.03983</link>
<guid>https://arxiv.org/abs/2505.03983</guid>
<content:encoded><![CDATA[
<div> DDPMs, Stochastic Localization, Autospeculative Decoding, Parallel Runtime Speedup, Inference Acceleration
<br />
Autospeculative decoding is introduced as an extension of the speculative decoding algorithm to DDPMs, eliminating the need for auxiliary draft models. The theoretical analysis reveals that Autospeculative Decoding achieves a significant parallel runtime speedup of $\tilde{O} (K^{\frac{1}{3}})$ over the sequential DDPM. By leveraging the connection between DDPMs and Stochastic Localization, the study demonstrates that the increments of DDPM satisfy an exchangeability property, facilitating the application of performance optimization techniques from autoregressive models to the diffusion setting. This near-black-box adaptation enables efficient inference-time optimizations in DDPMs. Practical implementation of Autospeculative Decoding shows notable acceleration of DDPM inference in various domains.
<br /><br />Summary: <div>
arXiv:2505.03983v1 Announce Type: new 
Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful tools for generative modeling. However, their sequential computation requirements lead to significant inference-time bottlenecks. In this work, we utilize the connection between DDPMs and Stochastic Localization to prove that, under an appropriate reparametrization, the increments of DDPM satisfy an exchangeability property. This general insight enables near-black-box adaptation of various performance optimization techniques from autoregressive models to the diffusion setting. To demonstrate this, we introduce \emph{Autospeculative Decoding} (ASD), an extension of the widely used speculative decoding algorithm to DDPMs that does not require any auxiliary draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O} (K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM. We also demonstrate that a practical implementation of autospeculative decoding accelerates DDPM inference significantly in various domains.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics</title>
<link>https://arxiv.org/abs/2505.03992</link>
<guid>https://arxiv.org/abs/2505.03992</guid>
<content:encoded><![CDATA[
<div> combinatorics, classification metrics, bias assessment, sample-size bias, machine learning models
Summary:
This article explores the impact of combinatorics on classification metrics used in evaluating machine learning models. It highlights the potential for sample-size bias induced by combinatorics and its implications on assessing bias, especially in social applications with disparate group sizes. The study analyzes the bias in commonly used metrics and proposes a model-agnostic assessment and correction technique to address the issue effectively. Additionally, it emphasizes the importance of handling undefined cases in metric calculations to prevent misleading evaluations. By shedding light on the challenges of combinatorics and probability in evaluation practices, the research contributes to advancing fair and trustworthy classification methods in machine learning. <br /><br />Summary: <div>
arXiv:2505.03992v1 Announce Type: new 
Abstract: Evaluating machine learning models is crucial not only for determining their technical accuracy but also for assessing their potential societal implications. While the potential for low-sample-size bias in algorithms is well known, we demonstrate the significance of sample-size bias induced by combinatorics in classification metrics. This revelation challenges the efficacy of these metrics in assessing bias with high resolution, especially when comparing groups of disparate sizes, which frequently arise in social applications. We provide analyses of the bias that appears in several commonly applied metrics and propose a model-agnostic assessment and correction technique. Additionally, we analyze counts of undefined cases in metric calculations, which can lead to misleading evaluations if improperly handled. This work illuminates the previously unrecognized challenge of combinatorics and probability in standard evaluation practices and thereby advances approaches for performing fair and trustworthy classification methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quiet Feature Learning in Algorithmic Tasks</title>
<link>https://arxiv.org/abs/2505.03997</link>
<guid>https://arxiv.org/abs/2505.03997</guid>
<content:encoded><![CDATA[
<div> phase transitions, Transformer-based language models, algorithmic tasks, internal representations, feature learning

Summary:
This study examines the training of Transformer-based language models on algorithmic tasks and identifies pronounced phase transitions in their loss curves. Unlike expected power-law scaling trends, the models show minimal improvement in validation loss over large compute ranges, followed by a sudden drop. Internal representation analysis reveals the learning of quiet features before the acquisition of loud features, coinciding with the loss decrease. Ablation experiments demonstrate that disrupting a single learned feature significantly impairs performance, indicating their causal role. The findings challenge the assumption that next-token predictive loss directly reflects progress and suggest that critical internal features may develop discreetly before driving rapid performance gains. <div>
arXiv:2505.03997v1 Announce Type: new 
Abstract: We train Transformer-based language models on ten foundational algorithmic tasks and observe pronounced phase transitions in their loss curves that deviate from established power-law scaling trends. Over large ranges of compute, the validation loss barely improves, then abruptly decreases. Probing the models' internal representations reveals the learning of quiet features during the stagnant phase, followed by sudden acquisition of loud features that coincide with the sharp drop in loss. Our ablation experiments show that disrupting a single learned feature can dramatically degrade performance, providing evidence of their causal role in task performance. These findings challenge the prevailing assumption that next-token predictive loss reliably tracks incremental progress; instead, key internal features may be developing below the surface until they coalesce, triggering a rapid performance gain.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Orthogonalization Scaling Laws</title>
<link>https://arxiv.org/abs/2505.04005</link>
<guid>https://arxiv.org/abs/2505.04005</guid>
<content:encoded><![CDATA[
<div> optimizer, muon, hyper-parameters, scaling laws, random matrices
Summary: 
The muon optimizer, seen as a potential replacement for Adam, has gained attention recently. Research has focused on understanding the scaling laws of hyper-parameters like weight decay and learning rate under muon. However, at larger scales, an issue arises with the iterative orthogonalization procedure in muon due to shrinking singular values of random matrices. This paper presents theoretical and empirical evidence of this scaling behavior on random matrices but does not propose a solution. The study highlights a potential limitation of muon at larger scales, emphasizing the need for further investigation to address this issue.<br /><br />Summary: <div>
arXiv:2505.04005v1 Announce Type: new 
Abstract: The muon optimizer has picked up much attention as of late as a possible replacement to the seemingly omnipresent Adam optimizer. Recently, care has been taken to document the scaling laws of hyper-parameters under muon such as weight decay and learning rate. However, at much larger scales the iterative orthogonalization procedure present in muon may suffer a possible issue as the singular values of random matrices shrink with scale. This paper shows this scaling behavior theoretically and empirically on random matrices but does not suggest what to do about it.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks</title>
<link>https://arxiv.org/abs/2505.04046</link>
<guid>https://arxiv.org/abs/2505.04046</guid>
<content:encoded><![CDATA[
arXiv:2505.04046v1 Announce Type: new 
Abstract: Recently, trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods implicitly assume that multi-view data is secure. In practice, however, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view learning models. This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning. To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor. Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them. Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed. Extensive experiments on multi-view classification tasks with adversarial attacks show that our RDML outperforms the state-of-the-art multi-view learning methods by a relatively large margin.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLAMAPIE: Proactive In-Ear Conversation Assistants</title>
<link>https://arxiv.org/abs/2505.04066</link>
<guid>https://arxiv.org/abs/2505.04066</guid>
<content:encoded><![CDATA[
arXiv:2505.04066v1 Announce Type: new 
Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?</title>
<link>https://arxiv.org/abs/2505.04075</link>
<guid>https://arxiv.org/abs/2505.04075</guid>
<content:encoded><![CDATA[
arXiv:2505.04075v1 Announce Type: new 
Abstract: This paper examines whether large language model (LLM) capabilities can continue to advance without additional compute by analyzing the development and role of algorithms used in state-of-the-art LLMs. Motivated by regulatory efforts that have largely focused on restricting access to high-performance hardware, we ask: Can LLMs progress in a compute-constrained environment, and how do algorithmic innovations perform under such conditions?
  To address these questions, we introduce a novel classification framework that distinguishes between compute-dependent innovations -- which yield disproportionate benefits at high compute levels (e.g., the Transformer architecture and mixture-of-experts models) and compute-independent innovations, which improve efficiency across all compute scales (e.g., rotary positional encoding, FlashAttention, or layer normalization). We quantify these contributions using a metric called compute-equivalent gain (CEG), which estimates the additional compute that would be required to achieve similar improvements without these algorithmic advancements.
  To validate this framework, we conduct small-scale training experiments with a scaled-down GPT-2 model. Our results confirm that compute-independent advancements yield meaningful performance gains even in resource-constrained settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast, compute-dependent advancements provided little benefit or even degraded performance at the small scale, reinforcing the importance of compute availability for certain algorithmic gains.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training</title>
<link>https://arxiv.org/abs/2505.04083</link>
<guid>https://arxiv.org/abs/2505.04083</guid>
<content:encoded><![CDATA[
arXiv:2505.04083v1 Announce Type: new 
Abstract: Graph neural networks have emerged as a potent class of neural networks capable of leveraging the connectivity and structure of real-world graphs to learn intricate properties and relationships between nodes. Many real-world graphs exceed the memory capacity of a GPU due to their sheer size, and using GNNs on them requires techniques such as mini-batch sampling to scale. However, this can lead to reduced accuracy in some cases, and sampling and data transfer from the CPU to the GPU can also slow down training. On the other hand, distributed full-graph training suffers from high communication overhead and load imbalance due to the irregular structure of graphs. We propose Plexus, a three-dimensional (3D) parallel approach for full-graph training that tackles these issues and scales to billion-edge graphs. Additionally, we introduce optimizations such as a permutation scheme for load balancing, and a performance model to predict the optimal 3D configuration. We evaluate Plexus on several graph datasets and show scaling results for up to 2048 GPUs on Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus achieves unprecedented speedups of 2.3x-12.5x over existing methods and a reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on Frontier.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: We need responsible, application-driven (RAD) AI research</title>
<link>https://arxiv.org/abs/2505.04104</link>
<guid>https://arxiv.org/abs/2505.04104</guid>
<content:encoded><![CDATA[
arXiv:2505.04104v1 Announce Type: new 
Abstract: This position paper argues that achieving meaningful scientific and societal advances with artificial intelligence (AI) requires a responsible, application-driven approach (RAD) to AI research. As AI is increasingly integrated into society, AI researchers must engage with the specific contexts where AI is being applied. This includes being responsive to ethical and legal considerations, technical and societal constraints, and public discourse. We present the case for RAD-AI to drive research through a three-staged approach: (1) building transdisciplinary teams and people-centred studies; (2) addressing context-specific methods, ethical commitments, assumptions, and metrics; and (3) testing and sustaining efficacy through staged testbeds and a community of practice. We present a vision for the future of application-driven AI research to unlock new value through technically feasible methods that are adaptive to the contextual needs and values of the communities they ultimately serve.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alpha Excel Benchmark</title>
<link>https://arxiv.org/abs/2505.04110</link>
<guid>https://arxiv.org/abs/2505.04110</guid>
<content:encoded><![CDATA[
arXiv:2505.04110v1 Announce Type: new 
Abstract: This study presents a novel benchmark for evaluating Large Language Models (LLMs) using challenges derived from the Financial Modeling World Cup (FMWC) Excel competitions. We introduce a methodology for converting 113 existing FMWC challenges into programmatically evaluable JSON formats and use this dataset to compare the performance of several leading LLMs. Our findings demonstrate significant variations in performance across different challenge categories, with models showing specific strengths in pattern recognition tasks but struggling with complex numerical reasoning. The benchmark provides a standardized framework for assessing LLM capabilities in realistic business-oriented tasks rather than abstract academic problems. This research contributes to the growing field of AI benchmarking by establishing proficiency among the 1.5 billion people who daily use Microsoft Excel as a meaningful evaluation metric that bridges the gap between academic AI benchmarks and practical business applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification</title>
<link>https://arxiv.org/abs/2505.04139</link>
<guid>https://arxiv.org/abs/2505.04139</guid>
<content:encoded><![CDATA[
arXiv:2505.04139v1 Announce Type: new 
Abstract: We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision tree model designed for expressive and interpretable classification. LHT fundamentally distinguishes itself through a non-iterative, statistically-driven approach to constructing splitting hyperplanes. Unlike methods that rely on iterative optimization or heuristics, LHT directly computes the hyperplane parameters, which are derived from feature weights based on the differences in feature expectations between classes within each node. This deterministic mechanism enables a direct and well-defined hyperplane construction process. Predictions leverage a unique piecewise linear membership function within leaf nodes, obtained via local least-squares fitting. We formally analyze the convergence of the LHT splitting process, ensuring that each split yields meaningful, non-empty partitions. Furthermore, we establish that the time complexity for building an LHT up to depth $d$ is $O(mnd)$, demonstrating the practical feasibility of constructing trees with powerful oblique splits using this methodology. The explicit feature weighting at each split provides inherent interpretability. Experimental results on benchmark datasets demonstrate LHT's competitive accuracy, positioning it as a practical, theoretically grounded, and interpretable alternative in the landscape of tree-based models. The implementation of the proposed method is available at https://github.com/Hongyi-Li-sz/LHT_model.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.04158</link>
<guid>https://arxiv.org/abs/2505.04158</guid>
<content:encoded><![CDATA[
arXiv:2505.04158v1 Announce Type: new 
Abstract: Multivariate time series forecasting is crucial across various industries, where accurate extraction of complex periodic and trend components can significantly enhance prediction performance. However, existing models often struggle to capture these intricate patterns. To address these challenges, we propose FilterTS, a novel forecasting model that utilizes specialized filtering techniques based on the frequency domain. FilterTS introduces a Dynamic Cross-Variable Filtering Module, a key innovation that dynamically leverages other variables as filters to extract and reinforce shared variable frequency components across variables in multivariate time series. Additionally, a Static Global Filtering Module captures stable frequency components, identified throughout the entire training set. Moreover, the model is built in the frequency domain, converting time-domain convolutions into frequency-domain multiplicative operations to enhance computational efficiency. Extensive experimental results on eight real-world datasets have demonstrated that FilterTS significantly outperforms existing methods in terms of prediction accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning - Empirical analysis based on UK COVID-19 epidemic data</title>
<link>https://arxiv.org/abs/2505.04161</link>
<guid>https://arxiv.org/abs/2505.04161</guid>
<content:encoded><![CDATA[
arXiv:2505.04161v1 Announce Type: new 
Abstract: Globally, the outbreaks of infectious diseases have exerted an extremely profound and severe influence on health security and the economy. During the critical phases of epidemics, devising effective intervention measures poses a significant challenge to both the academic and practical arenas. There is numerous research based on reinforcement learning to optimize intervention measures of infectious diseases. Nevertheless, most of these efforts have been confined within the differential equation based on infectious disease models. Although a limited number of studies have incorporated reinforcement learning methodologies into individual-based infectious disease models, the models employed therein have entailed simplifications and limitations, rendering it incapable of modeling the complexity and dynamics inherent in infectious disease transmission. We establish a decision-making framework based on an individual agent-based transmission model, utilizing reinforcement learning to continuously explore and develop a strategy function. The framework's validity is verified through both experimental and theoretical approaches. Covasim, a detailed and widely used agent-based disease transmission model, was modified to support reinforcement learning research. We conduct an exhaustive exploration of the application efficacy of multiple algorithms across diverse action spaces. Furthermore, we conduct an innovative preliminary theoretical analysis concerning the issue of "time coverage". The results of the experiment robustly validate the effectiveness and feasibility of the methodological framework of this study. The coping strategies gleaned therefrom prove highly efficacious in suppressing the expansion of the epidemic scale and safeguarding the stability of the economic system, thereby providing crucial reference perspectives for the formulation of global public health security strategies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Augmented Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.04163</link>
<guid>https://arxiv.org/abs/2505.04163</guid>
<content:encoded><![CDATA[
arXiv:2505.04163v1 Announce Type: new 
Abstract: Time series forecasting uses historical data to predict future trends, leveraging the relationships between past observations and available features. In this paper, we propose RAFT, a retrieval-augmented time series forecasting method to provide sufficient inductive biases and complement the model's learning capacity. When forecasting the subsequent time frames, we directly retrieve historical data candidates from the training dataset with patterns most similar to the input, and utilize the future values of these candidates alongside the inputs to obtain predictions. This simple approach augments the model's capacity by externally providing information about past patterns via retrieval modules. Our empirical evaluations on ten benchmark datasets show that RAFT consistently outperforms contemporary baselines with an average win ratio of 86%.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.04167</link>
<guid>https://arxiv.org/abs/2505.04167</guid>
<content:encoded><![CDATA[
arXiv:2505.04167v1 Announce Type: new 
Abstract: Irregular multivariate time series (IMTS) are prevalent in real-world applications across many fields, where varying sensor frequencies and asynchronous measurements pose significant modeling challenges. Existing solutions often rely on a pre-alignment strategy to normalize data, which can distort intrinsic patterns and escalate computational and memory demands. Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational Graph Convolutional Network that avoids pre-alignment and directly captures the complex interdependencies in IMTS by representing them as a fully connected graph. Each observation is represented as a node, allowing the model to effectively handle misaligned timestamps by mapping all inter-node relationships, thus faithfully preserving the asynchronous nature of the data. Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that strategically aggregates nodes to optimize graph embeddings, reducing computational overhead while maintaining detailed local and global context. Extensive experiments on four public datasets demonstrate that STRGCN achieves state-of-the-art accuracy, competitive memory usage and training speed.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion</title>
<link>https://arxiv.org/abs/2505.04173</link>
<guid>https://arxiv.org/abs/2505.04173</guid>
<content:encoded><![CDATA[
arXiv:2505.04173v1 Announce Type: new 
Abstract: Recent advancements in layout pattern generation have been dominated by deep generative models. However, relying solely on neural networks for legality guarantees raises concerns in many practical applications. In this paper, we present \tool{DiffPattern}-Flex, a novel approach designed to generate reliable layout patterns efficiently. \tool{DiffPattern}-Flex incorporates a new method for generating diverse topologies using a discrete diffusion model while maintaining a lossless and compute-efficient layout representation. To ensure legal pattern generation, we employ {an} optimization-based, white-box pattern assessment process based on specific design rules. Furthermore, fast sampling and efficient legalization technologies are employed to accelerate the generation process. Experimental results across various benchmarks demonstrate that \tool{DiffPattern}-Flex significantly outperforms existing methods and excels at producing reliable layout patterns.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Device LLM for Context-Aware Wi-Fi Roaming</title>
<link>https://arxiv.org/abs/2505.04174</link>
<guid>https://arxiv.org/abs/2505.04174</guid>
<content:encoded><![CDATA[
arXiv:2505.04174v1 Announce Type: new 
Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless connectivity in dynamic mobile environments. Conventional threshold-based or heuristic schemes often fail, leading to either sticky or excessive handovers. We introduce the first cross-layer use of an on-device large language model (LLM): high-level reasoning in the application layer that issues real-time actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i) context-aware AP selection, where structured prompts fuse environmental cues (e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold adjustment, where the model adaptively decides when to roam. To satisfy the tight latency and resource budgets of edge hardware, we apply a suite of optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and quantization. Experiments on indoor and outdoor datasets show that our approach surpasses legacy heuristics and DRL baselines, achieving a strong balance between roaming stability and signal quality. These findings underscore the promise of application-layer LLM reasoning for lower-layer wireless control in future edge systems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trajectory Entropy Reinforcement Learning for Predictable and Robust Control</title>
<link>https://arxiv.org/abs/2505.04193</link>
<guid>https://arxiv.org/abs/2505.04193</guid>
<content:encoded><![CDATA[
arXiv:2505.04193v1 Announce Type: new 
Abstract: Simplicity is a critical inductive bias for designing data-driven controllers, especially when robustness is important. Despite the impressive results of deep reinforcement learning in complex control tasks, it is prone to capturing intricate and spurious correlations between observations and actions, leading to failure under slight perturbations to the environment. To tackle this problem, in this work we introduce a novel inductive bias towards simple policies in reinforcement learning. The simplicity inductive bias is introduced by minimizing the entropy of entire action trajectories, corresponding to the number of bits required to describe information in action trajectories after the agent observes state trajectories. Our reinforcement learning agent, Trajectory Entropy Reinforcement Learning, is optimized to minimize the trajectory entropy while maximizing rewards. We show that the trajectory entropy can be effectively estimated by learning a variational parameterized action prediction model, and use the prediction model to construct an information-regularized reward function. Furthermore, we construct a practical algorithm that enables the joint optimization of models, including the policy and the prediction model. Experimental evaluations on several high-dimensional locomotion tasks show that our learned policies produce more cyclical and consistent action trajectories, and achieve superior performance, and robustness to noise and dynamic changes than the state-of-the-art.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Language Model for Feasible and Diverse Population Synthesis</title>
<link>https://arxiv.org/abs/2505.04196</link>
<guid>https://arxiv.org/abs/2505.04196</guid>
<content:encoded><![CDATA[
arXiv:2505.04196v1 Announce Type: new 
Abstract: Generating a synthetic population that is both feasible and diverse is crucial for ensuring the validity of downstream activity schedule simulation in activity-based models (ABMs). While deep generative models (DGMs), such as variational autoencoders and generative adversarial networks, have been applied to this task, they often struggle to balance the inclusion of rare but plausible combinations (i.e., sampling zeros) with the exclusion of implausible ones (i.e., structural zeros). To improve feasibility while maintaining diversity, we propose a fine-tuning method for large language models (LLMs) that explicitly controls the autoregressive generation process through topological orderings derived from a Bayesian Network (BN). Experimental results show that our hybrid LLM-BN approach outperforms both traditional DGMs and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically, our approach achieves approximately 95% feasibility, significantly higher than the ~80% observed in DGMs, while maintaining comparable diversity, making it well-suited for practical applications. Importantly, the method is based on a lightweight open-source LLM, enabling fine-tuning and inference on standard personal computing environments. This makes the approach cost-effective and scalable for large-scale applications, such as synthesizing populations in megacities, without relying on expensive infrastructure. By initiating the ABM pipeline with high-quality synthetic populations, our method improves overall simulation reliability and reduces downstream error propagation. The source code for these methods is available for research and practical application.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Causal Effects in Networks with Cluster-Based Bandits</title>
<link>https://arxiv.org/abs/2505.04200</link>
<guid>https://arxiv.org/abs/2505.04200</guid>
<content:encoded><![CDATA[
arXiv:2505.04200v1 Announce Type: new 
Abstract: The gold standard for estimating causal effects is randomized controlled trial (RCT) or A/B testing where a random group of individuals from a population of interest are given treatment and the outcome is compared to a random group of individuals from the same population. However, A/B testing is challenging in the presence of interference, commonly occurring in social networks, where individuals can impact each others outcome. Moreover, A/B testing can incur a high performance loss when one of the treatment arms has a poor performance and the test continues to treat individuals with it. Therefore, it is important to design a strategy that can adapt over time and efficiently learn the total treatment effect in the network. We introduce two cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the total treatment effect in a network while maximizing the expected reward by making a tradeoff between exploration and exploitation. We compare the performance of our MAB algorithms with a vanilla MAB algorithm that ignores clusters and the corresponding RCT methods on semi-synthetic data with simulated interference. The vanilla MAB algorithm shows higher reward-action ratio at the cost of higher treatment effect error due to undesired spillover. The cluster-based MAB algorithms show higher reward-action ratio compared to their corresponding RCT methods without sacrificing much accuracy in treatment effect estimation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets</title>
<link>https://arxiv.org/abs/2505.04204</link>
<guid>https://arxiv.org/abs/2505.04204</guid>
<content:encoded><![CDATA[
arXiv:2505.04204v1 Announce Type: new 
Abstract: Cybersecurity has become essential worldwide and at all levels, concerning individuals, institutions, and governments. A basic principle in cybersecurity is to be always alert. Therefore, automation is imperative in processes where the volume of daily operations is large. Several cybersecurity applications can be addressed as binary classification problems, including anomaly detection, fraud detection, intrusion detection, spam detection, or malware detection. We present three experiments. In the first experiment, we evaluate single classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting Decision Tree. In the second experiment, we test different sampling techniques including over-sampling, under-sampling, Synthetic Minority Over-sampling Technique, and Self-Paced Ensembling. In the last experiment, we evaluate Self-Paced Ensembling and its number of base classifiers. We found that imbalance learning techniques had positive and negative effects, as reported in related studies. Thus, these techniques should be applied with caution. Besides, we found different best performers for each dataset. Therefore, we recommend testing single classifiers and imbalance learning techniques for each new dataset and application involving imbalanced datasets as is the case in several cyber security applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2505.04223</link>
<guid>https://arxiv.org/abs/2505.04223</guid>
<content:encoded><![CDATA[
arXiv:2505.04223v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training across distributed clients while preserving data locality. Although FedAvg pioneered synchronous rounds for global model averaging, slower devices can delay collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by continuously integrating client updates, yet naive implementations risk client drift due to non-IID data and stale contributions. Some Blockchain-based FL approaches (e.g., BRAIN) employ robust weighting or scoring of updates to resist malicious or misaligned proposals. However, performance drops can still persist under severe data heterogeneity or high staleness, and synchronization overhead has emerged as a new concern due to its aggregator-free architectures.
  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL method that mitigates these limitations by incorporating two key ideas. First, our FastSync strategy eliminates the need to replay past model versions, enabling newcomers and infrequent participants to efficiently approximate the global model. Second, we adopt spherical linear interpolation (SLERP) when merging parameters, preserving models' directions and alleviating destructive interference from divergent local training.
  Experiments with a CNN image-classification model and a Transformer-based language model demonstrate that FRAIN achieves more stable and robust convergence than FedAvg, FedAsync, and BRAIN, especially under harsh environments: non-IID data distributions, networks that experience delays and require frequent re-synchronization, and the presence of malicious nodes.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technology prediction of a 3D model using Neural Network</title>
<link>https://arxiv.org/abs/2505.04241</link>
<guid>https://arxiv.org/abs/2505.04241</guid>
<content:encoded><![CDATA[
arXiv:2505.04241v1 Announce Type: new 
Abstract: Accurate estimation of production times is critical for effective manufacturing scheduling, yet traditional methods relying on expert analysis or historical data often fall short in dynamic or customized production environments. This paper introduces a data-driven approach that predicts manufacturing steps and their durations directly from a product's 3D model. By rendering the model into multiple 2D images and leveraging a neural network inspired by the Generative Query Network, the method learns to map geometric features into time estimates for predefined production steps enabling scalable, adaptive, and precise process planning across varied product types.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification</title>
<link>https://arxiv.org/abs/2505.04263</link>
<guid>https://arxiv.org/abs/2505.04263</guid>
<content:encoded><![CDATA[
arXiv:2505.04263v1 Announce Type: new 
Abstract: We develop a novel physics informed deep learning approach for solving nonlinear drift-diffusion equations on metric graphs. These models represent an important model class with a large number of applications in areas ranging from transport in biological cells to the motion of human crowds. While traditional numerical schemes require a large amount of tailoring, especially in the case of model design or parameter identification problems, physics informed deep operator networks (DeepONet) have emerged as a versatile tool for the solution of partial differential equations with the particular advantage that they easily incorporate parameter identification questions. We here present an approach where we first learn three DeepONet models for representative inflow, inner and outflow edges, resp., and then subsequently couple these models for the solution of the drift-diffusion metric graph problem by relying on an edge-based domain decomposition approach. We illustrate that our framework is applicable for the accurate evaluation of graph-coupled physics models and is well suited for solving optimization or inverse problems on these coupled networks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-stationary Diffusion For Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.04278</link>
<guid>https://arxiv.org/abs/2505.04278</guid>
<content:encoded><![CDATA[
arXiv:2505.04278v1 Announce Type: new 
Abstract: Due to the dynamics of underlying physics and external influences, the uncertainty of time series often varies over time. However, existing Denoising Diffusion Probabilistic Models (DDPMs) often fail to capture this non-stationary nature, constrained by their constant variance assumption from the additive noise model (ANM). In this paper, we innovatively utilize the Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of ANM. A diffusion-based probabilistic forecasting framework, termed Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of modeling the changing pattern of uncertainty. Specifically, NsDiff combines a denoising diffusion-based conditional generative model with a pre-trained conditional mean and variance estimator, enabling adaptive endpoint distribution modeling. Furthermore, we propose an uncertainty-aware noise schedule, which dynamically adjusts the noise levels to accurately reflect the data uncertainty at each step and integrates the time-varying variances into the diffusion process. Extensive experiments conducted on nine real-world and synthetic datasets demonstrate the superior performance of NsDiff compared to existing approaches. Code is available at https://github.com/wwy155/NsDiff.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing</title>
<link>https://arxiv.org/abs/2505.04318</link>
<guid>https://arxiv.org/abs/2505.04318</guid>
<content:encoded><![CDATA[
arXiv:2505.04318v1 Announce Type: new 
Abstract: As the adoption of deep learning models has grown beyond human capacity for verification, meta-algorithms are needed to ensure reliable model inference. Concept drift detection is a field dedicated to identifying statistical shifts that is underutilized in monitoring neural networks that may encounter inference data with distributional characteristics diverging from their training data. Given the wide variety of model architectures, applications, and datasets, it is important that concept drift detection algorithms are adaptable to different inference scenarios. In this paper, we introduce an application of the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection meta-algorithm applied to a multilayer perceptron, a convolutional neural network, and a transformer trained for machine vision as they are exposed to simulated drift during inference. To that end, we demonstrate how unexpected drops in accuracy due to concept drift can be detected without directly examining the inference outputs. Our approach enhances safety by ensuring models are continually evaluated for reliability across varying conditions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces</title>
<link>https://arxiv.org/abs/2505.04335</link>
<guid>https://arxiv.org/abs/2505.04335</guid>
<content:encoded><![CDATA[
arXiv:2505.04335v1 Announce Type: new 
Abstract: Clustering algorithms play a pivotal role in unsupervised learning by identifying and grouping similar objects based on shared characteristics. While traditional clustering techniques, such as hard and fuzzy center-based clustering, have been widely used, they struggle with complex, high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy $C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits notable limitations in non-Euclidean spaces. Euclidean spaces assume linear separability and uniform distance scaling, limiting their effectiveness in capturing complex, hierarchical, or non-Euclidean structures in fuzzy clustering. To overcome these challenges, we introduce Filtration-based Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for better representation of data relationships in non-Euclidean spaces. HypeFCM integrates the principles of fuzzy clustering with hyperbolic geometry and employs a weight-based filtering mechanism to improve performance. The algorithm initializes weights using a Dirichlet distribution and iteratively refines cluster centroids and membership assignments based on a hyperbolic metric in the Poincar\'e Disc model. Extensive experimental evaluations demonstrate that HypeFCM significantly outperforms conventional fuzzy clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Denoising Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2505.04338</link>
<guid>https://arxiv.org/abs/2505.04338</guid>
<content:encoded><![CDATA[
arXiv:2505.04338v1 Announce Type: new 
Abstract: We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for learning distributions on submanifolds of Euclidean space that are level sets of functions, including most of the manifolds relevant to applications. Existing methods for generative modeling on manifolds rely on substantial geometric information such as geodesic curves or eigenfunctions of the Laplace-Beltrami operator and, as a result, they are limited to manifolds where such information is available. In contrast, our method, built on a projection scheme, can be applied to more general manifolds, as it only requires being able to evaluate the value and the first order derivatives of the function that defines the submanifold. We provide a theoretical analysis of our method in the continuous-time limit, which elucidates the connection between our RDDPMs and score-based generative models on manifolds. The capability of our method is demonstrated on datasets from previous studies and on new datasets sampled from two high-dimensional manifolds, i.e. $\mathrm{SO}(10)$ and the configuration space of molecular system alanine dipeptide with fixed dihedral angle.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.04339</link>
<guid>https://arxiv.org/abs/2505.04339</guid>
<content:encoded><![CDATA[
arXiv:2505.04339v1 Announce Type: new 
Abstract: DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Granular Attention based Heterogeneous Hypergraph Neural Network</title>
<link>https://arxiv.org/abs/2505.04340</link>
<guid>https://arxiv.org/abs/2505.04340</guid>
<content:encoded><![CDATA[
arXiv:2505.04340v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong abilities to learn node representations by effectively extracting complex structural and semantic information in heterogeneous graphs. Most of the prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging meta-path based message passing to learn latent node representations. However, due to the pairwise nature of meta-paths, these models fail to capture high-order relations among nodes, resulting in suboptimal performance. Additionally, the challenge of ``over-squashing'', where long-range message passing in HeteGNNs leads to severe information distortion, further limits the efficacy of these models. To address these limitations, this paper proposes MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural Network for heterogeneous graph representation learning. MGA-HHN introduces two key innovations: (1) a novel approach for constructing meta-path based heterogeneous hypergraphs that explicitly models higher-order semantic information in heterogeneous graphs through multiple views, and (2) a multi-granular attention mechanism that operates at both the node and hyperedge levels. This mechanism enables the model to capture fine-grained interactions among nodes sharing the same semantic context within a hyperedge type, while preserving the diversity of semantics across different hyperedge types. As such, MGA-HHN effectively mitigates long-range message distortion and generates more expressive node representations. Extensive experiments on real-world benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art models, showcasing its effectiveness in node classification, node clustering and visualization tasks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration</title>
<link>https://arxiv.org/abs/2505.04346</link>
<guid>https://arxiv.org/abs/2505.04346</guid>
<content:encoded><![CDATA[
arXiv:2505.04346v1 Announce Type: new 
Abstract: Clustering aims to form groups of similar data points in an unsupervised regime. Yet, clustering complex datasets containing critically intertwined shapes poses significant challenges. The prevailing clustering algorithms widely depend on evaluating similarity measures based on Euclidean metrics. Exploring topological characteristics to perform clustering of complex datasets inevitably presents a better scope. The topological clustering algorithms predominantly perceive the point set through the lens of Simplicial complexes and Persistent homology. Despite these approaches, the existing topological clustering algorithms cannot somehow fully exploit topological structures and show inconsistent performances on some highly complicated datasets. This work aims to mitigate the limitations by identifying topologically similar neighbors through the Vietoris-Rips complex and Betti number filtration. In addition, we introduce the concept of the Betti sequences to capture flexibly essential features from the topological structures. Our proposed algorithm is adept at clustering complex, intertwined shapes contained in the datasets. We carried out experiments on several synthetic and real-world datasets. Our algorithm demonstrated commendable performances across the datasets compared to some of the well-known topology-based clustering algorithms.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid</title>
<link>https://arxiv.org/abs/2505.04367</link>
<guid>https://arxiv.org/abs/2505.04367</guid>
<content:encoded><![CDATA[
arXiv:2505.04367v1 Announce Type: new 
Abstract: The global energy landscape is undergoing a profound transformation, often referred to as the energy transition, driven by the urgent need to mitigate climate change, reduce greenhouse gas emissions, and ensure sustainable energy supplies. However, the undoubted complexity of new investments in renewables, as well as the phase out of high CO2-emission energy sources, hampers the pace of the energy transition and raises doubts as to whether new renewable energy sources are capable of solely meeting the climate target goals. This highlights the need to investigate alternative pathways to accelerate the energy transition, by identifying human activity domains with higher/excessive energy demands. Two notable examples where there is room for improvement, in the sense of reducing energy consumption and consequently CO2 emissions, are residential energy consumption and road transport. This dissertation investigates the development of novel Deep Learning techniques to create tools which solve limitations in these two key energy domains. Reduction of residential energy consumption can be achieved by empowering end-users with the user of Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep Reinforcement Learning can tackle road transport decarbonization.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four</title>
<link>https://arxiv.org/abs/2505.04371</link>
<guid>https://arxiv.org/abs/2505.04371</guid>
<content:encoded><![CDATA[
arXiv:2505.04371v1 Announce Type: new 
Abstract: Action selection based on flags is a Reinforcement Learning (RL) exploration policy that improves the exploration of the state space through the use of flags, which can identify the most promising actions to take in each state. The quantum counterpart of this exploration policy further improves upon this by taking advantage of a quadratic speedup for sampling flagged actions. This approach has already been successfully employed for the game of Checkers. In this work, we describe the application of this method to the context of Connect Four, in order to study its performance in a different setting, which can lead to a better generalization of the technique. We also kept track of a metric that wasn't taken into account in previous work: the average number of iterations to obtain a flagged action. Since going second is a significant disadvantage in Connect Four, we also had the intent of exploring how this more complex scenario would impact the performance of our approach. The experiments involved training and testing classical and quantum RL agents that played either going first or going second against a Randomized Negamax opponent. The results showed that both flagged exploration policies were clearly superior to a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact sample flagged actions in less iterations. Despite obtaining tagged actions more consistently, the win rates between the classical and quantum versions of the approach were identical, which could be due to the simplicity of the training scenario chosen.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets</title>
<link>https://arxiv.org/abs/2505.04389</link>
<guid>https://arxiv.org/abs/2505.04389</guid>
<content:encoded><![CDATA[
arXiv:2505.04389v1 Announce Type: new 
Abstract: Clustering is a fundamental task in data mining and machine learning, particularly for analyzing large-scale data. In this paper, we introduce Clust-Splitter, an efficient algorithm based on nonsmooth optimization, designed to solve the minimum sum-of-squares clustering problem in very large datasets. The clustering task is approached through a sequence of three nonsmooth optimization problems: two auxiliary problems used to generate suitable starting points, followed by a main clustering formulation. To solve these problems effectively, the limited memory bundle method is combined with an incremental approach to develop the Clust-Splitter algorithm. We evaluate Clust-Splitter on real-world datasets characterized by both a large number of attributes and a large number of data points and compare its performance with several state-of-the-art large-scale clustering algorithms. Experimental results demonstrate the efficiency of the proposed method for clustering very large datasets, as well as the high quality of its solutions, which are on par with those of the best existing methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast</title>
<link>https://arxiv.org/abs/2505.04396</link>
<guid>https://arxiv.org/abs/2505.04396</guid>
<content:encoded><![CDATA[
arXiv:2505.04396v1 Announce Type: new 
Abstract: The planning and operation of renewable energy, especially wind power, depend crucially on accurate, timely, and high-resolution weather information. Coarse-grid global numerical weather forecasts are typically downscaled to meet these requirements, introducing challenges of scale inconsistency, process representation error, computation cost, and entanglement of distinct uncertainty sources from chaoticity, model bias, and large-scale forcing. We address these challenges by learning the climatological distribution of a target wind farm using its high-resolution numerical weather simulations. An optimal combination of this learned high-resolution climatological prior with coarse-grid large scale forecasts yields highly accurate, fine-grained, full-variable, large ensemble of weather pattern forecasts. Using observed meteorological records and wind turbine power outputs as references, the proposed methodology verifies advantageously compared to existing numerical/statistical forecasting-downscaling pipelines, regarding either deterministic/probabilistic skills or economic gains. Moreover, a 100-member, 10-day forecast with spatial resolution of 1 km and output frequency of 15 min takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU hours for conventional numerical simulation. By drastically reducing computational costs while maintaining accuracy, our method paves the way for more efficient and reliable renewable energy planning and operation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization</title>
<link>https://arxiv.org/abs/2505.04412</link>
<guid>https://arxiv.org/abs/2505.04412</guid>
<content:encoded><![CDATA[
arXiv:2505.04412v1 Announce Type: new 
Abstract: Manifold learning aims to discover and represent low-dimensional structures underlying high-dimensional data while preserving critical topological and geometric properties. Existing methods often fail to capture local details with global topological integrity from noisy data or construct a balanced dimensionality reduction, resulting in distorted or fractured embeddings. We present an AutoEncoder-based method that integrates a manifold reconstruction layer, which uncovers latent manifold structures from noisy point clouds, and further provides regularizations on topological and geometric properties during dimensionality reduction, whereas the two components promote each other during training. Experiments on point cloud datasets demonstrate that our method outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in discovering manifold structures from noisy data and preserving them through dimensionality reduction, as validated by visualization and quantitative metrics. This work demonstrates the significance of combining manifold reconstruction with manifold learning to achieve reliable representation of the latent manifold, particularly when dealing with noisy real-world data. Code repository: https://github.com/Thanatorika/mrtg.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localized Diffusion Models for High Dimensional Distributions Generation</title>
<link>https://arxiv.org/abs/2505.04417</link>
<guid>https://arxiv.org/abs/2505.04417</guid>
<content:encoded><![CDATA[
arXiv:2505.04417v1 Announce Type: new 
Abstract: Diffusion models are the state-of-the-art tools for various generative tasks. However, estimating high-dimensional score functions makes them potentially suffer from the curse of dimensionality (CoD). This underscores the importance of better understanding and exploiting low-dimensional structure in the target distribution. In this work, we consider locality structure, which describes sparse dependencies between model components. Under locality structure, the score function is effectively low-dimensional, so that it can be estimated by a localized neural network with significantly reduced sample complexity. This motivates the localized diffusion model, where a localized score matching loss is used to train the score function within a localized hypothesis space. We prove that such localization enables diffusion models to circumvent CoD, at the price of additional localization error. Under realistic sample size scaling, we show both theoretically and numerically that a moderate localization radius can balance the statistical and localization error, leading to a better overall performance. The localized structure also facilitates parallel training of diffusion models, making it potentially more efficient for large-scale applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedBWO: Enhancing Communication Efficiency in Federated Learning</title>
<link>https://arxiv.org/abs/2505.04435</link>
<guid>https://arxiv.org/abs/2505.04435</guid>
<content:encoded><![CDATA[
arXiv:2505.04435v1 Announce Type: new 
Abstract: Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a shared model is collaboratively trained by various clients using their local datasets while keeping the data private. Considering resource-constrained devices, FL clients often suffer from restricted transmission capacity. Aiming to enhance the system performance, the communication between clients and server needs to be diminished. Current FL strategies transmit a tremendous amount of data (model weights) within the FL process, which needs a high communication bandwidth. Considering resource constraints, increasing the number of clients and, consequently, the amount of data (model weights) can lead to a bottleneck. In this paper, we introduce the Federated Black Widow Optimization (FedBWO) technique to decrease the amount of transmitted data by transmitting only a performance score rather than the local model weights from clients. FedBWO employs the BWO algorithm to improve local model updates. The conducted experiments prove that FedBWO remarkably improves the performance of the global model and the communication efficiency of the overall system. According to the experimental outcomes, FedBWO enhances the global model accuracy by an average of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically decreases the communication cost compared to other methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory</title>
<link>https://arxiv.org/abs/2505.04440</link>
<guid>https://arxiv.org/abs/2505.04440</guid>
<content:encoded><![CDATA[
arXiv:2505.04440v1 Announce Type: new 
Abstract: The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is highly dependent on the preset vigilance parameter, where deviations in its value can lead to significant fluctuations in clustering results, severely limiting its practicality for non-expert users. Existing approaches generally enhance vigilance parameter robustness through adaptive mechanisms such as particle swarm optimization and fuzzy logic rules. However, they often introduce additional hyperparameters or complex frameworks that contradict the original simplicity of the algorithm. To address this, we propose Iterative Refinement Adaptive Resonance Theory (IR-ART), which integrates three key phases into a unified iterative framework: (1) Cluster Stability Detection: A dynamic stability detection module that identifies unstable clusters by analyzing the change of sample size (number of samples in the cluster) in iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance region expansion mechanism that adaptively adjusts similarity thresholds. Independent of the specific execution of clustering, these three phases sequentially focus on analyzing the implicit knowledge within the iterative process, adjusting weights and vigilance parameters, thereby laying a foundation for the next iteration. Experimental evaluation on 15 datasets demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter values while preserving the parameter simplicity of Fuzzy ART. Case studies visually confirm the algorithm's self-optimization capability through iterative refinement, making it particularly suitable for non-expert users in resource-constrained scenarios.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs</title>
<link>https://arxiv.org/abs/2505.04441</link>
<guid>https://arxiv.org/abs/2505.04441</guid>
<content:encoded><![CDATA[
arXiv:2505.04441v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show promising performance on various programming tasks, including Automatic Program Repair (APR). However, most approaches to LLM-based APR are limited to the static analysis of the programs, while disregarding their runtime behavior. Inspired by knowledge-augmented NLP, in this work, we aim to remedy this potential blind spot by augmenting standard APR prompts with program execution traces. We evaluate our approach using the GPT family of models on three popular APR datasets. Our findings suggest that simply incorporating execution traces into the prompt provides a limited performance improvement over trace-free baselines, in only 2 out of 6 tested dataset / model configurations. We further find that the effectiveness of execution traces for APR diminishes as their complexity increases. We explore several strategies for leveraging traces in prompts and demonstrate that LLM-optimized prompts help outperform trace-free prompts more consistently. Additionally, we show trace-based prompting to be superior to finetuning a smaller LLM on a small-scale dataset; and conduct probing studies reinforcing the notion that execution traces can complement the reasoning abilities of the LLMs.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2505.04461</link>
<guid>https://arxiv.org/abs/2505.04461</guid>
<content:encoded><![CDATA[
arXiv:2505.04461v1 Announce Type: new 
Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discriminative Ordering Through Ensemble Consensus</title>
<link>https://arxiv.org/abs/2505.04464</link>
<guid>https://arxiv.org/abs/2505.04464</guid>
<content:encoded><![CDATA[
arXiv:2505.04464v1 Announce Type: new 
Abstract: Evaluating the performance of clustering models is a challenging task where the outcome depends on the definition of what constitutes a cluster. Due to this design, current existing metrics rarely handle multiple clustering models with diverse cluster definitions, nor do they comply with the integration of constraints when available. In this work, we take inspiration from consensus clustering and assume that a set of clustering models is able to uncover hidden structures in the data. We propose to construct a discriminative ordering through ensemble clustering based on the distance between the connectivity of a clustering model and the consensus matrix. We first validate the proposed method with synthetic scenarios, highlighting that the proposed score ranks the models that best match the consensus first. We then show that this simple ranking score significantly outperforms other scoring methods when comparing sets of different clustering algorithms that are not restricted to a fixed number of clusters and is compatible with clustering constraints.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral and Temporal Denoising for Differentially Private Optimization</title>
<link>https://arxiv.org/abs/2505.04468</link>
<guid>https://arxiv.org/abs/2505.04468</guid>
<content:encoded><![CDATA[
arXiv:2505.04468v1 Announce Type: new 
Abstract: This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a differentially private optimization method that addresses the challenge of preserving performance in DP-SGD, where added noise typically degrades model utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering to enhance gradient quality while preserving $(\varepsilon, \delta)$-DP guarantees. It employs a high-frequency shaping mask in the Fourier domain to concentrate differential privacy noise in less informative spectral components, preserving low-frequency gradient signals. A scalar-gain Kalman filter with finite-difference Hessian approximation further refines the denoised gradients. With a per-iteration complexity of $\mathcal{O}(d \log d)$, FFTKF demonstrates improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers. Theoretical analysis confirms that FFTKF maintains equivalent privacy guarantees while achieving a tighter privacy-utility trade-off through reduced noise and controlled bias.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations</title>
<link>https://arxiv.org/abs/2505.04471</link>
<guid>https://arxiv.org/abs/2505.04471</guid>
<content:encoded><![CDATA[
arXiv:2505.04471v1 Announce Type: new 
Abstract: Many conservative physical systems can be described using the Hamiltonian formalism. A notable example is the Vlasov-Poisson equations, a set of partial differential equations that govern the time evolution of a phase-space density function representing collisionless particles under a self-consistent potential. These equations play a central role in both plasma physics and cosmology. Due to the complexity of the potential involved, analytical solutions are rarely available, necessitating the use of numerical methods such as Particle-In-Cell. In this work, we introduce a novel approach based on Hamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic Neural Hamiltonian Flows. Our method transforms an initial Gaussian distribution in phase space into the final distribution using a sequence of invertible, volume-preserving transformations derived from Hamiltonian dynamics. The model is trained on a dataset comprising initial and final states at a fixed time T, generated via numerical simulations. After training, the model enables fast sampling of the final distribution from any given initial state. Moreover, by automatically learning an interpretable physical potential, it can generalize to intermediate states not seen during training, offering insights into the system's evolution across time.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules</title>
<link>https://arxiv.org/abs/2505.04535</link>
<guid>https://arxiv.org/abs/2505.04535</guid>
<content:encoded><![CDATA[
arXiv:2505.04535v1 Announce Type: new 
Abstract: Federated learning (FL) makes it possible to train models on data that would otherwise remain untapped and inaccessible. Simultaneously, pre-trained language models (LMs) have emerged as indispensable tools in modern workflows. These models exhibit extraordinary capabilities and are easily adapted to downstream tasks. This opens one of the most exciting frontiers in FL: fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid communication of parameters, a problem which is magnified by the sheer size of these modern models. Currently, the FedOpt family of algorithms is the prevailing approach in FL, though it relies on fixed, heuristic intervals for model synchronization. Recently, the FDA algorithm introduced a dynamic alternative by monitoring training progress, but it came with its own drawbacks; namely, a hard-to-tune threshold parameter and a rigid synchronization scheme. In this work, we introduce the FDA-Opt family of algorithms -- a unified generalization that extends the principles behind both FDA and FedOpt, while resolving their core limitations. We evaluate our approach on fine-tuning LMs across a range of downstream NLP tasks, and demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt operates under hyper-parameter settings originally optimized for its competitors. In other words, we show that FDA-Opt is a practical, drop-in replacement for FedOpt in modern FL libraries and systems: it requires no additional configuration and delivers superior performance out of the box.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Purity Law for Generalizable Neural TSP Solvers</title>
<link>https://arxiv.org/abs/2505.04558</link>
<guid>https://arxiv.org/abs/2505.04558</guid>
<content:encoded><![CDATA[
arXiv:2505.04558v1 Announce Type: new 
Abstract: Achieving generalization in neural approaches across different scales and distributions remains a significant challenge for the Traveling Salesman Problem~(TSP). A key obstacle is that neural networks often fail to learn robust principles for identifying universal patterns and deriving optimal solutions from diverse instances. In this paper, we first uncover Purity Law (PuLa), a fundamental structural principle for optimal TSP solutions, defining that edge prevalence grows exponentially with the sparsity of surrounding vertices. Statistically validated across diverse instances, PuLa reveals a consistent bias toward local sparsity in global optima. Building on this insight, we propose Purity Policy Optimization~(PUPO), a novel training paradigm that explicitly aligns characteristics of neural solutions with PuLa during the solution construction process to enhance generalization. Extensive experiments demonstrate that PUPO can be seamlessly integrated with popular neural solvers, significantly enhancing their generalization performance without incurring additional computational overhead during inference.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $\alpha$-$\beta$-Divergence</title>
<link>https://arxiv.org/abs/2505.04560</link>
<guid>https://arxiv.org/abs/2505.04560</guid>
<content:encoded><![CDATA[
arXiv:2505.04560v1 Announce Type: new 
Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student model by minimizing the divergence between their output distributions, typically using forward Kullback-Leibler divergence (FKLD) or reverse KLD (RKLD). It has become an effective training paradigm due to the broader supervision information provided by the teacher distribution compared to one-hot labels. We identify that the core challenge in KD lies in balancing two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}} effect, which refers to focusing on modes with large errors, and the \textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on modes with high student confidence. Through an analysis of how probabilities are reassigned during gradient updates, we observe that these two effects are entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too weak in FKLD, causing the student to fail to concentrate on the target class. In contrast, both are too strong in RKLD, causing the student to overly emphasize the target class while ignoring the broader distributional information from the teacher. To address this imbalance, we propose ABKD, a generic framework with $\alpha$-$\beta$-divergence. Our theoretical results show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving an effective trade-off between these effects. Extensive experiments on 17 language/vision datasets with 12 teacher-student settings confirm its efficacy. The code is available at https://github.com/ghwang-s/abkd.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data</title>
<link>https://arxiv.org/abs/2505.04566</link>
<guid>https://arxiv.org/abs/2505.04566</guid>
<content:encoded><![CDATA[
arXiv:2505.04566v1 Announce Type: new 
Abstract: This paper presents a multitask learning approach based on long-short-term memory (LSTM) networks for the joint prediction of arboviral outbreaks and case counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging historical public health data from DataSUS (2017-2023), the proposed model concurrently performs binary classification (outbreak detection) and regression (case forecasting) tasks. A sliding window strategy was adopted to construct temporal features using varying input lengths (60, 90, and 120 days), with hyperparameter optimization carried out using Keras Tuner. Model evaluation used time series cross-validation for robustness and a held-out test from 2023 for generalization assessment. The results show that longer windows improve dengue regression accuracy, while classification performance peaked at intermediate windows, suggesting an optimal trade-off between sequence length and generalization. The multitask architecture delivers competitive performance across diseases and tasks, demonstrating the feasibility and advantages of unified modeling strategies for scalable epidemic forecasting in data-limited public health scenarios.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization</title>
<link>https://arxiv.org/abs/2505.04578</link>
<guid>https://arxiv.org/abs/2505.04578</guid>
<content:encoded><![CDATA[
arXiv:2505.04578v1 Announce Type: new 
Abstract: Reinforcement learning (RL) fine-tuning transforms large language models while creating a vulnerability we experimentally verify: Our experiment shows that malicious RL fine-tuning dismantles safety guardrails with remarkable efficiency, requiring only 50 steps and minimal adversarial prompts, with harmful escalating from 0-2 to 7-9. This attack vector particularly threatens open-source models with parameter-level access. Existing defenses targeting supervised fine-tuning prove ineffective against RL's dynamic feedback mechanisms. We introduce Reward Neutralization, the first defense framework specifically designed against RL fine-tuning attacks, establishing concise rejection patterns that render malicious reward signals ineffective. Our approach trains models to produce minimal-information rejections that attackers cannot exploit, systematically neutralizing attempts to optimize toward harmful outputs. Experiments validate that our approach maintains low harmful scores (no greater than 2) after 200 attack steps, while standard models rapidly deteriorate. This work provides the first constructive proof that robust defense against increasingly accessible RL attacks is achievable, addressing a critical security gap for open-weight models.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness</title>
<link>https://arxiv.org/abs/2505.04599</link>
<guid>https://arxiv.org/abs/2505.04599</guid>
<content:encoded><![CDATA[
arXiv:2505.04599v1 Announce Type: new 
Abstract: Recent results in non-convex stochastic optimization demonstrate the convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0, L_1)$-smoothness condition, but the rate of convergence is a higher-order polynomial in terms of problem parameters like the smoothness constants. The complexity guaranteed by such algorithms to find an $\epsilon$-stationary point may be significantly larger than the optimal complexity of $\Theta \left( \Delta L \sigma^2 \epsilon^{-4} \right)$ achieved by SGD in the $L$-smooth setting, where $\Delta$ is the initial optimality gap, $\sigma^2$ is the variance of stochastic gradient. However, it is currently not known whether these higher-order dependencies can be tightened. To answer this question, we investigate complexity lower bounds for several adaptive optimization algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence in terms of problem parameters $\Delta, L_0, L_1$. We provide complexity bounds for three variations of AdaGrad, which show at least a quadratic dependence on problem parameters $\Delta, L_0, L_1$. Notably, we show that the decorrelated variant of AdaGrad-Norm requires at least $\Omega \left( \Delta^2 L_1^2 \sigma^2 \epsilon^{-4} \right)$ stochastic gradient queries to find an $\epsilon$-stationary point. We also provide a lower bound for SGD with a broad class of adaptive stepsizes. Our results show that, for certain adaptive algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult than the standard smooth setting, in terms of the initial optimality gap and the smoothness constants.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing Juntas Optimally with Samples</title>
<link>https://arxiv.org/abs/2505.04604</link>
<guid>https://arxiv.org/abs/2505.04604</guid>
<content:encoded><![CDATA[
arXiv:2505.04604v1 Announce Type: new 
Abstract: We prove tight upper and lower bounds of $\Theta\left(\tfrac{1}{\epsilon}\left( \sqrt{2^k \log\binom{n}{k} } + \log\binom{n}{k} \right)\right)$ on the number of samples required for distribution-free $k$-junta testing. This is the first tight bound for testing a natural class of Boolean functions in the distribution-free sample-based model. Our bounds also hold for the feature selection problem, showing that a junta tester must learn the set of relevant variables. For tolerant junta testing, we prove a sample lower bound of $\Omega(2^{(1-o(1)) k} + \log\binom{n}{k})$ showing that, unlike standard testing, there is no large gap between tolerant testing and learning.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales</title>
<link>https://arxiv.org/abs/2505.04608</link>
<guid>https://arxiv.org/abs/2505.04608</guid>
<content:encoded><![CDATA[
arXiv:2505.04608v1 Announce Type: new 
Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria,'' such as data shifts that violate certain exchangeability assumptions, or do not allow for online adaptation in response to shifts. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that accommodate online adaptation to mild covariate shifts (in the marginal input distribution) while raising alarms in response to more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.04619</link>
<guid>https://arxiv.org/abs/2505.04619</guid>
<content:encoded><![CDATA[
arXiv:2505.04619v1 Announce Type: new 
Abstract: Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design</title>
<link>https://arxiv.org/abs/2505.03745</link>
<guid>https://arxiv.org/abs/2505.03745</guid>
<content:encoded><![CDATA[
arXiv:2505.03745v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have achieved huge success in the natural language processing (NLP) field, driving a growing demand to extend their deployment from the cloud to edge devices. However, deploying LLMs on resource-constrained edge devices poses significant challenges, including (1) intensive computations and huge model sizes, (2) great memory and bandwidth demands introduced by the autoregressive generation process, and (3) limited scalability for handling long sequences. To address these challenges, we propose AccLLM, a comprehensive acceleration framework that enables efficient and fast long-context LLM inference through algorithm and hardware co-design. At the algorithmic level, we integrate (1) pruning, (2) {\Lambda}-shaped attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and 4-bit KV cache) quantization scheme, thus effectively reducing memory and bandwidth requirements while facilitating LLMs' long-sequence generation. At the hardware level, we design a dedicated FPGA-based accelerator with a reconfigurable computing engine to effectively and flexibly accommodate diverse operations arising from our compression algorithm, thereby fully translating the algorithmic innovations into tangible hardware efficiency. We validate AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency and a 2.98x throughput compared to the state-of-the-art work FlightLLM.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management</title>
<link>https://arxiv.org/abs/2505.03756</link>
<guid>https://arxiv.org/abs/2505.03756</guid>
<content:encoded><![CDATA[
arXiv:2505.03756v1 Announce Type: cross 
Abstract: Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for task-specific Large Language Model (LLM) applications. For multi-LoRA serving, caching hot KV caches and LoRA adapters in high bandwidth memory of accelerations can improve inference performance. However, existing Multi-LoRA inference systems fail to optimize serving performance like Time-To-First-Toke (TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore propose FASTLIBRA, a Multi-LoRA caching system to optimize the serving performance. FASTLIBRA comprises a dependency-aware cache manager and a performance-driven cache swapper. The cache manager maintains the usage dependencies between LoRAs and KV caches during the inference with a unified caching pool. The cache swapper determines the swap-in or out of LoRAs and KV caches based on a unified cost model, when the HBM is idle or busy, respectively. Experimental results show that ELORA reduces the TTFT by 63.4% on average, compared to state-of-the-art works.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation</title>
<link>https://arxiv.org/abs/2505.03757</link>
<guid>https://arxiv.org/abs/2505.03757</guid>
<content:encoded><![CDATA[
arXiv:2505.03757v1 Announce Type: cross 
Abstract: Coordinate transformation models often fail to account for nonlinear and spatially dependent distortions, leading to significant residual errors in geospatial applications. Here we propose a residual-based neural correction strategy, in which a neural network learns to model only the systematic distortions left by an initial geometric transformation. By focusing solely on residual patterns, the proposed method reduces model complexity and improves performance, particularly in scenarios with sparse or structured control point configurations. We evaluate the method using both simulated datasets with varying distortion intensities and sampling strategies, as well as under the real-world image georeferencing tasks. Compared with direct neural network coordinate converter and classical transformation models, the residual-based neural correction delivers more accurate and stable results under challenging conditions, while maintaining comparable performance in ideal cases. These findings demonstrate the effectiveness of residual modelling as a lightweight and robust alternative for improving coordinate transformation accuracy.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Splitwiser: Efficient LM inference with constrained resources</title>
<link>https://arxiv.org/abs/2505.03763</link>
<guid>https://arxiv.org/abs/2505.03763</guid>
<content:encoded><![CDATA[
arXiv:2505.03763v1 Announce Type: cross 
Abstract: Efficient inference of LLMs remains a crucial challenge, with two main phases: a compute-intensive prompt computation and a memory-intensive token generation. Despite existing batching and scheduling techniques, token generation phases fail to fully utilize compute resources, especially when compared to prompt computation phases. To address these challenges, we propose Splitwiser, a methodology that splits the two phases of an LLM inference request onto the same GPU, thereby reducing overhead and improving memory access and cache utilization. By eliminating the need to transfer data across devices, Splitwiser aims to minimize network-related overheads. In this report, we describe the basic structure of our proposed pipeline while sharing preliminary results and analysis. We implement our proposed multiprocessing design on two widely-used and independent LLM architectures: Huggingface and vLLM. We open-source our code for the respective implementations: 1) Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM (https://github.com/adney11/vllm-sysml).
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs</title>
<link>https://arxiv.org/abs/2505.03814</link>
<guid>https://arxiv.org/abs/2505.03814</guid>
<content:encoded><![CDATA[
arXiv:2505.03814v1 Announce Type: cross 
Abstract: As foundation models continue to scale, the size of trained models grows exponentially, presenting significant challenges for their evaluation. Current evaluation practices involve curating increasingly large datasets to assess the performance of large language models (LLMs). However, there is a lack of systematic analysis and guidance on determining the sufficiency of test data or selecting informative samples for evaluation. This paper introduces a certifiable and cost-efficient evaluation framework for LLMs. Our framework adapts to different evaluation objectives and outputs confidence intervals that contain true values with high probability. We use ``test sample complexity'' to quantify the number of test points needed for a certifiable evaluation and derive tight bounds on test sample complexity. Based on the developed theory, we develop a partition-based algorithm, named Cer-Eval, that adaptively selects test points to minimize the cost of LLM evaluation. Real-world experiments demonstrate that Cer-Eval can save 20% to 40% test points across various benchmarks, while maintaining an estimation error level comparable to the current evaluation process and providing a 95% confidence guarantee.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.03817</link>
<guid>https://arxiv.org/abs/2505.03817</guid>
<content:encoded><![CDATA[
arXiv:2505.03817v1 Announce Type: cross 
Abstract: This paper presents a holistic approach to attacker preference modeling from system-level audit logs using inverse reinforcement learning (IRL). Adversary modeling is an important capability in cybersecurity that lets defenders characterize behaviors of potential attackers, which enables attribution to known cyber adversary groups. Existing approaches rely on documenting an ever-evolving set of attacker tools and techniques to track known threat actors. Although attacks evolve constantly, attacker behavioral preferences are intrinsic and less volatile. Our approach learns the behavioral preferences of cyber adversaries from forensics data on their tools and techniques. We model the attacker as an expert decision-making agent with unknown behavioral preferences situated in a computer host. We leverage attack provenance graphs of audit logs to derive a state-action trajectory of the attack. We test our approach on open datasets of audit logs containing real attack data. Our results demonstrate for the first time that low-level forensics data can automatically reveal an adversary's subjective preferences, which serves as an additional dimension to modeling and documenting cyber adversaries. Attackers' preferences tend to be invariant despite their different tools and indicate predispositions that are inherent to the attacker. As such, these inferred preferences can potentially serve as unique behavioral signatures of attackers and improve threat attribution.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective</title>
<link>https://arxiv.org/abs/2505.03828</link>
<guid>https://arxiv.org/abs/2505.03828</guid>
<content:encoded><![CDATA[
arXiv:2505.03828v1 Announce Type: cross 
Abstract: E-commerce platforms generate vast volumes of user feedback, such as star ratings, written reviews, and comments. However, most recommendation engines rely primarily on numerical scores, often overlooking the nuanced opinions embedded in free text. This paper comprehensively reviews sentiment-aware recommendation systems from a natural language processing perspective, covering advancements from 2023 to early 2025. It highlights the benefits of integrating sentiment analysis into e-commerce recommenders to enhance prediction accuracy and explainability through detailed opinion extraction. Our survey categorizes recent work into four main approaches: deep learning classifiers that combine sentiment embeddings with user item interactions, transformer based methods for nuanced feature extraction, graph neural networks that propagate sentiment signals, and conversational recommenders that adapt in real time to user feedback. We summarize model architectures and demonstrate how sentiment flows through recommendation pipelines, impacting dialogue-based suggestions. Key challenges include handling noisy or sarcastic text, dynamic user preferences, and bias mitigation. Finally, we outline research gaps and provide a roadmap for developing smarter, fairer, and more user-centric recommendation tools.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Analysis of Adversarial Attacks against Spam Filters</title>
<link>https://arxiv.org/abs/2505.03831</link>
<guid>https://arxiv.org/abs/2505.03831</guid>
<content:encoded><![CDATA[
arXiv:2505.03831v1 Announce Type: cross 
Abstract: Deep learning has revolutionized email filtering, which is critical to protect users from cyber threats such as spam, malware, and phishing. However, the increasing sophistication of adversarial attacks poses a significant challenge to the effectiveness of these filters. This study investigates the impact of adversarial attacks on deep learning-based spam detection systems using real-world datasets. Six prominent deep learning models are evaluated on these datasets, analyzing attacks at the word, character sentence, and AI-generated paragraph-levels. Novel scoring functions, including spam weights and attention weights, are introduced to improve attack effectiveness. This comprehensive analysis sheds light on the vulnerabilities of spam filters and contributes to efforts to improve their security against evolving adversarial threats.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointExplainer: Towards Transparent Parkinson's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2505.03833</link>
<guid>https://arxiv.org/abs/2505.03833</guid>
<content:encoded><![CDATA[
arXiv:2505.03833v1 Announce Type: cross 
Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn signals for early diagnosis of Parkinson's disease. However, the lack of clear interpretability in existing diagnostic methods presents a challenge to clinical trust. In this paper, we propose PointExplainer, an explainable diagnostic strategy to identify hand-drawn regions that drive model diagnosis. Specifically, PointExplainer assigns discrete attribution values to hand-drawn segments, explicitly quantifying their relative contributions to the model's decision. Its key components include: (i) a diagnosis module, which encodes hand-drawn signals into 3D point clouds to represent hand-drawn trajectories, and (ii) an explanation module, which trains an interpretable surrogate model to approximate the local behavior of the black-box diagnostic model. We also introduce consistency measures to further address the issue of faithfulness in explanations. Extensive experiments on two benchmark datasets and a newly constructed dataset show that PointExplainer can provide intuitive explanations with no diagnostic performance degradation. The source code is available at https://github.com/chaoxuewang/PointExplainer.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation</title>
<link>https://arxiv.org/abs/2505.03840</link>
<guid>https://arxiv.org/abs/2505.03840</guid>
<content:encoded><![CDATA[
arXiv:2505.03840v1 Announce Type: cross 
Abstract: Clustering bandits have gained significant attention in recommender systems by leveraging collaborative information from neighboring users to better capture target user preferences. However, these methods often lack a clear definition of similar users and face challenges when users with unique preferences lack appropriate neighbors. In such cases, relying on divergent preferences of misidentified neighbors can degrade recommendation quality. To address these limitations, this paper proposes an adaptive Collaborative Combinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided bandit architecture, applying bandit principles to both the user and item sides. The user-bandit employs an enhanced Bayesian model to explore user similarity, identifying neighbors based on a similarity probability threshold. The item-bandit treats items as arms, generating diverse recommendations informed by the user-bandit's output. CoCoB dynamically adapts, leveraging neighbor preferences when available or focusing solely on the target user otherwise. Regret analysis under a linear contextual bandit setting and experiments on three real-world datasets demonstrate CoCoB's effectiveness, achieving an average 2.4% improvement in F1 score over state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos</title>
<link>https://arxiv.org/abs/2505.03845</link>
<guid>https://arxiv.org/abs/2505.03845</guid>
<content:encoded><![CDATA[
arXiv:2505.03845v1 Announce Type: cross 
Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with motor and non-motor symptoms. Depressive symptoms are prevalent in PD, affecting up to 45% of patients. They are often underdiagnosed due to overlapping motor features, such as hypomimia. This study explores deep learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention layers-to assess the presence and severity of depressive symptoms, as detected by the Geriatric Depression Scale (GDS), in PD patients through facial video analysis. The same parameters were assessed in a secondary analysis taking into account whether patients were one hour after (ON-medication state) or 12 hours without (OFF-medication state) dopaminergic medication. Using a dataset of 1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest performance, with up to 94% accuracy and 93.7% F1-score in binary classification (presence of absence of depressive symptoms), and 87.1% accuracy with an 85.4% F1-score in multiclass tasks (absence or mild or severe depressive symptoms).
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques</title>
<link>https://arxiv.org/abs/2505.03848</link>
<guid>https://arxiv.org/abs/2505.03848</guid>
<content:encoded><![CDATA[
arXiv:2505.03848v1 Announce Type: cross 
Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for defect identification and yield optimization, yet often exceeds manual inspection capabilities. Traditional clustering techniques struggle with high-dimensional, unlabeled data, limiting their effectiveness in capturing nuanced patterns. This paper introduces an advanced clustering framework that integrates deep Topological Data Analysis (TDA) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. Transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. Validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. This study highlights the transformative potential of combining TDA, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially Private Densest-$k$-Subgraph</title>
<link>https://arxiv.org/abs/2505.03858</link>
<guid>https://arxiv.org/abs/2505.03858</guid>
<content:encoded><![CDATA[
arXiv:2505.03858v1 Announce Type: cross 
Abstract: Many graph datasets involve sensitive network data, motivating the need for privacy-preserving graph mining. The Densest-$k$-subgraph (D$k$S) problem is a key primitive in graph mining that aims to extract a subset of $k$ vertices with the maximum internal connectivity. Although non-private algorithms are known for D$k$S, this paper is the first to design algorithms that offer formal differential privacy (DP) guarantees for the problem. We base our general approach on using the principal component (PC) of the graph adjacency matrix to output a subset of $k$ vertices under edge DP. For this task, we first consider output perturbation, which traditionally offer good scalability, but at the expense of utility. Our tight on the local sensitivity indicate a big gap with the global sensitivity, motivating the use of instance specific sensitive methods for private PC. Next, we derive a tight bound on the smooth sensitivity and show that it can be close to the global sensitivity. This leads us to consider the Propose-Test-Release (PTR) framework for private PC. Although computationally expensive in general, we design a novel approach for implementing PTR in the same time as computation of a non-private PC, while offering good utility for \DkS{}. Additionally, we also consider the iterative private power method (PPM) for private PC, albeit it is significantly slower than PTR on large networks. We run our methods on diverse real-world networks, with the largest having 3 million vertices, and show good privacy-utility trade-offs. Although PTR requires a slightly larger privacy budget, on average, it achieves a 180-fold improvement in runtime over PPM.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical and geometric methods in statistical, manifold, and machine learning</title>
<link>https://arxiv.org/abs/2505.03862</link>
<guid>https://arxiv.org/abs/2505.03862</guid>
<content:encoded><![CDATA[
arXiv:2505.03862v1 Announce Type: cross 
Abstract: We present and discuss applications of the category of probabilistic morphisms, initially developed in \cite{Le2023}, as well as some geometric methods to several classes of problems in statistical, machine and manifold learning which shall be, along with many other topics, considered in depth in the forthcoming book \cite{LMPT2024}.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems</title>
<link>https://arxiv.org/abs/2505.03864</link>
<guid>https://arxiv.org/abs/2505.03864</guid>
<content:encoded><![CDATA[
arXiv:2505.03864v1 Announce Type: cross 
Abstract: Artificial intelligence is rapidly evolving towards multi-agent systems where numerous AI agents collaborate and interact with external tools. Two key open standards, Google's Agent to Agent (A2A) protocol for inter-agent communication and Anthropic's Model Context Protocol (MCP) for standardized tool access, promise to overcome the limitations of fragmented, custom integration approaches. While their potential synergy is significant, this paper argues that effectively integrating A2A and MCP presents unique, emergent challenges at their intersection, particularly concerning semantic interoperability between agent tasks and tool capabilities, the compounded security risks arising from combined discovery and execution, and the practical governance required for the envisioned "Agent Economy". This work provides a critical analysis, moving beyond a survey to evaluate the practical implications and inherent difficulties of combining these horizontal and vertical integration standards. We examine the benefits (e.g., specialization, scalability) while critically assessing their dependencies and trade-offs in an integrated context. We identify key challenges increased by the integration, including novel security vulnerabilities, privacy complexities, debugging difficulties across protocols, and the need for robust semantic negotiation mechanisms. In summary, A2A+MCP offers a vital architectural foundation, but fully realizing its potential requires substantial advancements to manage the complexities of their combined operation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.03906</link>
<guid>https://arxiv.org/abs/2505.03906</guid>
<content:encoded><![CDATA[
arXiv:2505.03906v1 Announce Type: cross 
Abstract: Large language models (LLMs) have transformed software development through code generation capabilities, yet their effectiveness for high-performance computing (HPC) remains limited. HPC code requires specialized optimizations for parallelism, memory efficiency, and architecture-specific considerations that general-purpose LLMs often overlook. We present MARCO (Multi-Agent Reactive Code Optimizer), a novel framework that enhances LLM-generated code for HPC through a specialized multi-agent architecture. MARCO employs separate agents for code generation and performance evaluation, connected by a feedback loop that progressively refines optimizations. A key innovation is MARCO's web-search component that retrieves real-time optimization techniques from recent conference proceedings and research publications, bridging the knowledge gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem set demonstrates that MARCO achieves a 14.6% average runtime reduction compared to Claude 3.5 Sonnet alone, while the integration of the web-search component yields a 30.9% performance improvement over the base MARCO system. These results highlight the potential of multi-agent systems to address the specialized requirements of high-performance code generation, offering a cost-effective alternative to domain-specific model fine-tuning.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers</title>
<link>https://arxiv.org/abs/2505.04002</link>
<guid>https://arxiv.org/abs/2505.04002</guid>
<content:encoded><![CDATA[
arXiv:2505.04002v1 Announce Type: cross 
Abstract: Humans excel in navigating diverse, complex environments with agile motor skills, exemplified by parkour practitioners performing dynamic maneuvers, such as climbing up walls and jumping across gaps. Reproducing these agile movements with simulated characters remains challenging, in part due to the scarcity of motion capture data for agile terrain traversal behaviors and the high cost of acquiring such data. In this work, we introduce PARC (Physics-based Augmentation with Reinforcement Learning for Character Controllers), a framework that leverages machine learning and physics-based simulation to iteratively augment motion datasets and expand the capabilities of terrain traversal controllers. PARC begins by training a motion generator on a small dataset consisting of core terrain traversal skills. The motion generator is then used to produce synthetic data for traversing new terrains. However, these generated motions often exhibit artifacts, such as incorrect contacts or discontinuities. To correct these artifacts, we train a physics-based tracking controller to imitate the motions in simulation. The corrected motions are then added to the dataset, which is used to continue training the motion generator in the next iteration. PARC's iterative process jointly expands the capabilities of the motion generator and tracker, creating agile and versatile models for interacting with complex environments. PARC provides an effective approach to develop controllers for agile terrain traversal, which bridges the gap between the scarcity of motion data and the need for versatile character controllers.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Formulation of the Particle Flow Particle Filter</title>
<link>https://arxiv.org/abs/2505.04007</link>
<guid>https://arxiv.org/abs/2505.04007</guid>
<content:encoded><![CDATA[
arXiv:2505.04007v1 Announce Type: cross 
Abstract: This paper provides a formulation of the particle flow particle filter from the perspective of variational inference. We show that the transient density used to derive the particle flow particle filter follows a time-scaled trajectory of the Fisher-Rao gradient flow in the space of probability densities. The Fisher-Rao gradient flow is obtained as a continuous-time algorithm for variational inference, minimizing the Kullback-Leibler divergence between a variational density and the true posterior density.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLOT: Structuring the Output of Large Language Models</title>
<link>https://arxiv.org/abs/2505.04016</link>
<guid>https://arxiv.org/abs/2505.04016</guid>
<content:encoded><![CDATA[
arXiv:2505.04016v1 Announce Type: cross 
Abstract: Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2505.04021</link>
<guid>https://arxiv.org/abs/2505.04021</guid>
<content:encoded><![CDATA[
arXiv:2505.04021v1 Announce Type: cross 
Abstract: Serving large language models (LLMs) is expensive, especially for providers hosting many models, making cost reduction essential. The unique workload patterns of serving multiple LLMs (i.e., multi-LLM serving) create new opportunities and challenges for this task. The long-tail popularity of models and their long idle periods present opportunities to improve utilization through GPU sharing. However, existing GPU sharing systems lack the ability to adjust their resource allocation and sharing policies at runtime, making them ineffective at meeting latency service-level objectives (SLOs) under rapidly fluctuating workloads.
  This paper presents Prism, a multi-LLM serving system that unleashes the full potential of GPU sharing to achieve both cost efficiency and SLO attainment. At its core, Prism tackles a key limitation of existing systems$\unicode{x2014}$the lack of $\textit{cross-model memory coordination}$, which is essential for flexibly sharing GPU memory across models under dynamic workloads. Prism achieves this with two key designs. First, it supports on-demand memory allocation by dynamically mapping physical to virtual memory pages, allowing flexible memory redistribution among models that space- and time-share a GPU. Second, it improves memory efficiency through a two-level scheduling policy that dynamically adjusts sharing strategies based on models' runtime demands. Evaluations on real-world traces show that Prism achieves more than $2\times$ cost savings and $3.3\times$ SLO attainment compared to state-of-the-art systems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.04034</link>
<guid>https://arxiv.org/abs/2505.04034</guid>
<content:encoded><![CDATA[
arXiv:2505.04034v1 Announce Type: cross 
Abstract: Biological neurons exhibit diverse temporal spike patterns, which are believed to support efficient, robust, and adaptive neural information processing. While models such as Izhikevich can replicate a wide range of these firing dynamics, their complexity poses challenges for directly integrating them into scalable spiking neural networks (SNN) training pipelines. In this work, we propose two probabilistically driven, input-level temporal spike transformations: Poisson-Burst and Delayed-Burst that introduce biologically inspired temporal variability directly into standard Leaky Integrate-and-Fire (LIF) neurons. This enables scalable training and systematic evaluation of how spike timing dynamics affect privacy, generalization, and learning performance. Poisson-Burst modulates burst occurrence based on input intensity, while Delayed-Burst encodes input strength through burst onset timing. Through extensive experiments across multiple benchmarks, we demonstrate that Poisson-Burst maintains competitive accuracy and lower resource overhead while exhibiting enhanced privacy robustness against membership inference attacks, whereas Delayed-Burst provides stronger privacy protection at a modest accuracy trade-off. These findings highlight the potential of biologically grounded temporal spike dynamics in improving the privacy, generalization and biological plausibility of neuromorphic learning systems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning based convex approximation for constrained parametric optimization</title>
<link>https://arxiv.org/abs/2505.04037</link>
<guid>https://arxiv.org/abs/2505.04037</guid>
<content:encoded><![CDATA[
arXiv:2505.04037v1 Announce Type: cross 
Abstract: We propose an input convex neural network (ICNN)-based self-supervised learning framework to solve continuous constrained optimization problems. By integrating the augmented Lagrangian method (ALM) with the constraint correction mechanism, our framework ensures \emph{non-strict constraint feasibility}, \emph{better optimality gap}, and \emph{best convergence rate} with respect to the state-of-the-art learning-based methods. We provide a rigorous convergence analysis, showing that the algorithm converges to a Karush-Kuhn-Tucker (KKT) point of the original problem even when the internal solver is a neural network, and the approximation error is bounded. We test our approach on a range of benchmark tasks including quadratic programming (QP), nonconvex programming, and large-scale AC optimal power flow problems. The results demonstrate that compared to existing solvers (e.g., \texttt{OSQP}, \texttt{IPOPT}) and the latest learning-based methods (e.g., DC3, PDL), our approach achieves a superior balance among accuracy, feasibility, and computational efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation</title>
<link>https://arxiv.org/abs/2505.04097</link>
<guid>https://arxiv.org/abs/2505.04097</guid>
<content:encoded><![CDATA[
arXiv:2505.04097v1 Announce Type: cross 
Abstract: A three-dimensional convolutional neural network was developed to classify T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid output. Using stochastic noise injection and five-fold cross-validation, the model achieved test set accuracy of 0.912 and area under the ROC curve of 0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity and specificity both exceeded 0.90. These results align with prior work reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate the effectiveness of simple augmentation for 3D MRI classification and motivate future exploration of advanced augmentation methods and architectures such as 3D U-Net and vision transformers.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models</title>
<link>https://arxiv.org/abs/2505.04135</link>
<guid>https://arxiv.org/abs/2505.04135</guid>
<content:encoded><![CDATA[
arXiv:2505.04135v1 Announce Type: cross 
Abstract: We explore the use of Chain-of-Thought (CoT) prompting with large language models (LLMs) to improve the accuracy of granular sentiment categorization in app store reviews. Traditional numeric and polarity-based ratings often fail to capture the nuanced sentiment embedded in user feedback. We evaluated the effectiveness of CoT prompting versus simple prompting on 2000 Amazon app reviews by comparing each method's predictions to human judgements. CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning in enhancing sentiment analysis performance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages</title>
<link>https://arxiv.org/abs/2505.04150</link>
<guid>https://arxiv.org/abs/2505.04150</guid>
<content:encoded><![CDATA[
arXiv:2505.04150v1 Announce Type: cross 
Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions. However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations. OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class. Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay</title>
<link>https://arxiv.org/abs/2505.04209</link>
<guid>https://arxiv.org/abs/2505.04209</guid>
<content:encoded><![CDATA[
arXiv:2505.04209v1 Announce Type: cross 
Abstract: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). The relevance of advertiser keyphrases plays an important role in preventing the inundation of search systems with numerous irrelevant items that compete for attention in auctions, in addition to maintaining a healthy seller perception. In this work, we describe the shortcomings of training Advertiser keyphrase relevance filter models on click/sales/search relevance signals and the importance of aligning with human judgment, as sellers have the power to adopt or reject said keyphrase recommendations. In this study, we frame Advertiser keyphrase relevance as a complex interaction between 3 dynamical systems -- seller judgment, which influences seller adoption of our product, Advertising, which provides the keyphrases to bid on, and Search, who holds the auctions for the same keyphrases. This study discusses the practicalities of using human judgment via a case study at eBay Advertising and demonstrate that using LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our relevance models achieves a better harmony across the three systems -- provided that they are bound by a meticulous evaluation framework grounded in business metrics.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Independent Adaptive RAG: Let the Question Speak for Itself</title>
<link>https://arxiv.org/abs/2505.04253</link>
<guid>https://arxiv.org/abs/2505.04253</guid>
<content:encoded><![CDATA[
arXiv:2505.04253v1 Announce Type: cross 
Abstract: Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning</title>
<link>https://arxiv.org/abs/2505.04300</link>
<guid>https://arxiv.org/abs/2505.04300</guid>
<content:encoded><![CDATA[
arXiv:2505.04300v1 Announce Type: cross 
Abstract: Biologically-informed neural networks typically leverage pathway annotations to enhance performance in biomedical applications. We hypothesized that the benefits of pathway integration does not arise from its biological relevance, but rather from the sparsity it introduces. We conducted a comprehensive analysis of all relevant pathway-based neural network models for predictive tasks, critically evaluating each study's contributions. From this review, we curated a subset of methods for which the source code was publicly available. The comparison of the biologically informed state-of-the-art deep learning models and their randomized counterparts showed that models based on randomized information performed equally well as biologically informed ones across different metrics and datasets. Notably, in 3 out of the 15 analyzed models, the randomized versions even outperformed their biologically informed counterparts. Moreover, pathway-informed models did not show any clear advantage in interpretability, as randomized models were still able to identify relevant disease biomarkers despite lacking explicit pathway information. Our findings suggest that pathway annotations may be too noisy or inadequately explored by current methods. Therefore, we propose a methodology that can be applied to different domains and can serve as a robust benchmark for systematically comparing novel pathway-informed models against their randomized counterparts. This approach enables researchers to rigorously determine whether observed performance improvements can be attributed to biological insights.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise</title>
<link>https://arxiv.org/abs/2505.04375</link>
<guid>https://arxiv.org/abs/2505.04375</guid>
<content:encoded><![CDATA[
arXiv:2505.04375v1 Announce Type: cross 
Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for downstream tasks is well-established. Still, the impact of model size on the performance of vision transformers in similar scenarios, particularly under label noise, remains largely unexplored. Given the utility and versatility of transformer architectures, this study investigates their practicality under low-budget constraints and noisy labels. We explore how classification accuracy and calibration are affected by symmetric label noise in active learning settings, evaluating four vision transformer configurations (Base and Large with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations (Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label noise rates. Our findings show that larger ViT models (ViTl32 in particular) consistently outperform their smaller counterparts in both accuracy and calibration, even under moderate to high label noise, while Swin Transformers exhibit weaker robustness across all noise levels. We find that smaller patch sizes do not always lead to better performance, as ViTl16 performs consistently worse than ViTl32 while incurring a higher computational cost. We also find that information-based Active Learning strategies only provide meaningful accuracy improvements at moderate label noise rates, but they result in poorer calibration compared to models trained on randomly acquired labels, especially at high label noise rates. We hope these insights provide actionable guidance for practitioners looking to deploy vision transformers in resource-constrained environments, where balancing model complexity, label noise, and compute efficiency is critical in model fine-tuning or distillation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Optimal Transport and Voice Conversion</title>
<link>https://arxiv.org/abs/2505.04382</link>
<guid>https://arxiv.org/abs/2505.04382</guid>
<content:encoded><![CDATA[
arXiv:2505.04382v1 Announce Type: cross 
Abstract: In this work, we address the voice conversion (VC) task using a vector-based interface. To align audio embeddings between speakers, we employ discrete optimal transport mapping. Our evaluation results demonstrate the high quality and effectiveness of this method. Additionally, we show that applying discrete optimal transport as a post-processing step in audio generation can lead to the incorrect classification of synthetic audio as real.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep residual learning with product units</title>
<link>https://arxiv.org/abs/2505.04397</link>
<guid>https://arxiv.org/abs/2505.04397</guid>
<content:encoded><![CDATA[
arXiv:2505.04397v1 Announce Type: cross 
Abstract: We propose a deep product-unit residual neural network (PURe) that integrates product units into residual blocks to improve the expressiveness and parameter efficiency of deep convolutional networks. Unlike standard summation neurons, product units enable multiplicative feature interactions, potentially offering a more powerful representation of complex patterns. PURe replaces conventional convolutional layers with 2D product units in the second layer of each residual block, eliminating nonlinear activation functions to preserve structural information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS, PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper ResNet152, while converging nearly five times faster and demonstrating strong robustness to Poisson noise. On ImageNet, PURe architectures outperform standard ResNet models at similar depths, with PURe34 achieving a top-1 accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet variants (ResNet50, ResNet101) while utilizing significantly fewer parameters and computational resources. On CIFAR-10, PURe consistently outperforms ResNet variants across varying depths, with PURe272 reaching 95.01% test accuracy, comparable to ResNet1001 but at less than half the model size. These results demonstrate that PURe achieves a favorable balance between accuracy, efficiency, and robustness. Compared to traditional residual networks, PURe not only achieves competitive classification performance with faster convergence and fewer parameters, but also demonstrates greater robustness to noise. Its effectiveness across diverse datasets highlights the potential of product-unit-based architectures for scalable and reliable deep learning in computer vision.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs</title>
<link>https://arxiv.org/abs/2505.04401</link>
<guid>https://arxiv.org/abs/2505.04401</guid>
<content:encoded><![CDATA[
arXiv:2505.04401v1 Announce Type: cross 
Abstract: Optimizing discrete phase shifts in large-scale reconfigurable intelligent surfaces (RISs) is challenging due to their non-convex and non-linear nature. In this letter, we propose a heuristic-integrated deep reinforcement learning (DRL) framework that (1) leverages accumulated actions over multiple steps in the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates a greedy algorithm (GA) into each DRL step to refine the state via fine-grained, element-wise optimization of RIS configurations. By learning from GA-included states, the proposed approach effectively addresses RIS optimization within a small DRL action space, demonstrating its capability to optimize phase-shift configurations of large-scale RISs.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.04416</link>
<guid>https://arxiv.org/abs/2505.04416</guid>
<content:encoded><![CDATA[
arXiv:2505.04416v1 Announce Type: cross 
Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recognizing Ornaments in Vocal Indian Art Music with Active Annotation</title>
<link>https://arxiv.org/abs/2505.04419</link>
<guid>https://arxiv.org/abs/2505.04419</guid>
<content:encoded><![CDATA[
arXiv:2505.04419v1 Announce Type: cross 
Abstract: Ornamentations, embellishments, or microtonal inflections are essential to melodic expression across many musical traditions, adding depth, nuance, and emotional impact to performances. Recognizing ornamentations in singing voices is key to MIR, with potential applications in music pedagogy, singer identification, genre classification, and controlled singing voice generation. However, the lack of annotated datasets and specialized modeling approaches remains a major obstacle for progress in this research area. In this work, we introduce R\=aga Ornamentation Detection (ROD), a novel dataset comprising Indian classical music recordings curated by expert musicians. The dataset is annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked as event-based labels. Using this dataset, we develop an ornamentation detection model based on deep time-series analysis, preserving ornament boundaries during the chunking of long audio recordings. We conduct experiments using different train-test configurations within the ROD dataset and also evaluate our approach on a separate, manually annotated dataset of Indian classical concert recordings. Our experimental results support the superior performance of our proposed approach over the baseline CRNN.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform</title>
<link>https://arxiv.org/abs/2505.04451</link>
<guid>https://arxiv.org/abs/2505.04451</guid>
<content:encoded><![CDATA[
arXiv:2505.04451v1 Announce Type: cross 
Abstract: Automatic music transcription (AMT) is the problem of analyzing an audio recording of a musical piece and detecting notes that are being played. AMT is a challenging problem, particularly when it comes to polyphonic music. The goal of AMT is to produce a score representation of a music piece, by analyzing a sound signal containing multiple notes played simultaneously. In this work, we design a processing pipeline that can transform classical piano audio files in .wav format into a music score representation. The features from the audio signals are extracted using the constant-Q transform, and the resulting coefficients are used as an input to the convolutional neural network (CNN) model.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tutorial on Discriminative Clustering and Mutual Information</title>
<link>https://arxiv.org/abs/2505.04484</link>
<guid>https://arxiv.org/abs/2505.04484</guid>
<content:encoded><![CDATA[
arXiv:2505.04484v1 Announce Type: cross 
Abstract: To cluster data is to separate samples into distinctive groups that should ideally have some cohesive properties. Today, numerous clustering algorithms exist, and their differences lie essentially in what can be perceived as ``cohesive properties''. Therefore, hypotheses on the nature of clusters must be set: they can be either generative or discriminative. As the last decade witnessed the impressive growth of deep clustering methods that involve neural networks to handle high-dimensional data often in a discriminative manner; we concentrate mainly on the discriminative hypotheses. In this paper, our aim is to provide an accessible historical perspective on the evolution of discriminative clustering methods and notably how the nature of assumptions of the discriminative models changed over time: from decision boundaries to invariance critics. We notably highlight how mutual information has been a historical cornerstone of the progress of (deep) discriminative clustering methods. We also show some known limitations of mutual information and how discriminative clustering methods tried to circumvent those. We then discuss the challenges that discriminative clustering faces with respect to the selection of the number of clusters. Finally, we showcase these techniques using the dedicated Python package, GemClus, that we have developed for discriminative clustering.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Flow Matching using Latent Variables</title>
<link>https://arxiv.org/abs/2505.04486</link>
<guid>https://arxiv.org/abs/2505.04486</guid>
<content:encoded><![CDATA[
arXiv:2505.04486v1 Announce Type: cross 
Abstract: Flow matching models have shown great potential in image generation tasks among probabilistic generative models. Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data. Most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present \texttt{Latent-CFM}, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\sim 50\%$ less in some cases) and computation than state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance</title>
<link>https://arxiv.org/abs/2505.04494</link>
<guid>https://arxiv.org/abs/2505.04494</guid>
<content:encoded><![CDATA[
arXiv:2505.04494v1 Announce Type: cross 
Abstract: We study reinforcement learning by combining recent advances in regularized linear programming formulations with the classical theory of stochastic approximation. Motivated by the challenge of designing algorithms that leverage off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a novel primal-dual Projected Gradient Descent-Ascent algorithm for solving regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience replay-based gradient estimation with a two-timescale decomposition of the underlying nested optimization problem. The algorithm operates asynchronously, interacts with the environment through a single trajectory of correlated data, and updates its policy online in response to the dual variable associated with the occupation measure of the underlying MDP. We prove that PGDA-RL converges almost surely to the optimal value function and policy of the regularized MDP. Our convergence analysis relies on tools from stochastic approximation theory and holds under weaker assumptions than those required by existing primal-dual RL approaches, notably removing the need for a simulator or a fixed behavioral policy.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration</title>
<link>https://arxiv.org/abs/2505.04524</link>
<guid>https://arxiv.org/abs/2505.04524</guid>
<content:encoded><![CDATA[
arXiv:2505.04524v1 Announce Type: cross 
Abstract: Cost-effective machine vision systems dedicated to real-time and accurate face detection and recognition in public places are crucial for many modern applications. However, despite their high performance, which could be reached using specialized edge or cloud AI hardware accelerators, there is still room for improvement in throughput and power consumption. This paper aims to suggest a combined hardware-software approach that optimizes face detection and recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX Orin. First, it leverages the simultaneous usage of all its hardware engines to improve processing time. This offers an improvement over previous works where these tasks were mainly allocated automatically and exclusively to the CPU or, to a higher extent, to the GPU core. Additionally, the paper suggests integrating a face tracker module to avoid redundantly running the face recognition algorithm for every frame but only when a new face appears in the scene. The results of extended experiments suggest that simultaneous usage of all the hardware engines that are available in the Orin GPU and tracker integration into the pipeline yield an impressive throughput of 290 FPS (frames per second) on 1920 x 1080 input size frames containing in average of 6 faces/frame. Additionally, a substantial saving of power consumption of around 800 mW was achieved when compared to running the task on the CPU/GPU engines only and without integrating a tracker into the Orin GPU\'92s pipeline. This hardware-codesign approach can pave the way to design high-performance machine vision systems at the edge, critically needed in video monitoring in public places where several nearby cameras are usually deployed for a same scene.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Componential Prompt-Knowledge Alignment for Domain Incremental Learning</title>
<link>https://arxiv.org/abs/2505.04575</link>
<guid>https://arxiv.org/abs/2505.04575</guid>
<content:encoded><![CDATA[
arXiv:2505.04575v1 Announce Type: cross 
Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. KA-Prompt operates in two phases: (1) Initial Componential Structure Configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) Online Alignment Preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. Extensive experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt. Our source code is available at https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions</title>
<link>https://arxiv.org/abs/2505.04579</link>
<guid>https://arxiv.org/abs/2505.04579</guid>
<content:encoded><![CDATA[
arXiv:2505.04579v1 Announce Type: cross 
Abstract: In collaborative tasks, autonomous agents fall short of humans in their capability to quickly adapt to new and unfamiliar teammates. We posit that a limiting factor for zero-shot coordination is the lack of shared task abstractions, a mechanism humans rely on to implicitly align with teammates. To address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework leveraging hierarchical reinforcement learning to mimic the structured approach humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment, demonstrating statistically significant improvement over existing baselines when paired with both unseen agents and humans, providing better resilience to environmental shifts, and outperforming all state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees</title>
<link>https://arxiv.org/abs/2505.04583</link>
<guid>https://arxiv.org/abs/2505.04583</guid>
<content:encoded><![CDATA[
arXiv:2505.04583v1 Announce Type: cross 
Abstract: Rehabilitation robots are often used in game-like interactions for rehabilitation to increase a person's motivation to complete rehabilitation exercises. By adjusting exercise difficulty for a specific user throughout the exercise interaction, robots can maximize both the user's rehabilitation outcomes and the their motivation throughout the exercise. Previous approaches have assumed exercises have generic difficulty values that apply to all users equally, however, we identified that stroke survivors have varied and unique perceptions of exercise difficulty. For example, some stroke survivors found reaching vertically more difficult than reaching farther but lower while others found reaching farther more challenging than reaching vertically. In this paper, we formulate a causal tree-based method to calculate exercise difficulty based on the user's performance. We find that this approach accurately models exercise difficulty and provides a readily interpretable model of why that exercise is difficult for both users and caretakers.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Sampling for MRI-based Sequential Decision Making</title>
<link>https://arxiv.org/abs/2505.04586</link>
<guid>https://arxiv.org/abs/2505.04586</guid>
<content:encoded><![CDATA[
arXiv:2505.04586v1 Announce Type: cross 
Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching</title>
<link>https://arxiv.org/abs/2505.04603</link>
<guid>https://arxiv.org/abs/2505.04603</guid>
<content:encoded><![CDATA[
arXiv:2505.04603v1 Announce Type: cross 
Abstract: When the likelihood is analytically unavailable and computationally intractable, approximate Bayesian computation (ABC) has emerged as a widely used methodology for approximate posterior inference; however, it suffers from severe computational inefficiency in high-dimensional settings or under diffuse priors. To overcome these limitations, we propose Adaptive Bayesian Inference (ABI), a framework that bypasses traditional data-space discrepancies and instead compares distributions directly in posterior space through nonparametric distribution matching. By leveraging a novel Marginally-augmented Sliced Wasserstein (MSW) distance on posterior measures and exploiting its quantile representation, ABI transforms the challenging problem of measuring divergence between posterior distributions into a tractable sequence of one-dimensional conditional quantile regression tasks. Moreover, we introduce a new adaptive rejection sampling scheme that iteratively refines the posterior approximation by updating the proposal distribution via generative density estimation. Theoretically, we establish parametric convergence rates for the trimmed MSW distance and prove that the ABI posterior converges to the true posterior as the tolerance threshold vanishes. Through extensive empirical evaluation, we demonstrate that ABI significantly outperforms data-based Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free simulators, especially in high-dimensional or dependent observation regimes.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Two Sample Testing to Singular Gaussian Discrimination</title>
<link>https://arxiv.org/abs/2505.04613</link>
<guid>https://arxiv.org/abs/2505.04613</guid>
<content:encoded><![CDATA[
arXiv:2505.04613v1 Announce Type: cross 
Abstract: We establish that testing for the equality of two probability measures on a general separable and compact metric space is equivalent to testing for the singularity between two corresponding Gaussian measures on a suitable Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via the notion of kernel mean and covariance embedding of a probability measure. Discerning two singular Gaussians is fundamentally simpler from an information-theoretic perspective than non-parametric two-sample testing, particularly in high-dimensional settings. Our proof leverages the Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert spaces, and shows that discrepancies between distributions are heavily magnified through their corresponding Gaussian embeddings: at a population level, distinct probability measures lead to essentially separated Gaussian embeddings. This appears to be a new instance of the blessing of dimensionality that can be harnessed for the design of efficient inference tools in great generality.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond</title>
<link>https://arxiv.org/abs/2505.04621</link>
<guid>https://arxiv.org/abs/2505.04621</guid>
<content:encoded><![CDATA[
arXiv:2505.04621v1 Announce Type: cross 
Abstract: We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS) to text-conditioned audio diffusion models. While SDS was initially designed for text-to-3D generation using image diffusion, its core idea of distilling a powerful generative prior into a separate parametric representation extends to the audio domain. Leveraging a single pretrained model, Audio-SDS enables a broad range of tasks without requiring specialized datasets. In particular, we demonstrate how Audio-SDS can guide physically informed impact sound simulations, calibrate FM-synthesis parameters, and perform prompt-specified source separation. Our findings illustrate the versatility of distillation-based methods across modalities and establish a robust foundation for future work using generative priors in audio tasks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is the end of Insight in Sight ?</title>
<link>https://arxiv.org/abs/2505.04627</link>
<guid>https://arxiv.org/abs/2505.04627</guid>
<content:encoded><![CDATA[
arXiv:2505.04627v1 Announce Type: cross 
Abstract: It is shown that the weight matrices of a Physics-informed neural network (PINN)-based deep learning application to a rarefied gas dynamics problem described by the Boltzmann equation bear no evident link to the mathematical structure of the physical problem. Instead, the weights appear close to Gaussian distributed random matrices. Although significantly more work is needed to support a robust assessment in this direction, these results suggest that deep-learning and the numerical solution of the Boltzmann equation represent two equivalent, but largely distinct paths to the same physical knowledge. If so, Explainable AI might be an unrealistic target and possibly even an ill-posed one.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Cryptanalysis of a Quantum Random Number Generator</title>
<link>https://arxiv.org/abs/1905.02342</link>
<guid>https://arxiv.org/abs/1905.02342</guid>
<content:encoded><![CDATA[
arXiv:1905.02342v3 Announce Type: replace 
Abstract: Random number generators (RNGs) that are crucial for cryptographic applications have been the subject of adversarial attacks. These attacks exploit environmental information to predict generated random numbers that are supposed to be truly random and unpredictable. Though quantum random number generators (QRNGs) are based on the intrinsic indeterministic nature of quantum properties, the presence of classical noise in the measurement process compromises the integrity of a QRNG. In this paper, we develop a predictive machine learning (ML) analysis to investigate the impact of deterministic classical noise in different stages of an optical continuous variable QRNG. Our ML model successfully detects inherent correlations when the deterministic noise sources are prominent. After appropriate filtering and randomness extraction processes are introduced, our QRNG system, in turn, demonstrates its robustness against ML. We further demonstrate the robustness of our ML approach by applying it to uniformly distributed random numbers from the QRNG and a congruential RNG. Hence, our result shows that ML has potentials in benchmarking the quality of RNG devices.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods</title>
<link>https://arxiv.org/abs/2304.01215</link>
<guid>https://arxiv.org/abs/2304.01215</guid>
<content:encoded><![CDATA[
arXiv:2304.01215v2 Announce Type: replace 
Abstract: The beekeeping sector has experienced significant production fluctuations in recent years, largely due to increasingly frequent adverse weather events linked to climate change. These events can severely affect the environment, reducing its suitability for bee activity. We conduct a forecasting analysis of honey production across Italy using a range of machine learning models, with a particular focus on weather-related variables as key predictors. Our analysis relies on a dataset collected in 2022, which combines hive-level observations with detailed weather data. We train and compare several linear and nonlinear models, evaluating both their predictive accuracy and interpretability. By examining model explanations, we identify the main drivers of honey production. We also ensemble models from different families to assess whether combining predictions improves forecast accuracy. These insights support beekeepers in managing production risks and may inform the development of insurance products against unexpected losses due to poor harvests.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disjunctive Branch-And-Bound for Certifiably Optimal Low-Rank Matrix Completion</title>
<link>https://arxiv.org/abs/2305.12292</link>
<guid>https://arxiv.org/abs/2305.12292</guid>
<content:encoded><![CDATA[
arXiv:2305.12292v3 Announce Type: replace 
Abstract: Low-rank matrix completion consists of computing a matrix of minimal complexity that recovers a given set of observations as accurately as possible. Unfortunately, existing methods for matrix completion are heuristics that, while highly scalable and often identifying high-quality solutions, do not possess any optimality guarantees. We reexamine matrix completion with an optimality-oriented eye. We reformulate low-rank matrix completion problems as convex problems over the non-convex set of projection matrices and implement a disjunctive branch-and-bound scheme that solves them to certifiable optimality. Further, we derive a novel and often near-exact class of convex relaxations by decomposing a low-rank matrix as a sum of rank-one matrices and incentivizing that two-by-two minors in each rank-one matrix have determinant zero. In numerical experiments, our new convex relaxations decrease the optimality gap by two orders of magnitude compared to existing attempts, and our disjunctive branch-and-bound scheme solves $n \times m$ rank-$r$ matrix completion problems to certifiable optimality or near optimality in hours for $\max \{m, n\} \leq 2500$ and $r \leq 5$. Moreover, this improvement in the training error translates into an average $2\%$--$50\%$ improvement in the test set error.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contaminated Multivariate Time-Series Anomaly Detection with Spatio-Temporal Graph Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2308.12563</link>
<guid>https://arxiv.org/abs/2308.12563</guid>
<content:encoded><![CDATA[
arXiv:2308.12563v4 Announce Type: replace 
Abstract: Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data is contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three core modules: a Decontaminator to rectify anomalies (aka noise) present during training, a Long-range Variable Dependency Modeling module to capture long-term intra- and inter-variable dependencies within the decontaminated data that is considered as a surrogate of the pure normal data, and an Anomaly Scoring module to detect anomalies from all types. Our extensive experiments conducted on four reliable and diverse datasets conclusively demonstrate that TSAD-C surpasses existing methodologies, thus establishing a new state-of-the-art in the TSAD field.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder</title>
<link>https://arxiv.org/abs/2310.10745</link>
<guid>https://arxiv.org/abs/2310.10745</guid>
<content:encoded><![CDATA[
arXiv:2310.10745v3 Announce Type: replace 
Abstract: The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields an approximate closure of the dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the accuracy and stability of the Koopman operator approximation. Demonstrations showcase the technique's improved predictive capability for flow around a cylinder. It also provides a low dimensional approximation for Kuramoto-Sivashinsky (KS) with promising short-term predictability and robust long-term statistical performance. By bridging the gap between data-driven techniques and the mathematical foundations of Koopman theory, MZ-AE offers a promising avenue for improved understanding and prediction of complex nonlinear dynamics.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hard-Negative Sampling for Contrastive Learning: Optimal Representation Geometry and Neural- vs Dimensional-Collapse</title>
<link>https://arxiv.org/abs/2311.05139</link>
<guid>https://arxiv.org/abs/2311.05139</guid>
<content:encoded><![CDATA[
arXiv:2311.05139v3 Announce Type: replace 
Abstract: For a widely-studied data model and general loss and sample-hardening functions we prove that the losses of Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) are minimized by representations that exhibit Neural-Collapse (NC), i.e., the class means form an Equiangular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) losses are lower bounded by the corresponding SCL and UCL losses. In contrast to existing literature, our theoretical results for SCL do not require class-conditional independence of augmented views and work for a general loss function class that includes the widely used InfoNCE loss function. Moreover, our proofs are simpler, compact, and transparent. Similar to existing literature, our theoretical claims also hold for the practical scenario where batching is used for optimization. We empirically demonstrate, for the first time, that Adam optimization (with batching) of HSCL and HUCL losses with random initialization and suitable hardness levels can indeed converge to the NC-geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard-negatives or feature normalization, however, the representations learned via Adam suffer from Dimensional-Collapse (DC) and fail to attain the NC-geometry. These results exemplify the role of hard-negative sampling in contrastive representation learning and we conclude with several open theoretical problems for future work. The code can be found at https://github.com/rjiang03/HCL/tree/main
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransAxx: Efficient Transformers with Approximate Computing</title>
<link>https://arxiv.org/abs/2402.07545</link>
<guid>https://arxiv.org/abs/2402.07545</guid>
<content:encoded><![CDATA[
arXiv:2402.07545v2 Announce Type: replace 
Abstract: Vision Transformer (ViT) models which were recently introduced by the transformer architecture have shown to be very competitive and often become a popular alternative to Convolutional Neural Networks (CNNs). However, the high computational requirements of these models limit their practical applicability especially on low-power devices. Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models. In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models. Using TransAxx we analyze the sensitivity of transformer models on the ImageNet dataset to approximate multiplications and perform approximate-aware finetuning to regain accuracy. Furthermore, we propose a methodology to generate approximate accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS) algorithm to efficiently search the space of possible configurations using a hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy of our methodology in achieving significant trade-offs between accuracy and power, resulting in substantial gains without compromising on performance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral-Refiner: Accurate Fine-Tuning of Spatiotemporal Fourier Neural Operator for Turbulent Flows</title>
<link>https://arxiv.org/abs/2405.17211</link>
<guid>https://arxiv.org/abs/2405.17211</guid>
<content:encoded><![CDATA[
arXiv:2405.17211v3 Announce Type: replace 
Abstract: Recent advancements in operator-type neural networks have shown promising results in approximating the solutions of spatiotemporal Partial Differential Equations (PDEs). However, these neural networks often entail considerable training expenses, and may not always achieve the desired accuracy required in many scientific and engineering disciplines. In this paper, we propose a new learning framework to address these issues. A new spatiotemporal adaptation is proposed to generalize any Fourier Neural Operator (FNO) variant to learn maps between Bochner spaces, which can perform an arbitrary-length temporal super-resolution for the first time. To better exploit this capacity, a new paradigm is proposed to refine the commonly adopted end-to-end neural operator training and evaluations with the help from the wisdom from traditional numerical PDE theory and techniques. Specifically, in the learning problems for the turbulent flow modeled by the Navier-Stokes Equations (NSE), the proposed paradigm trains an FNO only for a few epochs. Then, only the newly proposed spatiotemporal spectral convolution layer is fine-tuned without the frequency truncation. The spectral fine-tuning loss function uses a negative Sobolev norm for the first time in operator learning, defined through a reliable functional-type a posteriori error estimator whose evaluation is exact thanks to the Parseval identity. Moreover, unlike the difficult nonconvex optimization problems in the end-to-end training, this fine-tuning loss is convex. Numerical experiments on commonly used NSE benchmarks demonstrate significant improvements in both computational efficiency and accuracy, compared to end-to-end evaluation and traditional numerical PDE solvers under certain conditions. The source code is publicly available at https://github.com/scaomath/torch-cfd.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed-Curvature Decision Trees and Random Forests</title>
<link>https://arxiv.org/abs/2406.05227</link>
<guid>https://arxiv.org/abs/2406.05227</guid>
<content:encoded><![CDATA[
arXiv:2406.05227v3 Announce Type: replace 
Abstract: We extend decision tree and random forest algorithms to product space manifolds: Cartesian products of Euclidean, hyperspherical, and hyperbolic manifolds. Such spaces have extremely expressive geometries capable of representing many arrangements of distances with low metric distortion. To date, all classifiers for product spaces fit a single linear decision boundary, and no regressor has been described. Our method enables a simple, expressive method for classification and regression in product manifolds. We demonstrate the superior accuracy of our tool compared to Euclidean methods operating in the ambient space or the tangent plane of the manifold across a range of constant-curvature and product manifolds. Code for our implementation and experiments is available at https://github.com/pchlenski/embedders.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Learning Loss Functions for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2406.09713</link>
<guid>https://arxiv.org/abs/2406.09713</guid>
<content:encoded><![CDATA[
arXiv:2406.09713v3 Announce Type: replace 
Abstract: Humans can often quickly and efficiently solve complex new learning tasks given only a small set of examples. In contrast, modern artificially intelligent systems often require thousands or millions of observations in order to solve even the most basic tasks. Meta-learning aims to resolve this issue by leveraging past experiences from similar learning tasks to embed the appropriate inductive biases into the learning system. Historically methods for meta-learning components such as optimizers, parameter initializations, and more have led to significant performance increases. This thesis aims to explore the concept of meta-learning to improve performance, through the often-overlooked component of the loss function. The loss function is a vital component of a learning system, as it represents the primary learning objective, where success is determined and quantified by the system's ability to optimize for that objective successfully.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing</title>
<link>https://arxiv.org/abs/2409.16832</link>
<guid>https://arxiv.org/abs/2409.16832</guid>
<content:encoded><![CDATA[
arXiv:2409.16832v5 Announce Type: replace 
Abstract: In the realm of emerging real-time networked applications like cyber-physical systems (CPS), the Age of Information (AoI) has merged as a pivotal metric for evaluating the timeliness. To meet the high computational demands, such as those in intelligent manufacturing within CPS, mobile edge computing (MEC) presents a promising solution for optimizing computing and reducing AoI. In this work, we study the timeliness of computational-intensive updates and explores jointly optimize the task updating and offloading policies to minimize AoI. Specifically, we consider edge load dynamics and formulate a task scheduling problem to minimize the expected time-average AoI. The fractional objective introduced by AoI and the semi-Markov game nature of the problem render this challenge particularly difficult, with existing approaches not directly applicable. To this end, we present a comprehensive framework to fractional reinforcement learning (RL). We first introduce a fractional single-agent RL framework and prove its linear convergence. We then extend this to a fractional multi-agent RL framework with a convergence analysis. To tackle the challenge of asynchronous control in semi-Markov game, we further design an asynchronous model-free fractional multi-agent RL algorithm, where each device makes scheduling decisions with the hybrid action space without knowing the system dynamics and decisions of other devices. Experimental results show that our proposed algorithms reduce the average AoI by up to 52.6% compared with the best baseline algorithm in our experiments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners</title>
<link>https://arxiv.org/abs/2410.02131</link>
<guid>https://arxiv.org/abs/2410.02131</guid>
<content:encoded><![CDATA[
arXiv:2410.02131v3 Announce Type: replace 
Abstract: The accurate interpretation of Electrocardiogram (ECG) signals is pivotal for diagnosing cardiovascular diseases. Integrating ECG signals with accompanying textual reports further holds immense potential to enhance clinical diagnostics by combining physiological data and qualitative insights. However, this integration faces significant challenges due to inherent modality disparities and the scarcity of labeled data for robust cross-modal learning. To address these obstacles, we propose D-BETA, a novel framework that pre-trains ECG and text data using a contrastive masked auto-encoder architecture. D-BETA uniquely combines the strengths of generative with boosted discriminative capabilities to achieve robust cross-modal representations. This is accomplished through masked modality modeling, specialized loss functions, and an improved negative sampling strategy tailored for cross-modal alignment. Extensive experiments on five public datasets across diverse downstream tasks demonstrate that D-BETA significantly outperforms existing methods, achieving an average AUC improvement of 15% in linear probing with only one percent of training data and 2% in zero-shot performance without requiring training data over state-of-the-art models. These results highlight the effectiveness of D-BETA, underscoring its potential to advance automated clinical diagnostics through multi-modal representations. Our sample code and checkpoint are made available at https://github.com/manhph2211/D-BETA.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Batched Bayesian optimization by maximizing the probability of including the optimum</title>
<link>https://arxiv.org/abs/2410.06333</link>
<guid>https://arxiv.org/abs/2410.06333</guid>
<content:encoded><![CDATA[
arXiv:2410.06333v2 Announce Type: replace 
Abstract: Batched Bayesian optimization (BO) can accelerate molecular design by efficiently identifying top-performing compounds from a large chemical library. Existing acquisition strategies for batch design in BO aim to balance exploration and exploitation. This often involves optimizing non-additive batch acquisition functions, necessitating approximation via myopic construction and/or diversity heuristics. In this work, we propose an acquisition strategy for discrete optimization that is motivated by pure exploitation, qPO (multipoint Probability of Optimality). qPO maximizes the probability that the batch includes the true optimum, which is expressible as the sum over individual acquisition scores and thereby circumvents the combinatorial challenge of optimizing a batch acquisition function. We differentiate the proposed strategy from parallel Thompson sampling and discuss how it implicitly captures diversity. Finally, we apply our method to the model-guided exploration of large chemical libraries and provide empirical evidence that it is competitive with and complements other state-of-the-art methods in batched Bayesian optimization.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling</title>
<link>https://arxiv.org/abs/2410.06397</link>
<guid>https://arxiv.org/abs/2410.06397</guid>
<content:encoded><![CDATA[
arXiv:2410.06397v2 Announce Type: replace 
Abstract: Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators require hybrid analog/digital algorithms to solve real-world problems, commonly using large-neighborhood local search (LNLS) frameworks. Unlike fully digital algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no principled hyperparameter selection schemes, particularly limiting cross-device training and inference.
  In this work, we provide non-asymptotic convergence guarantees for hybrid LNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools from classical sampling theory, we prove exponential KL-divergence convergence for randomized and cyclic block selection strategies using ideal DXs. With finite device variation, we provide explicit bounds on the 2-Wasserstein bias in terms of step duration, noise strength, and function parameters. Our BLD model provides a key link between established theory and novel computing platforms, and our theoretical results provide a closed-form expression linking device variation, algorithm hyperparameters, and performance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Lagrangian Wasserstein Flow for Time Series Imputation</title>
<link>https://arxiv.org/abs/2410.07550</link>
<guid>https://arxiv.org/abs/2410.07550</guid>
<content:encoded><![CDATA[
arXiv:2410.07550v2 Announce Type: replace 
Abstract: Time series imputation is important for numerous real-world applications. To overcome the limitations of diffusion model-based imputation methods, e.g., slow convergence in inference, we propose a novel method for time series imputation in this work, called Conditional Lagrangian Wasserstein Flow (CLWF). Following the principle of least action in Lagrangian mechanics, we learn the velocity by minimizing the corresponding kinetic energy. Moreover, to enhance the model's performance, we estimate the gradient of a task-specific potential function using a time-dependent denoising autoencoder and integrate it into the base estimator to reduce the sampling variance. Finally, the proposed method demonstrates competitive performance compared to other state-of-the-art imputation approaches.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Causality in Reinforcement Learning With Bagged Decision Times</title>
<link>https://arxiv.org/abs/2410.14659</link>
<guid>https://arxiv.org/abs/2410.14659</guid>
<content:encoded><![CDATA[
arXiv:2410.14659v3 Announce Type: replace 
Abstract: We consider reinforcement learning (RL) for a class of problems with bagged decision times. A bag contains a finite sequence of consecutive decision times. The transition dynamics are non-Markovian and non-stationary within a bag. All actions within a bag jointly impact a single reward, observed at the end of the bag. For example, in mobile health, multiple activity suggestions in a day collectively affect a user's daily commitment to being active. Our goal is to develop an online RL algorithm to maximize the discounted sum of the bag-specific rewards. To handle non-Markovian transitions within a bag, we utilize an expert-provided causal directed acyclic graph (DAG). Based on the DAG, we construct states as a dynamical Bayesian sufficient statistic of the observed history, which results in Markov state transitions within and across bags. We then formulate this problem as a periodic Markov decision process (MDP) that allows non-stationarity within a period. An online RL algorithm based on Bellman equations for stationary MDPs is generalized to handle periodic MDPs. We show that our constructed state achieves the maximal optimal value function among all state constructions for a periodic MDP. Finally, we evaluate the proposed method on testbed variants built from real data in a mobile health clinical trial.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification</title>
<link>https://arxiv.org/abs/2410.22377</link>
<guid>https://arxiv.org/abs/2410.22377</guid>
<content:encoded><![CDATA[
arXiv:2410.22377v2 Announce Type: replace 
Abstract: In recent years, spatio-temporal graph neural networks (GNNs) have attracted considerable interest in the field of time series analysis, due to their ability to capture dependencies among variables and across time points. The objective of the presented systematic literature review is hence to provide a comprehensive overview of the various modeling approaches and application domains of GNNs for time series classification and forecasting. A database search was conducted, and over 150 journal papers were selected for a detailed examination of the current state-of-the-art in the field. This examination is intended to offer to the reader a comprehensive collection of proposed models, links to related source code, available datasets, benchmark models, and fitting results. All this information is hoped to assist researchers in future studies. To the best of our knowledge, this is the first systematic literature review presenting a detailed comparison of the results of current spatio-temporal GNN models in different domains. In addition, in its final part this review discusses current limitations and challenges in the application of spatio-temporal GNNs, such as comparability, reproducibility, explainability, poor information capacity, and scalability.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VecCity: A Taxonomy-guided Library for Map Entity Representation Learning</title>
<link>https://arxiv.org/abs/2411.00874</link>
<guid>https://arxiv.org/abs/2411.00874</guid>
<content:encoded><![CDATA[
arXiv:2411.00874v2 Announce Type: replace 
Abstract: Electronic maps consist of diverse entities, such as points of interest (POIs), road networks, and land parcels, playing a vital role in applications like ITS and LBS. Map entity representation learning (MapRL) generates versatile and reusable data representations, providing essential tools for efficiently managing and utilizing map entity data. Despite the progress in MapRL, two key challenges constrain further development. First, existing research is fragmented, with models classified by the type of map entity, limiting the reusability of techniques across different tasks. Second, the lack of unified benchmarks makes systematic evaluation and comparison of models difficult. To address these challenges, we propose a novel taxonomy for MapRL that organizes models based on functional module-such as encoders, pre-training tasks, and downstream tasks-rather than by entity type. Building on this taxonomy, we present a taxonomy-driven library, VecCity, which offers easy-to-use interfaces for encoding, pre-training, fine-tuning, and evaluation. The library integrates datasets from nine cities and reproduces 21 mainstream MapRL models, establishing the first standardized benchmarks for the field. VecCity also allows users to modify and extend models through modular components, facilitating seamless experimentation. Our comprehensive experiments cover multiple types of map entities and evaluate 21 VecCity pre-built models across various downstream tasks. Experimental results demonstrate the effectiveness of VecCity in streamlining model development and provide insights into the impact of various components on performance. By promoting modular design and reusability, VecCity offers a unified framework to advance research and innovation in MapRL. The code is available at https://github.com/Bigscity-VecCity/VecCity.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SUICA: Learning Super-high Dimensional Sparse Implicit Neural Representations for Spatial Transcriptomics</title>
<link>https://arxiv.org/abs/2412.01124</link>
<guid>https://arxiv.org/abs/2412.01124</guid>
<content:encoded><![CDATA[
arXiv:2412.01124v2 Announce Type: replace 
Abstract: Spatial Transcriptomics (ST) is a method that captures gene expression profiles aligned with spatial coordinates. The discrete spatial distribution and the super-high dimensional sequencing results make ST data challenging to be modeled effectively. In this paper, we manage to model ST in a continuous and compact manner by the proposed tool, SUICA, empowered by the great approximation capability of Implicit Neural Representations (INRs) that can enhance both the spatial density and the gene expression. Concretely within the proposed SUICA, we incorporate a graph-augmented Autoencoder to effectively model the context information of the unstructured spots and provide informative embeddings that are structure-aware for spatial mapping. We also tackle the extremely skewed distribution in a regression-by-classification fashion and enforce classification-based loss functions for the optimization of SUICA. By extensive experiments of a wide range of common ST platforms under varying degradations, SUICA outperforms both conventional INR variants and SOTA methods regarding numerical fidelity, statistical correlation, and bio-conservation. The prediction by SUICA also showcases amplified gene signatures that enriches the bio-conservation of the raw data and benefits subsequent analysis. The code is available at https://github.com/Szym29/SUICA.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Photovoltaic Power Forecasting: An iTransformer and LSTM-Based Model Integrating Temporal and Covariate Interactions</title>
<link>https://arxiv.org/abs/2412.02302</link>
<guid>https://arxiv.org/abs/2412.02302</guid>
<content:encoded><![CDATA[
arXiv:2412.02302v2 Announce Type: replace 
Abstract: Accurate photovoltaic (PV) power forecasting is critical for integrating renewable energy sources into the grid, optimizing real-time energy management, and ensuring energy reliability amidst increasing demand. However, existing models often struggle with effectively capturing the complex relationships between target variables and covariates, as well as the interactions between temporal dynamics and multivariate data, leading to suboptimal forecasting accuracy. To address these challenges, we propose a novel model architecture that leverages the iTransformer for feature extraction from target variables and employs long short-term memory (LSTM) to extract features from covariates. A cross-attention mechanism is integrated to fuse the outputs of both models, followed by a Kolmogorov-Arnold network (KAN) mapping for enhanced representation. The effectiveness of the proposed model is validated using publicly available datasets from Australia, with experiments conducted across four seasons. Results demonstrate that the proposed model effectively capture seasonal variations in PV power generation and improve forecasting accuracy.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information-Geometric Barycenters for Bayesian Federated Learning</title>
<link>https://arxiv.org/abs/2412.11646</link>
<guid>https://arxiv.org/abs/2412.11646</guid>
<content:encoded><![CDATA[
arXiv:2412.11646v2 Announce Type: replace 
Abstract: Federated learning (FL) is a widely used and impactful distributed optimization framework that achieves consensus through averaging locally trained models. While effective, this approach may not align well with Bayesian inference, where the model space has the structure of a distribution space. Taking an information-geometric perspective, we reinterpret FL aggregation as the problem of finding the barycenter of local posteriors using a prespecified divergence metric, minimizing the average discrepancy across clients. This perspective provides a unifying framework that generalizes many existing methods and offers crisp insights into their theoretical underpinnings. We then propose BA-BFL, an algorithm that retains the convergence properties of Federated Averaging in non-convex settings. In non-independent and identically distributed scenarios, we conduct extensive comparisons with statistical aggregation techniques, showing that BA-BFL achieves performance comparable to state-of-the-art methods while offering a geometric interpretation of the aggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep Learning, exploring the impact of Bayesian layers on uncertainty quantification and model calibration.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence</title>
<link>https://arxiv.org/abs/2412.13779</link>
<guid>https://arxiv.org/abs/2412.13779</guid>
<content:encoded><![CDATA[
arXiv:2412.13779v2 Announce Type: replace 
Abstract: Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding knowledge forgetting of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named FedSSI, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Incremental Learning under Ambiguous Supervision</title>
<link>https://arxiv.org/abs/2501.13584</link>
<guid>https://arxiv.org/abs/2501.13584</guid>
<content:encoded><![CDATA[
arXiv:2501.13584v4 Announce Type: replace 
Abstract: Traditional Incremental Learning (IL) targets to handle sequential fully-supervised learning problems where novel classes emerge from time to time. However, due to inherent annotation uncertainty and ambiguity, collecting high-quality annotated data in a dynamic learning system can be extremely expensive. To mitigate this problem, we propose a novel weakly-supervised learning paradigm called Incremental Partial Label Learning (IPLL), where the sequentially arrived data relate to a set of candidate labels rather than the ground truth. Technically, we develop the Prototype-Guided Disambiguation and Replay Algorithm (PGDR) which leverages the class prototypes as a proxy to mitigate two intertwined challenges in IPLL, i.e., label ambiguity and catastrophic forgetting. To handle the former, PGDR encapsulates a momentum-based pseudo-labeling algorithm along with prototype-guided initialization, resulting in a balanced perception of classes. To alleviate forgetting, we develop a memory replay technique that collects well-disambiguated samples while maintaining representativeness and diversity. By jointly distilling knowledge from curated memory data, our framework exhibits a great disambiguation ability for samples of new tasks and achieves less forgetting of knowledge. Extensive experiments demonstrate that PGDR achieves superior
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression</title>
<link>https://arxiv.org/abs/2501.13790</link>
<guid>https://arxiv.org/abs/2501.13790</guid>
<content:encoded><![CDATA[
arXiv:2501.13790v2 Announce Type: replace 
Abstract: We analyze two variants of Local Gradient Descent applied to distributed logistic regression with heterogeneous, separable data and show convergence at the rate $O(1/KR)$ for $K$ local steps and sufficiently large $R$ communication rounds. In contrast, all existing convergence guarantees for Local GD applied to any problem are at least $\Omega(1/R)$, meaning they fail to show the benefit of local updates. The key to our improved guarantee is showing progress on the logistic regression objective when using a large stepsize $\eta \gg 1/K$, whereas prior analysis depends on $\eta \leq 1/K$.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Model Selection and Reuse for Downstream Adaptation</title>
<link>https://arxiv.org/abs/2501.18271</link>
<guid>https://arxiv.org/abs/2501.18271</guid>
<content:encoded><![CDATA[
arXiv:2501.18271v2 Announce Type: replace 
Abstract: Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called Model Label Learning (MLL). The proposal contains three key modules: \emph{model labeling}, which assigns labels to each VLM to describe their specialty and utility; \emph{model selection}, which matches the requirements of the target task with model labels; and \emph{model reuse}, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework</title>
<link>https://arxiv.org/abs/2502.00846</link>
<guid>https://arxiv.org/abs/2502.00846</guid>
<content:encoded><![CDATA[
arXiv:2502.00846v2 Announce Type: replace 
Abstract: We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SYN-LUNGS: Towards Simulating Lung Nodules with Anatomy-Informed Digital Twins for AI Training</title>
<link>https://arxiv.org/abs/2502.21187</link>
<guid>https://arxiv.org/abs/2502.21187</guid>
<content:encoded><![CDATA[
arXiv:2502.21187v3 Announce Type: replace 
Abstract: AI models for lung cancer screening are limited by data scarcity, impacting generalizability and clinical applicability. Generative models address this issue but are constrained by training data variability. We introduce SYN-LUNGS, a framework for generating high-quality 3D CT images with detailed annotations. SYN-LUNGS integrates XCAT3 phantoms for digital twin generation, X-Lesions for nodule simulation (varying size, location, and appearance), and DukeSim for CT image formation with vendor and parameter variability. The dataset includes 3,072 nodule images from 1,044 simulated CT scans, with 512 lesions and 174 digital twins. Models trained on clinical + simulated data outperform clinical only models, achieving 10% improvement in detection, 2-9% in segmentation and classification, and enhanced synthesis. By incorporating anatomy-informed simulations, SYN-LUNGS provides a scalable approach for AI model development, particularly in rare disease representation and improving model reliability.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters</title>
<link>https://arxiv.org/abs/2503.18216</link>
<guid>https://arxiv.org/abs/2503.18216</guid>
<content:encoded><![CDATA[
arXiv:2503.18216v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\sim$44% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in modern Transformer architectures.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty</title>
<link>https://arxiv.org/abs/2503.18314</link>
<guid>https://arxiv.org/abs/2503.18314</guid>
<content:encoded><![CDATA[
arXiv:2503.18314v3 Announce Type: replace 
Abstract: We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the influence of training samples from pre-trained models, avoiding retraining from scratch. LoTUS smooths the prediction probabilities of the model up to an information-theoretic bound, mitigating its over-confidence stemming from data memorization. We evaluate LoTUS on Transformer and ResNet18 models against eight baselines across five public datasets. Beyond established MU benchmarks, we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining is impractical, simulating real-world conditions. Moreover, we introduce the novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable evaluation under real-world conditions. The experimental results show that LoTUS outperforms state-of-the-art methods in terms of both efficiency and effectiveness. Code: https://github.com/cspartalis/LoTUS.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild</title>
<link>https://arxiv.org/abs/2503.18892</link>
<guid>https://arxiv.org/abs/2503.18892</guid>
<content:encoded><![CDATA[
arXiv:2503.18892v2 Announce Type: replace 
Abstract: DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training</title>
<link>https://arxiv.org/abs/2504.03783</link>
<guid>https://arxiv.org/abs/2504.03783</guid>
<content:encoded><![CDATA[
arXiv:2504.03783v3 Announce Type: replace 
Abstract: Federated Active Learning (FAL) has emerged as a promising framework to leverage large quantities of unlabeled data across distributed clients while preserving data privacy. However, real-world deployments remain limited by high annotation costs and communication-intensive sampling processes, particularly in a cross-silo setting, when clients possess substantial local datasets. This paper addresses the crucial question: What is the best practice to reduce communication costs in human-in-the-loop learning with minimal annotator effort? Existing FAL methods typically rely on iterative annotation processes that separate active sampling from federated updates, leading to multiple rounds of expensive communication and annotation. In response, we introduce FAST, a two-pass FAL framework that harnesses foundation models for weak labeling in a preliminary pass, followed by a refinement pass focused exclusively on the most uncertain samples. By leveraging representation knowledge from foundation models and integrating refinement steps into a streamlined workflow, FAST substantially reduces the overhead incurred by iterative active sampling. Extensive experiments on diverse medical and natural image benchmarks demonstrate that FAST outperforms existing FAL methods by an average of 4.36% while reducing communication rounds eightfold under a limited 5% labeling budget.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Metabolic Syndrome Prediction with Hybrid Data Balancing and Counterfactuals</title>
<link>https://arxiv.org/abs/2504.06987</link>
<guid>https://arxiv.org/abs/2504.06987</guid>
<content:encoded><![CDATA[
arXiv:2504.06987v2 Announce Type: replace 
Abstract: Metabolic Syndrome (MetS) is a cluster of interrelated risk factors that significantly increases the risk of cardiovascular diseases and type 2 diabetes. Despite its global prevalence, accurate prediction of MetS remains challenging due to issues such as class imbalance, data scarcity, and methodological inconsistencies in existing studies. In this paper, we address these challenges by systematically evaluating and optimizing machine learning (ML) models for MetS prediction, leveraging advanced data balancing techniques and counterfactual analysis. Multiple ML models, including XGBoost, Random Forest, TabNet, etc., were trained and compared under various data balancing techniques such as random oversampling (ROS), SMOTE, ADASYN, and CTGAN. Additionally, we introduce MetaBoost, a novel hybrid framework that integrates SMOTE, ADASYN, and CTGAN, optimizing synthetic data generation through weighted averaging and iterative weight tuning to enhance the model's performance (achieving up to a 1.87% accuracy improvement over individual balancing techniques). A comprehensive counterfactual analysis is conducted to quantify the feature-level changes required to shift individuals from high-risk to low-risk categories. The results indicate that blood glucose (50.3%) and triglycerides (46.7%) were the most frequently modified features, highlighting their clinical significance in MetS risk reduction. Additionally, probabilistic analysis shows elevated blood glucose (85.5% likelihood) and triglycerides (74.9% posterior probability) as the strongest predictors. This study not only advances the methodological rigor of MetS prediction but also provides actionable insights for clinicians and researchers, highlighting the potential of ML in mitigating the public health burden of metabolic syndrome.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Many-Shot Jailbreaking</title>
<link>https://arxiv.org/abs/2504.09604</link>
<guid>https://arxiv.org/abs/2504.09604</guid>
<content:encoded><![CDATA[
arXiv:2504.09604v2 Announce Type: replace 
Abstract: Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a "fake" assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the "fake" assistant. In this work, we probe the effectiveness of different fine-tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks</title>
<link>https://arxiv.org/abs/2504.12806</link>
<guid>https://arxiv.org/abs/2504.12806</guid>
<content:encoded><![CDATA[
arXiv:2504.12806v2 Announce Type: replace 
Abstract: The loss landscape of Variational Quantum Neural Networks (VQNNs) is characterized by local minima that grow exponentially with increasing qubits. Because of this, it is more challenging to recover information from model gradients during training compared to classical Neural Networks (NNs). In this paper we present a numerical scheme that successfully reconstructs input training, real-world, practical data from trainable VQNNs' gradients. Our scheme is based on gradient inversion that works by combining gradients estimation with the finite difference method and adaptive low-pass filtering. The scheme is further optimized with Kalman filter to obtain efficient convergence. Our experiments show that our algorithm can invert even batch-trained data, given the VQNN model is sufficiently over-parameterized.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tight Regret Bounds for Bayesian Optimization in One Dimension</title>
<link>https://arxiv.org/abs/1805.11792</link>
<guid>https://arxiv.org/abs/1805.11792</guid>
<content:encoded><![CDATA[
arXiv:1805.11792v3 Announce Type: replace-cross 
Abstract: We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time $T$ behaves as $\Omega(\sqrt{T})$ and $O(\sqrt{T\log T})$. This gives a tight characterization up to a $\sqrt{\log T}$ factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Mat\'ern-$\nu$ kernels, with the latter requiring $\nu > 2$. Our results certify the near-optimality of existing bounds (Srinivas {\em et al.}, 2009) for the SE kernel, while proving them to be strictly suboptimal for the Mat\'ern kernel with $\nu > 2$.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Welfare Analysis in Dynamic Models</title>
<link>https://arxiv.org/abs/1908.09173</link>
<guid>https://arxiv.org/abs/1908.09173</guid>
<content:encoded><![CDATA[
arXiv:1908.09173v4 Announce Type: replace-cross 
Abstract: This paper introduces metrics for welfare analysis in dynamic models. We develop estimation and inference for these parameters even in the presence of a high-dimensional state space. Examples of welfare metrics include average welfare, average marginal welfare effects, and welfare decompositions into direct and indirect effects similar to Oaxaca (1973) and Blinder (1973). We derive dual and doubly robust representations of welfare metrics that facilitate debiased inference. For average welfare, the value function does not have to be estimated. In general, debiasing can be applied to any estimator of the value function, including neural nets, random forests, Lasso, boosting, and other high-dimensional methods. In particular, we derive Lasso and Neural Network estimators of the value function and associated dynamic dual representation and establish associated mean square convergence rates for these functions. Debiasing is automatic in the sense that it only requires knowledge of the welfare metric of interest, not the form of bias correction. The proposed methods are applied to estimate a dynamic behavioral model of teacher absenteeism in \cite{DHR} and associated average teacher welfare.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence</title>
<link>https://arxiv.org/abs/2202.03482</link>
<guid>https://arxiv.org/abs/2202.03482</guid>
<content:encoded><![CDATA[
arXiv:2202.03482v3 Announce Type: replace-cross 
Abstract: With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept direction and their impact on CAV applications, including concept sensitivity testing and model correction for shortcut behavior caused by data artifacts. We demonstrate the benefits of pattern-based CAVs using the Pediatric Bone Age, ISIC2019, and FunnyBirds datasets with VGG, ResNet, ReXNet, EfficientNet, and Vision Transformer as model architectures.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto.gov: Learning-based Governance for Decentralized Finance (DeFi)</title>
<link>https://arxiv.org/abs/2302.09551</link>
<guid>https://arxiv.org/abs/2302.09551</guid>
<content:encoded><![CDATA[
arXiv:2302.09551v4 Announce Type: replace-cross 
Abstract: Decentralized finance (DeFi) is an integral component of the blockchain ecosystem, enabling a range of financial activities through smart-contract-based protocols. Traditional DeFi governance typically involves manual parameter adjustments by protocol teams or token holder votes, and is thus prone to human bias and financial risks, undermining the system's integrity and security. While existing efforts aim to establish more adaptive parameter adjustment schemes, there remains a need for a governance model that is both more efficient and resilient to significant market manipulations. In this paper, we introduce "Auto$.$gov", a learning-based governance framework that employs a deep Qnetwork (DQN) reinforcement learning (RL) strategy to perform semi-automated, data-driven parameter adjustments. We create a DeFi environment with an encoded action-state space akin to the Aave lending protocol for simulation and testing purposes, where Auto$.$gov has demonstrated the capability to retain funds that would have otherwise been lost to price oracle attacks. In tests with real-world data, Auto$.$gov outperforms the benchmark approaches by at least 14% and the static baseline model by tenfold, in terms of the preset performance metric--protocol profitability. Overall, the comprehensive evaluations confirm that Auto$.$gov is more efficient and effective than traditional governance methods, thereby enhancing the security, profitability, and ultimately, the sustainability of DeFi protocols.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-IDS: Doubly Disentangled Dynamic Intrusion Detection</title>
<link>https://arxiv.org/abs/2307.11079</link>
<guid>https://arxiv.org/abs/2307.11079</guid>
<content:encoded><![CDATA[
arXiv:2307.11079v3 Announce Type: replace-cross 
Abstract: Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models</title>
<link>https://arxiv.org/abs/2308.04729</link>
<guid>https://arxiv.org/abs/2308.04729</guid>
<content:encoded><![CDATA[
arXiv:2308.04729v2 Announce Type: replace-cross 
Abstract: Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at https://jenmusic.ai/audio-demos
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Audio Embeddings-- Bringing Features Back Outperforms CLAP !</title>
<link>https://arxiv.org/abs/2309.08751</link>
<guid>https://arxiv.org/abs/2309.08751</guid>
<content:encoded><![CDATA[
arXiv:2309.08751v3 Announce Type: replace-cross 
Abstract: With the advent of modern AI architectures, a shift has happened towards end-to-end architectures. This pivot has led to neural architectures being trained without domain-specific biases/knowledge, optimized according to the task. We in this paper, learn audio embeddings via diverse feature representations, in this case, domain-specific. For the case of audio classification over hundreds of categories of sound, we learn robust separate embeddings for diverse audio properties such as pitch, timbre, and neural representation, along with also learning it via an end-to-end architecture. We observe handcrafted embeddings, e.g., pitch and timbre-based, although on their own, are not able to beat a fully end-to-end representation, yet adding these together with end-to-end embedding helps us, significantly improve performance. This work would pave the way to bring some domain expertise with end-to-end models to learn robust, diverse representations, surpassing the performance of just training end-to-end models.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Nighttime Visible Satellite Imagery of Tropical Cyclones Using Conditional Generative Adversarial Networks</title>
<link>https://arxiv.org/abs/2401.11679</link>
<guid>https://arxiv.org/abs/2401.11679</guid>
<content:encoded><![CDATA[
arXiv:2401.11679v4 Announce Type: replace-cross 
Abstract: Visible (VIS) imagery is important for monitoring Tropical Cyclones (TCs) but is unavailable at night. This study presents a Conditional Generative Adversarial Networks (CGAN) model to generate nighttime VIS imagery with significantly enhanced accuracy and spatial resolution. Our method offers three key improvements compared to existing models. First, we replaced the L1 loss in the pix2pix framework with the Structural Similarity Index Measure (SSIM) loss, which significantly reduced image blurriness. Second, we selected multispectral infrared (IR) bands as input based on a thorough examination of their spectral properties, providing essential physical information for accurate simulation. Third, we incorporated the direction parameters of the sun and the satellite, which addressed the dependence of VIS images on sunlight directions and enabled a much larger training set from continuous daytime data. The model was trained and validated using data from the Advanced Himawari Imager (AHI) in the daytime, achieving statistical results of SSIM = 0.923 and Root Mean Square Error (RMSE) = 0.0299, which significantly surpasses existing models. We also performed a cross-satellite nighttime model validation using the Day/Night Band (DNB) of the Visible/Infrared Imager Radiometer Suite (VIIRS), which yields outstanding results compared to existing models. Our model is operationally applied to generate accurate VIS imagery with arbitrary virtual sunlight directions, significantly contributing to the nighttime monitoring of various meteorological phenomena.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opening Articulated Structures in the Real World</title>
<link>https://arxiv.org/abs/2402.17767</link>
<guid>https://arxiv.org/abs/2402.17767</guid>
<content:encoded><![CDATA[
arXiv:2402.17767v3 Announce Type: replace-cross 
Abstract: What does it take to build mobile manipulation systems that can competently operate on previously unseen objects in previously unseen environments? This work answers this question using opening of articulated structures as a mobile manipulation testbed. Specifically, our focus is on the end-to-end performance on this task without any privileged information, i.e. the robot starts at a location with the novel target articulated object in view, and has to approach the object and successfully open it. We first develop a system for this task, and then conduct 100+ end-to-end system tests across 13 real world test sites. Our large-scale study reveals a number of surprising findings: a) modular systems outperform end-to-end learned systems for this task, even when the end-to-end learned systems are trained on 1000+ demonstrations, b) perception, and not precise end-effector control, is the primary bottleneck to task success, and c) state-of-the-art articulation parameter estimation models developed in isolation struggle when faced with robot-centric viewpoints. Overall, our findings highlight the limitations of developing components of the pipeline in isolation and underscore the need for system-level research, providing a pragmatic roadmap for building generalizable mobile manipulation systems. Videos, code, and models are available on the project website: https://arjung128.github.io/opening-articulated-structures/
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing User Interest based on Stream Clustering and Memory Networks in Large-Scale Recommender Systems</title>
<link>https://arxiv.org/abs/2405.13238</link>
<guid>https://arxiv.org/abs/2405.13238</guid>
<content:encoded><![CDATA[
arXiv:2405.13238v5 Announce Type: replace-cross 
Abstract: Recommender Systems (RSs) provide personalized recommendation service based on user interest, which are widely used in various platforms. However, there are lots of users with sparse interest due to lacking consumption behaviors, which leads to poor recommendation results for them. This problem is widespread in large-scale RSs and is particularly difficult to address. To solve this challenging problem, we propose an innovative solution called User Interest Enhancement (UIE). UIE enhances user interest including user profile and user history behavior sequences by leveraging the enhancement vectors and personalized enhancement vectors generated based on dynamic streaming clustering of similar users and items from multiple perspectives, which are stored and updated in memory networks. UIE not only remarkably improves model performance for users with sparse interest, but also delivers notable gains for other users. As an end-to-end solution, UIE is easy to implement on top of existing ranking models. Furthermore, we extend our approach to long-tail items using similar methods, which also yields excellent improvements. We conduct extensive offline and online experiments in a large-scale industrial RS. The results demonstrate that our model substantially outperforms other existing approaches, especially for users with sparse interest. UIE has been deployed in several large-scale RSs at Tencent since 2022, which was made public on 21 May 2024. In addition, UIE-based methods have also been successfully applied in candidate generation, pre-ranking, and context-DNN stages. Multiple teams have developed solutions based on UIE, focusing primarily on updating clustering algorithms and attention mechanisms. As far as we know, UIE has been deployed by many companies. The thoughts of UIE, dynamic streaming clustering and similarity enhancement, have inspired subsequent relevant works.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Understanding Attention-Based In-Context Learning for Categorical Data</title>
<link>https://arxiv.org/abs/2405.17248</link>
<guid>https://arxiv.org/abs/2405.17248</guid>
<content:encoded><![CDATA[
arXiv:2405.17248v2 Announce Type: replace-cross 
Abstract: In-context learning based on attention models is examined for data with categorical outcomes, with inference in such models viewed from the perspective of functional gradient descent (GD). We develop a network composed of attention blocks, with each block employing a self-attention layer followed by a cross-attention layer, with associated skip connections. This model can exactly perform multi-step functional GD inference for in-context inference with categorical observations. We perform a theoretical analysis of this setup, generalizing many prior assumptions in this line of work, including the class of attention mechanisms for which it is appropriate. We demonstrate the framework empirically on synthetic data, image classification and language generation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA</title>
<link>https://arxiv.org/abs/2406.02044</link>
<guid>https://arxiv.org/abs/2406.02044</guid>
<content:encoded><![CDATA[
arXiv:2406.02044v3 Announce Type: replace-cross 
Abstract: The rapid adoption of Large Language Models (LLMs) has exposed critical security and ethical vulnerabilities, particularly their susceptibility to adversarial manipulations. This paper introduces QROA, a novel black-box jailbreak method designed to identify adversarial suffixes that can bypass LLM alignment safeguards when appended to a malicious instruction. Unlike existing suffix-based jailbreak approaches, QROA does not require access to the model's logit or any other internal information. It also eliminates reliance on human-crafted templates, operating solely through the standard query-response interface of LLMs. By framing the attack as an optimization bandit problem, QROA employs a surrogate model and token level optimization to efficiently explore suffix variations. Furthermore, we propose QROA-UNV, an extension that identifies universal adversarial suffixes for individual models, enabling one-query jailbreaks across a wide range of instructions. Testing on multiple models demonstrates Attack Success Rate (ASR) greater than 80\%. These findings highlight critical vulnerabilities, emphasize the need for advanced defenses, and contribute to the development of more robust safety evaluations for secure AI deployment. The code is made public on the following link: https://github.com/qroa/QROA
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Rates of Convergence for Entropy Regularization in Discounted Markov Decision Processes</title>
<link>https://arxiv.org/abs/2406.04163</link>
<guid>https://arxiv.org/abs/2406.04163</guid>
<content:encoded><![CDATA[
arXiv:2406.04163v3 Announce Type: replace-cross 
Abstract: We study the error introduced by entropy regularization in infinite-horizon, discrete, discounted Markov decision processes. We show that this error decreases exponentially in the inverse regularization strength both in a weighted KL-divergence and in value with a problem-specific exponent. This is in contrast to previously known estimates, of the order $O(\tau)$, where $\tau$ is the regularization strength. We provide a lower bound matching our upper bound up to a polynomial term, thereby characterizing the exponential convergence rate for entropy regularization. Our proof relies on the observation that the solutions of entropy-regularized Markov decision processes solve a gradient flow of the unregularized reward with respect to a Riemannian metric common in natural policy gradient methods. This correspondence allows us to identify the limit of this gradient flow as the generalized maximum entropy optimal policy, thereby characterizing the implicit bias of this gradient flow, which corresponds to a time-continuous version of the natural policy gradient method. We use our improved error estimates to show that for entropy-regularized natural policy gradient methods, the overall error decays exponentially in the square root of the number of iterations, improving over existing sublinear guarantees. Finally, we extend our analysis to settings beyond the entropy. In particular, we characterize the implicit bias regarding general convex potentials and their resulting generalized natural policy gradients.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling</title>
<link>https://arxiv.org/abs/2406.04321</link>
<guid>https://arxiv.org/abs/2406.04321</guid>
<content:encoded><![CDATA[
arXiv:2406.04321v3 Announce Type: replace-cross 
Abstract: In this work, we systematically study music generation conditioned solely on the video. First, we present a large-scale dataset comprising 360K video-music pairs, including various genres such as movie trailers, advertisements, and documentaries. Furthermore, we propose VidMuse, a simple framework for generating music aligned with video inputs. VidMuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video. By incorporating local and global visual cues, VidMuse enables the creation of musically coherent audio tracks that consistently match the video content through Long-Short-Term modeling. Through extensive experiments, VidMuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment. The code and datasets are available at https://vidmuse.github.io/.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection</title>
<link>https://arxiv.org/abs/2408.17432</link>
<guid>https://arxiv.org/abs/2408.17432</guid>
<content:encoded><![CDATA[
arXiv:2408.17432v2 Announce Type: replace-cross 
Abstract: Synthesizing the voices of unseen speakers remains a persisting challenge in multi-speaker text-to-speech (TTS). Existing methods model speaker characteristics through speaker conditioning during training, leading to increased model complexity and limiting reproducibility and accessibility. A lower-complexity method would enable speech synthesis research with limited computational and data resources to reach to a wider use. To this end, we propose SelectTTS, a simple and effective alternative. SelectTTS selects appropriate frames from the target speaker and decodes them using frame-level self-supervised learning (SSL) features. We demonstrate that this approach can effectively capture speaker characteristics for unseen speakers and achieves performance comparable to state-of-the-art multi-speaker TTS frameworks on both objective and subjective metrics. By directly selecting frames from the target speaker's speech, SelectTTS enables generalization to unseen speakers with significantly lower model complexity. Compared to baselines such as XTTS-v2 and VALL-E, SelectTTS achieves better speaker similarity while reducing model parameters by over 8x and training data requirements by 270x.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian computation with generative diffusion models by Multilevel Monte Carlo</title>
<link>https://arxiv.org/abs/2409.15511</link>
<guid>https://arxiv.org/abs/2409.15511</guid>
<content:encoded><![CDATA[
arXiv:2409.15511v3 Announce Type: replace-cross 
Abstract: Generative diffusion models have recently emerged as a powerful strategy to perform stochastic sampling in Bayesian inverse problems, delivering remarkably accurate solutions for a wide range of challenging applications. However, diffusion models often require a large number of neural function evaluations per sample in order to deliver accurate posterior samples. As a result, using diffusion models as stochastic samplers for Monte Carlo integration in Bayesian computation can be highly computationally expensive, particularly in applications that require a substantial number of Monte Carlo samples for conducting uncertainty quantification analyses. This cost is especially high in large-scale inverse problems such as computational imaging, which rely on large neural networks that are expensive to evaluate. With quantitative imaging applications in mind, this paper presents a Multilevel Monte Carlo strategy that significantly reduces the cost of Bayesian computation with diffusion models. This is achieved by exploiting cost-accuracy trade-offs inherent to diffusion models to carefully couple models of different levels of accuracy in a manner that significantly reduces the overall cost of the calculation, without reducing the final accuracy. The proposed approach achieves a $4\times$-to-$8\times$ reduction in computational cost w.r.t. standard techniques across three benchmark imaging problems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Create Cross-Modal Task Representations</title>
<link>https://arxiv.org/abs/2410.22330</link>
<guid>https://arxiv.org/abs/2410.22330</guid>
<content:encoded><![CDATA[
arXiv:2410.22330v2 Announce Type: replace-cross 
Abstract: Autoregressive vision-language models (VLMs) can handle many tasks within a single model, yet the representations that enable this capability remain opaque. We find that VLMs align conceptually equivalent inputs into a shared task vector, which is invariant to modality (text, image) and format (examples, instruction), and may simplify VLM processing. We measure this alignment via cross-modal transfer -- the ability of a task vector derived in one modality to trigger the correct generation in another -- on a range of tasks and model architectures. Although the task vector is highly compressed, we find that this single vector outperforms prompting the model with the full task information, unique to this cross-modal case. Furthermore, we show that task vectors can be transferred from a base language model to its fine-tuned vision-language counterpart, and that they can be derived solely from instructions without the need for examples. Taken together, our findings shed light on how VLMs internally process task information, and how they map different modalities into common semantic representations. Project page: https://vlm-cross-modal-reps.github.io.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASURA-FDPS-ML: Star-by-star Galaxy Simulations Accelerated by Surrogate Modeling for Supernova Feedback</title>
<link>https://arxiv.org/abs/2410.23346</link>
<guid>https://arxiv.org/abs/2410.23346</guid>
<content:encoded><![CDATA[
arXiv:2410.23346v2 Announce Type: replace-cross 
Abstract: We introduce new high-resolution galaxy simulations accelerated by a surrogate model that reduces the computation cost by approximately 75 percent. Massive stars with a Zero Age Main Sequence mass of more than about 10 $\mathrm{M_\odot}$ explode as core-collapse supernovae (CCSNe), which play a critical role in galaxy formation. The energy released by CCSNe is essential for regulating star formation and driving feedback processes in the interstellar medium (ISM). However, the short integration timesteps required for SNe feedback have presented significant bottlenecks in astrophysical simulations across various scales. Overcoming this challenge is crucial for enabling star-by-star galaxy simulations, which aim to capture the dynamics of individual stars and the inhomogeneous shell's expansion within the turbulent ISM. To address this, our new framework combines direct numerical simulations and surrogate modeling, including machine learning and Gibbs sampling. The star formation history and the time evolution of outflow rates in the galaxy match those obtained from resolved direct numerical simulations. Our new approach achieves high-resolution fidelity while reducing computational costs, effectively bridging the physical scale gap and enabling multi-scale simulations.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation</title>
<link>https://arxiv.org/abs/2411.05261</link>
<guid>https://arxiv.org/abs/2411.05261</guid>
<content:encoded><![CDATA[
arXiv:2411.05261v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term "cyclic manipulation". This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Efficient Prediction of excited-state properties using Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2412.09423</link>
<guid>https://arxiv.org/abs/2412.09423</guid>
<content:encoded><![CDATA[
arXiv:2412.09423v2 Announce Type: replace-cross 
Abstract: Understanding the properties of excited states of complex molecules is crucial for many chemical and physical processes. Calculating these properties is often significantly more resource-intensive than calculating their ground state counterparts. We present a quantum machine learning model that predicts excited-state properties from the molecular ground state for different geometric configurations. The model comprises a symmetry-invariant quantum neural network and a conventional neural network and is able to provide accurate predictions with only a few training data points. The proposed procedure is fully NISQ compatible. This is achieved by using a quantum circuit that requires a number of parameters linearly proportional to the number of molecular orbitals, along with a parameterized measurement observable, thereby reducing the number of necessary measurements. We benchmark the algorithm on three different molecules with three different system sizes: $H_2$ with four orbitals, LiH with five orbitals, and $H_4$ with six orbitals. For these molecules, we predict the excited state transition energies and transition dipole moments. We show that, in many cases, the procedure is able to outperform various classical models (support vector machines, Gaussian processes, and neural networks) that rely solely on classical features, by up to two orders of magnitude in the test mean squared error.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved subsample-and-aggregate via the private modified winsorized mean</title>
<link>https://arxiv.org/abs/2501.14095</link>
<guid>https://arxiv.org/abs/2501.14095</guid>
<content:encoded><![CDATA[
arXiv:2501.14095v2 Announce Type: replace-cross 
Abstract: We develop a univariate, differentially private mean estimator, called the private modified winsorized mean, designed to be used as the aggregator in subsample-and-aggregate. We demonstrate, via real data analysis, that common differentially private multivariate mean estimators may not perform well as the aggregator, even in large datasets, motivating our developments.We show that the modified winsorized mean is minimax optimal for several, large classes of distributions, even under adversarial contamination. We also demonstrate that, empirically, the private modified winsorized mean performs well compared to other private mean estimates. We consider the modified winsorized mean as the aggregator in subsample-and-aggregate, deriving a finite sample deviations bound for a subsample-and-aggregate estimate generated with the new aggregator. This result yields two important insights: (i) the optimal choice of subsamples depends on the bias of the estimator computed on the subsamples, and (ii) the rate of convergence of the subsample-and-aggregate estimator depends on the robustness of the estimator computed on the subsamples.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergence of Abstract Rules in Recurrent Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2501.14539</link>
<guid>https://arxiv.org/abs/2501.14539</guid>
<content:encoded><![CDATA[
arXiv:2501.14539v2 Announce Type: replace-cross 
Abstract: The emergence of abstract rules from exemplars is a cornerstone of genuine intelligence in both biological and artificial systems. However, the internal organizational principles underlying different abstract rules remain poorly understood. We propose a hierarchically modulated recurrent spiking neural network (HM-RSNN), inspired by astrocyte signaling. The model globally configures and locally fine-tunes intrinsic neuronal properties via a two-stage neuromodulatory mechanism. This design enhances neuronal adaptability and diversity, thus enabling fine-grained analysis of internal organizational principles. We evaluate abstract rule emergence across four cognitive task sets. To probe internal organization, we examine network-level connectivity via structural modularity analysis and neuron-level functional biases via bin-wise lesion studies based on intrinsic properties. Our experiments show that HM-RSNN successfully achieves abstract rule emergence, with rule-contingent organizational principles evident at both network and neuron levels. These findings highlight the critical role of dynamic internal organization in supporting flexible cognition.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proxy Prompt: Endowing SAM and SAM 2 with Auto-Interactive-Prompt for Medical Segmentation</title>
<link>https://arxiv.org/abs/2502.03501</link>
<guid>https://arxiv.org/abs/2502.03501</guid>
<content:encoded><![CDATA[
arXiv:2502.03501v2 Announce Type: replace-cross 
Abstract: In this paper, we aim to address the unmet demand for automated prompting and enhanced human-model interactions of SAM and SAM2 for the sake of promoting their widespread clinical adoption. Specifically, we propose Proxy Prompt (PP), auto-generated by leveraging non-target data with a pre-annotated mask. We devise a novel 3-step context-selection strategy for adaptively selecting the most representative contextual information from non-target data via vision mamba and selective maps, empowering the guiding capability of non-target image-mask pairs for segmentation on target image/video data. To reinforce human-model interactions in PP, we further propose a contextual colorization module via a dual-reverse cross-attention to enhance interactions between target features and contextual-embedding with amplifying distinctive features of user-defined object(s). Via extensive evaluations, our method achieves state-of-the-art performance on four public datasets and yields comparable results with fully-trained models, even when trained with only 16 image masks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers</title>
<link>https://arxiv.org/abs/2502.03885</link>
<guid>https://arxiv.org/abs/2502.03885</guid>
<content:encoded><![CDATA[
arXiv:2502.03885v3 Announce Type: replace-cross 
Abstract: Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism (TP) and Expert Parallelism (EP). However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such as TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches, but the fault explosion radius remains large at the cube level (e.g., 64 TPUs).
  We propose InfiniteHBD, a novel transceiver-centric HBD architecture that unifies connectivity and dynamic switching at the transceiver level using Optical Circuit Switching (OCS). By embedding OCS within each transceiver, InfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing the topology to adapt into variable-size rings. This design provides: i) datacenter-wide scalability without cost explosion; ii) fault resilience by isolating failures to a single node, and iii) full bandwidth utilization for fault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based low-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology co-designed with intra-/inter-node communication, and an HBD-DCN orchestration algorithm maximizing GPU utilization while minimizing cross-ToR datacenter network traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of the cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude lower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault ratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to NVIDIA DGX (8 GPUs per Node).
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer Gate</title>
<link>https://arxiv.org/abs/2502.12224</link>
<guid>https://arxiv.org/abs/2502.12224</guid>
<content:encoded><![CDATA[
arXiv:2502.12224v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Liger: Linearizing Large Language Models to Gated Recurrent Structures</title>
<link>https://arxiv.org/abs/2503.01496</link>
<guid>https://arxiv.org/abs/2503.01496</guid>
<content:encoded><![CDATA[
arXiv:2503.01496v2 Announce Type: replace-cross 
Abstract: Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\% of the Transformer-based LLM at 0.02\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers</title>
<link>https://arxiv.org/abs/2504.20752</link>
<guid>https://arxiv.org/abs/2504.20752</guid>
<content:encoded><![CDATA[
arXiv:2504.20752v2 Announce Type: replace-cross 
Abstract: Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACE: A Security Architecture for LLM-Integrated App Systems</title>
<link>https://arxiv.org/abs/2504.20984</link>
<guid>https://arxiv.org/abs/2504.20984</guid>
<content:encoded><![CDATA[
arXiv:2504.20984v2 Announce Type: replace-cross 
Abstract: LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.
  In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solution designed to mitigate attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new secure architecture for LLM-integrated app systems that provides security guarantees for system planning and execution. Specifically, ACE decouples planning into two phases by first creating an abstract execution plan using only trusted information, and then mapping the abstract plan to a concrete plan using installed system apps. We verify that the plans generated by our system satisfy user-specified secure information flow constraints via static analysis on the structured plan output. During execution, ACE enforces data and capability barriers between apps, and ensures that the execution is conducted according to the trusted abstract plan. We show experimentally that our system is secure against attacks from the INJECAGENT benchmark, a standard benchmark for control flow integrity in the face of indirect prompt injection attacks, and our newly introduced attacks. Our architecture represents a significant advancement towards hardening LLM-based systems containing system facilities of varying levels of trustworthiness.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Machine Learning in Healthcare: A Survey</title>
<link>https://arxiv.org/abs/2505.02874</link>
<guid>https://arxiv.org/abs/2505.02874</guid>
<content:encoded><![CDATA[
<div> Quantification, Uncertainty, Machine Learning, Healthcare, Survey  
Summary:  
Uncertainty Quantification (UQ) is crucial for improving Machine Learning (ML) systems in healthcare by enhancing robustness, reliability, and interpretability. The lack of principled uncertainty quantification in ML models is a major challenge in healthcare applications. This survey provides a comprehensive analysis of current UQ methods in healthcare, offering a framework for integrating these methods into different stages of the ML pipeline. Popular methods in healthcare and potential novel approaches from other domains are highlighted. The study aims to guide researchers and practitioners in selecting suitable UQ techniques to enhance the reliability, safety, and trust in ML-driven healthcare solutions.<br /><br />Summary: <div>
arXiv:2505.02874v1 Announce Type: new 
Abstract: Uncertainty Quantification (UQ) is pivotal in enhancing the robustness, reliability, and interpretability of Machine Learning (ML) systems for healthcare, optimizing resources and improving patient care. Despite the emergence of ML-based clinical decision support tools, the lack of principled quantification of uncertainty in ML models remains a major challenge. Current reviews have a narrow focus on analyzing the state-of-the-art UQ in specific healthcare domains without systematically evaluating method efficacy across different stages of model development, and despite a growing body of research, its implementation in healthcare applications remains limited. Therefore, in this survey, we provide a comprehensive analysis of current UQ in healthcare, offering an informed framework that highlights how different methods can be integrated into each stage of the ML pipeline including data processing, training and evaluation. We also highlight the most popular methods used in healthcare and novel approaches from other domains that hold potential for future adoption in the medical context. We expect this study will provide a clear overview of the challenges and opportunities of implementing UQ in the ML pipeline for healthcare, guiding researchers and practitioners in selecting suitable techniques to enhance the reliability, safety and trust from patients and clinicians on ML-driven healthcare solutions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition</title>
<link>https://arxiv.org/abs/2505.02877</link>
<guid>https://arxiv.org/abs/2505.02877</guid>
<content:encoded><![CDATA[
<div> Keywords: plant disease recognition, deep learning, collaborative inference, edge computing, reinforcement learning

Summary:
Plant disease recognition is crucial for agricultural production. Traditional methods are inaccurate and inefficient, prompting the use of deep learning techniques. However, running these models on edge devices presents challenges due to inference delays and high energy consumption. To address this, a collaborative inference framework between edge devices and cloud servers is proposed. A pruned DNN model using reinforcement learning improves inference speed and reduces energy consumption. The optimal split point is determined through a greedy strategy for efficient collaboration. The system is implemented using Gradio for user-friendly interaction. Experimental results show that the framework significantly increases inference speed while maintaining recognition accuracy, providing a novel solution for timely plant disease diagnosis and prevention.<br /><br />Summary: <div>
arXiv:2505.02877v1 Announce Type: new 
Abstract: Plant disease is a critical factor affecting agricultural production. Traditional manual recognition methods face significant drawbacks, including low accuracy, high costs, and inefficiency. Deep learning techniques have demonstrated significant benefits in identifying plant diseases, but they still face challenges such as inference delays and high energy consumption. Deep learning algorithms are difficult to run on resource-limited embedded devices. Offloading these models to cloud servers is confronted with the restriction of communication bandwidth, and all of these factors will influence the inference's efficiency. We propose a collaborative inference framework for recognizing plant diseases between edge devices and cloud servers to enhance inference speed. The DNN model for plant disease recognition is pruned through deep reinforcement learning to improve the inference speed and reduce energy consumption. Then the optimal split point is determined by a greedy strategy to achieve the best collaborated inference acceleration. Finally, the system for collaborative inference acceleration in plant disease recognition has been implemented using Gradio to facilitate friendly human-machine interaction. Experiments indicate that the proposed collaborative inference framework significantly increases inference speed while maintaining acceptable recognition accuracy, offering a novel solution for rapidly diagnosing and preventing plant diseases.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction</title>
<link>https://arxiv.org/abs/2505.02880</link>
<guid>https://arxiv.org/abs/2505.02880</guid>
<content:encoded><![CDATA[
<div> financial time series, machine learning models, large language models, market data, stock return prediction <br />
Summary:<br />
- Traditional machine learning models struggle in predicting financial time series due to low signal-to-noise ratios and complex patterns.
- Large language models (LLMs) show promise in overcoming these challenges with their expanded parameter spaces.
- The proposed $LLM4FTS$ framework enhances LLM capabilities by incorporating learnable patch segmentation and dynamic wavelet convolution modules.
- K-means++ clustering and adaptive patch segmentation identify scale-invariant patterns in market data, while the dynamic wavelet convolution module captures time-varying frequency characteristics.
- Extensive experiments on real-world financial datasets demonstrate the framework's effectiveness in capturing complex market patterns and achieving state-of-the-art results in stock return prediction, making it applicable in practical trading systems. <br /> <div>
arXiv:2505.02880v1 Announce Type: new 
Abstract: Predicting financial time series presents significant challenges due to inherent low signal-to-noise ratios and intricate temporal patterns. Traditional machine learning models exhibit limitations in this forecasting task constrained by their restricted model capacity. Recent advances in large language models (LLMs), with their greatly expanded parameter spaces, demonstrate promising potential for modeling complex dependencies in temporal sequences. However, existing LLM-based approaches typically focus on fixed-length patch analysis due to the Transformer architecture, ignoring market data's multi-scale pattern characteristics. In this study, we propose $LLM4FTS$, a novel framework that enhances LLM capabilities for temporal sequence modeling through learnable patch segmentation and dynamic wavelet convolution modules. Specifically,we first employ K-means++ clustering based on DTW distance to identify scale-invariant patterns in market data. Building upon pattern recognition results, we introduce adaptive patch segmentation that partitions temporal sequences while preserving maximal pattern integrity. To accommodate time-varying frequency characteristics, we devise a dynamic wavelet convolution module that emulates discrete wavelet transformation with enhanced flexibility in capturing time-frequency features. These three modules work together to improve large language model's ability to handle scale-invariant patterns in financial time series. Extensive experiments on real-world financial datasets substantiate the framework's efficacy, demonstrating superior performance in capturing complex market patterns and achieving state-of-the-art results in stock return prediction. The successful deployment in practical trading systems confirms its real-world applicability, representing a significant advancement in LLM applications for financial forecasting.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rewriting Pre-Training Data Boosts LLM Performance in Math and Code</title>
<link>https://arxiv.org/abs/2505.02881</link>
<guid>https://arxiv.org/abs/2505.02881</guid>
<content:encoded><![CDATA[
<div> Dataset, Large Language Models, Program Synthesis, Mathematical Reasoning, Llama 3.3 Community License <br />
<br />Summary: Large language models face limitations in program synthesis and mathematical reasoning due to the quality of their pre-training data. To address this, two openly licensed datasets, SwallowCode and SwallowMath, are introduced. SwallowCode enhances Python snippets by enforcing style conformity and improving algorithmic efficiency. SwallowMath improves mathematical solutions by providing concise step-by-step explanations. Continual pre-training with SwallowCode and SwallowMath significantly boosts performance on evaluation metrics, surpassing baseline models. Ablation studies show that each stage of the data refinement process contributes incrementally to the overall improvements. The publicly available datasets, prompts, and checkpoints facilitate reproducible research and advance large language model pre-training for specialized domains. <br /> <div>
arXiv:2505.02881v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) in program synthesis and mathematical reasoning is fundamentally limited by the quality of their pre-training corpora. We introduce two openly licensed datasets, released under the Llama 3.3 Community License, that significantly enhance LLM performance by systematically rewriting public data. SwallowCode (approximately 16.1 billion tokens) refines Python snippets from The-Stack-v2 through a novel four-stage pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM rewriting process that enforces style conformity and transforms snippets into self-contained, algorithmically efficient examples. Unlike prior methods that rely on exclusionary filtering or limited transformations, our transform-and-retain approach upgrades low-quality code, maximizing data utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by removing boilerplate, restoring context, and reformatting solutions into concise, step-by-step explanations. Within a fixed 50 billion token training budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1 by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing the baseline model's code generation capabilities. Similarly, substituting SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies confirm that each pipeline stage contributes incrementally, with rewriting delivering the largest gains. All datasets, prompts, and checkpoints are publicly available, enabling reproducible research and advancing LLM pre-training for specialized domains.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?</title>
<link>https://arxiv.org/abs/2505.02884</link>
<guid>https://arxiv.org/abs/2505.02884</guid>
<content:encoded><![CDATA[
<div> unlearning, large language models, data privacy, regulatory compliance, ethical AI deployment, knowledge removal<br />
<br />
The paper discusses the importance of unlearning in large language models (LLMs) for ensuring data privacy, regulatory compliance, and ethical AI deployment. It distinguishes unlearning from obfuscation and introduces a probing-based evaluation framework to assess the effectiveness of existing unlearning methods. The proposed DF-MCQ method flattens the model predictive distribution over generated multiple-choice questions using KL-divergence to remove knowledge about target individuals and induce refusal behavior. Experimental results show that DF-MCQ achieves unlearning with a high refusal rate and uncertainty level, surpassing obfuscation on probing questions. The study highlights the significance of true knowledge removal in unlearning processes for enhancing model security and compliance with privacy regulations.<br /><br />Summary: <div>
arXiv:2505.02884v1 Announce Type: new 
Abstract: Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger</title>
<link>https://arxiv.org/abs/2505.02888</link>
<guid>https://arxiv.org/abs/2505.02888</guid>
<content:encoded><![CDATA[
<div> Recursive Self-Improvement, AI agent, information-integration, Godelian self-reference, AutoML <br />
Summary:
The article introduces the Noise-to-Meaning Recursive Self-Improvement (N2M-RSI) model, which demonstrates that when an AI agent utilizes its own outputs as inputs and surpasses a certain information-integration threshold, its internal complexity will grow indefinitely. This framework combines concepts from large language models, Godelian self-reference, and AutoML without being tied to a specific implementation. The model can also be extended to interactions between multiple agents, suggesting potential super-linear effects with communication. To prioritize safety, specific implementation details are omitted, with only a brief, model-agnostic prototype provided in the appendix. <div>
arXiv:2505.02888v1 Announce Type: new 
Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal formal model showing that once an AI agent feeds its own outputs back as inputs and crosses an explicit information-integration threshold, its internal complexity will grow without bound under our assumptions. The framework unifies earlier ideas on self-prompting large language models, G\"odelian self-reference, and AutoML, yet remains implementation-agnostic. The model furthermore scales naturally to interacting swarms of agents, hinting at super-linear effects once communication among instances is permitted. For safety reasons, we omit system-specific implementation details and release only a brief, model-agnostic toy prototype in Appendix C.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Prediction of Sepsis: Feature-Aligned Transfer Learning</title>
<link>https://arxiv.org/abs/2505.02889</link>
<guid>https://arxiv.org/abs/2505.02889</guid>
<content:encoded><![CDATA[
<div> machine learning, sepsis, early detection, Feature Aligned Transfer Learning (FATL), population bias 

Summary:<br /><br />Sepsis is a critical medical condition that requires early detection to prevent severe outcomes. Existing models for sepsis prediction lack consistency in the features they use, hindering their effectiveness across different settings. The Feature Aligned Transfer Learning (FATL) method aims to address this by identifying and focusing on the most important features common across studies. Additionally, FATL combats population bias by incorporating knowledge from models trained on diverse populations in a weighted manner. This approach enhances the generalizability and effectiveness of the system for early sepsis detection. By improving early detection, FATL has the potential to enhance patient outcomes, reduce healthcare costs, and support more equitable healthcare delivery, particularly in resource-limited hospital settings. <div>
arXiv:2505.02889v1 Announce Type: new 
Abstract: Sepsis is a life threatening medical condition that occurs when the body has an extreme response to infection, leading to widespread inflammation, organ failure, and potentially death. Because sepsis can worsen rapidly, early detection is critical to saving lives. However, current diagnostic methods often identify sepsis only after significant damage has already occurred. Our project aims to address this challenge by developing a machine learning based system to predict sepsis in its early stages, giving healthcare providers more time to intervene.
  A major problem with existing models is the wide variability in the patient information or features they use, such as heart rate, temperature, and lab results. This inconsistency makes models difficult to compare and limits their ability to work across different hospitals and settings. To solve this, we propose a method called Feature Aligned Transfer Learning (FATL), which identifies and focuses on the most important and commonly reported features across multiple studies, ensuring the model remains consistent and clinically relevant.
  Most existing models are trained on narrow patient groups, leading to population bias. FATL addresses this by combining knowledge from models trained on diverse populations, using a weighted approach that reflects each models contribution. This makes the system more generalizable and effective across different patient demographics and clinical environments. FATL offers a practical and scalable solution for early sepsis detection, particularly in hospitals with limited resources, and has the potential to improve patient outcomes, reduce healthcare costs, and support more equitable healthcare delivery.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2505.02922</link>
<guid>https://arxiv.org/abs/2505.02922</guid>
<content:encoded><![CDATA[
<div> cache, LLM, inference, attention, sparsity
Summary:<br />
- RetroInfer introduces a system that optimizes long-context large language model (LLM) inference by reimagining the key-value (KV) cache as a vector storage system. <br />
- The system utilizes a wave index, which is an Attention-aWare VEctor index, to efficiently retrieve important tokens while taking into account attention sparsity. <br />
- Techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering are employed to enhance token retrieval accuracy. <br />
- The wave buffer component coordinates KV cache placement and optimizes computation and data transfer between GPU and CPU to maintain high throughput during inference. <br />
- RetroInfer demonstrates substantial speedup, up to 4.5X compared to full attention within GPU memory constraints and up to 10.5X over sparse attention methods when extending the KV cache to CPU memory, all while preserving the accuracy level of full attention models. <br /> <div>
arXiv:2505.02922v1 Announce Type: new 
Abstract: The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Quadratic Prediction Markets</title>
<link>https://arxiv.org/abs/2505.02959</link>
<guid>https://arxiv.org/abs/2505.02959</guid>
<content:encoded><![CDATA[
<div> Keywords: prediction market, learning algorithm, Smooth Quadratic Prediction Market, steepest gradient descent, adaptive liquidity

Summary: 
The study examines the design and application of prediction markets by comparing the Duality-based Cost Function Market Maker (DCFMM) with the proposed Smooth Quadratic Prediction Market. The new market incentivizes agents to implement general steepest gradient descent, offering a better worst-case monetary loss for AD securities while maintaining key guarantees such as price existence and no arbitrage. Trading behavior is analyzed under realistic constraints of bounded budgets and buy-only securities. An approach for adaptive liquidity in the Smooth Quadratic AD Prediction Market is outlined, suggesting a separation of price update rule and fee structure to maintain guarantees. The research showcases potential future designs for prediction markets while preserving key features and ensuring incentive compatibility. 

<br /><br />Summary: <div>
arXiv:2505.02959v1 Announce Type: new 
Abstract: When agents trade in a Duality-based Cost Function prediction market, they collectively implement the learning algorithm Follow-The-Regularized-Leader. We ask whether other learning algorithms could be used to inspire the design of prediction markets. By decomposing and modifying the Duality-based Cost Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction market, called the Smooth Quadratic Prediction Market, the incentivizes agents to collectively implement general steepest gradient descent. Relative to the DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary loss for AD securities while preserving axiom guarantees such as the existence of instantaneous price, information incorporation, expressiveness, no arbitrage, and a form of incentive compatibility. To motivate the application of the Smooth Quadratic Prediction Market, we independently examine agents' trading behavior under two realistic constraints: bounded budgets and buy-only securities. Finally, we provide an introductory analysis of an approach to facilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market. Our results suggest future designs where the price update rule is separate from the fee structure, yet guarantees are preserved.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning</title>
<link>https://arxiv.org/abs/2505.02974</link>
<guid>https://arxiv.org/abs/2505.02974</guid>
<content:encoded><![CDATA[
<div> Machine learning, surrogate models, physics simulations, dataset standardization, PLAID framework <br />
<br />
Summary: 
Machine learning-based surrogate models are valuable for speeding up simulation-driven workflows. However, their adoption is hindered by the lack of diverse and standardized datasets for physics simulations. To address this issue, the PLAID (Physics-Learning AI Datamodel) framework is introduced. PLAID provides a standard for describing simulation data and a library for creating and manipulating datasets across various physics domains. Six datasets are released under the PLAID standard, covering structural mechanics and computational fluid dynamics. Baseline benchmarks using representative learning methods are provided. Benchmarking tools are accessible on Hugging Face, allowing community participation and contribution to evaluation efforts. The PLAID framework aims to promote the widespread adoption of machine learning-based surrogate models in physics-driven workflows. <div>
arXiv:2505.02974v1 Announce Type: new 
Abstract: Machine learning-based surrogate models have emerged as a powerful tool to accelerate simulation-driven scientific workflows. However, their widespread adoption is hindered by the lack of large-scale, diverse, and standardized datasets tailored to physics-based simulations. While existing initiatives provide valuable contributions, many are limited in scope-focusing on specific physics domains, relying on fragmented tooling, or adhering to overly simplistic datamodels that restrict generalization. To address these limitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and extensible framework for representing and sharing datasets of physics simulations. PLAID defines a unified standard for describing simulation data and is accompanied by a library for creating, reading, and manipulating complex datasets across a wide range of physical use cases (gitlab.com/drti/plaid). We release six carefully crafted datasets under the PLAID standard, covering structural mechanics and computational fluid dynamics, and provide baseline benchmarks using representative learning methods. Benchmarking tools are made available on Hugging Face, enabling direct participation by the community and contribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems</title>
<link>https://arxiv.org/abs/2505.02985</link>
<guid>https://arxiv.org/abs/2505.02985</guid>
<content:encoded><![CDATA[
<div> Fractional-order stochastic gradient descent, FOSGD, Two-Scale Effective Dimension, 2SED, long-memory effects<br />
<br />
Summary:
2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD) integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to adapt the fractional exponent based on data. This approach dynamically adjusts the exponent to improve convergence by tracking model sensitivity and effective dimensionality, avoiding oscillations and instability. The method retains the benefits of fractional memory for non-convex optimization problems without the slow or erratic behavior of traditional fractional SGD. Empirical evaluations using an autoregressive model show faster convergence and more robust parameter estimates compared to baseline methods under Gaussian and $\alpha$-stable noise scenarios. The results demonstrate the potential of dimension-aware fractional techniques for enhancing modeling and estimation tasks.<br /><br />Summary: <div>
arXiv:2505.02985v1 Announce Type: new 
Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional exponents to capture long-memory effects in optimization. However, its utility is often limited by the difficulty of tuning and stabilizing these exponents. We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to adapt the fractional exponent in a data-driven manner. By tracking model sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the exponent to mitigate oscillations and hasten convergence. Theoretically, for onoconvex optimization problems, this approach preserves the advantages of fractional memory without the sluggish or unstable behavior observed in na\"ive fractional SGD. Empirical evaluations in Gaussian and $\alpha$-stable noise scenarios using an autoregressive (AR) model highlight faster convergence and more robust parameter estimates compared to baseline methods, underscoring the potential of dimension-aware fractional techniques for advanced modeling and estimation tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radio: Rate-Distortion Optimization for Large Language Model Compression</title>
<link>https://arxiv.org/abs/2505.03031</link>
<guid>https://arxiv.org/abs/2505.03031</guid>
<content:encoded><![CDATA[
<div> compression, language models, quantization, rate-distortion theory, optimization

Summary:
The article introduces a new technique for compressing large language models (LLMs) to improve their deployment on resource-limited devices and reduce compute costs. The approach is based on rate-distortion theory and involves a quantization technique that allows for post-training compression of models with hundreds of billions of weight parameters. This technique offers users the flexibility to specify the desired model size or accuracy level for compression. By incorporating simple rate-distortion optimization, the proposed method is scalable and efficient for large-scale language models. <div>
arXiv:2505.03031v1 Announce Type: new 
Abstract: In recent years, the compression of large language models (LLMs) has emerged as a key problem in facilitating LLM deployment on resource-limited devices, reducing compute costs, and mitigating the environmental footprint due to large-scale AI infrastructure. Here, we establish the foundations of LLM quantization from a rate-distortion theory perspective and propose a quantization technique based on simple rate-distortion optimization. Our technique scales to models containing hundreds of billions of weight parameters and offers users the flexibility to compress models, post-training, to a model size or accuracy specified by the user.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields</title>
<link>https://arxiv.org/abs/2505.03042</link>
<guid>https://arxiv.org/abs/2505.03042</guid>
<content:encoded><![CDATA[
<div> hash grid, neural fields, signal-fitting capabilities, multi-resolution, domain manipulation

Summary:
The article introduces a new perspective on the working principle of the hash grid structure in the Instant-NGP architecture, a state-of-the-art neural fields design. By framing the hash grid as a means of domain manipulation, the authors aim to explain how this structure enhances the expressivity of the neural network by creating multiple linear segments. Empirical experiments on 1-dimensional signals support this perspective, highlighting the grid's role in learning target signals. The study also addresses the issue of hyperparameter tuning in Instant-NGP, emphasizing the lack of principled understanding in current approaches. While the analysis primarily focuses on 1-dimensional signals, the proposed concept of domain manipulation is suggested to be applicable to higher dimensions as well. <div>
arXiv:2505.03042v1 Announce Type: new 
Abstract: Instant-NGP has been the state-of-the-art architecture of neural fields in recent years. Its incredible signal-fitting capabilities are generally attributed to its multi-resolution hash grid structure and have been used and improved in numerous following works. However, it is unclear how and why such a hash grid structure improves the capabilities of a neural network by such great margins. A lack of principled understanding of the hash grid also implies that the large set of hyperparameters accompanying Instant-NGP could only be tuned empirically without much heuristics. To provide an intuitive explanation of the working principle of the hash grid, we propose a novel perspective, namely domain manipulation. This perspective provides a ground-up explanation of how the feature grid learns the target signal and increases the expressivity of the neural field by artificially creating multiples of pre-existing linear segments. We conducted numerous experiments on carefully constructed 1-dimensional signals to support our claims empirically and aid our illustrations. While our analysis mainly focuses on 1-dimensional signals, we show that the idea is generalizable to higher dimensions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery</title>
<link>https://arxiv.org/abs/2505.03049</link>
<guid>https://arxiv.org/abs/2505.03049</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Materials Science, Chemistry, Molecular property prediction, Research workflows

Summary:
Large Language Models (LLMs) are revolutionizing materials science and chemistry research by enhancing molecular property prediction, materials design, automation, and knowledge extraction. The latest advancements in LLMs allow for integration of structured and unstructured data, aiding hypothesis generation and research workflow optimization. Through the Large Language Model Hackathon, 34 projects showcased the diverse applications of LLMs across key research areas such as molecular property prediction, automation, scientific communication, and knowledge extraction. These projects highlighted the versatility of LLMs as predictive models and rapid prototyping platforms. By improving performance with reasoning capabilities, additional training data, and innovative techniques, LLMs are becoming more effective, especially in low-data and interdisciplinary research settings. As LLMs become more integrated into scientific workflows, ongoing exploration and research are needed to address challenges around reliability, interpretability, and reproducibility. <br /><br />Summary: <div>
arXiv:2505.03049v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more. Recent developments demonstrate that the latest class of models are able to integrate structured and unstructured data, assist in hypothesis generation, and streamline research workflows. To explore the frontier of LLM capabilities across the research lifecycle, we review applications of LLMs through 34 total projects developed during the second annual Large Language Model Hackathon for Applications in Materials Science and Chemistry, a global hybrid event. These projects spanned seven key research areas: (1) molecular and material property prediction, (2) molecular and material design, (3) automation and novel interfaces, (4) scientific communication and education, (5) research data management and automation, (6) hypothesis generation and evaluation, and (7) knowledge extraction and reasoning from the scientific literature. Collectively, these applications illustrate how LLMs serve as versatile predictive models, platforms for rapid prototyping of domain-specific tools, and much more. In particular, improvements in both open source and proprietary LLM performance through the addition of reasoning, additional training data, and new techniques have expanded effectiveness, particularly in low-data environments and interdisciplinary research. As LLMs continue to improve, their integration into scientific workflows presents both new opportunities and new challenges, requiring ongoing exploration, continued refinement, and further research to address reliability, interpretability, and reproducibility.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks in Multimodal Systems: A Practitioner's Survey</title>
<link>https://arxiv.org/abs/2505.03084</link>
<guid>https://arxiv.org/abs/2505.03084</guid>
<content:encoded><![CDATA[
<div> multimodal models, Artificial Intelligence, adversarial attacks, text, image, video, audio  
Summary:  
This paper discusses the importance of multimodal models in Artificial Intelligence and the vulnerabilities they inherit from multiple modalities. It emphasizes the need for a practitioner-focused view on adversarial attacks in the multimodal world to help Machine Learning Practitioners adopt and deploy open-source models effectively. The survey covers adversarial attacks targeting text, image, video, and audio modalities, providing a comprehensive view of the evolving threat landscape. This is the first comprehensive summary of adversarial attacks in the multimodal world, highlighting the importance of understanding and addressing these threats for the security and robustness of multimodal models.<br /><br /> <div>
arXiv:2505.03084v1 Announce Type: new 
Abstract: The introduction of multimodal models is a huge step forward in Artificial Intelligence. A single model is trained to understand multiple modalities: text, image, video, and audio. Open-source multimodal models have made these breakthroughs more accessible. However, considering the vast landscape of adversarial attacks across these modalities, these models also inherit vulnerabilities of all the modalities, and ultimately, the adversarial threat amplifies. While broad research is available on possible attacks within or across these modalities, a practitioner-focused view that outlines attack types remains absent in the multimodal world. As more Machine Learning Practitioners adopt, fine-tune, and deploy open-source models in real-world applications, it's crucial that they can view the threat landscape and take the preventive actions necessary. This paper addresses the gap by surveying adversarial attacks targeting all four modalities: text, image, video, and audio. This survey provides a view of the adversarial attack landscape and presents how multimodal adversarial threats have evolved. To the best of our knowledge, this survey is the first comprehensive summarization of the threat landscape in the multimodal world.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models</title>
<link>https://arxiv.org/abs/2505.03109</link>
<guid>https://arxiv.org/abs/2505.03109</guid>
<content:encoded><![CDATA[
<div> DL models, Renewable Energy, LSTM, MLP, Regularization<br />
Summary:<br />
This research explores the use of Deep Learning (DL) models in the renewable energy domain due to the unpredictability and complexity of renewable energy sources. DL models are favored over traditional machine learning models for their ability to capture complex interactions among variables. Factors affecting DL model accuracy such as sampling, stationarity, and hyperparameter optimization are examined, with LSTM and MLP models showing superior performance. The study evaluates seven ML methods on weather and power generation datasets from Spain, utilizing regularization techniques to address overfitting issues. The research highlights the importance of developing robust methods for renewable energy applications, with LSTM and MLP models emerging as effective solutions for predicting power output from renewable sources. The study provides valuable insights into optimizing DL techniques for improved accuracy in renewable energy forecasting. <br /> <div>
arXiv:2505.03109v1 Announce Type: new 
Abstract: Unpredictability of renewable energy sources coupled with the complexity of those methods used for various purposes in this area calls for the development of robust methods such as DL models within the renewable energy domain. Given the nonlinear relationships among variables in renewable energy datasets, DL models are preferred over traditional machine learning (ML) models because they can effectively capture and model complex interactions between variables. This research aims to identify the factors responsible for the accuracy of DL techniques, such as sampling, stationarity, linearity, and hyperparameter optimization for different algorithms. The proposed DL framework compares various methods and alternative training/test ratios. Seven ML methods, such as Long-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network (CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and Encoder-Decoder (ED), were evaluated on two different datasets. The first dataset contains the weather and power generation data. It encompasses two distinct datasets, hourly energy demand data and hourly weather data in Spain, while the second dataset includes power output generated by the photovoltaic panels at 12 locations. This study deploys regularization approaches, including early stopping, neuron dropping, and L2 regularization, to reduce the overfitting problem associated with DL models. The LSTM and MLP models show superior performance. Their validation data exhibit exceptionally low root mean square error values.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs</title>
<link>https://arxiv.org/abs/2505.03112</link>
<guid>https://arxiv.org/abs/2505.03112</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Modulation Classification, Large-Language Models, signal processing techniques, wireless communications, Signal-to-Noise Ratios <br />
<br />
Summary: 
Automatic Modulation Classification (AMC) is crucial for efficient spectrum management and robust wireless communications. Traditional signal processing techniques are integrated with Large-Language Models (LLMs) in a novel framework to address AMC. Higher-order statistics and cumulant estimation are used to convert signal features into natural language prompts for the LLMs. By including exemplar contexts in the prompts, the LLMs can effectively classify signals in a single shot without the need for additional training or preprocessing. Experimental evaluations on synthetic datasets show that the framework performs well under various modulation schemes and Signal-to-Noise Ratios (SNRs). This approach lays the groundwork for robust foundation models in wireless communication that can adapt to diverse channel conditions, reducing the cost associated with developing channel-specific models. The framework opens up possibilities for scalable, interpretable, and versatile signal classification systems in next-generation wireless networks. <div>
arXiv:2505.03112v1 Announce Type: new 
Abstract: Automatic Modulation Classification (AMC) is critical for efficient spectrum management and robust wireless communications. However, AMC remains challenging due to the complex interplay of signal interference and noise. In this work, we propose an innovative framework that integrates traditional signal processing techniques with Large-Language Models (LLMs) to address AMC. Our approach leverages higher-order statistics and cumulant estimation to convert quantitative signal features into structured natural language prompts. By incorporating exemplar contexts into these prompts, our method exploits the LLM's inherent familiarity with classical signal processing, enabling effective one-shot classification without additional training or preprocessing (e.g., denoising). Experimental evaluations on synthetically generated datasets, spanning both noiseless and noisy conditions, demonstrate that our framework achieves competitive performance across diverse modulation schemes and Signal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust foundation models in wireless communications across varying channel conditions, significantly reducing the expense associated with developing channel-specific models. This work lays the foundation for scalable, interpretable, and versatile signal classification systems in next-generation wireless networks. The source code is available at https://github.com/RU-SIT/context-is-king
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion</title>
<link>https://arxiv.org/abs/2505.03118</link>
<guid>https://arxiv.org/abs/2505.03118</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-label classification, adaptive thresholding, global rarity, noisy conditions, lightweight architecture

Summary: 
The study introduces a novel approach to multi-label classification (MLC) that addresses the challenges of heavy class imbalance and noise in data. By incorporating an adaptive thresholding mechanism that combines global and local signals, the proposed model produces per-label, per-instance thresholds for more accurate predictions. Instead of using hard cutoffs, these thresholds are treated as differentiable penalties in the loss function, ensuring smoother supervision and improved calibration. The architecture is lightweight, interpretable, and modular, making it accessible for various applications. On the AmazonCat-13K benchmark dataset, the model achieves a macro-F1 score of 0.1712, surpassing tree-based and pretrained transformer-based methods. The researchers have made their code available for reproducibility and future research. <br /><br />Summary: <div>
arXiv:2505.03118v1 Announce Type: new 
Abstract: Multi-label classification (MLC) requires predicting multiple labels per sample, often under heavy class imbalance and noisy conditions. Traditional approaches apply fixed thresholds or treat labels independently, overlooking context and global rarity. We introduce an adaptive thresholding mechanism that fuses global (IDF-based) and local (KNN-based) signals to produce per-label, per-instance thresholds. Instead of applying these as hard cutoffs, we treat them as differentiable penalties in the loss, providing smooth supervision and better calibration. Our architecture is lightweight, interpretable, and highly modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712, substantially outperforming tree-based and pretrained transformer-based methods. We release full code for reproducibility and future extensions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation</title>
<link>https://arxiv.org/abs/2505.03155</link>
<guid>https://arxiv.org/abs/2505.03155</guid>
<content:encoded><![CDATA[
<div> Policy gradient, Linear function approximation, Softmax, Convergence, Stochastic bandit<br />
Summary:<br />
Policy gradient methods, commonly used in reinforcement learning, often rely on function approximation to handle large state-action spaces. This study focuses on Softmax PG with linear function approximation (Lin-SPG) and investigates the impact of approximation error on global convergence. Surprisingly, the approximation error is found to be irrelevant for the algorithm's global convergence, even in the stochastic bandit setting. The study identifies necessary and sufficient conditions on the feature representation that guarantee asymptotic global convergence of Lin-SPG. It is proven that Lin-SPG, with problem-specific learning rates and under specific feature conditions, converges to the optimal policy at a rate of O(1/T) after T iterations. Additionally, it is shown that Lin-SPG with any constant learning rate can ensure asymptotic global convergence to the optimal policy. <div>
arXiv:2505.03155v1 Announce Type: new 
Abstract: Policy gradient (PG) methods have played an essential role in the empirical successes of reinforcement learning. In order to handle large state-action spaces, PG methods are typically used with function approximation. In this setting, the approximation error in modeling problem-dependent quantities is a key notion for characterizing the global convergence of PG methods. We focus on Softmax PG with linear function approximation (referred to as $\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant to the algorithm's global convergence even for the stochastic bandit setting. Consequently, we first identify the necessary and sufficient conditions on the feature representation that can guarantee the asymptotic global convergence of $\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$ iterations of $\texttt{Lin-SPG}$ with a problem-specific learning rate result in an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that $\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure asymptotic global convergence to the optimal policy.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis</title>
<link>https://arxiv.org/abs/2505.03165</link>
<guid>https://arxiv.org/abs/2505.03165</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, reproducibility, software development, methodology, transparency 

Summary: 
This paper addresses the challenges of reproducibility in deep learning models, highlighting the importance of reliability and validity in software development. The difficulty in reproducing results may stem from differences in execution environments, software libraries, proprietary data, lack of transparency, and stochastic nature. A systematic approach is presented, including guidelines such as replicating the original software environment, implementing end-to-end training algorithms, disclosing architectural designs, and enhancing transparency in data processing. The study also emphasizes the need for sensitivity analysis to understand model performance across diverse conditions. By following these strategies, the gap between research and practice can be bridged, enabling effective reproduction and deployment of innovations in deep learning within software development. 

Summary: <div>
arXiv:2505.03165v1 Announce Type: new 
Abstract: The field of deep learning has witnessed significant breakthroughs, spanning various applications, and fundamentally transforming current software capabilities. However, alongside these advancements, there have been increasing concerns about reproducing the results of these deep learning methods. This is significant because reproducibility is the foundation of reliability and validity in software development, particularly in the rapidly evolving domain of deep learning. The difficulty of reproducibility may arise due to several reasons, including having differences from the original execution environment, incompatible software libraries, proprietary data and source code, lack of transparency, and the stochastic nature in some software. A study conducted by the Nature journal reveals that more than 70% of researchers failed to reproduce other researchers experiments and over 50% failed to reproduce their own experiments. Irreproducibility of deep learning poses significant challenges for researchers and practitioners. To address these concerns, this paper presents a systematic approach at analyzing and improving the reproducibility of deep learning models by demonstrating these guidelines using a case study. We illustrate the patterns and anti-patterns involved with these guidelines for improving the reproducibility of deep learning models. These guidelines encompass establishing a methodology to replicate the original software environment, implementing end-to-end training and testing algorithms, disclosing architectural designs, and enhancing transparency in data processing and training pipelines. We also conduct a sensitivity analysis to understand the model performance across diverse conditions. By implementing these strategies, we aim to bridge the gap between research and practice, so that innovations in deep learning can be effectively reproduced and deployed within software.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.03172</link>
<guid>https://arxiv.org/abs/2505.03172</guid>
<content:encoded><![CDATA[
<div> relabelling, goal-conditioned reinforcement learning, object-centric, interactions, sample efficiency
Summary:<br />
- Hindsight relabeling is powerful in overcoming sparsity in goal-conditioned reinforcement learning (GCRL), but struggles in object-centric domains.
- In object-centric domains, interactions between objects are crucial for meaningful trajectories.
- Hindsight Relabeling using Interactions (HInt) combines interactions with hindsight relabeling to improve sample efficiency in RL.
- Null Counterfactual Interaction Inference (NCII) leverages a definition of interactions based on null counterfactuals to infer interactions.
- NCII achieves significantly improved interaction inference accuracy in various domains, leading to up to 4x improvement in sample efficiency. <br /><br />Summary: <div>
arXiv:2505.03172v1 Announce Type: new 
Abstract: Hindsight relabeling is a powerful tool for overcoming sparsity in goal-conditioned reinforcement learning (GCRL), especially in certain domains such as navigation and locomotion. However, hindsight relabeling can struggle in object-centric domains. For example, suppose that the goal space consists of a robotic arm pushing a particular target block to a goal location. In this case, hindsight relabeling will give high rewards to any trajectory that does not interact with the block. However, these behaviors are only useful when the object is already at the goal -- an extremely rare case in practice. A dataset dominated by these kinds of trajectories can complicate learning and lead to failures. In object-centric domains, one key intuition is that meaningful trajectories are often characterized by object-object interactions such as pushing the block with the gripper. To leverage this intuition, we introduce Hindsight Relabeling using Interactions (HInt), which combines interactions with hindsight relabeling to improve the sample efficiency of downstream RL. However because interactions do not have a consensus statistical definition tractable for downstream GCRL, we propose a definition of interactions based on the concept of null counterfactual: a cause object is interacting with a target object if, in a world where the cause object did not exist, the target object would have different transition dynamics. We leverage this definition to infer interactions in Null Counterfactual Interaction Inference (NCII), which uses a "nulling'' operation with a learned model to infer interactions. NCII is able to achieve significantly improved interaction inference accuracy in both simple linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion</title>
<link>https://arxiv.org/abs/2505.03178</link>
<guid>https://arxiv.org/abs/2505.03178</guid>
<content:encoded><![CDATA[
<div> simulation, autonomous vehicles, risk-adjustable, multi-agent, safety 

Summary:
The article introduces the Risk-Adjustable Driving Environment (RADE), a simulation framework for generating safety-critical scenarios in high-fidelity simulations for testing autonomous vehicles. RADE uses a multi-agent diffusion architecture to model the behavior of all agents in the environment and conditions their trajectories based on a risk measure. Unlike traditional methods, RADE learns risk-conditioned behaviors from data, maintaining naturalistic interactions with controllable risk levels. Additionally, a tokenized dynamics check module ensures physical plausibility by efficiently filtering generated trajectories. Validated on the real-world rounD dataset, RADE maintains statistical realism across varying risk levels and increases the likelihood of safety-critical events as risk levels increase. This research demonstrates RADE's potential as a scalable and realistic tool for evaluating autonomous vehicle safety. 

<br /><br />Summary: <div>
arXiv:2505.03178v1 Announce Type: new 
Abstract: Generating safety-critical scenarios in high-fidelity simulations offers a promising and cost-effective approach for efficient testing of autonomous vehicles. Existing methods typically rely on manipulating a single vehicle's trajectory through sophisticated designed objectives to induce adversarial interactions, often at the cost of realism and scalability. In this work, we propose the Risk-Adjustable Driving Environment (RADE), a simulation framework that generates statistically realistic and risk-adjustable traffic scenes. Built upon a multi-agent diffusion architecture, RADE jointly models the behavior of all agents in the environment and conditions their trajectories on a surrogate risk measure. Unlike traditional adversarial methods, RADE learns risk-conditioned behaviors directly from data, preserving naturalistic multi-agent interactions with controllable risk levels. To ensure physical plausibility, we incorporate a tokenized dynamics check module that efficiently filters generated trajectories using a motion vocabulary. We validate RADE on the real-world rounD dataset, demonstrating that it preserves statistical realism across varying risk levels and naturally increases the likelihood of safety-critical events as the desired risk level grows up. Our results highlight RADE's potential as a scalable and realistic tool for AV safety evaluation.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making</title>
<link>https://arxiv.org/abs/2505.03181</link>
<guid>https://arxiv.org/abs/2505.03181</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, vision-language models, supervised fine-tuning, reinforcement learning, multi-modal agent domains

Summary: 
This research introduces a method to improve the performance of vision-language models (VLMs) in interactive environments by leveraging reinforcement learning (RL) techniques. While VLMs hold the potential for tasks requiring visual reasoning, they currently lag behind large language models (LLMs) in certain aspects, such as conforming to output syntax requirements. By incorporating supervised fine-tuning (SFT) on task-specific expert demonstrations and adopting an offline-to-online RL approach, the proposed method aims to overcome these limitations. Through off-policy RL, the VLM-based agents can learn from their own mistakes or from more capable models, enhancing their performance in various multi-modal agent domains. The study demonstrates the effectiveness of the approach across three different interactive environments, showcasing the potential for self-improvement and adaptability in VLM-based agents.<br /><br />Summary: <div>
arXiv:2505.03181v1 Announce Type: new 
Abstract: Recent research looks to harness the general knowledge and reasoning of large language models (LLMs) into agents that accomplish user-specified goals in interactive environments. Vision-language models (VLMs) extend LLMs to multi-modal data and provide agents with the visual reasoning necessary for new applications in areas such as computer automation. However, agent tasks emphasize skills where accessible open-weight VLMs lag behind their LLM equivalents. For example, VLMs are less capable of following an environment's strict output syntax requirements and are more focused on open-ended question answering. Overcoming these limitations requires supervised fine-tuning (SFT) on task-specific expert demonstrations. Our work approaches these challenges from an offline-to-online reinforcement learning (RL) perspective. RL lets us fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of our own model or more capable (larger) models. We explore an off-policy RL solution that retains the stability and simplicity of the widely used SFT workflow while allowing our agent to self-improve and learn from low-quality datasets. We demonstrate this technique with two open-weight VLMs across three multi-modal agent domains.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions</title>
<link>https://arxiv.org/abs/2505.03194</link>
<guid>https://arxiv.org/abs/2505.03194</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, consistency models, data generation, convergence, sampling

Summary:
Diffusion models are successful in generating data but are computationally expensive due to iterative sampling. Consistency models aim to directly map noise to data for faster generation. This paper analyzes the convergence of consistency models when the self-consistency property is approximately satisfied in the training distribution. The analysis applies to forward processes under mild data assumptions. For target distributions with bounded support or fast-decaying tails, generated samples are close to the target in Wasserstein distance. When the target distribution is smooth, additional perturbation steps can ensure generated samples are close in total variation distance. Case studies demonstrate the advantages of multistep sampling in consistency models. <br /><br />Summary: Diffusion models are powerful but computationally expensive for data generation. Consistency models offer fast one-step generation with multistep sampling for improved quality. This paper's analysis explores convergence under approximate self-consistency, showing close approximation of target distributions in Wasserstein and total variation distances. Two case studies illustrate the benefits of multistep sampling. <div>
arXiv:2505.03194v1 Announce Type: new 
Abstract: Diffusion models accomplish remarkable success in data generation tasks across various domains. However, the iterative sampling process is computationally expensive. Consistency models are proposed to learn consistency functions to map from noise to data directly, which allows one-step fast data generation and multistep sampling to improve sample quality. In this paper, we study the convergence of consistency models when the self-consistency property holds approximately under the training distribution. Our analysis requires only mild data assumption and applies to a family of forward processes. When the target data distribution has bounded support or has tails that decay sufficiently fast, we show that the samples generated by the consistency model are close to the target distribution in Wasserstein distance; when the target distribution satisfies some smoothness assumption, we show that with an additional perturbation step for smoothing, the generated samples are close to the target distribution in total variation distance. We provide two case studies with commonly chosen forward processes to demonstrate the benefit of multistep sampling.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights</title>
<link>https://arxiv.org/abs/2505.03205</link>
<guid>https://arxiv.org/abs/2505.03205</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, Regression tasks, Noisy data, Manifold, Approximation errors

Summary:
Transformers are foundational for language and video generation models like GPT and BERT. This study explores the performance of transformers for regression tasks with noisy input data on a manifold. The input data are in a tubular neighborhood of the manifold, and the ground truth function depends on the projection of the noisy data onto the manifold. By analyzing approximation and generalization errors, the study establishes that transformers can effectively handle low-dimensional structures in learning tasks even with high-dimensional noise present. The intrinsic dimension of the manifold plays a significant role in the performance of transformers. The study introduces a unique proof technique that constructs representations of basic arithmetic operations by transformers, showcasing their ability to understand and leverage low-complexity structures in learning tasks. 

<br /><br />Summary: <div>
arXiv:2505.03205v1 Announce Type: new 
Abstract: Transformers serve as the foundational architecture for large language and video generation models, such as GPT, BERT, SORA and their successors. Empirical studies have demonstrated that real-world data and learning tasks exhibit low-dimensional structures, along with some noise or measurement error. The performance of transformers tends to depend on the intrinsic dimension of the data/tasks, though theoretical understandings remain largely unexplored for transformers. This work establishes a theoretical foundation by analyzing the performance of transformers for regression tasks involving noisy input data on a manifold. Specifically, the input data are in a tubular neighborhood of a manifold, while the ground truth function depends on the projection of the noisy data onto the manifold. We prove approximation and generalization errors which crucially depend on the intrinsic dimension of the manifold. Our results demonstrate that transformers can leverage low-complexity structures in learning task even when the input data are perturbed by high-dimensional noise. Our novel proof technique constructs representations of basic arithmetic operations by transformers, which may hold independent interest.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partial Label Clustering</title>
<link>https://arxiv.org/abs/2505.03207</link>
<guid>https://arxiv.org/abs/2505.03207</guid>
<content:encoded><![CDATA[
<div> Keywords: Partial Label Learning, Clustering, Weakly Supervised Learning, Disambiguation, Dual-Graph Learning

Summary: 
This paper explores the partial label clustering problem within the context of weakly supervised learning. The approach involves constructing a weight matrix based on example relationships, disambiguating candidate labels to estimate the ground-truth label, and implementing must-link and cannot-link constraints. By propagating these constraints using an adversarial prior promoted dual-graph learning approach, clustering performance is enhanced. The integration of weight matrix construction, label disambiguation, and pairwise constraints propagation in a joint model leads to mutual enhancement. Theoretical proof is provided to support the idea that better disambiguated labels can improve clustering performance. Experimental results demonstrate the superiority of the proposed method compared to existing constrained clustering methods and show its effectiveness in limited sample annotation scenarios. The code implementation is publicly available for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.03207v1 Announce Type: new 
Abstract: Partial label learning (PLL) is a significant weakly supervised learning framework, where each training example corresponds to a set of candidate labels and only one label is the ground-truth label. For the first time, this paper investigates the partial label clustering problem, which takes advantage of the limited available partial labels to improve the clustering performance. Specifically, we first construct a weight matrix of examples based on their relationships in the feature space and disambiguate the candidate labels to estimate the ground-truth label based on the weight matrix. Then, we construct a set of must-link and cannot-link constraints based on the disambiguation results. Moreover, we propagate the initial must-link and cannot-link constraints based on an adversarial prior promoted dual-graph learning approach. Finally, we integrate weight matrix construction, label disambiguation, and pairwise constraints propagation into a joint model to achieve mutual enhancement. We also theoretically prove that a better disambiguated label matrix can help improve clustering performance. Comprehensive experiments demonstrate our method realizes superior performance when comparing with state-of-the-art constrained clustering methods, and outperforms PLL and semi-supervised PLL methods when only limited samples are annotated. The code is publicly available at https://github.com/xyt-ml/PLC.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.03209</link>
<guid>https://arxiv.org/abs/2505.03209</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, expert demonstrations, large language models, strategy-based learning, sample efficiency

Summary: 
DYSTIL is a novel reinforcement learning framework that integrates large language models to improve learning from expert demonstrations. It dynamically queries a language model to induce textual strategies based on advantage estimations and expert demonstrations, enhancing the RL agent's performance, generalization, and sample efficiency. DYSTIL allows for the observation and interpretation of the policy's evolution through textual channels. Tested on challenging environments, DYSTIL outperforms existing methods by 17.75% in average success rate while maintaining higher sample efficiency. This novel approach significantly enhances policy generalization and improves overall performance in reinforcement learning tasks. <div>
arXiv:2505.03209v1 Announce Type: new 
Abstract: Reinforcement learning from expert demonstrations has long remained a challenging research problem, and existing state-of-the-art methods using behavioral cloning plus further RL training often suffer from poor generalization, low sample efficiency, and poor model interpretability. Inspired by the strong reasoning abilities of large language models (LLMs), we propose a novel strategy-based reinforcement learning framework integrated with LLMs called DYnamic STrategy Induction with Llms for reinforcement learning (DYSTIL) to overcome these limitations. DYSTIL dynamically queries a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations, and gradually internalizes induced strategies into the RL agent through policy optimization to improve its performance through boosting policy generalization and enhancing sample efficiency. It also provides a direct textual channel to observe and interpret the evolution of the policy's underlying strategies during training. We test DYSTIL over challenging RL environments from Minigrid and BabyAI, and empirically demonstrate that DYSTIL significantly outperforms state-of-the-art baseline methods by 17.75% in average success rate while also enjoying higher sample efficiency during the learning process.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach</title>
<link>https://arxiv.org/abs/2505.03230</link>
<guid>https://arxiv.org/abs/2505.03230</guid>
<content:encoded><![CDATA[
<div> UAV-assisted mobile edge computing, SWIPT, directional antennas, energy efficiency, terminal battery sustainability<br />
<br />
Summary: 
This paper presents a UAV-assisted mobile edge computing system with directional antennas to support IoT devices with both computational resources and energy. The system addresses challenges such as limited UAV battery capacity and dynamic task arrivals through a bi-objective optimization problem formulation. The problem is reformulated as a Markov decision process (MDP) and an improved soft actor-critic (SAC) algorithm is proposed for efficient energy management and high computational performance. Simulation results show outperformance of various baselines, demonstrating the effectiveness of the approach in balancing energy consumption and resource allocation. The method also exhibits strong generalization across different scenarios, particularly in complex environments, validating the effectiveness of boundary penalty and charging reward mechanisms. <br /><br /> <div>
arXiv:2505.03230v1 Announce Type: new 
Abstract: The integration of simultaneous wireless information and power transfer (SWIPT) technology in 6G Internet of Things (IoT) networks faces significant challenges in remote areas and disaster scenarios where ground infrastructure is unavailable. This paper proposes a novel unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) system enhanced by directional antennas to provide both computational resources and energy support for ground IoT terminals. However, such systems require multiple trade-off policies to balance UAV energy consumption, terminal battery levels, and computational resource allocation under various constraints, including limited UAV battery capacity, non-linear energy harvesting characteristics, and dynamic task arrivals. To address these challenges comprehensively, we formulate a bi-objective optimization problem that simultaneously considers system energy efficiency and terminal battery sustainability. We then reformulate this non-convex problem with a hybrid solution space as a Markov decision process (MDP) and propose an improved soft actor-critic (SAC) algorithm with an action simplification mechanism to enhance its convergence and generalization capabilities. Simulation results have demonstrated that our proposed approach outperforms various baselines in different scenarios, achieving efficient energy management while maintaining high computational performance. Furthermore, our method shows strong generalization ability across different scenarios, particularly in complex environments, validating the effectiveness of our designed boundary penalty and charging reward mechanisms.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDPs with a State Sensing Cost</title>
<link>https://arxiv.org/abs/2505.03280</link>
<guid>https://arxiv.org/abs/2505.03280</guid>
<content:encoded><![CDATA[
<div> problem formulation, sequential decision-making, sensing cost, Markov Decision Process, optimal policy

Summary: 
This article addresses sequential decision-making problems where tracking the environment incurs a cost for sensing. The agent must decide when to sense the state, balancing the value of optimal actions with the cost of sensing. The problem is formulated as an expected discounted cost Markov Decision Process (MDP) with an expanded state space. While computing the optimal policy for this MDP is generally intractable, the article bounds the sub-optimality gap for optimal policies in a restricted class. Additionally, a heuristic algorithm based on policy improvement is proposed, which performs closely to the optimal policy in practice. The article concludes with a numerical case study benchmarking against existing methods. <div>
arXiv:2505.03280v1 Announce Type: new 
Abstract: In many practical sequential decision-making problems, tracking the state of the environment incurs a sensing/communication/computation cost. In these settings, the agent's interaction with its environment includes the additional component of deciding $\textit{when}$ to sense the state, in a manner that balances the value associated with optimal (state-specific) actions and the cost of sensing. We formulate this as an expected discounted cost Markov Decision Process (MDP), wherein the agent incurs an additional cost for sensing its next state, but has the option to take actions while remaining 'blind' to the system state.
  We pose this problem as a classical discounted cost MDP with an expanded (countably infinite) state space. While computing the optimal policy for this MDP is intractable in general, we bound the sub-optimality gap associated with optimal policies in a restricted class, where the number of consecutive non-sensing (a.k.a., blind) actions is capped. We also design a computationally efficient heuristic algorithm based on policy improvement, which in practice performs close to the optimal policy. Finally, we benchmark against the state of the art via a numerical case study.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-inspired Energy Transition Neural Network for Sequence Learning</title>
<link>https://arxiv.org/abs/2505.03281</link>
<guid>https://arxiv.org/abs/2505.03281</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, recurrent neural networks, long-term dependencies, PETNN, sequence tasks

Summary:
PETNN is a novel recurrent neural network architecture inspired by physics energy transition models. It is designed to effectively capture long-term dependencies in sequence modeling tasks. Unlike Transformers, PETNN's memory mechanism enables it to store information over extended periods, leading to superior performance in various sequence tasks. The study indicates that PETNN outperforms Transformer-based methods while exhibiting significantly lower complexity due to its recurrent nature. This suggests the potential for developing more efficient recurrent neural networks in domains traditionally dominated by Transformer models. Overall, PETNN showcases the effectiveness of pure RNNs in capturing long-term dependencies and presents a promising alternative to current state-of-the-art sequence modeling techniques.<br /><br />Summary: <div>
arXiv:2505.03281v1 Announce Type: new 
Abstract: Recently, the superior performance of Transformers has made them a more robust and scalable solution for sequence modeling than traditional recurrent neural networks (RNNs). However, the effectiveness of Transformer in capturing long-term dependencies is primarily attributed to their comprehensive pair-modeling process rather than inherent inductive biases toward sequence semantics. In this study, we explore the capabilities of pure RNNs and reassess their long-term learning mechanisms. Inspired by the physics energy transition models that track energy changes over time, we propose a effective recurrent structure called the``Physics-inspired Energy Transition Neural Network" (PETNN). We demonstrate that PETNN's memory mechanism effectively stores information over long-term dependencies. Experimental results indicate that PETNN outperforms transformer-based methods across various sequence tasks. Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower complexity. Our study presents an optimal foundational recurrent architecture and highlights the potential for developing effective recurrent neural networks in fields currently dominated by Transformer.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling the Rainbow: can value-based methods schedule?</title>
<link>https://arxiv.org/abs/2505.03323</link>
<guid>https://arxiv.org/abs/2505.03323</guid>
<content:encoded><![CDATA[
<div> Keywords: deep reinforcement learning, combinatorial optimization, value-based algorithms, job-shop scheduling, flexible job-shop scheduling

Summary:
Deep reinforcement learning has shown promise in solving complex combinatorial optimization problems. While policy-based methods are commonly used in this field, this study evaluates the effectiveness of value-based algorithms, such as deep q-network and its extensions, in solving the job-shop and flexible job-shop scheduling problems. The results challenge the belief that policy-based methods are always superior, as value-based approaches can perform equally well or even better. This suggests that value-based strategies should be given more attention in combinatorial optimization research. The code used in the study is openly available for further exploration.  <br /><br />Summary: <div>
arXiv:2505.03323v1 Announce Type: new 
Abstract: Recently, deep reinforcement learning has emerged as a promising approach for solving complex combinatorial optimization problems. Broadly, deep reinforcement learning methods fall into two categories: policy-based and value-based. While value-based approaches have achieved notable success in domains such as the Arcade Learning Environment, the combinatorial optimization community has predominantly favored policy-based methods, often overlooking the potential of value-based algorithms. In this work, we conduct a comprehensive empirical evaluation of value-based algorithms, including the deep q-network and several of its advanced extensions, within the context of two complex combinatorial problems: the job-shop and the flexible job-shop scheduling problems, two fundamental challenges with multiple industrial applications. Our results challenge the assumption that policy-based methods are inherently superior for combinatorial optimization. We show that several value-based approaches can match or even outperform the widely adopted proximal policy optimization algorithm, suggesting that value-based strategies deserve greater attention from the combinatorial optimization community. Our code is openly available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Absolute Zero: Reinforced Self-play Reasoning with Zero Data</title>
<link>https://arxiv.org/abs/2505.03335</link>
<guid>https://arxiv.org/abs/2505.03335</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, verifiable rewards, reasoning capabilities, language models, Absolute Zero<br />
Summary:<br />
The article introduces a new paradigm called Absolute Zero for reinforcement learning with verifiable rewards. In Absolute Zero, a model, Absolute Zero Reasoner (AZR), autonomously proposes tasks to maximize learning progress without external data reliance. It uses a code executor to validate tasks and verify answers, achieving state-of-the-art performance on coding and mathematical reasoning tasks. AZR surpasses existing models relying on human-curated examples, demonstrating scalability and adaptability across different model scales. This approach addresses concerns of limited human supervision and potential learning limitations for superintelligent systems, providing a promising framework for open-ended yet grounded learning. <div>
arXiv:2505.03335v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial Mechanistic Interpretability of Large Language Models</title>
<link>https://arxiv.org/abs/2505.03368</link>
<guid>https://arxiv.org/abs/2505.03368</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, geospatial mechanistic interpretability, spatial analysis, probing, sparse autoencoders

Summary: 
This chapter introduces a framework for studying geospatial mechanistic interpretability in Large Language Models (LLMs). The goal is to understand how these models process geographic information. The use of probing is outlined to reveal internal structures within LLMs, and the concept of mechanistic interpretability is introduced. Sparse autoencoders are discussed as a method to disentangle complex internal representations into more interpretable features. Through experiments using spatial autocorrelation, the study shows how LLMs process geographical information, particularly placenames, by displaying spatial patterns related to geographic locations. The framework provides insights into how LLMs think about geographic information and can shape the study and use of foundation models in geography.<br /><br />Summary: This chapter presents a framework for understanding how Large Language Models process geographical information, utilizing spatial analysis, probing techniques, and sparse autoencoders. By demonstrating how LLMs process geographical data through spatial patterns related to geographic locations, the study advances our understanding of internal representations in these models and their geospatial interpretability. The framework provides valuable insights into the workings of LLMs and can potentially influence the development and application of foundation models in geography. <div>
arXiv:2505.03368v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and "reasoning" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial mechanistic interpretability - using spatial analysis to reverse engineer how LLMs handle geographical information. Our aim is to advance our understanding of the internal representations that these complex models generate while processing geographical information - what one might call "how LLMs think about geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within LLMs. We then introduce the field of mechanistic interpretability, discussing the superposition hypothesis and the role of sparse autoencoders in disentangling polysemantic internal representations of LLMs into more interpretable, monosemantic features. In our experiments, we use spatial autocorrelation to show how features obtained for placenames display spatial patterns related to their geographic location and can thus be interpreted geospatially, providing insights into how these models process geographical information. We conclude by discussing how our framework can help shape the study and use of foundation models in geography.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPAP: Structured Pruning via Alternating Optimization and Penalty Methods</title>
<link>https://arxiv.org/abs/2505.03373</link>
<guid>https://arxiv.org/abs/2505.03373</guid>
<content:encoded><![CDATA[
<div> penalty method, structured pruning, language models, optimization theory, alternating minimization<br />
<br />
Summary: <br />
The article introduces SPAP, a novel structured pruning framework for large language models (LLMs) that addresses the challenges of computational and memory demands. SPAP utilizes a mixed-integer optimization model and a penalty method to minimize pruning errors efficiently. The framework also includes an alternating minimization algorithm tailored to the problem structure for efficient weight updates and performance recovery. Experimental results on various models demonstrate SPAP's superiority over existing methods, providing linear inference speedups and memory reductions proportional to sparsity. The approach offers a practical, optimization-driven solution for pruning LLMs while maintaining model performance. <div>
arXiv:2505.03373v1 Announce Type: new 
Abstract: The deployment of large language models (LLMs) is often constrained by their substantial computational and memory demands. While structured pruning presents a viable approach by eliminating entire network components, existing methods suffer from performance degradation, reliance on heuristic metrics, or expensive finetuning. To address these challenges, we propose SPAP (Structured Pruning via Alternating Optimization and Penalty Methods), a novel and efficient structured pruning framework for LLMs grounded in optimization theory. SPAP formulates the pruning problem through a mixed-integer optimization model, employs a penalty method that effectively makes pruning decisions to minimize pruning errors, and introduces an alternating minimization algorithm tailored to the splittable problem structure for efficient weight updates and performance recovery. Extensive experiments on OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at 30% sparsity) and proportional memory reductions. Our work offers a practical, optimization-driven solution for pruning LLMs while preserving model performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models</title>
<link>https://arxiv.org/abs/2505.03382</link>
<guid>https://arxiv.org/abs/2505.03382</guid>
<content:encoded><![CDATA[
<div> Keywords: active stress models, cardiac biomechanics, physics-informed neural networks, myocardial function, fibrotic scars <br />
<br />
Summary: 
This study focuses on using physics-informed neural networks (PINNs) to infer active contractility parameters in cardiac biomechanical models from medical imaging data. The accurate assessment of active stress parameters is crucial for understanding myocardial function. By parametrising the state and parameter field with neural networks and formulating an energy minimisation problem, the study reconstructs active stress fields with high spatial resolution, even in the presence of noise. The method is enhanced with adaptive weighting schemes, regularisation strategies, Fourier features, and suitable network architectures. The influence of loss weights on parameter reconstruction is thoroughly analysed. The approach is applied to detecting tissue inhomogeneities and fibrotic scars in myocardial tissue. This innovative method could significantly improve the diagnosis, treatment planning, and management of heart conditions associated with cardiac fibrosis. <br /><br /> Summary: <div>
arXiv:2505.03382v1 Announce Type: new 
Abstract: Active stress models in cardiac biomechanics account for the mechanical deformation caused by muscle activity, thus providing a link between the electrophysiological and mechanical properties of the tissue. The accurate assessment of active stress parameters is fundamental for a precise understanding of myocardial function but remains difficult to achieve in a clinical setting, especially when only displacement and strain data from medical imaging modalities are available. This work investigates, through an in-silico study, the application of physics-informed neural networks (PINNs) for inferring active contractility parameters in time-dependent cardiac biomechanical models from these types of imaging data. In particular, by parametrising the sought state and parameter field with two neural networks, respectively, and formulating an energy minimisation problem to search for the optimal network parameters, we are able to reconstruct in various settings active stress fields in the presence of noise and with a high spatial resolution. To this end, we also advance the vanilla PINN learning algorithm with the use of adaptive weighting schemes, ad-hoc regularisation strategies, Fourier features, and suitable network architectures. In addition, we thoroughly analyse the influence of the loss weights in the reconstruction of active stress parameters. Finally, we apply the method to the characterisation of tissue inhomogeneities and detection of fibrotic scars in myocardial tissue. This approach opens a new pathway to significantly improve the diagnosis, treatment planning, and management of heart conditions associated with cardiac fibrosis.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2505.03387</link>
<guid>https://arxiv.org/abs/2505.03387</guid>
<content:encoded><![CDATA[
<div> machine learning, omics datasets, feature selection, data augmentation, classification accuracy

Summary:
This study focuses on addressing the challenges of interpreting omics datasets by proposing a machine learning classification framework. The framework integrates feature selection and data augmentation techniques to improve model performance and interpretability. By conducting a bootstrap analysis on a small dataset (E MTAB 8026), the study demonstrates that the proposed model maintains high classification accuracy when applied to a larger test set. The results highlight the importance of balancing accuracy and feature selection in classification models, showing the benefits of incorporating synthetic data for better generalization, particularly in scenarios with limited sample availability. Overall, the study emphasizes the need for transparent and reliable model decisions in omics-based classification tasks.  

<br /><br />Summary: <div>
arXiv:2505.03387v1 Announce Type: new 
Abstract: Given the increasing complexity of omics datasets, a key challenge is not only improving classification performance but also enhancing the transparency and reliability of model decisions. Effective model performance and feature selection are fundamental for explainability and reliability. In many cases, high dimensional omics datasets suffer from limited number of samples due to clinical constraints, patient conditions, phenotypes rarity and others conditions. Current omics based classification models often suffer from narrow interpretability, making it difficult to discern meaningful insights where trust and reproducibility are critical. This study presents a machine learning based classification framework that integrates feature selection with data augmentation techniques to achieve high standard classification accuracy while ensuring better interpretability. Using the publicly available dataset (E MTAB 8026), we explore a bootstrap analysis in six binary classification scenarios to evaluate the proposed model's behaviour. We show that the proposed pipeline yields cross validated perfomance on small dataset that is conserved when the trained classifier is applied to a larger test set. Our findings emphasize the fundamental balance between accuracy and feature selection, highlighting the positive effect of introducing synthetic data for better generalization, even in scenarios with very limited samples availability.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept Factorization via Self-Representation and Adaptive Graph Structure Learning</title>
<link>https://arxiv.org/abs/2505.03390</link>
<guid>https://arxiv.org/abs/2505.03390</guid>
<content:encoded><![CDATA[
<div> CF models, clustering, graph structure learning, self-representation, adaptive learning<br />
<br />
Summary: Concept Factorization (CF) models have been successful in data clustering, with recent variants incorporating internal geometric structure and graph regularization. However, performance is heavily reliant on initial graph construction. To address this, the Concept Factorization Based on Self-Representation and Adaptive Graph Structure Learning (CFSRAG) Model is proposed. CFSRAG learns data affinity through self-representation and uses this affinity matrix for dynamic graph regularization, facilitating adaptive learning of data's internal geometric structure. The model's update rule, convergence analysis, and experimental results on four real datasets demonstrate its superiority over existing models. <div>
arXiv:2505.03390v1 Announce Type: new 
Abstract: Concept Factorization (CF) models have attracted widespread attention due to their excellent performance in data clustering. In recent years, many variant models based on CF have achieved great success in clustering by taking into account the internal geometric manifold structure of the dataset and using graph regularization techniques. However, their clustering performance depends greatly on the construction of the initial graph structure. In order to enable adaptive learning of the graph structure of the data, we propose a Concept Factorization Based on Self-Representation and Adaptive Graph Structure Learning (CFSRAG) Model. CFSRAG learns the affinity relationship between data through a self-representation method, and uses the learned affinity matrix to implement dynamic graph regularization constraints, thereby ensuring dynamic learning of the internal geometric structure of the data. Finally, we give the CFSRAG update rule and convergence analysis, and conduct comparative experiments on four real datasets. The results show that our model outperforms other state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Calibration for Membership Inference Attack on Large Language Models</title>
<link>https://arxiv.org/abs/2505.03392</link>
<guid>https://arxiv.org/abs/2505.03392</guid>
<content:encoded><![CDATA[
<div> Keywords: Membership Inference Attacks, Large Language Models, Automatic Calibration, Probability Calibration, Robustness

Summary:
Membership Inference Attacks (MIAs) have been used to identify if a given text was part of the pre-training data for Large Language Models (LLMs). However, existing methods often struggle with misidentifying non-members as members, leading to high false positive rates. The Automatic Calibration Membership Inference Attack (ACMIA) framework addresses this issue by utilizing a tunable temperature to effectively calibrate output probabilities. This framework, inspired by maximum likelihood estimation during LLM pre-training, offers three configurations to cater to different levels of model access and increase the probability gap between members and non-members. Extensive experiments on various open-source LLMs demonstrate that ACMIA outperforms existing approaches across three widely used benchmarks in terms of effectiveness, robustness, and generalizability. The code for ACMIA is available on Github for further exploration and implementation.<br /><br />Summary: <div>
arXiv:2505.03392v1 Announce Type: new 
Abstract: Membership Inference Attacks (MIAs) have recently been employed to determine whether a specific text was part of the pre-training data of Large Language Models (LLMs). However, existing methods often misinfer non-members as members, leading to a high false positive rate, or depend on additional reference models for probability calibration, which limits their practicality. To overcome these challenges, we introduce a novel framework called Automatic Calibration Membership Inference Attack (ACMIA), which utilizes a tunable temperature to calibrate output probabilities effectively. This approach is inspired by our theoretical insights into maximum likelihood estimation during the pre-training of LLMs. We introduce ACMIA in three configurations designed to accommodate different levels of model access and increase the probability gap between members and non-members, improving the reliability and robustness of membership inference. Extensive experiments on various open-source LLMs demonstrate that our proposed attack is highly effective, robust, and generalizable, surpassing state-of-the-art baselines across three widely used benchmarks. Our code is available at: \href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction Models That Learn to Avoid Missing Values</title>
<link>https://arxiv.org/abs/2505.03393</link>
<guid>https://arxiv.org/abs/2505.03393</guid>
<content:encoded><![CDATA[
<div> framework, training models, missing values, interpretability, machine learning<br />
<br />
Summary:
The article introduces missingness-avoiding (MA) machine learning, a framework designed to handle missing values at test time without compromising accuracy or interpretability. Traditional approaches often introduce bias or complexity through imputation or missingness indicators. The proposed MA learning algorithms for decision trees, tree ensembles, and sparse linear models include classifier-specific regularization terms in their objectives, enabling them to reduce reliance on missing values based on observed context. Experimental results on real-world datasets show that MA-DT, MA-LASSO, MA-RF, and MA-GBT effectively minimize the need for features with missing values while maintaining competitive predictive performance. This framework provides practitioners with a powerful tool to ensure interpretability in predictions even with test-time missing values. 
<br /><br /> <div>
arXiv:2505.03393v1 Announce Type: new 
Abstract: Handling missing values at test time is challenging for machine learning models, especially when aiming for both high accuracy and interpretability. Established approaches often add bias through imputation or excessive model complexity via missingness indicators. Moreover, either method can obscure interpretability, making it harder to understand how the model utilizes the observed variables in predictions. We propose missingness-avoiding (MA) machine learning, a general framework for training models to rarely require the values of missing (or imputed) features at test time. We create tailored MA learning algorithms for decision trees, tree ensembles, and sparse linear models by incorporating classifier-specific regularization terms in their learning objectives. The tree-based models leverage contextual missingness by reducing reliance on missing values based on the observed context. Experiments on real-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT effectively reduce the reliance on features with missing values while maintaining predictive performance competitive with their unregularized counterparts. This shows that our framework gives practitioners a powerful tool to maintain interpretability in predictions with test-time missing values.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2505.03418</link>
<guid>https://arxiv.org/abs/2505.03418</guid>
<content:encoded><![CDATA[
<div> Keywords: problem-solving, Large Language Models (LLMs), reasoning, knowledge augmentation, verification

Summary:
Large Language Models (LLMs) are powerful tools that combine computational power with human-like reasoning to tackle complex problems. This survey examines the capabilities and limitations of LLMs in problem-solving, exploring techniques such as Chain-of-Thought (CoT) reasoning, knowledge augmentation, and verification methods. The paper discusses challenges in domains like software engineering, mathematical reasoning, data analysis, and scientific research. It also highlights the limitations of current LLM solutions and proposes future directions for improving multi-step reasoning, domain knowledge integration, and result verification in LLM-based problem-solving.<br /><br />Summary: <div>
arXiv:2505.03418v1 Announce Type: new 
Abstract: Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense</title>
<link>https://arxiv.org/abs/2505.03424</link>
<guid>https://arxiv.org/abs/2505.03424</guid>
<content:encoded><![CDATA[
<div> Framework, Trusted AI, Graph Neural Networks, Interpretability, Robustness
Summary: 
The article introduces GNN-AID, an open-source framework designed for interpreting and defending machine learning models on graph data. Built on PyTorch-Geometric, GNN-AID offers trust methods, architectural layers, and preloaded datasets/models for analyzing GNN behavior. It includes a web interface for graph visualization and no-code features for model building. The framework supports MLOps techniques for reproducibility and result versioning. Developers can create and customize graph models, while researchers can explore interpretability, robustness, and defense strategies against attacks. The article also discusses conflicting defenses against evasion and poisoning attacks on graph data, highlighting the complexities in applying defense strategies. GNN-AID provides a flexible tool for developers and researchers to enhance their understanding and protection of graph-based machine learning models. <div>
arXiv:2505.03424v1 Announce Type: new 
Abstract: The growing need for Trusted AI (TAI) highlights the importance of interpretability and robustness in machine learning models. However, many existing tools overlook graph data and rarely combine these two aspects into a single solution. Graph Neural Networks (GNNs) have become a popular approach, achieving top results across various tasks. We introduce GNN-AID (Graph Neural Network Analysis, Interpretation, and Defense), an open-source framework designed for graph data to address this gap. Built as a Python library, GNN-AID supports advanced trust methods and architectural layers, allowing users to analyze graph datasets and GNN behavior using attacks, defenses, and interpretability methods.
  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models, and support for any GNNs through customizable interfaces. It also includes a web interface with tools for graph visualization and no-code features like an interactive model builder, simplifying the exploration and analysis of GNNs. The framework also supports MLOps techniques, ensuring reproducibility and result versioning to track and revisit analyses efficiently.
  GNN-AID is a flexible tool for developers and researchers. It helps developers create, analyze, and customize graph models, while also providing access to prebuilt datasets and models for quick experimentation. Researchers can use the framework to explore advanced topics on the relationship between interpretability and robustness, test defense strategies, and combine methods to protect against different types of attacks.
  We also show how defenses against evasion and poisoning attacks can conflict when applied to graph data, highlighting the complex connections between defense strategies.
  GNN-AID is available at \href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients</title>
<link>https://arxiv.org/abs/2505.03432</link>
<guid>https://arxiv.org/abs/2505.03432</guid>
<content:encoded><![CDATA[
<div> Generative Models, Score-Based Generative Models, Wasserstein-2 Convergence, Semiconvex Distributions, Non-Smooth Data Distributions<br />
Summary:<br />
Score-Based Generative Models (SGMs) utilize diffusion processes to model complex data distributions by perturbing them with Gaussian noise and then denoising the output. This method has shown superior performance across various domains but existing convergence analysis relies on strong regularity assumptions. This study introduces non-asymptotic Wasserstein-2 convergence guarantees for SGMs targeting semiconvex distributions with potentially discontinuous gradients. The bounds are explicit and achieve optimal dependence on data dimension and convergence rate. This framework accommodates a broad range of distributions such as Gaussian mixtures and double-well potentials. By leveraging semiconvexity without requiring smoothness assumptions, this research expands the theoretical understanding of SGMs, bridging the gap between empirical success and rigorous guarantees in non-smooth, complex data scenarios.<br /> <div>
arXiv:2505.03432v1 Announce Type: new 
Abstract: Score-based Generative Models (SGMs) approximate a data distribution by perturbing it with Gaussian noise and subsequently denoising it via a learned reverse diffusion process. These models excel at modeling complex data distributions and generating diverse samples, achieving state-of-the-art performance across domains such as computer vision, audio generation, reinforcement learning, and computational biology. Despite their empirical success, existing Wasserstein-2 convergence analysis typically assume strong regularity conditions-such as smoothness or strict log-concavity of the data distribution-that are rarely satisfied in practice. In this work, we establish the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs targeting semiconvex distributions with potentially discontinuous gradients. Our upper bounds are explicit and sharp in key parameters, achieving optimal dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of order one. The framework accommodates a wide class of practically relevant distributions, including symmetric modified half-normal distributions, Gaussian mixtures, double-well potentials, and elastic net potentials. By leveraging semiconvexity without requiring smoothness assumptions on the potential such as differentiability, our results substantially broaden the theoretical foundations of SGMs, bridging the gap between empirical success and rigorous guarantees in non-smooth, complex data regimes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)</title>
<link>https://arxiv.org/abs/2505.03490</link>
<guid>https://arxiv.org/abs/2505.03490</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative models, Time series imputation, Memorization, Privacy risks, Membership inference attacks

Summary:
The paper introduces the Loss-Based with Reference Model (LBRM) algorithm to address the issue of memorization in time series imputation models, which can lead to privacy risks. The LBRM method uses a reference model to improve the accuracy of membership inference attacks, distinguishing between training and test data. The proposed method effectively extracts and identifies memorized training data, significantly enhancing detection accuracy without fine-tuning and further increasing it with fine-tuning. The approach is validated through membership inference attacks on different architectures for time series imputation, showing the robustness and versatility of the LBRM method in various contexts. Overall, the results demonstrate a substantial improvement in detection accuracy, effectively mitigating privacy risks associated with memorization in time series imputation models. 

<br /><br />Summary: <div>
arXiv:2505.03490v1 Announce Type: new 
Abstract: Generative models can unintentionally memorize training data, posing significant privacy risks. This paper addresses the memorization phenomenon in time series imputation models, introducing the Loss-Based with Reference Model (LBRM) algorithm. The LBRM method leverages a reference model to enhance the accuracy of membership inference attacks, distinguishing between training and test data. Our contributions are twofold: first, we propose an innovative method to effectively extract and identify memorized training data, significantly improving detection accuracy. On average, without fine-tuning, the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased by approximately 60\%. Second, we validate our approach through membership inference attacks on two types of architectures designed for time series imputation, demonstrating the robustness and versatility of the LBRM approach in different contexts. These results highlight the significant enhancement in detection accuracy provided by the LBRM approach, addressing privacy risks in time series imputation models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning</title>
<link>https://arxiv.org/abs/2505.03509</link>
<guid>https://arxiv.org/abs/2505.03509</guid>
<content:encoded><![CDATA[
<div> Anomaly detection, large datasets, astronomy, computer vision, semi-supervised learning,<br />
Summary:<br />
1. AnomalyMatch is a framework that combines FixMatch algorithm with active learning for anomaly detection in large datasets like in astronomy and computer vision.<br />
2. It treats anomaly detection as a semi-supervised binary classification problem, utilizing limited labeled and abundant unlabeled images efficiently.<br />
3. The framework allows iterative model refinement through a user interface for expert verification and correction of false positives.<br />
4. Evaluations on astronomical dataset GalaxyMNIST and natural-image benchmark miniImageNet show strong performance in detecting anomalies with high precision after active learning cycles.<br />
5. AnomalyMatch is tailored for large-scale applications and can process predictions for millions of images efficiently, making it suitable for discovering scientifically valuable anomalies in vast astronomical datasets.<br /> <div>
arXiv:2505.03509v1 Announce Type: new 
Abstract: Anomaly detection in large datasets is essential in fields such as astronomy and computer vision; however, supervised methods typically require extensive anomaly labelling, which is often impractical. We present AnomalyMatch, an anomaly detection framework combining the semi-supervised FixMatch algorithm using EfficientNet classifiers with active learning. By treating anomaly detection as a semi-supervised binary classification problem, we efficiently utilise limited labelled and abundant unlabelled images. We allow iterative model refinement in a user interface for expert verification of high-confidence anomalies and correction of false positives. Built for astronomical data, AnomalyMatch generalises readily to other domains facing similar data challenges. Evaluations on the GalaxyMNIST astronomical dataset and the miniImageNet natural-image benchmark under severe class imbalance (1% anomalies for miniImageNet) display strong performance: starting from five to ten labelled anomalies and after three active learning cycles, we achieve an average AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective AUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with 71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images. AnomalyMatch is tailored for large-scale applications, efficiently processing predictions for 100 million images within three days on a single GPU. Integrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted discovery of scientifically valuable anomalies in vast astronomical datasets. Our results underscore the exceptional utility and scalability of this approach for anomaly discovery, highlighting the value of specialised approaches for domains characterised by severe label scarcity.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks</title>
<link>https://arxiv.org/abs/2505.03519</link>
<guid>https://arxiv.org/abs/2505.03519</guid>
<content:encoded><![CDATA[
<div> Evaluation framework, Model Inversion attacks, False positives, Adversarial features, Privacy leakage

Summary: This paper presents an in-depth study on the evaluation of Model Inversion (MI) attacks. Firstly, a comprehensive human-annotated dataset of MI attack samples is constructed, revealing false positives in the commonly used evaluation framework. The accuracy of MI attacks may have been overestimated due to this issue. The study uncovers the impact of Type I adversarial features on MI evaluation and suggests a relationship with adversarial transferability. The findings indicate that the actual privacy leakage in SOTA MI attacks is lower than previously reported. The paper highlights limitations in the current evaluation framework and proposes methods to mitigate false positive rates. It suggests considering human evaluation as a primary framework and encourages the development of more robust automatic evaluation methods.<br /><br />Summary: <div>
arXiv:2505.03519v1 Announce Type: new 
Abstract: Model Inversion (MI) attacks aim to reconstruct information of private training data by exploiting access to machine learning models. The most common evaluation framework for MI attacks/defenses relies on an evaluation model that has been utilized to assess progress across almost all MI attacks and defenses proposed in recent years. In this paper, for the first time, we present an in-depth study of MI evaluation. Firstly, we construct the first comprehensive human-annotated dataset of MI attack samples, based on 28 setups of different MI attacks, defenses, private and public datasets. Secondly, using our dataset, we examine the accuracy of the MI evaluation framework and reveal that it suffers from a significant number of false positives. These findings raise questions about the previously reported success rates of SOTA MI attacks. Thirdly, we analyze the causes of these false positives, design controlled experiments, and discover the surprising effect of Type I adversarial features on MI evaluation, as well as adversarial transferability, highlighting a relationship between two previously distinct research areas. Our findings suggest that the performance of SOTA MI attacks has been overestimated, with the actual privacy leakage being significantly less than previously reported. In conclusion, we highlight critical limitations in the widely used MI evaluation framework and present our methods to mitigate false positive rates. We remark that prior research has shown that Type I adversarial attacks are very challenging, with no existing solution. Therefore, we urge to consider human evaluation as a primary MI evaluation framework rather than merely a supplement as in previous MI research. We also encourage further work on developing more robust and reliable automatic evaluation frameworks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2505.03530</link>
<guid>https://arxiv.org/abs/2505.03530</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, Variational Autoencoders (VAEs), mechanistic interpretability, causal intervention, disentanglement.

Summary: 
This paper introduces a causal intervention framework for enhancing the interpretability of Variational Autoencoders (VAEs), focusing on understanding how semantic factors are processed within the network. The framework includes interventions such as input manipulations, latent space perturbations, activation patching, and causal mediation analysis to analyze "circuit motifs" in VAEs. By applying the framework to synthetic datasets and disentanglement benchmarks, the study demonstrates the ability to identify functional circuits, map computational graphs to causal graphs, and differentiate between polysemantic and monosemantic units. Metrics like causal effect strength, intervention specificity, and circuit modularity are introduced to quantify the interpretability of VAE components. Experimental results highlight the differences in interpretability between VAE variants, with FactorVAE showing higher disentanglement scores and effect strengths compared to standard VAE and Beta-VAE. Overall, this framework enhances the mechanistic understanding of generative models like VAEs and offers tools for designing more transparent and controllable architectures. 

<br /><br />Summary: <div>
arXiv:2505.03530v1 Announce Type: new 
Abstract: Mechanistic interpretability of deep learning models has emerged as a crucial research direction for understanding the functioning of neural networks. While significant progress has been made in interpreting discriminative models like transformers, understanding generative models such as Variational Autoencoders (VAEs) remains challenging. This paper introduces a comprehensive causal intervention framework for mechanistic interpretability of VAEs. We develop techniques to identify and analyze "circuit motifs" in VAEs, examining how semantic factors are encoded, processed, and disentangled through the network layers. Our approach uses targeted interventions at different levels: input manipulations, latent space perturbations, activation patching, and causal mediation analysis. We apply our framework to both synthetic datasets with known causal relationships and standard disentanglement benchmarks. Results show that our interventions can successfully isolate functional circuits, map computational graphs to causal graphs of semantic factors, and distinguish between polysemantic and monosemantic units. Furthermore, we introduce metrics for causal effect strength, intervention specificity, and circuit modularity that quantify the interpretability of VAE components. Experimental results demonstrate clear differences between VAE variants, with FactorVAE achieving higher disentanglement scores (0.084) and effect strengths (mean 4.59) compared to standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework advances the mechanistic understanding of generative models and provides tools for more transparent and controllable VAE architectures.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning</title>
<link>https://arxiv.org/abs/2505.03533</link>
<guid>https://arxiv.org/abs/2505.03533</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, federated learning, resource allocation, wireless networks, small-scale fading<br />
Summary:<br />
The paper introduces a novel resource allocation strategy for federated learning in wireless networks that considers rapid channel fluctuations. Using a multi-agent reinforcement learning framework, the proposed strategy addresses system and statistical heterogeneity by formulating the problem as a decentralized partially observable Markov decision process (Dec-POMDP) and solving it with the QMIX algorithm. Clients act as agents making local decisions based on observations and rewards from convergence analysis within each coherence time slot. The MARL approach simplifies decision-making while improving scalability. Experimental results demonstrate the superiority of the QMIX-based strategy over baseline methods, particularly in heterogeneous environments. Ablation studies emphasize the significance of considering small-scale fading dynamics for optimizing federated learning performance.<br /> <div>
arXiv:2505.03533v1 Announce Type: new 
Abstract: Judicious resource allocation can effectively enhance federated learning (FL) training performance in wireless networks by addressing both system and statistical heterogeneity. However, existing strategies typically rely on block fading assumptions, which overlooks rapid channel fluctuations within each round of FL gradient uploading, leading to a degradation in FL training performance. Therefore, this paper proposes a small-scale-fading-aware resource allocation strategy using a multi-agent reinforcement learning (MARL) framework. Specifically, we establish a one-step convergence bound of the FL algorithm and formulate the resource allocation problem as a decentralized partially observable Markov decision process (Dec-POMDP), which is subsequently solved using the QMIX algorithm. In our framework, each client serves as an agent that dynamically determines spectrum and power allocations within each coherence time slot, based on local observations and a reward derived from the convergence analysis. The MARL setting reduces the dimensionality of the action space and facilitates decentralized decision-making, enhancing the scalability and practicality of the solution. Experimental results demonstrate that our QMIX-based resource allocation strategy significantly outperforms baseline methods across various degrees of statistical heterogeneity. Additionally, ablation studies validate the critical importance of incorporating small-scale fading dynamics, highlighting its role in optimizing FL performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming</title>
<link>https://arxiv.org/abs/2505.03552</link>
<guid>https://arxiv.org/abs/2505.03552</guid>
<content:encoded><![CDATA[
<div> approach, Physics-enhanced Neural ODEs, dynamic optimization, high-order implicit Runge-Kutta method, NLP solver <br />
<br />
Summary: 
This paper introduces a innovative approach for training Physics-enhanced Neural ODEs (PeNODEs) by formulating the training process as a dynamic optimization problem. The model, involving neural components, is discretized using a high-order implicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, leading to a large-scale nonlinear program (NLP) which can be efficiently solved using state-of-the-art NLP solvers like Ipopt. This approach allows for simultaneous optimization of network parameters and state trajectories, addressing key challenges of ODE solver-based training such as stability, runtime, and accuracy. The paper extends a recent direct collocation-based method for Neural ODEs, generalizes to PeNODEs, includes physical constraints, and offers a custom, parallelized, open-source implementation. Benchmarks on a Quarter Vehicle Model and a Van-der-Pol oscillator indicate better accuracy, speed, and generalization with smaller networks in comparison to other training methods. Additionally, the integration of this approach into OpenModelica is planned to facilitate accessible training of Neural DAEs. <div>
arXiv:2505.03552v1 Announce Type: new 
Abstract: We propose a novel approach for training Physics-enhanced Neural ODEs (PeNODEs) by expressing the training process as a dynamic optimization problem. The full model, including neural components, is discretized using a high-order implicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting in a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art NLP solvers such as Ipopt. This formulation enables simultaneous optimization of network parameters and state trajectories, addressing key limitations of ODE solver-based training in terms of stability, runtime, and accuracy. Extending on a recent direct collocation-based method for Neural ODEs, we generalize to PeNODEs, incorporate physical constraints, and present a custom, parallelized, open-source implementation. Benchmarks on a Quarter Vehicle Model and a Van-der-Pol oscillator demonstrate superior accuracy, speed, and generalization with smaller networks compared to other training techniques. We also outline a planned integration into OpenModelica to enable accessible training of Neural DAEs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid AI-based generation of coverage paths for dispensing applications</title>
<link>https://arxiv.org/abs/2505.03560</link>
<guid>https://arxiv.org/abs/2505.03560</guid>
<content:encoded><![CDATA[
<div> Coverage Path Planning, Thermal Interface Materials, AI-based approach, Dispense Paths, Artificial Neural Network


Summary: 
Coverage Path Planning of Thermal Interface Materials (TIM) is important in power electronics and electronic control unit design. Traditionally done manually or via optimization approaches, a new AI-based method proposes using an Artificial Neural Network (ANN) to generate dispense paths for TIM applications. The ANN takes the target cooling area as input and outputs the dispense path, eliminating the need for labels and reducing computational effort. The resulting paths can be transferred directly to manufacturing equipment without air entrapments. This approach could potentially be applied to other manufacturing processes, offering real-time prediction of process parameters for desired target states. <div>
arXiv:2505.03560v1 Announce Type: new 
Abstract: Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial role in the design of power electronics and electronic control units. Up to now, this is done manually by experts or by using optimization approaches with a high computational effort. We propose a novel AI-based approach to generate dispense paths for TIM and similar dispensing applications. It is a drop-in replacement for optimization-based approaches. An Artificial Neural Network (ANN) receives the target cooling area as input and directly outputs the dispense path. Our proposed setup does not require labels and we show its feasibility on multiple target areas. The resulting dispense paths can be directly transferred to automated manufacturing equipment and do not exhibit air entrapments. The approach of using an ANN to predict process parameters for a desired target state in real-time could potentially be transferred to other manufacturing processes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ergodic Generative Flows</title>
<link>https://arxiv.org/abs/2505.03561</link>
<guid>https://arxiv.org/abs/2505.03561</guid>
<content:encoded><![CDATA[
<div> Ergodic Generative Flows, generative flow networks, imitation learning, diffeomorphisms, cross-entropy<br />
<br />
Summary:<br />
Generative Flow Networks have been extended to Ergodic Generative Flows (EGFs) to address challenges in continuous settings and imitation learning. EGFs leverage ergodicity to create simple generative flows with globally defined diffeomorphisms and tractable flow-matching loss. A new loss function, KL-weakFM loss, combines cross-entropy with weak flow-matching control for IL training without a separate reward model. IL-EGFs are evaluated on 2D tasks and NASA datasets on the sphere, using KL-weakFM loss. Additionally, toy 2D reinforcement learning experiments with target rewards are conducted using the FM loss. <div>
arXiv:2505.03561v1 Announce Type: new 
Abstract: Generative Flow Networks (GFNs) were initially introduced on directed acyclic graphs to sample from an unnormalized distribution density. Recent works have extended the theoretical framework for generative methods allowing more flexibility and enhancing application range. However, many challenges remain in training GFNs in continuous settings and for imitation learning (IL), including intractability of flow-matching loss, limited tests of non-acyclic training, and the need for a separate reward model in imitation learning. The present work proposes a family of generative flows called Ergodic Generative Flows (EGFs) which are used to address the aforementioned issues. First, we leverage ergodicity to build simple generative flows with finitely many globally defined transformations (diffeomorphisms) with universality guarantees and tractable flow-matching loss (FM loss). Second, we introduce a new loss involving cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It is designed for IL training without a separate reward model. We evaluate IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning experiments with a target reward, using the FM loss.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs</title>
<link>https://arxiv.org/abs/2505.03595</link>
<guid>https://arxiv.org/abs/2505.03595</guid>
<content:encoded><![CDATA[
arXiv:2505.03595v1 Announce Type: new 
Abstract: High-dimensional partial differential equations (PDEs) arise in diverse scientific and engineering applications but remain computationally intractable due to the curse of dimensionality. Traditional numerical methods struggle with the exponential growth in computational complexity, particularly on hypercubic domains, where the number of required collocation points increases rapidly with dimensionality. Here, we introduce Anant-Net, an efficient neural surrogate that overcomes this challenge, enabling the solution of PDEs in high dimensions. Unlike hyperspheres, where the internal volume diminishes as dimensionality increases, hypercubes retain or expand their volume (for unit or larger length), making high-dimensional computations significantly more demanding. Anant-Net efficiently incorporates high-dimensional boundary conditions and minimizes the PDE residual at high-dimensional collocation points. To enhance interpretability, we integrate Kolmogorov-Arnold networks into the Anant-Net architecture. We benchmark Anant-Net's performance on several linear and nonlinear high-dimensional equations, including the Poisson, Sine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and robustness across randomly sampled test points from high-dimensional space. Importantly, Anant-Net achieves these results with remarkable efficiency, solving 300-dimensional problems on a single GPU within a few hours. We also compare Anant-Net's results for accuracy and runtime with other state-of-the-art methods. Our findings establish Anant-Net as an accurate, interpretable, and scalable framework for efficiently solving high-dimensional PDEs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift</title>
<link>https://arxiv.org/abs/2505.03617</link>
<guid>https://arxiv.org/abs/2505.03617</guid>
<content:encoded><![CDATA[
arXiv:2505.03617v1 Announce Type: new 
Abstract: We evaluate the effectiveness of importance weighting in deep neural networks under label shift and covariate shift. On synthetic 2D data (linearly separable and moon-shaped) using logistic regression and MLPs, we observe that weighting strongly affects decision boundaries early in training but fades with prolonged optimization. On CIFAR-10 with various class imbalances, only L2 regularization (not dropout) helps preserve weighting effects. In a covariate-shift experiment, importance weighting yields no significant performance gain, highlighting challenges on complex data. Our results call into question the practical utility of importance weighting for real-world distribution shifts.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders</title>
<link>https://arxiv.org/abs/2505.03646</link>
<guid>https://arxiv.org/abs/2505.03646</guid>
<content:encoded><![CDATA[
arXiv:2505.03646v1 Announce Type: new 
Abstract: Despite the extensive use of deep autoencoders (AEs) in critical applications, their adversarial robustness remains relatively underexplored compared to classification models. AE robustness is characterized by the Lipschitz bounds of its components. Existing robustness evaluation frameworks based on white-box attacks do not fully exploit the vulnerabilities of intermediate ill-conditioned layers in AEs. In the context of optimizing imperceptible norm-bounded additive perturbations to maximize output damage, existing methods struggle to effectively propagate adversarial loss gradients throughout the network, often converging to less effective perturbations. To address this, we propose a novel layer-conditioning-based adversarial optimization objective that effectively guides the adversarial map toward regions of local Lipschitz bounds by enhancing loss gradient information propagation during attack optimization. We demonstrate through extensive experiments on state-of-the-art AEs that our adversarial objective results in stronger attacks, outperforming existing methods in both universal and sample-specific scenarios. As a defense method against this attack, we introduce an inference-time adversarially trained defense plugin that mitigates the effects of adversarial examples.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation</title>
<link>https://arxiv.org/abs/2505.03652</link>
<guid>https://arxiv.org/abs/2505.03652</guid>
<content:encoded><![CDATA[
arXiv:2505.03652v1 Announce Type: new 
Abstract: Normalizing flows (NFs) provide uncorrelated samples from complex distributions, making them an appealing tool for parameter estimation. However, the practical utility of NFs remains limited by their tendency to collapse to a single mode of a multimodal distribution. In this study, we show that annealing with an adaptive schedule based on the effective sample size (ESS) can mitigate mode collapse. We demonstrate that our approach can converge the marginal likelihood for a biochemical oscillator model fit to time-series data in ten-fold less computation time than a widely used ensemble Markov chain Monte Carlo (MCMC) method. We show that the ESS can also be used to reduce variance by pruning the samples. We expect these developments to be of general use for sampling with NFs and discuss potential opportunities for further improvements.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Integral Operators for Inverse problems in Spectroscopy</title>
<link>https://arxiv.org/abs/2505.03677</link>
<guid>https://arxiv.org/abs/2505.03677</guid>
<content:encoded><![CDATA[
arXiv:2505.03677v1 Announce Type: new 
Abstract: Deep learning has shown high performance on spectroscopic inverse problems when sufficient data is available. However, it is often the case that data in spectroscopy is scarce, and this usually causes severe overfitting problems with deep learning methods. Traditional machine learning methods are viable when datasets are smaller, but the accuracy and applicability of these methods is generally more limited.
  We introduce a deep learning method for classification of molecular spectra based on learning integral operators via integral equations of the first kind, which results in an algorithm that is less affected by overfitting issues on small datasets, compared to other deep learning models.
  The problem formulation of the deep learning approach is based on inverse problems, which have traditionally found important applications in spectroscopy. We perform experiments on real world data to showcase our algorithm. It is seen that the model outperforms traditional machine learning approaches such as decision tree and support vector machine, and for small datasets it outperforms other deep learning models. Therefore, our methodology leverages the power of deep learning, still maintaining the performance when the available data is very limited, which is one of the main issues that deep learning faces in spectroscopy, where datasets are often times of small size.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Survival Distributions with the Asymmetric Laplace Distribution</title>
<link>https://arxiv.org/abs/2505.03712</link>
<guid>https://arxiv.org/abs/2505.03712</guid>
<content:encoded><![CDATA[
arXiv:2505.03712v1 Announce Type: new 
Abstract: Probabilistic survival analysis models seek to estimate the distribution of the future occurrence (time) of an event given a set of covariates. In recent years, these models have preferred nonparametric specifications that avoid directly estimating survival distributions via discretization. Specifically, they estimate the probability of an individual event at fixed times or the time of an event at fixed probabilities (quantiles), using supervised learning. Borrowing ideas from the quantile regression literature, we propose a parametric survival analysis method based on the Asymmetric Laplace Distribution (ALD). This distribution allows for closed-form calculation of popular event summaries such as mean, median, mode, variation, and quantiles. The model is optimized by maximum likelihood to learn, at the individual level, the parameters (location, scale, and asymmetry) of the ALD distribution. Extensive results on synthetic and real-world data demonstrate that the proposed method outperforms parametric and nonparametric approaches in terms of accuracy, discrimination and calibration.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.03721</link>
<guid>https://arxiv.org/abs/2505.03721</guid>
<content:encoded><![CDATA[
arXiv:2505.03721v1 Announce Type: new 
Abstract: Solar sensor-based monitoring systems have become a crucial agricultural innovation, advancing farm management and animal welfare through integrating sensor technology, Internet-of-Things, and edge and cloud computing. However, the resilience of these systems to cyber-attacks and their adaptability to dynamic and constrained energy supplies remain largely unexplored. To address these challenges, we propose a sustainable smart farm network designed to maintain high-quality animal monitoring under various cyber and adversarial threats, as well as fluctuating energy conditions. Our approach utilizes deep reinforcement learning (DRL) to devise optimal policies that maximize both monitoring effectiveness and energy efficiency. To overcome DRL's inherent challenge of slow convergence, we integrate transfer learning (TL) and decision theory (DT) to accelerate the learning process. By incorporating DT-guided strategies, we optimize monitoring quality and energy sustainability, significantly reducing training time while achieving comparable performance rewards. Our experimental results prove that DT-guided DRL outperforms TL-enhanced DRL models, improving system performance and reducing training runtime by 47.5%.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Staleness Aware Incremental Learning for CTR Prediction</title>
<link>https://arxiv.org/abs/2505.02844</link>
<guid>https://arxiv.org/abs/2505.02844</guid>
<content:encoded><![CDATA[
arXiv:2505.02844v1 Announce Type: cross 
Abstract: Click-through Rate (CTR) prediction in real-world recommender systems often deals with billions of user interactions every day. To improve the training efficiency, it is common to update the CTR prediction model incrementally using the new incremental data and a subset of historical data. However, the feature embeddings of a CTR prediction model often get stale when the corresponding features do not appear in current incremental data. In the next period, the model would have a performance degradation on samples containing stale features, which we call the feature staleness problem. To mitigate this problem, we propose a Feature Staleness Aware Incremental Learning method for CTR prediction (FeSAIL) which adaptively replays samples containing stale features. We first introduce a staleness aware sampling algorithm (SAS) to sample a fixed number of stale samples with high sampling efficiency. We then introduce a staleness aware regularization mechanism (SAR) for a fine-grained control of the feature embedding updating. We instantiate FeSAIL with a general deep learning-based CTR prediction model and the experimental results demonstrate FeSAIL outperforms various state-of-the-art methods on four benchmark datasets.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization</title>
<link>https://arxiv.org/abs/2505.02887</link>
<guid>https://arxiv.org/abs/2505.02887</guid>
<content:encoded><![CDATA[
arXiv:2505.02887v1 Announce Type: cross 
Abstract: Target-specific peptides, such as conotoxins, exhibit exceptional binding affinity and selectivity toward ion channels and receptors. However, their therapeutic potential remains underutilized due to the limited diversity of natural variants and the labor-intensive nature of traditional optimization strategies. Here, we present CreoPep, a deep learning-based conditional generative framework that integrates masked language modeling with a progressive masking scheme to design high-affinity peptide mutants while uncovering novel structural motifs. CreoPep employs an integrative augmentation pipeline, combining FoldX-based energy screening with temperature-controlled multinomial sampling, to generate structurally and functionally diverse peptides that retain key pharmacological properties. We validate this approach by designing conotoxin inhibitors targeting the $\alpha$7 nicotinic acetylcholine receptor, achieving submicromolar potency in electrophysiological assays. Structural analysis reveals that CreoPep-generated variants engage in both conserved and novel binding modes, including disulfide-deficient forms, thus expanding beyond conventional design paradigms. Overall, CreoPep offers a robust and generalizable platform that bridges computational peptide design with experimental validation, accelerating the discovery of next-generation peptide therapeutics.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models</title>
<link>https://arxiv.org/abs/2505.02931</link>
<guid>https://arxiv.org/abs/2505.02931</guid>
<content:encoded><![CDATA[
arXiv:2505.02931v1 Announce Type: cross 
Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to identify and fix errors in source code. Before the rise of LLM-based agents, a common strategy was to increase the number of generated patches, sometimes to the thousands, to achieve better repair results on benchmarks. More recently, self-iterative capabilities enabled LLMs to refine patches over multiple rounds guided by feedback. However, literature often focuses on many iterations and disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the generation of multiple outputs and multiple rounds of iteration, while imposing a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs - DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR task. We further fine-tune each model on an APR dataset with three sizes (1K, 30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning dataset, we can achieve improvements of up to 78% in the number of plausible patches generated, challenging prior studies that reported limited gains using Full Fine-Tuning. However, we find that exceeding certain thresholds leads to diminishing outcomes, likely due to overfitting. Moreover, we show that base models greatly benefit from creating patches in an iterative fashion rather than generating them all at once. In addition, the benefit of iterative strategies becomes more pronounced in complex benchmarks. Even fine-tuned models, while benefiting less from iterations, still gain advantages, particularly on complex benchmarks. The research underscores the need for balanced APR strategies that combine multi-output generation and iterative refinement.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach</title>
<link>https://arxiv.org/abs/2505.02952</link>
<guid>https://arxiv.org/abs/2505.02952</guid>
<content:encoded><![CDATA[
arXiv:2505.02952v1 Announce Type: cross 
Abstract: Generative AI systems have revolutionized human interaction by enabling natural language-based coding and problem solving. However, the inherent ambiguity of natural language often leads to imprecise instructions, forcing users to iteratively test, correct, and resubmit their prompts. We propose an iterative approach that systematically narrows down these ambiguities through a structured series of clarification questions and alternative solution proposals, illustrated with input/output examples as well. Once every uncertainty is resolved, a final, precise solution is generated. Evaluated on a diverse dataset spanning coding, data analysis, and creative writing, our method demonstrates superior accuracy, competitive resolution times, and higher user satisfaction compared to conventional one-shot solutions, which typically require multiple manual iterations to achieve a correct output.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Single-Sample and Robust Online Resource Allocation</title>
<link>https://arxiv.org/abs/2505.02963</link>
<guid>https://arxiv.org/abs/2505.02963</guid>
<content:encoded><![CDATA[
arXiv:2505.02963v1 Announce Type: cross 
Abstract: Online Resource Allocation problem is a central problem in many areas of Computer Science, Operations Research, and Economics. In this problem, we sequentially receive $n$ stochastic requests for $m$ kinds of shared resources, where each request can be satisfied in multiple ways, consuming different amounts of resources and generating different values. The goal is to achieve a $(1-\epsilon)$-approximation to the hindsight optimum, where $\epsilon>0$ is a small constant, assuming each resource has a large budget.
  In this paper, we investigate the learnability and robustness of online resource allocation. Our primary contribution is a novel Exponential Pricing algorithm with the following properties: 1. It requires only a \emph{single sample} from each of the $n$ request distributions to achieve a $(1-\epsilon)$-approximation for online resource allocation with large budgets. Such an algorithm was previously unknown, even with access to polynomially many samples, as prior work either assumed full distributional knowledge or was limited to i.i.d.\,or random-order arrivals. 2. It is robust to corruptions in the outliers model and the value augmentation model. Specifically, it maintains its $(1 - \epsilon)$-approximation guarantee under both these robustness models, resolving the open question posed in Argue, Gupta, Molinaro, and Singla (SODA'22). 3. It operates as a simple item-pricing algorithm that ensures incentive compatibility.
  The intuition behind our Exponential Pricing algorithm is that the price of a resource should adjust exponentially as it is overused or underused. It differs from conventional approaches that use an online learning algorithm for item pricing. This departure guarantees that the algorithm will never run out of any resource, but loses the usual no-regret properties of online learning algorithms, necessitating a new analytical approach.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds</title>
<link>https://arxiv.org/abs/2505.02972</link>
<guid>https://arxiv.org/abs/2505.02972</guid>
<content:encoded><![CDATA[
arXiv:2505.02972v1 Announce Type: cross 
Abstract: Multi-Task Learning (MTL) seeks to boost statistical power and learning efficiency by discovering structure shared across related tasks. State-of-the-art MTL representation methods, however, usually treat the latent representation matrix as a point in ordinary Euclidean space, ignoring its often non-Euclidean geometry, thus sacrificing robustness when tasks are heterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL framework that embeds the shared representation on its natural Riemannian manifold and optimizes it via explicit manifold operations. Each training cycle performs (i) a Riemannian gradient step that respects the intrinsic curvature of the search space, followed by (ii) an efficient polar retraction to remain on the manifold, guaranteeing geometric fidelity at every iteration. The procedure applies to a broad class of matrix-factorized MTL models and retains the same per-iteration cost as Euclidean baselines. Across a set of synthetic experiments with task heterogeneity and on a wearable-sensor activity-recognition benchmark, GeoERM consistently improves estimation accuracy, reduces negative transfer, and remains stable under adversarial label noise, outperforming leading MTL and single-task alternatives.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter estimation for land-surface models using machine learning libraries</title>
<link>https://arxiv.org/abs/2505.02979</link>
<guid>https://arxiv.org/abs/2505.02979</guid>
<content:encoded><![CDATA[
arXiv:2505.02979v1 Announce Type: cross 
Abstract: The Neural Networks for Partial Differential Equations (NN4PDEs) approach is used to determine the parameters of a simple land-surface model using PyTorch's backpropagation engine. In order to test the inverse model, a synthetic dataset is created by running the model in forward mode with known parameter values to create soil temperature time series that can be used as observations for the inverse model. We show that it is not possible to obtain a reliable parameter estimation using a single observed soil temperature time series. Using measurements at two depths, reliable parameter estimates can be obtained although it is not possible to differentiate between latent and sensible heat fluxes. We apply the inverse model to urban flux tower data in Phoenix, United States, and show that the thermal conductivity, volumetric heat capacity, and the combined sensible-latent heat transfer coefficient can be reliably estimated using an observed value for the effective surface albedo. The resulting model accurately predicts the outgoing longwave radiation, conductive soil fluxes and the combined sensible-latent heat fluxes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New affine invariant ensemble samplers and their dimensional scaling</title>
<link>https://arxiv.org/abs/2505.02987</link>
<guid>https://arxiv.org/abs/2505.02987</guid>
<content:encoded><![CDATA[
arXiv:2505.02987v1 Announce Type: cross 
Abstract: We introduce some new affine invariant ensemble samplers that are easy to construct and improve upon existing widely used algorithms, especially for high-dimensional problems. Specifically, we propose a derivative-free ensemble side move sampler that performs favorably compared to popular samplers in the \texttt{emcee} package. Additionally, we develop a class of derivative-based ensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which outperform standard HMC without affine invariance when sampling highly skewed distributions. We provide asymptotic scaling analysis for high-dimensional Gaussian targets to further elucidate the properties of these affine invariant ensemble samplers. In particular, with derivative information, the affine invariant ensemble HMC can scale much better with dimension compared to derivative-free ensemble samplers.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale</title>
<link>https://arxiv.org/abs/2505.03005</link>
<guid>https://arxiv.org/abs/2505.03005</guid>
<content:encoded><![CDATA[
arXiv:2505.03005v1 Announce Type: cross 
Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.
  Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2505.03034</link>
<guid>https://arxiv.org/abs/2505.03034</guid>
<content:encoded><![CDATA[
arXiv:2505.03034v1 Announce Type: cross 
Abstract: Data derived from remote sensing or numerical simulations often have a regular gridded structure and are large in volume, making it challenging to find accurate spatial models that can fill in missing grid cells or simulate the process effectively, especially in the presence of spatial heterogeneity and heavy-tailed marginal distributions. To overcome this issue, we present a spatial autoregressive modeling framework, which maps observations at a location and its neighbors to independent random variables. This is a highly flexible modeling approach and well-suited for non-Gaussian fields, providing simpler interpretability. In particular, we consider the SAR model with Generalized Extreme Value distribution innovations to combine the observation at a central grid location with its neighbors, capturing extreme spatial behavior based on the heavy-tailed innovations. While these models are fast to simulate by exploiting the sparsity of the key matrices in the computations, the maximum likelihood estimation of the parameters is prohibitive due to the intractability of the likelihood, making optimization challenging. To overcome this, we train a convolutional neural network on a large training set that covers a useful parameter space, and then use the trained network for fast parameter estimation. Finally, we apply this model to analyze annual maximum precipitation data from ERA-Interim-driven Weather Research and Forecasting (WRF) simulations, allowing us to explore its spatial extreme behavior across North America.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Models to Understand (but not Generate) High-risk Data</title>
<link>https://arxiv.org/abs/2505.03052</link>
<guid>https://arxiv.org/abs/2505.03052</guid>
<content:encoded><![CDATA[
arXiv:2505.03052v1 Announce Type: cross 
Abstract: Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses</title>
<link>https://arxiv.org/abs/2505.03069</link>
<guid>https://arxiv.org/abs/2505.03069</guid>
<content:encoded><![CDATA[
arXiv:2505.03069v1 Announce Type: cross 
Abstract: We study the invertibility of nonlinear dynamical systems from the perspective of contraction and incremental stability analysis and propose a new invertible recurrent neural model: the BiLipREN. In particular, we consider a nonlinear state space model to be robustly invertible if an inverse exists with a state space realisation, and both the forward model and its inverse are contracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have bounded incremental gain. This property of bi-Lipschitzness implies both robustness in the sense of sensitivity to input perturbations, as well as robust distinguishability of different inputs from their corresponding outputs, i.e. the inverse model robustly reconstructs the input sequence despite small perturbations to the initial conditions and measured output. Building on this foundation, we propose a parameterization of neural dynamic models: bi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly invertible by construction. Moreover, biLipRENs can be composed with orthogonal linear systems to construct more general bi-Lipschitz dynamic models, e.g., a nonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We illustrate the utility of our proposed approach with numerical examples.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Adaptive Planner for Dynamic Manipulation</title>
<link>https://arxiv.org/abs/2505.03077</link>
<guid>https://arxiv.org/abs/2505.03077</guid>
<content:encoded><![CDATA[
arXiv:2505.03077v1 Announce Type: cross 
Abstract: This paper presents Latent Adaptive Planner (LAP), a novel approach for dynamic nonprehensile manipulation tasks that formulates planning as latent space inference, effectively learned from human demonstration videos. Our method addresses key challenges in visuomotor policy learning through a principled variational replanning framework that maintains temporal consistency while efficiently adapting to environmental changes. LAP employs Bayesian updating in latent space to incrementally refine plans as new observations become available, striking an optimal balance between computational efficiency and real-time adaptability. We bridge the embodiment gap between humans and robots through model-based proportional mapping that regenerates accurate kinematic-dynamic joint states and object positions from human demonstrations. Experimental evaluations across multiple complex manipulation benchmarks demonstrate that LAP achieves state-of-the-art performance, outperforming existing approaches in success rate, trajectory smoothness, and energy efficiency, particularly in dynamic adaptation scenarios. Our approach enables robots to perform complex interactions with human-like adaptability while providing an expandable framework applicable to diverse robotic platforms using the same human demonstration videos.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems</title>
<link>https://arxiv.org/abs/2505.03120</link>
<guid>https://arxiv.org/abs/2505.03120</guid>
<content:encoded><![CDATA[
arXiv:2505.03120v1 Announce Type: cross 
Abstract: Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable to adversarial attacks. It is crucial for an IDS to learn to recognize adversarial examples before malicious entities exploit them. In this paper, we generated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We validate the generalization and scalability of the adversarial samples to tackle a broad range of real attacks on Industrial Control Systems (ICS). We evaluated the impact by assessing multiple attacks generated using the proposed method. The model trained with adversarial samples detected attacks with 95% accuracy on real-world attack data not used during training. The study was conducted using an operational secure water treatment (SWaT) testbed.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control</title>
<link>https://arxiv.org/abs/2505.03134</link>
<guid>https://arxiv.org/abs/2505.03134</guid>
<content:encoded><![CDATA[
arXiv:2505.03134v1 Announce Type: cross 
Abstract: Visual defect detection in industrial glass manufacturing remains a critical challenge due to the low frequency of defective products, leading to imbalanced datasets that limit the performance of deep learning models and computer vision systems. This paper presents a novel approach using Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic defective glass product images for data augmentation, effectively addressing class imbalance issues in manufacturing quality control and automated visual inspection. The methodology significantly enhances image classification performance of standard CNN architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting anomalies by increasing the minority class representation. Experimental results demonstrate substantial improvements in key machine learning metrics, particularly in recall for defective samples across all tested deep neural network architectures while maintaining perfect precision. The most dramatic improvement was observed in ResNet50V2's overall classification accuracy, which increased from 78 percent to 93 percent when trained with the augmented data. This work provides a scalable, cost-effective approach to enhancing automated defect detection in glass manufacturing that can potentially be extended to other industrial quality assurance systems and industries with similar class imbalance challenges.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems</title>
<link>https://arxiv.org/abs/2505.03140</link>
<guid>https://arxiv.org/abs/2505.03140</guid>
<content:encoded><![CDATA[
arXiv:2505.03140v1 Announce Type: cross 
Abstract: Quantum machine learning for spin and molecular systems faces critical challenges of scarce labeled data and computationally expensive simulations. To address these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE), a novel self-supervised framework that pre-trains transformers on unlabeled quantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike random masking approaches, HMAE employs a physics-informed strategy based on quantum information theory to selectively mask Hamiltonian terms based on their physical significance. Experiments on 12,500 quantum Hamiltonians (60% real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5% accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground state energy prediction with merely 10 labeled examples - a statistically significant improvement (p < 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%) and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantage is exceptional sample efficiency - reducing required labeled examples by 3-5x compared to baseline methods - though we emphasize that ground truth values for fine-tuning and evaluation still require exact diagonalization or tensor networks. We explicitly acknowledge that our current approach is limited to small quantum systems (specifically limited to 12 qubits during training, with limited extension to 16-20 qubits in testing) and that, while promising within this regime, this size restriction prevents immediate application to larger systems of practical interest in materials science and quantum chemistry.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization</title>
<link>https://arxiv.org/abs/2505.03146</link>
<guid>https://arxiv.org/abs/2505.03146</guid>
<content:encoded><![CDATA[
arXiv:2505.03146v1 Announce Type: cross 
Abstract: This paper presents a Long Short-Term Memory network-based Fluid Experiment Data-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic forces on the underwater quadruped robot we constructed. Trained on experimental data from leg force and body drag tests conducted in both a recirculating water tank and a towing tank, FED-LSTM outperforms traditional Empirical Formulas (EF) commonly used for flow prediction over flat surfaces. The model demonstrates superior accuracy and adaptability in capturing complex fluid dynamics, particularly in straight-line and turning-gait optimizations via the NSGA-II algorithm. FED-LSTM reduces deflection errors during straight-line swimming and improves turn times without increasing the turning radius. Hardware experiments further validate the model's precision and stability over EF. This approach provides a robust framework for enhancing the swimming performance of legged robots, laying the groundwork for future advances in underwater robotic locomotion.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots</title>
<link>https://arxiv.org/abs/2505.03159</link>
<guid>https://arxiv.org/abs/2505.03159</guid>
<content:encoded><![CDATA[
arXiv:2505.03159v1 Announce Type: cross 
Abstract: PID controllers are widely used in control systems because of their simplicity and effectiveness. Although advanced optimization techniques such as Bayesian Optimization and Differential Evolution have been applied to address the challenges of automatic tuning of PID controllers, the influence of initial system states on convergence and the balance between exploration and exploitation remains underexplored. Moreover, experimenting the influence directly on real cyber-physical systems such as mobile robots is crucial for deriving realistic insights. In the present paper, a novel framework is introduced to evaluate the impact of systematically varying these factors on the PID auto-tuning processes that utilize Bayesian Optimization and Differential Evolution. Testing was conducted on two distinct PID-controlled robotic platforms, an omnidirectional robot and a differential drive mobile robot, to assess the effects on convergence rate, settling time, rise time, and overshoot percentage. As a result, the experimental outcomes yield evidence on the effects of the systematic variations, thereby providing an empirical basis for future research studies in the field.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Data Curation Using GPS &amp; NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets</title>
<link>https://arxiv.org/abs/2505.03174</link>
<guid>https://arxiv.org/abs/2505.03174</guid>
<content:encoded><![CDATA[
arXiv:2505.03174v1 Announce Type: cross 
Abstract: Instruction-Action (IA) data pairs are valuable for training robotic systems, especially autonomous vehicles (AVs), but having humans manually annotate this data is costly and time-inefficient. This paper explores the potential of using mobile application Global Positioning System (GPS) references and Natural Language Processing (NLP) to automatically generate large volumes of IA commands and responses without having a human generate or retroactively tag the data. In our pilot data collection, by driving to various destinations and collecting voice instructions from GPS applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. We provide details on our completely automated data collection prototype system, ADVLAT-Engine. We characterize collected GPS voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. Through research and exploration into the automation of IA data pairs using GPS references, the potential to increase the speed and volume at which high-quality IA datasets are created, while minimizing cost, can pave the way for robust vision-language-action (VLA) models to serve tasks in vision-language navigation (VLN) and human-interactive autonomous systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models</title>
<link>https://arxiv.org/abs/2505.03176</link>
<guid>https://arxiv.org/abs/2505.03176</guid>
<content:encoded><![CDATA[
arXiv:2505.03176v1 Announce Type: cross 
Abstract: Current self-supervised algorithms mostly rely on transformations such as data augmentation and masking to learn visual representations. This is achieved by inducing invariance or equivariance with respect to these transformations after encoding two views of an image. This dominant two-view paradigm can limit the flexibility of learned representations for downstream adaptation by creating performance trade-offs between invariance-related tasks such as image classification and more fine-grained equivariance-related tasks. In this work, we introduce \emph{seq-JEPA}, a world modeling paradigm based on joint-embedding predictive architecture that leverages architectural inductive biases to resolve this trade-off. Without requiring an additional equivariance predictor or loss term, seq-JEPA simultaneously learns two architecturally segregated representations: one equivariant to the specified transformations and another invariant to them and suited for tasks such as classification. To do so, our model processes a short sequence of different views (observations) of an input image. Each encoded view is concatenated with embeddings corresponding to the relative transformation (action) producing the next observation in the sequence. A transformer encoder outputs an aggregate representation of this sequence, which is subsequently conditioned on the action leading to the next observation to predict its representation. Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and image classification without sacrificing one for the other. Additionally, our framework excels at tasks that inherently require aggregating a sequence of observations, such as path integration across actions and predictive learning across eye movements.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Symbolic and Statistical Learning Framework to Discover Bioprocessing Regulatory Mechanism: Cell Culture Example</title>
<link>https://arxiv.org/abs/2505.03177</link>
<guid>https://arxiv.org/abs/2505.03177</guid>
<content:encoded><![CDATA[
arXiv:2505.03177v1 Announce Type: cross 
Abstract: Bioprocess mechanistic modeling is essential for advancing intelligent digital twin representation of biomanufacturing, yet challenges persist due to complex intracellular regulation, stochastic system behavior, and limited experimental data. This paper introduces a symbolic and statistical learning framework to identify key regulatory mechanisms and quantify model uncertainty. Bioprocess dynamics is formulated with stochastic differential equations characterizing intrinsic process variability, with a predefined set of candidate regulatory mechanisms constructed from biological knowledge. A Bayesian learning approach is developed, which is based on a joint learning of kinetic parameters and regulatory structure through a formulation of the mixture model. To enhance computational efficiency, a Metropolis-adjusted Langevin algorithm with adjoint sensitivity analysis is developed for posterior exploration. Compared to state-of-the-art Bayesian inference approaches, the proposed framework achieves improved sample efficiency and robust model selection. An empirical study demonstrates its ability to recover missing regulatory mechanisms and improve model fidelity under data-limited conditions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weighted Average Gradients for Feature Attribution</title>
<link>https://arxiv.org/abs/2505.03201</link>
<guid>https://arxiv.org/abs/2505.03201</guid>
<content:encoded><![CDATA[
arXiv:2505.03201v1 Announce Type: cross 
Abstract: In explainable AI, Integrated Gradients (IG) is a widely adopted technique for assessing the significance of feature attributes of the input on model outputs by evaluating contributions from a baseline input to the current input. The choice of the baseline input significantly influences the resulting explanation. While the traditional Expected Gradients (EG) method assumes baselines can be uniformly sampled and averaged with equal weights, this study argues that baselines should not be treated equivalently. We introduce Weighted Average Gradients (WG), a novel approach that unsupervisedly evaluates baseline suitability and incorporates a strategy for selecting effective baselines. Theoretical analysis demonstrates that WG satisfies essential explanation method criteria and offers greater stability than prior approaches. Experimental results further confirm that WG outperforms EG across diverse scenarios, achieving an improvement of 10-35\% on main metrics. Moreover, by evaluating baselines, our method can filter a subset of effective baselines for each input to calculate explanations, maintaining high accuracy while reducing computational cost. The code is available at: https://github.com/Tamnt240904/weighted_baseline.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lower Bounds for Greedy Teaching Set Constructions</title>
<link>https://arxiv.org/abs/2505.03223</link>
<guid>https://arxiv.org/abs/2505.03223</guid>
<content:encoded><![CDATA[
arXiv:2505.03223v1 Announce Type: cross 
Abstract: A fundamental open problem in learning theory is to characterize the best-case teaching dimension $\operatorname{TS}_{\min}$ of a concept class $\mathcal{C}$ with finite VC dimension $d$. Resolving this problem will, in particular, settle the conjectured upper bound on Recursive Teaching Dimension posed by [Simon and Zilles; COLT 2015]. Prior work used a natural greedy algorithm to construct teaching sets recursively, thereby proving upper bounds on $\operatorname{TS}_{\min}$, with the best known bound being $O(d^2)$ [Hu, Wu, Li, and Wang; COLT 2017]. In each iteration, this greedy algorithm chooses to add to the teaching set the $k$ labeled points that restrict the concept class the most. In this work, we prove lower bounds on the performance of this greedy approach for small $k$. Specifically, we show that for $k = 1$, the algorithm does not improve upon the halving-based bound of $O(\log(|\mathcal{C}|))$. Furthermore, for $k = 2$, we complement the upper bound of $O\left(\log(\log(|\mathcal{C}|))\right)$ from [Moran, Shpilka, Wigderson, and Yuhudayoff; FOCS 2015] with a matching lower bound. Most consequentially, our lower bound extends up to $k \le \lceil c d \rceil$ for small constant $c>0$: suggesting that studying higher-order interactions may be necessary to resolve the conjecture that $\operatorname{TS}_{\min} = O(d)$.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs</title>
<link>https://arxiv.org/abs/2505.03254</link>
<guid>https://arxiv.org/abs/2505.03254</guid>
<content:encoded><![CDATA[
arXiv:2505.03254v1 Announce Type: cross 
Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on resource-constrained devices. Quantization effectively compresses these models, reducing storage size and energy cost. However, in modern depthwise-separable architectures, the computational cost is distributed unevenly across its components, with pointwise operations being the most expensive. By applying a general quantization scheme to this imbalanced cost distribution, existing quantization approaches fail to fully exploit potential efficiency gains. To this end, we introduce PROM, a straightforward approach for quantizing modern depthwise-separable convolutional networks by selectively using two distinct bit-widths. Specifically, pointwise convolutions are quantized to ternary weights, while the remaining modules use 8-bit weights, which is achieved through a simple quantization-aware training procedure. Additionally, by quantizing activations to 8-bit, our method transforms pointwise convolutions with ternary weights into int8 additions, which enjoy broad support across hardware platforms and effectively eliminates the need for expensive multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost by more than an order of magnitude (23.9x) and its storage size by 2.7x compared to the float16 baseline while retaining similar classification performance on ImageNet. Our method advances the Pareto frontier for energy consumption vs. top-1 accuracy for quantized convolutional models on ImageNet. PROM addresses the challenges of quantizing depthwise-separable convolutional networks to both ternary and 8-bit weights, offering a simple way to reduce energy cost and storage size.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning</title>
<link>https://arxiv.org/abs/2505.03296</link>
<guid>https://arxiv.org/abs/2505.03296</guid>
<content:encoded><![CDATA[
arXiv:2505.03296v1 Announce Type: cross 
Abstract: We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel approach for flexible policy representation and imitation learning in robot manipulation. MiDiGap enables learning from as few as five demonstrations using only camera observations and generalizes across a wide range of challenging tasks. It excels at long-horizon behaviors such as making coffee, highly constrained motions such as opening doors, dynamic actions such as scooping with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns these tasks on a CPU in less than a minute and scales linearly to large datasets. We also develop a rich suite of tools for inference-time steering using evidence such as collision signals and robot kinematic constraints. This steering enables novel generalization capabilities, including obstacle avoidance and cross-embodiment policy transfer. MiDiGap achieves state-of-the-art performance on diverse few-shot manipulation benchmarks. On constrained RLBench tasks, it improves policy success by 76 percentage points and reduces trajectory cost by 67%. On multimodal tasks, it improves policy success by 48 percentage points and increases sample efficiency by a factor of 20. In cross-embodiment transfer, it more than doubles policy success. We make the code publicly available at https://midigap.cs.uni-freiburg.de.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Active Inference perspective on Neurofeedback Training</title>
<link>https://arxiv.org/abs/2505.03308</link>
<guid>https://arxiv.org/abs/2505.03308</guid>
<content:encoded><![CDATA[
arXiv:2505.03308v1 Announce Type: cross 
Abstract: Neurofeedback training (NFT) aims to teach self-regulation of brain activity through real-time feedback, but suffers from highly variable outcomes and poorly understood mechanisms, hampering its validation. To address these issues, we propose a formal computational model of the NFT closed loop. Using Active Inference, a Bayesian framework modelling perception, action, and learning, we simulate agents interacting with an NFT environment. This enables us to test the impact of design choices (e.g., feedback quality, biomarker validity) and subject factors (e.g., prior beliefs) on training. Simulations show that training effectiveness is sensitive to feedback noise or bias, and to prior beliefs (highlighting the importance of guiding instructions), but also reveal that perfect feedback is insufficient to guarantee high performance. This approach provides a tool for assessing and predicting NFT variability, interpret empirical data, and potentially develop personalized training protocols.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.03327</link>
<guid>https://arxiv.org/abs/2505.03327</guid>
<content:encoded><![CDATA[
arXiv:2505.03327v1 Announce Type: cross 
Abstract: Deep learning models have shown encouraging capabilities for mapping accurately forests at medium resolution with TanDEM-X interferometric SAR data. Such models, as most of current state-of-the-art deep learning techniques in remote sensing, are trained in a fully-supervised way, which requires a large amount of labeled data for training and validation. In this work, our aim is to exploit the high-resolution capabilities of the TanDEM-X mission to map forests at 6 m. The goal is to overcome the intrinsic limitations posed by midresolution products, which affect, e.g., the detection of narrow roads within vegetated areas and the precise delineation of forested regions contours. To cope with the lack of extended reliable reference datasets at such a high resolution, we investigate self-supervised learning techniques for extracting highly informative representations from the input features, followed by a supervised training step with a significantly smaller number of reliable labels. A 1 m resolution forest/non-forest reference map over Pennsylvania, USA, allows for comparing different training approaches for the development of an effective forest mapping framework with limited labeled samples. We select the best-performing approach over this test region and apply it in a real-case forest mapping scenario over the Amazon rainforest, where only very few labeled data at high resolution are available. In this challenging scenario, the proposed self-supervised framework significantly enhances the classification accuracy with respect to fully-supervised methods, trained using the same amount of labeled data, representing an extremely promising starting point for large-scale, very high-resolution forest mapping with TanDEM-X data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation</title>
<link>https://arxiv.org/abs/2505.03344</link>
<guid>https://arxiv.org/abs/2505.03344</guid>
<content:encoded><![CDATA[
arXiv:2505.03344v1 Announce Type: cross 
Abstract: Achieving both realism and controllability in interactive closed-loop traffic simulation remains a key challenge in autonomous driving. Data-driven simulation methods reproduce realistic trajectories but suffer from covariate shift in closed-loop deployment, compounded by simplified dynamics models that further reduce reliability. Conversely, physics-based simulation methods enhance reliable and controllable closed-loop interactions but often lack expert demonstrations, compromising realism. To address these challenges, we introduce a dual-stage AV-centered simulation framework that conducts open-loop imitation learning pre-training in a data-driven simulator to capture trajectory-level realism and multimodality, followed by closed-loop reinforcement learning fine-tuning in a physics-based simulator to enhance controllability and mitigate covariate shift. In the fine-tuning stage, we propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that preserves the trajectory-level multimodality through a GRPO-style group-relative advantage formulation, while enhancing controllability and training stability by replacing KL regularization with the dual-clip mechanism. Extensive experiments demonstrate that RIFT significantly improves the realism and controllability of generated traffic scenarios, providing a robust platform for evaluating autonomous vehicle performance in diverse and interactive scenarios.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solar Flare Forecast: A Comparative Analysis of Machine Learning Algorithms for Solar Flare Class Prediction</title>
<link>https://arxiv.org/abs/2505.03385</link>
<guid>https://arxiv.org/abs/2505.03385</guid>
<content:encoded><![CDATA[
arXiv:2505.03385v1 Announce Type: cross 
Abstract: Solar flares are among the most powerful and dynamic events in the solar system, resulting from the sudden release of magnetic energy stored in the Sun's atmosphere. These energetic bursts of electromagnetic radiation can release up to 10^32 erg of energy, impacting space weather and posing risks to technological infrastructure and therefore require accurate forecasting of solar flare occurrences and intensities. This study evaluates the predictive performance of three machine learning algorithms: Random Forest, k-Nearest Neighbors (KNN), and Extreme Gradient Boosting (XGBoost) for classifying solar flares into 4 categories (B, C, M, X). Using the dataset of 13 SHARP parameters, the effectiveness of the models was evaluated in binary and multiclass classification tasks. The analysis utilized 8 principal components (PC), capturing 95% of data variance, and 100 PCs, capturing 97.5% of variance. Our approach uniquely combines binary and multiclass classification with different levels of dimensionality reduction, an innovative methodology not previously explored in the context of solar flare prediction. Employing a 10-fold stratified cross-validation and grid search for hyperparameter tuning ensured robust model evaluation. Our findings indicate that Random Forest and XGBoost consistently demonstrate strong performance across all metrics, benefiting significantly from increased dimensionality. The insights of this study enhance future research by optimizing dimensionality reduction techniques and informing model selection for astrophysical tasks. By integrating this newly acquired knowledge into future research, more accurate space weather forecasting systems can be developed, along with a deeper understanding of solar physics.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath</title>
<link>https://arxiv.org/abs/2505.03397</link>
<guid>https://arxiv.org/abs/2505.03397</guid>
<content:encoded><![CDATA[
arXiv:2505.03397v1 Announce Type: cross 
Abstract: Qubit control protocols have traditionally leveraged a characterisation of the qubit-bath coupling via its power spectral density. Previous work proposed the inference of noise operators that characterise the influence of a classical bath using a grey-box approach that combines deep neural networks with physics-encoded layers. This overall structure is complex and poses challenges in scaling and real-time operations. Here, we show that no expensive neural networks are needed and that this noise operator description admits an efficient parameterisation. We refer to the resulting parameter space as the \textit{quantum feature space} of the qubit dynamics resulting from the coupled bath. We show that the Euclidean distance defined over the quantum feature space provides an effective method for classifying noise processes in the presence of a given set of controls. Using the quantum feature space as the input space for a simple machine learning algorithm (random forest, in this case), we demonstrate that it can effectively classify the stationarity and the broad class of noise processes perturbing a qubit. Finally, we explore how control pulse parameters map to the quantum feature space.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents</title>
<link>https://arxiv.org/abs/2505.03434</link>
<guid>https://arxiv.org/abs/2505.03434</guid>
<content:encoded><![CDATA[
arXiv:2505.03434v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) represent a landmark achievement in Artificial Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks such as text generation, code completion, and conversational coherence. These capabilities stem from their architecture, which mirrors human procedural memory -- the brain's ability to automate repetitive, pattern-driven tasks through practice. However, as LLMs are increasingly deployed in real-world applications, it becomes impossible to ignore their limitations operating in complex, unpredictable environments. This paper argues that LLMs, while transformative, are fundamentally constrained by their reliance on procedural memory. To create agents capable of navigating ``wicked'' learning environments -- where rules shift, feedback is ambiguous, and novelty is the norm -- we must augment LLMs with semantic memory and associative learning systems. By adopting a modular architecture that decouples these cognitive functions, we can bridge the gap between narrow procedural expertise and the adaptive intelligence required for real-world problem-solving.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Steganographic Potentials of Language Models</title>
<link>https://arxiv.org/abs/2505.03439</link>
<guid>https://arxiv.org/abs/2505.03439</guid>
<content:encoded><![CDATA[
arXiv:2505.03439v1 Announce Type: cross 
Abstract: The potential for large language models (LLMs) to hide messages within plain text (steganography) poses a challenge to detection and thwarting of unaligned AI agents, and undermines faithfulness of LLMs reasoning. We explore the steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL) to: (1) develop covert encoding schemes, (2) engage in steganography when prompted, and (3) utilize steganography in realistic scenarios where hidden reasoning is likely, but not prompted. In these scenarios, we detect the intention of LLMs to hide their reasoning as well as their steganography performance. Our findings in the fine-tuning experiments as well as in behavioral non fine-tuning evaluations reveal that while current models exhibit rudimentary steganographic abilities in terms of security and capacity, explicit algorithmic guidance markedly enhances their capacity for information concealment.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance</title>
<link>https://arxiv.org/abs/2505.03442</link>
<guid>https://arxiv.org/abs/2505.03442</guid>
<content:encoded><![CDATA[
arXiv:2505.03442v1 Announce Type: cross 
Abstract: Speech denoising is a generally adopted and impactful task, appearing in many common and everyday-life use cases. Although there are very powerful methods published, most of those are too complex for deployment in everyday and low-resources computational environments, like hand-held devices, intelligent glasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for alleviating this complexity mismatch and is based on the transferring/distilling of knowledge from a pre-trained complex model, the teacher, to another less complex one, the student. Existing KD methods for speech denoising are based on processes that potentially hamper the KD by bounding the learning of the student to the distribution, information ordering, and feature dimensionality learned by the teacher. In this paper, we present and assess a method that tries to treat this issue, by exploiting the well-known denoising-autoencoder framework, the linear inverted bottlenecks, and the properties of the cosine similarity. We use a public dataset and conduct repeated experiments with different mismatching scenarios between the teacher and the student, reporting the mean and standard deviation of the metrics of our method and another, state-of-the-art method that is used as a baseline. Our results show that with the proposed method, the student can perform better and can also retain greater mismatching conditions compared to the teacher.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.03452</link>
<guid>https://arxiv.org/abs/2505.03452</guid>
<content:encoded><![CDATA[
arXiv:2505.03452v1 Announce Type: cross 
Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a given use case can be complex and expensive. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To address this gap, we present a comprehensive study involving 5 HPO algorithms over 5 datasets from diverse domains, including a new one collected for this work on real-world product documentation. Our study explores the largest HPO search space considered to date, with two optimized evaluation metrics. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with iterative random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing models first is preferable to the prevalent practice of optimizing sequentially according to the RAG pipeline order.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending 3D Geometry and Machine Learning for Multi-View Stereopsis</title>
<link>https://arxiv.org/abs/2505.03470</link>
<guid>https://arxiv.org/abs/2505.03470</guid>
<content:encoded><![CDATA[
arXiv:2505.03470v1 Announce Type: cross 
Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric and geometric consistency constraints. In contrast, modern learning-based algorithms often rely on the plane sweep algorithm to infer 3D geometry, applying explicit geometric consistency (GC) checks only as a post-processing step, with no impact on the learning process itself. In this work, we introduce GC MVSNet plus plus, a novel approach that actively enforces geometric consistency of reference view depth maps across multiple source views (multi view) and at various scales (multi scale) during the learning phase (see Fig. 1). This integrated GC check significantly accelerates the learning process by directly penalizing geometrically inconsistent pixels, effectively halving the number of training iterations compared to other MVS methods. Furthermore, we introduce a densely connected cost regularization network with two distinct block designs simple and feature dense optimized to harness dense feature connections for enhanced regularization. Extensive experiments demonstrate that our approach achieves a new state of the art on the DTU and BlendedMVS datasets and secures second place on the Tanks and Temples benchmark. To our knowledge, GC MVSNet plus plus is the first method to enforce multi-view, multi-scale supervised geometric consistency during learning. Our code is available.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>am-ELO: A Stable Framework for Arena-based LLM Evaluation</title>
<link>https://arxiv.org/abs/2505.03475</link>
<guid>https://arxiv.org/abs/2505.03475</guid>
<content:encoded><![CDATA[
arXiv:2505.03475v1 Announce Type: cross 
Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm for modern AI models, especially large language models (LLMs). Existing framework based on ELO rating system suffers from the inevitable instability problem due to ranking inconsistency and the lack of attention to the varying abilities of annotators. In this paper, we introduce a novel stable arena framework to address these issues by enhancing the ELO Rating System. Specifically, we replace the iterative update method with a Maximum Likelihood Estimation (MLE) approach, m-ELO, and provide theoretical proof of the consistency and stability of the MLE approach for model ranking. Additionally, we proposed the am-ELO, which modify the Elo Rating's probability function to incorporate annotator abilities, enabling the simultaneous estimation of model scores and annotator reliability. Experiments demonstrate that this method ensures stability, proving that this framework offers a more robust, accurate, and stable evaluation method for LLMs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Musical Genre Trajectories through Pathlet Learning</title>
<link>https://arxiv.org/abs/2505.03480</link>
<guid>https://arxiv.org/abs/2505.03480</guid>
<content:encoded><![CDATA[
arXiv:2505.03480v1 Announce Type: cross 
Abstract: The increasing availability of user data on music streaming platforms opens up new possibilities for analyzing music consumption. However, understanding the evolution of user preferences remains a complex challenge, particularly as their musical tastes change over time. This paper uses the dictionary learning paradigm to model user trajectories across different musical genres. We define a new framework that captures recurring patterns in genre trajectories, called pathlets, enabling the creation of comprehensible trajectory embeddings. We show that pathlet learning reveals relevant listening patterns that can be analyzed both qualitatively and quantitatively. This work improves our understanding of users' interactions with music and opens up avenues of research into user behavior and fostering diversity in recommender systems. A dataset of 2000 user histories tagged by genre over 17 months, supplied by Deezer (a leading music streaming company), is also released with the code.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster MoE LLM Inference for Extremely Large Models</title>
<link>https://arxiv.org/abs/2505.03531</link>
<guid>https://arxiv.org/abs/2505.03531</guid>
<content:encoded><![CDATA[
arXiv:2505.03531v1 Announce Type: cross 
Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually becoming the mainstream approach for ultra-large-scale models. Existing optimization efforts for MoE models have focused primarily on coarse-grained MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE models are gaining popularity, yet research on them remains limited. Therefore, we want to discuss the efficiency dynamic under different service loads. Additionally, fine-grained models allow deployers to reduce the number of routed experts, both activated counts and total counts, raising the question of how this reduction affects the trade-off between MoE efficiency and performance. Our findings indicate that while deploying MoE models presents greater challenges, it also offers significant optimization opportunities. Reducing the number of activated experts can lead to substantial efficiency improvements in certain scenarios, with only minor performance degradation. Reducing the total number of experts provides limited efficiency gains but results in severe performance degradation. Our method can increase throughput by at least 10\% without any performance degradation. Overall, we conclude that MoE inference optimization remains an area with substantial potential for exploration and improvement.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets</title>
<link>https://arxiv.org/abs/2505.03585</link>
<guid>https://arxiv.org/abs/2505.03585</guid>
<content:encoded><![CDATA[
arXiv:2505.03585v1 Announce Type: cross 
Abstract: Distributionally Robust Optimisation (DRO) protects risk-averse decision-makers by considering the worst-case risk within an ambiguity set of distributions based on the empirical distribution or a model. To further guard against finite, noisy data, model-based approaches admit Bayesian formulations that propagate uncertainty from the posterior to the decision-making problem. However, when the model is misspecified, the decision maker must stretch the ambiguity set to contain the data-generating process (DGP), leading to overly conservative decisions. We address this challenge by introducing DRO with Robust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These are Maximum Mean Discrepancy ambiguity sets centred at a robust posterior predictive distribution that incorporates beliefs about the DGP. We show that the resulting optimisation problem obtains a dual formulation in the Reproducing Kernel Hilbert Space and we give probabilistic guarantees on the tolerance level of the ambiguity set. Our method outperforms other Bayesian and empirical DRO approaches in out-of-sample performance on the Newsvendor and Portfolio problems with various cases of model misspecification.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in Magnetic Resonance Spectroscopy</title>
<link>https://arxiv.org/abs/2505.03590</link>
<guid>https://arxiv.org/abs/2505.03590</guid>
<content:encoded><![CDATA[
arXiv:2505.03590v1 Announce Type: cross 
Abstract: Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure the metabolic composition of tissues, offering valuable insights into neurological disorders, tumor detection, and other metabolic dysfunctions. However, accurate metabolite quantification is hindered by challenges such as spectral overlap, low signal-to-noise ratio, and various artifacts. Traditional methods like linear-combination modeling are susceptible to ambiguities and commonly only provide a theoretical lower bound on estimation accuracy in the form of the Cram\'er-Rao bound. This work introduces a Bayesian inference framework using Sylvester normalizing flows (SNFs) to approximate posterior distributions over metabolite concentrations, enhancing quantification reliability. A physics-based decoder incorporates prior knowledge of MRS signal formation, ensuring realistic distribution representations. We validate the method on simulated 7T proton MRS data, demonstrating accurate metabolite quantification, well-calibrated uncertainties, and insights into parameter correlations and multi-modal distributions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Binding threshold units with artificial oscillatory neurons</title>
<link>https://arxiv.org/abs/2505.03648</link>
<guid>https://arxiv.org/abs/2505.03648</guid>
<content:encoded><![CDATA[
arXiv:2505.03648v1 Announce Type: cross 
Abstract: Artificial Kuramoto oscillatory neurons were recently introduced as an alternative to threshold units. Empirical evidence suggests that oscillatory units outperform threshold units in several tasks including unsupervised object discovery and certain reasoning problems. The proposed coupling mechanism for these oscillatory neurons is heterogeneous, combining a generalized Kuramoto equation with standard coupling methods used for threshold units. In this research note, we present a theoretical framework that clearly distinguishes oscillatory neurons from threshold units and establishes a coupling mechanism between them. We argue that, from a biological standpoint, oscillatory and threshold units realise distinct aspects of neural coding: roughly, threshold units model intensity of neuron firing, while oscillatory units facilitate information exchange by frequency modulation. To derive interaction between these two types of units, we constrain their dynamics by focusing on dynamical systems that admit Lyapunov functions. For threshold units, this leads to Hopfield associative memory model, and for oscillatory units it yields a specific form of generalized Kuramoto model. The resulting dynamical systems can be naturally coupled to form a Hopfield-Kuramoto associative memory model, which also admits a Lyapunov function. Various forms of coupling are possible. Notably, oscillatory neurons can be employed to implement a low-rank correction to the weight matrix of a Hopfield network. This correction can be viewed either as a form of Hebbian learning or as a popular LoRA method used for fine-tuning of large language models. We demonstrate the practical realization of this particular coupling through illustrative toy experiments.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weighted Random Dot Product Graphs</title>
<link>https://arxiv.org/abs/2505.03649</link>
<guid>https://arxiv.org/abs/2505.03649</guid>
<content:encoded><![CDATA[
arXiv:2505.03649v1 Announce Type: cross 
Abstract: Modeling of intricate relational patterns % through the analysis structures of network data has become a cornerstone of contemporary statistical research and related data science fields. Networks, represented as graphs, offer a natural framework for this analysis. This paper extends the Random Dot Product Graph (RDPG) model to accommodate weighted graphs, markedly broadening the model's scope to scenarios where edges exhibit heterogeneous weight distributions. We propose a nonparametric weighted (W)RDPG model that assigns a sequence of latent positions to each node. Inner products of these nodal vectors specify the moments of their incident edge weights' distribution via moment-generating functions. In this way, and unlike prior art, the WRDPG can discriminate between weight distributions that share the same mean but differ in other higher-order moments. We derive statistical guarantees for an estimator of the nodal's latent positions adapted from the workhorse adjacency spectral embedding, establishing its consistency and asymptotic normality. We also contribute a generative framework that enables sampling of graphs that adhere to a (prescribed or data-fitted) WRDPG, facilitating, e.g., the analysis and testing of observed graph metrics using judicious reference distributions. The paper is organized to formalize the model's definition, the estimation (or nodal embedding) process and its guarantees, as well as the methodologies for generating weighted graphs, all complemented by illustrative and reproducible examples showcasing the WRDPG's effectiveness in various network analytic applications.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vector valued optimal transport: from dynamic to static formulations</title>
<link>https://arxiv.org/abs/2505.03670</link>
<guid>https://arxiv.org/abs/2505.03670</guid>
<content:encoded><![CDATA[
arXiv:2505.03670v1 Announce Type: cross 
Abstract: Motivated by applications in classification of vector valued measures and multispecies PDE, we develop a theory that unifies existing notions of vector valued optimal transport, from dynamic formulations (\`a la Benamou-Brenier) to static formulations (\`a la Kantorovich). In our framework, vector valued measures are modeled as probability measures on a product space $\mathbb{R}^d \times G$, where $G$ is a weighted graph over a finite set of nodes and the graph geometry strongly influences the associated dynamic and static distances. We obtain sharp inequalities relating four notions of vector valued optimal transport and prove that the distances are mutually bi-H\"older equivalent. We discuss the theoretical and practical advantages of each metric and indicate potential applications in multispecies PDE and data analysis. In particular, one of the static formulations discussed in the paper is amenable to linearization, a technique that has been explored in recent years to accelerate the computation of pairwise optimal transport distances.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages</title>
<link>https://arxiv.org/abs/2505.03688</link>
<guid>https://arxiv.org/abs/2505.03688</guid>
<content:encoded><![CDATA[
arXiv:2505.03688v1 Announce Type: cross 
Abstract: The rapid progress in question-answering (QA) systems has predominantly benefited high-resource languages, leaving Indic languages largely underrepresented despite their vast native speaker base. In this paper, we present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset covering nine major Indic languages, systematically derived from the SQuAD dataset. Building on previous work with MahaSQuAD for Marathi, our approach adapts and extends translation techniques to maintain high linguistic fidelity and accurate answer-span alignment across diverse languages. IndicSQuAD comprises extensive training, validation, and test sets for each language, providing a robust foundation for model development. We evaluate baseline performances using language-specific monolingual BERT models and the multilingual MuRIL-BERT. The results indicate some challenges inherent in low-resource settings. Moreover, our experiments suggest potential directions for future work, including expanding to additional languages, developing domain-specific datasets, and incorporating multimodal data. The dataset and models are publicly shared at https://github.com/l3cube-pune/indic-nlp
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach</title>
<link>https://arxiv.org/abs/2505.03702</link>
<guid>https://arxiv.org/abs/2505.03702</guid>
<content:encoded><![CDATA[
arXiv:2505.03702v1 Announce Type: cross 
Abstract: Automating leaf manipulation in agricultural settings faces significant challenges, including the variability of plant morphologies and deformable leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf grasping that combines traditional computer vision with neural networks through self-supervised learning. Our method integrates YOLOv8 for instance segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf representations, which feed into both a geometric feature scoring pipeline and a neural refinement module (GraspPointCNN). The key innovation is our confidence-weighted fusion mechanism that dynamically balances the contribution of each approach based on prediction certainty. Our self-supervised framework uses the geometric pipeline as an expert teacher to automatically generate training data. Experiments demonstrate that our approach achieves an 88.0% success rate in controlled environments and 84.7% in real greenhouse conditions, significantly outperforming both purely geometric (75.3%) and neural (60.2%) methods. This work establishes a new paradigm for agricultural robotics where domain expertise is seamlessly integrated with machine learning capabilities, providing a foundation for fully automated crop monitoring systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning</title>
<link>https://arxiv.org/abs/2505.03703</link>
<guid>https://arxiv.org/abs/2505.03703</guid>
<content:encoded><![CDATA[
arXiv:2505.03703v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared representation space. However, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. While this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc. no generic and practical methods have so far been proposed to assess it precisely and even reduce it. We therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. Extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. Our code is available at the URL provided in the paper's abstract.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal cascade feature transfer for polymer property prediction</title>
<link>https://arxiv.org/abs/2505.03704</link>
<guid>https://arxiv.org/abs/2505.03704</guid>
<content:encoded><![CDATA[
arXiv:2505.03704v1 Announce Type: cross 
Abstract: In this paper, we propose a novel transfer learning approach called multi-modal cascade model with feature transfer for polymer property prediction.Polymers are characterized by a composite of data in several different formats, including molecular descriptors and additive information as well as chemical structures. However, in conventional approaches, prediction models were often constructed using each type of data separately. Our model enables more accurate prediction of physical properties for polymers by combining features extracted from the chemical structure by graph convolutional neural networks (GCN) with features such as molecular descriptors and additive information. The predictive performance of the proposed method is empirically evaluated using several polymer datasets. We report that the proposed method shows high predictive performance compared to the baseline conventional approach using a single feature.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Actor-Critics Can Achieve Optimal Sample Efficiency</title>
<link>https://arxiv.org/abs/2505.03710</link>
<guid>https://arxiv.org/abs/2505.03710</guid>
<content:encoded><![CDATA[
arXiv:2505.03710v1 Announce Type: cross 
Abstract: Actor-critic algorithms have become a cornerstone in reinforcement learning (RL), leveraging the strengths of both policy-based and value-based methods. Despite recent progress in understanding their statistical efficiency, no existing work has successfully learned an $\epsilon$-optimal policy with a sample complexity of $O(1/\epsilon^2)$ trajectories with general function approximation when strategic exploration is necessary.
  We address this open problem by introducing a novel actor-critic algorithm that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$ regret when the Bellman eluder dimension $d$ does not increase with $T$ at more than a $\log T$ rate.
  Here, $\mathcal{F}$ is the critic function class, $\mathcal{A}$ is the action space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm integrates optimism, off-policy critic estimation targeting the optimal Q-function, and rare-switching policy resets.
  We extend this to the setting of Hybrid RL, showing that initializing the critic with offline data yields sample efficiency gains compared to purely offline or online RL. Further, utilizing access to offline data, we provide a \textit{non-optimistic} provably efficient actor-critic algorithm that only additionally requires $N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$ in exchange for omitting optimism, where $c_{\text{off}}^*$ is the single-policy concentrability coefficient and $N_{\text{off}}$ is the number of offline samples. This addresses another open problem in the literature. We further provide numerical experiments to support our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima</title>
<link>https://arxiv.org/abs/2505.03717</link>
<guid>https://arxiv.org/abs/2505.03717</guid>
<content:encoded><![CDATA[
arXiv:2505.03717v1 Announce Type: cross 
Abstract: The classical low-rank matrix recovery problem is well-known to exhibit \emph{benign nonconvexity} under the restricted isometry property (RIP): local optimization is guaranteed to converge to the global optimum, where the ground truth is recovered. We investigate whether benign nonconvexity continues to hold when the factor matrices are constrained to be elementwise nonnegative -- a common practical requirement. In the simple setting of a rank-1 nonnegative ground truth, we confirm that benign nonconvexity holds in the fully-observed case with RIP constant $\delta=0$. Surprisingly, however, this property fails to extend to the partially-observed case with any arbitrarily small RIP constant $\delta\to0^{+}$, irrespective of rank overparameterization. This finding exposes a critical theoretical gap: the continuity argument widely used to explain the empirical robustness of low-rank matrix recovery fundamentally breaks down once nonnegative constraints are imposed.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control</title>
<link>https://arxiv.org/abs/2505.03738</link>
<guid>https://arxiv.org/abs/2505.03738</guid>
<content:encoded><![CDATA[
arXiv:2505.03738v1 Announce Type: cross 
Abstract: Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace: such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO's consistent performance supports autonomous task execution via imitation learning, underscoring the system's versatility and robustness.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>q-Learning in Continuous Time</title>
<link>https://arxiv.org/abs/2207.00713</link>
<guid>https://arxiv.org/abs/2207.00713</guid>
<content:encoded><![CDATA[
arXiv:2207.00713v4 Announce Type: replace 
Abstract: We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms interprets the well-known Q-learning algorithm SARSA, and another recovers a policy gradient (PG) based continuous-time algorithm proposed in Jia and Zhou (2022b). Finally, we conduct simulation experiments to compare the performance of our algorithms with those of PG-based algorithms in Jia and Zhou (2022b) and time-discretized conventional Q-learning algorithms.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-Form Diffusion Models</title>
<link>https://arxiv.org/abs/2310.12395</link>
<guid>https://arxiv.org/abs/2310.12395</guid>
<content:encoded><![CDATA[
arXiv:2310.12395v3 Announce Type: replace 
Abstract: Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves competitive sampling times while running on consumer-grade CPUs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLUM: Improving Inference Efficiency By Leveraging Repetition-Sparsity Trade-Off</title>
<link>https://arxiv.org/abs/2312.01581</link>
<guid>https://arxiv.org/abs/2312.01581</guid>
<content:encoded><![CDATA[
arXiv:2312.01581v2 Announce Type: replace 
Abstract: Efficient inference of Deep Neural Networks (DNNs) on resource-constrained edge devices is essential. Quantization and sparsity are key techniques that translate to repetition and sparsity within tensors at the hardware-software interface. This paper introduces the concept of repetition-sparsity trade-off that helps explain computational efficiency during inference. We propose PLUM, a unified co-design framework that integrates DNN inference systems and quantization (forward and backward pass) to leverage the repetition-sparsity trade-off to improve inference efficiency. Our results demonstrate that PLUM's quantization method is more accurate than binary quantization with the same number of non-zero weights. Detailed analysis indicates that signed binarization generates a smaller distribution of effectual (non-zero) parameters nested within a larger distribution of total parameters of latent full-precision weights for a DNN block. Finally, the proposed PLUM framework achieves a 26% speedup on real hardware, doubles energy efficiency, and reduces density by 2.8x compared to binary methods while retaining top-1 accuracy when compared to prior-art methods for ResNets on ImageNet (by achieving 66.2% top-1 accuracy), presenting an alternative solution for deploying efficient models in resource-limited environments.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective backdoor attack on graph neural networks in link prediction tasks</title>
<link>https://arxiv.org/abs/2401.02663</link>
<guid>https://arxiv.org/abs/2401.02663</guid>
<content:encoded><![CDATA[
arXiv:2401.02663v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) are a class of deep learning models capable of processing graph-structured data, and they have demonstrated significant performance in a variety of real-world applications. Recent studies have found that GNN models are vulnerable to backdoor attacks. When specific patterns (called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input data, the backdoor embedded in the GNN models is activated, which misclassifies the input data into the target class label specified by the attacker, whereas when there are no backdoor triggers in the input, the backdoor embedded in the GNN models is not activated, and the models work normally. Backdoor attacks are highly stealthy and expose GNN models to serious security risks. Currently, research on backdoor attacks against GNNs mainly focus on tasks such as graph classification and node classification, and backdoor attacks against link prediction tasks are rarely studied. In this paper, we propose a backdoor attack against the link prediction tasks based on GNNs and reveal the existence of such security vulnerability in GNN models, which make the backdoored GNN models to incorrectly predict unlinked two nodes as having a link relationship when a trigger appear. The method uses a single node as the trigger and poison selected node pairs in the training graph, and then the backdoor will be embedded in the GNN models through the training process. In the inference stage, the backdoor in the GNN models can be activated by simply linking the trigger node to the two end nodes of the unlinked node pairs in the input data, causing the GNN models to produce incorrect link prediction results for the target node pairs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Momentum-SAM: Sharpness Aware Minimization without Computational Overhead</title>
<link>https://arxiv.org/abs/2401.12033</link>
<guid>https://arxiv.org/abs/2401.12033</guid>
<content:encoded><![CDATA[
arXiv:2401.12033v2 Announce Type: replace 
Abstract: The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam. We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization. Code is available at https://github.com/MarlonBecker/MSAM.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreDF: Learning to Forecast in the Frequency Domain</title>
<link>https://arxiv.org/abs/2402.02399</link>
<guid>https://arxiv.org/abs/2402.02399</guid>
<content:encoded><![CDATA[
arXiv:2402.02399v2 Announce Type: replace 
Abstract: Time series modeling presents unique challenges due to autocorrelation in both historical data and future sequences. While current research predominantly addresses autocorrelation within historical data, the correlations among future labels are often overlooked. Specifically, modern forecasting models primarily adhere to the Direct Forecast (DF) paradigm, generating multi-step forecasts independently and disregarding label autocorrelation over time. In this work, we demonstrate that the learning objective of DF is biased in the presence of label autocorrelation. To address this issue, we propose the Frequency-enhanced Direct Forecast (FreDF), which mitigates label autocorrelation by learning to forecast in the frequency domain, thereby reducing estimation bias. Our experiments show that FreDF significantly outperforms existing state-of-the-art methods and is compatible with a variety of forecast models. Code is available at https://github.com/Master-PLC/FreDF.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems</title>
<link>https://arxiv.org/abs/2402.09780</link>
<guid>https://arxiv.org/abs/2402.09780</guid>
<content:encoded><![CDATA[
arXiv:2402.09780v4 Announce Type: replace 
Abstract: The Continuous Learning (CL) paradigm consists of continuously evolving the parameters of the Deep Neural Network (DNN) model to progressively learn to perform new tasks without reducing the performance on previous tasks, i.e., avoiding the so-called catastrophic forgetting. However, the DNN parameter update in CL-based autonomous systems is extremely resource-hungry. The existing DNN accelerators cannot be directly employed in CL because they only support the execution of the forward propagation. Only a few prior architectures execute the backpropagation and weight update, but they lack the control and management for CL. Towards this, we design a hardware architecture, TinyCL, to perform CL on resource-constrained autonomous systems. It consists of a processing unit that executes both forward and backward propagation, and a control unit that manages memory-based CL workload. To minimize the memory accesses, the sliding window of the convolutional layer moves in a snake-like fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at runtime to execute different operations. As per our knowledge, our proposed TinyCL represents the first hardware accelerator that executes CL on autonomous systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS technology node with the conventional ASIC design flow. It executes 1 epoch of training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while 1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s, thus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems</title>
<link>https://arxiv.org/abs/2402.11722</link>
<guid>https://arxiv.org/abs/2402.11722</guid>
<content:encoded><![CDATA[
arXiv:2402.11722v2 Announce Type: replace 
Abstract: Fourier Neural Operator (FNO) is a powerful and popular operator learning method. However, FNO is mainly used in forward prediction, yet a great many applications rely on solving inverse problems. In this paper, we propose an invertible Fourier Neural Operator (iFNO) for jointly tackling the forward and inverse problems. We developed a series of invertible Fourier blocks in the latent channel space to share the model parameters, exchange the information, and mutually regularize the learning for the bi-directional tasks. We integrated a variational auto-encoder to capture the intrinsic structures within the input space and to enable posterior inference so as to mitigate challenges of illposedness, data shortage, noises that are common in inverse problems. We proposed a three-step process to combine the invertible blocks and the VAE component for effective training. The evaluations on seven benchmark forward and inverse tasks have demonstrated the advantages of our approach.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning</title>
<link>https://arxiv.org/abs/2402.17967</link>
<guid>https://arxiv.org/abs/2402.17967</guid>
<content:encoded><![CDATA[
arXiv:2402.17967v2 Announce Type: replace 
Abstract: Transport systems on networks are crucial in various applications, but face a significant risk of being adversely affected by unforeseen circumstances such as disasters. The application of entropy-regularized optimal transport (OT) on graph structures has been investigated to enhance the robustness of transport on such networks. In this study, we propose an imitation-regularized OT (I-OT) that mathematically incorporates prior knowledge into the robustness of OT. This method is expected to enhance interpretability by integrating human insights into robustness and to accelerate practical applications. Furthermore, we mathematically verify the robustness of I-OT and discuss how these robustness properties relate to real-world applications. The effectiveness of this method is validated through a logistics simulation using automotive parts data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective</title>
<link>https://arxiv.org/abs/2403.16137</link>
<guid>https://arxiv.org/abs/2403.16137</guid>
<content:encoded><![CDATA[
arXiv:2403.16137v3 Announce Type: replace 
Abstract: Graph self-supervised learning (SSL) is now a go-to method for pre-training graph foundation models (GFMs). There is a wide variety of knowledge patterns embedded in the graph data, such as node properties and clusters, which are crucial to learning generalized representations for GFMs. However, existing surveys of GFMs have several shortcomings: they lack comprehensiveness regarding the most recent progress, have unclear categorization of self-supervised methods, and take a limited architecture-based perspective that is restricted to only certain types of graph models. As the ultimate goal of GFMs is to learn generalized graph knowledge, we provide a comprehensive survey of self-supervised GFMs from a novel knowledge-based perspective. We propose a knowledge-based taxonomy, which categorizes self-supervised graph models by the specific graph knowledge utilized. Our taxonomy consists of microscopic (nodes, links, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge (global structure, manifolds, etc.). It covers a total of 9 knowledge categories and more than 25 pretext tasks for pre-training GFMs, as well as various downstream task generalization strategies. Such a knowledge-based taxonomy allows us to re-examine graph models based on new architectures more clearly, such as graph language models, as well as provide more in-depth insights for constructing GFMs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lai Loss: A Novel Loss for Gradient Control</title>
<link>https://arxiv.org/abs/2405.07884</link>
<guid>https://arxiv.org/abs/2405.07884</guid>
<content:encoded><![CDATA[
arXiv:2405.07884v3 Announce Type: replace 
Abstract: In the field of machine learning, traditional regularization methods tend to directly add regularization terms to the loss function. This paper introduces the "Lai loss", a novel loss design that integrates the regularization terms (specifically, gradients) into the traditional loss function through straightforward geometric concepts. This design penalizes the gradients with the loss itself, allowing for control of the gradients while ensuring maximum accuracy. With this loss, we can effectively control the model's smoothness and sensitivity, potentially offering the dual benefits of improving the model's generalization performance and enhancing its noise resistance on specific features. Additionally, we proposed a training method that successfully addresses the challenges in practical applications. We conducted preliminary experiments using publicly available datasets from Kaggle, demonstrating that the design of Lai loss can control the model's smoothness and sensitivity while maintaining stable model performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OAC: Output-adaptive Calibration for Accurate Post-training Quantization</title>
<link>https://arxiv.org/abs/2405.15025</link>
<guid>https://arxiv.org/abs/2405.15025</guid>
<content:encoded><![CDATA[
arXiv:2405.15025v2 Announce Type: replace 
Abstract: Deployment of Large Language Models (LLMs) has major computational costs, due to their rapidly expanding size. Compression of LLMs reduces the memory footprint, latency, and energy required for their inference. Post-training Quantization (PTQ) techniques have been developed to compress LLMs while avoiding expensive re-training. Most PTQ approaches formulate the quantization error based on a layer-wise Euclidean loss, ignoring the model output. Then, each layer is calibrated using its layer-wise Hessian to update the weights towards minimizing the quantization error. The Hessian is also used for detecting the most salient weights to quantization. Such PTQ approaches are prone to accuracy drop in low-precision quantization. We propose Output-adaptive Calibration (OAC) to incorporate the model output in the calibration process. We formulate the quantization error based on the distortion of the output cross-entropy loss. OAC approximates the output-adaptive Hessian for each layer under reasonable assumptions to reduce the computational complexity. The output-adaptive Hessians are used to update the weight matrices and detect the salient weights towards maintaining the model output. Our proposed method outperforms the state-of-the-art baselines such as SpQR and BiLLM, especially, at extreme low-precision (2-bit and binary) quantization.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HINT: Hypernetwork Approach to Training Weight Interval Regions in Continual Learning</title>
<link>https://arxiv.org/abs/2405.15444</link>
<guid>https://arxiv.org/abs/2405.15444</guid>
<content:encoded><![CDATA[
arXiv:2405.15444v4 Announce Type: replace 
Abstract: Recently, a new Continual Learning (CL) paradigm was presented to control catastrophic forgetting, called Interval Continual Learning (InterContiNet), which relies on enforcing interval constraints on the neural network parameter space. Unfortunately, InterContiNet training is challenging due to the high dimensionality of the weight space, making intervals difficult to manage. To address this issue, we introduce HINT, a technique that employs interval arithmetic within the embedding space and utilizes a hypernetwork to map these intervals to the target network parameter space. We train interval embeddings for consecutive tasks and train a hypernetwork to transform these embeddings into weights of the target network. An embedding for a given task is trained along with the hypernetwork, preserving the response of the target network for the previous task embeddings. Interval arithmetic works with a more manageable, lower-dimensional embedding space rather than directly preparing intervals in a high-dimensional weight space. Our model allows faster and more efficient training. Furthermore, HINT maintains the guarantee of not forgetting. At the end of training, we can choose one universal embedding to produce a single network dedicated to all tasks. In such a framework, hypernetwork is used only for training and, finally, we can utilize one set of weights. HINT obtains significantly better results than InterContiNet and gives SOTA results on several benchmarks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sharper Global Convergence Analysis for Average Reward Reinforcement Learning via an Actor-Critic Approach</title>
<link>https://arxiv.org/abs/2407.18878</link>
<guid>https://arxiv.org/abs/2407.18878</guid>
<content:encoded><![CDATA[
arXiv:2407.18878v3 Announce Type: replace 
Abstract: This work examines average-reward reinforcement learning with general policy parametrization. Existing state-of-the-art (SOTA) guarantees for this problem are either suboptimal or hindered by several challenges, including poor scalability with respect to the size of the state-action space, high iteration complexity, and dependence on knowledge of mixing times and hitting times. To address these limitations, we propose a Multi-level Monte Carlo-based Natural Actor-Critic (MLMC-NAC) algorithm. Our work is the first to achieve a global convergence rate of $\tilde{\mathcal{O}}(1/\sqrt{T})$ for average-reward Markov Decision Processes (MDPs) (where $T$ is the horizon length), without requiring the knowledge of mixing and hitting times. Moreover, the convergence rate does not scale with the size of the state space, therefore even being applicable to infinite state spaces.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bellman Unbiasedness: Tractable and Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation</title>
<link>https://arxiv.org/abs/2407.21260</link>
<guid>https://arxiv.org/abs/2407.21260</guid>
<content:encoded><![CDATA[
arXiv:2407.21260v2 Announce Type: replace 
Abstract: Distributional reinforcement learning improves performance by capturing environmental stochasticity, but a comprehensive theoretical understanding of its effectiveness remains elusive. In addition, the intractable element of the infinite dimensionality of distributions has been overlooked. In this paper, we present a regret analysis of distributional reinforcement learning with general value function approximation in a finite episodic Markov decision process setting. We first introduce a key notion of $\textit{Bellman unbiasedness}$ which is essential for exactly learnable and provably efficient distributional updates in an online manner. Among all types of statistical functionals for representing infinite-dimensional return distributions, our theoretical results demonstrate that only moment functionals can exactly capture the statistical information. Secondly, we propose a provably efficient algorithm, $\texttt{SF-LSVI}$, that achieves a tight regret bound of $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$ where $H$ is the horizon, $K$ is the number of episodes, and $d_E$ is the eluder dimension of a function class.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Flatland: A Geometric Take on Matching Methods for Treatment Effect Estimation</title>
<link>https://arxiv.org/abs/2409.05459</link>
<guid>https://arxiv.org/abs/2409.05459</guid>
<content:encoded><![CDATA[
arXiv:2409.05459v2 Announce Type: replace 
Abstract: Matching is a popular approach in causal inference to estimate treatment effects by pairing treated and control units that are most similar in terms of their covariate information. However, classic matching methods completely ignore the geometry of the data manifold, which is crucial to define a meaningful distance for matching, and struggle when covariates are noisy and high-dimensional. In this work, we propose GeoMatching, a matching method to estimate treatment effects that takes into account the intrinsic data geometry induced by existing causal mechanisms among the confounding variables. First, we learn a low-dimensional, latent Riemannian manifold that accounts for uncertainty and geometry of the original input data. Second, we estimate treatment effects via matching in the latent space based on the learned latent Riemannian metric. We provide theoretical insights and empirical results in synthetic and real-world scenarios, demonstrating that GeoMatching yields more effective treatment effect estimators, even as we increase input dimensionality, in the presence of outliers, or in semi-supervised scenarios.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Meta-Learning from a Learning Lens</title>
<link>https://arxiv.org/abs/2409.08474</link>
<guid>https://arxiv.org/abs/2409.08474</guid>
<content:encoded><![CDATA[
arXiv:2409.08474v3 Announce Type: replace 
Abstract: Meta-learning seeks to learn a well-generalized model initialization from training tasks to solve unseen tasks. From the "learning to learn" perspective, the quality of the initialization is modeled with one-step gradient decent in the inner loop. However, contrary to theoretical expectations, our empirical analysis reveals that this may expose meta-learning to underfitting. To bridge the gap between theoretical understanding and practical implementation, we reconsider meta-learning from the "Learning" lens. We propose that the meta-learning model comprises two interrelated components: parameters for model initialization and a meta-layer for task-specific fine-tuning. These components will lead to the risks of overfitting and underfitting depending on tasks, and their solutions, fewer parameters vs. more meta-layer, are often in conflict. To address this, we aim to regulate the task information the model receives without modifying the data or model structure. Our theoretical analysis indicates that models adapted to different tasks can mutually reinforce each other, highlighting the effective information. Based on this insight, we propose TRLearner, a plug-and-play method that leverages task relation to calibrate meta-learning. It first extracts task relation matrices and then applies relation-aware consistency regularization to guide optimization. Extensive theoretical and empirical evaluations demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Weighting Strategy in KernelSHAP</title>
<link>https://arxiv.org/abs/2410.04883</link>
<guid>https://arxiv.org/abs/2410.04883</guid>
<content:encoded><![CDATA[
arXiv:2410.04883v2 Announce Type: replace 
Abstract: In Explainable AI (XAI), Shapley values are a popular model-agnostic framework for explaining predictions made by complex machine learning models. The computation of Shapley values requires estimating non-trivial contribution functions representing predictions with only a subset of the features present. As the number of these terms grows exponentially with the number of features, computational costs escalate rapidly, creating a pressing need for efficient and accurate approximation methods. For tabular data, the KernelSHAP framework is considered the state-of-the-art model-agnostic approximation framework. KernelSHAP approximates the Shapley values using a weighted sample of the contribution functions for different feature subsets. We propose a novel modification of KernelSHAP which replaces the stochastic weights with deterministic ones to reduce the variance of the resulting Shapley value approximations. This may also be combined with our simple, yet effective modification to the KernelSHAP variant implemented in the popular Python library SHAP. Additionally, we provide an overview of established methods. Numerical experiments demonstrate that our methods can reduce the required number of contribution function evaluations by $5\%$ to $50\%$ while preserving the same accuracy of the approximated Shapley values -- essentially reducing the running time by up to $50\%$. These computational advancements push the boundaries of the feature dimensionality and number of predictions that can be accurately explained with Shapley values within a feasible runtime.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Aided Kalman Filters</title>
<link>https://arxiv.org/abs/2410.12289</link>
<guid>https://arxiv.org/abs/2410.12289</guid>
<content:encoded><![CDATA[
arXiv:2410.12289v3 Announce Type: replace 
Abstract: The Kalman filter (KF) and its variants are among the most celebrated algorithms in signal processing. These methods are used for state estimation of dynamic systems by relying on mathematical representations in the form of simple state-space (SS) models, which may be crude and inaccurate descriptions of the underlying dynamics. Emerging data-centric artificial intelligence (AI) techniques tackle these tasks using deep neural networks (DNNs), which are model-agnostic. Recent developments illustrate the possibility of fusing DNNs with classic Kalman-type filtering, obtaining systems that learn to track in partially known dynamics. This article provides a tutorial-style overview of design approaches for incorporating AI in aiding KF-type algorithms. We review both generic and dedicated DNN architectures suitable for state estimation, and provide a systematic presentation of techniques for fusing AI tools with KFs and for leveraging partial SS modeling and data, categorizing design approaches into task-oriented and SS model-oriented. The usefulness of each approach in preserving the individual strengths of model-based KFs and data-driven DNNs is investigated in a qualitative and quantitative study, whose code is publicly available, illustrating the gains of hybrid model-based/data-driven designs. We also discuss existing challenges and future research directions that arise from fusing AI and Kalman-type algorithms.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Off-Grid Weather Forecasting with Multi-Modal Earth Observation Data</title>
<link>https://arxiv.org/abs/2410.12938</link>
<guid>https://arxiv.org/abs/2410.12938</guid>
<content:encoded><![CDATA[
arXiv:2410.12938v3 Announce Type: replace 
Abstract: Urgent applications like wildfire management and renewable energy generation require precise, localized weather forecasts near the Earth's surface. However, forecasts produced by machine learning models or numerical weather prediction systems are typically generated on large-scale regular grids, where direct downscaling fails to capture fine-grained, near-surface weather patterns. In this work, we propose a multi-modal transformer model trained end-to-end to downscale gridded forecasts to off-grid locations of interest. Our model directly combines local historical weather observations (e.g., wind, temperature, dewpoint) with gridded forecasts to produce locally accurate predictions at various lead times. Multiple data modalities are collected and concatenated at station-level locations, treated as a token at each station. Using self-attention, the token corresponding to the target location aggregates information from its neighboring tokens. Experiments using weather stations across the Northeastern United States show that our model outperforms a range of data-driven and non-data-driven off-grid forecasting methods. They also reveal that direct input of station data provides a phase shift in local weather forecasting accuracy, reducing the prediction error by up to 80% compared to pure gridded data based models. This approach demonstrates how to bridge the gap between large-scale weather models and locally accurate forecasts to support high-stakes, location-sensitive decision-making.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-free World Models for Online Imitation Learning</title>
<link>https://arxiv.org/abs/2410.14081</link>
<guid>https://arxiv.org/abs/2410.14081</guid>
<content:encoded><![CDATA[
arXiv:2410.14081v3 Announce Type: replace 
Abstract: Imitation learning (IL) enables agents to acquire skills directly from expert demonstrations, providing a compelling alternative to reinforcement learning. However, prior online IL approaches struggle with complex tasks characterized by high-dimensional inputs and complex dynamics. In this work, we propose a novel approach to online imitation learning that leverages reward-free world models. Our method learns environmental dynamics entirely in latent spaces without reconstruction, enabling efficient and accurate modeling. We adopt the inverse soft-Q learning objective, reformulating the optimization process in the Q-policy space to mitigate the instability associated with traditional optimization in the reward-policy space. By employing a learned latent dynamics model and planning for control, our approach consistently achieves stable, expert-level performance in tasks with high-dimensional observation or action spaces and intricate dynamics. We evaluate our method on a diverse set of benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating superior empirical performance compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate</title>
<link>https://arxiv.org/abs/2410.22086</link>
<guid>https://arxiv.org/abs/2410.22086</guid>
<content:encoded><![CDATA[
arXiv:2410.22086v3 Announce Type: replace 
Abstract: Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable Machine Learning Potential</title>
<link>https://arxiv.org/abs/2410.23858</link>
<guid>https://arxiv.org/abs/2410.23858</guid>
<content:encoded><![CDATA[
arXiv:2410.23858v3 Announce Type: replace 
Abstract: A neural network-based machine learning potential energy surface (PES) expressed in a matrix product operator (NN-MPO) is proposed. The MPO form enables efficient evaluation of high-dimensional integrals that arise in solving the time-dependent and time-independent Schr\"odinger equation and effectively overcomes the so-called curse of dimensionality. This starkly contrasts with other neural network-based machine learning PES methods, such as multi-layer perceptrons (MLPs), where evaluating high-dimensional integrals is not straightforward due to the fully connected topology in their backbone architecture. Nevertheless, the NN-MPO retains the high representational capacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a test mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled six-dimensional ab initio PES, using only 625 training points distributed across a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is available at https://github.com/KenHino/Pompon.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Mesh with Me: Generating Constructive Solid Geometry Instead of Meshes by Fine-Tuning a Code-Generation LLM</title>
<link>https://arxiv.org/abs/2411.15279</link>
<guid>https://arxiv.org/abs/2411.15279</guid>
<content:encoded><![CDATA[
arXiv:2411.15279v2 Announce Type: replace 
Abstract: While recent advancements in machine learning, such as LLMs, are revolutionizing software development and creative industries, they have had minimal impact on engineers designing mechanical parts, which remains largely a manual process. Existing approaches to generating 3D geometry most commonly use meshes as a 3D representation. While meshes are suitable for assets in video games or animations, they lack sufficient precision and adaptability for mechanical engineering purposes. This paper introduces a novel approach for the generation of 3D geometry that generates surface-based Constructive Solid Geometry (CSG) by leveraging a code-generation LLM. First, we create a dataset of 3D mechanical parts represented as code scripts by converting Boundary Representation geometry (BREP) into CSG-based Python scripts. Second, we create annotations in natural language using GPT-4. The resulting dataset is used to fine-tune a code-generation LLM. The fine-tuned LLM can complete geometries based on positional input and natural language in a plausible way, demonstrating geometric understanding.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications</title>
<link>https://arxiv.org/abs/2411.18915</link>
<guid>https://arxiv.org/abs/2411.18915</guid>
<content:encoded><![CDATA[
arXiv:2411.18915v4 Announce Type: replace 
Abstract: Business documents often contain substantial tabular and textual information with numerical values, requiring mathematical reasoning for effective document understanding. While Small Language Models (SLMs) still struggle at this task, tool-augmented multi-step agents perform better, at the cost of relying on closed-source or larger models, external data, or extensive prompt-engineering. This work introduces MATATA, a novel weakly supervised end-to-end approach to train multi-step reasoning language agents for document tabular applications. MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B SLMs. During its two-stage training, MATATA uses the final outcome of the multi-step reasoning chain as weak supervision. This approach avoids having to individually supervise each intermediate agent in the reasoning chain. By employing an adaptive planner and shared tools across different datasets, MATATA shows robust performance. Experiments demonstrate that MATATA achieves state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based frameworks on TabMWP. This novel weakly supervised approach enables training an end-to-end multi-step reasoning agent without intermediate supervision, supporting future developments of cost-effective powerful agentic systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets</title>
<link>https://arxiv.org/abs/2412.07775</link>
<guid>https://arxiv.org/abs/2412.07775</guid>
<content:encoded><![CDATA[
arXiv:2412.07775v5 Announce Type: replace 
Abstract: While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and/or slow convergence in finetuning. In response to this challenge, we take inspiration from recent successes in generative flow networks (GFlowNets) and propose a reinforcement learning method for diffusion model finetuning, dubbed Nabla-GFlowNet (abbreviated as $\nabla$-GFlowNet), that leverages the rich signal in reward gradients for probabilistic diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the Power of Continual Learning on Non-Centralized Devices: A Survey</title>
<link>https://arxiv.org/abs/2412.13840</link>
<guid>https://arxiv.org/abs/2412.13840</guid>
<content:encoded><![CDATA[
arXiv:2412.13840v2 Announce Type: replace 
Abstract: Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for enabling distributed devices such as vehicles and servers to handle streaming data from a joint non-stationary environment. To achieve high reliability and scalability in deploying this paradigm in distributed systems, it is essential to conquer challenges stemming from both spatial and temporal dimensions, manifesting as distribution shifts, catastrophic forgetting, heterogeneity, and privacy issues. This survey focuses on a comprehensive examination of the development of the non-centralized continual learning algorithms and the real-world deployment across distributed devices. We begin with an introduction to the background and fundamentals of non-centralized learning and continual learning. Then, we review existing solutions from three levels to represent how existing techniques alleviate the catastrophic forgetting and distribution shift. Additionally, we delve into the various types of heterogeneity issues, security, and privacy attributes, as well as real-world applications across three prevalent scenarios. Furthermore, we establish a large-scale benchmark to revisit this problem and analyze the performance of the state-of-the-art NCCL approaches. Finally, we discuss the important challenges and future research directions in NCCL.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HardML: A Benchmark For Evaluating Data Science And Machine Learning knowledge and reasoning in AI</title>
<link>https://arxiv.org/abs/2501.15627</link>
<guid>https://arxiv.org/abs/2501.15627</guid>
<content:encoded><![CDATA[
arXiv:2501.15627v2 Announce Type: replace 
Abstract: We present HardML, a benchmark designed to evaluate the knowledge and reasoning abilities in the fields of data science and machine learning. HardML comprises a diverse set of 100 challenging multiple-choice questions, handcrafted over a period of 6 months, covering the most popular and modern branches of data science and machine learning. These questions are challenging even for a typical Senior Machine Learning Engineer to answer correctly. To minimize the risk of data contamination, HardML uses mostly original content devised by the author. Current state of the art AI models achieve a 30% error rate on this benchmark, which is about 3 times larger than the one achieved on the equivalent, well known MMLU ML. While HardML is limited in scope and not aiming to push the frontier, primarily due to its multiple choice nature, it serves as a rigorous and modern testbed to quantify and track the progress of top AI. While plenty benchmarks and experimentation in LLM evaluation exist in other STEM fields like mathematics, physics and chemistry, the subfields of data science and machine learning remain fairly underexplored.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regularized second-order optimization of tensor-network Born machines</title>
<link>https://arxiv.org/abs/2501.18691</link>
<guid>https://arxiv.org/abs/2501.18691</guid>
<content:encoded><![CDATA[
arXiv:2501.18691v2 Announce Type: replace 
Abstract: Tensor-network Born machines (TNBMs) are quantum-inspired generative models for learning data distributions. Using tensor-network contraction and optimization techniques, the model learns an efficient representation of the target distribution, capable of capturing complex correlations with a compact parameterization. Despite their promise, the optimization of TNBMs presents several challenges. A key bottleneck of TNBMs is the logarithmic nature of the loss function commonly used for this problem. The single-tensor logarithmic optimization problem cannot be solved analytically, necessitating an iterative approach that slows down convergence and increases the risk of getting trapped in one of many non-optimal local minima. In this paper, we present an improved second-order optimization technique for TNBM training, which significantly enhances convergence rates and the quality of the optimized model. Our method employs a modified Newton's method on the manifold of normalized states, incorporating regularization of the loss landscape to mitigate local minima issues. We demonstrate the effectiveness of our approach by training a one-dimensional matrix product state (MPS) on both discrete and continuous datasets, showcasing its advantages in terms of stability and efficiency, and demonstrating its potential as a robust and scalable approach for optimizing quantum-inspired generative models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HadamRNN: Binary and Sparse Ternary Orthogonal RNNs</title>
<link>https://arxiv.org/abs/2502.00047</link>
<guid>https://arxiv.org/abs/2502.00047</guid>
<content:encoded><![CDATA[
arXiv:2502.00047v4 Announce Type: replace 
Abstract: Binary and sparse ternary weights in neural networks enable faster computations and lighter representations, facilitating their use on edge devices with limited computational power. Meanwhile, vanilla RNNs are highly sensitive to changes in their recurrent weights, making the binarization and ternarization of these weights inherently challenging. To date, no method has successfully achieved binarization or ternarization of vanilla RNN weights. We present a new approach leveraging the properties of Hadamard matrices to parameterize a subset of binary and sparse ternary orthogonal matrices. This method enables the training of orthogonal RNNs (ORNNs) with binary and sparse ternary recurrent weights, effectively creating a specific class of binary and sparse ternary vanilla RNNs. The resulting ORNNs, called HadamRNN and Block-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and sequential MNIST tasks, the IMDB dataset, two GLUE benchmarks, and two IoT benchmarks. Despite binarization or sparse ternarization, these RNNs maintain performance levels comparable to state-of-the-art full-precision models, highlighting the effectiveness of our approach. Notably, our approach is the first solution with binary recurrent weights capable of tackling the copy task over 1000 timesteps.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model</title>
<link>https://arxiv.org/abs/2502.14131</link>
<guid>https://arxiv.org/abs/2502.14131</guid>
<content:encoded><![CDATA[
arXiv:2502.14131v3 Announce Type: replace 
Abstract: We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling</title>
<link>https://arxiv.org/abs/2503.02445</link>
<guid>https://arxiv.org/abs/2503.02445</guid>
<content:encoded><![CDATA[
arXiv:2503.02445v3 Announce Type: replace 
Abstract: Time-series Generation (TSG) is a prominent research area with broad applications in simulations, data augmentation, and counterfactual analysis. While existing methods have shown promise in unconditional single-domain TSG, real-world applications demand for cross-domain approaches capable of controlled generation tailored to domain-specific constraints and instance-level requirements. In this paper, we argue that text can provide semantic insights, domain information and instance-specific temporal patterns, to guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused on generating realistic time series by incorporating textual descriptions. To address data scarcity in this setting, we propose a novel LLM-based Multi-Agent framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore, we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates semantic prototypes with text description for supporting domain-level guidance. This approach achieves state-of-the-art generation fidelity on 11 of 12 datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared to no text input generation, highlighting its potential for generating tailored time-series data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models</title>
<link>https://arxiv.org/abs/2503.06269</link>
<guid>https://arxiv.org/abs/2503.06269</guid>
<content:encoded><![CDATA[
arXiv:2503.06269v2 Announce Type: replace 
Abstract: Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-guided machine learning for county-level corn yield prediction under drought</title>
<link>https://arxiv.org/abs/2503.16328</link>
<guid>https://arxiv.org/abs/2503.16328</guid>
<content:encoded><![CDATA[
arXiv:2503.16328v2 Announce Type: replace 
Abstract: Remote sensing (RS) technique, enabling the non-contact acquisition of extensive ground observations, is a valuable tool for crop yield predictions. Traditional process-based models struggle to incorporate large volumes of RS data, and most users lack understanding of crop growth mechanisms. In contrast, machine learning (ML) models are often criticized as "black boxes" due to their limited interpretability. To address these limitations, we utilized Knowledge-Guided Machine Learning (KGML), a framework that leverages the strengths of both process-based and ML models. Existing works have either overlooked the role of soil moisture in corn growth or did not embed this effect into their models. To bridge this gap, we developed the Knowledge-Guided Machine Learning with Soil Moisture (KGML-SM) framework, treating soil moisture as an intermediate variable in corn growth to emphasize its key role in plant development. Additionally, based on the prior knowledge that the model may overestimate under drought conditions, we designed a drought-aware loss function that penalized predicted yield in drought-affected areas. Our experiments showed that the KGML-SM model outperformed other traditional ML models. We explored the relationships between drought, soil moisture, and corn yield prediction by assessing the importance of different features within the model, and analyzing how soil moisture impacts predictions across different regions and time periods. Finally we provided interpretability for prediction errors to guide future model optimization.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGIN: Defining an Approximately Powerful Hyperbolic GNN</title>
<link>https://arxiv.org/abs/2504.00142</link>
<guid>https://arxiv.org/abs/2504.00142</guid>
<content:encoded><![CDATA[
arXiv:2504.00142v3 Announce Type: replace 
Abstract: While graph neural networks (GNNs) operating in hyperbolic spaces have shown promise for modeling hierarchical and complex relational data, a critical limitation often overlooked is their potentially limited discriminative power compared to their Euclidean counterparts or fundamental graph isomorphism tests like the Weisfeiler-Lehman (WL) hierarchy. Existing hyperbolic aggregation schemes, while curvature-aware, may not sufficiently capture the intricate structural differences required to robustly distinguish non-isomorphic graphs owing to non-injective aggregation functions. To address this expressiveness gap in hyperbolic graph learning, we introduce the Lorentzian Graph Isomorphic Network (LGIN), a novel GNN designed to achieve enhanced discriminative capabilities within the Lorentzian model of hyperbolic space. LGIN proposes a new update rule that effectively combines local neighborhood information with a richer representation of graph structure designed to preserve the Lorentzian metric tensor. This represents a significant step towards building more expressive GNNs in non-Euclidean geometries, overcoming a common bottleneck in current hyperbolic methods. We conduct extensive evaluations across nine diverse benchmark datasets, including molecular and protein structures. LGIN consistently outperforms or matches state-of-the-art hyperbolic and Euclidean GNNs, showcasing its practical efficacy and validating its superior ability to capture complex graph structures and distinguish between different graphs. To the best of our knowledge, LGIN is the first work to study the framework behind a powerful GNN on the hyperbolic space. The code for our paper can be found at https://github.com/Deceptrax123/LGIN
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyTTP: Trajectory Prediction with Normalization-Free Transformers</title>
<link>https://arxiv.org/abs/2504.05356</link>
<guid>https://arxiv.org/abs/2504.05356</guid>
<content:encoded><![CDATA[
arXiv:2504.05356v2 Announce Type: replace 
Abstract: Accurate trajectory prediction is a cornerstone for the safe operation of autonomous driving systems, where understanding the dynamic behavior of surrounding agents is crucial. Transformer-based architectures have demonstrated significant promise in capturing complex spatio-temporality dependencies. However, their reliance on normalization layers can lead to computation overhead and training instabilities. In this work, we present a two-fold approach to address these challenges. First, we integrate DynamicTanh (DyT), which is the latest method to promote transformers, into the backbone, replacing traditional layer normalization. This modification simplifies the network architecture and improves the stability of the inference. We are the first work to deploy the DyT to the trajectory prediction task. Complementing this, we employ a snapshot ensemble strategy to further boost trajectory prediction performance. Using cyclical learning rate scheduling, multiple model snapshots are captured during a single training run. These snapshots are then aggregated via simple averaging at inference time, allowing the model to benefit from diverse hypotheses without incurring substantial additional computational cost. Extensive experiments on Argoverse datasets demonstrate that our combined approach significantly improves prediction accuracy, inference speed and robustness in diverse driving scenarios. This work underscores the potential of normalization-free transformer designs augmented with lightweight ensemble techniques in advancing trajectory forecasting for autonomous vehicles.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Combinatorial Interpretability of Neural Computation</title>
<link>https://arxiv.org/abs/2504.08842</link>
<guid>https://arxiv.org/abs/2504.08842</guid>
<content:encoded><![CDATA[
arXiv:2504.08842v2 Announce Type: replace 
Abstract: We introduce combinatorial interpretability, a methodology for understanding neural computation by analyzing the combinatorial structures in the sign-based categorization of a network's weights and biases. We demonstrate its power through feature channel coding, a theory that explains how neural networks compute Boolean expressions and potentially underlies other categories of neural network computation. According to this theory, features are computed via feature channels: unique cross-neuron encodings shared among the inputs the feature operates on. Because different feature channels share neurons, the neurons are polysemantic and the channels interfere with one another, making the computation appear inscrutable.
  We show how to decipher these computations by analyzing a network's feature channel coding, offering complete mechanistic interpretations of several small neural networks that were trained with gradient descent. Crucially, this is achieved via static combinatorial analysis of the weight matrices, without examining activations or training new autoencoding networks. Feature channel coding reframes the superposition hypothesis, shifting the focus from neuron activation directionality in high-dimensional space to the combinatorial structure of codes. It also allows us for the first time to exactly quantify and explain the relationship between a network's parameter size and its computational capacity (i.e. the set of features it can compute with low error), a relationship that is implicitly at the core of many modern scaling laws.
  Though our initial studies of feature channel coding are restricted to Boolean functions, we believe they provide a rich, controlled, and informative research space, and that the path we propose for combinatorial interpretation of neural computation can provide a basis for understanding both artificial and biological neural circuits.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion</title>
<link>https://arxiv.org/abs/2504.16875</link>
<guid>https://arxiv.org/abs/2504.16875</guid>
<content:encoded><![CDATA[
arXiv:2504.16875v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharp Global Guarantees for Nonconvex Low-rank Recovery in the Noisy Overparameterized Regime</title>
<link>https://arxiv.org/abs/2104.10790</link>
<guid>https://arxiv.org/abs/2104.10790</guid>
<content:encoded><![CDATA[
arXiv:2104.10790v3 Announce Type: replace-cross 
Abstract: Recent work established that rank overparameterization eliminates spurious local minima in nonconvex low-rank matrix recovery under the restricted isometry property (RIP). But this does not fully explain the practical success of overparameterization, because real algorithms can still become trapped at nonstrict saddle points (approximate second-order points with arbitrarily small negative curvature) even when all local minima are global. Moreover, the result does not accommodate for noisy measurements, but it is unclear whether such an extension is even possible, in view of the many discontinuous and unintuitive behaviors already known for the overparameterized regime. In this paper, we introduce a novel proof technique that unifies, simplifies, and strengthens two previously competing approaches -- one based on escape directions and the other based on the inexistence of counterexample -- to provide sharp global guarantees in the noisy overparameterized regime. We show, once local minima have been converted into global minima through slight overparameterization, that near-second-order points achieve the same minimax-optimal recovery bounds (up to small constant factors) as significantly more expensive convex approaches. Our results are sharp with respect to the noise level and the solution accuracy, and hold for both the symmetric parameterization $XX^{T}$, as well as the asymmetric parameterization $UV^{T}$ under a balancing regularizer; we demonstrate that the balancing regularizer is indeed necessary.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truncated LinUCB for Stochastic Linear Bandits</title>
<link>https://arxiv.org/abs/2202.11735</link>
<guid>https://arxiv.org/abs/2202.11735</guid>
<content:encoded><![CDATA[
arXiv:2202.11735v4 Announce Type: replace-cross 
Abstract: This paper considers contextual bandits with a finite number of arms, where the contexts are independent and identically distributed $d$-dimensional random vectors, and the expected rewards are linear in both the arm parameters and contexts. The LinUCB algorithm, which is near minimax optimal for related linear bandits, is shown to have a cumulative regret that is suboptimal in both the dimension $d$ and time horizon $T$, due to its over-exploration. A truncated version of LinUCB is proposed and termed "Tr-LinUCB", which follows LinUCB up to a truncation time $S$ and performs pure exploitation afterwards. The Tr-LinUCB algorithm is shown to achieve $O(d\log(T))$ regret if $S = Cd\log(T)$ for a sufficiently large constant $C$, and a matching lower bound is established, which shows the rate optimality of Tr-LinUCB in both $d$ and $T$ under a low dimensional regime. Further, if $S = d\log^{\kappa}(T)$ for some $\kappa>1$, the loss compared to the optimal is a multiplicative $\log\log(T)$ factor, which does not depend on $d$. This insensitivity to overshooting in choosing the truncation time of Tr-LinUCB is of practical importance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Framework for Exploratory Learning-Aided Community Detection Under Topological Uncertainty</title>
<link>https://arxiv.org/abs/2304.04497</link>
<guid>https://arxiv.org/abs/2304.04497</guid>
<content:encoded><![CDATA[
arXiv:2304.04497v4 Announce Type: replace-cross 
Abstract: In social networks, the discovery of community structures has received considerable attention as a fundamental problem in various network analysis tasks. However, due to privacy concerns or access restrictions, the network structure is often uncertain, thereby rendering established community detection approaches ineffective without costly network topology acquisition. To tackle this challenge, we present META-CODE, a unified framework for detecting overlapping communities via exploratory learning aided by easy-to-collect node metadata when networks are topologically unknown (or only partially known). Specifically, META-CODE consists of three iterative steps in addition to the initial network inference step: 1) node-level community-affiliation embeddings based on graph neural networks (GNNs) trained by our new reconstruction loss, 2) network exploration via community-affiliation-based node queries, and 3) network inference using an edge connectivity-based Siamese neural network model from the explored network. Through extensive experiments on three real-world datasets including two large networks, we demonstrate: (a) the superiority of META-CODE over benchmark community detection methods, achieving remarkable gains up to 65.55% on the Facebook dataset over the best competitor among our selected competitive methods in terms of normalized mutual information (NMI), (b) the impact of each module in META-CODE, (c) the effectiveness of node queries in META-CODE based on empirical evaluations and theoretical findings, and (d) the convergence of the inferred network.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking On-Chip Communication Anonymity using Flow Correlation Attacks</title>
<link>https://arxiv.org/abs/2309.15687</link>
<guid>https://arxiv.org/abs/2309.15687</guid>
<content:encoded><![CDATA[
arXiv:2309.15687v3 Announce Type: replace-cross 
Abstract: Network-on-Chip (NoC) is widely used to facilitate communication between components in sophisticated System-on-Chip (SoC) designs. Security of the on-chip communication is crucial because exploiting any vulnerability in shared NoC would be a goldmine for an attacker that puts the entire computing infrastructure at risk. We investigate the security strength of existing anonymous routing protocols in NoC architectures, making two pivotal contributions. Firstly, we develop and perform a machine learning (ML)-based flow correlation attack on existing anonymous routing techniques in Network-on-Chip (NoC) systems, revealing that they provide only packet-level anonymity. Secondly, we propose a novel, lightweight anonymous routing protocol featuring outbound traffic tunneling and traffic obfuscation. This protocol is designed to provide robust defense against ML-based flow correlation attacks, ensuring both packet-level and flow-level anonymity. Experimental evaluation using both real and synthetic traffic demonstrates that our proposed attack successfully deanonymizes state-of-the-art anonymous routing in NoC architectures with high accuracy (up to 99%) for diverse traffic patterns. It also reveals that our lightweight anonymous routing protocol can defend against ML-based attacks with minor hardware and performance overhead.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParFam -- (Neural Guided) Symbolic Regression Based on Continuous Global Optimization</title>
<link>https://arxiv.org/abs/2310.05537</link>
<guid>https://arxiv.org/abs/2310.05537</guid>
<content:encoded><![CDATA[
arXiv:2310.05537v4 Announce Type: replace-cross 
Abstract: The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually complicated and involve various hyperparameters. In this paper, we present our new approach ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a global optimizer, this approach results in a highly effective method to tackle the problem of SR. We theoretically analyze the expressivity of ParFam and demonstrate its performance with extensive numerical experiments based on the common SR benchmark suit SRBench, showing that we achieve state-of-the-art results. Moreover, we present an extension incorporating a pre-trained transformer network DL-ParFam to guide ParFam, accelerating the optimization process by up to two magnitudes. Our code and results can be found at https://github.com/Philipp238/parfam.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Data Selection with Performance: Performance-driven Reinforcement Learning for Active Learning in Object Detection</title>
<link>https://arxiv.org/abs/2310.08387</link>
<guid>https://arxiv.org/abs/2310.08387</guid>
<content:encoded><![CDATA[
arXiv:2310.08387v3 Announce Type: replace-cross 
Abstract: Active learning strategies aim to train high-performance models with minimal labeled data by selecting the most informative instances for labeling. However, existing methods for assessing data informativeness often fail to align directly with task model performance metrics, such as mean average precision (mAP) in object detection. This paper introduces Mean-AP Guided Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness for deep detection networks, directly optimizing the sampling strategy using mAP. MGRAL employs a reinforcement learning agent based on LSTM architecture to efficiently navigate the combinatorial challenge of batch sample selection and the non-differentiable nature between performance and selected batches. The agent optimizes selection using policy gradient with mAP improvement as the reward signal. To address the computational intensity of mAP estimation with unlabeled samples, we implement fast look-up tables, ensuring real-world feasibility. We evaluate MGRAL on PASCAL VOC and MS COCO benchmarks across various backbone architectures. Our approach demonstrates strong performance, establishing a new paradigm in reinforcement learning-based active learning for object detection.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators</title>
<link>https://arxiv.org/abs/2311.00579</link>
<guid>https://arxiv.org/abs/2311.00579</guid>
<content:encoded><![CDATA[
arXiv:2311.00579v2 Announce Type: replace-cross 
Abstract: Convolutional Neural Networks (CNNs) are widely used in various domains, including image recognition, medical diagnosis and autonomous driving. Recent advances in dataflow-based CNN accelerators have enabled CNN inference in resource-constrained edge devices. These dataflow accelerators utilize inherent data reuse of convolution layers to process CNN models efficiently. Concealing the architecture of CNN models is critical for privacy and security. This article evaluates memory-based side-channel information to recover CNN architectures from dataflow-based CNN inference accelerators. The proposed attack exploits spatial and temporal data reuse of the dataflow mapping on CNN accelerators and architectural hints to recover the structure of CNN models. Experimental results demonstrate that our proposed side-channel attack can recover the structures of popular CNN models, namely, Lenet, Alexnet, VGGnet16, and YOLOv2.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Structure Representation Learning of Confounders in Latent Space for Recommendation</title>
<link>https://arxiv.org/abs/2311.03382</link>
<guid>https://arxiv.org/abs/2311.03382</guid>
<content:encoded><![CDATA[
arXiv:2311.03382v2 Announce Type: replace-cross 
Abstract: Inferring user preferences from the historical feedback of users is a valuable problem in recommender systems. Conventional approaches often rely on the assumption that user preferences in the feedback data are equivalent to the real user preferences without additional noise, which simplifies the problem modeling. However, there are various confounders during user-item interactions, such as weather and even the recommendation system itself. Therefore, neglecting the influence of confounders will result in inaccurate user preferences and suboptimal performance of the model. Furthermore, the unobservability of confounders poses a challenge in further addressing the problem. To address these issues, we refine the problem and propose a more rational solution. Specifically, we consider the influence of confounders, disentangle them from user preferences in the latent space, and employ causal graphs to model their interdependencies without specific labels. By cleverly combining local and global causal graphs, we capture the user-specificity of confounders on user preferences. We theoretically demonstrate the identifiability of the obtained causal graph. Finally, we propose our model based on Variational Autoencoders, named Causal Structure representation learning of Confounders in latent space (CSC). We conducted extensive experiments on one synthetic dataset and five real-world datasets, demonstrating the superiority of our model. Furthermore, we demonstrate that the learned causal representations of confounders are controllable, potentially offering users fine-grained control over the objectives of their recommendation lists with the learned causal graphs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Inference for Uncertainty Quantification: an Analysis of Trade-offs</title>
<link>https://arxiv.org/abs/2403.13748</link>
<guid>https://arxiv.org/abs/2403.13748</guid>
<content:encoded><![CDATA[
arXiv:2403.13748v3 Announce Type: replace-cross 
Abstract: Given an intractable distribution $p$, the problem of variational inference (VI) is to find the best approximation from some more tractable family $Q$. Commonly, one chooses $Q$ to be a family of factorized distributions (i.e., the mean-field assumption), even though~$p$ itself does not factorize. We show that this mismatch leads to an impossibility theorem: if $p$ does not factorize, then any factorized approximation $q\in Q$ can correctly estimate at most one of the following three measures of uncertainty: (i) the marginal variances, (ii) the marginal precisions, or (iii) the generalized variance (which can be related to the entropy). In practice, the best variational approximation in $Q$ is found by minimizing some divergence $D(q,p)$ between distributions, and so we ask: how does the choice of divergence determine which measure of uncertainty, if any, is correctly estimated by VI? We consider the classic Kullback-Leibler divergences, the more general $\alpha$-divergences, and a score-based divergence which compares $\nabla \log p$ and $\nabla \log q$. We provide a thorough theoretical analysis in the setting where $p$ is a Gaussian and $q$ is a (factorized) Gaussian. We show that all the considered divergences can be \textit{ordered} based on the estimates of uncertainty they yield as objective functions for~VI. Finally, we empirically evaluate the validity of this ordering when the target distribution $p$ is not Gaussian.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strong Screening Rules for Group-based SLOPE Models</title>
<link>https://arxiv.org/abs/2405.15357</link>
<guid>https://arxiv.org/abs/2405.15357</guid>
<content:encoded><![CDATA[
arXiv:2405.15357v2 Announce Type: replace-cross 
Abstract: Tuning the regularization parameter in penalized regression models is an expensive task, requiring multiple models to be fit along a path of parameters. Strong screening rules drastically reduce computational costs by lowering the dimensionality of the input prior to fitting. We develop strong screening rules for group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE and Sparse-group SLOPE. The developed rules are applicable to the wider family of group-based OWL models, including OSCAR. Our experiments on both synthetic and real data show that the screening rules significantly accelerate the fitting process. The screening rules make it accessible for group SLOPE and sparse-group SLOPE to be applied to high-dimensional datasets, particularly those encountered in genetics.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Matrix Multiplication on Quantum Computer</title>
<link>https://arxiv.org/abs/2408.03085</link>
<guid>https://arxiv.org/abs/2408.03085</guid>
<content:encoded><![CDATA[
arXiv:2408.03085v2 Announce Type: replace-cross 
Abstract: As a core underlying operation in pattern recognition and machine learning, matrix multiplication plays a crucial role in modern machine learning models and constitutes a major contributor to computational expenditure. Hence, researchers have spent decades continuously searching for more efficient matrix multiplication algorithms.This paper firstly introduces an innovative and practical approach to universal quantum matrix multiplication. We designed optimized quantum adders and multipliers based on Quantum Fourier Transform (QFT), which significantly reduced the number of gates used compared to classical adders and multipliers. Subsequently, we construct the basic universal quantum matrix multiplication and extend it to the Strassen algorithm. We conduct comparative experiments to analyze the performance of the quantum matrix multiplication and evaluate the acceleration provided by the optimized quantum adder and multiplier. Finally, we investigate the advantages of the quantum Strassen algorithm and the basic quantum matrix multiplication. Our result opens, for the first time, a reliable pathway for designing universal quantum matrix multiplication. Following this pathway, quantum computing will unlock significantly greater potential for training modern machine learning models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Struggles of LLMs in Cross-lingual Code Clone Detection</title>
<link>https://arxiv.org/abs/2408.04430</link>
<guid>https://arxiv.org/abs/2408.04430</guid>
<content:encoded><![CDATA[
arXiv:2408.04430v3 Announce Type: replace-cross 
Abstract: With the involvement of multiple programming languages in modern software development, cross-lingual code clone detection has gained traction within the software engineering community. Numerous studies have explored this topic, proposing various promising approaches. Inspired by the significant advances in machine learning in recent years, particularly Large Language Models (LLMs), which have demonstrated their ability to tackle various tasks, this paper revisits cross-lingual code clone detection. We evaluate the performance of five (05) LLMs and eight prompts (08) for the identification of cross-lingual code clones. Additionally, we compare these results against two baseline methods. Finally, we evaluate a pre-trained embedding model to assess the effectiveness of the generated representations for classifying clone and non-clone pairs. The studies involving LLMs and Embedding models are evaluated using two widely used cross-lingual datasets, XLCoST and CodeNet. Our results show that LLMs can achieve high F1 scores, up to 0.99, for straightforward programming examples. However, they not only perform less well on programs associated with complex programming challenges but also do not necessarily understand the meaning of "code clones" in a cross-lingual setting. We show that embedding models used to represent code fragments from different programming languages in the same representation space enable the training of a basic classifier that outperforms all LLMs by ~1 and ~20 percentage points on the XLCoST and CodeNet datasets, respectively. This finding suggests that, despite the apparent capabilities of LLMs, embeddings provided by embedding models offer suitable representations to achieve state-of-the-art performance in cross-lingual code clone detection.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Reinforcement Learning in Quantitative Finance: A Survey</title>
<link>https://arxiv.org/abs/2408.10932</link>
<guid>https://arxiv.org/abs/2408.10932</guid>
<content:encoded><![CDATA[
arXiv:2408.10932v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-3D Print: Large Language Models To Monitor and Control 3D Printing</title>
<link>https://arxiv.org/abs/2408.14307</link>
<guid>https://arxiv.org/abs/2408.14307</guid>
<content:encoded><![CDATA[
arXiv:2408.14307v2 Announce Type: replace-cross 
Abstract: Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM). Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods. However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality. While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability. To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects. The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters. It then generates and executes a corrective action plan. We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise. Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of Data-driven Newsvendor: Unified Analysis and Spectrum of Achievable Regrets</title>
<link>https://arxiv.org/abs/2409.03505</link>
<guid>https://arxiv.org/abs/2409.03505</guid>
<content:encoded><![CDATA[
arXiv:2409.03505v3 Announce Type: replace-cross 
Abstract: In the Newsvendor problem, the goal is to guess the number that will be drawn from some distribution, with asymmetric consequences for guessing too high vs. too low. In the data-driven version, the distribution is unknown, and one must work with samples from the distribution. Data-driven Newsvendor has been studied under many variants: additive vs. multiplicative regret, high probability vs. expectation bounds, and different distribution classes. This paper studies all combinations of these variants, filling in many gaps in the literature and simplifying many proofs. In particular, we provide a unified analysis based on the notion of clustered distributions, which in conjunction with our new lower bounds, shows that the entire spectrum of regrets between $1/\sqrt{n}$ and $1/n$ can be possible. Simulations on commonly-used distributions demonstrate that our notion is the "correct" predictor of empirical regret across varying data sizes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution</title>
<link>https://arxiv.org/abs/2410.00153</link>
<guid>https://arxiv.org/abs/2410.00153</guid>
<content:encoded><![CDATA[
arXiv:2410.00153v3 Announce Type: replace-cross 
Abstract: Probing learned concepts in large language models (LLMs) is crucial for understanding how semantic knowledge is encoded internally. Training linear classifiers on probing tasks is a principle approach to denote the vector of a certain concept in the representation space. However, the single vector identified for a concept varies with both data and training, making it less robust and weakening its effectiveness in real-world applications. To address this challenge, we propose an approach to approximate the subspace representing a specific concept. Built on linear probing classifiers, we extend the concept vectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's effectiveness through measuring its faithfulness and plausibility across multiple LLMs with different sizes and architectures. Additionally, we use representation intervention tasks to showcase its efficacy in real-world applications such as emotion steering. Experimental results indicate that GCS concept vectors have the potential to balance steering performance and maintaining the fluency in natural language generation tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Human Mobility Modeling and Anomaly Detection</title>
<link>https://arxiv.org/abs/2410.01281</link>
<guid>https://arxiv.org/abs/2410.01281</guid>
<content:encoded><![CDATA[
arXiv:2410.01281v2 Announce Type: replace-cross 
Abstract: Given the temporal GPS coordinates from a large set of human agents, how can we model their mobility behavior toward effective anomaly (e.g. bad-actor or malicious behavior) detection without any labeled data? Human mobility and trajectory modeling have been extensively studied, showcasing varying abilities to manage complex inputs and balance performance-efficiency trade-offs. In this work, we formulate anomaly detection in complex human behavior by modeling raw GPS data as a sequence of stay-point events, each characterized by spatio-temporal features, along with trips (i.e. commute) between the stay-points. Our problem formulation allows us to leverage modern sequence models for unsupervised training and anomaly detection. Notably, we equip our proposed model USTAD (for Uncertainty-aware Spatio-Temporal Anomaly Detection) with aleatoric (i.e. data) uncertainty estimation to account for inherent stochasticity in certain individuals' behavior, as well as epistemic (i.e. model) uncertainty to handle data sparsity under a large variety of human behaviors. Together, aleatoric and epistemic uncertainties unlock a robust loss function as well as uncertainty-aware decision-making in anomaly scoring. Extensive experiments shows that USTAD improves anomaly detection AUCROC by 3\%-15\% over baselines in industry-scale data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonparametric IPSS: Fast, flexible feature selection with false discovery control</title>
<link>https://arxiv.org/abs/2410.02208</link>
<guid>https://arxiv.org/abs/2410.02208</guid>
<content:encoded><![CDATA[
arXiv:2410.02208v2 Announce Type: replace-cross 
Abstract: Feature selection is a critical task in machine learning and statistics. However, existing feature selection methods either (i) rely on parametric methods such as linear or generalized linear models, (ii) lack theoretical false discovery control, or (iii) identify few true positives. Here, we introduce a general feature selection method with finite-sample false discovery control based on applying integrated path stability selection (IPSS) to arbitrary feature importance scores. The method is nonparametric whenever the importance scores are nonparametric, and it estimates q-values, which are better suited to high-dimensional data than p-values. We focus on two special cases using importance scores from gradient boosting (IPSSGB) and random forests (IPSSRF). Extensive nonlinear simulations with RNA sequencing data show that both methods accurately control the false discovery rate and detect more true positives than existing methods. Both methods are also efficient, running in under 20 seconds when there are 500 samples and 5000 features. We apply IPSSGB and IPSSRF to detect microRNAs and genes related to cancer, finding that they yield better predictions with fewer features than existing approaches.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Queueing Matching Bandits with Preference Feedback</title>
<link>https://arxiv.org/abs/2410.10098</link>
<guid>https://arxiv.org/abs/2410.10098</guid>
<content:encoded><![CDATA[
arXiv:2410.10098v2 Announce Type: replace-cross 
Abstract: In this study, we consider multi-class multi-server asymmetric queueing systems consisting of $N$ queues on one side and $K$ servers on the other side, where jobs randomly arrive in queues at each time. The service rate of each job-server assignment is unknown and modeled by a feature-based Multi-nomial Logit (MNL) function. At each time, a scheduler assigns jobs to servers, and each server stochastically serves at most one job based on its preferences over the assigned jobs. The primary goal of the algorithm is to stabilize the queues in the system while learning the service rates of servers. To achieve this goal, we propose algorithms based on UCB and Thompson Sampling, which achieve system stability with an average queue length bound of $O(\min\{N,K\}/\epsilon)$ for a large time horizon $T$, where $\epsilon$ is a traffic slackness of the system. Furthermore, the algorithms achieve sublinear regret bounds of $\tilde{O}(\min\{\sqrt{T} Q_{\max},T^{3/4}\})$, where $Q_{\max}$ represents the maximum queue length over agents and times. Lastly, we provide experimental results to demonstrate the performance of our algorithms.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-agnostic basis functions for the 2-point correlation function of dark matter in linear theory</title>
<link>https://arxiv.org/abs/2410.21374</link>
<guid>https://arxiv.org/abs/2410.21374</guid>
<content:encoded><![CDATA[
arXiv:2410.21374v2 Announce Type: replace-cross 
Abstract: We consider approximating the linearly evolved 2-point correlation function (2pcf) of dark matter $\xi_{\rm lin}(r;\boldsymbol{\theta})$ in a cosmological model with parameters $\boldsymbol{\theta}$ as the linear combination $\xi_{\rm lin}(r;\boldsymbol{\theta})\approx\sum_i\,b_i(r)\,w_i(\boldsymbol{\theta})$, where the functions $\mathcal{B}=\{b_i(r)\}$ form a $\textit{model-agnostic basis}$ for the linear 2pcf. This decomposition is important for model-agnostic analyses of the baryon acoustic oscillation (BAO) feature in the nonlinear 2pcf of galaxies that fix $\mathcal{B}$ and leave the coefficients $\{w_i\}$ free. To date, such analyses have made simple but sub-optimal choices for $\mathcal{B}$, such as monomials. We develop a machine learning framework for systematically discovering a $\textit{minimal}$ basis $\mathcal{B}$ that describes $\xi_{\rm lin}(r)$ near the BAO feature in a wide class of cosmological models. We use a custom architecture, denoted $\texttt{BiSequential}$, for a neural network (NN) that explicitly realizes the separation between $r$ and $\boldsymbol{\theta}$ above. The optimal NN trained on data in which only $\{\Omega_{\rm m},h\}$ are varied in a $\textit{flat}$ $\Lambda$CDM model produces a basis $\mathcal{B}$ comprising $9$ functions capable of describing $\xi_{\rm lin}(r)$ to $\sim0.6\%$ accuracy in $\textit{curved}$ $w$CDM models varying 7 parameters within $\sim5\%$ of their fiducial, flat $\Lambda$CDM values. Scales such as the peak, linear point and zero-crossing of $\xi_{\rm lin}(r)$ are also recovered with very high accuracy. We compare our approach to other compression schemes in the literature, and speculate that $\mathcal{B}$ may also encompass $\xi_{\rm lin}(r)$ in modified gravity models near our fiducial $\Lambda$CDM model. Using our basis functions in model-agnostic BAO analyses can potentially lead to significant statistical gains.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAMMAL -- Molecular Aligned Multi-Modal Architecture and Language</title>
<link>https://arxiv.org/abs/2410.22367</link>
<guid>https://arxiv.org/abs/2410.22367</guid>
<content:encoded><![CDATA[
arXiv:2410.22367v3 Announce Type: replace-cross 
Abstract: Large language models applied to vast biological datasets have the potential to transform biology by uncovering disease mechanisms and accelerating drug development. However, current models are often siloed, trained separately on small-molecules, proteins, or transcriptomic data, limiting their ability to capture complex, multi-modal interactions. Effective drug discovery requires computational tools that integrate multiple biological entities while supporting prediction and generation, a challenge existing models struggle to address. For this purpose, we present MAMMAL - Molecular Aligned Multi-Modal Architecture and Language - a versatile method applied to create a multi-task foundation model that learns from large-scale biological datasets across diverse modalities, including proteins, small-molecules, and omics. MAMMAL's structured prompt syntax supports classification, regression, and generation tasks while handling token and scalar inputs and outputs. Evaluated on eleven diverse downstream tasks, it reaches a new state of the art (SOTA) in nine tasks and is comparable to SOTA in two tasks, all within a unified architecture, unlike prior task-specific models. Additionally, we explored Alphafold 3 binding prediction capabilities on antibody-antigen and nanobody-antigen complexes showing significantly better classification performance of MAMMAL in 3 out of 4 targets. The model code and pretrained weights are publicly available at https://github.com/BiomedSciAI/biomed-multi-alignment and https://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precision Glass Thermoforming Assisted by Neural Networks</title>
<link>https://arxiv.org/abs/2411.06762</link>
<guid>https://arxiv.org/abs/2411.06762</guid>
<content:encoded><![CDATA[
arXiv:2411.06762v2 Announce Type: replace-cross 
Abstract: Many glass products require thermoformed geometry with high precision. However, the traditional approach of developing a thermoforming process through trials and errors can cause large waste of time and resources and often end up with unsuccessfulness. Hence, there is a need to develop an efficient predictive model, replacing the costly simulations or experiments, to assist the design of precision glass thermoforming. In this work, we report a surrogate model, based on a dimensionless back-propagation neural network (BPNN), that can adequately predict the form errors and thus compensate for these errors in mold design using geometric features and process parameters as inputs. Our trials with simulation and industrial data indicate that the surrogate model can predict forming errors with adequate accuracy. Although perception errors (mold designers' decisions) and mold fabrication errors make the industrial training data less reliable than simulation data, our preliminary training and testing results still achieved a reasonable consistency with industrial data, suggesting that the surrogate models are directly implementable in the glass-manufacturing industry.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Autonomous Virtualization System for Two-Dimensional Semiconductor Quantum Dot Arrays</title>
<link>https://arxiv.org/abs/2411.12516</link>
<guid>https://arxiv.org/abs/2411.12516</guid>
<content:encoded><![CDATA[
arXiv:2411.12516v2 Announce Type: replace-cross 
Abstract: Arrays of gate-defined semiconductor quantum dots are among the leading candidates for building scalable quantum processors. High-fidelity initialization, control, and readout of spin qubit registers require exquisite and targeted control over key Hamiltonian parameters that define the electrostatic environment. However, due to the tight gate pitch, capacitive crosstalk between gates hinders independent tuning of chemical potentials and interdot couplings. While virtual gates offer a practical solution, determining all the required cross-capacitance matrices accurately and efficiently in large quantum dot registers is an open challenge. Here, we establish a modular automated virtualization system (MAViS) -- a general and modular framework for autonomously constructing a complete stack of multilayer virtual gates in real time. Our method employs machine learning techniques to rapidly extract features from two-dimensional charge stability diagrams. We then utilize computer vision and regression models to self-consistently determine all relative capacitive couplings necessary for virtualizing plunger and barrier gates in both low- and high-tunnel-coupling regimes. Using MAViS, we successfully demonstrate accurate virtualization of a dense two-dimensional array comprising ten quantum dots defined in a high-quality Ge/SiGe heterostructure. Our work offers an elegant and practical solution for the efficient control of large-scale semiconductor quantum dot systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>About Time: Advances, Challenges, and Outlooks of Action Understanding</title>
<link>https://arxiv.org/abs/2411.15106</link>
<guid>https://arxiv.org/abs/2411.15106</guid>
<content:encoded><![CDATA[
arXiv:2411.15106v2 Announce Type: replace-cross 
Abstract: We have witnessed impressive advances in video action understanding. Increased dataset sizes, variability, and computation availability have enabled leaps in performance and task diversification. Current systems can provide coarse- and fine-grained descriptions of video scenes, extract segments corresponding to queries, synthesize unobserved parts of videos, and predict context across multiple modalities. This survey comprehensively reviews advances in uni- and multi-modal action understanding across a range of tasks. We focus on prevalent challenges, overview widely adopted datasets, and survey seminal works with an emphasis on recent advances. We broadly distinguish between three temporal scopes: (1) recognition tasks of actions observed in full, (2) prediction tasks for ongoing partially observed actions, and (3) forecasting tasks for subsequent unobserved action(s). This division allows us to identify specific action modeling and video representation challenges. Finally, we outline future directions to address current shortcomings.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator for Exposure Correction</title>
<link>https://arxiv.org/abs/2411.15255</link>
<guid>https://arxiv.org/abs/2411.15255</guid>
<content:encoded><![CDATA[
arXiv:2411.15255v2 Announce Type: replace-cross 
Abstract: Exposure correction is a fundamental problem in computer vision and image processing. Recently, frequency domain-based methods have achieved impressive improvement, yet they still struggle with complex real-world scenarios under extreme exposure conditions. This is due to the local convolutional receptive fields failing to model long-range dependencies in the spectrum, and the non-generative learning paradigm being inadequate for retrieving lost details from severely degraded regions. In this paper, we propose Omnidirectional Spectral Mamba (OSMamba), a novel exposure correction network that incorporates the advantages of state space models and generative diffusion models to address these limitations. Specifically, OSMamba introduces an omnidirectional spectral scanning mechanism that adapts Mamba to the frequency domain to capture comprehensive long-range dependencies in both the amplitude and phase spectra of deep image features, hence enhancing illumination correction and structure recovery. Furthermore, we develop a dual-domain prior generator that learns from well-exposed images to generate a degradation-free diffusion prior containing correct information about severely under- and over-exposed regions for better detail restoration. Extensive experiments on multiple-exposure and mixed-exposure datasets demonstrate that the proposed OSMamba achieves state-of-the-art performance both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variable-Speed Teaching-Playback as Real-World Data Augmentation for Imitation Learning</title>
<link>https://arxiv.org/abs/2412.03252</link>
<guid>https://arxiv.org/abs/2412.03252</guid>
<content:encoded><![CDATA[
arXiv:2412.03252v2 Announce Type: replace-cross 
Abstract: Because imitation learning relies on human demonstrations in hard-to-simulate settings, the inclusion of force control in this method has resulted in a shortage of training data, even with a simple change in speed. Although the field of data augmentation has addressed the lack of data, conventional methods of data augmentation for robot manipulation are limited to simulation-based methods or downsampling for position control. This paper proposes a novel method of data augmentation that is applicable to force control and preserves the advantages of real-world datasets. We applied teaching-playback at variable speeds as real-world data augmentation to increase both the quantity and quality of environmental reactions at variable speeds. An experiment was conducted on bilateral control-based imitation learning using a method of imitation learning equipped with position-force control. We evaluated the effect of real-world data augmentation on two tasks, pick-and-place and wiping, at variable speeds, each from two human demonstrations at fixed speed. The results showed a maximum 55% increase in success rate from a simple change in speed of real-world reactions and improved accuracy along the duration/frequency command by gathering environmental reactions at variable speeds.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synergizing Large Language Models and Task-specific Models for Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2501.05675</link>
<guid>https://arxiv.org/abs/2501.05675</guid>
<content:encoded><![CDATA[
arXiv:2501.05675v4 Announce Type: replace-cross 
Abstract: In anomaly detection, methods based on large language models (LLMs) can incorporate expert knowledge by reading professional document, while task-specific small models excel at extracting normal data patterns and detecting value fluctuations from training data of target applications. Inspired by the human nervous system, where the brain stores expert knowledge and the peripheral nervous system and spinal cord handle specific tasks like withdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to facilitate collaboration between LLMs and task-specific models, leveraging the strengths of both models for anomaly detection.
  In particular, we first formulate the collaboration process and identify two key challenges in the collaboration:
  (1) the misalignment between the expression domains of the LLMs and task-specific small models, and (2) error accumulation arising from the predictions of both models.
  To address these challenges, we then introduce two key components in CoLLaTe: a model alignment module and a collaborative loss function. Through theoretical analysis and experimental validation, we demonstrate that these components effectively mitigate the identified challenges and achieve better performance than both LLM-based and task-specific models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lilan: A linear latent network approach for real-time solutions of stiff, nonlinear, ordinary differential equations</title>
<link>https://arxiv.org/abs/2501.08423</link>
<guid>https://arxiv.org/abs/2501.08423</guid>
<content:encoded><![CDATA[
arXiv:2501.08423v3 Announce Type: replace-cross 
Abstract: Solving stiff ordinary differential equations (StODEs) requires sophisticated numerical solvers, which are often computationally expensive. In particular, StODE's often cannot be solved with traditional explicit time integration schemes and one must resort to costly implicit methods to compute solutions. On the other hand, state-of-the-art machine learning (ML) based methods such as Neural ODE (NODE) poorly handle the timescale separation of various elements of the solutions to StODEs and require expensive implicit solvers for integration at inference time. In this work, we embark on a different path which involves learning a latent dynamics for StODEs, in which one completely avoids numerical integration. To that end, we consider a constant velocity latent dynamical system whose solution is a sequence of straight lines. Given the initial condition and parameters of the ODE, the encoder networks learn the slope (i.e the constant velocity) and the initial condition for the latent dynamics. In other words, the solution of the original dynamics is encoded into a sequence of straight lines which can be decoded back to retrieve the actual solution as and when required. Another key idea in our approach is a nonlinear transformation of time, which allows for the "stretching/squeezing" of time in the latent space, thereby allowing for varying levels of attention to different temporal regions in the solution. Additionally, we provide a simple universal-approximation-type proof showing that our approach can approximate the solution of stiff nonlinear system on a compact set to any degree of accuracy, {\epsilon}. We show that the dimension of the latent dynamical system in our approach is independent of {\epsilon}. Numerical investigation on prototype StODEs suggest that our method outperforms state-of-the art machine learning approaches for handling StODEs.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-reflecting Large Language Models: A Hegelian Dialectical Approach</title>
<link>https://arxiv.org/abs/2501.14917</link>
<guid>https://arxiv.org/abs/2501.14917</guid>
<content:encoded><![CDATA[
arXiv:2501.14917v4 Announce Type: replace-cross 
Abstract: Investigating NLP through a philosophical lens has recently caught researcher's eyes as it connects computational methods with classical schools of philosophy. This paper introduces a philosophical approach inspired by the \textit{Hegelian Dialectic} for LLMs' \textit{self-reflection}, utilizing a self-dialectical approach to emulate internal critiques and then synthesize new ideas by resolving the opposing points of view. Moreover, this paper investigates the effect of LLMs' temperature for generation by establishing a dynamic annealing approach, which promotes the creativity in the early stages and gradually refines it by focusing on the nuances, as well as a fixed-temperature strategy for generation. We assess the effectiveness of our proposed method in generating novel ideas and in improving the reasoning abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent Majority Voting (MAMV) strategy to assess the validity and novelty of the generated ideas, which proves useful in the absence of domain experts. Our experiments demonstrate promising results in generating ideas and enhancing problem-solving performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Transport-based Conformal Prediction</title>
<link>https://arxiv.org/abs/2501.18991</link>
<guid>https://arxiv.org/abs/2501.18991</guid>
<content:encoded><![CDATA[
arXiv:2501.18991v2 Announce Type: replace-cross 
Abstract: Conformal Prediction (CP) is a principled framework for quantifying uncertainty in blackbox learning models, by constructing prediction sets with finite-sample coverage guarantees. Traditional approaches rely on scalar nonconformity scores, which fail to fully exploit the geometric structure of multivariate outputs, such as in multi-output regression or multiclass classification. Recent methods addressing this limitation impose predefined convex shapes for the prediction sets, potentially misaligning with the intrinsic data geometry. We introduce a novel CP procedure handling multivariate score functions through the lens of optimal transport. Specifically, we leverage Monge-Kantorovich vector ranks and quantiles to construct prediction region with flexible, potentially non-convex shapes, better suited to the complex uncertainty patterns encountered in multivariate learning tasks. We prove that our approach ensures finite-sample, distribution-free coverage properties, similar to typical CP methods. We then adapt our method for multi-output regression and multiclass classification, and also propose simple adjustments to generate adaptive prediction regions with asymptotic conditional coverage guarantees. Finally, we evaluate our method on practical regression and classification problems, illustrating its advantages in terms of (conditional) coverage and efficiency.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Neural Network for Phonon-Assisted Optical Spectra in Semiconductors</title>
<link>https://arxiv.org/abs/2502.00798</link>
<guid>https://arxiv.org/abs/2502.00798</guid>
<content:encoded><![CDATA[
arXiv:2502.00798v2 Announce Type: replace-cross 
Abstract: Ab initio based accurate simulation of phonon-assisted optical spectra of semiconductors at finite temperatures remains a formidable challenge, as it requires large supercells for phonon sampling and computationally expensive high-accuracy exchange-correlation (XC) functionals. In this work, we present an efficient approach that combines deep learning tight-binding and potential models to address this challenge with ab initio fidelity. By leveraging molecular dynamics for atomic configuration sampling and deep learning-enabled rapid Hamiltonian evaluation, our approach enables large-scale simulations of temperature-dependent optical properties using advanced XC functionals (HSE, SCAN). Demonstrated on silicon and gallium arsenide across temperature 100-400 K, the method accurately captures phonon-induced bandgap renormalization and indirect/direct absorption processes which are in excellent agreement with experimental findings over five orders of magnitude. This work establishes a pathway for high-throughput investigation of electron-phonon coupled phenomena in complex materials, overcoming traditional computational limitations arising from large supercell used with computationally expensive XC-functionals.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting potentially abusive clauses in Chilean terms of services with natural language processing</title>
<link>https://arxiv.org/abs/2502.00865</link>
<guid>https://arxiv.org/abs/2502.00865</guid>
<content:encoded><![CDATA[
arXiv:2502.00865v2 Announce Type: replace-cross 
Abstract: This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and/or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-based Domain Randomization for Learning and Sequencing Robotic Skills</title>
<link>https://arxiv.org/abs/2502.01800</link>
<guid>https://arxiv.org/abs/2502.01800</guid>
<content:encoded><![CDATA[
arXiv:2502.01800v2 Announce Type: replace-cross 
Abstract: Domain randomization in reinforcement learning is an established technique for increasing the robustness of control policies trained in simulation. By randomizing environment properties during training, the learned policy can become robust to uncertainties along the randomized dimensions. While the environment distribution is typically specified by hand, in this paper we investigate automatically discovering a sampling distribution via entropy-regularized reward maximization of a normalizing-flow-based neural sampling distribution. We show that this architecture is more flexible and provides greater robustness than existing approaches that learn simpler, parameterized sampling distributions, as demonstrated in six simulated and one real-world robotics domain. Lastly, we explore how these learned sampling distributions, combined with a privileged value function, can be used for out-of-distribution detection in an uncertainty-aware multi-step manipulation planner.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taking a Big Step: Large Learning Rates in Denoising Score Matching Prevent Memorization</title>
<link>https://arxiv.org/abs/2502.03435</link>
<guid>https://arxiv.org/abs/2502.03435</guid>
<content:encoded><![CDATA[
arXiv:2502.03435v2 Announce Type: replace-cross 
Abstract: Denoising score matching plays a pivotal role in the performance of diffusion-based generative models. However, the empirical optimal score--the exact solution to the denoising score matching--leads to memorization, where generated samples replicate the training data. Yet, in practice, only a moderate degree of memorization is observed, even without explicit regularization. In this paper, we investigate this phenomenon by uncovering an implicit regularization mechanism driven by large learning rates. Specifically, we show that in the small-noise regime, the empirical optimal score exhibits high irregularity. We then prove that, when trained by stochastic gradient descent with a large enough learning rate, neural networks cannot stably converge to a local minimum with arbitrarily small excess risk. Consequently, the learned score cannot be arbitrarily close to the empirical optimal score, thereby mitigating memorization. To make the analysis tractable, we consider one-dimensional data and two-layer neural networks. Experiments validate the crucial role of the learning rate in preventing memorization, even beyond the one-dimensional setting.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Memory and Material Dependent Constitutive Laws</title>
<link>https://arxiv.org/abs/2502.05463</link>
<guid>https://arxiv.org/abs/2502.05463</guid>
<content:encoded><![CDATA[
arXiv:2502.05463v2 Announce Type: replace-cross 
Abstract: We propose and study a neural operator framework for learning memory- and material microstructure-dependent constitutive laws for heterogeneous materials. We work in the two-scale setting where homogenization theory provides a systematic approach to deriving macroscale constitutive laws, obviating the need to resolve complex microstructure repeatedly. However, the unit cell problems defining these constitutive models are typically not amenable to explicit evaluation. It is therefore of interest to learn constitutive models from data generated by the unit cell problem. Our proposed framework models homogenized constitutive laws with both memory- and microstructure-dependence through the use of Markovian recurrent and Fourier neural operators. The homogenization problem for Kelvin-Voigt viscoelastic materials is studied to provide firm theoretical foundations for our model. The theoretical properties of the cell problem in this Kelvin-Voigt setting motivate the proposed learning framework; and are also used to prove a universal approximation theorem for the learned macroscale constitutive model. Numerical experiments show that the proposed learning framework accurately learns memory- and microstructure-dependent viscoelastic and elasto-viscoplastic constitutive models, beyond the setting of the theory. Furthermore, we show that the learned constitutive models can be successfully deployed in macroscale simulation of material deformation for different microstructures without retraining.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models</title>
<link>https://arxiv.org/abs/2502.07328</link>
<guid>https://arxiv.org/abs/2502.07328</guid>
<content:encoded><![CDATA[
arXiv:2502.07328v3 Announce Type: replace-cross 
Abstract: The advent of Music-Language Models has greatly enhanced the automatic music generation capability of AI systems, but they are also limited in their coverage of the musical genres and cultures of the world. We present a study of the datasets and research papers for music generation and quantify the bias and under-representation of genres. We find that only 5.7% of the total hours of existing music datasets come from non-Western genres, which naturally leads to disparate performance of the models across genres. We then investigate the efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating this bias. Our experiments with two popular models -- MusicGen and Mustango, for two underrepresented non-Western music traditions -- Hindustani Classical and Turkish Makam music, highlight the promises as well as the non-triviality of cross-genre adaptation of music through small datasets, implying the need for more equitable baseline music-language models that are designed for cross-cultural transfer learning.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel computational workflows for natural and biomedical image processing based on hypercomplex algebras</title>
<link>https://arxiv.org/abs/2502.07758</link>
<guid>https://arxiv.org/abs/2502.07758</guid>
<content:encoded><![CDATA[
arXiv:2502.07758v5 Announce Type: replace-cross 
Abstract: Hypercomplex image processing extends conventional techniques in a unified paradigm encompassing algebraic and geometric principles. This work leverages quaternions and the two-dimensional orthogonal planes split framework (splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D planes) for natural/biomedical image analysis through the following computational workflows and outcomes: natural/biomedical image re-colorization, natural image de-colorization, natural/biomedical image contrast enhancement, computational re-staining and stain separation in histological images, and performance gains in machine/deep learning pipelines for histological images. The workflows are analyzed separately for natural and biomedical images to showcase the effectiveness of the proposed approaches. The proposed workflows can regulate color appearance (e.g. with alternative renditions and grayscale conversion) and image contrast, be part of automated image processing pipelines (e.g. isolating stain components, boosting learning models), and assist in digital pathology applications (e.g. enhancing biomarker visibility, enabling colorblind-friendly renditions). Employing only basic arithmetic and matrix operations, this work offers a computationally accessible methodology - in the hypercomplex domain - that showcases versatility and consistency across image processing tasks and a range of computer vision and biomedical applications. The proposed non-data-driven methods achieve comparable or better results (particularly in cases involving well-known methods) to those reported in the literature, showcasing the potential of robust theoretical frameworks with practical effectiveness. Results, methods, and limitations are detailed alongside discussion of promising extensions, emphasizing the potential of feature-rich mathematical/computational frameworks for natural and biomedical images.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Learning of CATE with Kernel Ridge Regression</title>
<link>https://arxiv.org/abs/2502.11331</link>
<guid>https://arxiv.org/abs/2502.11331</guid>
<content:encoded><![CDATA[
arXiv:2502.11331v2 Announce Type: replace-cross 
Abstract: The proliferation of data has sparked significant interest in leveraging findings from one study to estimate treatment effects in a different target population without direct outcome observations. However, the transfer learning process is frequently hindered by substantial covariate shift and limited overlap between (i) the source and target populations, as well as (ii) the treatment and control groups within the source. We propose a novel method for overlap-adaptive transfer learning of conditional average treatment effect (CATE) using kernel ridge regression (KRR). Our approach involves partitioning the labeled source data into two subsets. The first one is used to train candidate CATE models based on regression adjustment and pseudo-outcomes. An optimal model is then selected using the second subset and unlabeled target data, employing another pseudo-outcome-based strategy. We provide a theoretical justification for our method through sharp non-asymptotic MSE bounds, highlighting its adaptivity to both weak overlaps and the complexity of CATE function. Extensive numerical studies confirm that our method achieves superior finite-sample efficiency and adaptability. We conclude by demonstrating the effectiveness of our approach using a 401(k) eligibility dataset.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
arXiv:2502.13685v2 Announce Type: replace-cross 
Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain's ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent Diffusion Policies</title>
<link>https://arxiv.org/abs/2502.18438</link>
<guid>https://arxiv.org/abs/2502.18438</guid>
<content:encoded><![CDATA[
arXiv:2502.18438v2 Announce Type: replace-cross 
Abstract: In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in Teams), a new framework for generating ToM-conditioned trajectories. It combines a meta-learning mechanism, that performs ToM reasoning over teammates' underlying goals and future behavior, with a multiagent denoising-diffusion model, that generates plans for an agent and its teammates conditioned on both the agent's goals and its teammates' characteristics, as computed via ToM. We implemented an online planning system that dynamically samples new trajectories (replans) from the diffusion model whenever it detects a divergence between a previously generated plan and the current state of the world. We conducted several experiments using ToMCAT in a simulated cooking domain. Our results highlight the importance of the dynamic replanning mechanism in reducing the usage of resources without sacrificing team performance. We also show that recent observations about the world and teammates' behavior collected by an agent over the course of an episode combined with ToM inferences are crucial to generate team-aware plans for dynamic adaptation to teammates, especially when no prior information is provided about them.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Configuration-Space Barriers for Manipulation Planning and Control</title>
<link>https://arxiv.org/abs/2503.04929</link>
<guid>https://arxiv.org/abs/2503.04929</guid>
<content:encoded><![CDATA[
arXiv:2503.04929v2 Announce Type: replace-cross 
Abstract: Planning and control for high-dimensional robot manipulators in cluttered, dynamic environments require both computational efficiency and robust safety guarantees. Inspired by recent advances in learning configuration-space distance functions (CDFs) as robot body representations, we propose a unified framework for motion planning and control that formulates safety constraints as CDF barriers. A CDF barrier approximates the local free configuration space, substantially reducing the number of collision-checking operations during motion planning. However, learning a CDF barrier with a neural network and relying on online sensor observations introduce uncertainties that must be considered during control synthesis. To address this, we develop a distributionally robust CDF barrier formulation for control that explicitly accounts for modeling errors and sensor noise without assuming a known underlying distribution. Simulations and hardware experiments on a 6-DoF xArm manipulator show that our neural CDF barrier formulation enables efficient planning and robust real-time safe control in cluttered and dynamic environments, relying only on onboard point-cloud observations.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lessons from the trenches on evaluating machine-learning systems in materials science</title>
<link>https://arxiv.org/abs/2503.10837</link>
<guid>https://arxiv.org/abs/2503.10837</guid>
<content:encoded><![CDATA[
arXiv:2503.10837v2 Announce Type: replace-cross 
Abstract: Measurements are fundamental to knowledge creation in science, enabling consistent sharing of findings and serving as the foundation for scientific discovery. As machine learning systems increasingly transform scientific fields, the question of how to effectively evaluate these systems becomes crucial for ensuring reliable progress.
  In this review, we examine the current state and future directions of evaluation frameworks for machine learning in science. We organize the review around a broadly applicable framework for evaluating machine learning systems through the lens of statistical measurement theory, using materials science as our primary context for examples and case studies. We identify key challenges common across machine learning evaluation such as construct validity, data quality issues, metric design limitations, and benchmark maintenance problems that can lead to phantom progress when evaluation frameworks fail to capture real-world performance needs.
  By examining both traditional benchmarks and emerging evaluation approaches, we demonstrate how evaluation choices fundamentally shape not only our measurements but also research priorities and scientific progress. These findings reveal the critical need for transparency in evaluation design and reporting, leading us to propose evaluation cards as a structured approach to documenting measurement choices and limitations.
  Our work highlights the importance of developing a more diverse toolbox of evaluation techniques for machine learning in materials science, while offering insights that can inform evaluation practices in other scientific domains where similar challenges exist.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-Values Expand the Scope of Conformal Prediction</title>
<link>https://arxiv.org/abs/2503.13050</link>
<guid>https://arxiv.org/abs/2503.13050</guid>
<content:encoded><![CDATA[
arXiv:2503.13050v3 Announce Type: replace-cross 
Abstract: Conformal prediction is a powerful framework for distribution-free uncertainty quantification. The standard approach to conformal prediction relies on comparing the ranks of prediction scores: under exchangeability, the rank of a future test point cannot be too extreme relative to a calibration set. This rank-based method can be reformulated in terms of p-values. In this paper, we explore an alternative approach based on e-values, known as conformal e-prediction. E-values offer key advantages that cannot be achieved with p-values, enabling new theoretical and practical capabilities. In particular, we present three applications that leverage the unique strengths of e-values: batch anytime-valid conformal prediction, fixed-size conformal sets with data-dependent coverage, and conformal prediction under ambiguous ground truth. Overall, these examples demonstrate that e-value-based constructions provide a flexible expansion of the toolbox of conformal prediction.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Human-Machine Teaming: Concepts, Challenges, and Applications</title>
<link>https://arxiv.org/abs/2503.16518</link>
<guid>https://arxiv.org/abs/2503.16518</guid>
<content:encoded><![CDATA[
arXiv:2503.16518v2 Announce Type: replace-cross 
Abstract: Human-Machine Teaming (HMT) is revolutionizing collaboration across domains such as defense, healthcare, and autonomous systems by integrating AI-driven decision-making, trust calibration, and adaptive teaming. This survey presents a comprehensive taxonomy of HMT, analyzing theoretical models, including reinforcement learning, instance-based learning, and interdependence theory, alongside interdisciplinary methodologies. Unlike prior reviews, we examine team cognition, ethical AI, multi-modal interactions, and real-world evaluation frameworks. Key challenges include explainability, role allocation, and scalable benchmarking. We propose future research in cross-domain adaptation, trust-aware AI, and standardized testbeds. By bridging computational and social sciences, this work lays a foundation for resilient, ethical, and scalable HMT systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment</title>
<link>https://arxiv.org/abs/2503.18991</link>
<guid>https://arxiv.org/abs/2503.18991</guid>
<content:encoded><![CDATA[
arXiv:2503.18991v2 Announce Type: replace-cross 
Abstract: The alignment of large language models (LLMs) with human values remains critical yet hindered by four key challenges: (1) scarcity of balanced safety datasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to shallow alignment, and (4) inability to dynamically adapt rewards according to task difficulty. To address these limitations, we introduce HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a novel alignment approach inspired by shadow models in membership inference attacks. Our approach consists of two main components: (1) construction of a balanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using structured prompts that leverage the introspective reasoning capabilities of LLMs; and (2) training of category-specific reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization to task difficulty at both the data and model levels. Comprehensive experiments across four harmlessness and four usefulness benchmarks demonstrate that HAIR achieves state-of-the-art performance, outperforming all baseline methods in safety while maintaining high levels of usefulness.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coverage-Guaranteed Speech Emotion Recognition via Calibrated Uncertainty-Adaptive Prediction Sets</title>
<link>https://arxiv.org/abs/2503.22712</link>
<guid>https://arxiv.org/abs/2503.22712</guid>
<content:encoded><![CDATA[
arXiv:2503.22712v3 Announce Type: replace-cross 
Abstract: Road rage, often triggered by emotional suppression and sudden outbursts, significantly threatens road safety by causing collisions and aggressive behavior. Speech emotion recognition technologies can mitigate this risk by identifying negative emotions early and issuing timely alerts. However, current SER methods, such as those based on hidden markov models and Long short-term memory networks, primarily handle one-dimensional signals, frequently experience overfitting, and lack calibration, limiting their safety-critical effectiveness. We propose a novel risk-controlled prediction framework providing statistically rigorous guarantees on prediction accuracy. This approach employs a calibration set to define a binary loss function indicating whether the true label is included in the prediction set. Using a data-driven threshold $\beta$, we optimize a joint loss function to maintain an expected test loss bounded by a user-specified risk level $\alpha$. Evaluations across six baseline models and two benchmark datasets demonstrate our framework consistently achieves a minimum coverage of $1 - \alpha$, effectively controlling marginal error rates despite varying calibration-test split ratios (e.g., 0.1). The robustness and generalizability of the framework are further validated through an extension to small-batch online calibration under a local exchangeability assumption. We construct a non-negative test martingale to maintain prediction validity even in dynamic and non-exchangeable environments. Cross-dataset tests confirm our method's ability to uphold reliable statistical guarantees in realistic, evolving data scenarios.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MARIOH: Multiplicity-Aware Hypergraph Reconstruction</title>
<link>https://arxiv.org/abs/2504.00522</link>
<guid>https://arxiv.org/abs/2504.00522</guid>
<content:encoded><![CDATA[
arXiv:2504.00522v2 Announce Type: replace-cross 
Abstract: Hypergraphs offer a powerful framework for modeling higher-order interactions that traditional pairwise graphs cannot fully capture. However, practical constraints often lead to their simplification into projected graphs, resulting in substantial information loss and ambiguity in representing higher-order relationships. In this work, we propose MARIOH, a supervised approach for reconstructing the original hypergraph from its projected graph by leveraging edge multiplicity. To overcome the difficulties posed by the large search space, MARIOH integrates several key ideas: (a) identifying provable size-2 hyperedges, which reduces the candidate search space, (b) predicting the likelihood of candidates being hyperedges by utilizing both structural and multiplicity-related features, and (c) not only targeting promising hyperedge candidates but also examining less confident ones to explore alternative possibilities. Together, these ideas enable MARIOH to efficiently and effectively explore the search space. In our experiments using 10 real-world datasets, MARIOH achieves up to 74.51% higher reconstruction accuracy compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catch Me if You Search: When Contextual Web Search Results Affect the Detection of Hallucinations</title>
<link>https://arxiv.org/abs/2504.01153</link>
<guid>https://arxiv.org/abs/2504.01153</guid>
<content:encoded><![CDATA[
arXiv:2504.01153v3 Announce Type: replace-cross 
Abstract: While we increasingly rely on large language models (LLMs) for various tasks, these models are known to produce inaccurate content or `hallucinations' with potentially disastrous consequences. The recent integration of web search results into LLMs prompts the question of whether people utilize them to verify the generated content, thereby accurately detecting hallucinations. An online experiment (N = 560) investigated how the provision of search results, either static (i.e., fixed search results provided by LLM) or dynamic (i.e., participant-led searches), affects participants' perceived accuracy of LLM-generated content (i.e., genuine, minor hallucination, major hallucination), self-confidence in accuracy ratings, as well as their overall evaluation of the LLM, as compared to the control condition (i.e., no search results). Results showed that participants in both static and dynamic conditions (vs. control) rated hallucinated content to be less accurate and perceived the LLM more negatively. However, those in the dynamic condition rated genuine content as more accurate and demonstrated greater overall self-confidence in their assessments than those in the static search or control conditions. We highlighted practical implications of incorporating web search functionality into LLMs in real-world contexts.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval</title>
<link>https://arxiv.org/abs/2504.17529</link>
<guid>https://arxiv.org/abs/2504.17529</guid>
<content:encoded><![CDATA[
arXiv:2504.17529v2 Announce Type: replace-cross 
Abstract: Online community platforms require dynamic personalized retrieval and recommendation that can continuously adapt to evolving user interests and new documents. However, optimizing models to handle such changes in real-time remains a major challenge in large-scale industrial settings. To address this, we propose the Interest-aware Representation and Alignment (IRA) framework, an efficient and scalable approach that dynamically adapts to new interactions through a cumulative structure. IRA leverages two key mechanisms: (1) Interest Units that capture diverse user interests as contextual texts, while reinforcing or fading over time through cumulative updates, and (2) a retrieval process that measures the relevance between Interest Units and documents based solely on semantic relationships, eliminating dependence on click signals to mitigate temporal biases. By integrating cumulative Interest Unit updates with the retrieval process, IRA continuously adapts to evolving user preferences, ensuring robust and fine-grained personalization without being constrained by past training distributions. We validate the effectiveness of IRA through extensive experiments on real-world datasets, including its deployment in the Home Section of NAVER's CAFE, South Korea's leading community platform.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G</title>
<link>https://arxiv.org/abs/2504.17938</link>
<guid>https://arxiv.org/abs/2504.17938</guid>
<content:encoded><![CDATA[
arXiv:2504.17938v2 Announce Type: replace-cross 
Abstract: The Quality of Experience (QoE) is the users satisfaction while streaming a video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube reflects the smooth streaming session without any buffering and quality shift events. One of the most important factors nowadays affecting QoE of YouTube is frequent shifts from higher to lower resolutions and vice versa. These shifts ensure a smooth streaming session; however, it might get a lower mean opinion score. For instance, dropping from 1080p to 480p during a video can preserve continuity but might reduce the viewers enjoyment. Over time, OTT platforms are looking for alternative ways to boost user experience instead of relying on traditional Quality of Service (QoS) metrics such as bandwidth, latency, and throughput. As a result, we look into the relationship between quality shifting in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our findings state that these channel metrics positively correlate with shifts. Thus, in real-time, OTT can only rely on them to predict video streaming sessions into lower- and higher-resolution categories, thus providing more resources to improve user experience. Using traditional Machine Learning (ML) classifiers, we achieved an accuracy of 77-percent, while using only RSRP, RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency networks promise enhanced streaming capabilities, the proposed methodology can be used to improve OTT services.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-parameter superposable S-curves</title>
<link>https://arxiv.org/abs/2504.19488</link>
<guid>https://arxiv.org/abs/2504.19488</guid>
<content:encoded><![CDATA[
arXiv:2504.19488v3 Announce Type: replace-cross 
Abstract: Straight line equation $y=mx$ with slope $m$, when singularly perturbed as $ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or S-curves on a real plane. As $a\rightarrow 0$, we get back $y=mx$ which is a cumulative distribution function of a continuous uniform distribution that describes the occurrence of every event in an interval to be equally probable. As $a\rightarrow\infty$, the derivative of $y$ has finite support only at $y=0$ resembling a degenerate distribution. Based on these arguments, in this work, we propose that these S-curves can represent maximum entropy uniform distribution to a zero entropy single value. We also argue that these S-curves are superposable as they are only parametrically nonlinear but fundamentally linear. So far, the superposed forms have been used to capture the patterns of natural systems such as nonlinear dynamics of biological growth and kinetics of enzyme reactions. Here, we attempt to use the S-curve and its superposed form as statistical models. We fit the models on a classical dataset containing flower measurements of iris plants and analyze their usefulness in pattern recognition. Based on these models, we claim that any non-uniform pattern can be represented as a singular perturbation to uniform distribution. However, our parametric estimation procedure have some limitations such as sensitivity to initial conditions depending on the data at hand.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAC Learning is just Bipartite Matching (Sort of)</title>
<link>https://arxiv.org/abs/2502.00607</link>
<guid>https://arxiv.org/abs/2502.00607</guid>
<content:encoded><![CDATA[
<div> supervised learning, PAC model, bipartite matching, transductive model, one-inclusion graphs

Summary: 
This article aims to highlight the connection between supervised learning in the Probably Approximately Correct (PAC) model and bipartite matching. It discusses a transductive model of learning and one-inclusion graphs, which are a generalization of popular recreational hat puzzles. While the transductive model is not new, it has gained renewed interest as a tool for addressing complex questions in learning theory. The article serves as a tutorial on the relationship between the PAC and transductive models of learning, providing insights into their similarities and applications in the field. <div>
arXiv:2502.00607v3 Announce Type: replace 
Abstract: The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model, application to unsteady flows</title>
<link>https://arxiv.org/abs/2503.23236</link>
<guid>https://arxiv.org/abs/2503.23236</guid>
<content:encoded><![CDATA[
<div> Variaional Auto-Encoder, Confidence Measurement, Uncertainty Quantification, Nonlinear Reduction Strategy, Transient Flows<br />
<br />
Summary:<br />
Reduced order models (ROMs) are essential in fluid mechanics for cost-effective predictions. This study introduces a novel nonlinear reduction strategy tailored for transient flows, incorporating parametrisation and uncertainty quantification through a variational auto-encoder (VAE) using variational inference for confidence measurement. By utilizing a latent space transformer with attention mechanisms, the model can predict dynamical systems with enhanced generalization capabilities across diverse dynamics. The integration of confidence metrics enables informed decision-making and robust model performance, facilitating more efficient sampling of the parameter space to improve predictions across the entire range without complete evaluation data. This approach aims to address the challenges of nonlinear reduction techniques in transient environments, advancing the applicability and reliability of ROMs in engineering applications.<br /> 
Summary: <div>
arXiv:2503.23236v3 Announce Type: replace 
Abstract: Reduced order models (ROMs) play a critical role in fluid mechanics by providing low-cost predictions, making them an attractive tool for engineering applications. However, for ROMs to be widely applicable, they must not only generalise well across different regimes, but also provide a measure of confidence in their predictions. While recent data-driven approaches have begun to address nonlinear reduction techniques to improve predictions in transient environments, challenges remain in terms of robustness and parametrisation. In this work, we present a nonlinear reduction strategy specifically designed for transient flows that incorporates parametrisation and uncertainty quantification. Our reduction strategy features a variational auto-encoder (VAE) that uses variational inference for confidence measurement. We use a latent space transformer that incorporates recent advances in attention mechanisms to predict dynamical systems. Attention's versatility in learning sequences and capturing their dependence on external parameters enhances generalisation across a wide range of dynamics. Prediction, coupled with confidence, enables more informed decision making and addresses the need for more robust models. In addition, this confidence is used to cost-effectively sample the parameter space, improving model performance a priori across the entire parameter space without requiring evaluation data for the entire domain.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Uncertainty-Aware Graph Neural Network</title>
<link>https://arxiv.org/abs/2504.19820</link>
<guid>https://arxiv.org/abs/2504.19820</guid>
<content:encoded><![CDATA[
<div> Hierarchical Uncertainty-Aware Graph Neural Network, multi-scale representation learning, uncertainty estimation, self-supervised, semi-supervised classification<br />
<br />
Summary:
The paper introduces the Hierarchical Uncertainty-Aware Graph Neural Network (HU-GNN) that combines multi-scale representation learning, uncertainty estimation, and self-supervised embedding diversity in a single framework. HU-GNN forms node clusters and estimates uncertainty at various structural scales to guide message-passing and attention weighting, effectively handling noise and adversarial perturbations while maintaining predictive accuracy. The model offers theoretical contributions like a probabilistic formulation, uncertainty calibration guarantees, and robustness bounds. Experimental results demonstrate that HU-GNN achieves state-of-the-art robustness and interpretability in semi-supervised classification tasks. <div>
arXiv:2504.19820v2 Announce Type: replace 
Abstract: Recent research on graph neural networks (GNNs) has explored mechanisms for capturing local uncertainty and exploiting graph hierarchies to mitigate data sparsity and leverage structural properties. However, the synergistic integration of these two approaches remains underexplored. This work introduces a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural Network (HU-GNN), which unifies multi-scale representation learning, principled uncertainty estimation, and self-supervised embedding diversity within a single end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and estimates uncertainty at multiple structural scales from individual nodes to higher levels. These uncertainty estimates guide a robust message-passing mechanism and attention weighting, effectively mitigating noise and adversarial perturbations while preserving predictive accuracy on semi-supervised classification tasks. We also offer key theoretical contributions, including a probabilistic formulation, rigorous uncertainty-calibration guarantees, and formal robustness bounds. Extensive experiments on standard benchmarks demonstrate that our model achieves state-of-the-art robustness and interpretability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?</title>
<link>https://arxiv.org/abs/2504.19267</link>
<guid>https://arxiv.org/abs/2504.19267</guid>
<content:encoded><![CDATA[
<div> Keywords: visual storytelling, transformer-based architectures, multimodal models, VIST dataset, RoViST, GROOVIST

Summary:
This paper introduces a novel approach to visual storytelling by leveraging transformer-based architectures and large multimodal models on the Visual Storytelling (VIST) dataset. The proposed VIST-GPT model generates visually grounded and contextually appropriate narratives. Traditional evaluation metrics like BLEU, METEOR, ROUGE, and CIDEr are deemed insufficient for this task, so the authors introduce RoViST and GROOVIST metrics to better assess visual storytelling quality in terms of visual grounding, coherence, and non-redundancy. These reference-free metrics offer a more comprehensive evaluation that closely aligns with human judgment. Overall, the paper presents a cutting-edge method for enhancing the quality and evaluation of narratives in the field of visual storytelling. 

<br /><br />Summary: <div>
arXiv:2504.19267v2 Announce Type: replace-cross 
Abstract: Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perturbation Analysis of Singular Values in Concatenated Matrices</title>
<link>https://arxiv.org/abs/2505.01427</link>
<guid>https://arxiv.org/abs/2505.01427</guid>
<content:encoded><![CDATA[
<div> Singular value decomposition, low-rank approximation, perturbation framework, stability analysis, matrix clustering<br />
<br />
Summary:<br />
The article discusses the relationship between singular value spectra of concatenated matrices and their individual components. It introduces a perturbation framework that extends classical results like Weyl's inequality to concatenated matrices, providing analytical bounds for the stability of singular values under small perturbations in submatrices. The study shows that when concatenated matrices are close in norm, the dominant singular values of the resultant matrix remain stable. This insight allows for controlled trade-offs between accuracy and compression, offering potential for improved matrix clustering and compression strategies. The findings have implications in various fields including numerical linear algebra, signal processing, and data-driven modeling.<br /> <div>
arXiv:2505.01427v1 Announce Type: new 
Abstract: Concatenating matrices is a common technique for uncovering shared structures in data through singular value decomposition (SVD) and low-rank approximations. However, a fundamental question arises: how does the singular value spectrum of the concatenated matrix relate to the spectra of its individual components? In this work, we develop a perturbation framework that extends classical results such as Weyl's inequality to concatenated matrices. We establish analytical bounds that quantify the stability of singular values under small perturbations in the submatrices. Our results show that if the matrices being concatenated are close in norm, the dominant singular values of the concatenated matrix remain stable, enabling controlled trade-offs between accuracy and compression. These insights provide a theoretical foundation for improved matrix clustering and compression strategies, with applications in numerical linear algebra, signal processing, and data-driven modeling.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets</title>
<link>https://arxiv.org/abs/2505.01437</link>
<guid>https://arxiv.org/abs/2505.01437</guid>
<content:encoded><![CDATA[
<div> VAE, cost-sensitive learning, IoT-botnet detection, deep learning models, imbalanced datasets
Summary:
Variational Auto-encoder (VAE) and cost-sensitive learning were used to develop lightweight models for IoT-botnet detection. These models aim to improve the detection of minority class attack traffic instances often missed by machine learning models. The study evaluated the performance of two deep learning models, a standard DNN and BLSTM, on imbalanced datasets for multi-class traffic category detection. Both models showed high accuracy, precision, recall, and F1-score for all traffic classes. This approach enhances the security of IoT devices by effectively identifying and mitigating botnet-related attacks. <div>
arXiv:2505.01437v1 Announce Type: new 
Abstract: The Internet of Things (IoT) technology has rapidly gained popularity with applications widespread across a variety of industries. However, IoT devices have been recently serving as a porous layer for many malicious attacks to both personal and enterprise information systems with the most famous attacks being botnet-related attacks. The work in this study leveraged Variational Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet effective, models for IoT-botnet detection. The aim is to enhance the detection of minority class attack traffic instances which are often missed by machine learning models. The proposed approach is evaluated on a multi-class problem setting for the detection of traffic categories on highly imbalanced datasets. The performance of two deep learning models including the standard feed forward deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and both recorded commendable results in terms of accuracy, precision, recall and F1-score for all traffic classes.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials</title>
<link>https://arxiv.org/abs/2505.01438</link>
<guid>https://arxiv.org/abs/2505.01438</guid>
<content:encoded><![CDATA[
<div> Keywords: material stress analysis, two-phase random materials, spatiotemporal stress diffusion, spatiotemporal super-resolution, physics-informed network

Summary: 
The study introduces a framework for generating global stress data and achieving high-resolution spatiotemporal stress fields in two-phase random materials under dynamic loading. The Spatiotemporal Stress Diffusion (STS-diffusion) model incorporates a Space-Time U-Net and explores different attention positions for model accuracy. Additionally, a physics-informed network, Spatiotemporal Super-Resolution Physics-Informed Operator (ST-SRPINN), is developed for spatiotemporal super-resolution. This unsupervised learning method leverages physics-based constraints and the impact of data-driven and physics-informed loss function weights on model accuracy is investigated. ST-SRPINN can upscale the resolution of stress fields with only low-resolution data during training, providing a practical solution for accurately capturing stress concentration regions in material stress analysis.<br /><br />Summary: <div>
arXiv:2505.01438v1 Announce Type: new 
Abstract: Material stress analysis is a critical aspect of material design and performance optimization. Under dynamic loading, the global stress evolution in materials exhibits complex spatiotemporal characteristics, especially in two-phase random materials (TRMs). Such kind of material failure is often associated with stress concentration, and the phase boundaries are key locations where stress concentration occurs. In practical engineering applications, the spatiotemporal resolution of acquired microstructural data and its dynamic stress evolution is often limited. This poses challenges for deep learning methods in generating high-resolution spatiotemporal stress fields, particularly for accurately capturing stress concentration regions. In this study, we propose a framework for global stress generation and spatiotemporal super-resolution in TRMs under dynamic loading. First, we introduce a diffusion model-based approach, named as Spatiotemporal Stress Diffusion (STS-diffusion), for generating global spatiotemporal stress data. This framework incorporates Space-Time U-Net (STU-net), and we systematically investigate the impact of different attention positions on model accuracy. Next, we develop a physics-informed network for spatiotemporal super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning method. The influence of data-driven and physics-informed loss function weights on model accuracy is explored in detail. Benefiting from physics-based constraints, ST-SRPINN requires only low-resolution stress field data during training and can upscale the spatiotemporal resolution of stress fields to arbitrary magnifications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.01440</link>
<guid>https://arxiv.org/abs/2505.01440</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Human-in-the-Loop, Interactive Double Deep Q-network, Autonomous Driving, Policy Development<br />
Summary:<br />
The study introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop (HITL) approach that integrates human expertise with machine learning for applications like autonomous driving. iDDQN modifies the Q-value update equation to collaborate with human insights in the training process, enhancing model performance. An offline evaluative framework evaluates the effectiveness of human interventions by simulating the agent's trajectory without human input. Empirical results in simulated autonomous driving scenarios demonstrate that iDDQN outperforms established approaches like Behavioral Cloning and Deep Q-Learning from Demonstrations in incorporating human expertise for better performance and adaptability. <div>
arXiv:2505.01440v1 Announce Type: new 
Abstract: Integrating human expertise with machine learning is crucial for applications demanding high accuracy and safety, such as autonomous driving. This study introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop (HITL) approach that enhances Reinforcement Learning (RL) by merging human insights directly into the RL training process, improving model performance. Our proposed iDDQN method modifies the Q-value update equation to integrate human and agent actions, establishing a collaborative approach for policy development. Additionally, we present an offline evaluative framework that simulates the agent's trajectory as if no human intervention had occurred, to assess the effectiveness of human interventions. Empirical results in simulated autonomous driving scenarios demonstrate that iDDQN outperforms established approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for improving performance and adaptability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding</title>
<link>https://arxiv.org/abs/2505.01445</link>
<guid>https://arxiv.org/abs/2505.01445</guid>
<content:encoded><![CDATA[
<div> Keywords: injection moulding, machine learning, explainable AI, feature impact analysis, root cause analysis

Summary:
This study explores the use of machine learning models in predicting the quality characteristics of products in the injection moulding process. It highlights the importance of explainable AI methods in providing insights into the root causes of deviations in product properties. The research demonstrates the existence of interactions among input machine settings and compares various model-agnostic explainability methods to analyze feature impacts. The study shows that different explainability methods can lead to different feature attributions, which in turn affect the accuracy of cause identification in injection moulding. By performing explanations on both random forest and multilayer perceptron models, the study aims to provide actionable insights for improving the quality control process. Overall, the findings emphasize the significance of using explainable AI for accurate root cause analysis and quality control in injection moulding processes.<br /><br />Summary: <div>
arXiv:2505.01445v1 Announce Type: new 
Abstract: If a product deviates from its desired properties in the injection moulding process, its root cause analysis can be aided by models that relate the input machine settings with the output quality characteristics. The machine learning models tested in the quality prediction are mostly black boxes; therefore, no direct explanation of their prognosis is given, which restricts their applicability in the quality control. The previously attempted explainability methods are either restricted to tree-based algorithms only or do not emphasize on the fact that some explainability methods can lead to wrong root cause identification of a product's deviation from its desired properties. This study first shows that the interactions among the multiple input machine settings do exist in real experimental data collected as per a central composite design. Then, the model-agnostic explainable AI methods are compared for the first time to show that different explainability methods indeed lead to different feature impact analysis in injection moulding. Moreover, it is shown that the better feature attribution translates to the correct cause identification and actionable insights for the injection moulding process. Being model agnostic, explanations on both random forest and multilayer perceptron are performed for the cause analysis, as both models have the mean absolute percentage error of less than 0.05% on the experimental dataset.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenAVS: Training-Free Open-Vocabulary Audio Visual Segmentation with Foundational Models</title>
<link>https://arxiv.org/abs/2505.01448</link>
<guid>https://arxiv.org/abs/2505.01448</guid>
<content:encoded><![CDATA[
<div> Keywords: Audio-visual segmentation, OpenAVS, multimedia foundation models, self-training, unsupervised learning

Summary: 
OpenAVS introduces a novel language-based approach for Audio-Visual Segmentation (AVS) that aligns audio and visual modalities using text as a proxy. By leveraging multimedia foundation models, OpenAVS infers masks through audio-to-text prompt generation, LLM-guided prompt translation, and text-to-visual sounding object segmentation. The architecture is designed to transfer knowledge effectively to the downstream AVS task and can be integrated with any supervised AVS model using OpenAVS-ST, a model-agnostic framework that enables self-training with pseudo-labels. Experimental results on benchmark datasets show that OpenAVS outperforms existing unsupervised, zero-shot, and few-shot AVS methods, achieving significant performance gains in challenging scenarios. The approach demonstrates superior performance in mIoU and F-score metrics, highlighting its effectiveness in open-vocabulary audio-visual segmentation tasks. 

<br /><br />Summary: <div>
arXiv:2505.01448v1 Announce Type: new 
Abstract: Audio-visual segmentation aims to separate sounding objects from videos by predicting pixel-level masks based on audio signals. Existing methods primarily concentrate on closed-set scenarios and direct audio-visual alignment and fusion, which limits their capability to generalize to new, unseen situations. In this paper, we propose OpenAVS, a novel training-free language-based approach that, for the first time, effectively aligns audio and visual modalities using text as a proxy for open-vocabulary Audio-Visual Segmentation (AVS). Equipped with multimedia foundation models, OpenAVS directly infers masks through 1) audio-to-text prompt generation, 2) LLM-guided prompt translation, and 3) text-to-visual sounding object segmentation. The objective of OpenAVS is to establish a simple yet flexible architecture that relies on the most appropriate foundation models by fully leveraging their capabilities to enable more effective knowledge transfer to the downstream AVS task. Moreover, we present a model-agnostic framework OpenAVS-ST that enables the integration of OpenAVS with any advanced supervised AVS model via pseudo-label based self-training. This approach enhances performance by effectively utilizing large-scale unlabeled data when available. Comprehensive experiments on three benchmark datasets demonstrate the superior performance of OpenAVS. It surpasses existing unsupervised, zero-shot, and few-shot AVS methods by a significant margin, achieving absolute performance gains of approximately 9.4% and 10.9% in mIoU and F-score, respectively, in challenging scenarios.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COSMOS: Predictable and Cost-Effective Adaptation of LLMs</title>
<link>https://arxiv.org/abs/2505.01449</link>
<guid>https://arxiv.org/abs/2505.01449</guid>
<content:encoded><![CDATA[
<div> Prediction, Large language models, COSMOS, Adaptation strategies, Computational cost
Summary:
Large language models (LLMs) use various adaptation strategies to achieve high performance in tasks. However, selecting the optimal model and strategy within resource constraints is challenging. The study proposes COSMOS, a prediction framework that accurately estimates adaptation outcomes at minimal cost. It includes embedding-augmented lightweight proxy models for fine-tuning performance prediction and low-sample scaling laws for retrieval-augmented in-context learning forecast. Evaluation on eight benchmarks shows COSMOS achieves high prediction accuracy and reduces computational costs by up to 98.71%. The results demonstrate efficient prediction of adaptation outcomes is possible, significantly decreasing the computational overhead of LLM deployment while maintaining performance standards.<br /><br />Summary: <div>
arXiv:2505.01449v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Film-Making Production Dialogue, Narration, Monologue Adaptive Moving Dubbing Benchmarks</title>
<link>https://arxiv.org/abs/2505.01450</link>
<guid>https://arxiv.org/abs/2505.01450</guid>
<content:encoded><![CDATA[
<div> Keywords: Movie dubbing, TA-Dubbing, benchmarks, evaluation, open-sourcing

Summary: <br /><br />Movie dubbing has made significant advancements, but evaluating its real-world effectiveness remains a challenge. Existing metrics lack the complexity to fully assess dialogue, narration, monologue, and actor adaptability in movie dubbing. To address this, Talking Adaptive Dubbing Benchmarks (TA-Dubbing) have been introduced to enhance film production through adaptive dubbing. TA-Dubbing covers various dimensions of movie dubbing, including metric evaluations for movie understanding and speech generation. It is versatile in benchmarking state-of-the-art dubbing models and large language models. The full open-sourcing of TA-Dubbing, including video suits, evaluation methods, and annotations, is available on GitHub. Continuous integration of new dubbing models into the TA-Dubbing leaderboard aims to drive progress in the field of movie dubbing. <div>
arXiv:2505.01450v1 Announce Type: new 
Abstract: Movie dubbing has advanced significantly, yet assessing the real-world effectiveness of these models remains challenging. A comprehensive evaluation benchmark is crucial for two key reasons: 1) Existing metrics fail to fully capture the complexities of dialogue, narration, monologue, and actor adaptability in movie dubbing. 2) A practical evaluation system should offer valuable insights to improve movie dubbing quality and advancement in film production. To this end, we introduce Talking Adaptive Dubbing Benchmarks (TA-Dubbing), designed to improve film production by adapting to dialogue, narration, monologue, and actors in movie dubbing. TA-Dubbing offers several key advantages: 1) Comprehensive Dimensions: TA-Dubbing covers a variety of dimensions of movie dubbing, incorporating metric evaluations for both movie understanding and speech generation. 2) Versatile Benchmarking: TA-Dubbing is designed to evaluate state-of-the-art movie dubbing models and advanced multi-modal large language models. 3) Full Open-Sourcing: We fully open-source TA-Dubbing at https://github.com/woka- 0a/DeepDubber- V1 including all video suits, evaluation methods, annotations. We also continuously integrate new movie dubbing models into the TA-Dubbing leaderboard at https://github.com/woka- 0a/DeepDubber-V1 to drive forward the field of movie dubbing.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Machine Learning for Cyberattack Identification from Traffic Flows</title>
<link>https://arxiv.org/abs/2505.01488</link>
<guid>https://arxiv.org/abs/2505.01488</guid>
<content:encoded><![CDATA[
<div> Keywords: automation, cyberattacks, traffic flow data, anomaly detection system, Explainable AI

Summary:
In this study, the focus is on the increasing automation of traffic management systems, which are vulnerable to cyberattacks that disrupt urban mobility and public safety. Traditional network-layer defenses are often not accessible to transportation agencies, leading to the development of a machine learning-based approach that relies solely on traffic flow data. The researchers simulate cyberattacks in a virtualized traffic network environment and develop a deep learning-based anomaly detection system to identify compromised signals, with Longest Stop Duration and Total Jam Distance being identified as key indicators. The use of Explainable AI techniques helps in interpreting critical decision factors and diagnosing misclassification errors. Two primary challenges are identified: transitional data inconsistencies and model limitations that can lead to stealth attacks evading detection, particularly in low-traffic conditions. This work aims to enhance AI-driven traffic security by improving detection accuracy and trustworthiness in smart transportation systems. 

<br /><br />Summary: <div>
arXiv:2505.01488v1 Announce Type: new 
Abstract: The increasing automation of traffic management systems has made them prime targets for cyberattacks, disrupting urban mobility and public safety. Traditional network-layer defenses are often inaccessible to transportation agencies, necessitating a machine learning-based approach that relies solely on traffic flow data. In this study, we simulate cyberattacks in a semi-realistic environment, using a virtualized traffic network to analyze disruption patterns. We develop a deep learning-based anomaly detection system, demonstrating that Longest Stop Duration and Total Jam Distance are key indicators of compromised signals. To enhance interpretability, we apply Explainable AI (XAI) techniques, identifying critical decision factors and diagnosing misclassification errors. Our analysis reveals two primary challenges: transitional data inconsistencies, where mislabeled recovery-phase traffic misleads the model, and model limitations, where stealth attacks in low-traffic conditions evade detection. This work enhances AI-driven traffic security, improving both detection accuracy and trustworthiness in smart transportation systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning for Cyber-Attack Identification from Traffic Flows</title>
<link>https://arxiv.org/abs/2505.01489</link>
<guid>https://arxiv.org/abs/2505.01489</guid>
<content:encoded><![CDATA[
<div> Keywords: cyber-attacks, traffic control system, simulation, Raspberry Pi, detection strategies <br />
Summary: <br />
This paper discusses the simulation of cyber-attacks on the traffic control system in Daytona Beach, FL, using Raspberry Pi virtual machines and OPNSense firewall. The study aims to detect attacks by analyzing traffic flow patterns, particularly focusing on instances where traffic lights are manipulated to turn all green or red at busy intersections. Despite challenges like imbalanced data and overlapping traffic patterns, the research achieved an 85% accuracy rate in detecting intrusions using traffic flow statistics. Key factors for successful detection included occupancy, jam length, and halting durations. The combination of these indicators proved effective in identifying cyber attacks on the traffic control system. <div>
arXiv:2505.01489v1 Announce Type: new 
Abstract: This paper presents our simulation of cyber-attacks and detection strategies on the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual machines and the OPNSense firewall, along with traffic dynamics from SUMO and exploitation via the Metasploit framework. We try to answer the research questions: are we able to identify cyber attacks by only analyzing traffic flow patterns. In this research, the cyber attacks are focused particularly when lights are randomly turned all green or red at busy intersections by adversarial attackers. Despite challenges stemming from imbalanced data and overlapping traffic patterns, our best model shows 85\% accuracy when detecting intrusions purely using traffic flow statistics. Key indicators for successful detection included occupancy, jam length, and halting durations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.01523</link>
<guid>https://arxiv.org/abs/2505.01523</guid>
<content:encoded><![CDATA[
<div> Approach, Fine-tune, Large language models, Budgeted subset selection, Mathematical domain <br />
<br />
The article introduces a refined approach to efficiently fine-tune large language models (LLMs) on specific domains such as mathematics by using a budgeted subset selection method. By combining utility and diversity metrics, the approach selects informative and representative training examples to achieve near-full dataset performance while reducing computational cost and training time. The utility metric considers perplexity and Chain-of-Thought (CoT) loss to identify challenging examples crucial for model learning, while the diversity metric ensures coverage across mathematical subdomains. Evaluation on LLaMA-3 8B and Phi-3 models shows that the proposed method outperforms baseline approaches such as random selection, diversity-based sampling, and existing state-of-the-art subset selection techniques in terms of performance and efficiency. <br /><br />Summary: <div>
arXiv:2505.01523v1 Announce Type: new 
Abstract: We propose a refined approach to efficiently fine-tune large language models (LLMs) on specific domains like the mathematical domain by employing a budgeted subset selection method. Our approach combines utility and diversity metrics to select the most informative and representative training examples. The final goal is to achieve near-full dataset performance with meticulously selected data points from the entire dataset while significantly reducing computational cost and training time and achieving competitive performance as the full dataset. The utility metric incorporates both perplexity and Chain-of-Thought (CoT) loss to identify challenging examples that contribute most to model learning, while the diversity metric ensures broad coverage across mathematical subdomains. We evaluate our method on LLaMA-3 8B and Phi-3 models, comparing against several baseline approaches, including random selection, diversity-based sampling, and existing state-of-the-art subset selection techniques.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextures: Representations from Contexts</title>
<link>https://arxiv.org/abs/2505.01557</link>
<guid>https://arxiv.org/abs/2505.01557</guid>
<content:encoded><![CDATA[
<div> contexture theory, representation learning methods, singular functions, model scaling, context evaluation

Summary: The paper introduces the concept of the contexture theory, which characterizes representation learning as learning from the association between input and a context variable. It shows that popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context. The theory applies to various learning paradigms such as supervised, self-supervised, and manifold learning, and shows that representations learning the contexture are optimal for tasks compatible with the context. The theory suggests that further scaling up model size yields diminishing returns once it approximates the top singular functions. It emphasizes the need for better contexts for further improvement. The paper also proposes a metric to evaluate the usefulness of a context without knowledge of downstream tasks, showing a correlation with encoder performance on real datasets. <div>
arXiv:2505.01557v1 Announce Type: new 
Abstract: Despite the empirical success of foundation models, we do not have a systematic characterization of the representations that these models learn. In this paper, we establish the contexture theory. It shows that a large class of representation learning methods can be characterized as learning from the association between the input and a context variable. Specifically, we show that many popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context, in which case we say that the representation learns the contexture. We demonstrate the generality of the contexture theory by proving that representation learning within various learning paradigms -- supervised, self-supervised, and manifold learning -- can all be studied from such a perspective. We also prove that the representations that learn the contexture are optimal on those tasks that are compatible with the context. One important implication of the contexture theory is that once the model is large enough to approximate the top singular functions, further scaling up the model size yields diminishing returns. Therefore, scaling is not all we need, and further improvement requires better contexts. To this end, we study how to evaluate the usefulness of a context without knowing the downstream tasks. We propose a metric and show by experiments that it correlates well with the actual performance of the encoder on many real datasets.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation</title>
<link>https://arxiv.org/abs/2505.01584</link>
<guid>https://arxiv.org/abs/2505.01584</guid>
<content:encoded><![CDATA[
<div> neural networks, network conditions, plasticity loss, Silent Neuron theory, Reset Silent Neuron (ReSiN)<br />
<br />
Summary: 
The article discusses the challenges of adapting to non-stationary network conditions and the limitations of current solutions based on stationary assumptions. It highlights the problem of neural networks suffering from plasticity loss, affecting their ability to adapt to evolving network conditions. The Silent Neuron theory is introduced as a comprehensive framework to understand plasticity degradation. The Reset Silent Neuron (ReSiN) approach is proposed to address this limitation by strategically resetting neurons based on forward and backward propagation states. In an adaptive video streaming system implementation, ReSiN demonstrated significant improvements over existing solutions, achieving higher bitrate and better quality of experience while maintaining smoothness. The solution also performs well in stationary environments, showcasing robust adaptability across different network conditions. <div>
arXiv:2505.01584v1 Announce Type: new 
Abstract: Adapting to non-stationary network conditions presents significant challenges for resource adaptation. However, current solutions primarily rely on stationary assumptions. While data-driven reinforcement learning approaches offer promising solutions for handling network dynamics, our systematic investigation reveals a critical limitation: neural networks suffer from plasticity loss, significantly impeding their ability to adapt to evolving network conditions. Through theoretical analysis of neural propagation mechanisms, we demonstrate that existing dormant neuron metrics inadequately characterize neural plasticity loss. To address this limitation, we have developed the Silent Neuron theory, which provides a more comprehensive framework for understanding plasticity degradation. Based on these theoretical insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural plasticity through strategic neuron resets guided by both forward and backward propagation states. In our implementation of an adaptive video streaming system, ReSiN has shown significant improvements over existing solutions, achieving up to 168% higher bitrate and 108% better quality of experience (QoE) while maintaining comparable smoothness. Furthermore, ReSiN consistently outperforms in stationary environments, demonstrating its robust adaptability across different network conditions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Fairness in House Price Prediction: A Case Study of America's Expanding Metropolises</title>
<link>https://arxiv.org/abs/2505.01591</link>
<guid>https://arxiv.org/abs/2505.01591</guid>
<content:encoded><![CDATA[
<div> ML-driven house price prediction, ethnic bias, racial bias, fairness, bias mitigation

Summary: 
The paper focuses on the development of Machine Learning (ML) models for house price prediction and investigates the presence of ethnic and racial bias in these models. Through a combination of structural and neighborhood-level attributes, ML models are developed and assessed for bias towards protected attributes such as race and ethnicity. The study finds varying levels of bias in the ML-driven house price prediction models and explores different bias mitigation solutions. The experimental results show that in-processing bias mitigation approaches tend to be more effective than pre-processing methods in addressing bias in this problem domain. The research highlights the importance of ethical considerations in ML-driven solutions for improving housing conditions and promoting social equity. The code for the study is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.01591v1 Announce Type: new 
Abstract: As a basic human need, housing plays a key role in enhancing health, well-being, and educational outcome in society, and the housing market is a major factor for promoting quality of life and ensuring social equity. To improve the housing conditions, there has been extensive research on building Machine Learning (ML)-driven house price prediction solutions to accurately forecast the future conditions, and help inform actions and policies in the field. In spite of their success in developing high-accuracy models, there is a gap in our understanding of the extent to which various ML-driven house price prediction approaches show ethnic and/or racial bias, which in turn is essential for the responsible use of ML, and ensuring that the ML-driven solutions do not exacerbate inequity. To fill this gap, this paper develops several ML models from a combination of structural and neighborhood-level attributes, and conducts comprehensive assessments on the fairness of ML models under various definitions of privileged groups. As a result, it finds that the ML-driven house price prediction models show various levels of bias towards protected attributes (i.e., race and ethnicity in this study). Then, it investigates the performance of different bias mitigation solutions, and the experimental results show their various levels of effectiveness on different ML-driven methods. However, in general, the in-processing bias mitigation approach tends to be more effective than the pre-processing one in this problem domain. Our code is available at https://github.com/wahab1412/housing_fairness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't be lazy: CompleteP enables compute-efficient deep transformers</title>
<link>https://arxiv.org/abs/2505.01618</link>
<guid>https://arxiv.org/abs/2505.01618</guid>
<content:encoded><![CDATA[
<div> parameterizations, LLM training, compute efficiency, model depth, CompleteP <br />
Summary: 
The study explores the compute efficiency of LLM training with different parameterizations, focusing on the transfer of optimal hyperparameters across changes in model depth. Some parameterizations struggle with transferring base hyperparameters, leading to suboptimal training or costly re-tuning. Additionally, certain parameterizations may result in lazy learning, restricting the depth and nonlinearity of layers. The proposed parameterization, CompleteP, achieves depth-wise hyperparameter transfer and non-lazy learning in all layers. This approach allows for improved compute efficiency, enabling a wider range of model width/depth ratios for various hardware and operational contexts. CompleteP delivers 12-34% efficiency gains compared to existing methods. <br /><br /> <div>
arXiv:2505.01618v1 Announce Type: new 
Abstract: We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the unique parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34\% compute efficiency improvements over the prior state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skill-based Safe Reinforcement Learning with Risk Planning</title>
<link>https://arxiv.org/abs/2505.01619</link>
<guid>https://arxiv.org/abs/2505.01619</guid>
<content:encoded><![CDATA[
<div> Safe RL, skill planning, PU learning, risk predictor, robotic simulation <br />
<br />
Summary: 
This paper introduces a Safe Skill Planning (SSkP) approach for enhancing safe reinforcement learning (RL) by utilizing offline demonstration data. The SSkP method consists of two stages: first, a skill risk predictor is learned using PU learning from the offline data, and second, a risk planning process is employed to improve online safe RL by learning a risk-averse policy while adapting the skill risk predictor to the environment. Experimental results in various robotic simulation environments show that the proposed approach outperforms existing state-of-the-art safe RL methods consistently. The SSkP approach effectively combines offline data with online interactions to learn a safe policy and mitigate risks, demonstrating the potential for enhancing safe RL in real-world applications. <br /> <div>
arXiv:2505.01619v1 Announce Type: new 
Abstract: Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components</title>
<link>https://arxiv.org/abs/2505.01627</link>
<guid>https://arxiv.org/abs/2505.01627</guid>
<content:encoded><![CDATA[
<div> Keywords: conceptual design phase, functional modeling, Function-Behavior-Structure framework, Large Language Models, domain adaptation<br />
Summary: <br />
The article discusses the importance of the conceptual design phase in product development, where designers generate solutions based on functional requirements. Functional modeling, particularly using the Function-Behavior-Structure framework, helps translate functional intent into behavioral and structural descriptions. However, the lack of comprehensive functional data often hinders effective design decision-making. The study proposes a new approach using Large Language Models (LLMs), like GPT architectures, for automated classification of mechanical assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the accuracy and consistency of function annotation can be improved. A case study showcasing fine-tuning GPT-3.5 Turbo on the Oregon State Design Repository data demonstrates the effectiveness of this approach. The domain-adapted LLM helps generate high-quality functional data, enhancing the representation of mechanical parts and supporting more effective design exploration in early-phase engineering. <br /> <div>
arXiv:2505.01627v1 Announce Type: new 
Abstract: The conceptual design phase represents a critical early stage in the product development process, where designers generate potential solutions that meet predefined design specifications based on functional requirements. Functional modeling, a foundational aspect of this phase, enables designers to reason about product functions before specific structural details are determined. A widely adopted approach to functional modeling is the Function-Behavior-Structure (FBS) framework, which supports the transformation of functional intent into behavioral and structural descriptions. However, the effectiveness of function-based design is often hindered by the lack of well-structured and comprehensive functional data. This scarcity can negatively impact early design decision-making and hinder the development of accurate behavioral models. Recent advances in Large Language Models (LLMs), such as those based on GPT architectures, offer a promising avenue to address this gap. LLMs have demonstrated significant capabilities in language understanding and natural language processing (NLP), making them suitable for automated classification tasks. This study proposes a novel LLM-based domain adaptation (DA) framework using fine-tuning for the automated classification of mechanical assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the traditionally manual and subjective process of function annotation can be improved in both accuracy and consistency. A case study demonstrates fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the domain-adapted LLM can generate high-quality functional data, enhancing the semantic representation of mechanical parts and supporting more effective design exploration in early-phase engineering.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causally Fair Node Classification on Non-IID Graph Data</title>
<link>https://arxiv.org/abs/2505.01652</link>
<guid>https://arxiv.org/abs/2505.01652</guid>
<content:encoded><![CDATA[
<div> Machine learning, fairness, graph data, causal relationships, non-IID<br />
<br />
Summary: This paper introduces a novel approach to fair machine learning by addressing biases in predictions on graph data. It focuses on non-Independent and Identically Distributed (non-IID) settings, where data instances are interconnected in social networks. By leveraging the Network Structural Causal Model (NSCM) framework and two key assumptions, Decomposability and Graph Independence, the Message Passing Variational Autoencoder for Causal Inference (MPVA) is developed to compute interventional distributions and enable causally fair node classification. Empirical evaluations on both synthetic and real datasets show that MPVA outperforms existing methods by effectively mitigating bias. The study highlights the potential of causality-based fairness in complex machine learning applications and suggests further research into relaxing initial assumptions to enhance model fairness. <br /><br /> <div>
arXiv:2505.01652v1 Announce Type: new 
Abstract: Fair machine learning seeks to identify and mitigate biases in predictions against unfavorable populations characterized by demographic attributes, such as race and gender. Recently, a few works have extended fairness to graph data, such as social networks, but most of them neglect the causal relationships among data instances. This paper addresses the prevalent challenge in fairness-aware ML algorithms, which typically assume Independent and Identically Distributed (IID) data. We tackle the overlooked domain of non-IID, graph-based settings where data instances are interconnected, influencing the outcomes of fairness interventions. We base our research on the Network Structural Causal Model (NSCM) framework and posit two main assumptions: Decomposability and Graph Independence, which enable the computation of interventional distributions in non-IID settings using the $do$-calculus. Based on that, we develop the Message Passing Variational Autoencoder for Causal Inference (MPVA) to compute interventional distributions and facilitate causally fair node classification through estimated interventional distributions. Empirical evaluations on semi-synthetic and real-world datasets demonstrate that MPVA outperforms conventional methods by effectively approximating interventional distributions and mitigating bias. The implications of our findings underscore the potential of causality-based fairness in complex ML applications, setting the stage for further research into relaxing the initial assumptions to enhance model fairness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification</title>
<link>https://arxiv.org/abs/2505.01660</link>
<guid>https://arxiv.org/abs/2505.01660</guid>
<content:encoded><![CDATA[
<div> head classes, long-tailed distribution, SAM process, Focal-SAM, generalization ability<br />
Summary:<br />
The article introduces Focal-SAM as a method to address the challenges posed by long-tailed distributions in real-world datasets. It aims to improve generalization by flattening the loss landscape, offering fine-grained control without the need for multiple backpropagations like existing methods ImbSAM and CC-SAM. Focal-SAM assigns different penalties to class-wise sharpness, striking a balance between computational efficiency and control over the loss landscape. The method is theoretically analyzed for its generalization ability and a sharper generalization bound is derived. Extensive experiments on various models confirm the effectiveness of Focal-SAM in improving generalization to tail classes. <div>
arXiv:2505.01660v1 Announce Type: new 
Abstract: Real-world datasets often follow a long-tailed distribution, making generalization to tail classes difficult. Recent methods resorted to long-tail variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to improve generalization by flattening the loss landscape. However, these attempts face a trade-off between computational efficiency and control over the loss landscape. On the one hand, ImbSAM is efficient but offers only coarse control as it excludes head classes from the SAM process. On the other hand, CC-SAM provides fine-grained control through class-dependent perturbations but at the cost of efficiency due to multiple backpropagations. Seeing this dilemma, we introduce Focal-SAM, which assigns different penalties to class-wise sharpness, achieving fine-grained control without extra backpropagations, thus maintaining efficiency. Furthermore, we theoretically analyze Focal-SAM's generalization ability and derive a sharper generalization bound. Extensive experiments on both traditional and foundation models validate the effectiveness of Focal-SAM.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptively Point-weighting Curriculum Learning</title>
<link>https://arxiv.org/abs/2505.01665</link>
<guid>https://arxiv.org/abs/2505.01665</guid>
<content:encoded><![CDATA[
<div> Curriculum learning, Adaptively point-weighting, deep networks, training strategy, training effectiveness <br />
Summary: 
The study introduces the Adaptively Point-Weighting (APW) curriculum learning algorithm for training deep networks. APW assigns weights to training samples based on their training error and the network's current state. It prioritizes easy samples in the early training phase to capture overall characteristics quickly and switches to hard samples later for improved fitting performance. The algorithm's properties, including training effectiveness, feasibility, stability, and generalization performance, are theoretically analyzed and validated through numerical experiments. APW demonstrates superiority in training deep networks, supported by empirical evidence and theoretical findings. <div>
arXiv:2505.01665v1 Announce Type: new 
Abstract: Curriculum learning (CL) is referred to as a training strategy that makes easy samples learned first and then fits hard samples. It imitates the process of humans learning knowledge, and has become a potential manner of effectively training deep networks. In this study, we develop the adaptively point-weighting (APW) curriculum learning algorithm, which adaptively assigns the weight to every training sample not only based on its training error but also considering the current training state of the network. Specifically, in the early training phase, it increases the weights of easy samples to make the network rapidly capture the overall characteristics of the dataset; and in the later training phase, the weights of hard points rise to improve the fitting performance on the discrete local regions. Moreover, we also present the theoretical analysis on the properties of APW including training effectiveness, training feasibility, training stability, and generalization performance. The numerical experiments support the superiority of APW and demonstrate the validity of our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking</title>
<link>https://arxiv.org/abs/2505.01700</link>
<guid>https://arxiv.org/abs/2505.01700</guid>
<content:encoded><![CDATA[
<div> Keywords: protein-ligand docking, benchmark, AI docking methods, post-processing relaxation, dataset <br />
<br />
Summary: 
The article introduces PoseX, an open-source benchmark for protein-ligand docking evaluation through self-docking and cross-docking scenarios. The benchmark includes a curated dataset with entries for both self-docking and cross-docking, incorporating 22 docking methods across different categories. The authors designed a post-processing relaxation method to refine binding poses and released a real-time leaderboard for model ranking. Insights from extensive experiments show that AI-based approaches have surpassed traditional physics-based methods in docking accuracy, with AI methods showing improved generalization. The post-processing relaxation significantly alleviates stereochemical deficiencies in AI-based methods. Combining AI docking methods with relaxation achieves optimal performance. However, AI co-folding methods face challenges with ligand chirality issues not resolved by relaxation. The code, dataset, and leaderboard are available on GitHub for further exploration. <div>
arXiv:2505.01700v1 Announce Type: new 
Abstract: Recently, significant progress has been made in protein-ligand docking, especially in modern deep learning methods, and some benchmarks were proposed, e.g., PoseBench, Plinder. However, these benchmarks suffer from less practical evaluation setups (e.g., blind docking, self docking), or heavy framework that involves training, raising challenges to assess docking methods efficiently. To fill this gap, we proposed PoseX, an open-source benchmark focusing on self-docking and cross-docking, to evaluate the algorithmic advances practically and comprehensively. Specifically, first, we curate a new evaluation dataset with 718 entries for self docking and 1,312 for cross docking; second, we incorporate 22 docking methods across three methodological categories, including (1) traditional physics-based methods (e.g., Schr\"odinger Glide), (2) AI docking methods (e.g., DiffDock), (3) AI co-folding methods (e.g., AlphaFold3); third, we design a relaxation method as post-processing to minimize conformation energy and refine binding pose; fourth, we released a leaderboard to rank submitted models in real time. We draw some key insights via extensive experiments: (1) AI-based approaches have already surpassed traditional physics-based approaches in overall docking accuracy (RMSD). The longstanding generalization issues that have plagued AI molecular docking have been significantly alleviated in the latest models. (2) The stereochemical deficiencies of AI-based approaches can be greatly alleviated with post-processing relaxation. Combining AI docking methods with the enhanced relaxation method achieves the best performance to date. (3) AI co-folding methods commonly face ligand chirality issues, which cannot be resolved by relaxation. The code, curated dataset and leaderboard are released at https://github.com/CataAI/PoseX.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems</title>
<link>https://arxiv.org/abs/2505.01736</link>
<guid>https://arxiv.org/abs/2505.01736</guid>
<content:encoded><![CDATA[
<div> physics-encoded spectral attention network, PDEs, machine learning, forecasting, complex systems <br />
Summary: <br />
The article presents a new approach called Physics-encoded Spectral Attention Network (PeSANet) for accurately modeling and forecasting complex systems governed by partial differential equations (PDEs). Traditional numerical methods often struggle with incomplete or unknown physical laws, while machine learning approaches face challenges with limited data. PeSANet integrates local and global information using a physics-encoded block to approximate local differential operators and a spectral-enhanced block to capture global dependencies in the frequency domain. A novel spectral attention mechanism is introduced to model inter-spectrum relationships and learn long-range spatial features. Experimental results show that PeSANet outperforms existing methods in forecasting accuracy, especially in long-term predictions, making it a promising solution for simulating complex systems with limited data and incomplete physics. <br /> <div>
arXiv:2505.01736v1 Announce Type: new 
Abstract: Accurately modeling and forecasting complex systems governed by partial differential equations (PDEs) is crucial in various scientific and engineering domains. However, traditional numerical methods struggle in real-world scenarios due to incomplete or unknown physical laws. Meanwhile, machine learning approaches often fail to generalize effectively when faced with scarce observational data and the challenge of capturing local and global features. To this end, we propose the Physics-encoded Spectral Attention Network (PeSANet), which integrates local and global information to forecast complex systems with limited data and incomplete physical priors. The model consists of two key components: a physics-encoded block that uses hard constraints to approximate local differential operators from limited data, and a spectral-enhanced block that captures long-range global dependencies in the frequency domain. Specifically, we introduce a novel spectral attention mechanism to model inter-spectrum relationships and learn long-range spatial features. Experimental results demonstrate that PeSANet outperforms existing methods across all metrics, particularly in long-term forecasting accuracy, providing a promising solution for simulating complex systems with limited data and incomplete physics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients</title>
<link>https://arxiv.org/abs/2505.01744</link>
<guid>https://arxiv.org/abs/2505.01744</guid>
<content:encoded><![CDATA[
<div> Keywords: low-rank adapter, low-rank gradient projection, memory efficiency, optimization, stability

Summary: 
The study introduces a novel framework called VLoRP, which extends low-rank gradient projection by allowing for control over the trade-off between memory efficiency and performance through projection granularity. The finer-grained projections offered by VLoRP enhance stability and efficiency even within a fixed memory constraint. The study also introduces ProjFactor, an adaptive memory-efficient optimizer, that significantly reduces memory usage while maintaining competitive performance, even with gradient accumulation. The theoretical analysis presented demonstrates the descent and convergence of VLoRP under both SGD and ProjFactor optimization methods. Extensive experiments across various tasks validate the findings, including common sense reasoning, MMLU, and GSM8K. Overall, the study highlights the importance of projection granularity in enhancing memory efficiency and performance in gradient projection methods. 

<br /><br />Summary: <div>
arXiv:2505.01744v1 Announce Type: new 
Abstract: Building upon the success of low-rank adapter (LoRA), low-rank gradient projection (LoRP) has emerged as a promising solution for memory-efficient fine-tuning. However, existing LoRP methods typically treat each row of the gradient matrix as the default projection unit, leaving the role of projection granularity underexplored. In this work, we propose a novel framework, VLoRP, that extends low-rank gradient projection by introducing an additional degree of freedom for controlling the trade-off between memory efficiency and performance, beyond the rank hyper-parameter. Through this framework, we systematically explore the impact of projection granularity, demonstrating that finer-grained projections lead to enhanced stability and efficiency even under a fixed memory budget. Regarding the optimization for VLoRP, we present ProjFactor, an adaptive memory-efficient optimizer, that significantly reduces memory requirement while ensuring competitive performance, even in the presence of gradient accumulation. Additionally, we provide a theoretical analysis of VLoRP, demonstrating the descent and convergence of its optimization trajectory under both SGD and ProjFactor. Extensive experiments are conducted to validate our findings, covering tasks such as commonsense reasoning, MMLU, and GSM8K.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition</title>
<link>https://arxiv.org/abs/2505.01783</link>
<guid>https://arxiv.org/abs/2505.01783</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, online learning, conformal prediction, false discovery rate, calibration data <br />
<br />
Summary: 
The article introduces a novel framework, C-PP-COAD, for online anomaly detection in fields like cybersecurity and healthcare. This framework leverages synthetic calibration data to overcome limitations posed by scarce real calibration data. By adaptively integrating real data based on contextual cues, C-PP-COAD ensures rigorous anomaly detection performance over time. It utilizes conformal p-values, active p-value statistics, and online FDR control mechanisms to maintain reliable performance. Experiments on synthetic and real-world datasets demonstrate that C-PP-COAD reduces reliance on real calibration data while still maintaining guaranteed FDR control. <div>
arXiv:2505.01783v1 Announce Type: new 
Abstract: Online anomaly detection is essential in fields such as cybersecurity, healthcare, and industrial monitoring, where promptly identifying deviations from expected behavior can avert critical failures or security breaches. While numerous anomaly scoring methods based on supervised or unsupervised learning have been proposed, current approaches typically rely on a continuous stream of real-world calibration data to provide assumption-free guarantees on the false discovery rate (FDR). To address the inherent challenges posed by limited real calibration data, we introduce context-aware prediction-powered conformal online anomaly detection (C-PP-COAD). Our framework strategically leverages synthetic calibration data to mitigate data scarcity, while adaptively integrating real data based on contextual cues. C-PP-COAD utilizes conformal p-values, active p-value statistics, and online FDR control mechanisms to maintain rigorous and reliable anomaly detection performance over time. Experiments conducted on both synthetic and real-world datasets demonstrate that C-PP-COAD significantly reduces dependency on real calibration data without compromising guaranteed FDR control.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning</title>
<link>https://arxiv.org/abs/2505.01788</link>
<guid>https://arxiv.org/abs/2505.01788</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Privacy-preserving, Federated Learning, Personalized Learning, Machine Learning

Summary: 
The research paper discusses the importance of privacy-preserving AI and the shift towards Federated Learning to address data privacy concerns. The study focuses on Federated Personalized Learning (PPMLFPL) as an innovative framework for personalized model refinement while maintaining data confidentiality. Through a performance analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) is highlighted for efficient execution. Additionally, the study recommends the use of Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) for privacy-preserving machine learning tasks in federated personalized learning settings. The results indicate the effectiveness of PPMLFPL in striking a balance between personalized model refinement and data privacy, offering valuable insights for future advancements in privacy-conscious data-driven technologies.

<br /><br />Summary: <div>
arXiv:2505.01788v1 Announce Type: new 
Abstract: The widespread adoption of Artificial Intelligence (AI) has been driven by significant advances in intelligent system research. However, this progress has raised concerns about data privacy, leading to a growing awareness of the need for privacy-preserving AI. In response, there has been a seismic shift in interest towards the leading paradigm for training Machine Learning (ML) models on decentralized data silos while maintaining data privacy, Federated Learning (FL). This research paper presents a comprehensive performance analysis of a cutting-edge approach to personalize ML model while preserving privacy achieved through Privacy Preserving Machine Learning with the innovative framework of Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns about data privacy, this study evaluates the effectiveness of PPMLFPL addressing the critical balance between personalized model refinement and maintaining the confidentiality of individual user data. According to our analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated personalized learning settings is strongly suggested. The results offer valuable insights creating it a promising scope for future advancements in the field of privacy-conscious data-driven technologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction for Indoor Positioning with Correctness Coverage Guarantees</title>
<link>https://arxiv.org/abs/2505.01810</link>
<guid>https://arxiv.org/abs/2505.01810</guid>
<content:encoded><![CDATA[
<div> CP, deep learning, indoor positioning, IoT, fingerprint-based localization

Summary:<br />
- This paper explores the application of conformal prediction (CP) to deep learning-based indoor positioning for high-precision location-based services in complex indoor environments.
- Traditional algorithms and deep learning methods often face challenges such as poor generalization and lack of interpretability, which CP aims to address by providing statistical guarantees through non-conformity scores and prediction sets.
- The model achieved high accuracy on both training and testing datasets, showcasing its performance and generalization capability.
- Conformal risk control for path navigation tasks helps manage false discovery and false negative rates.
- A conformal p-value framework is also introduced to control the proportion of position-error points, with experiments demonstrating effectiveness across different lightweight models like MobileNetV1, VGG19, and EfficientNet. 

<br /><br />Summary: <div>
arXiv:2505.01810v1 Announce Type: new 
Abstract: With the advancement of Internet of Things (IoT) technologies, high-precision indoor positioning has become essential for Location-Based Services (LBS) in complex indoor environments. Fingerprint-based localization is popular, but traditional algorithms and deep learning-based methods face challenges such as poor generalization, overfitting, and lack of interpretability. This paper applies conformal prediction (CP) to deep learning-based indoor positioning. CP transforms the uncertainty of the model into a non-conformity score, constructs prediction sets to ensure correctness coverage, and provides statistical guarantees. We also introduce conformal risk control for path navigation tasks to manage the false discovery rate (FDR) and the false negative rate (FNR).The model achieved an accuracy of approximately 100% on the training dataset and 85% on the testing dataset, effectively demonstrating its performance and generalization capability. Furthermore, we also develop a conformal p-value framework to control the proportion of position-error points. Experiments on the UJIIndoLoc dataset using lightweight models such as MobileNetV1, VGG19, MobileNetV2, ResNet50, and EfficientNet show that the conformal prediction technique can effectively approximate the target coverage, and different models have different performance in terms of prediction set size and uncertainty quantification.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LSTM-PINN Hybrid Method to the specific problem of population forecasting</title>
<link>https://arxiv.org/abs/2505.01819</link>
<guid>https://arxiv.org/abs/2505.01819</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, population dynamics, policy interventions, LSTM-PINN framework, demographic forecasting

Summary: 
The study introduces two novel frameworks, PINN and LSTM-PINN, integrating policy-aware fertility functions into a partial differential equation for simulating age-structured population dynamics from 2024 to 2054. The PINN model enforces governing equations via collocation-based training, accurately capturing underlying population dynamics. The LSTM-PINN framework incorporates sequential memory mechanisms to capture long-range dependencies in the age-time domain, enhancing training performance. The models successfully simulate demographic shifts under three fertility policy scenarios, demonstrating the effectiveness of integrating domain knowledge into data-driven forecasting. This study contributes to modeling age-structured population dynamics under policy interventions, providing valuable insights for data-informed demographic forecasting and long-term policy planning in response to evolving population challenges. 

<br /><br />Summary: <div>
arXiv:2505.01819v1 Announce Type: new 
Abstract: Deep learning has emerged as a powerful tool in scientific modeling, particularly for complex dynamical systems; however, accurately capturing age-structured population dynamics under policy-driven fertility changes remains a significant challenge due to the lack of effective integration between domain knowledge and long-term temporal dependencies. To address this issue, we propose two physics-informed deep learning frameworks--PINN and LSTM-PINN--that incorporate policy-aware fertility functions into a transport-reaction partial differential equation to simulate population evolution from 2024 to 2054. The standard PINN model enforces the governing equation and boundary conditions via collocation-based training, enabling accurate learning of underlying population dynamics and ensuring stable convergence. Building on this, the LSTM-PINN framework integrates sequential memory mechanisms to effectively capture long-range dependencies in the age-time domain, achieving robust training performance across multiple loss components. Simulation results under three distinct fertility policy scenarios-the Three-child policy, the Universal two-child policy, and the Separate two-child policy--demonstrate the models' ability to reflect policy-sensitive demographic shifts and highlight the effectiveness of integrating domain knowledge into data-driven forecasting. This study provides a novel and extensible framework for modeling age-structured population dynamics under policy interventions, offering valuable insights for data-informed demographic forecasting and long-term policy planning in the face of emerging population challenges.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.01822</link>
<guid>https://arxiv.org/abs/2505.01822</guid>
<content:encoded><![CDATA[
<div> Diffusion Models, Conditional Decision Generation, Energy Function, Reinforcement Learning, Offline RL Tasks 
Summary: 
Analytic Energy-guided Policy Optimization (AEPO) addresses the challenge of estimating intermediate energy in diffusion models for RL by providing a closed-form solution for guidance and training an intermediate energy neural network. The method is applied to over 30 offline RL tasks and outperforms baseline methods in D4RL benchmarks. Theoretical analysis and target estimation of log-expectation formulation are provided for the diffusion model's conditional Gaussian transformation. AEPO successfully addresses the intractability issue in the generation process by training the energy neural network to approximate the target estimation. Results from extensive experiments demonstrate the method's effectiveness in offline RL tasks, showcasing its competitiveness and superiority over existing baselines. <br /><br />Summary: <div>
arXiv:2505.01822v1 Announce Type: new 
Abstract: Conditional decision generation with diffusion models has shown powerful competitiveness in reinforcement learning (RL). Recent studies reveal the relation between energy-function-guidance diffusion models and constrained RL problems. The main challenge lies in estimating the intermediate energy, which is intractable due to the log-expectation formulation during the generation process. To address this issue, we propose the Analytic Energy-guided Policy Optimization (AEPO). Specifically, we first provide a theoretical analysis and the closed-form solution of the intermediate guidance when the diffusion model obeys the conditional Gaussian transformation. Then, we analyze the posterior Gaussian distribution in the log-expectation formulation and obtain the target estimation of the log-expectation under mild assumptions. Finally, we train an intermediate energy neural network to approach the target estimation of log-expectation formulation. We apply our method in 30+ offline RL tasks to demonstrate the effectiveness of our method. Extensive experiments illustrate that our method surpasses numerous representative baselines in D4RL offline reinforcement learning benchmarks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Trustworthy Federated Learning with Untrusted Participants</title>
<link>https://arxiv.org/abs/2505.01874</link>
<guid>https://arxiv.org/abs/2505.01874</guid>
<content:encoded><![CDATA[
<div> Algorithm, distributed learning, data privacy, robust gradient aggregation, malicious parties

Summary:
CafCor presents a solution for trustworthy distributed learning that ensures resilience against malicious parties while maintaining data privacy. The algorithm relies on shared randomness between pairs of workers to achieve strong privacy-utility trade-offs without the need for a trusted central server. By integrating robust gradient aggregation with correlated noise injection, CafCor outperforms local differential privacy methods and approaches the utility of central differential privacy. Empirical results on standard benchmarks demonstrate the practicality of CafCor, showcasing that privacy and robustness can coexist in distributed systems without compromising utility or requiring trust in the server. <div>
arXiv:2505.01874v1 Announce Type: new 
Abstract: Resilience against malicious parties and data privacy are essential for trustworthy distributed learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of workers shares a randomness seed unknown to others. In a setting where malicious workers may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, leveraging shared randomness between workers. We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted. Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OODTE: A Differential Testing Engine for the ONNX Optimizer</title>
<link>https://arxiv.org/abs/2505.01892</link>
<guid>https://arxiv.org/abs/2505.01892</guid>
<content:encoded><![CDATA[
<div> Keywords: ONNX Optimizer, OODTE, compiler optimizations, model accuracy, differential testing

Summary: 
OODTE is a utility designed to assess the correctness of the ONNX Optimizer by utilizing a differential testing approach on a variety of ONNX models. Through this process, 15 issues were detected, with 14 being previously unknown, highlighting problems such as optimizer crashes and accuracy deviations. Notably, 9.2% of models experienced issues leading to optimizer crashes or invalid model generation using primary optimization strategies. Furthermore, discrepancies in model accuracy were observed in 30% of classification models, and to a lesser extent in semantic segmentation and object detection models as well. The iterative approach taken by OODTE helped localize the root causes of accuracy differences, providing valuable insights into the performance of the ONNX Optimizer. <div>
arXiv:2505.01892v1 Announce Type: new 
Abstract: With $700$ stars on GitHub and part of the official ONNX repository, the ONNX Optimizer consists of the standard method to apply graph-based optimizations on ONNX models. However, its ability to preserve model accuracy across optimizations, has not been rigorously explored. We propose OODTE, a utility to automatically and thoroughly assess the correctness of the ONNX Optimizer. OODTE follows a simple, yet effective differential testing and evaluation approach that can be easily adopted to other compiler optimizers. In particular, OODTE utilizes a number of ONNX models, then optimizes them and executes both the original and the optimized variants across a user-defined set of inputs, while automatically logging any issues with the optimization process. Finally, for successfully optimized models, OODTE compares the results, and, if any accuracy deviations are observed, it iteratively repeats the process for each pass of the ONNX Optimizer, to localize the root cause of the differences observed. Using OODTE, we sourced well-known $130$ models from the official ONNX Model Hub, used for a wide variety of tasks (classification, object detection, semantic segmentation, text summarization, question and answering, sentiment analysis) from the official ONNX model hub. We detected 15 issues, 14 of which were previously unknown, associated with optimizer crashes and accuracy deviations. We also observed $9.2$% of all model instances presenting issues leading into the crash of the optimizer, or the generation of an invalid model while using the primary optimizer strategies. In addition, $30$% of the classification models presented accuracy differences across the original and the optimized model variants, while $16.6$% of semantic segmentation and object detection models are also affected, at least to a limited extent.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup</title>
<link>https://arxiv.org/abs/2505.01902</link>
<guid>https://arxiv.org/abs/2505.01902</guid>
<content:encoded><![CDATA[
<div> Keywords: FIFA World Cup, machine learning, predictive modeling, player performance, data analysis

Summary: 
This paper introduces a machine learning framework for predicting FIFA World Cup match outcomes, utilizing both team-level historical data and player-specific performance metrics. By considering individual player attributes and team composition, the framework captures nuanced interactions often missed by aggregate models. The methodology creates year-specific team profiles to adjust for evolving rosters and player development. Classification techniques, dimensionality reduction, and hyperparameter optimization are employed to build robust predictive models. Experimental results using FIFA 2022 World Cup data show the approach's superior accuracy compared to baseline methods. The study emphasizes the significance of incorporating player-specific data for improved predictive performance, providing insights into player synergy, strategic match-ups, and tournament scenarios. This research showcases the potential of player-centric data in sports analytics, paving the way for future exploration of advanced learning architectures like graph neural networks to model intricate team interactions. 

Summary:<br /><br />Keywords: FIFA World Cup, machine learning, predictive modeling, player performance, data analysis <div>
arXiv:2505.01902v1 Announce Type: new 
Abstract: Accurate prediction of FIFA World Cup match outcomes holds significant value for analysts, coaches, bettors, and fans. This paper presents a machine learning framework specifically designed to forecast match winners in FIFA World Cup. By integrating both team-level historical data and player-specific performance metrics such as goals, assists, passing accuracy, and tackles, we capture nuanced interactions often overlooked by traditional aggregate models. Our methodology processes multi-year data to create year-specific team profiles that account for evolving rosters and player development. We employ classification techniques complemented by dimensionality reduction and hyperparameter optimization, to yield robust predictive models. Experimental results on data from the FIFA 2022 World Cup demonstrate our approach's superior accuracy compared to baseline method. Our findings highlight the importance of incorporating individual player attributes and team-level composition to enhance predictive performance, offering new insights into player synergy, strategic match-ups, and tournament progression scenarios. This work underscores the transformative potential of rich, player-centric data in sports analytics, setting a foundation for future exploration of advanced learning architectures such as graph neural networks to model complex team interactions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LookAlike: Consistent Distractor Generation in Math MCQs</title>
<link>https://arxiv.org/abs/2505.01903</link>
<guid>https://arxiv.org/abs/2505.01903</guid>
<content:encoded><![CDATA[
<div> method, large language models, distractor generation, error generation, math education

Summary:
The article introduces LookAlike, a method that improves the consistency of distractors generated by large language models (LLMs) for multiple-choice questions (MCQs) in domains like math education. LookAlike utilizes synthetic preference pairs mined from model inconsistencies to optimize error-distractor consistency. The method alternates between supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to enhance training stability. Unlike previous approaches relying on heuristics or manually annotated data, LookAlike leverages its own inconsistencies for scalable and stable training. Evaluation on a dataset of over 1,400 math MCQs shows that LookAlike achieves 51.6% accuracy in distractor generation and 57.2% in error generation, surpassing existing state-of-the-art methods. These results demonstrate the effectiveness of preference-based regularization and inconsistency mining for generating consistent math MCQ distractors at scale. 

<br /><br />Summary: <div>
arXiv:2505.01903v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to generate distractors for multiple-choice questions (MCQs), especially in domains like math education. However, existing approaches are limited in ensuring that the generated distractors are consistent with common student errors. We propose LookAlike, a method that improves error-distractor consistency via preference optimization. Our two main innovations are: (a) mining synthetic preference pairs from model inconsistencies, and (b) alternating supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike prior work that relies on heuristics or manually annotated preference data, LookAlike uses its own generation inconsistencies as dispreferred samples, thus enabling scalable and stable training. Evaluated on a real-world dataset of 1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an existing state-of-the-art method (45.6% / 47.7%). These improvements highlight the effectiveness of preference-based regularization and inconsistency mining for generating consistent math MCQ distractors at scale.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.01912</link>
<guid>https://arxiv.org/abs/2505.01912</guid>
<content:encoded><![CDATA[
<div> Generative modeling, deep learning, molecule discovery, out-of-distribution prediction, benchmark, OOD error<br />
<br />
Summary: 
Advances in deep learning and generative modeling have sparked interest in data-driven molecule discovery pipelines. The study introduces BOOM, a benchmark for out-of-distribution molecular property predictions. Despite the need for accurate OOD predictions in molecule discovery, current ML models struggle to generalize OOD. More than 140 model-property combinations were evaluated, with no existing models achieving strong OOD generalization. Models with high inductive bias excel in specific OOD tasks, while foundation models lack OOD extrapolation capabilities. Ablation experiments demonstrate the impact of various factors on OOD performance. Developing ML models with robust OOD generalization presents a new challenge in chemical ML model development. The open-source benchmark will be available on Github. <br /><br />Summary: <div>
arXiv:2505.01912v1 Announce Type: new 
Abstract: Advances in deep learning and generative modeling have driven interest in data-driven molecule discovery pipelines, whereby machine learning (ML) models are used to filter and design novel molecules without requiring prohibitively expensive first-principles simulations. Although the discovery of novel molecules that extend the boundaries of known chemistry requires accurate out-of-distribution (OOD) predictions, ML models often struggle to generalize OOD. Furthermore, there are currently no systematic benchmarks for molecular OOD prediction tasks. We present BOOM, $\boldsymbol{b}$enchmarks for $\boldsymbol{o}$ut-$\boldsymbol{o}$f-distribution $\boldsymbol{m}$olecular property predictions -- a benchmark study of property-based out-of-distribution models for common molecular property prediction models. We evaluate more than 140 combinations of models and property prediction tasks to benchmark deep learning models on their OOD performance. Overall, we do not find any existing models that achieve strong OOD generalization across all tasks: even the top performing model exhibited an average OOD error 3x larger than in-distribution. We find that deep learning models with high inductive bias can perform well on OOD tasks with simple, specific properties. Although chemical foundation models with transfer and in-context learning offer a promising solution for limited training data scenarios, we find that current foundation models do not show strong OOD extrapolation capabilities. We perform extensive ablation experiments to highlight how OOD performance is impacted by data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation. We propose that developing ML models with strong OOD generalization is a new frontier challenge in chemical ML model development. This open-source benchmark will be made available on Github.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unemployment Dynamics Forecasting with Machine Learning Regression Models</title>
<link>https://arxiv.org/abs/2505.01933</link>
<guid>https://arxiv.org/abs/2505.01933</guid>
<content:encoded><![CDATA[
<div> Regression, machine learning, unemployment data, forecasting, feature importance

Summary:
- Explored regression and machine learning techniques for timely U.S. unemployment forecasts.
- Compared seven models including linear regression, tree-based ensembles, and LSTM network.
- Tuned hyperparameters, evaluated performance, and assessed ability to predict unemployment direction.
- Tree-based ensembles and CatBoost outperformed linear approaches, while LSTM captured temporal patterns effectively.
- Interpretability tools highlighted job openings and consumer sentiment as influential predictors. <br /><br />Summary: <div>
arXiv:2505.01933v1 Announce Type: new 
Abstract: In this paper, I explored how a range of regression and machine learning techniques can be applied to monthly U.S. unemployment data to produce timely forecasts. I compared seven models: Linear Regression, SGDRegressor, Random Forest, XGBoost, CatBoost, Support Vector Regression, and an LSTM network, training each on a historical span of data and then evaluating on a later hold-out period. Input features include macro indicators (GDP growth, CPI), labor market measures (job openings, initial claims), financial variables (interest rates, equity indices), and consumer sentiment.
  I tuned model hyperparameters via cross-validation and assessed performance with standard error metrics and the ability to predict the correct unemployment direction. Across the board, tree-based ensembles (and CatBoost in particular) deliver noticeably better forecasts than simple linear approaches, while the LSTM captures underlying temporal patterns more effectively than other nonlinear methods. SVR and SGDRegressor yield modest gains over standard regression but don't match the consistency of the ensemble and deep-learning models.
  Interpretability tools ,feature importance rankings and SHAP values, point to job openings and consumer sentiment as the most influential predictors across all methods. By directly comparing linear, ensemble, and deep-learning approaches on the same dataset, our study shows how modern machine-learning techniques can enhance real-time unemployment forecasting, offering economists and policymakers richer insights into labor market trends.
  In the comparative evaluation of the models, I employed a dataset comprising thirty distinct features over the period from January 2020 through December 2024.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Scale Graph Learning for Anti-Sparse Downscaling</title>
<link>https://arxiv.org/abs/2505.01948</link>
<guid>https://arxiv.org/abs/2505.01948</guid>
<content:encoded><![CDATA[
<div> Method, Stream temperature prediction, Multi-Scale Graph Learning, Fine-scale data, Water resources monitoring

Summary:
The article introduces a new method, Multi-Scale Graph Learning (MSGL), for accurate prediction of stream water temperature at fine spatial resolutions. By utilizing a multi-task learning framework, the MSGL method combines coarse-scale graph learning with fine-scale graph learning to improve prediction accuracy. The method also introduces cross-scale interpolation learning to establish connections across different spatial scales, enhancing overall model performance. Additionally, the Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL) breaks away from synchronous training approaches. Through extensive experiments in the Delaware River Basin, USA, the MSGL method demonstrates state-of-the-art performance in downscaling daily stream temperatures, showcasing its potential for water resources monitoring and management. <div>
arXiv:2505.01948v1 Announce Type: new 
Abstract: Water temperature can vary substantially even across short distances within the same sub-watershed. Accurate prediction of stream water temperature at fine spatial resolutions (i.e., fine scales, $\leq$ 1 km) enables precise interventions to maintain water quality and protect aquatic habitats. Although spatiotemporal models have made substantial progress in spatially coarse time series modeling, challenges persist in predicting at fine spatial scales due to the lack of data at that scale.To address the problem of insufficient fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This method employs a multi-task learning framework where coarse-scale graph learning, bolstered by larger datasets, simultaneously enhances fine-scale graph learning. Although existing multi-scale or multi-resolution methods integrate data from different spatial scales, they often overlook the spatial correspondences across graph structures at various scales. To address this, our MSGL introduces an additional learning task, cross-scale interpolation learning, which leverages the hydrological connectedness of stream locations across coarse- and fine-scale graphs to establish cross-scale connections, thereby enhancing overall model performance. Furthermore, we have broken free from the mindset that multi-scale learning is limited to synchronous training by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL). Extensive experiments demonstrate the state-of-the-art performance of our method for anti-sparse downscaling of daily stream temperatures in the Delaware River Basin, USA, highlighting its potential utility for water resources monitoring and management.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Probabilistic Control of Language Models</title>
<link>https://arxiv.org/abs/2505.01954</link>
<guid>https://arxiv.org/abs/2505.01954</guid>
<content:encoded><![CDATA[
<div> semantic control, LM generations, toxicity, sentiment, politeness

Summary:
This study introduces a novel approach for semantic control of language model (LM) generations by leveraging gradient information from a sequence-level verifier. The goal is to steer LM generations towards satisfying non-lexical constraints such as toxicity, sentiment, and politeness. Unlike existing methods that only handle syntactic constraints or rely on ineffective sampling, this approach efficiently reasons over all generated sentences that meet the target attribute. By creating a local LM distribution favoring semantically similar sentences and estimating the probability of satisfying the constraint based on the verifier's evaluation, the method enables precise steering of LM generations. Experimental results demonstrate the effectiveness of this approach in controlling toxicity, sentiment, and topic adherence of LM generations with a high probability of constraint satisfaction (>95%) while maintaining generation quality. 

<br /><br />Summary: <div>
arXiv:2505.01954v1 Announce Type: new 
Abstract: Semantic control entails steering LM generations towards satisfying subtle non-lexical constraints, e.g., toxicity, sentiment, or politeness, attributes that can be captured by a sequence-level verifier. It can thus be viewed as sampling from the LM distribution conditioned on the target attribute, a computationally intractable problem due to the non-decomposable nature of the verifier. Existing approaches to LM control either only deal with syntactic constraints which cannot capture the aforementioned attributes, or rely on sampling to explore the conditional LM distribution, an ineffective estimator for low-probability events. In this work, we leverage a verifier's gradient information to efficiently reason over all generations that satisfy the target attribute, enabling precise steering of LM generations by reweighing the next-token distribution. Starting from an initial sample, we create a local LM distribution favoring semantically similar sentences. This approximation enables the tractable computation of an expected sentence embedding. We use this expected embedding, informed by the verifier's evaluation at the initial sample, to estimate the probability of satisfying the constraint, which directly informs the update to the next-token distribution. We evaluated the effectiveness of our approach in controlling the toxicity, sentiment, and topic-adherence of LMs yielding generations satisfying the constraint with high probability (>95%) without degrading their quality.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting</title>
<link>https://arxiv.org/abs/2505.01959</link>
<guid>https://arxiv.org/abs/2505.01959</guid>
<content:encoded><![CDATA[
<div> Keywords: Carbon intensity, ensemble learning, forecast, regional variability, accuracy

Summary:  
EnsembleCI is introduced as an adaptive, ensemble learning-based approach for carbon intensity (CI) forecasting, aiming to improve upon the limitations of the current state-of-the-art method, CarbonCast. The new approach addresses regional variability and adaptability issues, offering enhanced flexibility in CI predictions. In evaluations across 11 regional grids, EnsembleCI outperformed CarbonCast by achieving lower mean absolute percentage error (MAPE) and improving prediction accuracy by an average of 19.58%. Despite some variability across grids due to regional diversity, EnsembleCI demonstrated greater robustness in long-term forecasting. The approach also identified region-specific key features, highlighting its interpretability and practical relevance. The availability of EnsembleCI source code and data provides transparency and reproducibility for further research and industry applications. <div>
arXiv:2505.01959v1 Announce Type: new 
Abstract: Carbon intensity (CI) measures the average carbon emissions generated per unit of electricity, making it a crucial metric for quantifying and managing the environmental impact. Accurate CI predictions are vital for minimizing carbon footprints, yet the state-of-the-art method (CarbonCast) falls short due to its inability to address regional variability and lack of adaptability. To address these limitations, we introduce EnsembleCI, an adaptive, end-to-end ensemble learning-based approach for CI forecasting. EnsembleCI combines weighted predictions from multiple sublearners, offering enhanced flexibility and regional adaptability. In evaluations across 11 regional grids, EnsembleCI consistently surpasses CarbonCast, achieving the lowest mean absolute percentage error (MAPE) in almost all grids and improving prediction accuracy by an average of 19.58%. While performance still varies across grids due to inherent regional diversity, EnsembleCI reduces variability and exhibits greater robustness in long-term forecasting compared to CarbonCast and identifies region-specific key features, underscoring its interpretability and practical relevance. These findings position EnsembleCI as a more accurate and reliable solution for CI forecasting. EnsembleCI source code and data used in this paper are available at https://github.com/emmayly/EnsembleCI.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection</title>
<link>https://arxiv.org/abs/2505.01979</link>
<guid>https://arxiv.org/abs/2505.01979</guid>
<content:encoded><![CDATA[
<div> Hierarchical Reinforcement Learning, Delay Effects, Spurious Correlations, Causal Relationships, D3HRL <br />
Summary: <br />
The proposed D3HRL approach addresses challenges in Hierarchical Reinforcement Learning by modeling delayed effects as causal relationships and using distributed causal discovery to learn them. It employs conditional independence testing to eliminate spurious correlations and constructs hierarchical policies based on identified causal relationships. Through iterative execution, D3HRL explores the complete causal chain of tasks and demonstrates sensitivity to delay effects. Experimental results in 2D-MineCraft and MiniGrid environments show that D3HRL accurately identifies causal relationships, leading to reliable decision-making in complex scenarios. <br /> <div>
arXiv:2505.01979v1 Announce Type: new 
Abstract: Current Hierarchical Reinforcement Learning (HRL) algorithms excel in long-horizon sequential decision-making tasks but still face two challenges: delay effects and spurious correlations. To address them, we propose a causal HRL approach called D3HRL. First, D3HRL models delayed effects as causal relationships across different time spans and employs distributed causal discovery to learn these relationships. Second, it employs conditional independence testing to eliminate spurious correlations. Finally, D3HRL constructs and trains hierarchical policies based on the identified true causal relationships. These three steps are iteratively executed, gradually exploring the complete causal chain of the task. Experiments conducted in 2D-MineCraft and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects and accurately identifies causal relationships, leading to reliable decision-making in complex environments.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Always Skip Attention</title>
<link>https://arxiv.org/abs/2505.01996</link>
<guid>https://arxiv.org/abs/2505.01996</guid>
<content:encoded><![CDATA[
<div> skip connections, self-attention, Vision Transformers, Token Graying, deep architectures <br />
Summary:
The study focuses on the critical role of skip connections in training modern Vision Transformers (ViTs). It notes that self-attention in ViTs requires skip connections for successful training, unlike other components of ViTs. This dependency on skip connections is a recent development, as previous deep architectures like CNNs have performed well without them. The paper theorizes that the self-attention mechanism is inherently unstable and relies on skip connections for regularization. Additionally, the study introduces Token Graying as a simple yet effective method to improve input token conditions. The effectiveness of Token Graying is validated in both supervised and self-supervised training approaches. Overall, the research sheds light on the unique relationship between skip connections, self-attention, and the training of Vision Transformers. <br /><br />Summary: <div>
arXiv:2505.01996v1 Announce Type: new 
Abstract: We highlight a curious empirical result within modern Vision Transformers (ViTs). Specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. This is in contrast to other elements of a ViT that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. Further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (\eg, CNNs) exhibiting good performance in their absence. In this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. Additionally, we propose Token Graying -- a simple yet effective complement (to skip connections) that further improves the condition of input tokens. We validate our approach in both supervised and self-supervised training methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach</title>
<link>https://arxiv.org/abs/2505.01997</link>
<guid>https://arxiv.org/abs/2505.01997</guid>
<content:encoded><![CDATA[
<div> alignment, Large Language Models, calibration, preference, fine-tuning

Summary:
Large Language Models (LLMs) rely on preference alignment but suffer from poor calibration post-alignment. The issue stems from preference collapse, leading to overconfidence and calibration problems in LLMs. Fine-tuning with domain-specific knowledge helps mitigate overconfidence. Models fall into calibratable and non-calibratable regimes based on Expected Calibration Error (ECE) bounds. In the calibratable regime, calibration-aware fine-tuning maintains proper calibration without performance loss. However, as models aim for better performance, they may become non-calibratable, requiring an EM-algorithm-based ECE regularization during fine-tuning to manage calibration error. Experimental results confirm the efficacy of these strategies in enhancing calibration in LLMs. 

<br /><br />Summary: <div>
arXiv:2505.01997v1 Announce Type: new 
Abstract: One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting</title>
<link>https://arxiv.org/abs/2505.02011</link>
<guid>https://arxiv.org/abs/2505.02011</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time series forecasting, Transformer variants, CNN Autoencoder, Score Attention mechanism, computational efficiency

Summary:<br />
- Multivariate time series forecasting is crucial for various applications like weather prediction and traffic analysis.
- Transformer variants have improved prediction accuracy in this area.
- Different tokenization techniques, such as point-wise, channel-wise, and patch-wise tokenization, have enhanced input data processing.
- A novel approach, the CNN Autoencoder-based Score Attention mechanism (CASA), has been introduced to address limitations in time complexity, computational resources, and cross-dimensional interactions.
- Experiments on real-world datasets show that CASA reduces computational resources by up to 77.7%, speeds up inference by 44.0%, and achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics.<br /><br /> <div>
arXiv:2505.02011v1 Announce Type: new 
Abstract: Multivariate long-term time series forecasting is critical for applications such as weather prediction, and traffic analysis. In addition, the implementation of Transformer variants has improved prediction accuracy. Following these variants, different input data process approaches also enhanced the field, such as tokenization techniques including point-wise, channel-wise, and patch-wise tokenization. However, previous studies still have limitations in time complexity, computational resources, and cross-dimensional interactions. To address these limitations, we introduce a novel CNN Autoencoder-based Score Attention mechanism (CASA), which can be introduced in diverse Transformers model-agnosticically by reducing memory and leading to improvement in model performance. Experiments on eight real-world datasets validate that CASA decreases computational resources by up to 77.7%, accelerates inference by 44.0%, and achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wide &amp; Deep Learning for Node Classification</title>
<link>https://arxiv.org/abs/2505.02020</link>
<guid>https://arxiv.org/abs/2505.02020</guid>
<content:encoded><![CDATA[
<div> Keywords: Wide & Deep, Graph Convolutional Networks (GCNs), Intersect memory, Initial residual, Large language models (LLMs)

Summary: 
Wide & Deep, a recommendation system architecture by Google, combines the strengths of linear and deep models. Graph Convolutional Networks (GCNs) are widely used in node classification tasks but face challenges like heterophily and limited expressiveness. In response, a new framework called GCNIII is proposed, integrating Wide & Deep techniques to improve generalization and reduce over-fitting in supervised tasks. The framework incorporates Intersect memory, Initial residual, and Identity mapping to enhance performance across various tasks. Additionally, leveraging Large Language Models (LLMs) for node feature engineering boosts the accuracy of GCNIII in cross-domain node classification. The empirical evidence presented demonstrates the effectiveness of GCNIII in achieving a balanced trade-off between over-fitting and over-generalization, making it a promising advancement in graph-based learning systems. The implementation of GCNIII is available for public use on GitHub. 

<br /><br />Summary: <div>
arXiv:2505.02020v1 Announce Type: new 
Abstract: Wide & Deep, a simple yet effective learning architecture for recommendation systems developed by Google, has had a significant impact in both academia and industry due to its combination of the memorization ability of generalized linear models and the generalization ability of deep models. Graph convolutional networks (GCNs) remain dominant in node classification tasks; however, recent studies have highlighted issues such as heterophily and expressiveness, which focus on graph structure while seemingly neglecting the potential role of node features. In this paper, we propose a flexible framework GCNIII, which leverages the Wide & Deep architecture and incorporates three techniques: Intersect memory, Initial residual and Identity mapping. We provide comprehensive empirical evidence showing that GCNIII can more effectively balance the trade-off between over-fitting and over-generalization on various semi- and full- supervised tasks. Additionally, we explore the use of large language models (LLMs) for node feature engineering to enhance the performance of GCNIII in cross-domain node classification tasks. Our implementation is available at https://github.com/CYCUCAS/GCNIII.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks</title>
<link>https://arxiv.org/abs/2505.02022</link>
<guid>https://arxiv.org/abs/2505.02022</guid>
<content:encoded><![CDATA[
<div> Nanobodies, antibody fragments, camelid, single-domain, therapeutics <br />
<br />
Summary: <br />
The article introduces NbBench, a benchmark suite for nanobody representation learning, addressing the lack of a unified benchmark in this field. It evaluates eleven models across eight tasks and nine datasets, focusing on structure annotation, binding prediction, and developability assessment. Results show that antibody language models perform well in antigen-related tasks, but struggle with regression tasks like thermostability and affinity. No single model consistently outperforms others across all tasks. NbBench standardizes datasets, task definitions, and evaluation protocols, providing a reproducible foundation for nanobody modeling advancements. <div>
arXiv:2505.02022v1 Announce Type: new 
Abstract: Nanobodies, single-domain antibody fragments derived from camelid heavy-chain-only antibodies, exhibit unique advantages such as compact size, high stability, and strong binding affinity, making them valuable tools in therapeutics and diagnostics. While recent advances in pretrained protein and antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular understanding, nanobody-specific modeling remains underexplored and lacks a unified benchmark. To address this gap, we introduce NbBench, the first comprehensive benchmark suite for nanobody representation learning. Spanning eight biologically meaningful tasks across nine curated datasets, NbBench encompasses structure annotation, binding prediction, and developability assessment. We systematically evaluate eleven representative models--including general-purpose protein LMs, antibody-specific LMs, and nanobody-specific LMs--in a frozen setting. Our analysis reveals that antibody language models excel in antigen-related tasks, while performance on regression tasks such as thermostability and affinity remains challenging across all models. Notably, no single model consistently outperforms others across all tasks. By standardizing datasets, task definitions, and evaluation protocols, NbBench offers a reproducible foundation for assessing and advancing nanobody modeling.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning</title>
<link>https://arxiv.org/abs/2505.02027</link>
<guid>https://arxiv.org/abs/2505.02027</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph In-Context Learning, Prompt Generator, Prompt Selector, Prompt Augmenter, Adaptive Prompt Optimization

Summary:
Graph In-Context Learning involves adapting pre-trained graph models to new graphs without updating parameters. Existing methods randomly select prompts, resulting in noise and reduced performance. GraphPrompter optimizes prompt generation, selection, and usage for better in-context learning. The Prompt Generator highlights informative edges and reduces noise. The Prompt Selector uses the $k$-nearest neighbors algorithm and selection layers to choose appropriate samples. The Prompt Augmenter enhances generalization with a cache replacement strategy. GraphPrompter improves in-context learning by over 8% compared to baselines. The code is available on GitHub. <div>
arXiv:2505.02027v1 Announce Type: new 
Abstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression Profiles</title>
<link>https://arxiv.org/abs/2505.02033</link>
<guid>https://arxiv.org/abs/2505.02033</guid>
<content:encoded><![CDATA[
<div> Keywords: DNA microarray, brain tumors, quantum computing, deep learning, classification

Summary:
In this study, the researchers propose a novel model called "Deep VQC" based on the Variational Quantum Classifier approach for the classification of brain tumors using gene expression data. The model successfully classified four different types of brain tumors alongside healthy samples with high accuracy. The study highlights the potential of quantum AI methods in efficiently analyzing and classifying complex structures like brain tumors based on gene expression features. The traditional AI-based approaches like machine learning and deep learning face challenges in managing high-dimensional data and complex gene relationships. Quantum methods leverage properties like superposition and entanglement for more efficient parallel processing and offer faster and more effective solutions to computationally demanding problems. The Deep VQC model outperformed or matched the classification performance of classical ML algorithms, showcasing the promise and effectiveness of quantum AI in the field of complex disease classification. 

<br /><br />Summary: <div>
arXiv:2505.02033v1 Announce Type: new 
Abstract: DNA microarray technology enables the simultaneous measurement of expression levels of thousands of genes, thereby facilitating the understanding of the molecular mechanisms underlying complex diseases such as brain tumors and the identification of diagnostic genetic signatures. To derive meaningful biological insights from the high-dimensional and complex gene features obtained through this technology and to analyze gene properties in detail, classical AI-based approaches such as machine learning and deep learning are widely employed. However, these methods face various limitations in managing high-dimensional vector spaces and modeling the intricate relationships among genes. In particular, challenges such as hyperparameter tuning, computational costs, and high processing power requirements can hinder their efficiency. To overcome these limitations, quantum computing and quantum AI approaches are gaining increasing attention. Leveraging quantum properties such as superposition and entanglement, quantum methods enable more efficient parallel processing of high-dimensional data and offer faster and more effective solutions to problems that are computationally demanding for classical methods. In this study, a novel model called "Deep VQC" is proposed, based on the Variational Quantum Classifier approach. Developed using microarray data containing 54,676 gene features, the model successfully classified four different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and pilocytic astrocytoma-alongside healthy samples with high accuracy. Furthermore, compared to classical ML algorithms, our model demonstrated either superior or comparable classification performance. These results highlight the potential of quantum AI methods as an effective and promising approach for the analysis and classification of complex structures such as brain tumors based on gene expression features.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secrets of GFlowNets' Learning Behavior: A Theoretical Study</title>
<link>https://arxiv.org/abs/2505.02035</link>
<guid>https://arxiv.org/abs/2505.02035</guid>
<content:encoded><![CDATA[
<div> learning behavior, convergence, sample complexity, implicit regularization, robustness
Summary:<br />
- This work provides a theoretical investigation of the learning behavior of Generative Flow Networks (GFlowNets), focusing on convergence, sample complexity, implicit regularization, and robustness.
- The study aims to elucidate the mechanisms underlying GFlowNets' learning dynamics to understand their strengths and limitations better.
- It bridges a critical gap in the theoretical understanding of GFlowNets, laying the groundwork for their reliable and interpretable use in generative modeling.
- The findings contribute to a deeper understanding of the factors influencing GFlowNet performance and offer guidelines for effective design and deployment.
- The research seeks to advance the theoretical frontiers of GFlowNets and promote their broader adoption within the AI community.
<br />Summary: <div>
arXiv:2505.02035v1 Announce Type: new 
Abstract: Generative Flow Networks (GFlowNets) have emerged as a powerful paradigm for generating composite structures, demonstrating considerable promise across diverse applications. While substantial progress has been made in exploring their modeling validity and connections to other generative frameworks, the theoretical understanding of their learning behavior remains largely uncharted. In this work, we present a rigorous theoretical investigation of GFlowNets' learning behavior, focusing on four fundamental dimensions: convergence, sample complexity, implicit regularization, and robustness. By analyzing these aspects, we seek to elucidate the intricate mechanisms underlying GFlowNet's learning dynamics, shedding light on its strengths and limitations. Our findings contribute to a deeper understanding of the factors influencing GFlowNet performance and provide insights into principled guidelines for their effective design and deployment. This study not only bridges a critical gap in the theoretical landscape of GFlowNets but also lays the foundation for their evolution as a reliable and interpretable framework for generative modeling. Through this, we aspire to advance the theoretical frontiers of GFlowNets and catalyze their broader adoption in the AI community.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Logistic Bandits</title>
<link>https://arxiv.org/abs/2505.02069</link>
<guid>https://arxiv.org/abs/2505.02069</guid>
<content:encoded><![CDATA[
<div> Neural logistic bandits, reward function, logistic link function, neural network, regret upper bound <br />
Summary:<br />
This study focuses on neural logistic bandits, aiming to learn an unknown reward function using a neural network within a logistic link function. Existing methods have limitations related to variance and feature dimension dependency. The introduction of a novel Bernstein-type inequality for self-normalized vector-valued martingales enables the derivation of a regret upper bound that grows with the effective dimension rather than the feature dimension. Two proposed algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, offer improved regret upper bounds compared to previous approaches. These algorithms guarantee regret upper bounds of order $\widetilde{O}(\widetilde{d}\sqrt{\kappa T})$ and $\widetilde{O}(\widetilde{d}\sqrt{T/\kappa})$, respectively. Numerical results on synthetic and real datasets validate the theoretical findings. <div>
arXiv:2505.02069v1 Announce Type: new 
Abstract: We study the problem of neural logistic bandits, where the main task is to learn an unknown reward function within a logistic link function using a neural network. Existing approaches either exhibit unfavorable dependencies on $\kappa$, where $1/\kappa$ represents the minimum variance of reward distributions, or suffer from direct dependence on the feature dimension $d$, which can be huge in neural network-based settings. In this work, we introduce a novel Bernstein-type inequality for self-normalized vector-valued martingales that is designed to bypass a direct dependence on the ambient dimension. This lets us deduce a regret upper bound that grows with the effective dimension $\widetilde{d}$, not the feature dimension, while keeping a minimal dependence on $\kappa$. Based on the concentration inequality, we propose two algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of order $\widetilde{O}(\widetilde{d}\sqrt{\kappa T})$ and $\widetilde{O}(\widetilde{d}\sqrt{T/\kappa})$, respectively, improving on the existing results. Lastly, we report numerical results on both synthetic and real datasets to validate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Defense Against Adversarial Attacks in Time Series Classification</title>
<link>https://arxiv.org/abs/2505.02073</link>
<guid>https://arxiv.org/abs/2505.02073</guid>
<content:encoded><![CDATA[
<div> Defense methods, Time series classification, Adversarial attacks, Data augmentation, Ensemble method  
Summary:  
- The paper introduces five data augmentation-based defense methods for time series classification to enhance robustness against adversarial attacks.  
- These methods are tailored for time series data and offer better defense performance than techniques like adversarial training (AT) while requiring only a small increase in computational resources.  
- By combining these methods into an ensemble, the model not only achieves superior defense performance compared to PGD-based AT but also improves generalization ability.  
- The ensemble method requires less computational resources than PGD-based AT, making it a more efficient alternative for robust TSC models.  
- The research also provides insights into integrating data augmentation-based defense with large-scale pre-trained models for further advancements in the field of time series feature learning.  
<br /><br />Summary: <div>
arXiv:2505.02073v1 Announce Type: new 
Abstract: As time series classification (TSC) gains prominence, ensuring robust TSC models against adversarial attacks is crucial. While adversarial defense is well-studied in Computer Vision (CV), the TSC field has primarily relied on adversarial training (AT), which is computationally expensive. In this paper, five data augmentation-based defense methods tailored for time series are developed, with the most computationally intensive method among them increasing the computational resources by only 14.07% compared to the original TSC model. Moreover, the deployment process for these methods is straightforward. By leveraging these advantages of our methods, we create two combined methods. One of these methods is an ensemble of all the proposed techniques, which not only provides better defense performance than PGD-based AT but also enhances the generalization ability of TSC models. Moreover, the computational resources required for our ensemble are less than one-third of those required for PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as foundation models are increasingly explored for time series feature learning, our work provides insights into integrating data augmentation-based adversarial defense with large-scale pre-trained models in future research.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Local Causal World Models with State Space Models and Attention</title>
<link>https://arxiv.org/abs/2505.02074</link>
<guid>https://arxiv.org/abs/2505.02074</guid>
<content:encoded><![CDATA[
<div> modeling, world, causality, State Space Model, neural<br />
Summary:<br />
The article discusses the importance of world modeling for agents interacting with the physical world and the necessity of learning causal representations. The research focuses on assessing the ability of State Space Model (SSM) architecture in causal discovery compared to Transformer models. Empirical results indicate that SSMs can effectively model environment dynamics and learn causal models simultaneously, potentially outperforming Transformers in certain scenarios. The study opens up opportunities for further exploration into enhancing SSMs with causal awareness, highlighting the advantages of SSMs in capturing causality in world modeling tasks. <div>
arXiv:2505.02074v1 Announce Type: new 
Abstract: World modelling, i.e. building a representation of the rules that govern the world so as to predict its evolution, is an essential ability for any agent interacting with the physical world. Despite their impressive performance, many solutions fail to learn a causal representation of the environment they are trying to model, which would be necessary to gain a deep enough understanding of the world to perform complex tasks. With this work, we aim to broaden the research in the intersection of causality theory and neural world modelling by assessing the potential for causal discovery of the State Space Model (SSM) architecture, which has been shown to have several advantages over the widespread Transformer. We show empirically that, compared to an equivalent Transformer, a SSM can model the dynamics of a simple environment and learn a causal model at the same time with equivalent or better performance, thus paving the way for further experiments that lean into the strength of SSMs and further enhance them with causal awareness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations</title>
<link>https://arxiv.org/abs/2505.02094</link>
<guid>https://arxiv.org/abs/2505.02094</guid>
<content:encoded><![CDATA[
<div> Datasets, Reinforcement Learning, Interaction Demonstration, Trajectory Graph, State Transition Field <br />
<br />Summary: 
The article addresses challenges in Reinforcement Learning from Interaction Demonstration (RLID) due to noisy and sparse demonstration data by introducing data augmentation techniques. The two techniques presented are the Stitched Trajectory Graph (STG) for discovering transitions between demonstration skills and the State Transition Field (STF) for connecting arbitrary states within the demonstration neighborhood. An Adaptive Trajectory Sampling (ATS) strategy is developed for curriculum generation and a historical encoding mechanism is introduced for memory-dependent skill learning. These enhancements enable more robust skill acquisition that generalizes beyond the demonstrated skills. Extensive experiments across different interaction tasks show significant improvements in convergence stability, generalization capability, and recovery robustness compared to existing methods. <br /> <div>
arXiv:2505.02094v1 Announce Type: new 
Abstract: We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Representation Learning for Electronic Design Automation</title>
<link>https://arxiv.org/abs/2505.02105</link>
<guid>https://arxiv.org/abs/2505.02105</guid>
<content:encoded><![CDATA[
arXiv:2505.02105v1 Announce Type: new 
Abstract: Representation learning has become an effective technique utilized by electronic design automation (EDA) algorithms, which leverage the natural representation of workflow elements as images, grids, and graphs. By addressing challenges related to the increasing complexity of circuits and stringent power, performance, and area (PPA) requirements, representation learning facilitates the automatic extraction of meaningful features from complex data formats, including images, grids, and graphs. This paper examines the application of representation learning in EDA, covering foundational concepts and analyzing prior work and case studies on tasks that include timing prediction, routability analysis, and automated placement. Key techniques, including image-based methods, graph-based approaches, and hybrid multimodal solutions, are presented to illustrate the improvements provided in routing, timing, and parasitic prediction. The provided advancements demonstrate the potential of representation learning to enhance efficiency, accuracy, and scalability in current integrated circuit design flows.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code</title>
<link>https://arxiv.org/abs/2505.02124</link>
<guid>https://arxiv.org/abs/2505.02124</guid>
<content:encoded><![CDATA[
arXiv:2505.02124v1 Announce Type: new 
Abstract: Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a program that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.02138</link>
<guid>https://arxiv.org/abs/2505.02138</guid>
<content:encoded><![CDATA[
arXiv:2505.02138v1 Announce Type: new 
Abstract: Multivariate time series forecasting (MTSF) endeavors to predict future observations given historical data, playing a crucial role in time series data management systems. With advancements in large language models (LLMs), recent studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF. However, the deployment of LLMs often suffers from low efficiency during the inference phase. To address this problem, we introduce TimeKD, an efficient MTSF framework that leverages the calibrated language models and privileged knowledge distillation. TimeKD aims to generate high-quality future representations from the proposed cross-modality teacher model and cultivate an effective student model. The cross-modality teacher model adopts calibrated language models (CLMs) with ground truth prompts, motivated by the paradigm of Learning Under Privileged Information (LUPI). In addition, we design a subtractive cross attention (SCA) mechanism to refine these representations. To cultivate an effective student model, we propose an innovative privileged knowledge distillation (PKD) mechanism including correlation and feature distillation. PKD enables the student to replicate the teacher's behavior while minimizing their output discrepancy. Extensive experiments on real data offer insight into the effectiveness, efficiency, and scalability of the proposed TimeKD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora</title>
<link>https://arxiv.org/abs/2505.02147</link>
<guid>https://arxiv.org/abs/2505.02147</guid>
<content:encoded><![CDATA[
arXiv:2505.02147v1 Announce Type: new 
Abstract: Herb classification presents a critical challenge in botanical research, particularly in regions with rich biodiversity such as Nepal. This study introduces a novel deep learning approach for classifying 60 different herb species using Convolutional Neural Networks (CNNs) and transfer learning techniques. Using a manually curated dataset of 12,000 herb images, we developed a robust machine learning model that addresses existing limitations in herb recognition methodologies. Our research employed multiple model architectures, including DenseNet121, 50-layer Residual Network (ResNet50), 16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2, and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating superior performance. Data augmentation and regularization techniques were applied to mitigate overfitting and enhance the generalizability of the model. This work advances herb classification techniques, preserving traditional botanical knowledge and promoting sustainable herb utilization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning</title>
<link>https://arxiv.org/abs/2505.02181</link>
<guid>https://arxiv.org/abs/2505.02181</guid>
<content:encoded><![CDATA[
arXiv:2505.02181v1 Announce Type: new 
Abstract: Population count (popcount) is a crucial operation for many low-complexity machine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising new ML method, particularly well-suited for solving classification tasks. The inference mechanism in TM consists of propositional logic-based structures within each class, followed by a majority voting scheme, which makes the classification decision. In TM, the voters are the outputs of Boolean clauses. The voting mechanism comprises two operations: popcount for each class and determining the class with the maximum vote by means of an argmax operation.
  While TMs offer a lightweight ML alternative, their performance is often limited by the high computational cost of popcount and comparison required to produce the argmax result. In this paper, we propose an innovative approach to accelerate and optimize these operations by performing them in the time domain. Our time-domain implementation uses programmable delay lines (PDLs) and arbiters to efficiently manage these tasks through delay-based mechanisms. We also present an FPGA design flow for practical implementation of the time-domain popcount, addressing delay skew and ensuring that the behavior matches that of the model's intended functionality. By leveraging the natural compatibility of the proposed popcount with asynchronous architectures, we demonstrate significant improvements in an asynchronous TM, including up to 38% reduction in latency, 43.1% reduction in dynamic power, and 15% savings in resource utilization, compared to synchronous TMs using adder-based popcount.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units</title>
<link>https://arxiv.org/abs/2505.02206</link>
<guid>https://arxiv.org/abs/2505.02206</guid>
<content:encoded><![CDATA[
arXiv:2505.02206v1 Announce Type: new 
Abstract: Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exogenous Isomorphism for Counterfactual Identifiability</title>
<link>https://arxiv.org/abs/2505.02212</link>
<guid>https://arxiv.org/abs/2505.02212</guid>
<content:encoded><![CDATA[
arXiv:2505.02212v1 Announce Type: new 
Abstract: This paper investigates $\sim_{\mathcal{L}_3}$-identifiability, a form of complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH) framework, ensuring that all Structural Causal Models (SCMs) satisfying the given assumptions provide consistent answers to all causal questions. To simplify this problem, we introduce exogenous isomorphism and propose $\sim_{\mathrm{EI}}$-identifiability, reflecting the strength of model identifiability required for $\sim_{\mathcal{L}_3}$-identifiability. We explore sufficient assumptions for achieving $\sim_{\mathrm{EI}}$-identifiability in two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual transport, and Triangular Monotonic SCMs (TM-SCMs), which extend $\sim_{\mathcal{L}_2}$-identifiability. Our results unify and generalize existing theories, providing theoretical guarantees for practical applications. Finally, we leverage neural TM-SCMs to address the consistency problem in counterfactual reasoning, with experiments validating both the effectiveness of our method and the correctness of the theory.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Qwen3 Quantization</title>
<link>https://arxiv.org/abs/2505.02214</link>
<guid>https://arxiv.org/abs/2505.02214</guid>
<content:encoded><![CDATA[
arXiv:2505.02214v1 Announce Type: new 
Abstract: The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Efficiency of Muon for Pretraining</title>
<link>https://arxiv.org/abs/2505.02222</link>
<guid>https://arxiv.org/abs/2505.02222</guid>
<content:encoded><![CDATA[
arXiv:2505.02222v1 Announce Type: new 
Abstract: We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning</title>
<link>https://arxiv.org/abs/2505.02228</link>
<guid>https://arxiv.org/abs/2505.02228</guid>
<content:encoded><![CDATA[
arXiv:2505.02228v1 Announce Type: new 
Abstract: Imitation Learning (IL) has achieved remarkable success across various domains, including robotics, autonomous driving, and healthcare, by enabling agents to learn complex behaviors from expert demonstrations. However, existing IL methods often face instability challenges, particularly when relying on adversarial reward or value formulations in world model frameworks. In this work, we propose a novel approach to online imitation learning that addresses these limitations through a reward model based on random network distillation (RND) for density estimation. Our reward model is built on the joint estimation of expert and behavioral distributions within the latent space of the world model. We evaluate our method across diverse benchmarks, including DMControl, Meta-World, and ManiSkill2, showcasing its ability to deliver stable performance and achieve expert-level results in both locomotion and manipulation tasks. Our approach demonstrates improved stability over adversarial methods while maintaining expert-level performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Causal Inference in Healthcare: Methods, Challenges, and Applications</title>
<link>https://arxiv.org/abs/2505.02238</link>
<guid>https://arxiv.org/abs/2505.02238</guid>
<content:encoded><![CDATA[
arXiv:2505.02238v1 Announce Type: new 
Abstract: Federated causal inference enables multi-site treatment effect estimation without sharing individual-level data, offering a privacy-preserving solution for real-world evidence generation. However, data heterogeneity across sites, manifested in differences in covariate, treatment, and outcome, poses significant challenges for unbiased and efficient estimation. In this paper, we present a comprehensive review and theoretical analysis of federated causal effect estimation across both binary/continuous and time-to-event outcomes. We classify existing methods into weight-based strategies and optimization-based frameworks and further discuss extensions including personalized models, peer-to-peer communication, and model decomposition. For time-to-event outcomes, we examine federated Cox and Aalen-Johansen models, deriving asymptotic bias and variance under heterogeneity. Our analysis reveals that FedProx-style regularization achieves near-optimal bias-variance trade-offs compared to naive averaging and meta-analysis. We review related software tools and conclude by outlining opportunities, challenges, and future directions for scalable, fair, and trustworthy federated causal inference in distributed healthcare systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation</title>
<link>https://arxiv.org/abs/2505.02247</link>
<guid>https://arxiv.org/abs/2505.02247</guid>
<content:encoded><![CDATA[
arXiv:2505.02247v1 Announce Type: new 
Abstract: 3D Geometric Graph Neural Networks (GNNs) have emerged as transformative tools for modeling molecular data. Despite their predictive power, these models often suffer from limited interpretability, raising concerns for scientific applications that require reliable and transparent insights. While existing methods have primarily focused on explaining molecular substructures in 2D GNNs, the transition to 3D GNNs introduces unique challenges, such as handling the implicit dense edge structures created by a cut-off radius. To tackle this, we introduce a novel explanation method specifically designed for 3D GNNs, which localizes the explanation to the immediate neighborhood of each node within the 3D space. Each node is assigned an radius of influence, defining the localized region within which message passing captures spatial and structural interactions crucial for the model's predictions. This method leverages the spatial and geometric characteristics inherent in 3D graphs. By constraining the subgraph to a localized radius of influence, the approach not only enhances interpretability but also aligns with the physical and structural dependencies typical of 3D graph applications, such as molecular learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Wrapping for Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2505.02277</link>
<guid>https://arxiv.org/abs/2505.02277</guid>
<content:encoded><![CDATA[
arXiv:2505.02277v1 Announce Type: new 
Abstract: Uncertainty estimation is pivotal in machine learning, especially for classification tasks, as it improves the robustness and reliability of models. We introduce a novel `Epistemic Wrapping' methodology aimed at improving uncertainty estimation in classification. Our approach uses Bayesian Neural Networks (BNNs) as a baseline and transforms their outputs into belief function posteriors, effectively capturing epistemic uncertainty and offering an efficient and general methodology for uncertainty quantification. Comprehensive experiments employing a Bayesian Neural Network (BNN) baseline and an Interval Neural Network for inference on the MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate that our Epistemic Wrapper significantly enhances generalisation and uncertainty quantification.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Approximation Theorem of Deep Q-Networks</title>
<link>https://arxiv.org/abs/2505.02288</link>
<guid>https://arxiv.org/abs/2505.02288</guid>
<content:encoded><![CDATA[
arXiv:2505.02288v1 Announce Type: new 
Abstract: We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Guided Sampling of Flat Modes in Discrete Spaces</title>
<link>https://arxiv.org/abs/2505.02296</link>
<guid>https://arxiv.org/abs/2505.02296</guid>
<content:encoded><![CDATA[
arXiv:2505.02296v1 Announce Type: new 
Abstract: Sampling from flat modes in discrete spaces is a crucial yet underexplored problem. Flat modes represent robust solutions and have broad applications in combinatorial optimization and discrete generative modeling. However, existing sampling algorithms often overlook the mode volume and struggle to capture flat modes effectively. To address this limitation, we propose \emph{Entropic Discrete Langevin Proposal} (EDLP), which incorporates local entropy into the sampling process through a continuous auxiliary variable under a joint distribution. The local entropy term guides the discrete sampler toward flat modes with a small overhead. We provide non-asymptotic convergence guarantees for EDLP in locally log-concave discrete distributions. Empirically, our method consistently outperforms traditional approaches across tasks that require sampling from flat basins, including Bernoulli distribution, restricted Boltzmann machines, combinatorial optimization, and binary neural networks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.02299</link>
<guid>https://arxiv.org/abs/2505.02299</guid>
<content:encoded><![CDATA[
arXiv:2505.02299v1 Announce Type: new 
Abstract: Machine Learning (ML) models are trained on in-distribution (ID) data but often encounter out-of-distribution (OOD) inputs during deployment -- posing serious risks in safety-critical domains. Recent works have focused on designing scoring functions to quantify OOD uncertainty, with score thresholds typically set based solely on ID data to achieve a target true positive rate (TPR), since OOD data is limited before deployment. However, these TPR-based thresholds leave false positive rates (FPR) uncontrolled, often resulting in high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring functions and thresholds lack the adaptivity needed to handle newly observed, evolving OOD inputs, leading to sub-optimal performance. To address these challenges, we propose a human-in-the-loop framework that \emph{safely updates both scoring functions and thresholds on the fly} based on real-world OOD inputs. Our method maximizes TPR while strictly controlling FPR at all times, even as the system adapts over time. We provide theoretical guarantees for FPR control under stationary conditions and present extensive empirical evaluations on OpenOOD benchmarks to demonstrate that our approach outperforms existing methods by achieving higher TPRs while maintaining FPR control.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Local Neural Operators to perform Equation-Free System-Level Analysis</title>
<link>https://arxiv.org/abs/2505.02308</link>
<guid>https://arxiv.org/abs/2505.02308</guid>
<content:encoded><![CDATA[
arXiv:2505.02308v1 Announce Type: new 
Abstract: Neural Operators (NOs) provide a powerful framework for computations involving physical laws that can be modelled by (integro-) partial differential equations (PDEs), directly learning maps between infinite-dimensional function spaces that bypass both the explicit equation identification and their subsequent numerical solving. Still, NOs have so far primarily been employed to explore the dynamical behavior as surrogates of brute-force temporal simulations/predictions. Their potential for systematic rigorous numerical system-level tasks, such as fixed-point, stability, and bifurcation analysis - crucial for predicting irreversible transitions in real-world phenomena - remains largely unexplored. Toward this aim, inspired by the Equation-Free multiscale framework, we propose and implement a framework that integrates (local) NOs with advanced iterative numerical methods in the Krylov subspace, so as to perform efficient system-level stability and bifurcation analysis of large-scale dynamical systems. Beyond fixed point, stability, and bifurcation analysis enabled by local in time NOs, we also demonstrate the usefulness of local in space as well as in space-time ("patch") NOs in accelerating the computer-aided analysis of spatiotemporal dynamics. We illustrate our framework via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE, which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN) model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node bifurcations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques</title>
<link>https://arxiv.org/abs/2505.02309</link>
<guid>https://arxiv.org/abs/2505.02309</guid>
<content:encoded><![CDATA[
arXiv:2505.02309v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training</title>
<link>https://arxiv.org/abs/2505.02360</link>
<guid>https://arxiv.org/abs/2505.02360</guid>
<content:encoded><![CDATA[
arXiv:2505.02360v1 Announce Type: new 
Abstract: Adversarial training is a cornerstone of robust deep learning, but fast methods like the Fast Gradient Sign Method (FGSM) often suffer from Catastrophic Overfitting (CO), where models become robust to single-step attacks but fail against multi-step variants. While existing solutions rely on noise injection, regularization, or gradient clipping, we propose a novel solution that purely controls the $l^p$ training norm to mitigate CO.
  Our study is motivated by the empirical observation that CO is more prevalent under the $l^{\infty}$ norm than the $l^2$ norm. Leveraging this insight, we develop a framework for generalized $l^p$ attack as a fixed point problem and craft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to $l^{\infty}$. This leads to our core insight: CO emerges when highly concentrated gradients where information localizes in few dimensions interact with aggressive norm constraints. By quantifying gradient concentration through Participation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM that automatically tunes the training norm based on gradient information. Extensive experiments demonstrate that this approach achieves strong robustness without requiring additional regularization or noise injection, providing a novel and theoretically-principled pathway to mitigate the CO problem.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks</title>
<link>https://arxiv.org/abs/2505.02369</link>
<guid>https://arxiv.org/abs/2505.02369</guid>
<content:encoded><![CDATA[
arXiv:2505.02369v1 Announce Type: new 
Abstract: Generalizing well in deep neural networks remains a core challenge, particularly due to their tendency to converge to sharp minima that degrade robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking flatter minima but perturbs parameters using the full gradient, which can include statistically insignificant directions. We propose ZSharp, a simple yet effective extension to SAM that applies layer-wise Z-score normalization followed by percentile-based filtering to retain only statistically significant gradient components. This selective perturbation aligns updates with curvature-sensitive directions, enhancing generalization without requiring architectural changes. ZSharp introduces only one additional hyperparameter, the percentile threshold, and remains fully compatible with existing SAM variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet, VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and its variants in test accuracy, particularly on deeper and transformer-based models. These results demonstrate that ZSharp is a principled and lightweight improvement for sharpness-aware optimization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices</title>
<link>https://arxiv.org/abs/2505.02380</link>
<guid>https://arxiv.org/abs/2505.02380</guid>
<content:encoded><![CDATA[
arXiv:2505.02380v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate exceptional performance across various tasks, but their large storage and computational requirements constrain their deployment on edge devices. To address this, we propose EntroLLM, a novel compression framework that integrates mixed quantization with entropy coding to reduce storage overhead while maintaining model accuracy. Our method applies a layer-wise mixed quantization scheme - choosing between symmetric and asymmetric quantization based on individual layer weight distributions - to optimize compressibility. We then employ Huffman encoding for lossless compression of the quantized weights, significantly reducing memory bandwidth requirements. Furthermore, we introduce parallel Huffman decoding, which enables efficient retrieval of encoded weights during inference, ensuring minimal latency impact. Our experiments on edge-compatible LLMs, including smolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct, demonstrate that EntroLLM achieves up to $30%$ storage reduction compared to uint8 models and up to $65%$ storage reduction compared to uint4 models, while preserving perplexity and accuracy, on language benchmark tasks. We further show that our method enables $31.9%$ - $146.6%$ faster inference throughput on memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by reducing the required data movement. The proposed approach requires no additional re-training and is fully compatible with existing post-training quantization methods, making it a practical solution for edge LLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret</title>
<link>https://arxiv.org/abs/2505.02383</link>
<guid>https://arxiv.org/abs/2505.02383</guid>
<content:encoded><![CDATA[
arXiv:2505.02383v1 Announce Type: new 
Abstract: We address differentially private stochastic bandit problems from the angles of exploring the deep connections among Thompson Sampling with Gaussian priors, Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade off privacy and regret. DP-TS-UCB satisfies $ \tilde{O} \left(T^{0.25(1-\alpha)}\right)$-GDP and enjoys an $O \left(K\ln^{\alpha+1}(T)/\Delta \right)$ regret bound, where $\alpha \in [0,1]$ controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB relies on anti-concentration bounds of Gaussian distributions and links exploration mechanisms in Thompson Sampling-based algorithms and Upper Confidence Bound-based algorithms, which may be of independent interest.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative Analysis of Performance Drop in DeepSeek Model Quantization</title>
<link>https://arxiv.org/abs/2505.02390</link>
<guid>https://arxiv.org/abs/2505.02390</guid>
<content:encoded><![CDATA[
arXiv:2505.02390v1 Announce Type: new 
Abstract: Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the models' 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of DQ3\_K\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL</title>
<link>https://arxiv.org/abs/2505.02391</link>
<guid>https://arxiv.org/abs/2505.02391</guid>
<content:encoded><![CDATA[
arXiv:2505.02391v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A probabilistic view on Riemannian machine learning models for SPD matrices</title>
<link>https://arxiv.org/abs/2505.02402</link>
<guid>https://arxiv.org/abs/2505.02402</guid>
<content:encoded><![CDATA[
arXiv:2505.02402v1 Announce Type: new 
Abstract: The goal of this paper is to show how different machine learning tools on the Riemannian manifold $\mathcal{P}_d$ of Symmetric Positive Definite (SPD) matrices can be united under a probabilistic framework. For this, we will need several Gaussian distributions defined on $\mathcal{P}_d$. We will show how popular classifiers on $\mathcal{P}_d$ can be reinterpreted as Bayes Classifiers using these Gaussian distributions. These distributions will also be used for outlier detection and dimension reduction. By showing that those distributions are pervasive in the tools used on $\mathcal{P}_d$, we allow for other machine learning tools to be extended to $\mathcal{P}_d$.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models</title>
<link>https://arxiv.org/abs/2505.02417</link>
<guid>https://arxiv.org/abs/2505.02417</guid>
<content:encoded><![CDATA[
arXiv:2505.02417v1 Announce Type: new 
Abstract: Text-to-Time Series generation holds significant potential to address challenges such as data sparsity, imbalance, and limited availability of multimodal time series datasets across domains. While diffusion models have achieved remarkable success in Text-to-X (e.g., vision and audio data) generation, their use in time series generation remains in its nascent stages. Existing approaches face two critical limitations: (1) the lack of systematic exploration of general-proposed time series captions, which are often domain-specific and struggle with generalization; and (2) the inability to generate time series of arbitrary lengths, limiting their applicability to real-world scenarios. In this work, we first categorize time series captions into three levels: point-level, fragment-level, and instance-level. Additionally, we introduce a new fragment-level dataset containing over 600,000 high-resolution time series-text pairs. Second, we propose Text-to-Series (T2S), a diffusion-based framework that bridges the gap between natural language and time series in a domain-agnostic manner. T2S employs a length-adaptive variational autoencoder to encode time series of varying lengths into consistent latent embeddings. On top of that, T2S effectively aligns textual representations with latent embeddings by utilizing Flow Matching and employing Diffusion Transformer as the denoiser. We train T2S in an interleaved paradigm across multiple lengths, allowing it to generate sequences of any desired length. Extensive evaluations demonstrate that T2S achieves state-of-the-art performance across 13 datasets spanning 12 domains.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards One-shot Federated Learning: Advances, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2505.02426</link>
<guid>https://arxiv.org/abs/2505.02426</guid>
<content:encoded><![CDATA[
arXiv:2505.02426v1 Announce Type: new 
Abstract: One-shot FL enables collaborative training in a single round, eliminating the need for iterative communication, making it particularly suitable for use in resource-constrained and privacy-sensitive applications. This survey offers a thorough examination of One-shot FL, highlighting its distinct operational framework compared to traditional federated approaches. One-shot FL supports resource-limited devices by enabling single-round model aggregation while maintaining data locality. The survey systematically categorizes existing methodologies, emphasizing advancements in client model initialization, aggregation techniques, and strategies for managing heterogeneous data distributions. Furthermore, we analyze the limitations of current approaches, particularly in terms of scalability and generalization in non-IID settings. By analyzing cutting-edge techniques and outlining open challenges, this survey aspires to provide a comprehensive reference for researchers and practitioners aiming to design and implement One-shot FL systems, advancing the development and adoption of One-shot FL solutions in a real-world, resource-constrained scenario.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairPO: Robust Preference Optimization for Fair Multi-Label Learning</title>
<link>https://arxiv.org/abs/2505.02433</link>
<guid>https://arxiv.org/abs/2505.02433</guid>
<content:encoded><![CDATA[
arXiv:2505.02433v1 Announce Type: new 
Abstract: We propose FairPO, a novel framework designed to promote fairness in multi-label classification by directly optimizing preference signals with a group robustness perspective. In our framework, the set of labels is partitioned into privileged and non-privileged groups, and a preference-based loss inspired by Direct Preference Optimization (DPO) is employed to more effectively differentiate true positive labels from confusing negatives within the privileged group, while preserving baseline classification performance for non-privileged labels. By framing the learning problem as a robust optimization over groups, our approach dynamically adjusts the training emphasis toward groups with poorer performance, thereby mitigating bias and ensuring a fairer treatment across diverse label categories. In addition, we outline plans to extend this approach by investigating alternative loss formulations such as Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization (CPO) to exploit reference-free reward formulations and contrastive training signals. Furthermore, we plan to extend FairPO with multilabel generation capabilities, enabling the model to dynamically generate diverse and coherent label sets for ambiguous inputs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability</title>
<link>https://arxiv.org/abs/2505.02435</link>
<guid>https://arxiv.org/abs/2505.02435</guid>
<content:encoded><![CDATA[
arXiv:2505.02435v1 Announce Type: new 
Abstract: Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Continual Learning in Keyword Spotting using Binary Neural Networks</title>
<link>https://arxiv.org/abs/2505.02469</link>
<guid>https://arxiv.org/abs/2505.02469</guid>
<content:encoded><![CDATA[
arXiv:2505.02469v1 Announce Type: new 
Abstract: Keyword spotting (KWS) is an essential function that enables interaction with ubiquitous smart devices. However, in resource-limited devices, KWS models are often static and can thus not adapt to new scenarios, such as added keywords. To overcome this problem, we propose a Continual Learning (CL) approach for KWS built on Binary Neural Networks (BNNs). The framework leverages the reduced computation and memory requirements of BNNs while incorporating techniques that enable the seamless integration of new keywords over time. This study evaluates seven CL techniques on a 16-class use case, reporting an accuracy exceeding 95% for a single additional keyword and up to 86% for four additional classes. Sensitivity to the amount of training samples in the CL phase, and differences in computational complexities are being evaluated. These evaluations demonstrate that batch-based algorithms are more sensitive to the CL dataset size, and that differences between the computational complexities are insignificant. These findings highlight the potential of developing an effective and computationally efficient technique for continuously integrating new keywords in KWS applications that is compatible with resource-constrained devices.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.02486</link>
<guid>https://arxiv.org/abs/2505.02486</guid>
<content:encoded><![CDATA[
arXiv:2505.02486v1 Announce Type: new 
Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting. In this paper, we explore forgetting in this context, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model's knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks' answer styles, making the results unusable. By contrast, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can obscure the model's knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for transforming data styles across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization, enabling the model to retain existing competencies. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Robust Aggregation for Federated Learning</title>
<link>https://arxiv.org/abs/2505.02490</link>
<guid>https://arxiv.org/abs/2505.02490</guid>
<content:encoded><![CDATA[
arXiv:2505.02490v1 Announce Type: new 
Abstract: Federated Learning enables collaborative training of machine learning models on decentralized data. This scheme, however, is vulnerable to adversarial attacks, when some of the clients submit corrupted model updates. In real-world scenarios, the total number of compromised clients is typically unknown, with the extent of attacks potentially varying over time. To address these challenges, we propose an adaptive approach for robust aggregation of model updates based on Bayesian inference. The mean update is defined by the maximum of the likelihood marginalized over probabilities of each client to be `honest'. As a result, the method shares the simplicity of the classical average estimators (e.g., sample mean or geometric median), being independent of the number of compromised clients. At the same time, it is as effective against attacks as methods specifically tailored to Federated Learning, such as Krum. We compare our approach with other aggregation schemes in federated setting on three benchmark image classification data sets. The proposed method consistently achieves state-of-the-art performance across various attack types with static and varying number of malicious clients.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Design Choices for Autoregressive Deep Learning Climate Models</title>
<link>https://arxiv.org/abs/2505.02506</link>
<guid>https://arxiv.org/abs/2505.02506</guid>
<content:encoded><![CDATA[
arXiv:2505.02506v1 Announce Type: new 
Abstract: Deep Learning models have achieved state-of-the-art performance in medium-range weather prediction but often fail to maintain physically consistent rollouts beyond 14 days. In contrast, a few atmospheric models demonstrate stability over decades, though the key design choices enabling this remain unclear. This study quantitatively compares the long-term stability of three prominent DL-MWP architectures - FourCastNet, SFNO, and ClimaX - trained on ERA5 reanalysis data at 5.625{\deg} resolution. We systematically assess the impact of autoregressive training steps, model capacity, and choice of prognostic variables, identifying configurations that enable stable 10-year rollouts while preserving the statistical properties of the reference dataset. Notably, rollouts with SFNO exhibit the greatest robustness to hyperparameter choices, yet all models can experience instability depending on the random seed and the set of prognostic variables
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Population PK Covariates from VAE-Generated Latent Spaces</title>
<link>https://arxiv.org/abs/2505.02514</link>
<guid>https://arxiv.org/abs/2505.02514</guid>
<content:encoded><![CDATA[
arXiv:2505.02514v1 Announce Type: new 
Abstract: Population pharmacokinetic (PopPK) modelling is a fundamental tool for understanding drug behaviour across diverse patient populations and enabling personalized dosing strategies to improve therapeutic outcomes. A key challenge in PopPK analysis lies in identifying and modelling covariates that influence drug absorption, as these relationships are often complex and nonlinear. Traditional methods may fail to capture hidden patterns within the data. In this study, we propose a data-driven, model-free framework that integrates Variational Autoencoders (VAEs) deep learning model and LASSO regression to uncover key covariates from simulated tacrolimus pharmacokinetic (PK) profiles. The VAE compresses high-dimensional PK signals into a structured latent space, achieving accurate reconstruction with a mean absolute percentage error (MAPE) of 2.26%. LASSO regression is then applied to map patient-specific covariates to the latent space, enabling sparse feature selection through L1 regularization. This approach consistently identifies clinically relevant covariates for tacrolimus including SNP, age, albumin, and hemoglobin which are retained across the tested regularization strength levels, while effectively discarding non-informative features. The proposed VAE-LASSO methodology offers a scalable, interpretable, and fully data-driven solution for covariate selection, with promising applications in drug development and precision pharmacotherapy.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization</title>
<link>https://arxiv.org/abs/2505.02515</link>
<guid>https://arxiv.org/abs/2505.02515</guid>
<content:encoded><![CDATA[
arXiv:2505.02515v1 Announce Type: new 
Abstract: Traditional domain generalization approaches predominantly focus on leveraging target domain-aware features while overlooking the critical role of source domain-specific characteristics, particularly in federated settings with inherent data isolation. To address this gap, we propose the Federated Source Domain Awareness Framework (FedSDAF), the first method to systematically exploit source domain-aware features for enhanced federated domain generalization (FedDG). The FedSDAF framework consists of two synergistic components: the Domain-Invariant Adapter, which preserves critical domain-invariant features, and the Domain-Aware Adapter, which extracts and integrates source domain-specific knowledge using a Multihead Self-Attention mechanism (MHSA). Furthermore, we introduce a bidirectional knowledge distillation mechanism that fosters knowledge sharing among clients while safeguarding privacy. Our approach represents the first systematic exploitation of source domain-aware features, resulting in significant advancements in model generalization capability.Extensive experiments on four standard benchmarks (OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently surpasses state-of-the-art federated domain generalization approaches, with accuracy gains of 5.2-13.8%. The source code is available at https://github.com/pizzareapers/FedSDAF.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations</title>
<link>https://arxiv.org/abs/2505.02537</link>
<guid>https://arxiv.org/abs/2505.02537</guid>
<content:encoded><![CDATA[
arXiv:2505.02537v1 Announce Type: new 
Abstract: Conventional techniques for imposing monotonicity in MLPs by construction involve the use of non-negative weight constraints and bounded activation functions, which pose well-known optimization challenges. In this work, we generalize previous theoretical results, showing that MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators for monotonic functions. Additionally, we show an equivalence between the saturation side in the activations and the sign of the weight constraint. This connection allows us to prove that MLPs with convex monotone activations and non-positive constrained weights also qualify as universal approximators, in contrast to their non-negative constrained counterparts. Our results provide theoretical grounding to the empirical effectiveness observed in previous works while leading to possible architectural simplification. Moreover, to further alleviate the optimization difficulties, we propose an alternative formulation that allows the network to adjust its activations according to the sign of the weights. This eliminates the requirement for weight reparameterization, easing initialization and improving training stability. Experimental evaluation reinforces the validity of the theoretical results, showing that our novel approach compares favourably to traditional monotonic architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data</title>
<link>https://arxiv.org/abs/2505.02540</link>
<guid>https://arxiv.org/abs/2505.02540</guid>
<content:encoded><![CDATA[
arXiv:2505.02540v1 Announce Type: new 
Abstract: In Federated Learning, heterogeneity in client data distributions often means that a single global model does not have the best performance for individual clients. Consider for example training a next-word prediction model for keyboards: user-specific language patterns due to demographics (dialect, age, etc.), language proficiency, and writing style result in a highly non-IID dataset across clients. Other examples are medical images taken with different machines, or driving data from different vehicle types. To address this, we propose a simple yet effective personalized federated learning framework (pFedLIA) that utilizes a computationally efficient influence approximation, called `Lazy Influence', to cluster clients in a distributed manner before model aggregation. Within each cluster, data owners collaborate to jointly train a model that captures the specific data patterns of the clients. Our method has been shown to successfully recover the global model's performance drop due to the non-IID-ness in various synthetic and real-world settings, specifically a next-word prediction task on the Nordic languages as well as several benchmark tasks. It matches the performance of a hypothetical Oracle clustering, and significantly improves on existing baselines, e.g., an improvement of 17% on CIFAR100.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bielik v3 Small: Technical Report</title>
<link>https://arxiv.org/abs/2505.02550</link>
<guid>https://arxiv.org/abs/2505.02550</guid>
<content:encoded><![CDATA[
arXiv:2505.02550v1 Announce Type: new 
Abstract: We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness questions the interpretability of graph neural networks: what to do?</title>
<link>https://arxiv.org/abs/2505.02566</link>
<guid>https://arxiv.org/abs/2505.02566</guid>
<content:encoded><![CDATA[
arXiv:2505.02566v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have become a cornerstone in graph-based data analysis, with applications in diverse domains such as bioinformatics, social networks, and recommendation systems. However, the interplay between model interpretability and robustness remains poorly understood, especially under adversarial scenarios like poisoning and evasion attacks. This paper presents a comprehensive benchmark to systematically analyze the impact of various factors on the interpretability of GNNs, including the influence of robustness-enhancing defense mechanisms.
  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across five datasets from two distinct domains, employing four interpretability metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how defenses against poisoning and evasion attacks, applied before and during model training, affect interpretability and highlights critical trade-offs between robustness and interpretability. The framework will be published as open source.
  The results reveal significant variations in interpretability depending on the chosen defense methods and model architecture characteristics. By establishing a standardized benchmark, this work provides a foundation for developing GNNs that are both robust to adversarial threats and interpretable, facilitating trust in their deployment in sensitive applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Federated Graph Learning: A Data Condensation Perspective</title>
<link>https://arxiv.org/abs/2505.02573</link>
<guid>https://arxiv.org/abs/2505.02573</guid>
<content:encoded><![CDATA[
arXiv:2505.02573v1 Announce Type: new 
Abstract: Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client graphs.However, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era</title>
<link>https://arxiv.org/abs/2505.02583</link>
<guid>https://arxiv.org/abs/2505.02583</guid>
<content:encoded><![CDATA[
arXiv:2505.02583v1 Announce Type: new 
Abstract: The proliferation of edge devices has generated an unprecedented volume of time series data across different domains, motivating various well-customized methods. Recently, Large Language Models (LLMs) have emerged as a new paradigm for time series analytics by leveraging the shared sequential nature of textual data and time series. However, a fundamental cross-modality gap between time series and LLMs exists, as LLMs are pre-trained on textual corpora and are not inherently optimized for time series. Many recent proposals are designed to address this issue. In this survey, we provide an up-to-date overview of LLMs-based cross-modality modeling for time series analytics. We first introduce a taxonomy that classifies existing approaches into four groups based on the type of textual data employed for time series modeling. We then summarize key cross-modality strategies, e.g., alignment and fusion, and discuss their applications across a range of downstream tasks. Furthermore, we conduct experiments on multimodal datasets from different application domains to investigate effective combinations of textual data and cross-modality strategies for enhancing time series analytics. Finally, we suggest several promising directions for future research. This survey is designed for a range of professionals, researchers, and practitioners interested in LLM-based time series modeling.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Loss Space in Neural Networks is Continuous and Fully Connected</title>
<link>https://arxiv.org/abs/2505.02604</link>
<guid>https://arxiv.org/abs/2505.02604</guid>
<content:encoded><![CDATA[
arXiv:2505.02604v1 Announce Type: new 
Abstract: Visualizations of the loss landscape in neural networks suggest that minima are isolated points. However, both theoretical and empirical studies indicate that it is possible to connect two different minima with a path consisting of intermediate points that also have low loss. In this study, we propose a new algorithm which investigates low-loss paths in the full parameter space, not only between two minima. Our experiments on LeNet5, ResNet18, and Compact Convolutional Transformer architectures consistently demonstrate the existence of such continuous paths in the parameter space. These results suggest that the low-loss region is a fully connected and continuous space in the parameter space. Our findings provide theoretical insight into neural network over-parameterization, highlighting that parameters collectively define a high-dimensional low-loss space, implying parameter redundancy exists only within individual models and not throughout the entire low-loss space. Additionally, our work also provides new visualization methods and opportunities to improve model generalization by exploring the low-loss space that is closer to the origin.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mirror Mean-Field Langevin Dynamics</title>
<link>https://arxiv.org/abs/2505.02621</link>
<guid>https://arxiv.org/abs/2505.02621</guid>
<content:encoded><![CDATA[
arXiv:2505.02621v1 Announce Type: new 
Abstract: The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional on the Wasserstein space over $\mathbb{R}^d$, and has gained attention recently as a model for the gradient descent dynamics of interacting particle systems such as infinite-width two-layer neural networks. However, many problems of interest have constrained domains, which are not solved by existing mean-field algorithms due to the global diffusion term. We study the optimization of probability measures constrained to a convex subset of $\mathbb{R}^d$ by proposing the \emph{mirror mean-field Langevin dynamics} (MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain linear convergence guarantees for the continuous MMFLD via a uniform log-Sobolev inequality, and uniform-in-time propagation of chaos results for its time- and particle-discretized counterpart.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition</title>
<link>https://arxiv.org/abs/2505.02627</link>
<guid>https://arxiv.org/abs/2505.02627</guid>
<content:encoded><![CDATA[
arXiv:2505.02627v1 Announce Type: new 
Abstract: Compositional generalization is a crucial property in artificial intelligence, enabling models to handle novel combinations of known components. While most deep learning models lack this capability, certain models succeed in specific tasks, suggesting the existence of governing conditions. This paper derives a necessary and sufficient condition for compositional generalization in neural networks. Conceptually, it requires that (i) the computational graph matches the true compositional structure, and (ii) components encode just enough information in training. The condition is supported by mathematical proofs. This criterion combines aspects of architecture design, regularization, and training data properties. A carefully designed minimal example illustrates an intuitive understanding of the condition. We also discuss the potential of the condition for assessing compositional generalization before training. This work is a fundamental theoretical study of compositional generalization in neural networks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.02634</link>
<guid>https://arxiv.org/abs/2505.02634</guid>
<content:encoded><![CDATA[
arXiv:2505.02634v1 Announce Type: new 
Abstract: The main objective of this paper is to introduce a transfer learning-enhanced, multi-objective, deep reinforcement learning (DRL) methodology that is able to optimise the geometry of any airfoil based on concomitant aerodynamic and structural criteria. To showcase the method, we aim to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural integrity of the airfoil -- as modelled by its maximum thickness -- and train the DRL agent using a list of different transfer learning (TL) strategies. The performance of the DRL agent is compared with Particle Swarm Optimisation (PSO), a traditional gradient-free optimisation method. Results indicate that DRL agents are able to perform multi-objective shape optimisation, that the DRL approach outperforms PSO in terms of computational efficiency and shape optimisation performance, and that the TL-enhanced DRL agent achieves performance comparable to the DRL one, while further saving substantial computational resources.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning</title>
<link>https://arxiv.org/abs/2505.02639</link>
<guid>https://arxiv.org/abs/2505.02639</guid>
<content:encoded><![CDATA[
arXiv:2505.02639v1 Announce Type: new 
Abstract: Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints</title>
<link>https://arxiv.org/abs/2505.02640</link>
<guid>https://arxiv.org/abs/2505.02640</guid>
<content:encoded><![CDATA[
arXiv:2505.02640v1 Announce Type: new 
Abstract: Internet of Things (IoT) systems increasingly operate in environments where devices must respond in real time while managing fluctuating resource constraints, including energy and bandwidth. Yet, current approaches often fall short in addressing scenarios where operational constraints evolve over time. To address these limitations, we propose a novel Budgeted Multi-Armed Bandit framework tailored for IoT applications with dynamic operational limits. Our model introduces a decaying violation budget, which permits limited constraint violations early in the learning process and gradually enforces stricter compliance over time. We present the Budgeted Upper Confidence Bound (UCB) algorithm, which adaptively balances performance optimization and compliance with time-varying constraints. We provide theoretical guarantees showing that Budgeted UCB achieves sublinear regret and logarithmic constraint violations over the learning horizon. Extensive simulations in a wireless communication setting show that our approach achieves faster adaptation and better constraint satisfaction than standard online learning methods. These results highlight the framework's potential for building adaptive, resource-aware IoT systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.02655</link>
<guid>https://arxiv.org/abs/2505.02655</guid>
<content:encoded><![CDATA[
arXiv:2505.02655v1 Announce Type: new 
Abstract: The Transformer model has shown strong performance in multivariate time series forecasting by leveraging channel-wise self-attention. However, this approach lacks temporal constraints when computing temporal features and does not utilize cumulative historical series effectively.To address these limitations, we propose the Structured Channel-wise Transformer with Cumulative Historical state (SCFormer). SCFormer introduces temporal constraints to all linear transformations, including the query, key, and value matrices, as well as the fully connected layers within the Transformer. Additionally, SCFormer employs High-order Polynomial Projection Operators (HiPPO) to deal with cumulative historical time series, allowing the model to incorporate information beyond the look-back window during prediction. Extensive experiments on multiple real-world datasets demonstrate that SCFormer significantly outperforms mainstream baselines, highlighting its effectiveness in enhancing time series forecasting. The code is publicly available at https://github.com/ShiweiGuo1995/SCFormer
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Note on Statistically Accurate Tabular Data Generation Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.02659</link>
<guid>https://arxiv.org/abs/2505.02659</guid>
<content:encoded><![CDATA[
arXiv:2505.02659v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in synthetic tabular data generation, yet existing methods struggle to preserve complex feature dependencies, particularly among categorical variables. This work introduces a probability-driven prompting approach that leverages LLMs to estimate conditional distributions, enabling more accurate and scalable data synthesis. The results highlight the potential of prompting probobility distributions to enhance the statistical fidelity of LLM-generated tabular data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework</title>
<link>https://arxiv.org/abs/2505.02712</link>
<guid>https://arxiv.org/abs/2505.02712</guid>
<content:encoded><![CDATA[
arXiv:2505.02712v1 Announce Type: new 
Abstract: Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Efficient Weight Farcasting with 1-Layer Neural Network</title>
<link>https://arxiv.org/abs/2505.02714</link>
<guid>https://arxiv.org/abs/2505.02714</guid>
<content:encoded><![CDATA[
arXiv:2505.02714v1 Announce Type: new 
Abstract: Addressing the computational challenges inherent in training large-scale deep neural networks remains a critical endeavor in contemporary machine learning research. While previous efforts have focused on enhancing training efficiency through techniques such as gradient descent with momentum, learning rate scheduling, and weight regularization, the demand for further innovation continues to burgeon as model sizes keep expanding. In this study, we introduce a novel framework which diverges from conventional approaches by leveraging long-term time series forecasting techniques. Our method capitalizes solely on initial and final weight values, offering a streamlined alternative for complex model architectures. We also introduce a novel regularizer that is tailored to enhance the forecasting performance of our approach. Empirical evaluations conducted on synthetic weight sequences and real-world deep learning architectures, including the prominent large language model DistilBERT, demonstrate the superiority of our method in terms of forecasting accuracy and computational efficiency. Notably, our framework showcases improved performance while requiring minimal additional computational overhead, thus presenting a promising avenue for accelerating the training process across diverse tasks and architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation</title>
<link>https://arxiv.org/abs/2505.02737</link>
<guid>https://arxiv.org/abs/2505.02737</guid>
<content:encoded><![CDATA[
arXiv:2505.02737v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have positioned them as a prominent solution for Natural Language Processing tasks. Notably, they can approach these problems in a zero or few-shot manner, thereby eliminating the need for training or fine-tuning task-specific models. However, LLMs face some challenges, including hallucination and the presence of outdated knowledge or missing information from specific domains in the training data. These problems cannot be easily solved by retraining the models with new data as it is a time-consuming and expensive process. To mitigate these issues, Knowledge Graphs (KGs) have been proposed as a structured external source of information to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for zero-shot Entity Disambiguation (ED). For that purpose, we leverage the hierarchical representation of the entities' classes in a KG to gradually prune the candidate space as well as the entities' descriptions to enrich the input prompt with additional factual knowledge. Our evaluation on popular ED datasets shows that the proposed method outperforms non-enhanced and description-only enhanced LLMs, and has a higher degree of adaptability than task-specific models. Furthermore, we conduct an error analysis and discuss the impact of the leveraged KG's semantic expressivity on the ED performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Bayesian and variance networks disentangle aleatoric and epistemic uncertainties</title>
<link>https://arxiv.org/abs/2505.02743</link>
<guid>https://arxiv.org/abs/2505.02743</guid>
<content:encoded><![CDATA[
arXiv:2505.02743v1 Announce Type: new 
Abstract: Real-world data contains aleatoric uncertainty - irreducible noise arising from imperfect measurements or from incomplete knowledge about the data generation process. Mean variance estimation (MVE) networks can learn this type of uncertainty but require ad-hoc regularization strategies to avoid overfitting and are unable to predict epistemic uncertainty (model uncertainty). Conversely, Bayesian neural networks predict epistemic uncertainty but are notoriously difficult to train due to the approximate nature of Bayesian inference. We propose to cooperatively train a variance network with a Bayesian neural network and demonstrate that the resulting model disentangles aleatoric and epistemic uncertainties while improving the mean estimation. We demonstrate the effectiveness and scalability of this method across a diverse range of datasets, including a time-dependent heteroscedastic regression dataset we created where the aleatoric uncertainty is known. The proposed method is straightforward to implement, robust, and adaptable to various model architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2505.02795</link>
<guid>https://arxiv.org/abs/2505.02795</guid>
<content:encoded><![CDATA[
arXiv:2505.02795v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have achieved remarkable breakthroughs, revolutionizing the natural language processing domain and beyond. Due to immense parameter sizes, fine-tuning these models with private data for diverse downstream tasks has become mainstream. Though federated learning (FL) offers a promising solution for fine-tuning LLMs without sharing raw data, substantial computing costs hinder its democratization. Moreover, in real-world scenarios, private client devices often possess heterogeneous computing resources, further complicating LLM fine-tuning. To combat these challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient fine-tuning (PEFT) framework built on split learning (SL) and low-rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on heterogeneous client devices. HSplitLoRA first identifies important weights based on their contributions to LLM training. It then dynamically configures the decomposition ranks of LoRA adapters for selected weights and determines the model split point according to varying computing budgets of client devices. Finally, a noise-free adapter aggregation mechanism is devised to support heterogeneous adapter aggregation without introducing noise. Extensive experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks in training accuracy and convergence speed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Quantifying the Hessian Structure of Neural Networks</title>
<link>https://arxiv.org/abs/2505.02809</link>
<guid>https://arxiv.org/abs/2505.02809</guid>
<content:encoded><![CDATA[
arXiv:2505.02809v1 Announce Type: new 
Abstract: Empirical studies reported that the Hessian matrix of neural networks (NNs) exhibits a near-block-diagonal structure, yet its theoretical foundation remains unclear. In this work, we reveal two forces that shape the Hessian structure: a ``static force'' rooted in the architecture design, and a ``dynamic force'' arisen from training. We then provide a rigorous theoretical analysis of ``static force'' at random initialization. We study linear models and 1-hidden-layer networks with the mean-square (MSE) loss and the Cross-Entropy (CE) loss for classification tasks. By leveraging random matrix theory, we compare the limit distributions of the diagonal and off-diagonal Hessian blocks and find that the block-diagonal structure arises as $C \rightarrow \infty$, where $C$ denotes the number of classes. Our findings reveal that $C$ is a primary driver of the near-block-diagonal structure. These results may shed new light on the Hessian structure of large language models (LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations</title>
<link>https://arxiv.org/abs/2505.01433</link>
<guid>https://arxiv.org/abs/2505.01433</guid>
<content:encoded><![CDATA[
arXiv:2505.01433v1 Announce Type: cross 
Abstract: Understanding the binding specificity between T-cell receptors (TCRs) and peptide-major histocompatibility complexes (pMHCs) is central to immunotherapy and vaccine development. However, current predictive models struggle with generalization, especially in data-scarce settings and when faced with novel epitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced Recognition Network), a deep learning framework that combines large-scale protein language models with chemical representations of peptides. By encoding TCR \b{eta}-chain sequences using ESM-1b and transforming peptide sequences into SMILES strings processed by MolFormer, LANTERN captures rich biological and chemical features critical for TCR-peptide recognition. Through extensive benchmarking against existing models such as ChemBERTa, TITAN, and NetTCR, LANTERN demonstrates superior performance, particularly in zero-shot and few-shot learning scenarios. Our model also benefits from a robust negative sampling strategy and shows significant clustering improvements via embedding analysis. These results highlight the potential of LANTERN to advance TCR-pMHC binding prediction and support the development of personalized immunotherapies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine</title>
<link>https://arxiv.org/abs/2505.01435</link>
<guid>https://arxiv.org/abs/2505.01435</guid>
<content:encoded><![CDATA[
arXiv:2505.01435v1 Announce Type: cross 
Abstract: Language models for scientific tasks are trained on text from scientific publications, most distributed as PDFs that require parsing. PDF parsing approaches range from inexpensive heuristics (for simple documents) to computationally intensive ML-driven systems (for complex or degraded ones). The choice of the "best" parser for a particular document depends on its computational cost and the accuracy of its output. To address these issues, we introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine (AdaParse), a data-driven strategy for assigning an appropriate parser to each document. We enlist scientists to select preferred parser outputs and incorporate this information through direct preference optimization (DPO) into AdaParse, thereby aligning its selection process with human judgment. AdaParse then incorporates hardware requirements and predicted accuracy of each parser to orchestrate computational resources efficiently for large-scale parsing campaigns. We demonstrate that AdaParse, when compared to state-of-the-art parsers, improves throughput by $17\times$ while still achieving comparable accuracy (0.2 percent better) on a benchmark set of 1000 scientific documents. AdaParse's combination of high accuracy and parallel scalability makes it feasible to parse large-scale scientific document corpora to support the development of high-quality, trillion-token-scale text datasets. The implementation is available at https://github.com/7shoe/AdaParse/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning</title>
<link>https://arxiv.org/abs/2505.01454</link>
<guid>https://arxiv.org/abs/2505.01454</guid>
<content:encoded><![CDATA[
arXiv:2505.01454v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet it faces significant challenges in communication efficiency and vulnerability to poisoning attacks. While sparsification techniques mitigate communication overhead by transmitting only critical model parameters, they inadvertently amplify security risks: adversarial clients can exploit sparse updates to evade detection and degrade model performance. Existing defense mechanisms, designed for standard FL communication scenarios, are ineffective in addressing these vulnerabilities within sparsified FL. To bridge this gap, we propose FLARE, a novel federated learning framework that integrates sparse index mask inspection and model update sign similarity analysis to detect and mitigate poisoning attacks in sparsified FL. Extensive experiments across multiple datasets and adversarial scenarios demonstrate that FLARE significantly outperforms existing defense strategies, effectively securing sparsified FL against poisoning attacks while maintaining communication efficiency.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seasonal Prediction with Neural GCM and Simplified Boundary Forcings: Large-scale Atmospheric Variability and Tropical Cyclone Activity</title>
<link>https://arxiv.org/abs/2505.01455</link>
<guid>https://arxiv.org/abs/2505.01455</guid>
<content:encoded><![CDATA[
arXiv:2505.01455v1 Announce Type: cross 
Abstract: Machine learning (ML) models are successful with weather forecasting and have shown progress in climate simulations, yet leveraging them for useful climate predictions needs exploration. Here we show this feasibility using NeuralGCM, a hybrid ML-physics atmospheric model, for seasonal predictions of large-scale atmospheric variability and Northern Hemisphere tropical cyclone (TC) activity. Inspired by physical model studies, we simplify boundary conditions, assuming sea surface temperature (SST) and sea ice follow their climatological cycle but persist anomalies present at initialization. With such forcings, NeuralGCM simulates realistic atmospheric circulation and TC climatology patterns. Furthermore, this configuration yields useful seasonal predictions (July-November) for the tropical atmosphere and various TC activity metrics. Notably, the prediction skill for TC frequency in the North Atlantic and East Pacific basins is comparable to existing physical models. These findings highlight the promise of leveraging ML models with physical insights to model TC risks and deliver seamless weather-climate predictions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling</title>
<link>https://arxiv.org/abs/2505.01459</link>
<guid>https://arxiv.org/abs/2505.01459</guid>
<content:encoded><![CDATA[
arXiv:2505.01459v1 Announce Type: cross 
Abstract: This paper introduces MoxE, a novel architecture that synergistically combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of Experts (MoE) framework to address critical scalability and efficiency challenges in large language models (LLMs). The proposed method effectively leverages xLSTM's innovative memory structures while strategically introducing sparsity through MoE to substantially reduce computational overhead. At the heart of our approach is a novel entropy-based routing mechanism, designed to dynamically route tokens to specialized experts, thereby ensuring efficient and balanced resource utilization. This entropy awareness enables the architecture to effectively manage both rare and common tokens, with mLSTM blocks being favored to handle rare tokens. To further enhance generalization, we introduce a suite of auxiliary losses, including entropy-based and group-wise balancing losses, ensuring robust performance and efficient training. Theoretical analysis and empirical evaluations rigorously demonstrate that MoxE achieves significant efficiency gains and enhanced effectiveness compared to existing approaches, marking a notable advancement in scalable LLM architectures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services</title>
<link>https://arxiv.org/abs/2505.01460</link>
<guid>https://arxiv.org/abs/2505.01460</guid>
<content:encoded><![CDATA[
arXiv:2505.01460v1 Announce Type: cross 
Abstract: Due to the increasing number of tasks that are solved on remote servers, identifying and classifying traffic is an important task to reduce the load on the server. There are various methods for classifying traffic. This paper discusses machine learning models for solving this problem. However, such ML models are also subject to attacks that affect the classification result of network traffic. To protect models, we proposed a solution based on an autoencoder
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Cloud Security through Topic Modelling</title>
<link>https://arxiv.org/abs/2505.01463</link>
<guid>https://arxiv.org/abs/2505.01463</guid>
<content:encoded><![CDATA[
arXiv:2505.01463v1 Announce Type: cross 
Abstract: Protecting cloud applications is crucial in an age where security constantly threatens the digital world. The inevitable cyber-attacks throughout the CI/CD pipeline make cloud security innovations necessary. This research is motivated by applying Natural Language Processing (NLP) methodologies, such as Topic Modelling, to analyse cloud security data and predict future attacks. This research aims to use topic modelling, specifically Latent Dirichlet Allocation (LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and PLSA, security-related text data, such as reports, logs, and other relevant documents, will be analysed and sorted into relevant topics (such as phishing or encryption). These algorithms may apply through Python using the Gensim framework. The topics shall be utilised to detect vulnerabilities within relevant CI/CD pipeline records or log data. This application of Topic Modelling anticipates providing a new form of vulnerability detection, improving overall security throughout the CI/CD pipeline.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos</title>
<link>https://arxiv.org/abs/2505.01481</link>
<guid>https://arxiv.org/abs/2505.01481</guid>
<content:encoded><![CDATA[
arXiv:2505.01481v1 Announce Type: cross 
Abstract: Synthetic video generation with foundation models has gained attention for its realism and wide applications. While these models produce high-quality frames, they often fail to respect common sense and physical laws, resulting in abnormal content. Existing metrics like VideoScore emphasize general quality but ignore such violations and lack interpretability. A more insightful approach is using multi-modal large language models (MLLMs) as interpretable evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities in synthetic videos remains underexplored. To address this, we introduce VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora, and Kling, paired with expert-designed QA tasks solvable via human-level reasoning across various categories. We assess several SoTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat, these models still hallucinate on basic commonsense and physics tasks in synthetic settings, underscoring the challenge of hallucination. We further fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real and synthetic commonsense/physics data. Results show notable accuracy gains, especially with counterexample integration, advancing MLLMs' reasoning capabilities. Our data is available at https://github.com/zli12321/VideoHallu.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps</title>
<link>https://arxiv.org/abs/2505.01484</link>
<guid>https://arxiv.org/abs/2505.01484</guid>
<content:encoded><![CDATA[
arXiv:2505.01484v1 Announce Type: cross 
Abstract: Given a text, can we determine whether it was generated by a large language model (LLM) or by a human? A widely studied approach to this problem is watermarking. We propose an undetectable and elementary watermarking scheme in the closed setting. Also, in the harder open setting, where the adversary has access to most of the model, we propose an unremovable watermarking scheme.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The DCR Delusion: Measuring the Privacy Risk of Synthetic Data</title>
<link>https://arxiv.org/abs/2505.01524</link>
<guid>https://arxiv.org/abs/2505.01524</guid>
<content:encoded><![CDATA[
arXiv:2505.01524v1 Announce Type: cross 
Abstract: Synthetic data has become an increasingly popular way to share data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, practitioners and researchers often rely on simpler proxy metrics such as Distance to Closest Record (DCR). These metrics estimate privacy by measuring the similarity between the training data and generated synthetic data. This similarity is also compared against that between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, it passes the test and is considered private. In this work we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models such as Baynet and CTGAN and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue DCR and other distance-based metrics to be flawed by design and show a example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoneyBee: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning</title>
<link>https://arxiv.org/abs/2505.01538</link>
<guid>https://arxiv.org/abs/2505.01538</guid>
<content:encoded><![CDATA[
arXiv:2505.01538v1 Announce Type: cross 
Abstract: As vector databases gain traction in enterprise applications, robust access control has become critical to safeguard sensitive data. Access control in these systems is often implemented through hybrid vector queries, which combine nearest neighbor search on vector data with relational predicates based on user permissions. However, existing approaches face significant trade-offs: creating dedicated indexes for each user minimizes query latency but introduces excessive storage redundancy, while building a single index and applying access control after vector search reduces storage overhead but suffers from poor recall and increased query latency. This paper introduces HoneyBee, a dynamic partitioning framework that bridges the gap between these approaches by leveraging the structure of Role-Based Access Control (RBAC) policies. RBAC, widely adopted in enterprise settings, groups users into roles and assigns permissions to those roles, creating a natural "thin waist" in the permission structure that is ideal for partitioning decisions. Specifically, HoneyBee produces overlapping partitions where vectors can be strategically replicated across different partitions to reduce query latency while controlling storage overhead. By introducing analytical models for the performance and recall of the vector search, HoneyBee formulates the partitioning strategy as a constrained optimization problem to dynamically balance storage, query efficiency, and recall. Evaluations on RBAC workloads demonstrate that HoneyBee reduces storage redundancy compared to role partitioning and achieves up to 6x faster query speeds than row-level security (RLS) with only 1.4x storage increase, offering a practical middle ground for secure and efficient vector search.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models</title>
<link>https://arxiv.org/abs/2505.01539</link>
<guid>https://arxiv.org/abs/2505.01539</guid>
<content:encoded><![CDATA[
arXiv:2505.01539v1 Announce Type: cross 
Abstract: Generative large language models as tools in the legal domain have the potential to improve the justice system. However, the reasoning behavior of current generative models is brittle and poorly understood, hence cannot be responsibly applied in the domains of law and evidence. In this paper, we introduce an approach for creating benchmarks that can be used to evaluate the reasoning capabilities of generative language models. These benchmarks are dynamically varied, scalable in their complexity, and have formally unambiguous interpretations. In this study, we illustrate the approach on the basis of witness testimony, focusing on the underlying argument attack structure. We dynamically generate both linear and non-linear argument attack graphs of varying complexity and translate these into reasoning puzzles about witness testimony expressed in natural language. We show that state-of-the-art large language models often fail in these reasoning puzzles, already at low complexity. Obvious mistakes are made by the models, and their inconsistent performance indicates that their reasoning capabilities are brittle. Furthermore, at higher complexity, even state-of-the-art models specifically presented for reasoning capabilities make mistakes. We show the viability of using a parametrized benchmark with varying complexity to evaluate the reasoning capabilities of generative language models. As such, the findings contribute to a better understanding of the limitations of the reasoning capabilities of generative models, which is essential when designing responsible AI systems in the legal domain.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the effectiveness of Large Language Models in the mechanical design domain</title>
<link>https://arxiv.org/abs/2505.01559</link>
<guid>https://arxiv.org/abs/2505.01559</guid>
<content:encoded><![CDATA[
arXiv:2505.01559v1 Announce Type: cross 
Abstract: In this work, we seek to understand the performance of large language models in the mechanical engineering domain. We leverage the semantic data found in the ABC dataset, specifically the assembly names that designers assigned to the overall assemblies, and the individual semantic part names that were assigned to each part. After pre-processing the data we developed two unsupervised tasks to evaluate how different model architectures perform on domain-specific data: a binary sentence-pair classification task and a zero-shot classification task. We achieved a 0.62 accuracy for the binary sentence-pair classification task with a fine-tuned model that focuses on fighting over-fitting: 1) modifying learning rates, 2) dropout values, 3) Sequence Length, and 4) adding a multi-head attention layer. Our model on the zero-shot classification task outperforms the baselines by a wide margin, and achieves a top-1 classification accuracy of 0.386. The results shed some light on the specific failure modes that arise when learning from language in this domain.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Always Tell Me The Odds: Fine-grained Conditional Probability Estimation</title>
<link>https://arxiv.org/abs/2505.01595</link>
<guid>https://arxiv.org/abs/2505.01595</guid>
<content:encoded><![CDATA[
arXiv:2505.01595v1 Announce Type: cross 
Abstract: We present a state-of-the-art model for fine-grained probability estimation of propositions conditioned on context. Recent advances in large language models (LLMs) have significantly enhanced their reasoning capabilities, particularly on well-defined tasks with complete information. However, LLMs continue to struggle with making accurate and well-calibrated probabilistic predictions under uncertainty or partial information. While incorporating uncertainty into model predictions often boosts performance, obtaining reliable estimates of that uncertainty remains understudied. In particular, LLM probability estimates tend to be coarse and biased towards more frequent numbers. Through a combination of human and synthetic data creation and assessment, scaling to larger models, and better supervision, we propose a set of strong and precise probability estimation models. We conduct systematic evaluations across tasks that rely on conditional probability estimation and show that our approach consistently outperforms existing fine-tuned and prompting-based methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phantora: Live GPU Cluster Simulation for Machine Learning System Performance Estimation</title>
<link>https://arxiv.org/abs/2505.01616</link>
<guid>https://arxiv.org/abs/2505.01616</guid>
<content:encoded><![CDATA[
arXiv:2505.01616v1 Announce Type: cross 
Abstract: To accommodate ever-increasing model complexity, modern machine learning (ML) systems have to scale to large GPU clusters. Changes in ML model architecture, ML system implementation, and cluster configuration can significantly affect overall ML system performance. However, quantifying the performance impact before deployment is challenging. Existing performance estimation methods use performance modeling or static workload simulation. These techniques are not general: they requires significant human effort and computation capacity to generate training data or a workload. It is also difficult to adapt ML systems to use these techniques. This paper introduces, Phantora, a live GPU cluster simulator for performance estimation. Phantora runs minimally modified ML models and frameworks, intercepting and simulating GPU-related operations to enable high-fidelity performance estimation. Phantora overcomes several research challenges in integrating an event-driven network simulator with live system execution, and introduces a set of techniques to improve simulation speed, scalability, and accuracy. Our evaluation results show that Phantora can deliver similar estimation accuracy to the state-of-the-art workload simulation approach with only one GPU, while reducing human effort and increasing generalizability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation</title>
<link>https://arxiv.org/abs/2505.01636</link>
<guid>https://arxiv.org/abs/2505.01636</guid>
<content:encoded><![CDATA[
arXiv:2505.01636v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and task generalization. However, their application to structured data analysis remains fragile due to inconsistencies in schema interpretation, misalignment between user intent and model output, and limited mechanisms for self-correction when failures occur. This paper introduces the STROT Framework (Structured Task Reasoning and Output Transformation), a method for structured prompting and feedback-driven transformation logic generation aimed at improving the reliability and semantic alignment of LLM-based analytical workflows. STROT begins with lightweight schema introspection and sample-based field classification, enabling dynamic context construction that captures both the structure and statistical profile of the input data. This contextual information is embedded in structured prompts that guide the model toward generating task-specific, interpretable outputs. To address common failure modes in complex queries, STROT incorporates a refinement mechanism in which the model iteratively revises its outputs based on execution feedback and validation signals. Unlike conventional approaches that rely on static prompts or single-shot inference, STROT treats the LLM as a reasoning agent embedded within a controlled analysis loop -- capable of adjusting its output trajectory through planning and correction. The result is a robust and reproducible framework for reasoning over structured data with LLMs, applicable to diverse data exploration and analysis tasks where interpretability, stability, and correctness are essential.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Morello: Compiling Fast Neural Networks with Dynamic Programming and Spatial Compression</title>
<link>https://arxiv.org/abs/2505.01637</link>
<guid>https://arxiv.org/abs/2505.01637</guid>
<content:encoded><![CDATA[
arXiv:2505.01637v1 Announce Type: cross 
Abstract: High-throughput neural network inference requires coordinating many optimization decisions, including parallel tiling, microkernel selection, and data layout. The product of these decisions forms a search space of programs which is typically intractably large. Existing approaches (e.g., auto-schedulers) often address this problem by sampling this space heuristically. In contrast, we introduce a dynamic-programming-based approach to explore more of the search space by iteratively decomposing large program specifications into smaller specifications reachable from a set of rewrites, then composing a final program from each rewrite that minimizes an affine cost model. To reduce memory requirements, we employ a novel memoization table representation, which indexes specifications by coordinates in $Z_{\geq 0}$ and compresses identical, adjacent solutions. This approach can visit a much larger set of programs than prior work. To evaluate the approach, we developed Morello, a compiler which lowers specifications roughly equivalent to a few-node XLA computation graph to x86. Notably, we found that an affine cost model is sufficient to surface high-throughput programs. For example, Morello synthesized a collection of matrix multiplication benchmarks targeting a Zen 1 CPU, including a 1x2048x16384, bfloat16-to-float32 vector-matrix multiply, which was integrated into Google's gemma.cpp.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Likelihood-Free Parameter Estimation for L\'evy Processes</title>
<link>https://arxiv.org/abs/2505.01639</link>
<guid>https://arxiv.org/abs/2505.01639</guid>
<content:encoded><![CDATA[
arXiv:2505.01639v1 Announce Type: cross 
Abstract: L\'evy processes are widely used in financial modeling due to their ability to capture discontinuities and heavy tails, which are common in high-frequency asset return data. However, parameter estimation remains a challenge when associated likelihoods are unavailable or costly to compute. We propose a fast and accurate method for L\'evy parameter estimation using the neural Bayes estimation (NBE) framework -- a simulation-based, likelihood-free approach that leverages permutation-invariant neural networks to approximate Bayes estimators. Through extensive simulations across several L\'evy models, we show that NBE outperforms traditional methods in both accuracy and runtime, while also enabling rapid bootstrap-based uncertainty quantification. We illustrate our approach on a challenging high-frequency cryptocurrency return dataset, where the method captures evolving parameter dynamics and delivers reliable and interpretable inference at a fraction of the computational cost of traditional methods. NBE provides a scalable and practical solution for inference in complex financial models, enabling parameter estimation and uncertainty quantification over an entire year of data in just seconds. We additionally investigate nearly a decade of high-frequency Bitcoin returns, requiring less than one minute to estimate parameters under the proposed approach.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Doppelganger Active Galactic Nuclei across redshifts from spectroscopic surveys</title>
<link>https://arxiv.org/abs/2505.01642</link>
<guid>https://arxiv.org/abs/2505.01642</guid>
<content:encoded><![CDATA[
arXiv:2505.01642v1 Announce Type: cross 
Abstract: Active Galactic Nuclei (AGNs) are among the most luminous objects in the universe, making them valuable probes for studying galaxy evolution. However, understanding how AGN properties evolve over cosmic time remains a fundamental challenge. This study investigates whether AGNs at low redshift (nearby) can serve as proxies for their high-redshift (distant) counterparts by identifying spectral 'doppelg\"angers', AGNs with remarkably similar emission line properties despite being separated by vast cosmic distances. We analyze key spectral features of bona fide AGNs using the Sloan Digital Sky Survey's Data Release 16, including continuum and emission lines: Nitrogen (N V), Carbon (C IV), Magnesium (Mg II), Hydrogen-beta (H$\beta$), and Iron (Fe II - optical and UV) emission lines. We incorporated properties such as equivalent width, velocity dispersion in the form of full width at half maximum (FWHM), and continuum luminosities (135nm, 300nm, and 510nm) closest to these prominent lines. Our initial findings suggest the existence of multiple AGNs with highly similar spectra, hinting at the possibility that local AGNs may indeed share intrinsic properties with high-redshift ones. We showcase here one of the better candidate pairs of AGNs resulting from our analyses.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp Estimation</title>
<link>https://arxiv.org/abs/2505.01654</link>
<guid>https://arxiv.org/abs/2505.01654</guid>
<content:encoded><![CDATA[
arXiv:2505.01654v1 Announce Type: cross 
Abstract: T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic system developed for autonomous leaf localization, selection, and grasping in greenhouse environments. The system integrates a 6-degree-of-freedom manipulator with a stereo vision pipeline to identify and interact with target leaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo provides dense depth maps, allowing the reconstruction of 3D leaf masks. These observations are processed through a leaf grasping algorithm that selects the optimal leaf based on clutter, visibility, and distance, and determines a grasp point by analyzing local surface flatness, top-down approachability, and margin from edges. The selected grasp point guides a trajectory executed by ROS-based motion controllers, driving a custom microneedle-equipped end-effector to clamp the leaf and simulate tissue sampling. Experiments conducted with artificial plants under varied poses demonstrate that the T-Rex system can consistently detect, plan, and perform physical interactions with plant-like targets, achieving a grasp success rate of 66.6\%. This paper presents the system architecture, implementation, and testing of T-Rex as a step toward plant sampling automation in Controlled Environment Agriculture (CEA).
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations</title>
<link>https://arxiv.org/abs/2505.01670</link>
<guid>https://arxiv.org/abs/2505.01670</guid>
<content:encoded><![CDATA[
arXiv:2505.01670v1 Announce Type: cross 
Abstract: This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that the brain signals of the subjects can be aligned in this common space during training to form a semantically aligned common brain. This is leveraged to demonstrate that aligning subject-specific lightweight modules to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios. We evaluate our methods on different datasets, demonstrating that the common space is subject and dataset-agnostic.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm</title>
<link>https://arxiv.org/abs/2505.01706</link>
<guid>https://arxiv.org/abs/2505.01706</guid>
<content:encoded><![CDATA[
arXiv:2505.01706v1 Announce Type: cross 
Abstract: Direct Preference Optimisation (DPO) has emerged as a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable and efficient alternative to approaches that use Reinforcement learning via Human Feedback. In this work, we investigate the performance of DPO using open-source preference datasets. One of the major drawbacks of DPO is that it doesn't induce granular scoring and treats all the segments of the responses with equal propensity. However, this is not practically true for human preferences since even "good" responses have segments that may not be preferred by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the advantages it provides over the standard DPO by comparing their win rates. It is observed that these methods, even though effective, are not robust to label/score noise. To counter this, we propose an approach of incorporating segment-level score noise robustness to the 2D-DPO algorithm. Along with theoretical backing, we also provide empirical verification in favour of the algorithm and introduce other noise models that can be present.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs</title>
<link>https://arxiv.org/abs/2505.01742</link>
<guid>https://arxiv.org/abs/2505.01742</guid>
<content:encoded><![CDATA[
arXiv:2505.01742v1 Announce Type: cross 
Abstract: Neural image compression, necessary in various machine-to-machine communication scenarios, suffers from its heavy encode-decode structures and inflexibility in switching between different compression levels. Consequently, it raises significant challenges in applying the neural image compression to edge devices that are developed for powerful servers with high computational and storage capacities. We take a step to solve the challenges by proposing a new transformer-based edge-compute-free image coding framework called Easz. Easz shifts the computational overhead to the server, and hence avoids the heavy encoding and model switching overhead on the edge. Easz utilizes a patch-erase algorithm to selectively remove image contents using a conditional uniform-based sampler. The erased pixels are reconstructed on the receiver side through a transformer-based framework. To further reduce the computational overhead on the receiver, we then introduce a lightweight transformer-based reconstruction structure to reduce the reconstruction load on the receiver side. Extensive evaluations conducted on a real-world testbed demonstrate multiple advantages of Easz over existing compression approaches, in terms of adaptability to different compression levels, computational efficiency, and image reconstruction quality.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding</title>
<link>https://arxiv.org/abs/2505.01743</link>
<guid>https://arxiv.org/abs/2505.01743</guid>
<content:encoded><![CDATA[
arXiv:2505.01743v1 Announce Type: cross 
Abstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (HBU) in low-resolution vision systems, such as depth, thermal, and infrared. However, existing large vision language model (LVLM) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as RGB images. A quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. In this paper, we propose a novel, labor-saving system, Llambda, designed to support low-resolution HBU. The core idea is to leverage limited labeled data and a large amount of unlabeled data to guide LLMs in generating informative captions, which can be combined with raw data to effectively fine-tune LVLM models for understanding low-resolution videos in HBU. First, we propose a Contrastive-Oriented Data Labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. Second, we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. Therefore, it can improve LLMs' understanding of sequential data and then generate high-quality video captions. Finally, to ensure on-device deployability, we employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data. We evaluate Llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that Llambda outperforms several state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dynamic view of the double descent</title>
<link>https://arxiv.org/abs/2505.01751</link>
<guid>https://arxiv.org/abs/2505.01751</guid>
<content:encoded><![CDATA[
arXiv:2505.01751v1 Announce Type: cross 
Abstract: It has been observed by Belkin et al.\ that overparametrized neural networks exhibit a `double descent' phenomenon. That is, as the model complexity, as reflected in the number of features, increases, the training error initially decreases, then increases, and then decreases again. A counterpart of this phenomenon in the time domain has been noted in the context of epoch-wise training, viz., that the training error decreases with time, then increases, then decreases again. This note presents a plausible explanation for this phenomenon by using the theory of two time scale stochastic approximation and singularly perturbed differential equations, applied to the continuous time limit of the gradient dynamics. This adds a `dynamic' angle to an already well studied theme.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias</title>
<link>https://arxiv.org/abs/2505.01754</link>
<guid>https://arxiv.org/abs/2505.01754</guid>
<content:encoded><![CDATA[
arXiv:2505.01754v1 Announce Type: cross 
Abstract: Biased news reporting poses a significant threat to informed decision-making and the functioning of democracies. This study introduces a novel methodology for scalable, minimally biased analysis of media bias in political news. The proposed approach examines event selection, labeling, word choice, and commission and omission biases across news sources by leveraging natural language processing techniques, including hierarchical topic modeling, sentiment analysis, and ontology learning with large language models. Through three case studies related to current political events, we demonstrate the methodology's effectiveness in identifying biases across news sources at various levels of granularity. This work represents a significant step towards scalable, minimally biased media bias analysis, laying the groundwork for tools to help news consumers navigate an increasingly complex media landscape.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis</title>
<link>https://arxiv.org/abs/2505.01785</link>
<guid>https://arxiv.org/abs/2505.01785</guid>
<content:encoded><![CDATA[
arXiv:2505.01785v1 Announce Type: cross 
Abstract: Estimating the causal effect of time-varying treatments on survival outcomes is a challenging task in many domains, particularly in medicine where treatment protocols adapt over time. While recent advances in representation learning have improved causal inference for static treatments, extending these methods to dynamic treatment regimes with survival outcomes remains under-explored. In this paper, we introduce TV-SurvCaus, a novel framework that extends representation balancing techniques to the time-varying treatment setting for survival analysis. We provide theoretical guarantees through (1) a generalized bound for time-varying precision in estimation of heterogeneous effects, (2) variance control via sequential balancing weights, (3) consistency results for dynamic treatment regimes, (4) convergence rates for representation learning with temporal dependencies, and (5) a formal bound on the bias due to treatment-confounder feedback. Our neural architecture incorporates sequence modeling to handle temporal dependencies while balancing time-dependent representations. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that TV-SurvCaus outperforms existing methods in estimating individualized treatment effects with time-varying covariates and treatments. Our framework advances the field of causal inference by enabling more accurate estimation of treatment effects in dynamic, longitudinal settings with survival outcomes.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis</title>
<link>https://arxiv.org/abs/2505.01800</link>
<guid>https://arxiv.org/abs/2505.01800</guid>
<content:encoded><![CDATA[
arXiv:2505.01800v1 Announce Type: cross 
Abstract: The increasing sophistication of AI-generated texts highlights the urgent need for accurate and transparent detection tools, especially in educational settings, where verifying authorship is essential. Existing literature has demonstrated that the application of stylometric features with machine learning classifiers can yield excellent results. Building on this foundation, this study proposes a comprehensive framework that integrates stylometric analysis with psycholinguistic theories, offering a clear and interpretable approach to distinguishing between AI-generated and human-written texts. This research specifically maps 31 distinct stylometric features to cognitive processes such as lexical retrieval, discourse planning, cognitive load management, and metacognitive self-monitoring. In doing so, it highlights the unique psycholinguistic patterns found in human writing. Through the intersection of computational linguistics and cognitive science, this framework contributes to the development of reliable tools aimed at preserving academic integrity in the era of generative AI.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surrogate to Poincar\'e inequalities on manifolds for dimension reduction in nonlinear feature spaces</title>
<link>https://arxiv.org/abs/2505.01807</link>
<guid>https://arxiv.org/abs/2505.01807</guid>
<content:encoded><![CDATA[
arXiv:2505.01807v1 Announce Type: cross 
Abstract: We aim to approximate a continuously differentiable function $u:\mathbb{R}^d \rightarrow \mathbb{R}$ by a composition of functions $f\circ g$ where $g:\mathbb{R}^d \rightarrow \mathbb{R}^m$, $m\leq d$, and $f : \mathbb{R}^m \rightarrow \mathbb{R}$ are built in a two stage procedure. For a fixed $g$, we build $f$ using classical regression methods, involving evaluations of $u$. Recent works proposed to build a nonlinear $g$ by minimizing a loss function $\mathcal{J}(g)$ derived from Poincar\'e inequalities on manifolds, involving evaluations of the gradient of $u$. A problem is that minimizing $\mathcal{J}$ may be a challenging task. Hence in this work, we introduce new convex surrogates to $\mathcal{J}$. Leveraging concentration inequalities, we provide sub-optimality results for a class of functions $g$, including polynomials, and a wide class of input probability measures. We investigate performances on different benchmarks for various training sample sizes. We show that our approach outperforms standard iterative methods for minimizing the training Poincar\'e inequality based loss, often resulting in better approximation errors, especially for rather small training sets and $m=1$.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge</title>
<link>https://arxiv.org/abs/2505.01812</link>
<guid>https://arxiv.org/abs/2505.01812</guid>
<content:encoded><![CDATA[
arXiv:2505.01812v1 Announce Type: cross 
Abstract: Humans and intelligent animals can effortlessly internalize new information ("news") and accurately extract the implications for performing downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the news is explicitly given as context, fine-tuning remains challenging for the models to consolidate learning in weights. In this paper, we introduce $\textit{New News}$, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. We first demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our news dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications and Self-QAs -- designed to distill the knowledge from the model with context into the weights of the model without the context, which we term $\textit{System-2 Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news. Furthermore, we discover the $\textit{contexual shadowing effect}$, where training with the news $\textit{in context}$ followed by its rephrases or QAs degrade learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp</title>
<link>https://arxiv.org/abs/2505.01816</link>
<guid>https://arxiv.org/abs/2505.01816</guid>
<content:encoded><![CDATA[
arXiv:2505.01816v1 Announce Type: cross 
Abstract: The Open Radio Access Network (O-RAN) architecture is revolutionizing cellular networks with its open, multi-vendor design and AI-driven management, aiming to enhance flexibility and reduce costs. Although it has many advantages, O-RAN is not threat-free. While previous studies have mainly examined vulnerabilities arising from O-RAN's intelligent components, this paper is the first to focus on the security challenges and vulnerabilities introduced by transitioning from single-operator to multi-operator RAN architectures. This shift increases the risk of untrusted third-party operators managing different parts of the network. To explore these vulnerabilities and their potential mitigation, we developed an open-access testbed environment that integrates a wireless network simulator with the official O-RAN Software Community (OSC) RAN intelligent component (RIC) cluster. This environment enables realistic, live data collection and serves as a platform for demonstrating APATE (adversarial perturbation against traffic efficiency), an evasion attack in which a malicious cell manipulates its reported key performance indicators (KPIs) and deceives the O-RAN traffic steering to gain unfair allocations of user equipment (UE). To ensure that O-RAN's legitimate activity continues, we introduce MARRS (monitoring adversarial RAN reports), a detection framework based on a long-short term memory (LSTM) autoencoder (AE) that learns contextual features across the network to monitor malicious telemetry (also demonstrated in our testbed). Our evaluation showed that by executing APATE, an attacker can obtain a 248.5% greater UE allocation than it was supposed to in a benign scenario. In addition, the MARRS detection method was also shown to successfully classify malicious cell activity, achieving accuracy of 99.2% and an F1 score of 0.978.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey</title>
<link>https://arxiv.org/abs/2505.01821</link>
<guid>https://arxiv.org/abs/2505.01821</guid>
<content:encoded><![CDATA[
arXiv:2505.01821v1 Announce Type: cross 
Abstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rank-One Modified Value Iteration</title>
<link>https://arxiv.org/abs/2505.01828</link>
<guid>https://arxiv.org/abs/2505.01828</guid>
<content:encoded><![CDATA[
arXiv:2505.01828v1 Announce Type: cross 
Abstract: In this paper, we provide a novel algorithm for solving planning and learning problems of Markov decision processes. The proposed algorithm follows a policy iteration-type update by using a rank-one approximation of the transition probability matrix in the policy evaluation step. This rank-one approximation is closely related to the stationary distribution of the corresponding transition probability matrix, which is approximated using the power method. We provide theoretical guarantees for the convergence of the proposed algorithm to optimal (action-)value function with the same rate and computational complexity as the value iteration algorithm in the planning problem and as the Q-learning algorithm in the learning problem. Through our extensive numerical simulations, however, we show that the proposed algorithm consistently outperforms first-order algorithms and their accelerated versions for both planning and learning problems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian learning of the optimal action-value function in a Markov decision process</title>
<link>https://arxiv.org/abs/2505.01859</link>
<guid>https://arxiv.org/abs/2505.01859</guid>
<content:encoded><![CDATA[
arXiv:2505.01859v1 Announce Type: cross 
Abstract: The Markov Decision Process (MDP) is a popular framework for sequential decision-making problems, and uncertainty quantification is an essential component of it to learn optimal decision-making strategies. In particular, a Bayesian framework is used to maintain beliefs about the optimal decisions and the unknown ingredients of the model, which are also to be learned from the data, such as the rewards and state dynamics. However, many existing Bayesian approaches for learning the optimal decision-making strategy are based on unrealistic modelling assumptions and utilise approximate inference techniques. This raises doubts whether the benefits of Bayesian uncertainty quantification are fully realised or can be relied upon.
  We focus on infinite-horizon and undiscounted MDPs, with finite state and action spaces, and a terminal state. We provide a full Bayesian framework, from modelling to inference to decision-making. For modelling, we introduce a likelihood function with minimal assumptions for learning the optimal action-value function based on Bellman's optimality equations, analyse its properties, and clarify connections to existing works. For deterministic rewards, the likelihood is degenerate and we introduce artificial observation noise to relax it, in a controlled manner, to facilitate more efficient Monte Carlo-based inference. For inference, we propose an adaptive sequential Monte Carlo algorithm to both sample from and adjust the sequence of relaxed posterior distributions. For decision-making, we choose actions using samples from the posterior distribution over the optimal strategies. While commonly done, we provide new insight that clearly shows that it is a generalisation of Thompson sampling from multi-arm bandit problems. Finally, we evaluate our framework on the Deep Sea benchmark problem and demonstrate the exploration benefits of posterior sampling in MDPs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework</title>
<link>https://arxiv.org/abs/2505.01866</link>
<guid>https://arxiv.org/abs/2505.01866</guid>
<content:encoded><![CDATA[
arXiv:2505.01866v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative model training while preserving data privacy, but its classical cryptographic underpinnings are vulnerable to quantum attacks. This vulnerability is particularly critical in sensitive domains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure Blockchain-based Federated Learning), a framework integrating post-quantum cryptography (PQC) with blockchain verification to secure FL against quantum adversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly Dilithium) signatures to authenticate model updates and leverage optimized smart contracts for decentralized validation. Extensive evaluations on diverse datasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient cryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms) with a fixed signature size of 3309 Bytes. Blockchain integration incurs a manageable overhead, with average transaction times around 4.8 s and gas usage per update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the cryptographic overhead relative to transaction time remains minimal (around 0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the bottleneck in blockchain-based FL. The system maintains competitive model accuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with round times showing sublinear growth with increasing client numbers. Our open-source implementation and reproducible benchmarks validate the feasibility of deploying long-term, quantum-resistant security in practical FL systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications</title>
<link>https://arxiv.org/abs/2505.01881</link>
<guid>https://arxiv.org/abs/2505.01881</guid>
<content:encoded><![CDATA[
arXiv:2505.01881v1 Announce Type: cross 
Abstract: Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images</title>
<link>https://arxiv.org/abs/2505.01884</link>
<guid>https://arxiv.org/abs/2505.01884</guid>
<content:encoded><![CDATA[
arXiv:2505.01884v1 Announce Type: cross 
Abstract: Inland water body segmentation from Synthetic Aperture Radar (SAR) images is an important task needed for several applications, such as flood mapping. While SAR sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from SAR images is not straightforward. Inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. U-Net is a widely used deep learning model for land-water segmentation of SAR images. In practice, manual annotation is often used to generate the corresponding water masks as ground truth. Manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. In this work, we simulate manual errors in the form of adversarial attacks on the U-Net model and study the robustness of the model to human errors in annotation. Our results indicate that U-Net can tolerate a certain level of corruption before its performance drops significantly. This finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. The code and the new dataset, along with adversarial examples for robust training, are publicly available. (Github link - https://github.com/GVCL/IWSeg-SAR-Poison.git)
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling</title>
<link>https://arxiv.org/abs/2505.01917</link>
<guid>https://arxiv.org/abs/2505.01917</guid>
<content:encoded><![CDATA[
arXiv:2505.01917v1 Announce Type: cross 
Abstract: Generative diffusion models have achieved remarkable success in producing high-quality images. However, because these models typically operate in continuous intensity spaces - diffusing independently per pixel and color channel - they are fundamentally ill-suited for applications where quantities such as particle counts or material units are inherently discrete and governed by strict conservation laws such as mass preservation, limiting their applicability in scientific workflows. To address this limitation, we propose Discrete Spatial Diffusion (DSD), a framework based on a continuous-time, discrete-state jump stochastic process that operates directly in discrete spatial domains while strictly preserving mass in both forward and reverse diffusion processes. By using spatial diffusion to achieve mass preservation, we introduce stochasticity naturally through a discrete formulation. We demonstrate the expressive flexibility of DSD by performing image synthesis, class conditioning, and image inpainting across widely-used image benchmarks, with the ability to condition on image intensity. Additionally, we highlight its applicability to domain-specific scientific data for materials microstructure, bridging the gap between diffusion models and mass-conditioned scientific applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster logconcave sampling from a cold start in high dimension</title>
<link>https://arxiv.org/abs/2505.01937</link>
<guid>https://arxiv.org/abs/2505.01937</guid>
<content:encoded><![CDATA[
arXiv:2505.01937v1 Announce Type: cross 
Abstract: We present a faster algorithm to generate a warm start for sampling an arbitrary logconcave density specified by an evaluation oracle, leading to the first sub-cubic sampling algorithms for inputs in (near-)isotropic position. A long line of prior work incurred a warm-start penalty of at least linear in the dimension, hitting a cubic barrier, even for the special case of uniform sampling from convex bodies.
  Our improvement relies on two key ingredients of independent interest. (1) We show how to sample given a warm start in weaker notions of distance, in particular $q$-R\'enyi divergence for $q=\widetilde{\mathcal{O}}(1)$, whereas previous analyses required stringent $\infty$-R\'enyi divergence (with the exception of Hit-and-Run, whose known mixing time is higher). This marks the first improvement in the required warmness since Lov\'asz and Simonovits (1991). (2) We refine and generalize the log-Sobolev inequality of Lee and Vempala (2018), originally established for isotropic logconcave distributions in terms of the diameter of the support, to logconcave distributions in terms of a geometric average of the support diameter and the largest eigenvalue of the covariance matrix.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach</title>
<link>https://arxiv.org/abs/2505.01947</link>
<guid>https://arxiv.org/abs/2505.01947</guid>
<content:encoded><![CDATA[
arXiv:2505.01947v1 Announce Type: cross 
Abstract: UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites</title>
<link>https://arxiv.org/abs/2505.01966</link>
<guid>https://arxiv.org/abs/2505.01966</guid>
<content:encoded><![CDATA[
arXiv:2505.01966v1 Announce Type: cross 
Abstract: Modular self-reconfigurable satellites refer to satellite clusters composed of individual modular units capable of altering their configurations. The configuration changes enable the execution of diverse tasks and mission objectives. Existing path planning algorithms for reconfiguration often suffer from high computational complexity, poor generalization capability, and limited support for diverse target configurations. To address these challenges, this paper proposes a goal-oriented reinforcement learning-based path planning algorithm. This algorithm is the first to address the challenge that previous reinforcement learning methods failed to overcome, namely handling multiple target configurations. Moreover, techniques such as Hindsight Experience Replay and Invalid Action Masking are incorporated to overcome the significant obstacles posed by sparse rewards and invalid actions. Based on these designs, our model achieves a 95% and 73% success rate in reaching arbitrary target configurations in a modular satellite cluster composed of four and six units, respectively.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization over Trained (and Sparse) Neural Networks: A Surrogate within a Surrogate</title>
<link>https://arxiv.org/abs/2505.01985</link>
<guid>https://arxiv.org/abs/2505.01985</guid>
<content:encoded><![CDATA[
arXiv:2505.01985v1 Announce Type: cross 
Abstract: We can approximate a constraint or an objective function that is uncertain or nonlinear with a neural network that we embed in the optimization model. This approach, which is known as constraint learning, faces the challenge that optimization models with neural network surrogates are harder to solve. Such difficulties have motivated studies on model reformulation, specialized optimization algorithms, and - to a lesser extent - pruning of the embedded networks. In this work, we double down on the use of surrogates by applying network pruning to produce a surrogate of the neural network itself. In the context of using a Mixed-Integer Linear Programming (MILP) solver to verify neural networks, we obtained faster adversarial perturbations for dense neural networks by using sparse surrogates, especially - and surprisingly - if not taking the time to finetune the sparse network to make up for the loss in accuracy. In other words, we show that a pruned network with bad classification performance can still be a good - and more efficient - surrogate.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extended Fiducial Inference for Individual Treatment Effects via Deep Neural Networks</title>
<link>https://arxiv.org/abs/2505.01995</link>
<guid>https://arxiv.org/abs/2505.01995</guid>
<content:encoded><![CDATA[
arXiv:2505.01995v1 Announce Type: cross 
Abstract: Individual treatment effect estimation has gained significant attention in recent data science literature. This work introduces the Double Neural Network (Double-NN) method to address this problem within the framework of extended fiducial inference (EFI). In the proposed method, deep neural networks are used to model the treatment and control effect functions, while an additional neural network is employed to estimate their parameters. The universal approximation capability of deep neural networks ensures the broad applicability of this method. Numerical results highlight the superior performance of the proposed Double-NN method compared to the conformal quantile regression (CQR) method in individual treatment effect estimation. From the perspective of statistical inference, this work advances the theory and methodology for statistical inference of large models. Specifically, it is theoretically proven that the proposed method permits the model size to increase with the sample size $n$ at a rate of $O(n^{\zeta})$ for some $0 \leq \zeta<1$, while still maintaining proper quantification of uncertainty in the model parameters. This result marks a significant improvement compared to the range $0\leq \zeta < \frac{1}{2}$ required by the classical central limit theorem. Furthermore, this work provides a rigorous framework for quantifying the uncertainty of deep neural networks under the neural scaling law, representing a substantial contribution to the statistical understanding of large-scale neural network models.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs</title>
<link>https://arxiv.org/abs/2505.02009</link>
<guid>https://arxiv.org/abs/2505.02009</guid>
<content:encoded><![CDATA[
arXiv:2505.02009v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. Upon publishing, we will also opensource our model signal on the entire C4 dataset. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Black-Box-Optimization through Offline Q-function Learning</title>
<link>https://arxiv.org/abs/2505.02010</link>
<guid>https://arxiv.org/abs/2505.02010</guid>
<content:encoded><![CDATA[
arXiv:2505.02010v1 Announce Type: cross 
Abstract: Recent progress in Meta-Black-Box-Optimization (MetaBBO) has demonstrated that using RL to learn a meta-level policy for dynamic algorithm configuration (DAC) over an optimization task distribution could significantly enhance the performance of the low-level BBO algorithm. However, the online learning paradigms in existing works makes the efficiency of MetaBBO problematic. To address this, we propose an offline learning-based MetaBBO framework in this paper, termed Q-Mamba, to attain both effectiveness and efficiency in MetaBBO. Specifically, we first transform DAC task into long-sequence decision process. This allows us further introduce an effective Q-function decomposition mechanism to reduce the learning difficulty within the intricate algorithm configuration space. Under this setting, we propose three novel designs to meta-learn DAC policy from offline data: we first propose a novel collection strategy for constructing offline DAC experiences dataset with balanced exploration and exploitation. We then establish a decomposition-based Q-loss that incorporates conservative Q-learning to promote stable offline learning from the offline dataset. To further improve the offline learning efficiency, we equip our work with a Mamba architecture which helps long-sequence learning effectiveness and efficiency by selective state model and hardware-aware parallel scan respectively. Through extensive benchmarking, we observe that Q-Mamba achieves competitive or even superior performance to prior online/offline baselines, while significantly improving the training efficiency of existing online baselines. We provide sourcecodes of Q-Mamba at https://github.com/MetaEvo/Q-Mamba.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning the Simplest Neural ODE</title>
<link>https://arxiv.org/abs/2505.02019</link>
<guid>https://arxiv.org/abs/2505.02019</guid>
<content:encoded><![CDATA[
arXiv:2505.02019v1 Announce Type: cross 
Abstract: Since the advent of the ``Neural Ordinary Differential Equation (Neural ODE)'' paper, learning ODEs with deep learning has been applied to system identification, time-series forecasting, and related areas. Exploiting the diffeomorphic nature of ODE solution maps, neural ODEs has also enabled their use in generative modeling. Despite the rich potential to incorporate various kinds of physical information, training Neural ODEs remains challenging in practice. This study demonstrates, through the simplest one-dimensional linear model, why training Neural ODEs is difficult. We then propose a new stabilization method and provide an analytical convergence analysis. The insights and techniques presented here serve as a concise tutorial for researchers beginning work on Neural ODEs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin</title>
<link>https://arxiv.org/abs/2505.02056</link>
<guid>https://arxiv.org/abs/2505.02056</guid>
<content:encoded><![CDATA[
arXiv:2505.02056v1 Announce Type: cross 
Abstract: Adapting vision-language models (VLMs) to downstream tasks with pseudolabels has gained increasing attention. A major obstacle is that the pseudolabels generated by VLMs tend to be imbalanced, leading to inferior performance. While existing methods have explored various strategies to address this, the underlying causes of imbalance remain insufficiently investigated. To fill this gap, we delve into imbalanced pseudolabels and identify two primary contributing factors: concept mismatch and concept confusion. To mitigate these two issues, we propose a novel framework incorporating concept alignment and confusion-aware calibrated margin mechanisms. The core of our approach lies in enhancing underperforming classes and promoting balanced predictions across categories, thus mitigating imbalance. Extensive experiments on six benchmark datasets with three learning paradigms demonstrate that the proposed method effectively enhances the accuracy and balance of pseudolabels, achieving a relative improvement of 6.29% over the SoTA method. Our code is avaliable at https://anonymous.4open.science/r/CAP-C642/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning</title>
<link>https://arxiv.org/abs/2505.02071</link>
<guid>https://arxiv.org/abs/2505.02071</guid>
<content:encoded><![CDATA[
arXiv:2505.02071v1 Announce Type: cross 
Abstract: We propose the Compact Clustering Attention (COCA) layer, an effective building block that introduces a hierarchical strategy for object-centric representation learning, while solving the unsupervised object discovery task on single images. COCA is an attention-based clustering module capable of extracting object-centric representations from multi-object scenes, when cascaded into a bottom-up hierarchical network architecture, referred to as COCA-Net. At its core, COCA utilizes a novel clustering algorithm that leverages the physical concept of compactness, to highlight distinct object centroids in a scene, providing a spatial inductive bias. Thanks to this strategy, COCA-Net generates high-quality segmentation masks on both the decoder side and, notably, the encoder side of its pipeline. Additionally, COCA-Net is not bound by a predetermined number of object masks that it generates and handles the segmentation of background elements better than its competitors. We demonstrate COCA-Net's segmentation performance on six widely adopted datasets, achieving superior or competitive results against the state-of-the-art models across nine different evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation</title>
<link>https://arxiv.org/abs/2505.02075</link>
<guid>https://arxiv.org/abs/2505.02075</guid>
<content:encoded><![CDATA[
arXiv:2505.02075v1 Announce Type: cross 
Abstract: Vision Foundation Models (VFMs) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. As VFMs' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. However, VFMs typically produce low-resolution features, limiting their direct applicability in this context. One way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines VFM features resolution. To assess the effectiveness of this approach, we investigate Interactive Segmentation (IS) as a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, IS creates a challenging environment that demands comprehensive visual scene understanding. Our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves VFM features quality. The code is released at https://github.com/havrylovv/iSegProbe
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications</title>
<link>https://arxiv.org/abs/2505.02091</link>
<guid>https://arxiv.org/abs/2505.02091</guid>
<content:encoded><![CDATA[
arXiv:2505.02091v1 Announce Type: cross 
Abstract: Solving non-convex resource allocation problems poses significant challenges in wireless communication systems, often beyond the capability of traditional optimization techniques. To address this issue, we propose LLM-OptiRA, the first framework that leverages large language models (LLMs) to automatically detect and transform non-convex components into solvable forms, enabling fully automated resolution of non-convex resource allocation problems in wireless communication systems. LLM-OptiRA not only simplifies problem-solving by reducing reliance on expert knowledge, but also integrates error correction and feasibility validation mechanisms to ensure robustness. Experimental results show that LLM-OptiRA achieves an execution rate of 96% and a success rate of 80% on GPT-4, significantly outperforming baseline approaches in complex optimization tasks across diverse scenarios.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Curvature-Aware Hypergradient Approximation for Bilevel Optimization</title>
<link>https://arxiv.org/abs/2505.02101</link>
<guid>https://arxiv.org/abs/2505.02101</guid>
<content:encoded><![CDATA[
arXiv:2505.02101v1 Announce Type: cross 
Abstract: Bilevel optimization is a powerful tool for many machine learning problems, such as hyperparameter optimization and meta-learning. Estimating hypergradients (also known as implicit gradients) is crucial for developing gradient-based methods for bilevel optimization. In this work, we propose a computationally efficient technique for incorporating curvature information into the approximation of hypergradients and present a novel algorithmic framework based on the resulting enhanced hypergradient computation. We provide convergence rate guarantees for the proposed framework in both deterministic and stochastic scenarios, particularly showing improved computational complexity over popular gradient-based methods in the deterministic setting. This improvement in complexity arises from a careful exploitation of the hypergradient structure and the inexact Newton method. In addition to the theoretical speedup, numerical experiments demonstrate the significant practical performance benefits of incorporating curvature information.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.02146</link>
<guid>https://arxiv.org/abs/2505.02146</guid>
<content:encoded><![CDATA[
arXiv:2505.02146v1 Announce Type: cross 
Abstract: Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering "Write Once, Run Anywhere" of tensor programs an open question.
  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically translating tensor programs across DLS via both large language models (LLMs) and symbolic program synthesis, i.e., neural-symbolic synthesis. The key insight is leveraging the powerful code generation ability of LLM to make costly search-based symbolic synthesis computationally tractable. Concretely, we propose multiple LLM-assisted compilation passes via pre-defined meta-prompts for program transformation. During each program transformation, efficient symbolic program synthesis is employed to repair incorrect code snippets with a limited scale. To attain high performance, we propose a hierarchical auto-tuning approach to systematically explore both the parameters and sequences of transformation passes. Experiments on 4 DLS with distinct programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA, AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler correctly translates different tensor programs at the accuracy of 95% on average, and the performance of translated programs achieves up to 2.0x over vendor-provided manually-optimized libraries. As a result, the programming productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor programs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
arXiv:2505.02156v1 Announce Type: cross 
Abstract: Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
arXiv:2505.02170v1 Announce Type: cross 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Constrained by a fixed budget, decision-makers draft a squad whose players are expected to perform well in the upcoming weeks to maximize total points. This paper proposes novel deterministic and robust integer programming models that select the optimal starting eleven and the captain. A new hybrid scoring metric is constructed using an interpretable artificial intelligence framework and underlying match performance data. Several objective functions and estimation techniques are introduced for the programming model. To the best of my knowledge, this is the first study to approach fantasy football through this lens. The models' performance is evaluated using data from the 2023/24 Premier League season. Results indicate that the proposed hybrid method achieved the highest score while maintaining consistent performance. Utilizing the Monte Carlo simulation, the strategic choice of averaging techniques for estimating cost vectors, and the proposed hybrid approach are shown to be effective during the out-of-sample period. This paper also provides a thorough analysis of the optimal formations and players selected by the models, offering valuable insights into effective fantasy football strategies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering</title>
<link>https://arxiv.org/abs/2505.02173</link>
<guid>https://arxiv.org/abs/2505.02173</guid>
<content:encoded><![CDATA[
arXiv:2505.02173v1 Announce Type: cross 
Abstract: Time series clustering is an unsupervised learning method for classifying time series data into groups with similar behavior. It is used in applications such as healthcare, finance, economics, energy, and climate science. Several time series clustering methods have been introduced and used for over four decades. Most of them focus on measuring either Euclidean distances or association dissimilarities between time series. In this work, we propose a new dissimilarity measure called ranked Pearson correlation dissimilarity (RDPC), which combines a weighted average of a specified fraction of the largest element-wise differences with the well-known Pearson correlation dissimilarity. It is incorporated into hierarchical clustering. The performance is evaluated and compared with existing clustering algorithms. The results show that the RDPC algorithm outperforms others in complicated cases involving different seasonal patterns, trends, and peaks. Finally, we demonstrate our method by clustering a random sample of customers from a Thai electricity consumption time series dataset into seven groups with unique characteristics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Variable Estimation in Bayesian Black-Litterman Models</title>
<link>https://arxiv.org/abs/2505.02185</link>
<guid>https://arxiv.org/abs/2505.02185</guid>
<content:encoded><![CDATA[
arXiv:2505.02185v1 Announce Type: cross 
Abstract: We revisit the Bayesian Black-Litterman (BL) portfolio model and remove its reliance on subjective investor views. Classical BL requires an investor "view": a forecast vector $q$ and its uncertainty matrix $\Omega$ that describe how much a chosen portfolio should outperform the market. Our key idea is to treat $(q,\Omega)$ as latent variables and learn them from market data within a single Bayesian network. Consequently, the resulting posterior estimation admits closed-form expression, enabling fast inference and stable portfolio weights. Building on these, we propose two mechanisms to capture how features interact with returns: shared-latent parametrization and feature-influenced views; both recover classical BL and Markowitz portfolios as special cases. Empirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve Sharpe ratios by 50% and cut turnover by 55% relative to Markowitz and the index baselines. This work turns BL into a fully data-driven, view-free, and coherent Bayesian framework for portfolio optimization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees</title>
<link>https://arxiv.org/abs/2505.02224</link>
<guid>https://arxiv.org/abs/2505.02224</guid>
<content:encoded><![CDATA[
arXiv:2505.02224v1 Announce Type: cross 
Abstract: A decision tree is an easy-to-understand tool that has been widely used for classification tasks. On the one hand, due to privacy concerns, there has been an urgent need to create privacy-preserving classifiers that conceal the user's input from the classifier. On the other hand, with the rise of cloud computing, data owners are keen to reduce risk by outsourcing their model, but want security guarantees that third parties cannot steal their decision tree model. To address these issues, Joye and Salehi introduced a theoretical protocol that efficiently evaluates decision trees while maintaining privacy by leveraging their comparison protocol that is resistant to timing attacks. However, their approach was not only inefficient but also prone to side-channel attacks. Therefore, in this paper, we propose a new decision tree inference protocol in which the model is shared and evaluated among multiple entities. We partition our decision tree model by each level to be stored in a new entity we refer to as a "level-site." Utilizing this approach, we were able to gain improved average run time for classifier evaluation for a non-complete tree, while also having strong mitigations against side-channel attacks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning</title>
<link>https://arxiv.org/abs/2505.02232</link>
<guid>https://arxiv.org/abs/2505.02232</guid>
<content:encoded><![CDATA[
arXiv:2505.02232v1 Announce Type: cross 
Abstract: Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heterosynaptic Circuits Are Universal Gradient Machines</title>
<link>https://arxiv.org/abs/2505.02248</link>
<guid>https://arxiv.org/abs/2505.02248</guid>
<content:encoded><![CDATA[
arXiv:2505.02248v1 Announce Type: cross 
Abstract: We propose a design principle for the learning circuits of the biological brain. The principle states that almost any dendritic weights updated via heterosynaptic plasticity can implement a generalized and efficient class of gradient-based meta-learning. The theory suggests that a broad class of biologically plausible learning algorithms, together with the standard machine learning optimizers, can be grounded in heterosynaptic circuit motifs. This principle suggests that the phenomenology of (anti-) Hebbian (HBP) and heterosynaptic plasticity (HSP) may emerge from the same underlying dynamics, thus providing a unifying explanation. It also suggests an alternative perspective of neuroplasticity, where HSP is promoted to the primary learning and memory mechanism, and HBP is an emergent byproduct. We present simulations that show that (a) HSP can explain the metaplasticity of neurons, (b) HSP can explain the flexibility of the biology circuits, and (c) gradient learning can arise quickly from simple evolutionary dynamics that do not compute any explicit gradient. While our primary focus is on biology, the principle also implies a new approach to designing AI training algorithms and physically learnable AI hardware. Conceptually, our result demonstrates that contrary to the common belief, gradient computation may be extremely easy and common in nature.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift</title>
<link>https://arxiv.org/abs/2505.02257</link>
<guid>https://arxiv.org/abs/2505.02257</guid>
<content:encoded><![CDATA[
arXiv:2505.02257v1 Announce Type: cross 
Abstract: In regions lacking medically certified causes of death, verbal autopsy (VA) is a critical and widely used tool to ascertain the cause of death through interviews with caregivers. Data collected by VAs are often analyzed using probabilistic algorithms. The performance of these algorithms often degrades due to distributional shift across populations. Most existing VA algorithms rely on centralized training, requiring full access to training data for joint modeling. This is often infeasible due to privacy and logistical constraints. In this paper, we propose a novel Bayesian Federated Learning (BFL) framework that avoids data sharing across multiple training sources. Our method enables reliable individual-level cause-of-death classification and population-level quantification of cause-specific mortality fractions (CSMFs), in a target domain with limited or no local labeled data. The proposed framework is modular, computationally efficient, and compatible with a wide range of existing VA algorithms as candidate models, facilitating flexible deployment in real-world mortality surveillance systems. We validate the performance of BFL through extensive experiments on two real-world VA datasets under varying levels of distribution shift. Our results show that BFL significantly outperforms the base models built on a single domain and achieves comparable or better performance compared to joint modeling.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Modeling of Dielectric Response in Time Domain using Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.02258</link>
<guid>https://arxiv.org/abs/2505.02258</guid>
<content:encoded><![CDATA[
arXiv:2505.02258v1 Announce Type: cross 
Abstract: Dielectric response (DR) of insulating materials is key input information for designing electrical insulation systems and defining safe operating conditions of various HV devices. In dielectric materials, different polarization and conduction processes occur at different time scales, making it challenging to physically interpret raw measured data. To analyze DR measurement results, equivalent circuit models (ECMs) are commonly used, reducing the complexity of the physical system to a number of circuit elements that capture the dominant response. This paper examines the use of physics-informed neural networks (PINNs) for inverse modeling of DR in time domain using parallel RC circuits. To assess their performance, we test PINNs on synthetic data generated from analytical solutions of corresponding ECMs, incorporating Gaussian noise to simulate measurement errors. Our results show that PINNs are highly effective at solving well-conditioned inverse problems, accurately estimating up to five unknown RC parameters with minimal requirements on neural network size, training duration, and hyperparameter tuning. Furthermore, we extend the ECMs to incorporate temperature dependence and demonstrate that PINNs can accurately recover embedded, nonlinear temperature functions from noisy DR data sampled at different temperatures. This case study in modeling DR in time domain presents a solution with wide-ranging potential applications in disciplines relying on ECMs, utilizing the latest technology in machine learning for scientific computation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Integer Encoding via Integral Balance</title>
<link>https://arxiv.org/abs/2505.02259</link>
<guid>https://arxiv.org/abs/2505.02259</guid>
<content:encoded><![CDATA[
arXiv:2505.02259v1 Announce Type: cross 
Abstract: We introduce a novel method for encoding integers using smooth real-valued functions whose integral properties implicitly reflect discrete quantities. In contrast to classical representations, where the integer appears as an explicit parameter, our approach encodes the number N in the set of natural numbers through the cumulative balance of a smooth function f_N(t), constructed from localized Gaussian bumps with alternating and decaying coefficients. The total integral I(N) converges to zero as N tends to infinity, and the integer can be recovered as the minimal point of near-cancellation.
  This method enables continuous and differentiable representations of discrete states, supports recovery through spline-based or analytical inversion, and extends naturally to multidimensional tuples (N1, N2, ...). We analyze the structure and convergence of the encoding series, demonstrate numerical construction of the integral map I(N), and develop procedures for integer recovery via numerical inversion. The resulting framework opens a path toward embedding discrete logic within continuous optimization pipelines, machine learning architectures, and smooth symbolic computation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Transformer Embeddings</title>
<link>https://arxiv.org/abs/2505.02266</link>
<guid>https://arxiv.org/abs/2505.02266</guid>
<content:encoded><![CDATA[
arXiv:2505.02266v1 Announce Type: cross 
Abstract: Embedding layers in transformer-based NLP models typically account for the largest share of model parameters, scaling with vocabulary size but not yielding performance gains proportional to scale. We propose an alternative approach in which token embedding vectors are first generated deterministically, directly from the token IDs using a Fourier expansion of their normalized values, followed by a lightweight multilayer perceptron (MLP) that captures higher-order interactions. We train standard transformers and our architecture on natural language inference tasks (SNLI and MNLI), and evaluate zero-shot performance on sentence textual similarity (STS-B). Our results demonstrate that the proposed method achieves competitive performance using significantly fewer parameters, trains faster, and operates effectively without the need for dropout. This proof-of-concept study highlights the potential for scalable, memory-efficient language models and motivates further large-scale experimentation based on our findings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles</title>
<link>https://arxiv.org/abs/2505.02281</link>
<guid>https://arxiv.org/abs/2505.02281</guid>
<content:encoded><![CDATA[
arXiv:2505.02281v1 Announce Type: cross 
Abstract: This study explores the performance of a random Gaussian smoothing zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly quasar-convex (SQC) functions in both unconstrained and constrained settings. For the unconstrained problem, we establish the ZO algorithm's convergence to a global minimum along with its complexity when applied to both QC and SQC functions. For the constrained problem, we introduce the new notion of proximal-quasar-convexity and prove analogous results to the unconstrained case. Specifically, we show the complexity bounds and the convergence of the algorithm to a neighbourhood of a global minimum whose size can be controlled under a variance reduction scheme. Theoretical findings are illustrated through investigating the performance of the algorithm applied to a range of problems in machine learning and optimisation. Specifically, we observe scenarios where the ZO method outperforms gradient descent. We provide a possible explanation for this phenomenon.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities</title>
<link>https://arxiv.org/abs/2505.02314</link>
<guid>https://arxiv.org/abs/2505.02314</guid>
<content:encoded><![CDATA[
arXiv:2505.02314v1 Announce Type: cross 
Abstract: The exponential growth of artificial intelligence (AI) applications has exposed the inefficiency of conventional von Neumann architectures, where frequent data transfers between compute units and memory create significant energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses this challenge by performing multiply-accumulate (MAC) operations directly in the memory arrays, substantially reducing data movement. However, designing robust ACIM accelerators requires accurate modeling of device- and circuit-level non-idealities. In this work, we present NeuroSim V1.5, introducing several key advances: (1) seamless integration with TensorRT's post-training quantization flow enabling support for more neural networks including transformers, (2) a flexible noise injection methodology built on pre-characterized statistical models, making it straightforward to incorporate data from SPICE simulations or silicon measurements, (3) expanded device support including emerging non-volatile capacitive memories, and (4) up to 6.5x faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The combination of these capabilities uniquely enables systematic design space exploration across both accuracy and hardware efficiency metrics. Through multiple case studies, we demonstrate optimization of critical design parameters while maintaining network accuracy. By bridging high-fidelity noise modeling with efficient simulation, NeuroSim V1.5 advances the design and validation of next-generation ACIM accelerators. All NeuroSim versions are available open-source at https://github.com/neurosim/NeuroSim.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation</title>
<link>https://arxiv.org/abs/2505.02350</link>
<guid>https://arxiv.org/abs/2505.02350</guid>
<content:encoded><![CDATA[
arXiv:2505.02350v1 Announce Type: cross 
Abstract: Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using sparse ellipsoidal radial basis function networks, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represent the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding code is publicly available at https://github.com/lianbobo/SE-RBFNet.git.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning simple heuristic rules for classifying materials based on chemical composition</title>
<link>https://arxiv.org/abs/2505.02361</link>
<guid>https://arxiv.org/abs/2505.02361</guid>
<content:encoded><![CDATA[
arXiv:2505.02361v1 Announce Type: cross 
Abstract: In the past decade, there has been a significant interest in the use of machine learning approaches in materials science research. Conventional deep learning approaches that rely on complex, nonlinear models have become increasingly important in computational materials science due to their high predictive accuracy. In contrast to these approaches, we have shown in a recent work that a remarkably simple learned heuristic rule -- based on the concept of topogivity -- can classify whether a material is topological using only its chemical composition. In this paper, we go beyond the topology classification scenario by also studying the use of machine learning to develop simple heuristic rules for classifying whether a material is a metal based on chemical composition. Moreover, we present a framework for incorporating chemistry-informed inductive bias based on the structure of the periodic table. For both the topology classification and the metallicity classification tasks, we empirically characterize the performance of simple heuristic rules fit with and without chemistry-informed inductive bias across a wide range of training set sizes. We find evidence that incorporating chemistry-informed inductive bias can reduce the amount of training data required to reach a given level of test accuracy.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models</title>
<link>https://arxiv.org/abs/2505.02362</link>
<guid>https://arxiv.org/abs/2505.02362</guid>
<content:encoded><![CDATA[
arXiv:2505.02362v1 Announce Type: cross 
Abstract: Email spam detection is a critical task in modern communication systems, essential for maintaining productivity, security, and user experience. Traditional machine learning and deep learning approaches, while effective in static settings, face significant limitations in adapting to evolving spam tactics, addressing class imbalance, and managing data scarcity. These challenges necessitate innovative approaches that reduce dependency on extensive labeled datasets and frequent retraining. This study investigates the effectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced Natural Language Processing (NLP) techniques such as BERT for email spam detection. By employing BERT to preprocess and extract critical information from email content, and FLAN-T5 to classify emails in a Zero-Shot framework, the proposed approach aims to address the limitations of traditional spam detection systems. The integration of FLAN-T5 and BERT enables robust spam detection without relying on extensive labeled datasets or frequent retraining, making it highly adaptable to unseen spam patterns and adversarial environments. This research highlights the potential of leveraging zero-shot learning and NLPs for scalable and efficient spam detection, providing insights into their capability to address the dynamic and challenging nature of spam detection tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings</title>
<link>https://arxiv.org/abs/2505.02366</link>
<guid>https://arxiv.org/abs/2505.02366</guid>
<content:encoded><![CDATA[
arXiv:2505.02366v1 Announce Type: cross 
Abstract: Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \textbf{J}oint \textbf{T}ensor representation modulus constraint and \textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence \textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing</title>
<link>https://arxiv.org/abs/2505.02370</link>
<guid>https://arxiv.org/abs/2505.02370</guid>
<content:encoded><![CDATA[
arXiv:2505.02370v1 Announce Type: cross 
Abstract: Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RM-R1: Reward Modeling as Reasoning</title>
<link>https://arxiv.org/abs/2505.02387</link>
<guid>https://arxiv.org/abs/2505.02387</guid>
<content:encoded><![CDATA[
arXiv:2505.02387v1 Announce Type: cross 
Abstract: Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans</title>
<link>https://arxiv.org/abs/2505.02388</link>
<guid>https://arxiv.org/abs/2505.02388</guid>
<content:encoded><![CDATA[
arXiv:2505.02388v1 Announce Type: cross 
Abstract: Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.02439</link>
<guid>https://arxiv.org/abs/2505.02439</guid>
<content:encoded><![CDATA[
arXiv:2505.02439v1 Announce Type: cross 
Abstract: The building thermodynamics model, which predicts real-time indoor temperature changes under potential HVAC (Heating, Ventilation, and Air Conditioning) control operations, is crucial for optimizing HVAC control in buildings. While pioneering studies have attempted to develop such models for various building environments, these models often require extensive data collection periods and rely heavily on expert knowledge, making the modeling process inefficient and limiting the reusability of the models. This paper explores a model ensemble perspective that utilizes existing developed models as base models to serve a target building environment, thereby providing accurate predictions while reducing the associated efforts. Given that building data streams are non-stationary and the number of base models may increase, we propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically select and weight the base models. Our approach employs a two-tiered decision-making process: the high-level focuses on model selection, while the low-level determines the weights of the selected models. We thoroughly evaluate the proposed approach through offline experiments and an on-site case study, and the experimental results demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning of personalized priors from past MRI scans enables fast, quality-enhanced point-of-care MRI with low-cost systems</title>
<link>https://arxiv.org/abs/2505.02470</link>
<guid>https://arxiv.org/abs/2505.02470</guid>
<content:encoded><![CDATA[
arXiv:2505.02470v1 Announce Type: cross 
Abstract: Magnetic resonance imaging (MRI) offers superb-quality images, but its accessibility is limited by high costs, posing challenges for patients requiring longitudinal care. Low-field MRI provides affordable imaging with low-cost devices but is hindered by long scans and degraded image quality, including low signal-to-noise ratio (SNR) and tissue contrast. We propose a novel healthcare paradigm: using deep learning to extract personalized features from past standard high-field MRI scans and harnessing them to enable accelerated, enhanced-quality follow-up scans with low-cost systems. To overcome the SNR and contrast differences, we introduce ViT-Fuser, a feature-fusion vision transformer that learns features from past scans, e.g. those stored in standard DICOM CDs. We show that \textit{a single prior scan is sufficient}, and this scan can come from various MRI vendors, field strengths, and pulse sequences. Experiments with four datasets, including glioblastoma data, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that ViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality images from accelerated low-field scans, with robustness to out-of-distribution data. Our freely available framework thus enables rapid, diagnostic-quality, low-cost imaging for wide healthcare applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>El Agente: An Autonomous Agent for Quantum Chemistry</title>
<link>https://arxiv.org/abs/2505.02484</link>
<guid>https://arxiv.org/abs/2505.02484</guid>
<content:encoded><![CDATA[
arXiv:2505.02484v1 Announce Type: cross 
Abstract: Computational chemistry tools are widely used to study the behaviour of chemical phenomena. Yet, the complexity of these tools can make them inaccessible to non-specialists and challenging even for experts. In this work, we introduce El Agente Q, an LLM-based multi-agent system that dynamically generates and executes quantum chemistry workflows from natural language user prompts. The system is built on a novel cognitive architecture featuring a hierarchical memory framework that enables flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling and submission. El Agente Q is benchmarked on six university-level course exercises and two case studies, demonstrating robust problem-solving performance (averaging >87% task success) and adaptive error handling through in situ debugging. It also supports longer-term, multi-step task execution for more complex workflows, while maintaining transparency through detailed action trace logs. Together, these capabilities lay the foundation for increasingly autonomous and accessible quantum chemistry.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions</title>
<link>https://arxiv.org/abs/2505.02501</link>
<guid>https://arxiv.org/abs/2505.02501</guid>
<content:encoded><![CDATA[
arXiv:2505.02501v1 Announce Type: cross 
Abstract: We introduce Corr2Distrib, the first correspondence-based method which estimates a 6D camera pose distribution from an RGB image, explaining the observations. Indeed, symmetries and occlusions introduce visual ambiguities, leading to multiple valid poses. While a few recent methods tackle this problem, they do not rely on local correspondences which, according to the BOP Challenge, are currently the most effective way to estimate a single 6DoF pose solution. Using correspondences to estimate a pose distribution is not straightforward, since ambiguous correspondences induced by visual ambiguities drastically decrease the performance of PnP. With Corr2Distrib, we turn these ambiguities into an advantage to recover all valid poses. Corr2Distrib first learns a symmetry-aware representation for each 3D point on the object's surface, characterized by a descriptor and a local frame. This representation enables the generation of 3DoF rotation hypotheses from single 2D-3D correspondences. Next, we refine these hypotheses into a 6DoF pose distribution using PnP and pose scoring. Our experimental evaluations on complex non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art solutions for both pose distribution estimation and single pose estimation from an RGB image, demonstrating the potential of correspondences-based approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resolving Memorization in Empirical Diffusion Model for Manifold Data in High-Dimensional Spaces</title>
<link>https://arxiv.org/abs/2505.02508</link>
<guid>https://arxiv.org/abs/2505.02508</guid>
<content:encoded><![CDATA[
arXiv:2505.02508v1 Announce Type: cross 
Abstract: Diffusion models is a popular computational tool to generate new data samples. It utilizes a forward diffusion process that add noise to the data distribution and then use a reverse process to remove noises to produce samples from the data distribution. However, when the empirical data distribution consists of $n$ data point, using the empirical diffusion model will necessarily produce one of the existing data points. This is often referred to as the memorization effect, which is usually resolved by sophisticated machine learning procedures in the current literature. This work shows that the memorization problem can be resolved by a simple inertia update step at the end of the empirical diffusion model simulation. Our inertial diffusion model requires only the empirical diffusion model score function and it does not require any further training. We show that choosing the inertia diffusion model sample distribution is an $O\left(n^{-\frac{2}{d+4}}\right)$ Wasserstein-1 approximation of a data distribution lying on a $C^2$ manifold of dimension $d$. Since this estimate is significant smaller the Wasserstein1 distance between population and empirical distributions, it rigorously shows the inertial diffusion model produces new data samples. Remarkably, this upper bound is completely free of the ambient space dimension, since there is no training involved. Our analysis utilizes the fact that the inertial diffusion model samples are approximately distributed as the Gaussian kernel density estimator on the manifold. This reveals an interesting connection between diffusion model and manifold learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics</title>
<link>https://arxiv.org/abs/2505.02516</link>
<guid>https://arxiv.org/abs/2505.02516</guid>
<content:encoded><![CDATA[
arXiv:2505.02516v1 Announce Type: cross 
Abstract: Advanced neural interfaces are transforming applications ranging from neuroscience research to diagnostic tools (for mental state recognition, tremor and seizure detection) as well as prosthetic devices (for motor and communication recovery). By integrating complex functions into miniaturized neural devices, these systems unlock significant opportunities for personalized assistive technologies and adaptive therapeutic interventions. Leveraging high-density neural recordings, on-site signal processing, and machine learning (ML), these interfaces extract critical features, identify disease neuro-markers, and enable accurate, low-latency neural decoding. This integration facilitates real-time interpretation of neural signals, adaptive modulation of brain activity, and efficient control of assistive devices. Moreover, the synergy between neural interfaces and ML has paved the way for self-sufficient, ubiquitous platforms capable of operating in diverse environments with minimal hardware costs and external dependencies. In this work, we review recent advancements in AI-driven decoding algorithms and energy-efficient System-on-Chip (SoC) platforms for next-generation miniaturized neural devices. These innovations highlight the potential for developing intelligent neural interfaces, addressing critical challenges in scalability, reliability, interpretability, and user adaptability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning and Online Replication of Grasp Forces from Electromyography Signals for Prosthetic Finger Control</title>
<link>https://arxiv.org/abs/2505.02574</link>
<guid>https://arxiv.org/abs/2505.02574</guid>
<content:encoded><![CDATA[
arXiv:2505.02574v1 Announce Type: cross 
Abstract: Partial hand amputations significantly affect the physical and psychosocial well-being of individuals, yet intuitive control of externally powered prostheses remains an open challenge. To address this gap, we developed a force-controlled prosthetic finger activated by electromyography (EMG) signals. The prototype, constructed around a wrist brace, functions as a supernumerary finger placed near the index, allowing for early-stage evaluation on unimpaired subjects. A neural network-based model was then implemented to estimate fingertip forces from EMG inputs, allowing for online adjustment of the prosthetic finger grip strength. The force estimation model was validated through experiments with ten participants, demonstrating its effectiveness in predicting forces. Additionally, online trials with four users wearing the prosthesis exhibited precise control over the device. Our findings highlight the potential of using EMG-based force estimation to enhance the functionality of prosthetic fingers.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning</title>
<link>https://arxiv.org/abs/2505.02576</link>
<guid>https://arxiv.org/abs/2505.02576</guid>
<content:encoded><![CDATA[
arXiv:2505.02576v1 Announce Type: cross 
Abstract: Reasoning tasks are crucial in many domains, especially in science and engineering. Although large language models (LLMs) have made progress in reasoning tasks using techniques such as chain-of-thought and least-to-most prompting, these approaches still do not effectively scale to complex problems in either their performance or execution time. Moreover, they often require additional supervision for each new task, such as in-context examples. In this work, we introduce Recursive Decomposition with Dependencies (RDD), a scalable divide-and-conquer method for solving reasoning problems that requires less supervision than prior approaches. Our method can be directly applied to a new problem class even in the absence of any task-specific guidance. Furthermore, RDD supports sub-task dependencies, allowing for ordered execution of sub-tasks, as well as an error recovery mechanism that can correct mistakes made in previous steps. We evaluate our approach on two benchmarks with six difficulty levels each and in two in-context settings: one with task-specific examples and one without. Our results demonstrate that RDD outperforms other methods in a compute-matched setting as task complexity increases, while also being more computationally efficient.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.02579</link>
<guid>https://arxiv.org/abs/2505.02579</guid>
<content:encoded><![CDATA[
arXiv:2505.02579v1 Announce Type: cross 
Abstract: Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lane-Wise Highway Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.02613</link>
<guid>https://arxiv.org/abs/2505.02613</guid>
<content:encoded><![CDATA[
arXiv:2505.02613v1 Announce Type: cross 
Abstract: This paper proposes a scalable and interpretable framework for lane-wise highway traffic anomaly detection, leveraging multi-modal time series data extracted from surveillance cameras. Unlike traditional sensor-dependent methods, our approach uses AI-powered vision models to extract lane-specific features, including vehicle count, occupancy, and truck percentage, without relying on costly hardware or complex road modeling. We introduce a novel dataset containing 73,139 lane-wise samples, annotated with four classes of expert-validated anomalies: three traffic-related anomalies (lane blockage and recovery, foreign object intrusion, and sustained congestion) and one sensor-related anomaly (camera angle shift). Our multi-branch detection system integrates deep learning, rule-based logic, and machine learning to improve robustness and precision. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods in precision, recall, and F1-score, providing a cost-effective and scalable solution for real-world intelligent transportation systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropic Mirror Descent for Linear Systems: Polyak's Stepsize and Implicit Bias</title>
<link>https://arxiv.org/abs/2505.02614</link>
<guid>https://arxiv.org/abs/2505.02614</guid>
<content:encoded><![CDATA[
arXiv:2505.02614v1 Announce Type: cross 
Abstract: This paper focuses on applying entropic mirror descent to solve linear systems, where the main challenge for the convergence analysis stems from the unboundedness of the domain. To overcome this without imposing restrictive assumptions, we introduce a variant of Polyak-type stepsizes. Along the way, we strengthen the bound for $\ell_1$-norm implicit bias, obtain sublinear and linear convergence results, and generalize the convergence result to arbitrary convex $L$-smooth functions. We also propose an alternative method that avoids exponentiation, resembling the original Hadamard descent, but with provable convergence.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye Movements as Indicators of Deception: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2505.02649</link>
<guid>https://arxiv.org/abs/2505.02649</guid>
<content:encoded><![CDATA[
arXiv:2505.02649v1 Announce Type: cross 
Abstract: Gaze may enhance the robustness of lie detectors but remains under-studied. This study evaluated the efficacy of AI models (using fixations, saccades, blinks, and pupil size) for detecting deception in Concealed Information Tests across two datasets. The first, collected with Eyelink 1000, contains gaze data from a computerized experiment where 87 participants revealed, concealed, or faked the value of a previously selected card. The second, collected with Pupil Neon, involved 36 participants performing a similar task but facing an experimenter. XGBoost achieved accuracies up to 74% in a binary classification task (Revealing vs. Concealing) and 49% in a more challenging three-classification task (Revealing vs. Concealing vs. Faking). Feature analysis identified saccade number, duration, amplitude, and maximum pupil size as the most important for deception prediction. These results demonstrate the feasibility of using gaze and AI to enhance lie detectors and encourage future research that may improve on this.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter</title>
<link>https://arxiv.org/abs/2505.02664</link>
<guid>https://arxiv.org/abs/2505.02664</guid>
<content:encoded><![CDATA[
arXiv:2505.02664v1 Announce Type: cross 
Abstract: Grasp pose detection in cluttered, real-world environments remains a significant challenge due to noisy and incomplete sensory data combined with complex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0) method, a lightweight yet highly effective hypothesis-and-test robotics grasping framework which leverages an ensemble of Graph Neural Networks for efficient geometric reasoning from point cloud data. Building on the success of GtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp detection but was limited by assumptions of complete, noise-free point clouds and 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to efficiently produce 7-Dof grasp candidates. Candidates are assessed with an ensemble Graph Neural Network model which includes points within the gripper jaws (inside points) and surrounding contextual points (outside points). This improved representation boosts grasp detection performance over previous methods using the same generator. GtG 2.0 shows up to a 35% improvement in Average Precision on the GraspNet-1Billion benchmark compared to hypothesis-and-test and Graph Neural Network-based methods, ranking it among the top three frameworks. Experiments with a 3-Dof Delta Parallel robot and Kinect-v1 camera show a success rate of 91% and a clutter completion rate of 100%, demonstrating its flexibility and reliability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report: Evaluating Goal Drift in Language Model Agents</title>
<link>https://arxiv.org/abs/2505.02709</link>
<guid>https://arxiv.org/abs/2505.02709</guid>
<content:encoded><![CDATA[
arXiv:2505.02709v1 Announce Type: cross 
Abstract: As language models (LMs) are increasingly deployed as autonomous agents, their robust adherence to human-assigned objectives becomes crucial for safe operation. When these agents operate independently for extended periods without human oversight, even initially well-specified goals may gradually shift. Detecting and measuring goal drift - an agent's tendency to deviate from its original objective over time - presents significant challenges, as goals can shift gradually, causing only subtle behavioral changes. This paper proposes a novel approach to analyzing goal drift in LM agents. In our experiments, agents are first explicitly given a goal through their system prompt, then exposed to competing objectives through environmental pressures. We demonstrate that while the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains nearly perfect goal adherence for more than 100,000 tokens in our most difficult evaluation setting, all evaluated models exhibit some degree of goal drift. We also find that goal drift correlates with models' increasing susceptibility to pattern-matching behaviors as the context length grows.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry</title>
<link>https://arxiv.org/abs/2505.02722</link>
<guid>https://arxiv.org/abs/2505.02722</guid>
<content:encoded><![CDATA[
arXiv:2505.02722v1 Announce Type: cross 
Abstract: Although large language models (LLMs) have demonstrated impressive reasoning capabilities across general domains, their effectiveness in real-world clinical practice remains limited. This is likely due to their insufficient exposure to real-world clinical data during training, as such data is typically not included due to privacy concerns. To address this, we propose enhancing the clinical reasoning capabilities of LLMs by leveraging real-world clinical data. We constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the in-domain test set, as evidenced by both quantitative metrics and expert evaluations. Furthermore, its enhanced reasoning capabilities generalized to a sepsis dataset involving different tasks and patient cohorts, an open-ended consultations on antibiotics use task, and other diseases. Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2505.02735</link>
<guid>https://arxiv.org/abs/2505.02735</guid>
<content:encoded><![CDATA[
arXiv:2505.02735v1 Announce Type: cross 
Abstract: Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Knowledge Graphs to harvest datasets for efficient CLIP model training</title>
<link>https://arxiv.org/abs/2505.02746</link>
<guid>https://arxiv.org/abs/2505.02746</guid>
<content:encoded><![CDATA[
arXiv:2505.02746v1 Announce Type: cross 
Abstract: Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Bidding Policies for First-Price Auctions with Budget Constraints under Non-stationarity</title>
<link>https://arxiv.org/abs/2505.02796</link>
<guid>https://arxiv.org/abs/2505.02796</guid>
<content:encoded><![CDATA[
arXiv:2505.02796v1 Announce Type: cross 
Abstract: We study how a budget-constrained bidder should learn to adaptively bid in repeated first-price auctions to maximize her cumulative payoff. This problem arose due to an industry-wide shift from second-price auctions to first-price auctions in display advertising recently, which renders truthful bidding (i.e., always bidding one's private value) no longer optimal. We propose a simple dual-gradient-descent-based bidding policy that maintains a dual variable for budget constraint as the bidder consumes her budget. In analysis, we consider two settings regarding the bidder's knowledge of her private values in the future: (i) an uninformative setting where all the distributional knowledge (can be non-stationary) is entirely unknown to the bidder, and (ii) an informative setting where a prediction of the budget allocation in advance. We characterize the performance loss (or regret) relative to an optimal policy with complete information on the stochasticity. For uninformative setting, We show that the regret is \tilde{O}(\sqrt{T}) plus a variation term that reflects the non-stationarity of the value distributions, and this is of optimal order. We then show that we can get rid of the variation term with the help of the prediction; specifically, the regret is \tilde{O}(\sqrt{T}) plus the prediction error term in the informative setting.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoLibra: Agent Metric Induction from Open-Ended Feedback</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
arXiv:2505.02820v1 Announce Type: cross 
Abstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., "If you find that the button is disabled, don't click it again", or "This agent has too much autonomy to decide what to do on its own", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: "coverage" and "redundancy". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TWIST: Teleoperated Whole-Body Imitation System</title>
<link>https://arxiv.org/abs/2505.02833</link>
<guid>https://arxiv.org/abs/2505.02833</guid>
<content:encoded><![CDATA[
arXiv:2505.02833v1 Announce Type: cross 
Abstract: Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data</title>
<link>https://arxiv.org/abs/2302.06375</link>
<guid>https://arxiv.org/abs/2302.06375</guid>
<content:encoded><![CDATA[
arXiv:2302.06375v4 Announce Type: replace 
Abstract: There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2306.15546</link>
<guid>https://arxiv.org/abs/2306.15546</guid>
<content:encoded><![CDATA[
arXiv:2306.15546v3 Announce Type: replace 
Abstract: The intersection of Foundation Model (FM) and Federated Learning (FL) presents a unique opportunity to unlock new possibilities for real-world applications. On the one hand, FL, as a collaborative learning paradigm, help address challenges in FM development by expanding data availability, enabling computation sharing, facilitating the collaborative development of FMs, tackling continuous data update, avoiding FM monopoly, response delay and FM service down. On the other hand, FM, equipped with pre-trained knowledge and exceptional performance, can serve as a robust starting point for FL. It can also generate synthetic data to enrich data diversity and enhance overall performance of FL. Meanwhile, FM unlocks new sharing paradigm and multi-task and multi-modality capabilities for FL. By examining the interplay between FL and FM, this paper presents the motivations, challenges, and future directions of empowering FL with FM and empowering FM with FL. We hope that this work provides a good foundation to inspire future research efforts to drive advancements in both fields.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection</title>
<link>https://arxiv.org/abs/2309.15670</link>
<guid>https://arxiv.org/abs/2309.15670</guid>
<content:encoded><![CDATA[
arXiv:2309.15670v2 Announce Type: replace 
Abstract: In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT), a well-known methodology of transformers, have been shown the best results of all methods implemented. Finally, a web application has been developed to demonstrate the performance of the pre-trained top-performer model (BERT) for multi-label ER in Bangla.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A simple connection from loss flatness to compressed neural representations</title>
<link>https://arxiv.org/abs/2310.01770</link>
<guid>https://arxiv.org/abs/2310.01770</guid>
<content:encoded><![CDATA[
arXiv:2310.01770v4 Announce Type: replace 
Abstract: Sharpness, a geometric measure in the parameter space that reflects the flatness of the loss landscape, has long been studied for its potential connections to neural network behavior. While sharpness is often associated with generalization, recent work highlights inconsistencies in this relationship, leaving its true significance unclear. In this paper, we investigate how sharpness influences the local geometric features of neural representations in feature space, offering a new perspective on its role. We introduce this problem and study three measures for compression: the Local Volumetric Ratio (LVR), based on volume compression, the Maximum Local Sensitivity (MLS), based on sensitivity to input changes, and the Local Dimensionality, based on how uniform the sensitivity is on different directions. We show that LVR and MLS correlate with the flatness of the loss around the local minima; and that this correlation is predicted by a relatively simple mathematical relationship: a flatter loss corresponds to a lower upper bound on the compression metrics of neural representations. Our work builds upon the linear stability insight by Ma and Ying, deriving inequalities between various compression metrics and quantities involving sharpness. Our inequalities readily extend to reparametrization-invariant sharpness as well. Through empirical experiments on various feedforward, convolutional, and transformer architectures, we find that our inequalities predict a consistently positive correlation between local representation compression and sharpness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supercharging Graph Transformers with Advective Diffusion</title>
<link>https://arxiv.org/abs/2310.06417</link>
<guid>https://arxiv.org/abs/2310.06417</guid>
<content:encoded><![CDATA[
arXiv:2310.06417v2 Announce Type: replace 
Abstract: The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off</title>
<link>https://arxiv.org/abs/2402.07002</link>
<guid>https://arxiv.org/abs/2402.07002</guid>
<content:encoded><![CDATA[
arXiv:2402.07002v2 Announce Type: replace 
Abstract: To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This capability implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our theoretical results with experiments on representative datasets and observe significant performance improvements and strict privacy guarantees under different privacy settings. The code is available at https://github.com/6lyc/FedCEO_Collaborate-with-Each-Other.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization</title>
<link>https://arxiv.org/abs/2402.15290</link>
<guid>https://arxiv.org/abs/2402.15290</guid>
<content:encoded><![CDATA[
arXiv:2402.15290v4 Announce Type: replace 
Abstract: Existing models encounter bottlenecks in balancing performance and computational efficiency when modeling long sequences. Although the state space model (SSM) has achieved remarkable success in handling long sequence tasks, it still faces the problem of large number of parameters. In order to further improve the efficiency of SSM, we propose a new state space layer based on multiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is built on the convolutional representation of multi-input and multi-input (MIMO) SSM. We propose a variety of effective strategies to improve the computational efficiency. The diagonalization of the system matrix first decouples the original system. Then a fast tensor convolution is proposed based on the fast Fourier transform. In addition, the block diagonalization of the SSM further reduces the model parameters and improves the model flexibility. Extensive experimental results show that the performance of the proposed model on multiple databases matches the performance of state-of-the-art models, such as S4, and is significantly better than Transformers and LSTM. In the model efficiency benchmark, the parameters of eSSM are only 12.89\% of LSTM and 13.24\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and 1.35 times faster than Mamba. Code is available at: \href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>