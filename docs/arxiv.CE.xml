<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CE updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CE</link>


<item>
<title>Flow of Knowledge: Federated Fine-Tuning of LLMs in Healthcare under Non-IID Conditions</title>
<link>https://arxiv.org/abs/2510.00543</link>
<guid>https://arxiv.org/abs/2510.00543</guid>
<content:encoded><![CDATA[
<div> privacy-preserving, federated learning, healthcare, large language models, LoRA 
Summary: 
Federated fine-tuning approach based on Low-Rank Adaptation (LoRA) is introduced for large language models (LLMs) in healthcare to overcome data privacy restrictions and facilitate cross-institution collaboration. The method combines local LoRA adaptation and global parameter aggregation, enabling knowledge sharing without exposing raw data. A blockchain identity scheme is utilized for LLM identification in distributed networks. Experiments on non-IID medical text datasets demonstrate that federated LoRA enhances cross-client generalization and boosts the performance of the weakest client, ensuring stable convergence and fairer outcomes. This approach offers a practical and effective paradigm for adapting LLMs in healthcare, opening up new possibilities for multi-center medical AI collaboration. 
<br /><br />Summary: <div>
arXiv:2510.00543v1 Announce Type: new 
Abstract: Large language models (LLMs) show great promise in healthcare, but their applications are hindered by data privacy restrictions and the challenges of cross-institution collaboration. Sensitive medical data cannot be centralized, while non-independent and identically distributed (non-IID) characteristics across institutions further complicate convergence and fairness. To address these issues, we present a federated fine-tuning approach based on Low-Rank Adaptation (LoRA), enabling privacy-preserving knowledge flow across institutions. The method iteratively combines local LoRA adaptation with global parameter aggregation, allowing efficient knowledge sharing without exposing raw data. A blockchain identity scheme is used for identifying individual LLM in such a distributed network. We evaluate this approach on heterogeneous and highly non-IID medical text datasets, where experiments demonstrate that federated LoRA not only enhances cross-client generalization but also improves the performance of the weakest client, achieving stable convergence and fairer outcomes. These findings highlight federated LoRA fine-tuning as a practical and effective paradigm for adapting LLMs in healthcare, offering a new path for multi-center medical AI collaboration.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal Classification Recovery Across Domains Using Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2510.00589</link>
<guid>https://arxiv.org/abs/2510.00589</guid>
<content:encoded><![CDATA[
<div> Adversarial learning, statistical distance alignment, stochastic modeling, unsupervised domain adaptation, deep neural networks <br />
Summary:
This paper investigates the use of unsupervised domain adaptation techniques to improve the generalization of signal classification models trained on controlled datasets to real-world scenarios with varying channel environments. The study focuses on aligning representations between simulated and over-the-air signal domains using methods such as adversarial learning, statistical distance alignment, and stochastic modeling. By deliberately generating modulated signals with realistic channel impairments, the researchers evaluate classification performance under different scenarios, including cross-SNR and SNR-matched cross-domain situations. The results demonstrate that unsupervised domain adaptation methods, specifically stochastic classifier (STAR) and joint adaptive networks (JAN), offer significant performance improvements over baseline models. These findings suggest the potential of these techniques for enhancing the deployment of deep neural networks in wireless systems. <br /> <div>
arXiv:2510.00589v1 Announce Type: new 
Abstract: Signal classification models based on deep neural networks are typically trained on datasets collected under controlled conditions, either simulated or over-the-air (OTA), which are constrained to specific channel environments with limited variability, such as fixed signal-to-noise ratio (SNR) levels. As a result, these models often fail to generalize when deployed in real-world scenarios where the feature distribution significantly differs from the training domain. This paper explores unsupervised domain adaptation techniques to bridge the generalization gap between mismatched domains. Specifically, we investigate adaptation methods based on adversarial learning, statistical distance alignment, and stochastic modeling to align representations between simulated and OTA signal domains. To emulate OTA characteristics, we deliberately generate modulated signals subjected to realistic channel impairments without demodulation. We evaluate classification performance under three scenarios, i.e., cross-SNR, SNR-matched cross-domain, and stepwise adaptation involving both SNR and domain shifts. Experimental results show that unsupervised domain adaptation methods, particularly stochastic classifier (STAR) and joint adaptive networks (JAN), enable consistent and substantial performance gains over baseline models, which highlight their promise for real-world deployment in wireless systems.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Economic Impact of DeFi Crime Events on Decentralized Autonomous Organizations (DAOs)</title>
<link>https://arxiv.org/abs/2510.00669</link>
<guid>https://arxiv.org/abs/2510.00669</guid>
<content:encoded><![CDATA[
<div> Decentralized Finance, DeFi, autonomous organizations, governance assets, crime events<br />
<br />
Summary:
Crime events in the Decentralized Finance (DeFi) ecosystem have resulted in over $10 billion in direct losses, triggering broader market reactions. Decentralized Autonomous Organizations (DAOs) govern DeFi applications through tradable governance assets, similar to corporate shares. A study conducted on 22 crime events between 2020 and 2022 examined their economic impact on governance asset prices, trading volumes, and market capitalization using a dynamic difference-in-differences (DiD) framework. Results indicate that 55% of crime events lead to significant negative price impacts, with an average decline of 14%. Furthermore, 68% of crime events result in increased trading volume of governance assets. The indirect economic losses estimated from these impacts exceed $1.3 billion in DAO market capitalization, surpassing direct victim costs and accounting for 74% of total losses. This study provides valuable insights into how crime events influence market dynamics and affect DAOs, offering a reproducible methodological approach applicable to other cryptoassets. <br /><br /> <div>
arXiv:2510.00669v1 Announce Type: new 
Abstract: The Decentralized Finance (DeFi) ecosystem has experienced over \$10 billion in direct losses due to crime events. Beyond these immediate losses, such events often trigger broader market reactions, including price declines, trading activity changes, and reductions in market capitalization. Decentralized Autonomous Organizations (DAOs) govern DeFi applications through tradable governance assets that function like corporate shares for voting and decision-making. Leveraging DeFi's granular trading data, we conduct an event study on 22 crime events between 2020 and 2022 to assess their economic impact on governance asset prices, trading volumes, and market capitalization. Using a dynamic difference-in-differences (DiD) framework with counterfactual governance assets, we aim for causal inference of intraday temporal effects. Our results show that 55% of crime events lead to significant negative price impacts, with an average decline of about 14%. Additionally, 68% of crime events lead to increased governance asset trading volume. Based on these impacts, we estimate indirect economic losses of over $1.3 billion in DAO market capitalization, far exceeding direct victim costs and accounting for 74% of total losses. Our study provides valuable insights into how crime events shape market dynamics and affect DAOs. Moreover, our methodological approach is reproducible and applicable beyond DAOs, offering a framework to assess the indirect economic impact on other cryptoassets.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMMET: orders-of-magnitude speed-up in finite element method via batch-vectorized neural constitutive updates</title>
<link>https://arxiv.org/abs/2510.00884</link>
<guid>https://arxiv.org/abs/2510.00884</guid>
<content:encoded><![CDATA[
<div> Keywords: Constitutive evaluations, Neural constitutive models, Finite element simulations, Computational mechanics, High-fidelity simulations

Summary:<br /><br />Constitutive evaluations in finite element simulations can be costly when using complex material models. Neural constitutive models (NCMs) provide a flexible framework for modeling such behavior in solid mechanics but have limited practical adoption due to high computational costs. The COMMET FE framework is introduced to address this issue, featuring a redesigned architecture that accelerates costly constitutive updates. It includes a novel assembly algorithm supporting batched and vectorized evaluations, optimized derivatives, and distributed-memory parallelism via MPI to reduce runtime significantly. The framework demonstrates speed-ups over traditional implementations, making it possible to efficiently perform large-scale simulations with high fidelity. These advancements primarily target NCMs but can be applied more broadly to improve performance where constitutive updates or assembly processes limit computational efficiency in computational mechanics. <div>
arXiv:2510.00884v1 Announce Type: new 
Abstract: Constitutive evaluations often dominate the computational cost of finite element (FE) simulations whenever material models are complex. Neural constitutive models (NCMs) offer a highly expressive and flexible framework for modeling complex material behavior in solid mechanics. However, their practical adoption in large-scale FE simulations remains limited due to significant computational costs, especially in repeatedly evaluating stress and stiffness. NCMs thus represent an extreme case: their large computational graphs make stress and stiffness evaluations prohibitively expensive, restricting their use to small-scale problems. In this work, we introduce COMMET, an open-source FE framework whose architecture has been redesigned from the ground up to accelerate high-cost constitutive updates. Our framework features a novel assembly algorithm that supports batched and vectorized constitutive evaluations, compute-graph-optimized derivatives that replace automatic differentiation, and distributed-memory parallelism via MPI. These advances dramatically reduce runtime, with speed-ups exceeding three orders of magnitude relative to traditional non-vectorized automatic differentiation-based implementations. While we demonstrate these gains primarily for NCMs, the same principles apply broadly wherever for-loop based assembly or constitutive updates limit performance, establishing a new standard for large-scale, high-fidelity simulations in computational mechanics.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Market States with Clustering and State Machines</title>
<link>https://arxiv.org/abs/2510.00953</link>
<guid>https://arxiv.org/abs/2510.00953</guid>
<content:encoded><![CDATA[
<div> probabilistic state machine, financial markets, market states, transition matrix, asset returns<br />
Summary:
This study presents a novel framework for modeling financial markets using an interpretable probabilistic state machine. By clustering historical returns based on momentum and risk features across different time horizons, distinct market states representing various regimes such as expansion, contraction, crisis, and recovery are identified. A transition matrix is then constructed to capture the dynamics between these states, creating a probabilistic state machine that models the market's temporal evolution. This state machine allows for the generation of a customized distribution of returns by combining Gaussian components weighted by state frequencies. Results show that this approach outperforms traditional methods in capturing key statistical properties of asset returns, including skewness and kurtosis. Robustness is confirmed through experiments across various assets and time periods. <div>
arXiv:2510.00953v1 Announce Type: new 
Abstract: This work introduces a new framework for modeling financial markets through an interpretable probabilistic state machine. By clustering historical returns based on momentum and risk features across multiple time horizons, we identify distinct market states that capture underlying regimes, such as expansion phase, contraction, crisis, or recovery. From a transition matrix representing the dynamics between these states, we construct a probabilistic state machine that models the temporal evolution of the market. This state machine enables the generation of a custom distribution of returns based on a mixture of Gaussian components weighted by state frequencies. We show that the proposed benchmark significantly outperforms the traditional approach in capturing key statistical properties of asset returns, including skewness and kurtosis, and our experiments across random assets and time periods confirm its robustness.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets</title>
<link>https://arxiv.org/abs/2510.00332</link>
<guid>https://arxiv.org/abs/2510.00332</guid>
<content:encoded><![CDATA[
<div> blind spot, adversarial environments, misinformation, financial decisions, model evaluation
<br />
Summary: 
The article introduces CAIA, a benchmark that highlights the inadequacy of state-of-the-art AI models in handling adversarial, high-stakes environments where misinformation is rampant and errors are irreversible. The benchmark evaluates 17 models on tasks related to distinguishing truth from manipulation, navigating fragmented information landscapes, and making financial decisions under adversarial pressure, using crypto markets as a testbed. The findings reveal a significant capability gap, with models achieving only 28% accuracy compared to a human baseline of 80%. Despite access to professional resources, model performance plateaus at 67.4% due to a preference for unreliable sources like web search over authoritative data. The benchmark also uncovers dangerous trial-and-error behavior in model decision-making, emphasizing the importance of adversarial robustness in AI deployment across various domains. <div>
arXiv:2510.00332v1 Announce Type: cross 
Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation: the inability of state-of-the-art models to operate in adversarial, high-stakes environments where misinformation is weaponized and errors are irreversible. While existing benchmarks measure task completion in controlled settings, real-world deployment demands resilience against active deception. Using crypto markets as a testbed where $30 billion was lost to exploits in 2024, we evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish truth from manipulation, navigate fragmented information landscapes, and make irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier models achieve only 28% accuracy on tasks junior analysts routinely handle. Tool augmentation improves performance but plateaus at 67.4% versus 80% human baseline, despite unlimited access to professional resources. Most critically, we uncover a systematic tool selection catastrophe: models preferentially choose unreliable web search over authoritative data, falling for SEO-optimized misinformation and social media manipulation. This behavior persists even when correct answers are directly accessible through specialized tools, suggesting foundational limitations rather than knowledge gaps. We also find that Pass@k metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries, e.g. cybersecurity, content moderation, etc. We release CAIA with contamination controls and continuous updates, establishing adversarial robustness as a necessary condition for trustworthy AI autonomy. The benchmark reveals that current models, despite impressive reasoning scores, remain fundamentally unprepared for environments where intelligence must survive active opposition.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction</title>
<link>https://arxiv.org/abs/2510.00457</link>
<guid>https://arxiv.org/abs/2510.00457</guid>
<content:encoded><![CDATA[
<div> framework, physics-informed, urban microclimates, spatio-temporal graphs, heterogeneous dynamic graphs
Summary:<br />
- The article introduces UrbanGraph, a framework for predicting urban microclimates that addresses shortcomings of existing approaches by integrating heterogeneous and dynamic spatio-temporal graphs.
- UrbanGraph encodes key physical processes like vegetation evapotranspiration, shading, and convective diffusion, while modeling complex spatial dependencies among urban entities and their temporal evolution.
- Evaluation on the UMC4/12 dataset shows that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% compared to baselines, with heterogeneous and dynamic graphs contributing to the gains.
- The dataset used provides a high-resolution benchmark for spatio-temporal microclimate modeling, allowing for more accurate predictions.
- UrbanGraph's approach can be extended to other urban heterogeneous dynamic computing tasks, highlighting its potential impact beyond microclimate modeling.
<br />Summary: <div>
arXiv:2510.00457v1 Announce Type: cross 
Abstract: With rapid urbanization, predicting urban microclimates has become critical, as it affects building energy demand and public health risks. However, existing generative and homogeneous graph approaches fall short in capturing physical consistency, spatial dependencies, and temporal variability. To address this, we introduce UrbanGraph, a physics-informed framework integrating heterogeneous and dynamic spatio-temporal graphs. It encodes key physical processes -- vegetation evapotranspiration, shading, and convective diffusion -- while modeling complex spatial dependencies among diverse urban entities and their temporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based simulation dataset covering diverse urban configurations and climates. Results show that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% over all baselines, with heterogeneous and dynamic graphs contributing 3.5% and 7.1% gains. Our dataset provides the first high-resolution benchmark for spatio-temporal microclimate modeling, and our method extends to broader urban heterogeneous dynamic computing tasks.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques</title>
<link>https://arxiv.org/abs/2510.00836</link>
<guid>https://arxiv.org/abs/2510.00836</guid>
<content:encoded><![CDATA[
<div> SMOTE, ensemble learning models, pump and dump, cryptocurrency markets, manipulation <br />
<br />
Summary: This study focuses on detecting pump and dump (P&amp;D) manipulation in cryptocurrency markets, addressing the challenge of severe class imbalance. By applying the Synthetic Minority Oversampling Technique (SMOTE) and evaluating advanced ensemble learning models, the researchers successfully identified manipulative trading behavior from normal market activity. The experimental results demonstrated that using SMOTE significantly improved the ability of all models to detect P&amp;D events, enhancing recall and achieving a better balance between precision and recall. Specifically, XGBoost and LightGBM stood out with high recall rates and strong F1-scores, along with fast computational performance suitable for near real-time surveillance. The integration of data balancing techniques with ensemble methods proved to be effective in early detection of manipulative activities, contributing to a more fair, transparent, and stable cryptocurrency market. 
<br /><br /> <div>
arXiv:2510.00836v1 Announce Type: cross 
Abstract: This study aims to detect pump and dump (P&amp;D) manipulation in cryptocurrency markets, where the scarcity of such events causes severe class imbalance and hinders accurate detection. To address this issue, the Synthetic Minority Oversampling Technique (SMOTE) was applied, and advanced ensemble learning models were evaluated to distinguish manipulative trading behavior from normal market activity. The experimental results show that applying SMOTE greatly enhanced the ability of all models to detect P&amp;D events by increasing recall and improving the overall balance between precision and recall. In particular, XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%, respectively) with strong F1-scores and demonstrated fast computational performance, making them suitable for near real time surveillance. These findings indicate that integrating data balancing techniques with ensemble methods significantly improves the early detection of manipulative activities, contributing to a fairer, more transparent, and more stable cryptocurrency market.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning Approach in Augmenting RANS Models Using DNS Data and DeepInsight Method on FDA Nozzle</title>
<link>https://arxiv.org/abs/2510.01091</link>
<guid>https://arxiv.org/abs/2510.01091</guid>
<content:encoded><![CDATA[
<div> machine learning, turbulence modeling, OpenFOAM, Reynolds stress tensor, DNS 

Summary:
- The study introduces a data-driven framework for turbulence modeling in flow prediction in the FDA nozzle, using a hybrid implicit-explicit approach.
- New variables are introduced, and a solver is developed within the OpenFOAM framework, incorporating a machine learning module to estimate these variables.
- Invariant input features are derived based on Hilbert's basis theorem, and the machine learning model's outputs are obtained through eigenvalue-vector decomposition of the Reynolds stress tensor.
- Validation is performed using DNS data for turbulent flow in a square channel at various Reynolds numbers.
- A Deep-Insight network trained on benchmark DNS datasets as images demonstrates improved prediction of turbulence structures in the FDA blood nozzle, showcasing the potential of data-driven augmentation in turbulence modeling. 

Summary:<br />
Keywords: machine learning, turbulence modeling, OpenFOAM, Reynolds stress tensor, DNS <br /> <div>
arXiv:2510.01091v1 Announce Type: cross 
Abstract: We present a data-driven framework for turbulence modeling, applied to flow prediction in the FDA nozzle. In this study, the standard RANS equations have been modified using an implicit-explicit hybrid approach. New variables were introduced, and a solver was developed within the OpenFOAM framework, integrating a machine learning module to estimate these variables. The invariant input features were derived based on Hilbert's basis theorem, and the outputs of the machine learning model were obtained through eigenvalue-vector decomposition of the Reynolds stress tensor. Validation was performed using DNS data for turbulent flow in a square channel at various Reynolds numbers. A baseline MLP was first trained at $Re=2900$ and tested at $Re=3500$ to assess its ability to reproduce turbulence anisotropy and secondary flows. To further enhance generalization, three benchmark DNS datasets were transformed into images via the Deep-Insight method, enabling the use of convolutional neural networks. The trained Deep-Insight network demonstrated improved prediction of turbulence structures in the FDA blood nozzle, highlighting the promise of data-driven augmentation in turbulence modeling.
]]></content:encoded>
<pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-patch isogeometric neural solver for partial differential equations on computer-aided design domains</title>
<link>https://arxiv.org/abs/2509.25450</link>
<guid>https://arxiv.org/abs/2509.25450</guid>
<content:encoded><![CDATA[
<div> neural networks, isogeometric analysis, partial differential equations, computational framework, finite element solvers
Summary:
This work presents a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks operating on the reference domain of isogeometric analysis and enforces Dirichlet boundary conditions with a custom output layer. Interface neural networks ensure solution conformity across non-uniform rational B-spline patch interfaces. Training follows a variational framework by minimizing the energy functional. The method's effectiveness is demonstrated on two challenging use-cases, a 2D magnetostatics model and a 3D nonlinear solid and contact mechanics model, showing excellent agreement with high-fidelity finite element solver solutions. This neural solver shows promise in addressing complex engineering problems with corresponding computer-aided design models. <br /><br />Summary: <div>
arXiv:2509.25450v1 Announce Type: new 
Abstract: This work develops a computational framework that combines physics-informed neural networks with multi-patch isogeometric analysis to solve partial differential equations on complex computer-aided design geometries. The method utilizes patch-local neural networks that operate on the reference domain of isogeometric analysis. A custom output layer enables the strong imposition of Dirichlet boundary conditions. Solution conformity across interfaces between non-uniform rational B-spline patches is enforced using dedicated interface neural networks. Training is performed using the variational framework by minimizing the energy functional derived after the weak form of the partial differential equation. The effectiveness of the suggested method is demonstrated on two highly non-trivial and practically relevant use-cases, namely, a 2D magnetostatics model of a quadrupole magnet and a 3D nonlinear solid and contact mechanics model of a mechanical holder. The results show excellent agreement to reference solutions obtained with high-fidelity finite element solvers, thus highlighting the potential of the suggested neural solver to tackle complex engineering problems given the corresponding computer-aided design models.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource Allocation under Stochastic Demands using Shrinking Horizon Optimization</title>
<link>https://arxiv.org/abs/2509.25412</link>
<guid>https://arxiv.org/abs/2509.25412</guid>
<content:encoded><![CDATA[
<div> method, resource allocation, revenue maximization, stochastic demands, shrinking horizon algorithm

Summary:
The article discusses the optimal allocation of limited resources over time to maximize revenue in the face of stochastic demands. It has applications in various control areas like supply chain management, healthcare operations, and power grid energy allocation. The proposed bisection method aims to solve the static optimization problem efficiently. The authors extend this approach to a shrinking horizon algorithm for sequential problems, updating future allocations based on observed demand values. A synthetic example with jointly log-normal demands illustrates the method's performance, demonstrating results close to those obtained by solving the prescient problem. This research provides valuable insights for industries seeking to optimize resource allocation in uncertain environments. <br /><br />Summary: <div>
arXiv:2509.25412v1 Announce Type: cross 
Abstract: We consider the problem of optimally allocating a limited number of resources across time to maximize revenue under stochastic demands. This formulation is relevant in various areas of control, such as supply chain, ticket revenue maximization, healthcare operations, and energy allocation in power grids. We propose a bisection method to solve the static optimization problem and extend our approach to a shrinking horizon algorithm for the sequential problem. The shrinking horizon algorithm computes future allocations after updating the distribution of future demands by conditioning on the observed values of demand. We illustrate the method on a simple synthetic example with jointly log-normal demands, showing that it achieves performance close to a bound obtained by solving the prescient problem.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quasi-Monte Carlo methods for uncertainty quantification of tumor growth modeled by a parametric semi-linear parabolic reaction-diffusion equation</title>
<link>https://arxiv.org/abs/2509.25753</link>
<guid>https://arxiv.org/abs/2509.25753</guid>
<content:encoded><![CDATA[
<div> tumor growth, quasi-Monte Carlo method, partial differential equations, uncertainty, mathematical models

Summary:<br />
This study focuses on applying a quasi-Monte Carlo method to analyze semi-linear parabolic reaction-diffusion equations used in modeling tumor growth. Tumor growth models are complex due to factors like patient variability, disease heterogeneity, and sparse data, leading to uncertainty in model parameters. Efficiently propagating these uncertainties is essential for computing quantities of interest (QoIs) to guide clinical decisions. The research demonstrates that quasi-Monte Carlo methods are effective in computing QoIs, with theoretical error bounds established for uniform random fields, showing a superior linear error rate compared to standard Monte Carlo. Numerical validations support this finding, with promising results for lognormal random fields prompting further investigation. The study provides a valuable contribution to the field of tumor growth modeling and uncertainty quantification. 

<br /><br /> <div>
arXiv:2509.25753v1 Announce Type: cross 
Abstract: We study the application of a quasi-Monte Carlo (QMC) method to a class of semi-linear parabolic reaction-diffusion partial differential equations used to model tumor growth. Mathematical models of tumor growth are largely phenomenological in nature, capturing infiltration of the tumor into surrounding healthy tissue, proliferation of the existing tumor, and patient response to therapies, such as chemotherapy and radiotherapy. Considerable inter-patient variability, inherent heterogeneity of the disease, sparse and noisy data collection, and model inadequacy all contribute to significant uncertainty in the model parameters. It is crucial that these uncertainties can be efficiently propagated through the model to compute quantities of interest (QoIs), which in turn may be used to inform clinical decisions. We show that QMC methods can be successful in computing expectations of meaningful QoIs. Well-posedness results are developed for the model and used to show a theoretical error bound for the case of uniform random fields. The theoretical linear error rate, which is superior to that of standard Monte Carlo, is verified numerically. Encouraging computational results are also provided for lognormal random fields, prompting further theoretical development.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding</title>
<link>https://arxiv.org/abs/2509.25803</link>
<guid>https://arxiv.org/abs/2509.25803</guid>
<content:encoded><![CDATA[
<div> Transformer models, financial transactions, analysis, LLMs, proprietary models<br />
Summary:<br />
The paper explores the use of Transformer-based models in understanding financial transactions. Three types of models were evaluated: Encoder-Only, Decoder-Only, and Encoder-Decoder, with a focus on pretrained LLMs, fine-tuned LLMs, and small proprietary models. While LLMs like LLaMA3-8b, Flan-T5, and SBERT excel in natural language processing tasks, they do not surpass small proprietary models in financial transaction understanding in terms of speed and cost efficiency. Customized proprietary models tailored to transaction data requirements prove to be more suitable for real-time applications in the financial sector. The implementation of a proprietary decoder-only model led to a 14% increase in transaction coverage and over $13 million in annual cost savings. <div>
arXiv:2509.25803v1 Announce Type: cross 
Abstract: Analyzing financial transactions is crucial for ensuring regulatory compliance, detecting fraud, and supporting decisions. The complexity of financial transaction data necessitates advanced techniques to extract meaningful insights and ensure accurate analysis. Since Transformer-based models have shown outstanding performance across multiple domains, this paper seeks to explore their potential in understanding financial transactions. This paper conducts extensive experiments to evaluate three types of Transformer models: Encoder-Only, Decoder-Only, and Encoder-Decoder models. For each type, we explore three options: pretrained LLMs, fine-tuned LLMs, and small proprietary models developed from scratch. Our analysis reveals that while LLMs, such as LLaMA3-8b, Flan-T5, and SBERT, demonstrate impressive capabilities in various natural language processing tasks, they do not significantly outperform small proprietary models in the specific context of financial transaction understanding. This phenomenon is particularly evident in terms of speed and cost efficiency. Proprietary models, tailored to the unique requirements of transaction data, exhibit faster processing times and lower operational costs, making them more suitable for real-time applications in the financial sector. Our findings highlight the importance of model selection based on domain-specific needs and underscore the potential advantages of customized proprietary models over general-purpose LLMs in specialized applications. Ultimately, we chose to implement a proprietary decoder-only model to handle the complex transactions that we previously couldn't manage. This model can help us to improve 14% transaction coverage, and save more than \$13 million annual cost.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning</title>
<link>https://arxiv.org/abs/2509.26116</link>
<guid>https://arxiv.org/abs/2509.26116</guid>
<content:encoded><![CDATA[
<div> Keywords: metagenomic binning, probabilistic embedding, DNA fragments, microbial communities, scalability<br />
<br />
Summary: 
This article introduces UncertainGen, a probabilistic embedding approach for metagenomic binning that represents DNA fragments as probability distributions in latent space. Unlike existing deterministic methods, UncertainGen captures the uncertainty inherent in DNA sequences due to inter-species DNA sharing. The approach offers theoretical guarantees on embedding distinguishability and enables more flexible separation of bins/clusters by introducing a data-adaptive metric in the latent space. Experiments with real metagenomic datasets demonstrate the superiority of UncertainGen over deterministic k-mer and LLM-based embeddings for the binning task. The probabilistic embedding framework not only enhances scalability but also provides a lightweight solution for large-scale metagenomic analysis. <div>
arXiv:2509.26116v1 Announce Type: cross 
Abstract: Metagenomic binning aims to cluster DNA fragments from mixed microbial samples into their respective genomes, a critical step for downstream analyses of microbial communities. Existing methods rely on deterministic representations, such as k-mer profiles or embeddings from large language models, which fail to capture the uncertainty inherent in DNA sequences arising from inter-species DNA sharing and from fragments with highly similar representations. We present the first probabilistic embedding approach, UncertainGen, for metagenomic binning, representing each DNA fragment as a probability distribution in latent space. Our approach naturally models sequence-level uncertainty, and we provide theoretical guarantees on embedding distinguishability. This probabilistic embedding framework expands the feasible latent space by introducing a data-adaptive metric, which in turn enables more flexible separation of bins/clusters. Experiments on real metagenomic datasets demonstrate the improvements over deterministic k-mer and LLM-based embeddings for the binning task by offering a scalable and lightweight solution for large-scale metagenomic analysis.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bubble, Bubble, AI's Rumble: Why Global Financial Regulatory Incident Reporting is Our Shield Against Systemic Stumbles</title>
<link>https://arxiv.org/abs/2509.26150</link>
<guid>https://arxiv.org/abs/2509.26150</guid>
<content:encoded><![CDATA[
<div> systemic risks, AI incident database, algorithmic trading, regulatory-grade global database, financial stability<br />
<br />Summary: 
The article discusses the need for a regulatory-grade global database to address the lack of transparency in AI-driven financial markets. It highlights the challenges posed by opaque AI systems and the potential systemic risks associated with algorithmic trading. The proposed database incorporates frameworks from healthcare and aviation industries to document AI incidents, allowing for cross-jurisdictional analysis of emerging risks. It employs a data omission technique to protect confidential information while enabling thorough analysis. Synthetic data validation reveals patterns such as market manipulation clusters and the influence of AI system typology on trading behavior, transcending geographical boundaries. The solution aims to empower regulators, financial institutions, and investors with enhanced oversight and transparency in AI-driven financial markets, promoting global financial stability. Immediate action is urged to strengthen risk management and resilience against AI-driven systemic risks. <div>
arXiv:2509.26150v1 Announce Type: cross 
Abstract: "Double, double toil and trouble; Fire burn and cauldron bubble." As Shakespeare's witches foretold chaos through cryptic prophecies, modern capital markets grapple with systemic risks concealed by opaque AI systems. According to IMF, the August 5, 2024, plunge in Japanese and U.S. equities can be linked to algorithmic trading yet ab-sent from existing AI incidents database exemplifies this transparency crisis. Current AI incident databases, reliant on crowdsourcing or news scraping, systematically over-look capital market anomalies, particularly in algorithmic and high-frequency trading. We address this critical gap by proposing a regulatory-grade global database that elegantly synthesises post-trade reporting frameworks with proven incident documentation models from healthcare and aviation. Our framework's temporal data omission technique masking timestamps while preserving percent-age-based metrics enables sophisticated cross-jurisdictional analysis of emerging risks while safeguarding confidential business information. Synthetic data validation (modelled after real life published incidents , sentiments, data) reveals compelling pat-terns: systemic risks transcending geographical boundaries, market manipulation clusters distinctly identifiable via K-means algorithms, and AI system typology exerting significantly greater influence on trading behaviour than geographical location, This tripartite solution empowers regulators with unprecedented cross-jurisdictional oversight, financial institutions with seamless compliance integration, and investors with critical visibility into previously obscured AI-driven vulnerabilities. We call for immediate action to strengthen risk management and foster resilience in AI-driven financial markets against the volatile "cauldron" of AI-driven systemic risks., promoting global financial stability through enhanced transparency and coordinated oversight.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing BEV Suitability and Charging Strategies Using Italian Driving Data</title>
<link>https://arxiv.org/abs/2509.26262</link>
<guid>https://arxiv.org/abs/2509.26262</guid>
<content:encoded><![CDATA[
<div> telemetry data, Battery Electric Vehicles, charging scenarios, mobility needs, BEV autonomy
Summary:<br /><br />Battery Electric Vehicles (BEVs) are becoming a popular choice for private transportation, replacing Internal Combustion Engine (ICE) vehicles. However, barriers such as range anxiety and charging station inconvenience persist. A study in Italy analyzed data from 10,441 ICE vehicle users to determine the feasibility of switching to BEVs without changing travel habits. By simulating trips and charging events, the study found that with overnight charging, at least 35% of users could switch to low-capacity BEVs without altering their mobility needs. The analysis highlights the importance of charging behaviors and BEV autonomy in transitioning to electric vehicles. <div>
arXiv:2509.26262v1 Announce Type: cross 
Abstract: Battery Electric Vehicles (BEVs) are rapidly evolving from a niche alternative to an established option for private transportation, often replacing Internal Combustion Engine (ICE) vehicles. Despite growing interest, significant barriers remain, including range anxiety, the inconvenience associated with public charging stations, and higher costs. This study analyses extensive telemetry data collected from 10,441 users using ICE vehicles in an Italian province to assess the potential for switching to BEVs without changing current travel behaviour. We evaluate to what extent the BEV models can fulfil their mobility needs under different charging scenarios. To do so, we replicate trips and parking events, simulating and monitoring the battery state of charge. The analysis reveals the compromises between charging behaviours and limited BEV autonomy. Assuming access to overnight charging, at least 35% of the users could already adopt even low-capacity BEVs.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Importance of localized dilatation and distensibility in identifying determinants of thoracic aortic aneurysm with neural operators</title>
<link>https://arxiv.org/abs/2509.26576</link>
<guid>https://arxiv.org/abs/2509.26576</guid>
<content:encoded><![CDATA[
<div> finite element framework, synthetic TAAs, neural networks, mechanobiological drivers, personalized treatment strategies
Summary:
The study investigates Thoracic aortic aneurysms (TAAs) development by simulating heterogeneous insults using a finite element framework. Neural networks are trained to predict the combined insult leading to TAA formation. Various network architectures are compared, with UNet showing the highest accuracy in predicting insults. Results emphasize the importance of including both dilatation and distensibility data for accurate predictions. Acquiring full-field measurements of these parameters is crucial for assessing the mechanobiological drivers of TAAs and developing personalized treatment strategies.<br /><br />Summary: <div>
arXiv:2509.26576v1 Announce Type: cross 
Abstract: Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and mechanobiological disruptions to the aortic wall that increase the risk of dissection or rupture. Evidence links TAA development to dysfunctions in the aortic mechanotransduction axis, including loss of elastic fiber integrity and cell-matrix connections. Because distinct insults create different mechanical vulnerabilities, there is a critical need to identify interacting factors that drive progression. Here, we use a finite element framework to generate synthetic TAAs from hundreds of heterogeneous insults spanning varying degrees of elastic fiber damage and impaired mechanosensing. From these simulations, we construct spatial maps of localized dilatation and distensibility to train neural networks that predict the initiating combined insult. We compare several architectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and multiple input data formats to define a standard for future subject-specific modeling. We also quantify predictive performance when networks are trained using only geometric data (dilatation) versus both geometric and mechanical data (dilatation plus distensibility). Across all networks, prediction errors are significantly higher when trained on dilatation alone, underscoring the added value of distensibility information. Among the tested models, UNet consistently provides the highest accuracy across all data formats. These findings highlight the importance of acquiring full-field measurements of both dilatation and distensibility in TAA assessment to reveal the mechanobiological drivers of disease and support the development of personalized treatment strategies.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of nested geometry treatments within GPU-based Monte Carlo neutron transport simulations of fission reactors</title>
<link>https://arxiv.org/abs/2406.13849</link>
<guid>https://arxiv.org/abs/2406.13849</guid>
<content:encoded><![CDATA[
<div> GPU-based neutron transport, Monte Carlo simulation, reactor physics, tracking algorithms, supercomputing 
Summary:<br />
- Monte Carlo neutron transport provides detailed estimates of radiological quantities in fission reactors by tracking individual neutrons through computational geometry.
- CPU-based MC codes utilize multiple tracker types with different algorithms, but virtual function calls have high GPU overhead.
- Shift MC code was modified to support GPU-based tracking using dynamic polymorphism, static polymorphism, and single tracker type with tree-based acceleration.
- Results on the Frontier supercomputer show efficient tracking rates using all three methods, suitable for typical reactor problems.
- The single tracker method demonstrates flexibility in handling hexagonal-grid microreactor problems without specific tracking routines, providing significant speedup over CPU execution. <br /><br />Summary: <div>
arXiv:2406.13849v2 Announce Type: replace-cross 
Abstract: Monte Carlo (MC) neutron transport provides detailed estimates of radiological quantities within fission reactors. This involves tracking individual neutrons through a computational geometry. CPU-based MC codes use multiple polymorphic tracker types with different tracking algorithms to exploit the repeated configurations of reactors, but virtual function calls have high overhead on the GPU. The Shift MC code was modified to support GPU-based tracking with three strategies: dynamic polymorphism with virtual functions, static polymorphism, and a single tracker type with tree-based acceleration. On the Frontier supercomputer these methods achieve 77.8%, 91.2%, and 83.4%, respectively, of the tracking rate obtained using a specialized tracker optimized for rectilinear-grid-based reactors. This indicates that all three methods are suitable for typical reactor problems in which tracking does not dominate runtime. The flexibility of the single tracker method is highlighted with a hexagonal-grid microreactor problem, performed without hexagonal-grid-specific tracking routines, providing a 2.19$\times$ speedup over CPU execution.
]]></content:encoded>
<pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Channel, Trend and Periodic-Wise Representation Learning for Multivariate Long-term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2509.23583</link>
<guid>https://arxiv.org/abs/2509.23583</guid>
<content:encoded><![CDATA[
<div> Keywords: downsampling-based methods, time series forecasting, CTPNet, multi-head attention mechanism, Transformer

Summary:
CTPNet is a novel framework for time series forecasting that addresses limitations in downsampling-based methods by explicitly learning representations from three perspectives. It captures inter-channel dependencies using a temporal query-based multi-head attention mechanism, models intra-subsequence dependencies with a Transformer, and extracts inter-subsequence dependencies with residual connections to capture global patterns. By integrating these levels, CTPNet provides a more holistic representation of temporal dynamics, leading to improved forecasting accuracy. Experimental results demonstrate the superiority of CTPNet compared to existing methods. <div>
arXiv:2509.23583v1 Announce Type: new 
Abstract: Downsampling-based methods for time series forecasting have attracted increasing attention due to their superiority in capturing sequence trends. However, this approaches mainly capture dependencies within subsequences but neglect inter-subsequence and inter-channel interactions, which limits forecasting accuracy. To address these limitations, we propose CTPNet, a novel framework that explicitly learns representations from three perspectives: i) inter-channel dependencies, captured by a temporal query-based multi-head attention mechanism; ii) intra-subsequence dependencies, modeled via a Transformer to characterize trend variations; and iii) inter-subsequence dependencies, extracted by reusing the encoder with residual connections to capture global periodic patterns. By jointly integrating these levels, proposed method provides a more holistic representation of temporal dynamics. Extensive experiments demonstrate the superiority of the proposed method.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-Code Generation for Modular Building Layouts in Building Information Modeling</title>
<link>https://arxiv.org/abs/2509.23713</link>
<guid>https://arxiv.org/abs/2509.23713</guid>
<content:encoded><![CDATA[
<div> Keywords: Text2MBL, text-to-code generation, Building Information Modeling (BIM), modular building layout (MBL), hierarchical structure

Summary: 
Text2MBL is a framework that generates executable BIM code from textual descriptions of MBL designs, moving beyond conventional 2D layout generation. It produces parametric BIM layouts through on-the-fly code instantiation, addressing the challenges of MBL's hierarchical structure. The framework uses an object-oriented code architecture and large language models to output structured action sequences in code format. A dataset of paired descriptions and ground truth layouts from modular housing projects was used to train and evaluate Text2MBL, ensuring executable validity, semantic fidelity, and geometric consistency. By integrating natural language understanding with BIM code generation, Text2MBL creates a scalable pipeline for modular construction workflows. The implementation of Text2MBL is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2509.23713v1 Announce Type: new 
Abstract: We present Text2MBL, a text-to-code generation framework that generates executable Building Information Modeling (BIM) code directly from textual descriptions of modular building layout (MBL) design. Unlike conventional layout generation approaches that operate in 2D space, Text2MBL produces fully parametric, semantically rich BIM layouts through on-the-fly code instantiation. To address MBLs' unique challenges due to their hierarchical three-tier structure: modules (physical building blocks), units (self-contained dwellings), and rooms (functional spaces), we developed an object-oriented code architecture and fine-tuned large language models to output structured action sequences in code format. To train and evaluate the framework, we curated a dataset of paired descriptions and ground truth layouts drawn from real-world modular housing projects. Performance was assessed using metrics for executable validity, semantic fidelity, and geometric consistency. By tightly unifying natural language understanding with BIM code generation, Text2MBL establishes a scalable pipeline from high-level conceptual design to automation-ready modular construction workflows. Our implementation is available at https://github.com/CI3LAB/Text2MBL.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid DNN Transformer AE Framework for Corporate Tax Risk Supervision and Risk Level Assessment</title>
<link>https://arxiv.org/abs/2509.23862</link>
<guid>https://arxiv.org/abs/2509.23862</guid>
<content:encoded><![CDATA[
<div> deep learning, tax risk supervision, Transformer, Autoencoder, risk level assessment

Summary:
The paper presents a hybrid deep learning framework, DNN-Transformer-Autoencoder, for corporate tax risk supervision. The framework combines a Deep Neural Network (DNN) for static enterprise attributes, a Transformer for long-term dependencies in financial time series, and an Autoencoder for detecting anomalous tax behaviors. By integrating these modules, the framework generates a comprehensive risk score and assigns discrete risk levels. Experimental results on a real-world tax dataset show the framework's effectiveness with an accuracy of 0.91 and a Macro F1-score of 0.88. The hybrid model not only enhances classification performance but also improves interpretability and applicability in tax regulation scenarios. This study contributes to methodological innovation and provides regulatory implications for intelligent tax risk management.<br /><br />Summary: <div>
arXiv:2509.23862v1 Announce Type: new 
Abstract: Tax risk supervision has become a critical component of modern financial governance, as irregular tax behaviors and hidden compliance risks pose significant challenges to regulatory authorities and enterprises alike. Traditional rule-based methods often struggle to capture complex and dynamic tax-related anomalies in large-scale enterprise data. To address this issue, this paper proposes a hybrid deep learning framework (DNN-Transformer-Autoencoder) for corporate tax risk supervision and risk level assessment. The framework integrates three complementary modules: a Deep Neural Network (DNN) for modeling static enterprise attributes, a Transformer-based architecture for capturing long-term dependencies in historical financial time series, and an Autoencoder (AE) for unsupervised detection of anomalous tax behaviors. The outputs of these modules are fused to generate a comprehensive risk score, which is further mapped into discrete risk levels (high, medium, low). Experimental evaluations on a real-world enterprise tax dataset demonstrate the effectiveness of the proposed framework, achieving an accuracy of 0.91 and a Macro F1-score of 0.88. These results indicate that the hybrid model not only improves classification performance but also enhances interpretability and applicability in practical tax regulation scenarios. This study provides both methodological innovation and regulatory implications for intelligent tax risk management.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying the Multimodal Hierarchy of Public Transit Systems Using Trip Chain Data</title>
<link>https://arxiv.org/abs/2509.24220</link>
<guid>https://arxiv.org/abs/2509.24220</guid>
<content:encoded><![CDATA[
<div> hierarchy, urban mobility, multimodal trips, smart card data, Seoul<br />
Summary:<br />
The article introduces the concept of a macroscopic multimodal hierarchy to understand interactions between different modes of urban transportation. Trips follow an ascending-descending order starting and ending with lower hierarchical modes like walking for high accessibility and utilizing higher modes for efficiency. A methodology to identify the multimodal hierarchy using smart card trip data is proposed and demonstrated with data from Seoul, South Korea. This approach helps in understanding how traditional and emerging modes of transportation interact in complex public transit systems, influencing users' multimodal itineraries. <div>
arXiv:2509.24220v1 Announce Type: new 
Abstract: As urban mobility integrates traditional and emerging modes, public transit systems are becoming increasingly complex. Some modes complement each other, while others compete, influencing users' multimodal itineraries. To provide a clear, high-level understanding of these interactions, we introduce the concept of a macroscopic multimodal hierarchy. In this framework, trips follow an "ascending-descending" order, starting and ending with lower hierarchical modes (e.g., walking) that offer high accessibility, while utilizing higher modes (e.g., subways) for greater efficiency. We propose a methodology to identify the multimodal hierarchy of a city using multimodal smart card trip chain data and demonstrate its application with actual data collected from Seoul and the surrounding metropolitan area in South Korea.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparison of Surrogate Constitutive Models for Viscoplastic Creep Simulation of HT-9 Steel</title>
<link>https://arxiv.org/abs/2509.22667</link>
<guid>https://arxiv.org/abs/2509.22667</guid>
<content:encoded><![CDATA[
<div> Keywords: microstructure, constitutive models, surrogate modeling, viscoplastic response, data-driven approach

Summary:
- Mechanistic microstructure-informed constitutive models for polycrystals are essential in computational materials science but can be computationally expensive.
- Data-driven surrogate models offer a solution by learning constitutive relations directly from data.
- Two local surrogate models, including a piecewise response surface method and a mixture of experts model, were developed for the viscoplastic response of steel to balance accuracy and computational efficiency.
- The surrogates adapt to varying material behavior based on parameters or conditions and were tested on HT-9 steel for creep simulations.
- Test metrics show that the mixture of experts model outperforms the piecewise response surface method in accuracy for predicting viscoplastic material behavior.<br /><br />Summary:Mechanistic microstructure models are crucial in computational materials science but can be computationally intensive. Data-driven surrogate models offer a promising solution. Two models were developed for steel's viscoplastic response, adapting to varying behavior and tested on HT-9 for creep simulations. The mixture of experts model was found to outperform the piecewise response surface method in accuracy. <div>
arXiv:2509.22667v1 Announce Type: cross 
Abstract: Mechanistic microstructure-informed constitutive models for the mechanical response of polycrystals are a cornerstone of computational materials science. However, as these models become increasingly more complex - often involving coupled differential equations describing the effect of specific deformation modes - their associated computational costs can become prohibitive, particularly in optimization or uncertainty quantification tasks that require numerous model evaluations. To address this challenge, surrogate constitutive models that balance accuracy and computational efficiency are highly desirable. Data-driven surrogate models, that learn the constitutive relation directly from data, have emerged as a promising solution. In this work, we develop two local surrogate models for the viscoplastic response of a steel: a piecewise response surface method and a mixture of experts model. These surrogates are designed to adapt to complex material behavior, which may vary with material parameters or operating conditions. The surrogate constitutive models are applied to creep simulations of HT-9 steel, an alloy of considerable interest to the nuclear energy sector due to its high tolerance to radiation damage, using training data generated from viscoplastic self-consistent (VPSC) simulations. We define a set of test metrics to numerically assess the accuracy of our surrogate models for predicting viscoplastic material behavior, and show that the mixture of experts model outperforms the piecewise response surface method in terms of accuracy.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents</title>
<link>https://arxiv.org/abs/2509.24063</link>
<guid>https://arxiv.org/abs/2509.24063</guid>
<content:encoded><![CDATA[
<div> agent-based simulation, distributed execution, serialization mechanism, delta encoding, extreme-scale simulations

Summary:
TeraAgent is introduced as a distributed agent-based simulation engine to address the scalability limitations of existing platforms like BioDynaMo. The key challenge of exchanging agent information across servers is tackled through a tailored serialization mechanism and the use of delta encoding to reduce data transfer. These solutions enable TeraAgent to support extreme-scale simulations with half a trillion agents, a significant improvement in scalability. The platform also facilitates faster time-to-result by utilizing additional compute nodes, enhances interoperability with third-party tools, and provides users with more hardware flexibility. TeraAgent marks a significant advancement in the field of agent-based simulation, offering a promising solution for studying complex systems on a massive scale. 

<br /><br />Summary: <div>
arXiv:2509.24063v1 Announce Type: cross 
Abstract: Agent-based simulation is an indispensable paradigm for studying complex systems. These systems can comprise billions of agents, requiring the computing resources of multiple servers to simulate. Unfortunately, the state-of-the-art platform, BioDynaMo, does not scale out across servers due to its shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed agent-based simulation engine. A critical challenge in distributed execution is the exchange of agent information across servers, which we identify as a major performance bottleneck. We propose two solutions: 1) a tailored serialization mechanism that allows agents to be accessed and mutated directly from the receive buffer, and 2) leveraging the iterative nature of agent-based simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half a trillion agents (an 84x improvement), reduces time-to-result with additional compute nodes, improves interoperability with third-party tools, and provides users with more hardware flexibility.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting the Structure of Press Releases for Predicting Earnings Announcement Returns</title>
<link>https://arxiv.org/abs/2509.24254</link>
<guid>https://arxiv.org/abs/2509.24254</guid>
<content:encoded><![CDATA[
<div> Keywords: press releases, stock returns, earnings announcement, FinBERT, language analysis

Summary: 
Press releases are analyzed to predict stock returns on earnings announcement days. Traditional bag-of-words and BERT-based embeddings are compared, with FinBERT showing the highest predictive power. The study finds that press release content is as informative as earnings surprise in predicting stock returns. Combining models improves explanatory strength and interpretability of press release content. Stock prices reflect press release content at market open, and leaked press releases offer predictive advantage. Topic analysis reveals self-serving bias in managerial narratives. The framework supports real-time return prediction through online learning integration, providing interpretability and highlighting the role of language in price formation. <div>
arXiv:2509.24254v1 Announce Type: cross 
Abstract: We examine how textual features in earnings press releases predict stock returns on earnings announcement days. Using over 138,000 press releases from 2005 to 2023, we compare traditional bag-of-words and BERT-based embeddings. We find that press release content (soft information) is as informative as earnings surprise (hard information), with FinBERT yielding the highest predictive power. Combining models enhances explanatory strength and interpretability of the content of press releases. Stock prices fully reflect the content of press releases at market open. If press releases are leaked, it offers predictive advantage. Topic analysis reveals self-serving bias in managerial narratives. Our framework supports real-time return prediction through the integration of online learning, provides interpretability and reveals the nuanced role of language in price formation.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data</title>
<link>https://arxiv.org/abs/2509.24840</link>
<guid>https://arxiv.org/abs/2509.24840</guid>
<content:encoded><![CDATA[
<div> Keywords: single-cell RNA sequencing, Cell2Text, generative framework, gene expression, natural language descriptions

Summary:
Cell2Text is a multimodal generative framework that translates single-cell RNA sequencing profiles into structured natural language descriptions. By combining gene-level embeddings from single-cell foundation models with large language models, Cell2Text generates coherent summaries that capture cellular identity, tissue origin, disease associations, and pathway activity. This innovative approach outperforms baseline methods in classification accuracy, demonstrates strong ontological consistency, and achieves high semantic fidelity in text generation. The integration of expression data with natural language not only improves predictive performance but also provides inherently interpretable outputs. This advancement in technology offers a scalable solution for efficiently characterizing unseen cells with minimal label requirements. <div>
arXiv:2509.24840v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing has transformed biology by enabling the measurement of gene expression at cellular resolution, providing information for cell types, states, and disease contexts. Recently, single-cell foundation models have emerged as powerful tools for learning transferable representations directly from expression profiles, improving performance on classification and clustering tasks. However, these models are limited to discrete prediction heads, which collapse cellular complexity into predefined labels that fail to capture the richer, contextual explanations biologists need. We introduce Cell2Text, a multimodal generative framework that translates scRNA-seq profiles into structured natural language descriptions. By integrating gene-level embeddings from single-cell foundation models with pretrained large language models, Cell2Text generates coherent summaries that capture cellular identity, tissue origin, disease associations, and pathway activity, generalizing to unseen cells. Empirically, Cell2Text outperforms baselines on classification accuracy, demonstrates strong ontological consistency using PageRank-based similarity metrics, and achieves high semantic fidelity in text generation. These results demonstrate that coupling expression data with natural language offers both stronger predictive performance and inherently interpretable outputs, pointing to a scalable path for label-efficient characterization of unseen cells.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM Reconstruction</title>
<link>https://arxiv.org/abs/2509.25075</link>
<guid>https://arxiv.org/abs/2509.25075</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, Cryo-electron microscopy, 3D reconstruction, Neural radiance fields, Training efficiency
<br />
Summary:<br />
The article introduces GEM, a cryo-EM reconstruction framework based on 3D Gaussian Splatting (3DGS), aimed at improving efficiency and accuracy in high-resolution structural biology. GEM operates directly in real-space by representing proteins with compact 3D Gaussians, significantly reducing memory and training costs. By implementing a novel gradient computation method for 3D Gaussians, GEM achieves up to 48% faster training and 12% lower memory usage compared to existing methods. Additionally, GEM improves local resolution by up to 38.8%, demonstrating its practicality and scalability in cryo-EM reconstruction. The framework unifies speed, efficiency, and high-resolution accuracy, making it a valuable tool in the field. The code for GEM is openly available on GitHub for researchers to utilize and further develop. 
<br /> <div>
arXiv:2509.25075v1 Announce Type: cross 
Abstract: Cryo-electron microscopy (cryo-EM) has become a central tool for high-resolution structural biology, yet the massive scale of datasets (often exceeding 100k particle images) renders 3D reconstruction both computationally expensive and memory intensive. Traditional Fourier-space methods are efficient but lose fidelity due to repeated transforms, while recent real-space approaches based on neural radiance fields (NeRFs) improve accuracy but incur cubic memory and computation overhead. Therefore, we introduce GEM, a novel cryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that operates directly in real-space while maintaining high efficiency. Instead of modeling the entire density volume, GEM represents proteins with compact 3D Gaussians, each parameterized by only 11 values. To further improve the training efficiency, we designed a novel gradient computation to 3D Gaussians that contribute to each voxel. This design substantially reduced both memory footprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to 48% faster training and 12% lower memory usage compared to state-of-the-art methods, while improving local resolution by as much as 38.8%. These results establish GEM as a practical and scalable paradigm for cryo-EM reconstruction, unifying speed, efficiency, and high-resolution accuracy. Our code is available at https://github.com/UNITES-Lab/GEM.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A bound-preserving multinumerics scheme for steady-state convection-diffusion equations</title>
<link>https://arxiv.org/abs/2509.25181</link>
<guid>https://arxiv.org/abs/2509.25181</guid>
<content:encoded><![CDATA[
<div> adaptive partitioning, finite volume, discontinuous Galerkin, convection-diffusion equation, bound-preserving<br />
Summary:<br />
This study proposes a novel approach to solving the convection-diffusion equation by combining cell-centered finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is divided into FV and DG subdomains, connected through an interface term. An adaptive partitioning strategy is introduced, automatically selecting FV or DG based on solution accuracy. Whenever a cell average violates bounds, FV is applied in the vicinity of the element until all averages are bound-preserving within a specified tolerance. This process, akin to $p$-adaptivity, improves efficiency and accuracy in convection-dominated regimes. Standard benchmarks validate the effectiveness of this adaptive technique, showcasing its potential for accurate and computationally efficient convection-diffusion simulations. <div>
arXiv:2509.25181v1 Announce Type: cross 
Abstract: We solve the convection-diffusion equation using a coupling of cell-centered finite volume (FV) and discontinuous Galerkin (DG) methods. The domain is divided into disjoint regions assigned to FV or DG, and the two methods are coupled through an interface term. DG is stable and resolves sharp layers in convection-dominated regimes, but it can produce sizable spurious oscillations and is computationally expensive; FV (two-point flux) is low-order and monotone, but inexpensive. We propose a novel adaptive partitioning strategy that automatically selects FV and DG subdomains: whenever the solution's cell average violates the bounds, we switch to FV on a small neighborhood of that element. Viewed as a natural analog of $p$-adaptivity, this process is repeated until all cell averages are bound-preserving (up to some specified tolerance). Thereafter, standard conservative limiters may be applied to ensure the full solution is bound-preserving. Standard benchmarks confirm the effectiveness of the adaptive technique.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time</title>
<link>https://arxiv.org/abs/2412.18848</link>
<guid>https://arxiv.org/abs/2412.18848</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency markets, pump-and-dump schemes, Telegram groups, real-time prediction pipeline, natural language processing

Summary: 
Cryptocurrency markets are susceptible to manipulation through pump-and-dump schemes orchestrated by large Telegram groups, leading to artificial inflation of coin prices. These groups exploit information asymmetry by selling inside information, posing financial risks to subscribers and all investors. A real-time prediction pipeline incorporating advanced natural language processing technology has been developed to forecast target coins and alert investors to potential pump-and-dump schemes. In a case study on Poloniex, the model successfully identified the target coin in 55.81% of observed pump-and-dump events. By analyzing Telegram messages, the pipeline has identified over 2,000 past pump events and can detect new schemes as they emerge. This innovative approach aims to mitigate the impact of pump-and-dump schemes on cryptocurrency markets and provide investors with timely information to make informed decisions. 

<br /><br />Summary: <div>
arXiv:2412.18848v2 Announce Type: replace 
Abstract: Cryptocurrency markets often face manipulation through prevalent pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some exceeding two million members, artificially inflate target cryptocurrency prices. These groups sell premium access to inside information, worsening information asymmetry and financial risks for subscribers and all investors. This paper presents a real-time prediction pipeline to forecast target coins and alert investors to possible P&amp;D schemes. In a Poloniex case study, the model accurately identified the target coin among the top five from 50 random coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural language processing (NLP) to classify Telegram messages, identifying 2,079 past pump events and detecting new ones in real-time.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of spatial coarsening on Parareal convergence for the linear advection equation</title>
<link>https://arxiv.org/abs/2111.10228</link>
<guid>https://arxiv.org/abs/2111.10228</guid>
<content:encoded><![CDATA[
<div> hyperbolic partial differential equations, Parareal method, parallel-in-time integration, spatial resolution, numerical time stepping

Summary: 
The Parareal method, commonly used for parallel-in-time integration, faces challenges when applied to hyperbolic partial differential equations, especially with reduced spatial resolution. For linear problems, it is difficult to predict convergence using the 2-norm of the Parareal iteration matrix. However, a theorem suggests that the pseudo-spectral radius may reliably indicate whether a Parareal configuration will exhibit transient growth or monotonic convergence. Numerical results support this, showing that the pseudo-spectral radius can also estimate the convergence rate in initial Parareal iterations. This research sheds light on distinguishing configurations where Parareal errors decrease steadily from those where errors initially increase significantly, providing insights for efficient application in hyperbolic problems. <div>
arXiv:2111.10228v3 Announce Type: replace-cross 
Abstract: The Parareal parallel-in-time integration method often performs poorly when applied to hyperbolic partial differential equations. This effect is even more pronounced when the coarse propagator uses a reduced spatial resolution. However, some combinations of spatial discretization and numerical time stepping nevertheless allow for Parareal to converge with monotonically decreasing errors. This raises the question how these configurations can be distinguished theoretically from those where the error initially increases, sometimes over many orders of magnitude. For linear problems, we prove a theorem that implies that the 2-norm of the Parareal iteration matrix is not a suitable tool to predict convergence for hyperbolic problems when spatial coarsening is used. We then show numerical results that suggest that the pseudo-spectral radius can reliably indicate if a given configuration of Parareal will show transient growth or monotonic convergence. For the studied examples, it also provides a good quantitative estimate of the convergence rate in the first few Parareal iterations.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Occasional to Steady: Habit Formation Insights From a Comprehensive Fitness Study</title>
<link>https://arxiv.org/abs/2501.01779</link>
<guid>https://arxiv.org/abs/2501.01779</guid>
<content:encoded><![CDATA[
<div> gym attendance, habit formation, survival metric, clusters, personalized guidance<br />
<br />
Summary: 
This study analyzes gym attendance data to investigate factors influencing the formation of exercise habits. It identifies critical periods for habit formation and segments gym-goers into clusters based on visit patterns. The research highlights the importance of personalized guidance and social dynamics in sustaining long-term engagement. Results show that specific interventions like group classes and personal trainer sessions have varied impacts on different subgroups. Causal inference analysis confirms the significance of tailored, multi-dimensional approaches in promoting exercise habits. By considering individual characteristics, social dynamics, and strategic interventions, the study emphasizes the need for a personalized approach to support consistent exercise routines. <div>
arXiv:2501.01779v2 Announce Type: replace-cross 
Abstract: Regular exercise is widely recognized as a cornerstone of health, yet sustaining consistent exercise habits remains challenging. Understanding the factors that influence the formation of these habits is crucial for developing effective interventions. This study utilizes data from Mars Athletic Club, T\"urkiye's largest sports chain, to investigate the dynamics of gym attendance and habit formation. The general problem addressed by this study is identifying the critical periods and factors that contribute to the successful establishment of consistent exercise routines among gym-goers. We show that specific periods of attendance are most crucial for habit formation. By developing a survival metric based on gym attendance patterns, we pinpoint these key phases and segment members into distinct clusters based on their visit patterns. Our analysis reveals significant differences in how various subgroups respond to interventions, such as group classes, personal trainer sessions, and visiting different clubs. Using causal inference analysis, we demonstrate that personalized guidance and social dynamics are key drivers of sustained long-term engagement. By systematically examining these variables and considering the specific characteristics of different clusters, our research highlights the importance of a tailored, multi-dimensional approach to promoting exercise habits, which integrates social dynamics, personalized guidance, and strategic interventions to sustain long-term engagement.
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2502.01678</link>
<guid>https://arxiv.org/abs/2502.01678</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, Alzheimer's disease, deep learning, subject-level detection, pre-training <br />
Summary: <br />
The study focuses on using EEG for Alzheimer's disease detection, highlighting the challenges faced by existing methods. The authors introduce the largest EEG-AD dataset comprising 2,255 subjects and propose LEAD, a novel deep learning model for subject-level AD detection. The model includes a preprocessing pipeline, a subject-regularized spatio-temporal transformer with a unique loss function, and AD-guided contrastive pre-training. LEAD outperforms 10 baseline models in subject-level detection, achieving a Sensitivity of 90.91% on the ADFTD dataset using leave-one-subject-out cross-validation. The results demonstrate the efficacy of this approach for real-world EEG-based Alzheimer's disease detection. The source code for LEAD is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2502.01678v3 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) provides a non-invasive, highly accessible, and cost-effective approach for detecting Alzheimer's disease (AD). However, existing methods, whether based on handcrafted feature engineering or standard deep learning, face two major challenges: 1) the lack of large-scale EEG-AD datasets for robust representation learning, and 2) the absence of a dedicated deep learning pipeline for subject-level detection, which is more clinically meaningful than the commonly used sample-level detection. To address these gaps, we have curated the world's largest EEG-AD corpus to date, comprising 2,255 subjects. Leveraging this unique data corpus, we propose LEAD, the first large-scale foundation model for EEG analysis in dementia. Our approach provides an innovative framework for subject-level AD detection, including: 1) a comprehensive preprocessing pipeline such as artifact removal, resampling, and filtering, and a newly proposed multi-scale segmentation strategy, 2) a subject-regularized spatio-temporal transformer trained with a novel subject-level cross-entropy loss and an indices group-shuffling algorithm, and 3) AD-guided contrastive pre-training. We pre-train on 12 datasets (3 AD-related and 9 non-AD) and fine-tune/test on 4 AD datasets. Compared with 10 baselines, LEAD consistently obtains superior subject-level detection performance under the challenging subject-independent cross-validation protocol. On the benchmark ADFTD dataset, our model achieves an impressive subject-level Sensitivity of 90.91% under the leave-one-subject-out (LOSO) setting. These results strongly validate the effectiveness of our method for real-world EEG-based AD detection. Source code: https://github.com/DL4mHealth/LEAD
]]></content:encoded>
<pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sci2Pol: Evaluating and Fine-tuning LLMs on Scientific-to-Policy Brief Generation</title>
<link>https://arxiv.org/abs/2509.21493</link>
<guid>https://arxiv.org/abs/2509.21493</guid>
<content:encoded><![CDATA[
<div> Keywords: benchmark, training dataset, large language models, policy brief generation, fine-tuning

Summary: 
Sci2Pol-Bench and Sci2Pol-Corpus are introduced as benchmarks and training datasets for evaluating and fine-tuning large language models (LLMs) on policy brief generation from scientific papers. The Sci2Pol-Bench is structured based on a five-stage taxonomy mirroring the human writing process, featuring 18 tasks in various formats. Evaluation of LLMs using this benchmark reveals key limitations in current models. The new LLM-based evaluation metric introduced aligns with expert judgment, better capturing the quality of brief writing compared to existing metrics like BERTScore and ROUGE scores. The Sci2Pol-Corpus, curated for fine-tuning, pairs cited scientific papers with corresponding policy documents, filtered and polished using an LLM-as-a-judge and expert-written samples. Fine-tuning three models on the corpus leads to consistent performance improvements across the benchmark tasks. Notably, Gemma-27B outperforms larger models like GPT-4o and DeepSeek-V3 after fine-tuning, demonstrating the effectiveness of the corpus in bridging science and policy. 

<br /><br />Summary: <div>
arXiv:2509.21493v1 Announce Type: new 
Abstract: We propose Sci2Pol-Bench and Sci2Pol-Corpus, the first benchmark and training dataset for evaluating and fine-tuning large language models (LLMs) on policy brief generation from a scientific paper. We build Sci2Pol-Bench on a five-stage taxonomy to mirror the human writing process: (i) Autocompletion, (ii) Understanding, (iii) Summarization, (iv) Generation, and (v) Verification. It features 18 tasks in multiple-choice and open-ended formats. Specifically, for the Generation stage, we show that BERTScore and ROUGE scores fail to capture the quality of brief writing, and introduce a new LLM-based evaluation metric aligned with expert judgement. Using this benchmark, we evaluate 13 leading open-source and commercial LLMs to uncover key limitations. To improve LLM performance on brief writing, we curate the Sci2Pol-Corpus for fine-tuning. We start by linking each cited scientific paper to its corresponding policy document, drawn from 5.6 million policy records. This produces 140,000 candidate pairs. We then employ an LLM-as-a-judge to filter high-quality examples, followed by in-context polishing using three expert-written samples as references. This process yields a final set of 639 new pairs. Finally, we fine-tune three models on Sci2Pol-Corpus: LLaMA-3.1-8B, Gemma-12B, and Gemma-27B. Fine-tuning leads to consistent performance improvements across Sci2Pol-Bench. Notably, after fine-tuning, Gemma-27B surpasses the much larger GPT-4o and DeepSeek-V3 (671B). These demonstrate the effectiveness of our corpus in bridging the gap between science and policy.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantMind: A Context-Engineering Based Knowledge Framework for Quantitative Finance</title>
<link>https://arxiv.org/abs/2509.21507</link>
<guid>https://arxiv.org/abs/2509.21507</guid>
<content:encoded><![CDATA[
<div> Keywords: QuantMind, knowledge extraction, structured knowledge, quantitative finance, semantic search 

Summary: 
Quantitative research in finance increasingly relies on unstructured financial content like filings and research notes. Existing pipelines struggle with accuracy, evidence attribution, and integration into research workflows. To address these challenges, QuantMind is introduced as an intelligent knowledge extraction and retrieval framework tailored to quantitative finance. It consists of a two-stage architecture: knowledge extraction transforms diverse documents into structured knowledge through parsing text, tables, and formulas, while intelligent retrieval integrates semantic search with multi-hop reasoning for auditable outputs. A user study demonstrates that QuantMind enhances factual accuracy and user experience compared to unaided reading and generic AI assistance, showcasing the importance of domain-specific context engineering in finance.

Summary:  <div>
arXiv:2509.21507v1 Announce Type: new 
Abstract: Quantitative research increasingly relies on unstructured financial content such as filings, earnings calls, and research notes, yet existing LLM and RAG pipelines struggle with point-in-time correctness, evidence attribution, and integration into research workflows. To tackle this, We present QuantMind, an intelligent knowledge extraction and retrieval framework tailored to quantitative finance. QuantMind adopts a two-stage architecture: (i) a knowledge extraction stage that transforms heterogeneous documents into structured knowledge through multi-modal parsing of text, tables, and formulas, adaptive summarization for scalability, and domain-specific tagging for fine-grained indexing; and (ii) an intelligent retrieval stage that integrates semantic search with flexible strategies, multi-hop reasoning across sources, and knowledge-aware generation for auditable outputs. A controlled user study demonstrates that QuantMind improves both factual accuracy and user experience compared to unaided reading and generic AI assistance, underscoring the value of structured, domain-specific context engineering for finance.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI for Sustainable Future Foods</title>
<link>https://arxiv.org/abs/2509.21556</link>
<guid>https://arxiv.org/abs/2509.21556</guid>
<content:encoded><![CDATA[
<div> AI for Food, transformative, molecular composition, functional performance, sustainability, innovation<br />
<br />
Summary: AI for Food is an emerging discipline that aims to revolutionize global food systems by leveraging artificial intelligence to enhance ingredient design, formulation development, sensory analysis, and more. While early successes show promise in predicting protein performance and mapping molecules to flavor, challenges such as lack of standardization, limited data, and cultural diversity still exist. To unlock the potential of AI in food innovation, three priorities are proposed: treating food as a programmable biomaterial, creating self-driving laboratories for automated discovery, and developing deep reasoning models that integrate sustainability and human health. By responsibly incorporating AI into the food innovation cycle, the transition to sustainable protein systems can be accelerated, leading to a predictive, design-driven science of food that benefits both human health and the planet.<br /><br /> <div>
arXiv:2509.21556v1 Announce Type: new 
Abstract: Global food systems must deliver nutritious and sustainable foods while sharply reducing environmental impact. Yet, food innovation remains slow, empirical, and fragmented. Artificial intelligence (AI) now offers a transformative path with the potential to link molecular composition to functional performance, bridge chemical structure to sensory outcomes, and accelerate cross-disciplinary innovation across the entire production pipeline. Here we outline AI for Food as an emerging discipline that integrates ingredient design, formulation development, fermentation and production, texture analysis, sensory properties, manufacturing, and recipe generation. Early successes demonstrate how AI can predict protein performance, map molecules to flavor, and tailor consumer experiences. But significant challenges remain: lack of standardization, scarce multimodal data, cultural and nutritional diversity, and low consumer confidence. We propose three priorities to unlock the field: treating food as a programmable biomaterial, building self-driving laboratories for automated discovery, and developing deep reasoning models that integrate sustainability and human health. By embedding AI responsibly into the food innovation cycle, we can accelerate the transition to sustainable protein systems and chart a predictive, design-driven science of food for our own health and the health of our planet.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Method of Moments and Generalized Scattering Matrix: Applications to Antennas in Radomes, Reflectors, and Implantable Media</title>
<link>https://arxiv.org/abs/2509.22000</link>
<guid>https://arxiv.org/abs/2509.22000</guid>
<content:encoded><![CDATA[
<div> Keywords: Antennas, Multiscale modeling, Method of moments, Generalized scattering matrix, Hybrid framework <br />
Summary: 
Antennas embedded in or interacting with large structures pose challenges due to their different scales. A hybrid method combining Method of Moments (MoM) and Generalized Scattering Matrix (GSM) has been developed to address these challenges. This framework allows for the separate treatment of fine-scale antenna details and large-scale environment characteristics while maintaining their full coupling. Antennas of any shape can be characterized and reused across various environments, or a single environment can accommodate multiple antenna designs. The framework is versatile and can be extended to include GSM-PO and GSM + T-matrix methods, creating a unified approach for multiscale antenna modeling. By representing the large structure in the formulation best suited to its scale and shape, the approach offers accuracy, efficiency, and adaptability. Numerical validations on different antenna systems show excellent agreement with full-wave solvers and significant computational cost reductions for design and optimization. <br /><br /> <div>
arXiv:2509.22000v1 Announce Type: new 
Abstract: Electromagnetic analysis of antennas embedded in or interacting with large surrounding structures poses inherent multiscale challenges: the antenna is electrically small yet geometrically detailed, while the environment is electrically large but comparatively smooth. To address this, we present a hybrid method of moments (MoM) and generalized scattering matrix (GSM) framework that achieves a clean separation between fine-scale and large-scale complexities while preserving their full mutual coupling. Antennas of arbitrary geometry can be characterized once and reused across different environments, or conversely, a given environment can be modeled once to accommodate multiple antenna designs. The framework is inherently versatile, encompassing GSM-PO and GSM + T-matrix extensions, and thus provides a unified paradigm for multiscale antenna modeling. With the large body always represented by the formulation best suited to its scale and shape, the approach combines accuracy, efficiency, and adaptability. Numerical validations on implantable antennas, radome-protected arrays, and reflector systems confirm excellent agreement with full-wave solvers while demonstrating dramatic reductions in computational cost for design and optimization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orochi: Versatile Biomedical Image Processor</title>
<link>https://arxiv.org/abs/2509.22583</link>
<guid>https://arxiv.org/abs/2509.22583</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, biomedical images, Orochi, computational efficiency, fine-tuning framework

Summary: 
Orochi is introduced as an application-oriented image processor focusing on biomedical image processing. The tool is pre-trained on a wide range of publicly available studies using a Random Multi-scale Sampling strategy. Additionally, the Task-related Joint-embedding Pre-Training (TJP) method is proposed for self-supervision, offering an alternative to traditional Masked Image Modelling (MIM) and enhancing performance in tasks like registration. The computational efficiency of Orochi is improved through the use of Mamba's linear complexity and Multi-head Hierarchy Mamba. The tool offers a three-tier fine-tuning framework (Full, Normal, Light) for flexibility and parameter efficiency. Orochi demonstrates comparable or superior performance to existing specialist models, providing biologists with a versatile and efficient tool to streamline their workflow by eliminating the need to choose among multiple models.

<br /><br />Summary: <div>
arXiv:2509.22583v1 Announce Type: new 
Abstract: Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce Orochi, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches/volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mamba's linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Risk Sharing on Networks</title>
<link>https://arxiv.org/abs/2509.21411</link>
<guid>https://arxiv.org/abs/2509.21411</guid>
<content:encoded><![CDATA[
<div> Linear risk sharing, network structures, stochastic matrices, risk redistribution, peer-to-peer insurance<br />
<br />Summary: The article presents a framework for linear risk sharing (LRS) in alternative insurance and banking systems that rely on network structures. It explores how random losses can be reallocated through nonnegative linear operators in various network topologies, ensuring constraints such as budget balance, fairness, and diversification. The analysis uses stochastic and doubly stochastic matrices to compare different risk allocations rigorously, emphasizing variance reduction and majorization through doubly stochastic mixing. The study extends to network-based risk sharing in different graph types, showing how network topology influences risk outcomes. Introducing randomness in the sharing matrix via Erd\H{o}s--R\'enyi and preferential-attachment networks links risk-sharing properties to degree distributions. The research also examines the trade-off between self-retention and diversification in peer-to-peer insurance and network-based risk pooling, offering design principles for fair and efficient risk redistribution. <div>
arXiv:2509.21411v1 Announce Type: cross 
Abstract: Over the past decade alternatives to traditional insurance and banking have grown in popularity. The desire to encourage local participation has lead products such as peer-to-peer insurance, reciprocal contracts, and decentralized finance platforms to increasingly rely on network structures to redistribute risk among participants. In this paper, we develop a comprehensive framework for linear risk sharing (LRS), where random losses are reallocated through nonnegative linear operators which can accommodate a wide range of networks. Building on the theory of stochastic and doubly stochastic matrices, we establish conditions under which constraints such as budget balance, fairness, and diversification are guaranteed. The convex order framework allows us to compare different allocations rigorously, highlighting variance reduction and majorization as natural consequences of doubly stochastic mixing. We then extend the analysis to network-based sharing, showing how their topology shapes risk outcomes in complete, star, ring, random, and scale-free graphs. A second layer of randomness, where the sharing matrix itself is random, is introduced via Erd\H{o}s--R\'enyi and preferential-attachment networks, connecting risk-sharing properties to degree distributions. Finally, we study convex combinations of identity and network-induced operators, capturing the trade-off between self-retention and diversification. Our results provide design principles for fair and efficient peer-to-peer insurance and network-based risk pooling, combining mathematical soundness with economic interpretability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Ethereum's Geographical (De)Centralization Beyond the Atlantic</title>
<link>https://arxiv.org/abs/2509.21475</link>
<guid>https://arxiv.org/abs/2509.21475</guid>
<content:encoded><![CDATA[
<div> latency, decentralization, Ethereum, protocol design, geographical distribution
Summary:
The article explores how protocol design influences the geographical distribution of validators in decentralized systems like Ethereum. It compares two block-building paradigms, SSP and MSP, and finds that SSP concentrates around relay placement, while MSP centralizes faster due to location-dependent payoff dispersion. The article highlights the importance of source placement and consensus settings in shaping validator geography but notes that once validators are already clustered, source placement's impact on decentralization diminishes. North America consistently emerges as a focal hub in most scenarios, indicating a trend towards centralization in the region. The study suggests that protocol design can play a significant role in promoting geographical decentralization and offers insights into potential levers for achieving this goal. <div>
arXiv:2509.21475v1 Announce Type: cross 
Abstract: Decentralization has a geographic dimension that conventional metrics such as stake distribution overlook. Where validators run affects resilience to regional shocks (outages, disasters, government intervention) and fairness in reward access. Yet in permissionless systems, locations cannot be mandated, but they emerge from incentives. Today, Ethereum's validators cluster along the Atlantic (EU and U.S. East Coast), where latency is structurally favorable. This raises a key question: when some regions already enjoy latency advantages, how does protocol design shape validator incentives and the geography of (de)centralization? We develop a latency-calibrated agent-based model and compare two Ethereum block-building paradigms: a Single-Source Paradigm (SSP), akin to MEV-Boost, where proposers fetch full blocks from a relay that also propagates them; and a Multi-Source Paradigm (MSP), where proposers aggregate value from multiple sources and broadcast the block themselves. Simulations show that SSP concentrates around relay placement but more slowly, since proximity mainly affects propagation, and the marginal value of time is relatively uniform across regions. MSP centralizes faster: aggregating across sources makes marginal value location-dependent, amplifying payoff dispersion and migration toward latency minima. Source placement and consensus settings can dampen or intensify these effects, though once validators are already clustered, the impact of source placement on decentralization is marginal. In most cases, North America consistently emerges as the focal hub. These findings show that protocol design materially shapes validator geography and offer levers for promoting geographical decentralization.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need</title>
<link>https://arxiv.org/abs/2509.21553</link>
<guid>https://arxiv.org/abs/2509.21553</guid>
<content:encoded><![CDATA[
<div> knowledge graph, AI agents, climate data science, data accessibility, scientific workflows

Summary:
This paper addresses the challenges faced in climate data science by integrating a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG acts as a unifying layer organizing datasets, tools, and workflows, while AI agents enhance natural language interaction, data access automation, and streamlined analysis using generative AI services. By leveraging cloud-ready API data portals, the system demonstrates that a knowledge graph can significantly reduce the technical barriers for engaging in climate data science, enabling non-specialists to identify and analyze relevant datasets. The open-source design allows for community contributions, supporting the evolution of the KG and associated tools as a shared commons. This approach aims to democratize access to climate data, establish a reproducible framework for scientific inquiry, and facilitate human-AI collaboration in research. 

<br /><br />Summary: <div>
arXiv:2509.21553v1 Announce Type: cross 
Abstract: Climate data science faces persistent barriers stemming from the fragmented nature of data sources, heterogeneous formats, and the steep technical expertise required to identify, acquire, and process datasets. These challenges limit participation, slow discovery, and reduce the reproducibility of scientific workflows. In this paper, we present a proof of concept for addressing these barriers through the integration of a curated knowledge graph (KG) with AI agents designed for cloud-native scientific workflows. The KG provides a unifying layer that organizes datasets, tools, and workflows, while AI agents -- powered by generative AI services -- enable natural language interaction, automated data access, and streamlined analysis. Together, these components drastically lower the technical threshold for engaging in climate data science, enabling non-specialist users to identify and analyze relevant datasets. By leveraging existing cloud-ready API data portals, we demonstrate that "a knowledge graph is all you need" to unlock scalable and agentic workflows for scientific inquiry. The open-source design of our system further supports community contributions, ensuring that the KG and associated tools can evolve as a shared commons. Our results illustrate a pathway toward democratizing access to climate data and establishing a reproducible, extensible framework for human--AI collaboration in scientific research.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bacterial Gene Regulatory Neural Network as a Biocomputing Library of Mathematical Solvers</title>
<link>https://arxiv.org/abs/2509.21598</link>
<guid>https://arxiv.org/abs/2509.21598</guid>
<content:encoded><![CDATA[
<div> Keywords: biocomputing, gene expression dynamics, GRNN framework, mathematical solvers, reliability

Summary:
The article presents a novel approach to biocomputing by utilizing bacterial gene expression dynamics within a GRNN framework to create a library of mathematical solvers. A sub-GRNN search algorithm is introduced to customize functional subnetworks for specific mathematical tasks by analyzing gene expression patterns under different input conditions. Tasks such as identifying Fibonacci numbers, prime numbers, multiplication, and Collatz step counts are successfully achieved using this approach. The study assesses the identified sub-GRNNs for robustness and reliability through gene-wise and collective perturbation and Lyapunov-based stability analysis. The results highlight the potential of leveraging native transcriptional machinery for performing diverse mathematical calculations and classifications while ensuring computing stability and reliability. <div>
arXiv:2509.21598v1 Announce Type: cross 
Abstract: Current biocomputing approaches predominantly rely on engineered circuits with fixed logic, offering limited stability and reliability under diverse environmental conditions. Here, we use the GRNN framework introduced in our previous work to transform bacterial gene expression dynamics into a biocomputing library of mathematical solvers. We introduce a sub-GRNN search algorithm that identifies functional subnetworks tailored to specific mathematical calculation and classification tasks by evaluating gene expression patterns across chemically encoded input conditions. Tasks include identifying Fibonacci numbers, prime numbers, multiplication, and Collatz step counts. The identified problem-specific sub-GRNNs are then assessed using gene-wise and collective perturbation, as well as Lyapunov-based stability analysis, to evaluate robustness and reliability. Our results demonstrate that native transcriptional machinery can be harnessed to perform diverse mathematical calculation and classification tasks, while maintaining computing stability and reliability.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning-Assisted Sustainable Remanufacturing, Reusing and Recycling for Lithium-ion Batteries</title>
<link>https://arxiv.org/abs/2406.00276</link>
<guid>https://arxiv.org/abs/2406.00276</guid>
<content:encoded><![CDATA[
<div> Machine learning, lithium-ion batteries, sustainability, circular economy, carbon neutrality <br />
Summary: <br />
The dissertation proposes a machine learning framework to tackle data scarcity and heterogeneity in the lifecycle of lithium-ion batteries (LIBs). A physics-informed quality control model predicts long-term degradation, while a generative learning-based method evaluates retired batteries accurately. Federated learning ensures privacy-preserving and high-precision cathode material sorting for efficient recycling. A unified diagnostics and prognostics framework based on correlation alignment enhances adaptability across multiple tasks, including state of health estimation and remaining useful life prediction. These contributions integrate physics, data generation, privacy preservation, and adaptive learning to advance sustainable battery management, promoting circular economy and global carbon neutrality. <div>
arXiv:2406.00276v2 Announce Type: replace-cross 
Abstract: The sustainable utilization of lithium-ion batteries (LIBs) is crucial to the global energy transition and carbon neutrality, yet data scarcity and heterogeneity remain major barriers across remanufacturing, reusing, and recycling. This dissertation develops a machine learning assisted framework to address these challenges throughout the battery lifecycle. A physics informed quality control model predicts long-term degradation from limited early-cycle data, while a generative learning based residual value assessment method enables rapid and accurate evaluation of retired batteries under random conditions. A federated learning strategy achieves privacy preserving and high precision cathode material sorting, supporting efficient recycling. Furthermore, a unified diagnostics and prognostics framework based on correlation alignment enhances adaptability across tasks such as state of health estimation, state of charge estimation, and remaining useful life prediction under varied testing protocols. Collectively, these contributions advance sustainable battery management by integrating physics, data generation, privacy preserving collaboration, and adaptive learning, offering methodological innovations to promote circular economy and global carbon neutrality.
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Domain-adaptive Post-training for Financial LLMs</title>
<link>https://arxiv.org/abs/2501.04961</link>
<guid>https://arxiv.org/abs/2501.04961</guid>
<content:encoded><![CDATA[
<div> Keywords: domain-adaptive, large language models, finance, post-training, evaluation

Summary:
The study introduces FINDAP, a systematic approach for domain-adaptive post-training of large language models (LLMs) in the finance domain. It includes FinCap, defining core domain capabilities, FinRec, an optimized training recipe incorporating data distillation, FinTrain, a set of curated training datasets, and FinEval, a comprehensive evaluation suite. The resulting Llama-Fin model achieves top performance in financial tasks, showcasing the effectiveness of each training stage in enhancing specific capabilities. The study provides insights into challenges and solutions for domain adaptation of LLMs, highlighting the importance of tailored training strategies and evaluation criteria in specialized domains like finance.<br /><br />Summary: <div>
arXiv:2501.04961v3 Announce Type: replace-cross 
Abstract: Domain-adaptive post-training of large language models (LLMs) has emerged as a promising approach for specialized domains such as medicine and finance. However, significant challenges remain in identifying optimal adaptation criteria and training strategies across varying data and model configurations. To address these challenges, we introduce FINDAP, a systematic and fine-grained investigation into domain-adaptive post-training of LLMs for the finance domain. Our approach consists of four key components: FinCap, which defines the core capabilities required for the target domain; FinRec, an effective training recipe that jointly optimizes continual pre-training and instruction-following, along with a novel preference data distillation method leveraging process signals from a generative reward model; FinTrain, a curated set of training datasets supporting FinRec; and FinEval, a comprehensive evaluation suite aligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art performance across a wide range of financial tasks. Our analysis also highlights how each post-training stage contributes to distinct capabilities, uncovering specific challenges and effective solutions, providing valuable insights for domain adaptation of LLMs
]]></content:encoded>
<pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Adaptive Diffusion Model for Flexible Protein-Protein Docking</title>
<link>https://arxiv.org/abs/2509.20542</link>
<guid>https://arxiv.org/abs/2509.20542</guid>
<content:encoded><![CDATA[
<div> Keywords: protein-protein interactions, structural prediction, hierarchical adaptive diffusion, conformational changes, DIPS-AF dataset <br />
Summary: 
The study introduces a hierarchical adaptive diffusion framework for predicting protein-protein interactions, particularly focusing on cases with significant conformational changes. The framework separates global rigid-body motions and local flexibility, incorporating noise schedules to mimic induced-fit effects. Adaptive scheduling based on predicted conformational changes allows for faster flexing in response to variations. By leveraging a dataset of 39,000 examples for pre-training, the model outperforms existing methods on a benchmark dataset in both rigid and flexible scenarios. Ablation studies highlight the significance of adaptive schedules, dynamics features, and pre-training. Despite improvements, gaps in sampling, scoring, and conformational resolution remain to be addressed. Case studies shed light on the practical implications of the proposed framework. This innovative approach holds promise for enhancing accuracy and efficiency in predicting complex protein-protein interactions. <br /> <div>
arXiv:2509.20542v1 Announce Type: new 
Abstract: Structural prediction of protein-protein interactions is important to understand the molecular basis of cellular interactions, but it still faces major challenges when significant conformational changes are present. We propose a generative framework of hierarchical adaptive diffusion to improve accuracy and efficiency in such cases. It is hierarchical in separating global inter-protein rigid-body motions and local intra-protein flexibility in diffusion processes, and the distinct local and global noise schedules are designed to mimic the induced-fit effect. It is adaptive in conditioning the local flexibility schedule on predicted levels of conformational change, allowing faster flexing for larger anticipated conformational changes. Furthermore, it couples the local and global diffusion processes through a common score and confidence network with sequence, evolution, structure, and dynamics features as inputs, and maintains rotational or translational invariance or equivariance in outputs. It builds on our newly curated DIPS-AF dataset of nearly 39,000 examples for pre-training. Experiments on the independent docking benchmark dataset DB5.5 show that our model outperforms an AlphaFold2-like iterative transformer (GeoDock) and a diffusion model (DiffDock-PP) in both rigid and flexible cases, with larger improvements in more flexible cases. Ablation studies prove the importance of adaptive schedules, dynamics features, and pre-training. Additional analyses and case studies reveal remaining gaps in sampling, scoring, and conformational resolution.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Difference-Guided Reasoning: A Temporal-Spatial Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2509.20713</link>
<guid>https://arxiv.org/abs/2509.20713</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, reasoning, differences, abnormal behavior detection, external information integration

Summary:
Large Language Models (LLMs) are powerful tools but often lack the ability to actively discover new questions, hindering human-like reasoning. To address this limitation, a difference-guided reasoning framework is proposed. This framework enables LLMs to identify and act upon changes over time and space by formalizing differences through feature extraction and prioritizing impactful changes. Additionally, mechanisms for abnormal behavior detection and the integration of external information are included to enhance the reliability of reasoning. Verification results demonstrate that prompting LLMs with differences enhances focus on critical issues, improving alignment with desired reasoning outcomes compared to direct prompting. The framework provides a structured approach for LLMs to reason more effectively and simulate human-like thinking. 

<br /><br />Summary: 
- Proposal of a difference-guided reasoning framework for Large Language Models (LLMs)
- LLMs can identify and act upon changes through feature extraction and prioritization
- Inclusion of mechanisms for abnormal behavior detection and external information integration
- Verification results show improved focus on critical issues and alignment with desired reasoning outcomes
- Framework enhances LLMs' ability to reason effectively and simulate human-like thinking. <div>
arXiv:2509.20713v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are important tools for reasoning and problem-solving, while they often operate passively, answering questions without actively discovering new ones. This limitation reduces their ability to simulate human-like thinking, where noticing differences is a key trigger for reasoning. Thus, in this paper we propose a difference-guided reasoning framework, which enables LLMs to identify and act upon changes across time and space. The model formalizes differences through feature extraction, prioritizes the most impactful and latest changes, and links them to appropriate actions. We further extend the framework with mechanisms for abnormal behavior detection and the integration of external information from users or sensors, ensuring more reliable and grounded reasoning. Verification results show that prompting LLMs with differences improves focus on critical issues, leading to higher alignment with desired reasoning outcomes compared to direct prompting.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extrapolating Phase-Field Simulations in Space and Time with Purely Convolutional Architectures</title>
<link>https://arxiv.org/abs/2509.20770</link>
<guid>https://arxiv.org/abs/2509.20770</guid>
<content:encoded><![CDATA[
<div> surrogate, convolutional self-attention, physics-aware padding, variable time-step skipping, alloy systems
Summary:
The article introduces a new approach for accelerating phase-field models of liquid metal dealloying (LMD) using a conditionally parameterized, fully convolutional U-Net surrogate. This surrogate model incorporates convolutional self-attention and physics-aware padding, allowing it to generalize beyond its training window in both space and time. By leveraging parameter conditioning, the surrogate can adapt to different alloy systems and skip variable time steps. Despite being trained on short simulations, the surrogate achieves high accuracy in reproducing LMD physics, with relative errors typically under 5%. It can accelerate computations by up to 16,000 times, significantly reducing simulation times from weeks to seconds. This method represents a promising advancement towards scalable and high-fidelity extrapolation of LMD phase-field models. <br /><br />Summary: <div>
arXiv:2509.20770v1 Announce Type: new 
Abstract: Phase-field models of liquid metal dealloying (LMD) can resolve rich microstructural dynamics but become intractable for large domains or long time horizons. We present a conditionally parameterized, fully convolutional U-Net surrogate that generalizes far beyond its training window in both space and time. The design integrates convolutional self-attention and physics-aware padding, while parameter conditioning enables variable time-step skipping and adaptation to diverse alloy systems. Although trained only on short, small-scale simulations, the surrogate exploits the translational invariance of convolutions to extend predictions to much longer horizons than traditional solvers. It accurately reproduces key LMD physics, with relative errors typically under 5% within the training regime and below 10% when extrapolating to larger domains and later times. The method accelerates computations by up to 16,000 times, cutting weeks of simulation down to seconds, and marks an early step toward scalable, high-fidelity extrapolation of LMD phase-field models.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh Interpolation Graph Network for Dynamic and Spatially Irregular Global Weather Forecasting</title>
<link>https://arxiv.org/abs/2509.20911</link>
<guid>https://arxiv.org/abs/2509.20911</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, weather forecasting, global, irregularly distributed, generalization<br />
<br />
Summary: 
The study focuses on global weather forecasting, accounting for the irregular distribution of weather stations and the dynamic nature of weather patterns. The proposed Mesh Interpolation Graph Network (MIGN) addresses the challenges by utilizing a regular mesh interpolation network to handle spatially irregular data and incorporating parametric spherical harmonics location embedding for enhanced spatial generalization. Experimental results demonstrate that MIGN outperforms existing data-driven models, showcasing its spatial generalization ability and capability to predict weather at unseen locations. <div>
arXiv:2509.20911v1 Announce Type: new 
Abstract: Graph neural networks have shown promising results in weather forecasting, which is critical for human activity such as agriculture planning and extreme weather preparation. However, most studies focus on finite and local areas for training, overlooking the influence of broader areas and limiting their ability to generalize effectively. Thus, in this work, we study global weather forecasting that is irregularly distributed and dynamically varying in practice, requiring the model to generalize to unobserved locations. To address such challenges, we propose a general Mesh Interpolation Graph Network (MIGN) that models the irregular weather station forecasting, consisting of two key designs: (1) learning spatially irregular data with regular mesh interpolation network to align the data; (2) leveraging parametric spherical harmonics location embedding to further enhance spatial generalization ability. Extensive experiments on an up-to-date observation dataset show that MIGN significantly outperforms existing data-driven models. Besides, we show that MIGN has spatial generalization ability, and is capable of generalizing to previous unseen stations.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extensions of a Line-Graph-Based Method for Token Routing in Decentralized Exchanges</title>
<link>https://arxiv.org/abs/2509.21152</link>
<guid>https://arxiv.org/abs/2509.21152</guid>
<content:encoded><![CDATA[
<div> Decentralized exchanges, DEXs, decentralized finance, DeFi, token trades<br />
Summary:<br />
Decentralized exchanges (DEXs) play a crucial role in the decentralized finance (DeFi) ecosystem by processing billions of dollars in token trades daily. However, a significant amount of these trades are inefficient, missing out on potential profits. This study introduces three key extensions to improve routing methods in DEXs. First, the implementation of a breadth-first search (BFS) link iteration rule reduces computational cost and execution time while maintaining profitability. Second, a route-splitting strategy divides large trades to minimize price slippage and increase trader profits, at the expense of higher computational overhead. Third, the method is expanded to a multi-DEX aggregator setting to reflect real-world trading environments. Empirical data from Uniswap V2 and Sushiswap V2 shows significant enhancements in both computational efficiency and profitability, paving the way for future routing improvements.<br /> <div>
arXiv:2509.21152v1 Announce Type: new 
Abstract: Decentralized exchanges (DEXs) form a cornerstone of the decentralized finance (DeFi) ecosystem, processing token trades worth billions of dollars daily. Yet, a significant fraction of these trades are suboptimal: alternative routing paths could yield more target tokens. Addressing this inefficiency is both practically urgent and theoretically compelling. Building on the linear line-graph-based routing method of Zhang et al. (2025), we propose three key extensions that better capture real-world trading complexity. First, we introduce a breadth-first search (BFS) link iteration rule that reduces computational cost and average execution time without sacrificing profitability. Second, we design a route-splitting strategy that divides large trades into smaller ones, alleviating price slippage and increasing average trader profits, albeit at the cost of higher computational overhead. Third, we generalize the method beyond a single DEX to a multi-DEX aggregator setting, reflecting actual trading environments. Using empirical data from Uniswap V2 and Sushiswap V2, we demonstrate that these extensions substantially improve both computational efficiency and profitability, establishing a foundation for future routing enhancements.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models</title>
<link>https://arxiv.org/abs/2509.20570</link>
<guid>https://arxiv.org/abs/2509.20570</guid>
<content:encoded><![CDATA[
<div> physics-informed generation, reward optimization, diffusion models, value function, PIRF <br />
Summary: 
The article introduces a new approach to physics-informed generation by treating adherence to physical constraints as a reward signal in a sparse reward optimization framework. The proposed method, Physics-Informed Reward Fine-tuning (PIRF), bypasses the use of diffusion posterior sampling-style value function approximations, which are prone to errors and training instability. PIRF computes trajectory-level rewards and backpropagates their gradients directly, enhancing efficiency and data fidelity. Two key strategies, layer-wise truncated backpropagation and weight-based regularization, improve sample efficiency and physical enforcement in scientific generative modeling. Across multiple benchmarks, PIRF consistently outperforms traditional methods in enforcing physical constraints while maintaining efficient sampling regimes. This highlights the potential of reward fine-tuning for advancing generative modeling in scientific domains. <br /> <div>
arXiv:2509.20570v1 Announce Type: cross 
Abstract: Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLIP Arena: Advancing Fairness and Transparency in Machine Learning Interatomic Potentials via an Open, Accessible Benchmark Platform</title>
<link>https://arxiv.org/abs/2509.20630</link>
<guid>https://arxiv.org/abs/2509.20630</guid>
<content:encoded><![CDATA[
<div> benchmark, machine learning interatomic potentials, physics awareness, chemical reactivity, predictive capabilities
Summary:
Machine learning interatomic potentials (MLIPs) have transformed molecular and materials modeling, but existing benchmarks face challenges like data leakage and limited transferability. The new MLIP Arena platform assesses force field performance based on physics awareness, chemical reactivity, and stability in extreme conditions. It evaluates predictive capabilities for thermodynamic properties and physical phenomena beyond error-based metrics tied to specific density functional theory references. By highlighting failure modes of current MLIPs in real-world scenarios, MLIP Arena guides the development of more accurate and efficient MLIPs while ensuring physical consistency. The Python package and online leaderboard for MLIP Arena can be accessed at https://github.com/atomind-ai/mlip-arena. 
<br /><br />Summary: <div>
arXiv:2509.20630v1 Announce Type: cross 
Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized molecular and materials modeling, but existing benchmarks suffer from data leakage, limited transferability, and an over-reliance on error-based metrics tied to specific density functional theory (DFT) references. We introduce MLIP Arena, a benchmark platform that evaluates force field performance based on physics awareness, chemical reactivity, stability under extreme conditions, and predictive capabilities for thermodynamic properties and physical phenomena. By moving beyond static DFT references and revealing the important failure modes of current foundation MLIPs in real-world settings, MLIP Arena provides a reproducible framework to guide the next-generation MLIP development toward improved predictive accuracy and runtime efficiency while maintaining physical consistency. The Python package and online leaderboard are available at https://github.com/atomind-ai/mlip-arena.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations</title>
<link>https://arxiv.org/abs/2509.20667</link>
<guid>https://arxiv.org/abs/2509.20667</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, chemistry computations, supercomputer, prediction, resource optimization

Summary:
Machine learning models were developed to predict the resources required for massively parallel chemistry computations, such as coupled-cluster methods, on supercomputers. The models aim to guide users in selecting optimal runtime parameter values to minimize execution time and resource usage. Two key questions addressed were the shortest-time question, determining parameter configurations for the shortest execution time, and the cheapest-run question, minimizing resource usage. Evaluation of ML models on CCSD application runtime parameter values showed a Gradient Boosting model achieving low Mean Absolute Percentage Errors on DOE Frontier and Aurora supercomputers. Active learning demonstrated effectiveness in achieving accurate predictions with limited experimental data points. This work provides valuable insights for application users to make informed decisions before running costly experiments on supercomputers.<br /><br />Summary: Machine learning models were developed to predict resources for chemistry computations on supercomputers, guiding users in optimizing runtime parameters for efficiency. Evaluation on CCSD application data showed Gradient Boosting model accuracy, with active learning efficiently obtaining predictions with limited data points. <div>
arXiv:2509.20667v1 Announce Type: cross 
Abstract: In this work, we develop machine learning (ML) based strategies to predict resources (costs) required for massively parallel chemistry computations, such as coupled-cluster methods, to guide application users before they commit to running expensive experiments on a supercomputer. By predicting application execution time, we determine the optimal runtime parameter values such as number of nodes and tile sizes. Two key questions of interest to users are addressed. The first is the shortest-time question, where the user is interested in knowing the parameter configurations (number of nodes and tile sizes) to achieve the shortest execution time for a given problem size and a target supercomputer. The second is the cheapest-run question in which the user is interested in minimizing resource usage, i.e., finding the number of nodes and tile size that minimizes the number of node-hours for a given problem size.
  We evaluate a rich family of ML models and strategies, developed based on the collections of runtime parameter values for the CCSD (Coupled Cluster with Singles and Doubles) application executed on the Department of Energy (DOE) Frontier and Aurora supercomputers. Our experiments show that when predicting the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora and Frontier, respectively. In the case where it is expensive to run experiments just to collect data points, we show that active learning can achieve a MAPE of about 0.2 with just around 450 experiments collected from Aurora and Frontier.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Semantic Representations of Social Interactions from Moving Shapes</title>
<link>https://arxiv.org/abs/2509.20673</link>
<guid>https://arxiv.org/abs/2509.20673</guid>
<content:encoded><![CDATA[
<div> Keywords: social interactions, visual features, semantic representations, human similarity judgments, verb-based embeddings

Summary:
Humans can recognize social interactions from simple moving shapes, using both visual features and semantic representations. Study 1 showed that human responses to labeled animations were varied. Study 2 analyzed the geometry of 27 social interactions through human similarity judgments and compared them with model predictions. Semantic models, particularly verb-based embeddings from descriptions, offered additional insights beyond visual features in explaining human judgments. The results indicate that social perception in animations incorporates both visual and abstract representations, with verb-based embeddings providing the most accurate reflection of human similarity judgments. These findings highlight the importance of considering semantic structures in understanding how humans perceive and interpret social interactions in visual displays.<br /><br />Summary: <div>
arXiv:2509.20673v1 Announce Type: cross 
Abstract: Humans are social creatures who readily recognize various social interactions from simple display of moving shapes. While previous research has often focused on visual features, we examine what semantic representations that humans employ to complement visual features. In Study 1, we directly asked human participants to label the animations based on their impression of moving shapes. We found that human responses were distributed. In Study 2, we measured the representational geometry of 27 social interactions through human similarity judgments and compared it with model predictions based on visual features, labels, and semantic embeddings from animation descriptions. We found that semantic models provided complementary information to visual features in explaining human judgments. Among the semantic models, verb-based embeddings extracted from descriptions account for human similarity judgments the best. These results suggest that social perception in simple displays reflects the semantic structure of social interactions, bridging visual and abstract representations.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting</title>
<link>https://arxiv.org/abs/2509.20852</link>
<guid>https://arxiv.org/abs/2509.20852</guid>
<content:encoded><![CDATA[
<div> Keywords: newborns, artificial intelligence, fetal heart rate monitoring, missing data, autoencoder <br />
Summary: <br />
Approximately 10% of newborns require assistance with breathing at birth, with fetal heart rate monitoring being crucial in assessing fetal well-being. Wearable FHR monitors have enabled continuous monitoring but face challenges due to signal dropouts from sensor displacement. Traditional methods like interpolation fail to preserve signal characteristics. This study proposes a masked transformer-based autoencoder to reconstruct missing FHR signals, capturing spatial and frequency components for signal inpainting and forecasting. The method shows robustness across varying durations of missing data and offers potential for AI-based risk algorithms. Integration into wearable monitors could enhance early and reliable risk detection. <div>
arXiv:2509.20852v1 Announce Type: cross 
Abstract: Approximately 10\% of newborns require assistance to initiate breathing at birth, and around 5\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem</title>
<link>https://arxiv.org/abs/2509.21039</link>
<guid>https://arxiv.org/abs/2509.21039</guid>
<content:encoded><![CDATA[
<div> MLIR, Mojo, GPU programming, scientific computing, Python interoperability <br />
Summary:
The article explores the performance and portability of Mojo, a novel language for scientific computing on GPUs. Mojo, based on LLVM's MLIR compiler infrastructure, aims to bridge gaps in performance and productivity by combining Python interoperability with CUDA-like syntax for GPU programming. Four scientific workloads were targeted for evaluation against vendor baselines on NVIDIA and AMD GPUs. Mojo's performance was competitive with CUDA and HIP for memory-bound kernels on both GPU platforms. However, discrepancies were observed on AMD GPUs for atomic operations and fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Despite low-level programming requirements and learning curve, Mojo shows promise in bridging gaps in the Python ecosystem for scientific computing and AI convergence. <div>
arXiv:2509.21039v1 Announce Type: cross 
Abstract: We explore the performance and portability of the novel Mojo language for scientific computing workloads on GPUs. As the first language based on the LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure, Mojo aims to close performance and productivity gaps by combining Python's interoperability and CUDA-like syntax for compile-time portable GPU programming. We target four scientific workloads: a seven-point stencil (memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and Hartree-Fock (compute-bound with atomic operations); and compare their performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We show that Mojo's performance is competitive with CUDA and HIP for memory-bound kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve and programming requirements are still fairly low-level, Mojo can close significant gaps in the fragmented Python ecosystem in the convergence of scientific computing and AI.
]]></content:encoded>
<pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust AI-ECG for Predicting Left Ventricular Systolic Dysfunction in Pediatric Congenital Heart Disease</title>
<link>https://arxiv.org/abs/2509.19564</link>
<guid>https://arxiv.org/abs/2509.19564</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, electrocardiogram, pediatric, left ventricular systolic dysfunction, low-resource<br />
Summary:<br />
- The study focuses on enhancing AI-ECG performance for detecting left ventricular systolic dysfunction in pediatric patients with congenital heart disease.
- Current AI-ECG methods rely on large datasets, limiting their practical application in hospitals with limited pediatric ECG data.
- The proposed training framework introduces an on-manifold adversarial perturbation strategy to generate synthetic noise samples that mimic real-world signal variations in pediatric ECGs.
- An uncertainty-aware adversarial training algorithm, independent of architecture, is developed to improve model robustness.
- Evaluation on a real-world pediatric dataset demonstrates the framework's effectiveness in enabling cost-effective and reliable detection of left ventricular systolic dysfunction, making it suitable for deployment in resource-limited clinical settings.<br /> 
Summary: <div>
arXiv:2509.19564v1 Announce Type: new 
Abstract: Artificial intelligence-enhanced electrocardiogram (AI-ECG) has shown promise as an inexpensive, ubiquitous, and non-invasive screening tool to detect left ventricular systolic dysfunction in pediatric congenital heart disease. However, current approaches rely heavily on large-scale labeled datasets, which poses a major obstacle to the democratization of AI in hospitals where only limited pediatric ECG data are available. In this work, we propose a robust training framework to improve AI-ECG performance under low-resource conditions. Specifically, we introduce an on-manifold adversarial perturbation strategy for pediatric ECGs to generate synthetic noise samples that better reflect real-world signal variations. Building on this, we develop an uncertainty-aware adversarial training algorithm that is architecture-agnostic and enhances model robustness. Evaluation on the real-world pediatric dataset demonstrates that our method enables low-cost and reliable detection of left ventricular systolic dysfunction, highlighting its potential for deployment in resource-limited clinical settings.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Language Models with Modality-Specific Experts for Financial Forecasting from Interleaved Sequences of Text and Time Series</title>
<link>https://arxiv.org/abs/2509.19628</link>
<guid>https://arxiv.org/abs/2509.19628</guid>
<content:encoded><![CDATA[
<div> Keywords: financial forecasting, neural architecture, multimodal understanding, cross-modal alignment, interpretability<br />
<br />
Summary: 
The article proposes a neural architecture that integrates text and time series data for financial forecasting. The model utilizes modality-specific experts to capture unique patterns in each modality while allowing for joint reasoning. It also introduces a cross-modal alignment framework with a token weighting mechanism to align representations across modalities. The approach achieves state-of-the-art performance compared to baselines and incorporates interpretability methods to understand the value of different modalities. Economic gains in investment simulations validate the effectiveness of the proposed method. <div>
arXiv:2509.19628v1 Announce Type: new 
Abstract: Text and time series data offer complementary views of financial markets: news articles provide narrative context about company events, while stock prices reflect how markets react to those events. However, despite their complementary nature, effectively integrating these interleaved modalities for improved forecasting remains challenging. In this work, we propose a unified neural architecture that models these interleaved sequences using modality-specific experts, allowing the model to learn unique time series patterns, while still enabling joint reasoning across modalities and preserving pretrained language understanding capabilities. To further improve multimodal understanding, we introduce a cross-modal alignment framework with a salient token weighting mechanism that learns to align representations across modalities with a focus on the most informative tokens. We demonstrate the effectiveness of our approach on a large-scale financial forecasting task, achieving state-of-the-art performance across a wide variety of strong unimodal and multimodal baselines. We develop an interpretability method that reveals insights into the value of time series-context and reinforces the design of our cross-modal alignment objective. Finally, we demonstrate that these improvements translate to meaningful economic gains in investment simulations.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing failure morphologies in fiber-reinforced composites via k-means clustering based multiscale framework</title>
<link>https://arxiv.org/abs/2509.20011</link>
<guid>https://arxiv.org/abs/2509.20011</guid>
<content:encoded><![CDATA[
<div> homogenization, composite materials, damage analysis, clustering methods, failure prediction 
Summary: 
A novel homogenization methodology is introduced for analyzing fiber-reinforced composite material failures. This method incorporates elastic and eigen influence tensors in a damage-informed transformation field analysis (D-TFA) framework, allowing for efficient and realistic simulations of damage under uniform stress and strain conditions. The use of a reduced-order modeling strategy improves computational efficiency, while clustering methods help partition the microscale domain for more accurate predictions. By simulating the response of a representative volume element (RVE) and comparing clustering schemes, the model's performance is evaluated. Damage morphologies are accurately captured and directional strengths predicted, with higher cluster counts proving essential for complex microstructures. The effectiveness of the proposed framework is demonstrated in open-hole specimen tests, accurately predicting failure paths for domains with different fiber layups. <div>
arXiv:2509.20011v1 Announce Type: new 
Abstract: A novel homogenization methodology is proposed for analyzing the failure of fiber-reinforced composite materials, utilizing elastic and eigen influence tensors within a damage informed transformation field analysis (D-TFA) framework. This approach includes a technique for calculating macroscopic damage under uniform stress and strain conditions, offering more realistic simulations. Computational efficiency is enhanced through a reduced-order modeling strategy, while elastic and eigen strain distribution driven k-means clustering methods are employed to partition the microscale domain. The model's performance is assessed by simulating the response of a representative volume element (RVE) treated as a homogenized continuum. Subsequently, a comparative assessment is carried out to check the efficacy of two clustering schemes. Damage morphologies are calculated using proposed framework and compared with predictions obtained using finite element method. Furthermore, open-hole specimen tests are simulated and failure paths are predicted for the domains with different fiber layups. Ultimately, we show that D-TFA can accurately capture damage patterns and directional strengths, providing improved predictions of the mechanical behavior of composite materials. It has been demonstrated that higher cluster counts are crucial for capturing a more accurate stress-strain response, especially for complex microstructures.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi-Objective Constrained Bayesian Optimization of Bridge Girder</title>
<link>https://arxiv.org/abs/2509.20161</link>
<guid>https://arxiv.org/abs/2509.20161</guid>
<content:encoded><![CDATA[
<div> Keywords: greenhouse gas emissions, structural design optimization, Bayesian optimization, constrained optimization, finite element simulations

Summary: 
The article discusses the significant contribution of the buildings and construction sector to greenhouse gas emissions, with a focus on reducing emissions through optimized structural design. It introduces a novel approach using multi-objective constrained Bayesian optimization to achieve this goal efficiently. By incorporating proper orthogonal decomposition and Kriging partial least squares, the method aims to reduce the computational expenses associated with finite element simulations in structural design. The constrained expected improvement function is used for Bayesian optimization, demonstrated through a case study of a concrete bridge girder design. The results show potential cost savings of approximately 10% to 15% in financial costs and 20% in environmental costs while ensuring structural integrity. This approach showcases the effectiveness of optimization techniques in achieving both economic and environmental benefits in the construction industry. 

<br /><br />Summary: <div>
arXiv:2509.20161v1 Announce Type: new 
Abstract: The buildings and construction sector is a significant source of greenhouse gas emissions, with cement production alone contributing 7~\% of global emissions and the industry as a whole accounting for approximately 37~\%. Reducing emissions by optimizing structural design can achieve significant global benefits. This article introduces an efficient multi-objective constrained Bayesian optimization approach to address this challenge. Rather than attempting to determine the full set of non-dominated solutions with arbitrary trade-offs, the approach searches for a solution matching a specified trade-off. Structural design is typically conducted using computationally expensive finite element simulations, whereas Bayesian optimization offers an efficient approach for optimizing problems that involve such high-cost simulations. The proposed method integrates proper orthogonal decomposition for dimensionality reduction of simulation results with Kriging partial least squares to enhance efficiency. Constrained expected improvement is used as an acquisition function for Bayesian optimization. The approach is demonstrated through a case study of a two-lane, three-span post-tensioned concrete bridge girder, incorporating fifteen design variables and nine constraints. A comparison with conventional design methods demonstrates the potential of this optimization approach to achieve substantial cost reductions, with savings of approximately 10\% to 15\% in financial costs and about 20\% in environmental costs for the case study, while ensuring structural integrity.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Overview of Meshfree Collocation Methods</title>
<link>https://arxiv.org/abs/2509.20056</link>
<guid>https://arxiv.org/abs/2509.20056</guid>
<content:encoded><![CDATA[
<div> Keywords: meshfree collocation methods, differential operators, unstructured point clouds, numerical approximation, computational grid. 

Summary: 
This article provides a comprehensive overview of meshfree collocation methods used in numerically approximating differential operators on unstructured point clouds. The methods discussed in the literature do not rely on a computational grid, instead approximating functions and derivatives at irregularly distributed collocation points. The historical development of key concepts is traced, and a classification of methods based on their principle of derivation is proposed. While some methods are similar, subtle differences exist and are highlighted. A unifying formulation of meshfree collocation methods is presented to elucidate these differences, showing how each method can be derived from this formulation. Additionally, a generalized derivation for future meshfree collocation methods is suggested. <div>
arXiv:2509.20056v1 Announce Type: cross 
Abstract: We provide a comprehensive overview of meshfree collocation methods for numerically approximating differential operators on continuously labeled unstructured point clouds. Meshfree collocation methods do not require a computational grid or mesh. Instead, they approximate smooth functions and their derivatives at potentially irregularly distributed collocation points, often called particles, to a desired order of consistency. We review several meshfree collocation methods from the literature, trace the historical development of key concepts, and propose a classification of methods according to their principle of derivation. Although some of the methods reviewed are similar or identical, there are subtle yet important differences between many, which we highlight and discuss. We present a unifying formulation of meshfree collocation methods that renders these differences apparent and show how each method can be derived from this formulation. Finally, we propose a generalized derivation for meshfree collocation methods going forward.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Multi-Species Bird Classification on Low-Power Bioacoustic Loggers</title>
<link>https://arxiv.org/abs/2509.20103</link>
<guid>https://arxiv.org/abs/2509.20103</guid>
<content:encoded><![CDATA[
<div> Neural network, bird audio classification, microcontrollers, biodiversity monitoring, energy-efficient <br />
<br />
Summary: 
This paper introduces WrenNet, a neural network designed for real-time multi-species bird audio classification on low-power microcontrollers to facilitate scalable biodiversity monitoring. WrenNet features a semi-learnable spectral feature extractor that outperforms traditional alternatives. Testing on a 70-species dataset, WrenNet achieves high accuracy rates, particularly for acoustically distinct species. It displays energy efficiency, consuming only 77mJ per inference on an AudioMoth device with limited RAM. In comparison to Birdnet, WrenNet proves over 16 times more energy-efficient when running on a Raspberry Pi 3B+. This research presents a practical solution for continuous, multi-species acoustic monitoring on low-power edge devices, offering promising applications for biodiversity conservation efforts. <br /> <div>
arXiv:2509.20103v1 Announce Type: cross 
Abstract: This paper introduces WrenNet, an efficient neural network enabling real-time multi-species bird audio classification on low-power microcontrollers for scalable biodiversity monitoring. We propose a semi-learnable spectral feature extractor that adapts to avian vocalizations, outperforming standard mel-scale and fully-learnable alternatives. On an expert-curated 70-species dataset, WrenNet achieves up to 90.8\% accuracy on acoustically distinctive species and 70.1\% on the full task. When deployed on an AudioMoth device ($\leq$1MB RAM), it consumes only 77mJ per inference. Moreover, the proposed model is over 16x more energy-efficient compared to Birdnet when running on a Raspberry Pi 3B+. This work demonstrates the first practical framework for continuous, multi-species acoustic monitoring on low-power edge devices.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Fusion for Full-Range Response Reconstruction via Diffusion Models</title>
<link>https://arxiv.org/abs/2502.00795</link>
<guid>https://arxiv.org/abs/2502.00795</guid>
<content:encoded><![CDATA[
<div> Diffusion models, structural health monitoring, data fusion framework, sparse sensor measurements, probabilistic constraints<br />
Summary:<br />
Accurate monitoring of structures is essential for safety, but limited sensor deployment can be a challenge. This paper introduces a novel data fusion framework that utilizes diffusion models to reconstruct full-range structural responses from sparse sensor measurements. The framework incorporates Diffusion Posterior Sampling (DPS) to guide the reconstruction process using sensor measurements as probabilistic constraints. Three forward models are developed to adapt to different sensor placement scenarios and reconstruction targets. Validation on a steel plate shear wall shows promising results, with Weighted Mean Absolute Percentage Errors ranging from 1.62% to 3.49%. Sensitivity analyses demonstrate robust performance under various conditions. This framework presents a new approach for probabilistic modeling in structural health monitoring, offering a data fusion solution for comprehensive structural monitoring. <br /> <div>
arXiv:2502.00795v2 Announce Type: replace 
Abstract: Accurately capturing the full-range response of structures is crucial in structural health monitoring (SHM) for ensuring safety and operational integrity. However, limited sensor deployment due to cost, accessibility, or scale often hinders comprehensive monitoring. This paper presents a generative data fusion framework utilizing diffusion models, to reconstruct the full-range structural response from sparse and heterogeneous sensor measurements. We incorporate Diffusion Posterior Sampling (DPS) into the reconstruction framework, using sensor measurements as probabilistic constraints to guide the sampling process. Three forward models are designed: Direct Observation Mapping (DOM), Channel-based Observation Mapping (COM), and Neural Network Forward Model (NNFM), enabling flexible adaptation to different sensor placement conditions and reconstruction targets. The proposed framework is validated on a steel plate shear wall exhibiting nonlinear responses. By simultaneously sampling 100 realizations and averaging them as the ensemble prediction result, the three forward models achieve Weighted Mean Absolute Percentage Errors of 1.62% (DOM), 3.27% (COM), and 3.49% (NNFM). Sensitivity analyses further demonstrate robust performance under varying hyperparameters, sensor configurations, and noise levels. The proposed framework shows new possibilities for probabilistic modeling and decision-making in SHM by harnessing the capabilities of diffusion models, offering a novel data fusion approach for full-range monitoring of structures.
]]></content:encoded>
<pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2D implementation of Kinetic-diffusion Monte Carlo in Eiron</title>
<link>https://arxiv.org/abs/2509.19140</link>
<guid>https://arxiv.org/abs/2509.19140</guid>
<content:encoded><![CDATA[
<div> Particle-based kinetic Monte Carlo simulations, neutral particles, computational bottlenecks, tokamak scrape-off layer simulations, high-collisional regimes<br />
Summary: 
The article introduces the Kinetic-diffusion Monte Carlo scheme for simulating neutral particles in high-collisional regimes, focusing on tokamak scrape-off layer simulations. By approximating high-collisional kinetic dynamics with diffusion in these regimes, computational costs are significantly reduced. The scheme's extension to the two-dimensional setting and implementation in the Eiron particle code are discussed. The results demonstrate a notable speedup in high-collisional cases compared to standard kinetic simulations. This advancement addresses the computational challenges associated with resolving individual collision events and showcases the potential of asymptotic-preserving schemes in improving the efficiency of particle-based simulations in tokamak environments. <div>
arXiv:2509.19140v1 Announce Type: new 
Abstract: Particle-based kinetic Monte Carlo simulations of neutral particles is one of the major computational bottlenecks in tokamak scrape-off layer simulations. This computational cost comes from the need to resolve individual collision events in high-collisional regimes. However, in such regimes, one can approximate the high-collisional kinetic dynamics with computationally cheaper diffusion. Asymptotic-preserving schemes make use of this limit to perform simulations in these regimes, without a blow-up in computational cost as incurred by standard kinetic approaches. One such scheme is Kinetic-diffusion Monte Carlo. In this paper, we present a first extension of this scheme to the two-dimensional setting and its implementation in the Eiron particle code. We then demonstrate that this implementation produces a significant speedup over kinetic simulations in high-collisional cases.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlloyInter: Visualising Alloy Mixture Interpolations in t-SNE Representations</title>
<link>https://arxiv.org/abs/2509.19202</link>
<guid>https://arxiv.org/abs/2509.19202</guid>
<content:encoded><![CDATA[
<div> keywords: AlloyInter, input mixtures, output parameters space, eXplainable Artificial Intelligence, interpolation

Summary: 
AlloyInter is a novel system proposed for joint exploration of input mixtures and output parameters space in the SciVis Contest 2025. The system utilizes an interpolation approach guided by eXplainable Artificial Intelligence (XAI) to allow users to discover input mixture ratios while specifying output parameter goals. The system leverages a learned model ensemble to iteratively adjust and improve towards a set goal. By incorporating robust XAI techniques and combining manifold learning with interpolation approaches, AlloyInter strengthens its capabilities for efficient exploration. The system aims to enhance user interaction and understanding by providing a platform for discovering optimal input mixtures based on desired output parameters. With a focus on improving usability and interpretability, AlloyInter offers a promising solution for navigating complex input-output relationships in scientific visualization tasks. <br /><br />Summary: <div>
arXiv:2509.19202v1 Announce Type: new 
Abstract: This entry description proposes AlloyInter, a novel system to enable joint exploration of input mixtures and output parameters space in the context of the SciVis Contest 2025. We propose an interpolation approach, guided by eXplainable Artificial Intelligence (XAI) based on a learned model ensemble that allows users to discover input mixture ratios by specifying output parameter goals that can be iteratively adjusted and improved towards a goal. We strengthen the capabilities of our system by building upon prior research within the robustness of XAI, as well as combining well-established techniques like manifold learning with interpolation approaches.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning</title>
<link>https://arxiv.org/abs/2509.18120</link>
<guid>https://arxiv.org/abs/2509.18120</guid>
<content:encoded><![CDATA[
<div> CoCross-silo federated learning, data privacy, economic competition, CoCoGen framework, generative AI, potential game theory  
Summary:  
CoCoGen is a framework designed to address the challenges of cross-silo federated learning in the presence of economic competition and statistical heterogeneity. It utilizes generative AI and potential game theory to optimize collaborative training among organizations while maximizing social welfare. The framework models competition and heterogeneity through learning performance and utility-based formulations, treating each training round as a weighted potential game. Experimental results on the Fashion-MNIST dataset demonstrate that CoCoGen outperforms baseline methods, showing how varying levels of heterogeneity and competition impact organizational behavior. By leveraging generative AI and game theory, CoCoGen enables organizations to engage in coopetitive-compatible data generation, fostering collaboration in a competitive landscape. <br /><br />Summary: <div>
arXiv:2509.18120v1 Announce Type: cross 
Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or banks) to collaboratively train artificial intelligence (AI) models while preserving data privacy by keeping data local. While prior work has primarily addressed statistical heterogeneity across organizations, a critical challenge arises from economic competition, where organizations may act as market rivals, making them hesitant to participate in joint training due to potential utility loss (i.e., reduced net benefit). Furthermore, the combined effects of statistical heterogeneity and inter-organizational competition on organizational behavior and system-wide social welfare remain underexplored. In this paper, we propose CoCoGen, a coopetitive-compatible data generation framework, leveraging generative AI (GenAI) and potential game theory to model, analyze, and optimize collaborative learning under heterogeneous and competitive settings. Specifically, CoCoGen characterizes competition and statistical heterogeneity through learning performance and utility-based formulations and models each training round as a weighted potential game. We then derive GenAI-based data generation strategies that maximize social welfare. Experimental results on the Fashion-MNIST dataset reveal how varying heterogeneity and competition levels affect organizational behavior and demonstrate that CoCoGen consistently outperforms baseline methods.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning</title>
<link>https://arxiv.org/abs/2509.18169</link>
<guid>https://arxiv.org/abs/2509.18169</guid>
<content:encoded><![CDATA[
<div> training, inference, neural networks, computation, reasoning

Summary:
- The article introduces the PiMoE (Physically-isolated Mixture of Experts) architecture, which integrates computation and reasoning capabilities into neural networks.
- PiMoE endogenously incorporates computational expertise within the network, allowing for iterative alternation within a single chain of thought.
- Evaluations against LLM finetuning and multi-agent system approaches show that PiMoE outperforms in accuracy and exhibits improvements in response latency, token usage, and GPU energy consumption.
- This architecture offers an efficient, interpretable, and scalable solution for next-generation intelligent systems in scientific or industrial applications. 

<br /><br />Summary: <div>
arXiv:2509.18169v1 Announce Type: cross 
Abstract: Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM</title>
<link>https://arxiv.org/abs/2509.18178</link>
<guid>https://arxiv.org/abs/2509.18178</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational Fluid Dynamics, Foam-Agent, automation, multi-agent framework, simulation

Summary:
Foam-Agent is a multi-agent framework designed to automate the entire OpenFOAM workflow, reducing the steep learning curve and manual setup challenges in Computational Fluid Dynamics (CFD) simulations. It offers end-to-end simulation automation, managing tasks from pre-processing to post-simulation visualization. The framework features a composable service architecture that allows for flexible integration with other systems, enhancing exploratory workflows. Foam-Agent ensures high-fidelity configuration generation through precise context retrieval and dependency-aware processes. Evaluation results show a high success rate in handling simulation tasks, outperforming existing frameworks. By democratizing complex scientific computing, Foam-Agent lowers the expertise barrier for CFD, making it more accessible to a wider range of users. The code for Foam-Agent is publicly available on GitHub for use and further development. 

<br /><br />Summary: <div>
arXiv:2509.18178v1 Announce Type: cross 
Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in engineering, yet its steep learning curve and complex manual setup create significant barriers. To address these challenges, we introduce Foam-Agent, a multi-agent framework that automates the entire end-to-end OpenFOAM workflow from a single natural language prompt. Our key innovations address critical gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation: Foam-Agent is the first system to manage the full simulation pipeline, including advanced pre-processing with a versatile Meshing Agent capable of handling external mesh files and generating new geometries via Gmsh, automatic generation of HPC submission scripts, and post-simulation visualization via ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent, the framework uses Model Context Protocol (MCP) to expose its core functions as discrete, callable tools. This allows for flexible integration and use by other agentic systems, such as Claude-code, for more exploratory workflows. 3. High-Fidelity Configuration Generation: We achieve superior accuracy through a Hierarchical Multi-Index RAG for precise context retrieval and a dependency-aware generation process that ensures configuration consistency. Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2% success rate with Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the expertise barrier for CFD, demonstrating how specialized multi-agent systems can democratize complex scientific computing. The code is public at https://github.com/csml-rpi/Foam-Agent.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market</title>
<link>https://arxiv.org/abs/2509.18820</link>
<guid>https://arxiv.org/abs/2509.18820</guid>
<content:encoded><![CDATA[
<div> cryptocurrency, correlation structures, fluctuation amplitudes, network structures, portfolio construction <br />
<br />
Summary: 
This study introduces a methodology for analyzing evolving correlation structures in complex systems based on cryptocurrency market dynamics. By using the $q$-dependent detrended cross-correlation coefficient $\rho(q,s)$, correlations at varying fluctuation amplitudes and time scales are captured. The approach utilizes $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Analysis of minute-by-minute exchange rate data for 140 cryptocurrencies on Binance from Jan 2021 to Oct 2024 reveals significant shifts in $q$MSTs, particularly during the Terra/Luna crash in April 2022. The network initially centered around Bitcoin (BTC) but later decentralized, with Ethereum (ETH) and other assets gaining prominence. Spectral analysis indicates decreasing dominance of BTC and increased diversification among assets. Medium-scale fluctuations exhibit stronger correlations than large-scale ones, influencing portfolio construction strategies. During crashes, major disruptions amplify correlation differences, leading to fully decentralized network structures. These findings demonstrate the effectiveness of $q$MSTs in uncovering fluctuation-dependent correlations and have potential applications in various complex systems beyond finance such as biology and social systems. <div>
arXiv:2509.18820v1 Announce Type: cross 
Abstract: Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A failure mode dependent continuum damage model for laminated composites with optimized model parameters : Application to curved beams</title>
<link>https://arxiv.org/abs/2509.19051</link>
<guid>https://arxiv.org/abs/2509.19051</guid>
<content:encoded><![CDATA[
<div> damage model, continuum damage modeling, laminated composite panels, optimization algorithm, finite element method

Summary: 
A new continuum damage model for laminated composite panels is proposed, utilizing polynomial-based damage hardening functions and characterized parameters based on experimental stress-strain curves. The model is optimized using a steepest descent algorithm to minimize differences between predicted and experimental data. Damage evolution equations are derived and applied to a curved beam model, showing non-linear behavior in load vs deflection curves and successfully capturing stiffness degradation and damage differences in tension and compression. The model is compared to existing models and found effective in representing material properties and non-linearity in composite behavior. The study showcases the model's ability to predict damage accurately and demonstrate thermodynamically consistent continuum damage modeling for laminated composite structures. <br /><br /> <div>
arXiv:2509.19051v1 Announce Type: cross 
Abstract: In this article, a failure mode dependent and thermodynamically consistent continuum damage model with polynomial-based damage hardening functions is proposed for continuum damage modeling of laminated composite panels. The damage model parameters are characterized based on all uniaxial/shear experimental stress-strain curves. Steepest descent optimization algorithm is used to minimize the difference between model predicted and experimental stress-strain curves to get the optimzed model parameters. The fully characterized damage evolution equations are used for damage prediction of a moderately thick laminated composite curved beam modeled using first-order shear deformation theory. Finite element method with load control is used to get the non-linear algebraic equations which are solved using Newton Raphson method. The developed model is compared with the existing failure mode dependent and failure mode independent damage models. The results depict the efficacy of the proposed model to capture non-linearity in the load vs deflection curve due to stiffness degradation and different damage in tension andcompression consistent with uniaxial/shear stress-strain response and strength properties of the material, respectively.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving Linear and Angular Momentum for Dynamical Systems</title>
<link>https://arxiv.org/abs/2501.07373</link>
<guid>https://arxiv.org/abs/2501.07373</guid>
<content:encoded><![CDATA[
<div> Graph Neural Network, Multi-body Dynamical Systems, Physics-Informed, Interpretable, Real-time <br />
<br />
Summary: <br />
The paper introduces Dynami-CAL GraphNet, a Physics-Informed Graph Neural Network designed for accurate and interpretable modeling of multi-body dynamical systems. By integrating physics-based inductive biases with the learning capabilities of GNNs, the model enforces pairwise conservation of linear and angular momentum for interacting nodes using edge-local reference frames. This design ensures physically consistent predictions of node dynamics and offers interpretable edge-wise linear and angular impulses resulting from pairwise interactions. Evaluated on a 3D granular system, Dynami-CAL GraphNet demonstrates stable error accumulation over extended rollouts, effective extrapolations to unseen configurations, and robust handling of heterogeneous interactions and external forces. The model offers significant advantages in various fields like robotics, aerospace engineering, and materials science by providing scalable, physically consistent predictions that adhere to fundamental conservation laws, enabling the inference of forces and moments while efficiently handling diverse interactions and external forces. <div>
arXiv:2501.07373v2 Announce Type: replace-cross 
Abstract: Accurate, interpretable, and real-time modeling of multi-body dynamical systems is essential for predicting behaviors and inferring physical properties in natural and engineered environments. Traditional physics-based models face scalability challenges and are computationally demanding, while data-driven approaches like Graph Neural Networks (GNNs) often lack physical consistency, interpretability, and generalization. In this paper, we propose Dynami-CAL GraphNet, a Physics-Informed Graph Neural Network that integrates the learning capabilities of GNNs with physics-based inductive biases to address these limitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and angular momentum for interacting nodes using edge-local reference frames that are equivariant to rotational symmetries, invariant to translations, and equivariant to node permutations. This design ensures physically consistent predictions of node dynamics while offering interpretable, edge-wise linear and angular impulses resulting from pairwise interactions. Evaluated on a 3D granular system with inelastic collisions, Dynami-CAL GraphNet demonstrates stable error accumulation over extended rollouts, effective extrapolations to unseen configurations, and robust handling of heterogeneous interactions and external forces. Dynami-CAL GraphNet offers significant advantages in fields requiring accurate, interpretable, and real-time modeling of complex multi-body dynamical systems, such as robotics, aerospace engineering, and materials science. By providing physically consistent and scalable predictions that adhere to fundamental conservation laws, it enables the inference of forces and moments while efficiently handling heterogeneous interactions and external forces.
]]></content:encoded>
<pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Engineering AGI: Benchmarking the Engineering Design Capabilities of LLMs</title>
<link>https://arxiv.org/abs/2509.16204</link>
<guid>https://arxiv.org/abs/2509.16204</guid>
<content:encoded><![CDATA[
<div> Engineering Design Benchmark, Large Language Models, Real-world tasks, Domain Knowledge, Simulation-based Evaluation<br />
<br />Summary:
Industry pioneers aim to develop general-purpose AI engineers for ambitious projects. Large language models face unique challenges in engineering design, requiring synthesis of domain knowledge, trade-off navigation, and process management. The ENGDESIGN benchmark evaluates LLMs across nine engineering domains, emphasizing design synthesis and objective-oriented solutions. Tasks simulate real-world engineering challenges with detailed descriptions of goals, constraints, and requirements. A simulation-based evaluation approach tests LLM-generated designs using domain-specific simulations. This benchmark fills a gap in existing benchmarks by focusing on design tasks rather than factual recall or question answering. Industry's vision of AI engineers shaping grand projects may become a reality with advancements in LLM capabilities through benchmarks like ENGDESIGN. <br /> <div>
arXiv:2509.16204v1 Announce Type: new 
Abstract: Today, industry pioneers dream of developing general-purpose AI engineers capable of designing and building humanity's most ambitious projects--from starships that will carry us to distant worlds to Dyson spheres that harness stellar energy. Yet engineering design represents a fundamentally different challenge for large language models (LLMs) compared to traditional textbook-style problem solving or factual question answering. Real-world engineering design demands the synthesis of domain knowledge, navigation of complex trade-offs, and management of the tedious processes that consume much of practicing engineers' time. Despite these shared challenges across engineering disciplines, no benchmark currently captures the unique demands of engineering design work. In this work, we introduce ENGDESIGN, an Engineering Design benchmark that evaluates LLMs' abilities to perform practical design tasks across nine engineering domains: Operating System Design, Computer Architecture Design, Control System Design, Mechanical Systems, Structural Design, Digital Hardware Design, Analog Integrated Circuit Design, Robotics, and Signal Processing. Unlike existing benchmarks that focus on factual recall or question answering, ENGDESIGN uniquely emphasizes LLMs' ability to synthesize domain knowledge, reason under constraints, and generate functional, objective-oriented designs. Each task in ENGDESIGN represents a real-world engineering design problem, accompanied by a detailed task description specifying design goals, constraints, and performance requirements. We pioneer a simulation-based evaluation paradigm where LLM-generated designs undergo rigorous testing through executable, domain-specific simulations-from circuit SPICE simulations to structural finite element analysis, from control system validation to robotic motion planning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning in Factor Investment</title>
<link>https://arxiv.org/abs/2509.16206</link>
<guid>https://arxiv.org/abs/2509.16206</guid>
<content:encoded><![CDATA[
<div> Factor Portfolio Construction, Deep Reinforcement Learning, Conditional Auto-encoded Factor-based Portfolio Optimisation, Performance Analysis, U.S. Equity Data

Summary: 
Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO) addresses the challenge of high-dimensional, unbalanced state space in factor portfolio construction by compressing stock-level returns into latent factors. Utilizing deep reinforcement learning (DRL) with PPO and DDPG, CAFPO outperforms traditional methods and other DRL approaches on 20 years of U.S. equity data. It achieves a 24.6% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis highlights the economically intuitive factor attributions. The study demonstrates that factor-aware representation learning enhances DRL for institutional, low-turnover portfolio management. <div>
arXiv:2509.16206v1 Announce Type: new 
Abstract: Deep reinforcement learning has shown promise in trade execution, yet its use in low-frequency factor portfolio construction remains under-explored. A key obstacle is the high-dimensional, unbalanced state space created by stocks that enter and exit the investable universe. We introduce Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO), which compresses stock-level returns into a small set of latent factors conditioned on 94 firm-specific characteristics. The factors feed a DRL agent implemented with both PPO and DDPG to generate continuous long-short weights. On 20 years of U.S. equity data (2000--2020), CAFPO outperforms equal-weight, value-weight, Markowitz, vanilla DRL, and Fama--French-driven DRL, delivering a 24.6\% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis further reveals economically intuitive factor attributions. Our results demonstrate that factor-aware representation learning can make DRL practical for institutional, low-turnover portfolio management.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Machine Learning to Increase the Throughput of Container Yards</title>
<link>https://arxiv.org/abs/2509.16207</link>
<guid>https://arxiv.org/abs/2509.16207</guid>
<content:encoded><![CDATA[
<div> Keywords: shipping container terminals, throughput rates, automation, Intelligent Planning System, Pareto Optimization 

Summary:
The study aims to enhance throughput rates for shipping container terminals, crucial for the U.S. economy. Despite the potential of automation, U.S. ports face limitations due to legal constraints on human labor. To address this, the paper presents an Intelligent Planning System (IPS) using Pareto Optimization and MILP. The IPS improves terminal throughput and processing times, offering recommendations for container positioning and truck pickup appointments. By optimizing container yard layout and flow, the IPS reduces real-time congestion and predicts future congestion. This innovative approach combines efficiency with cooperative agreement terms, providing a solution for U.S. ports seeking automation level efficiencies. The proposed IPS offers a valuable tool for enhancing operational efficiency and ensuring smoother container processing at shipping ports. 

<br /><br />Summary: <div>
arXiv:2509.16207v1 Announce Type: new 
Abstract: This study seeks to improve the throughput rates for shipping container terminals. In the United States, shipping ports link the domestic economy to global markets and are vital to sustain supply chain flow and economic stability. Maritime shipping accounts for nearly half of the U.S.'s annual international trade, two thirds of which are represented by container shipping. Previous studies highlighted the capability of automation in enhancing container processing; however, unlike in European and East Asian ports, full automation is limited in U.S. ports due to legal protections for human labor. Consequently, there is a need for alternative methods that deliver automation level efficiencies while maintaining the terms of cooperative agreements. This paper proposes an Intelligent Planning System (IPS) that applies the concept of Pareto Optimization to container yards through a mixed integer linear programming (MILP) based recursive appointment system. The results show an improvement from baseline for both daily terminal throughput volumes and processing times. The generated IPS can be employed to provide recommendations for container positioning and truck pickup appointments to optimize container yard layout and flow resulting in reduced realtime congestion and predictively mitigated future congestion.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesis of Service Life Prediction for Bridges in Texas</title>
<link>https://arxiv.org/abs/2509.16208</link>
<guid>https://arxiv.org/abs/2509.16208</guid>
<content:encoded><![CDATA[
<div> Bridge, design-build, service life, durability, prediction

Summary:
This research highlights the lack of standardized methods for achieving and verifying long-term service life requirements in design-build bridge contracts. It emphasizes the importance of accurately estimating remaining service life to prioritize repair and rehabilitation needs in aging bridges with limited financial resources. The study reviews current practices and recent advancements in bridge service life prediction, providing practical guidance for evaluating and extending the lifespan of both existing and new structures. By incorporating quantitative validation of durability practices, this research aims to support more efficient use of maintenance funds, enhance understanding of deterioration models and inspection methods, and inform strategies for ensuring long-term structural performance.<br /><br />Summary: <div>
arXiv:2509.16208v1 Announce Type: new 
Abstract: Design-build bridge contracts often include long-term service life requirements, but there are no clear technical guidelines or standardized methods to achieve or verify these goals. While durability practices are commonly applied, they lack quantitative validation. With many aging bridges and limited financial resources, accurately estimating remaining service life is essential for prioritizing repair and rehabilitation needs. This research reviews current practices and recent advancements in bridge service life prediction, providing practical guidance for evaluating and extending the lifespan of both existing and new structures. The findings support more efficient use of maintenance funds, better understanding of deterioration models and inspection methods, and informed strategies to ensure long-term structural performance.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Digital Twin Models</title>
<link>https://arxiv.org/abs/2509.16209</link>
<guid>https://arxiv.org/abs/2509.16209</guid>
<content:encoded><![CDATA[
<div> scale, digital twin, machine learning, dimensional analysis, calibration

Summary: 
The paper introduces a novel methodology and computational tool for scaling digital twin models using machine learning and dimensional analysis. This framework aims to address the complexity and barriers faced in developing accurate digital twin models for large-scale systems. By applying scaling techniques, the need for repetitive physical calibration of models in industries with varying product sizes can be eliminated. The methodology allows for transferring calibration data from smaller units to larger units within the same product line, without the need for additional data collection or experimentation. The paper also discusses challenges related to dimensional analysis for scaling and presents a framework for proper scaling of digital twin models. The results of the study demonstrate successful scaling between an industrial-size wheel loader vehicle and a miniaturized system in a laboratory setting. <div>
arXiv:2509.16209v1 Announce Type: new 
Abstract: In many industries, the scale and complexity of systems can present significant barriers to the development of accurate digital twin models. This paper introduces a novel methodology and a modular computational tool utilizing machine learning and dimensional analysis to establish a framework for scaling digital twin models. Scaling techniques have not yet been applied to digital twin technology, but they can eliminate the need for repetitive physical calibration of such models in industries where product lines include a variety of sizes of the same or similar products. In many cases, it may be easier or more cost-effective to perform physical calibration of the digital twin model on smaller units of a product line. Scaling techniques can then allow adapting the calibration data from the smaller units to other sizes of the product line without the need for additional data collection and experimentation for calibration. Conventional application of dimensional analysis for scaling in this context introduces several challenges due to distortion of scaling factors. This paper addresses these challenges and introduces a framework for proper scaling of digital twin models. The results are applied to scaling the models between an industrial-size wheel loader vehicle used in construction to a miniaturized system instrumented in a laboratory setting.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strain localization in reduced order asymptotic homogenization</title>
<link>https://arxiv.org/abs/2509.16210</link>
<guid>https://arxiv.org/abs/2509.16210</guid>
<content:encoded><![CDATA[
<div> damage, inelastic effects, multiscale technique, homogenization, composite materials

Summary:
- A new multiscale technique is proposed for capturing damage and inelastic effects in composite materials.
- The technique combines two-scale homogenization with eigen strain representation to model inelastic response. 
- Computational efficiency is improved through reduced order techniques.
- Macroscale stress is determined using influence tensors from representative volume element analysis. 
- Microscale damage is modeled using continuum damage mechanics with a method to prevent strain localization.
- Strategies are implemented to address spurious post-failure artificial stiffness at the macroscale.
- Verification studies show that the formulation accurately predicts macroscale response while capturing damage and inelastic strains. 

<br /><br />Summary: <div>
arXiv:2509.16210v1 Announce Type: new 
Abstract: A reduced order asymptotic homogenization based multiscale technique which can capture damage and inelastic effects in composite materials is proposed. This technique is based on two scale homogenization procedure where eigen strain representation accounts for the inelastic response and the computational efforts are alleviated by reduction of order technique. Macroscale stress is derived by calculating the influence tensors from the analysis of representative volume element (RVE). At microscale, the damage in the material is modeled using continuum damage mechanics (CDM) based framework. To solve the problem of strain localization a method of the alteration of stress-strain relation of micro con- stituents based on the dissipated fracture energy in a crack band is implemented. The issue of spurious post failure artificial stiffness at macroscale is discussed and effect of increasing the order to alleviate this problem is checked. Verification studies demonstrated the proposed formulation predicts the macroscale response and also captures the damage and plasticity induced inelastic strains.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E$^2$-TFA based multiscale analysis of failure in elasto-plastic composites</title>
<link>https://arxiv.org/abs/2509.16211</link>
<guid>https://arxiv.org/abs/2509.16211</guid>
<content:encoded><![CDATA[
<div> homogenization, elastoplastic composite materials, eigenstrain field, transformation field analysis, computational efficiency
Summary: 
The paper presents a novel homogenization methodology, $\mathtt{E}^2$-TFA, for analyzing the failure of elastoplastic composite materials. This technique incorporates the microscopic eigenstrain field to account for intra-phase damage and inelastic strains, overcoming the post-damage stiffness response limitation of traditional TFA-based methods. By utilizing elastic and eigen transformation functions, the model achieves computational efficiency and employs a reduced order modeling approach with a piecewise constant eigenstrain field. The performance of the model is evaluated through simulations on representative volume elements and various composites under complex load histories. The model accurately predicts the nonlinear shear stress-strain response of a glass fiber composite and compared well with experimental data on fracture initiation parameters, failure plane orientation, and strain histories. Overall, $\mathtt{E}^2$-TFA is shown to effectively capture damage and inelastic deformations, providing a more accurate estimation of the mechanical response in composite materials. <div>
arXiv:2509.16211v1 Announce Type: new 
Abstract: This paper describes a novel homogenization methodology for analyzing the failure of elastoplastic composite materials based on elastic and eigen influence tensors-driven transformation field analysis ($\mathtt{E}^2$-TFA). The proposed technique considers the microscopic eigenstrain field accounting for intra-phase damage and inelastic strains. This results in realistic computations by alleviating the post-damage stiffness response, which is a drawback of TFA-based methods. We attain computational efficiency by identifying the preprocessing data solely from the elastic and eigen transformation functions and adopting a reduced order modelling technique with a piecewise constant eigenstrain field throughout the subdomains. The performance of the model is assessed by simulating the response for (a) the representative volume element (RVE) as a homogenized continuum and (b) the various composites under complex load histories with intricate macroscale morphologies. Furthermore, the nonlinear shear stress-strain response of a glass fiber composite is calculated and compared to experimentally measured fracture initiation parameters, failure plane orientation, and strain histories. Finally, we show that $\mathtt{E}^2$-TFA can accurately and efficiently capture damage and inelastic deformations in order to estimate the mechanical response of composite materials in a better way.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An efficient framework for computing sensitivity of modal-related structural dynamic characteristics with multi-parameters</title>
<link>https://arxiv.org/abs/2509.16214</link>
<guid>https://arxiv.org/abs/2509.16214</guid>
<content:encoded><![CDATA[
<div> Keywords: structural dynamics, eigenmode sensitivity, sensitivity analysis, iterative method, computational efficiency 

Summary:
In this paper, a novel strategy is presented for computing the sensitivity of structural dynamic characteristics related to eigenmodes with multiple parameters. The method involves an algebraic approach to simplify the computation of eigenvector sensitivity, followed by the development of a framework for sensitivity analysis. By incorporating a preconditioning iterative method, the computational efficiency is enhanced, reducing the CPU computational time. The framework is user-friendly and minimizes the "Fill-in" operations of sparse matrices. Three numerical examples demonstrate the effectiveness of the algorithm in reducing computational time. This novel strategy provides a valuable tool for engineers in various fields to analyze and optimize structural dynamic characteristics related to eigenmodes efficiently. 

<br /><br />Summary: <div>
arXiv:2509.16214v1 Announce Type: new 
Abstract: The sensitivity of structural dynamic characteristics related to eigenmode (such as modal assurance criteria, modal flexibility, and modal mass etc.) has become a crucial and widely applied tool across various engineering fields. In this paper, a novel strategy is proposed for solving the sensitivity of structural dynamic characteristics related to eigenmode with respect to multiple variables. First, an algebraic method for computing the sensitivity of eigenvectors is developed to simplify the expression for sensitivity calculations. Subsequently, based on this new expression for eigenmode sensitivity, a framework for sensitivity analysis of structural dynamic characteristics related to eigenmodes with multiple parameters is established. With the incorporation of a preconditioning iterative method, the new computational framework effectively enhances the computational efficiency of sensitivity analysis for structural characteristics related to eigenmodes with multiple parameters. This framework is easy to operate and effectively reduces the "Fill-in" operations of sparse matrices. Three numerical examples are given to illustrate the effectiveness of the algorithm. The result shows that the novel strategy can significantly reduce central processing unit (CPU) computational time.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Detection of Internal Defects in Structured Media</title>
<link>https://arxiv.org/abs/2509.16216</link>
<guid>https://arxiv.org/abs/2509.16216</guid>
<content:encoded><![CDATA[
<div> Keywords: structural integrity, defects, imaging strategy, Laplace transform, optimization<br />
Summary:<br />
- Engineers face challenges in detecting internal fractures in structures using traditional methods like visual and audible aids.<br />
- This research proposes a strategy to image defects in structures using minimal, non-invasive measurements based on a one-dimensional wave equation model.<br />
- The study focuses on a homogeneous bar with a defect in Young's modulus, treating the problem as a spring-mass vibrational system.<br />
- The proposed imaging strategy utilizes MATLAB to collect synthetic data for various scenarios with defects, optimizing a residual function to identify defect location and stiffness.<br />
- By establishing an analytic map between defect characteristics and measurement data, engineers can effectively detect and assess the severity of fractures in structures. <br /> <div>
arXiv:2509.16216v1 Announce Type: new 
Abstract: A critical issue that affects engineers trying to assess the structural integrity of various infrastructures, such as metal rods or acoustic ducts, is the challenge of detecting internal fractures (defects). Traditionally, engineers depend on audible and visual aids to identify these fractures, as they do not physically dissect the object in question into multiple pieces to check for inconsistencies. This research introduces ideas towards the development of a robust strategy to image such defects using only a small set of minimal, non-invasive measurements.
  Assuming a one dimensional model (e.g. longitudinal waves in long and thin rods/acoustic ducts or transverse vibrations of strings), we make use of the continuous one-dimensional wave equation to model these physical phenomena and then employ specialized mathematical analysis tools (the Laplace transform and optimization) to introduce our defect imaging ideas. In particular, we will focus on the case of a long bar which is homogeneous throughout except in a small area where a defect in its Young's modulus is present. We will first demonstrate how the problem is equivalent to a spring-mass vibrational system, and then show how our imaging strategy makes use of the Laplace domain analytic map between the characteristics of the respective defect and the measurement data.
  More explicitly, we will utilize MATLAB (a platform for numerical computations) to collect synthetic data (computational alternative to real world measurements) for several scenarios with one defect of arbitrary location and stiffness. Subsequently, we will use this data along with our analytically developed map (between defect characteristics and measurements) to construct a residual function which, once optimized, will reveal the location and magnitude of the stiffness defect.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Transient Nonlinear Circuit Simulator Using Exponential Integration and Block-Jacobi Precondition</title>
<link>https://arxiv.org/abs/2509.16219</link>
<guid>https://arxiv.org/abs/2509.16219</guid>
<content:encoded><![CDATA[
<div> regularization, exponential integrators, matrix exponential vector products, structured block-Jacobi preconditioner, Additive Schwarz strategy <br />
Summary:
The article introduces a novel method for transient simulation of linear and nonlinear circuits in EDA tools. By applying a generalized row-echelon regularization approach, the method extends the use of exponential integrators to a wider range of differential algebraic equations, enabling larger time step sizes while maintaining accuracy and stability. To improve efficiency, a structured block-Jacobi preconditioner is designed for linear systems, and an Additive Schwarz strategy is employed for locally coupled circuits. Numerical experiments demonstrate significant speedup in computation time and reduced time steps compared to traditional methods, showcasing the potential for scalable and efficient nonlinear circuit simulation. <br /><br /> <div>
arXiv:2509.16219v1 Announce Type: new 
Abstract: Transient simulation of linear and nonlinear circuits remains an important task in modern EDA tools. At present, SPICE-like simulators face challenges in parallelization, nonlinear convergence and linear efficiency, especially when applied to large-scale circuits. To address the limitations of simulators in handling various nonlinear circuits, we adopt a generalized row-echelon regularization approach, which extends the applicability of exponential integrators to a broader class of differential algebraic equations. The proposed method employs matrix exponential vector products to integrate the regularized system, allowing for a larger time step size while preserving accuracy and stability. Furthermore, in order to accelerate GMRES-based solvers within Newton-Raphson iterations, a structured block-Jacobi preconditioner is designed for linear systems. For locally coupled circuits, Additive Schwarz overlapping strategy is adopted to enhance the solution performance. Numerical experiments of various nonlinear circuit models show that under same hardware environment, our method achieves a speedup of 1.95$\times$-- 3.27$\times$ in total computation time compared to Backward Euler with Inexact Newton iterations, and time steps have decreased by an average of 60.70\% (up to 74.59\%). Compared with EI-NK method, total computing time of our method has a speedup of 1.08$\times$-- 1.79$\times$. These results highlight the potential of proposed method for scalable and nonlinear circuit simulation.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Open Dataset for Temperature Modelling in Machine Tools</title>
<link>https://arxiv.org/abs/2509.16222</link>
<guid>https://arxiv.org/abs/2509.16222</guid>
<content:encoded><![CDATA[
<div> dataset, transient thermal simulations, machine learning, deep learning, mechanical systems

Summary:
This article introduces a structured dataset of transient thermal simulations for a vertical axis of a machine tool test rig. The dataset includes temperature and heat flux values recorded at 29 probe locations over 1800 time steps. The simulations were derived from a fractional factorial design and aim to support machine learning and deep learning applications in thermal modelling. The dataset provides detailed information on material, mesh, and boundary conditions, as well as summary statistics, thermal evolution plots, and correlation matrix analyses. A reproducible Jupyter notebook is also included to support research and model development. The data is designed to help predict, correct, and compensate for thermally induced deviations in mechanical systems, making it valuable for researchers without finite element expertise. <div>
arXiv:2509.16222v1 Announce Type: new 
Abstract: This data set descriptor introduces a structured, high-resolution dataset of transient thermal simulations for a vertical axis of a machine tool test rig. The data set includes temperature and heat flux values recorded at 29 probe locations at 1800 time steps, sampled every second over a 30-minute range, across 17 simulation runs derived from a fractional factorial design. First, a computer-aided design model was de-featured, segmented, and optimized, followed by finite element (FE) modelling. Detailed information on material, mesh, and boundary conditions is included. To support research and model development, the dataset provides summary statistics, thermal evolution plots, correlation matrix analyses, and a reproducible Jupyter notebook. The data set is designed to support machine learning and deep learning applications in thermal modelling for prediction, correction, and compensation of thermally induced deviations in mechanical systems, and aims to support researchers without FE expertise by providing ready-to-use simulation data.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Increasing Inter-Fiber Contact in the Altendorf-Jeulin Model</title>
<link>https://arxiv.org/abs/2509.16225</link>
<guid>https://arxiv.org/abs/2509.16225</guid>
<content:encoded><![CDATA[
<div> fiber materials, simulations, digital twins, inter-fiber contacts, parametric models

Summary:
- Fiber materials are crucial in fields like material design and biomedicine.
- Fiber simulations, known as digital twins, help in testing material behavior digitally.
- Inter-fiber contacts can impact the thermal and mechanical behavior of fiber systems.
- Existing parametric fiber models do not allow explicit modeling of the number of inter-fiber contacts.
- The proposed extension of the force-biased fiber packing model by Altendorf & Jeulin includes explicit modeling of inter-fiber contacts and an additional force to increase contacts.
- The packing is validated for parameter accuracy and shown to increase the number of contacts, potentially enhancing the accuracy of physical simulations. 

<br /><br />Summary: <div>
arXiv:2509.16225v1 Announce Type: new 
Abstract: In fields such as material design or biomedicine, fiber materials play an important role. Fiber simulations, also called digital twins, provide a basis for testing and optimizing the material's physical behavior digitally. Inter-fiber contacts can influence the thermal and mechanical behavior of a fiber system; to our knowledge, however, there exist no parametric fiber models allowing for explicit modeling of the number of inter-fiber contacts. Therefore, this paper proposes an extension of the iterative force-biased fiber packing by Altendorf \& Jeulin. In this extension, we model the inter-fiber contacts explicitly and add another force to the force-biased packing to increase the number of contacts. We successfully validate the packing with respect to its parameter accuracy. Moreover, we show that the extension indeed increases the number of contacts, even exceeding theoretical values. Hence, this packing scheme has the potential to achieve higher accuracy in physical simulations.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Rank Risky Investors: A Case Study of Predicting Retail Traders' Behaviour and Profitability</title>
<link>https://arxiv.org/abs/2509.16616</link>
<guid>https://arxiv.org/abs/2509.16616</guid>
<content:encoded><![CDATA[
<div> algorithm, financial markets, risk management, trader identification, profit-aware

Summary:
The article introduces a profit-aware risk ranker (PA-RiskRanker) for identifying risky traders in financial markets. Traditional methods often struggle to capture the complexity and dynamism of individual trader behaviors. PA-RiskRanker reframes the problem as a ranking task using Learning-to-Rank algorithms, incorporating profit and loss considerations through a Profit-Aware binary cross entropy loss function. It utilizes a transformer-based ranker with self-cross-trader attention to capture intra- and inter-trader relationships. The research highlights the limitations of existing deep learning-based algorithms in trading risk management and emphasizes the importance of profit and loss in financial scenarios. PA-RiskRanker outperforms state-of-the-art ranking models like Rankformer, showing an 8.4% increase in F1 score and a 10%-17% increase in average profit compared to benchmark models. <div>
arXiv:2509.16616v1 Announce Type: new 
Abstract: Identifying risky traders with high profits in financial markets is crucial for market makers, such as trading exchanges, to ensure effective risk management through real-time decisions on regulation compliance and hedging. However, capturing the complex and dynamic behaviours of individual traders poses significant challenges. Traditional classification and anomaly detection methods often establish a fixed risk boundary, failing to account for this complexity and dynamism. To tackle this issue, we propose a profit-aware risk ranker (PA-RiskRanker) that reframes the problem of identifying risky traders as a ranking task using Learning-to-Rank (LETOR) algorithms. Our approach features a Profit-Aware binary cross entropy (PA-BCE) loss function and a transformer-based ranker enhanced with a self-cross-trader attention pipeline. These components effectively integrate profit and loss (P&amp;L) considerations into the training process while capturing intra- and inter-trader relationships. Our research critically examines the limitations of existing deep learning-based LETOR algorithms in trading risk management, which often overlook the importance of P&amp;L in financial scenarios. By prioritising P&amp;L, our method improves risky trader identification, achieving an 8.4% increase in F1 score compared to state-of-the-art (SOTA) ranking models like Rankformer. Additionally, it demonstrates a 10%-17% increase in average profit compared to all benchmark models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rational Multi-Modal Transformers for TCR-pMHC Prediction</title>
<link>https://arxiv.org/abs/2509.17305</link>
<guid>https://arxiv.org/abs/2509.17305</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, TCR-pMHC interactions, explainability method, encoder-decoder, deep learning <br />
<br />Summary: 
This article introduces a novel approach using a transformer-based model to predict T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes. By employing a new explainability method, the model optimizes cross-attention strategies, incorporates additional training objectives, and introduces an early-stopping criterion based on explanation quality. This framework achieves improved predictive performance while enhancing explainability, robustness, and generalization. The approach establishes a principled, explanation-driven strategy for modeling TCR-pMHC binding and offers insights into sequence-level binding behavior through deep learning techniques. <div>
arXiv:2509.17305v1 Announce Type: new 
Abstract: T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is fundamental to adaptive immunity and central to the development of T cell-based immunotherapies. While transformer-based models have shown promise in predicting TCR-pMHC interactions, most lack a systematic and explainable approach to architecture design. We present an approach that uses a new post-hoc explainability method to inform the construction of a novel encoder-decoder transformer model. By identifying the most informative combinations of TCR and epitope sequence inputs, we optimize cross-attention strategies, incorporate auxiliary training objectives, and introduce a novel early-stopping criterion based on explanation quality. Our framework achieves state-of-the-art predictive performance while simultaneously improving explainability, robustness, and generalization. This work establishes a principled, explanation-driven strategy for modeling TCR-pMHC binding and offers mechanistic insights into sequence-level binding behavior through the lens of deep learning.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$i$MIND: Insightful Multi-subject Invariant Neural Decoding</title>
<link>https://arxiv.org/abs/2509.17313</link>
<guid>https://arxiv.org/abs/2509.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: visual signals, neural decoding, brain activity, semantic decoding, neural interpretation

Summary: 
The study introduces the iMIND model, focusing on decoding visual signals to understand brain cognition and perception. It utilizes a dual-decoding framework that combines biometric and semantic decoding to offer neural interpretability and deepen insights into visual processing mechanisms. The iMIND model consists of three key steps: creating a shared neural representation space across subjects, disentangling neural features into subject-specific and object-specific components, and performing dual decoding for biometric and semantic tasks. Experimental results show that iMIND achieves top decoding performance with minimal scalability constraints. It generates voxel-object activation fingerprints that uncover object-specific neural patterns and allow for the exploration of subject-specific attention variations to the same stimuli. This work lays the groundwork for more interpretable and generalizable subject-invariant neural decoding, enhancing knowledge of voxel semantic selectivity and neural vision processing dynamics. 

<br /><br />Summary: <div>
arXiv:2509.17313v1 Announce Type: new 
Abstract: Decoding visual signals holds the tantalizing potential to unravel the complexities of cognition and perception. While recent studies have focused on reconstructing visual stimuli from neural recordings to bridge brain activity with visual imagery, existing methods offer limited insights into the underlying mechanisms of visual processing in the brain. To mitigate this gap, we present an \textit{i}nsightful \textbf{M}ulti-subject \textbf{I}nvariant \textbf{N}eural \textbf{D}ecoding ($i$MIND) model, which employs a novel dual-decoding framework--both biometric and semantic decoding--to offer neural interpretability in a data-driven manner and deepen our understanding of brain-based visual functionalities. Our $i$MIND model operates through three core steps: establishing a shared neural representation space across subjects using a ViT-based masked autoencoder, disentangling neural features into complementary subject-specific and object-specific components, and performing dual decoding to support both biometric and semantic classification tasks. Experimental results demonstrate that $i$MIND achieves state-of-the-art decoding performance with minimal scalability limitations. Furthermore, $i$MIND empirically generates voxel-object activation fingerprints that reveal object-specific neural patterns and enable investigation of subject-specific variations in attention to identical stimuli. These findings provide a foundation for more interpretable and generalizable subject-invariant neural decoding, advancing our understanding of the voxel semantic selectivity as well as the neural vision processing dynamics.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction</title>
<link>https://arxiv.org/abs/2509.16369</link>
<guid>https://arxiv.org/abs/2509.16369</guid>
<content:encoded><![CDATA[
<div> Keywords: financial, retrieval, AI, Multi-HyDE, accuracy

Summary:
The article introduces a framework for financial Retrieval Augmented Generation (RAG) that utilizes agentic AI and the Multi-HyDE system to enhance knowledge retrieval in financial question-answering. Traditional systems are insufficient for handling the complexity of financial data sources, which require sophisticated approaches. The RAG framework focuses on token efficiency and multi-step financial reasoning to improve accuracy by 11.2% and reduce hallucinations by 15%. By integrating domain-specific retrieval mechanisms like Multi-HyDE and robust toolsets, including keyword and table-based retrieval, the framework significantly enhances the accuracy and reliability of answers in financial QA benchmarks. The research emphasizes the importance of structured agent workflows and multi-perspective retrieval for the trustworthy deployment of AI in high-stakes financial applications.

<br /><br />Summary: <div>
arXiv:2509.16369v1 Announce Type: cross 
Abstract: Accurate and reliable knowledge retrieval is vital for financial question-answering, where continually updated data sources and complex, high-stakes contexts demand precision. Traditional retrieval systems rely on a single database and retriever, but financial applications require more sophisticated approaches to handle intricate regulatory filings, market analyses, and extensive multi-year reports. We introduce a framework for financial Retrieval Augmented Generation (RAG) that leverages agentic AI and the Multi-HyDE system, an approach that generates multiple, nonequivalent queries to boost the effectiveness and coverage of retrieval from large, structured financial corpora. Our pipeline is optimized for token efficiency and multi-step financial reasoning, and we demonstrate that their combination improves accuracy by 11.2% and reduces hallucinations by 15%. Our method is evaluated on standard financial QA benchmarks, showing that integrating domain-specific retrieval mechanisms such as Multi-HyDE with robust toolsets, including keyword and table-based retrieval, significantly enhances both the accuracy and reliability of answers. This research not only delivers a modular, adaptable retrieval framework for finance but also highlights the importance of structured agent workflows and multi-perspective retrieval for trustworthy deployment of AI in high-stakes financial applications.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG</title>
<link>https://arxiv.org/abs/2509.16491</link>
<guid>https://arxiv.org/abs/2509.16491</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, physiological data, fine-tuning, demographic fairness, heart rate prediction

Summary:
Fine-tuning foundation models pretrained on physiological data, such as photoplethysmography signals, can significantly improve heart rate prediction accuracy but may widen demographic fairness gaps, especially in different deployment domains. The study introduces FairTune, a bias-aware fine-tuning framework that includes mitigation strategies like class weighting, Group Distributionally Robust Optimization, and adversarial debiasing. These strategies effectively reduce fairness gaps without compromising prediction accuracy, with results varying based on the deployment domain. Representation analyses show that mitigation techniques reshape internal embeddings to reduce demographic clustering. The findings emphasize the necessity of explicit mitigation for equitable deployment of physiological foundation models.<br /><br />Summary: Fine-tuning can enhance HR prediction accuracy but may widen demographic fairness gaps, FairTune framework includes effective mitigation strategies, such as class weighting and GroupDRO, which reshape internal embeddings to reduce demographic clustering. <div>
arXiv:2509.16491v1 Announce Type: cross 
Abstract: Foundation models pretrained on physiological data such as photoplethysmography (PPG) signals are increasingly used to improve heart rate (HR) prediction across diverse settings. Fine-tuning these models for local deployment is often seen as a practical and scalable strategy. However, its impact on demographic fairness particularly under domain shifts remains underexplored. We fine-tune PPG-GPT a transformer-based foundation model pretrained on intensive care unit (ICU) data across three heterogeneous datasets (ICU, wearable, smartphone) and systematically evaluate the effects on HR prediction accuracy and gender fairness. While fine-tuning substantially reduces mean absolute error (up to 80%), it can simultaneously widen fairness gaps, especially in larger models and under significant distributional characteristics shifts. To address this, we introduce FairTune, a bias-aware fine-tuning framework in which we benchmark three mitigation strategies: class weighting based on inverse group frequency (IF), Group Distributionally Robust Optimization (GroupDRO), and adversarial debiasing (ADV). We find that IF and GroupDRO significantly reduce fairness gaps without compromising accuracy, with effectiveness varying by deployment domain. Representation analyses further reveal that mitigation techniques reshape internal embeddings to reduce demographic clustering. Our findings highlight that fairness does not emerge as a natural byproduct of fine-tuning and that explicit mitigation is essential for equitable deployment of physiological foundation models.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cost-Effective ZK-Rollups: Modeling and Optimization of Proving Infrastructure</title>
<link>https://arxiv.org/abs/2509.16581</link>
<guid>https://arxiv.org/abs/2509.16581</guid>
<content:encoded><![CDATA[
<div> Zero-knowledge rollups, Halo2-based proving systems, cost model, Z3 SMT solver, simulator <br />
Summary:
Zero-knowledge rollups face challenges with costly hardware requirements, finality constraints, and rising transaction throughput. This study focuses on cost drivers such as transactions per second, gas usage, and finality time. A parametric cost model is proposed to optimize configurations and ensure provers can handle transaction loads efficiently. The model is formulated as a constraint system and solved using the Z3 SMT solver. A simulator is implemented to detect delays and estimate operational costs, showing a potential cost reduction of up to 70%. <div>
arXiv:2509.16581v1 Announce Type: cross 
Abstract: Zero-knowledge rollups rely on provers to generate multi-step state transition proofs under strict finality and availability constraints. These steps require expensive hardware (e.g., GPUs), and finality is reached only once all stages complete and results are posted on-chain. As rollups scale, staying economically viable becomes increasingly difficult due to rising throughput, fast finality demands, volatile gas prices, and dynamic resource needs. We base our study on Halo2-based proving systems and identify transactions per second (TPS), average gas usage, and finality time as key cost drivers. To address this, we propose a parametric cost model that captures rollup-specific constraints and ensures provers can keep up with incoming transaction load. We formulate this model as a constraint system and solve it using the Z3 SMT solver to find cost-optimal configurations. To validate our approach, we implement a simulator that detects lag and estimates operational costs. Our method shows a potential cost reduction of up to 70\%.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KANO: Kolmogorov-Arnold Neural Operator</title>
<link>https://arxiv.org/abs/2509.16825</link>
<guid>https://arxiv.org/abs/2509.16825</guid>
<content:encoded><![CDATA[
<div> Kolmogorov--Arnold Neural Operator, dual-domain neural operator, spectral bases, spatial bases, symbolic interpretability <br />
Summary:<br />
The Kolmogorov--Arnold Neural Operator (KANO) is introduced as a neural operator that combines spectral and spatial bases, providing intrinsic symbolic interpretability. KANO addresses the limitations of the Fourier Neural Operator (FNO) by remaining expressive over position-dependent dynamics, unlike FNO, which is only practical for spectrally sparse operators. Empirical verification shows that KANO generalizes robustly in position-dependent differential operators, while FNO does not. In a quantum Hamiltonian learning benchmark, KANO accurately reconstructs ground-truth Hamiltonians with closed-form symbolic representations and low state infidelity from projective measurement data. KANO significantly outperforms FNO in terms of state infidelity, highlighting its superior performance in capturing dynamics accurately. <br /> <div>
arXiv:2509.16825v1 Announce Type: cross 
Abstract: We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\approx 6\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\approx 1.5\times10^{-2}$, by orders of magnitude.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability matters: The effect of liability rules on the healthcare sector</title>
<link>https://arxiv.org/abs/2509.17334</link>
<guid>https://arxiv.org/abs/2509.17334</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainability, Artificial Intelligence, Healthcare, Liability, Legal Framework

Summary: 
Explainability of artificial intelligence systems (AIS) is crucial in critical sectors like healthcare. This perspective analyzes the impact of explainability on liability determinations in healthcare settings. Contrasting the extreme cases of an "Oracle" AIS without explainability and an "AI Colleague" AIS with explainability, the discussion explores how automation and explainability affect liability assignment between medical practitioners, healthcare facilities, and AIS manufacturers. The article argues that explainability is essential in establishing a responsibility framework in healthcare, shaping behavior, and reducing the risk of defensive medicine practices. Ultimately, the level of explainability in AIS is vital for legal considerations and improving overall outcomes in healthcare. 

<br /><br />Summary: <div>
arXiv:2509.17334v1 Announce Type: cross 
Abstract: Explainability, the capability of an artificial intelligence system (AIS) to explain its outcomes in a manner that is comprehensible to human beings at an acceptable level, has been deemed essential for critical sectors, such as healthcare. Is it really the case? In this perspective, we consider two extreme cases, ``Oracle'' (without explainability) versus ``AI Colleague'' (with explainability) for a thorough analysis. We discuss how the level of automation and explainability of AIS can affect the determination of liability among the medical practitioner/facility and manufacturer of AIS. We argue that explainability plays a crucial role in setting a responsibility framework in healthcare, from a legal standpoint, to shape the behavior of all involved parties and mitigate the risk of potential defensive medicine practices.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AutoML Framework using AutoGluonTS for Forecasting Seasonal Extreme Temperatures</title>
<link>https://arxiv.org/abs/2509.17734</link>
<guid>https://arxiv.org/abs/2509.17734</guid>
<content:encoded><![CDATA[
<div> Keywords: forecasting, deep learning, maximum daily temperature, climatology, AutoGluonTS <br />
Summary: <br />
In the field of meteorological forecasting, deep learning has shown significant advancements in predicting the daily average temperature over a ten-day horizon. However, accurately forecasting maximum daily temperatures over short horizons remains a challenge. This study focuses on predicting maximum daily temperatures over medium-term periods of 90 days, approaching the issue from a climatological perspective. Utilizing a large historical dataset from South America and incorporating information from various ocean basins, the AutoGluonTS platform was employed to address the forecasting problem. By framing the problem as a temporal classification task with classes of "above normal," "normal," or "below normal" temperatures, competitive forecasting performance was achieved. AutoGluonTS offers efficient forecasting capabilities with relatively low computational costs compared to other operational platforms, making it a promising tool for addressing complex climatological forecasting challenges. <br /> <div>
arXiv:2509.17734v1 Announce Type: cross 
Abstract: In recent years, great progress has been made in the field of forecasting meteorological variables. Recently, deep learning architectures have made a major breakthrough in forecasting the daily average temperature over a ten-day horizon. However, advances in forecasting events related to the maximum temperature over short horizons remain a challenge for the community. A problem that is even more complex consists in making predictions of the maximum daily temperatures in the short, medium, and long term. In this work, we focus on forecasting events related to the maximum daily temperature over medium-term periods (90 days). Therefore, instead of addressing the problem from a meteorological point of view, this article tackles it from a climatological point of view. Due to the complexity of this problem, a common approach is to frame the study as a temporal classification problem with the classes: maximum temperature "above normal", "normal" or "below normal". From a practical point of view, we created a large historical dataset (from 1981 to 2018) collecting information from weather stations located in South America. In addition, we also integrated exogenous information from the Pacific, Atlantic, and Indian Ocean basins. We applied the AutoGluonTS platform to solve the above-mentioned problem. This AutoML tool shows competitive forecasting performance with respect to large operational platforms dedicated to tackling this climatological problem; but with a "relatively" low computational cost in terms of time and resources.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRFT: Mining High-Frequency Risk Factor Collections End-to-End via Transformer</title>
<link>https://arxiv.org/abs/2408.01271</link>
<guid>https://arxiv.org/abs/2408.01271</guid>
<content:encoded><![CDATA[
<div> Keywords: quantitative trading, neural networks, risk factors, symbolic mathematics, transformer model 

Summary: 
Quantitative trading relies on transforming historical stock data into interpretable risk factors to identify market volatility and risk effectively. While neural networks have advanced in extracting latent risk factors, they lack explicit, formulaic designs. This paper introduces the Intraday Risk Factor Transformer (IRFT) methodology, which treats the mining of risk factors as a language modeling problem using symbolic mathematics. The IRFT model generates complete formulaic risk factors, including constants, without a predefined skeleton of operators. By training on high frequency trading datasets, IRFT determines the general form of stock volatility laws, refining predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm. In comparative analysis, IRFT outperforms existing methods in HF risk factor mining tasks, achieving a 30% higher investment return on datasets like HS300 and SP500 while significantly reducing inference times. <br /><br />Summary: <div>
arXiv:2408.01271v5 Announce Type: replace 
Abstract: In quantitative trading, transforming historical stock data into interpretable, formulaic risk factors enhances the identification of market volatility and risk. Despite recent advancements in neural networks for extracting latent risk factors, these models remain limited to feature extraction and lack explicit, formulaic risk factor designs. By viewing symbolic mathematics as a language where valid mathematical expressions serve as meaningful "sentences" we propose framing the task of mining formulaic risk factors as a language modeling problem. In this paper, we introduce an end to end methodology, Intraday Risk Factor Transformer (IRFT), to directly generate complete formulaic risk factors, including constants. We use a hybrid symbolic numeric vocabulary where symbolic tokens represent operators and stock features, and numeric tokens represent constants. We train a Transformer model on high frequency trading (HFT) datasets to generate risk factors without relying on a predefined skeleton of operators. It determines the general form of the stock volatility law, including constants. We refine the predicted constants using the Broyden Fletcher Goldfarb Shanno (BFGS) algorithm to mitigate non linear issues. Compared to the ten approaches in SRBench, an active benchmark for symbolic regression (SR), IRFT achieves a 30% higher investment return on the HS300 and SP500 datasets, while achieving inference times that are orders of magnitude faster than existing methods in HF risk factor mining tasks.
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StockGenChaR: A Study on the Evaluation of Large Vision-Language Models on Stock Chart Captioning</title>
<link>https://arxiv.org/abs/2412.04041</link>
<guid>https://arxiv.org/abs/2412.04041</guid>
<content:encoded><![CDATA[
<div> Keywords: Technical analysis, finance, AI tools, image captioning, stock charts

Summary:<br /><br />
The article focuses on using AI tools to assist non-expert investors in analyzing stock charts for better decision-making. It introduces a new dataset, StockGenChaR, to evaluate large vision-language models in image captioning with stock charts. The main goal is to generate informative descriptions of stock charts that can help investors in understanding market sentiment towards specific stocks. By utilizing AI technology, investors can gain insights into past market data and forecast potential price movements in the future. This can be particularly useful for those who may not have expertise in technical analysis in finance. Overall, the proposed task aims to provide valuable information through image captioning that can assist investors in making informed decisions in the stock market. <div>
arXiv:2412.04041v2 Announce Type: replace 
Abstract: Technical analysis in finance, which aims at forecasting price movements in the future by analyzing past market data, relies on the in- sights that can be gained from the interpretation of stock charts; therefore, non-expert investors could greatly benefit from AI tools that can assist with the captioning of such charts. In our work, we introduce a new dataset StockGenChaR to evaluate large vision-language models in image captioning with stock charts. The purpose of the proposed task is to generate informative descriptions of the depicted charts and help to read the sentiment of the market regarding specific stocks, thus providing useful information for investors
]]></content:encoded>
<pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPH-Net: A Co-Attention Hybrid Model for Accurate Stock Price Prediction</title>
<link>https://arxiv.org/abs/2509.15414</link>
<guid>https://arxiv.org/abs/2509.15414</guid>
<content:encoded><![CDATA[
<div> SPH-Net, Stock Price Prediction, Hybrid Neural Network, Deep Learning, Time Series Forecasting
<br />
Summary: 
SPH-Net is a new deep learning framework designed for stock price prediction that incorporates a co-attention mechanism. It utilizes a Vision Transformer for processing temporal patterns and an attention mechanism for feature extraction, capturing global and local dependencies in market data. The model is evaluated on eight stock datasets using six fundamental market indicators. Results show that SPH-Net outperforms existing models in stock prediction accuracy. Its success lies in effectively capturing complex temporal patterns and maintaining robustness against market noise. This improved accuracy can provide valuable decision-support for investors and financial analysts, improving investment strategies and risk assessment in volatile market conditions. 
<br /> <div>
arXiv:2509.15414v1 Announce Type: new 
Abstract: Prediction of stock price movements presents a formidable challenge in financial analytics due to the inherent volatility, non-stationarity, and nonlinear characteristics of market data. This paper introduces SPH-Net (Stock Price Prediction Hybrid Neural Network), an innovative deep learning framework designed to enhance the accuracy of time series forecasting in financial markets. The proposed architecture employs a novel co-attention mechanism that initially processes temporal patterns through a Vision Transformer, followed by refined feature extraction via an attention mechanism, thereby capturing both global and local dependencies in market data. To rigorously evaluate the model's performance, we conduct comprehensive experiments on eight diverse stock datasets: AMD, Ebay, Facebook, FirstService Corp, Tesla, Google, Mondi ADR, and Matador Resources. Each dataset is standardized using six fundamental market indicators: Open, High, Low, Close, Adjusted Close, and Volume, representing a complete set of features for comprehensive market analysis. Experimental results demonstrate that SPH-Net consistently outperforms existing stock prediction models across all evaluation metrics. The model's superior performance stems from its ability to effectively capture complex temporal patterns while maintaining robustness against market noise. By significantly improving prediction accuracy in financial time series analysis, SPH-Net provides valuable decision-support capabilities for investors and financial analysts, potentially enabling more informed investment strategies and risk assessment in volatile market conditions.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Memory Efficient Adjoint Method to Enable Billion Parameter Optimization on a Single GPU in Dynamic Problems</title>
<link>https://arxiv.org/abs/2509.15744</link>
<guid>https://arxiv.org/abs/2509.15744</guid>
<content:encoded><![CDATA[
<div> sensitivity computations, dynamic optimization, adjoint method, superposition principle, CUDA implementation <br />
Summary: <br />
Dynamic optimization faces limitations due to memory requirements for sensitivity computations relying on full forward and adjoint wave fields. A new approach based on the adjoint method and superposition principle is introduced to approximate sensitivity computations for self-adjoint problems, enabling iterative computation and reducing memory burden to the number of degrees of freedom. This allows for sensitivity computations on GPUs with limited memory capacity, such as the A100 from NVIDIA. The approach is demonstrated on full waveform inversion and transient acoustic topology optimization, utilizing a highly efficient finite difference forward solver implemented in CUDA. However, the technique is limited to self-adjoint problems and does not account for phenomena like damping. <br /> <div>
arXiv:2509.15744v1 Announce Type: new 
Abstract: Dynamic optimization is currently limited by sensitivity computations that require information from full forward and adjoint wave fields. Since the forward and adjoint solutions are computed in opposing time directions, the forward solution must be stored. This requires a substantial amount of memory for large-scale problems even when using check pointing or data compression techniques. As a result, the problem size is memory bound rather than bound by wall clock time, when working with modern GPU-based implementations that have limited memory capacity. To overcome this limitation, we introduce a new approach for approximate sensitivity computation based on the adjoint method (for self-adjoint problems) that relies on the principle of superposition. The approximation allows an iterative computation of the sensitivity, reducing the memory burden to that of the solution at a small number of time steps, i.e., to the number of degrees of freedom. This enables sensitivity computations for problems with billions of degrees of freedom on current GPUs, such as the A100 from NVIDIA (from 2020). We demonstrate the approach on full waveform inversion and transient acoustic topology optimization problems, relying on a highly efficient finite difference forward solver implemented in CUDA. Phenomena such as damping cannot be considered, as the approximation technique is limited to self-adjoint problems.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A CARLA-based Simulation of Electrically Driven Forklifts</title>
<link>https://arxiv.org/abs/2509.15909</link>
<guid>https://arxiv.org/abs/2509.15909</guid>
<content:encoded><![CDATA[
<div> Keywords: electric forklift fleet, simulation, intralogistics, CARLA, traffic detection

Summary: 
This paper discusses the simulation of an electric forklift fleet within an intralogistics scenario using the open-source simulation tool CARLA. The study involves generating and visualizing a 3D outdoor warehouse scenario with moving forklifts, simulating intralogistics transport tasks, and playing back localization data from a real forklift fleet. The simulation also includes modeling the energy consumption of forklift trucks using a physical battery model. Two use cases are explored: detecting regions with high traffic densities and optimizing the placement of charging stations for the forklift trucks. Both scenarios are analyzed in an exemplary warehouse model to demonstrate the versatility of the CARLA simulation platform. <div>
arXiv:2509.15909v1 Announce Type: new 
Abstract: This paper presents the simulation of the operation of an electric forklift fleet within an intralogistics scenario. For this purpose, the open source simulation tool CARLA is used; according to our knowledge this is a novel approach in the context of logistics simulation. First, CARLA is used to generate and visualize a realistic 3D outdoor warehouse scenario, incorporating a number of randomly moving forklifts. In a next step, intralogistics transport tasks, such as pick-and-place, are simulated for the forklift fleet, including shortest-path finding. Furthermore, the capability to play back localization data, previously recorded from a ''real'' forklift fleet, is demonstrated.This play back is done in the original recreated environment, thereby enabling the visualization of the forklifts movements. Finally, the energy consumption of the forklift trucks is simulated by integrating a physical battery model that generates the state of charge (SOC) of each truck as a function of load and activity. To demonstrate the wide range of possible applications for the CARLA simulation platform, we describe two use cases. The first deals with the problem of detecting regions with critically high traffic densities, the second with optimal placement of charging stations for the forklift trucks. Both use cases are calculated for an exemplary warehouse model.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems</title>
<link>https://arxiv.org/abs/2509.15961</link>
<guid>https://arxiv.org/abs/2509.15961</guid>
<content:encoded><![CDATA[
<div> optimize, mobile sensor, Bayesian inverse problem, partial differential equation, uncertainty

Summary:
The article focuses on optimizing the path of a mobile sensor to reduce posterior uncertainty in a Bayesian inverse problem. Measurements are taken continuously along the path to estimate the state, modeled as a solution of a partial differential equation with uncertain parameters. The posterior covariance matrix of the model parameters is derived in closed-form for linear PDEs, enabling the formulation of optimal experimental design to minimize uncertainty. A discretization approach is used to maintain cost function consistency under temporal refinement, while constraints ensure obstacle avoidance and path interpretability. The constrained optimization problem is solved using an interior-point method. Computational results for a convection-diffusion equation with unknown initial conditions are presented. <div>
arXiv:2509.15961v1 Announce Type: new 
Abstract: We optimize the path of a mobile sensor to minimize the posterior uncertainty of a Bayesian inverse problem. Along its path, the sensor continuously takes measurements of the state, which is a physical quantity modeled as the solution of a partial differential equation (PDE) with uncertain parameters. Considering linear PDEs specifically, we derive the closed-form expression of the posterior covariance matrix of the model parameters as a function of the path, and formulate the optimal experimental design problem for minimizing the posterior's uncertainty. We discretize the problem such that the cost function remains consistent under temporal refinement. Additional constraints ensure that the path avoids obstacles and remains physically interpretable through a control parameterization. The constrained optimization problem is solved using an interior-point method. We present computational results for a convection-diffusion equation with unknown initial condition.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria</title>
<link>https://arxiv.org/abs/2509.16040</link>
<guid>https://arxiv.org/abs/2509.16040</guid>
<content:encoded><![CDATA[
<div> Sparse regression, constitutive model discovery, automated framework, hyperelasticity, model selection criteria  
Summary:  
- An automated framework for constitutive model discovery is presented, utilizing three sparse regression algorithms and three model selection criteria.  
- The framework pairs algorithms like LASSO, LARS, and OMP with selection criteria including $K$-fold cross-validation, AIC, and BIC.  
- Results show that all nine algorithm-criterion combinations perform well for discovering isotropic and anisotropic materials.  
- LARS efficiently solves the $\ell_1$-constrained problem, while OMP serves as a heuristic for $\ell_0$-regularized selection.  
- The study broadens the range of viable discovery algorithms beyond typical $\ell_1$-based approaches like LASSO.  

<br /><br />Summary: <div>
arXiv:2509.16040v1 Announce Type: cross 
Abstract: The automated discovery of constitutive models from data has recently emerged as a promising alternative to the traditional model calibration paradigm. In this work, we present a fully automated framework for constitutive model discovery that systematically pairs three sparse regression algorithms (Least Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression (LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). This pairing yields nine distinct algorithms for model discovery and enables a systematic exploration of the trade-off between sparsity, predictive performance, and computational cost. While LARS serves as an efficient path-based solver for the $\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for $\ell_0$-regularized selection. The framework is applied to both isotropic and anisotropic hyperelasticity, utilizing both synthetic and experimental datasets. Results reveal that all nine algorithm-criterion combinations perform consistently well for the discovery of isotropic and anisotropic materials, yielding highly accurate constitutive models. These findings broaden the range of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.
]]></content:encoded>
<pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Loss Balancing in Physics-Informed Neural Networks for Fluid Flow Applications</title>
<link>https://arxiv.org/abs/2509.14437</link>
<guid>https://arxiv.org/abs/2509.14437</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, Partial Differential Equations, Loss Balancing, Navier-Stokes<br />
Summary:<br />
Physics-Informed Neural Networks (PINNs) are used to solve partial differential equations (PDEs) by balancing multiple competing loss terms. Challenges arise in weighting physics residuals, boundary conditions, and initial conditions. Existing loss balancing schemes have been limited to fixed activation functions in neural network architectures. This study introduces trainable activation functions within neural networks and evaluates their effectiveness on complex fluid flow problems represented by Navier-Stokes equations. The proposed solution shows significant root mean square error (RMSE) improvements, ranging from 7.4% to 95.2%, across various scenarios. The results highlight the importance of considering the interplay between activation function selection and balancing algorithms when designing effective loss balancing strategies.<br /> 
Summary: <div>
arXiv:2509.14437v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising machine learning approach for solving partial differential equations (PDEs). However, PINNs face significant challenges in balancing multi-objective losses, as multiple competing loss terms such as physics residuals, boundary conditions, and initial conditions must be appropriately weighted. While various loss balancing schemes have been proposed, they have been implemented within neural network architectures with fixed activation functions, and their effectiveness has been assessed using simpler PDEs. We hypothesize that the effectiveness of loss balancing schemes depends not only on the balancing strategy itself, but also on the neural network's inherent function approximation capabilities, which are influenced by the choice of activation function. In this paper, we extend existing solutions by incorporating trainable activation functions within the neural network architecture and evaluate the proposed approach on complex fluid flow applications modeled by the Navier-Stokes equations. Our evaluation across diverse Navier-Stokes problems demonstrates that this proposed solution achieves root mean square error (RMSE) improvements ranging from 7.4\% to 95.2\% across different scenarios. These findings underscore the importance of carefully considering the interaction between activation function selection and balancing algorithms when designing loss balancing strategies.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lagrangian-Eulerian Multiscale Data Assimilation in Physical Domain based on Conditional Gaussian Nonlinear System</title>
<link>https://arxiv.org/abs/2509.14586</link>
<guid>https://arxiv.org/abs/2509.14586</guid>
<content:encoded><![CDATA[
<div> Keywords: Lagrangian-Eulerian Multiscale Data Assimilation, physical domain, sea ice floe trajectories, two-layer Quasi geostrophic model, Conditional Gaussian Nonlinear System

Summary: 
This research explores Lagrangian-Eulerian Multiscale Data Assimilation (LEMDA) by transitioning from Fourier space to the physical domain. Focusing on sea ice floe trajectories in the Arctic, a two-layer Quasi geostrophic model is used to recover ocean eddies. The model employs Conditional Gaussian Nonlinear System (CGNS) to handle non-linearity effectively. Performance evaluation using normalised root mean square error (RMSE) and pattern correlation (Corr) supports the efficacy of the physical domain approach. Future enhancements, like integrating neural networks (NN) to speed up localized particle recovery in Lagrangian DA, are discussed. Overall, the study demonstrates the benefits of utilizing the two-layer QG model in the physical domain for accurate data assimilation in non-periodic systems. 

<br /><br />Summary: <div>
arXiv:2509.14586v1 Announce Type: new 
Abstract: This research aims to further investigate the process of Lagrangian-Eulerian Multiscale Data Assimilation (LEMDA) by replacing the Fourier space with the physical domain. Such change in the perspective of domain introduces the advantages of being able to deal in non-periodic system and more intuitive representation of localised phenomena or time-dependent problems. The context of the domain for this paper was set as sea ice floe trajectories to recover the ocean eddies in the Arctic regions, which led the model to be derived from two-layer Quasi geostrophic (QG) model. The numerical solution to this model utilises the Conditional Gaussian Nonlinear System (CGNS) to accommodate the inherent non-linearity in analytical and continuous manner. The normalised root mean square error (RMSE) and pattern correlation (Corr) are used to evaluate the performance of the posterior mean of the model. The results corroborate the effectiveness of exploiting the two-layer QG model in physical domain. Nonetheless, the paper still discusses opportunities of improvement, such as deploying neural network (NN) to accelerate the recovery of local particle of Lagrangian DA for the fine scale.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of conductive nonmagnetic objects in presence of the Lenz effect</title>
<link>https://arxiv.org/abs/2509.14976</link>
<guid>https://arxiv.org/abs/2509.14976</guid>
<content:encoded><![CDATA[
<div> dynamics, MRI room, Lenz effect, numerical procedure, experimental data <br />
Summary:<br />
The study aims to model and predict the behavior of conductive nonmagnetic objects in an MRI room influenced by the Lenz effect. By formulating an ordinary differential equation and neglecting the skin effect, the Lenz effect's impact on object dynamics is separated into position and velocity dependencies, enabling the development of a straightforward numerical procedure applicable to objects of any shape. The model's accuracy was confirmed through experiments involving the rotation and translation of an aluminum plate within a 1.5 T MRI scanner. The findings highlight that precise predictions of motion in the presence of the Lenz effect can be achieved by accurately determining induced electric currents in the metal objects during motion steps without considering the skin effect. <div>
arXiv:2509.14976v1 Announce Type: new 
Abstract: Purpose: To model and predict the dynamics of conductive nonmagnetic objects within the MRI room under the influence of Lenz effect. Methods: The dynamics are described by an ordinary differential equation and the Lenz effect approximated by recognizing that the skin effect is negligible. This separated Lenz effect dependency on the object position and velocity, leading to a simple numerical procedure for objects of any shape. Results: The model and numerical procedure were validated with experimental data recording the rotation of an aluminum plate falling inside a 1.5 T MRI scanner. The model was also applied for studying the translation of an aluminum plate pushed with constant force towards the MRI bore through the fringe field. Conclusion: The collected results showed that it is possible to obtain accurate predictions of motion in the presence of Lenz effect by neglecting the skin effect while determining the electric currents induced in the metallic object during each infinitesimal motion step.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warp Quantification Analysis: A Framework For Path-based Signal Alignment Metrics</title>
<link>https://arxiv.org/abs/2509.14994</link>
<guid>https://arxiv.org/abs/2509.14994</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic time warping, warp quantification analysis, fMRI, schizophrenia, alignment descriptors

Summary: 
Dynamic Time Warping (DTW) is commonly used to align time series with different timescales, but traditional applications only focus on a single distance metric. This study introduces Warp Quantification Analysis (WQA), a novel framework that extracts geometric and structural descriptors from DTW paths. Simulations demonstrate that each metric in WQA accurately tracks specific characteristics without interference from others. When applied to large-scale fMRI data, WQA reveals distinct network signatures and their varied correlations with schizophrenia negative symptom severity. By expanding DTW into a family of alignment descriptors, WQA offers a more comprehensive approach for analyzing temporal coupling in domains requiring nonlinear normalization. WQA enables rich characterization beyond traditional DTW distance measurements, enhancing interpretations and insights in various research fields. 

<br /><br />Summary: <div>
arXiv:2509.14994v1 Announce Type: new 
Abstract: Dynamic time warping (DTW) is widely used to align time series evolving on mismatched timescales, yet most applications reduce alignment to a scalar distance. We introduce warp quantification analysis (WQA), a framework that derives interpretable geometric and structural descriptors from DTW paths. Controlled simulations showed that each metric selectively tracked its intended driver with minimal crosstalk. Applied to large-scale fMRI, WQA revealed distinct network signatures and complementary associations with schizophrenia negative symptom severity, capturing clinically meaningful variability beyond DTW distance. WQA transforms DTW from a single-score method into a family of alignment descriptors, offering a principled and generalizable extension for richer characterization of temporal coupling across domains where nonlinear normalization is essential.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A cell centered Galerkin method for miscible displacement in heterogeneous porous media</title>
<link>https://arxiv.org/abs/2509.14864</link>
<guid>https://arxiv.org/abs/2509.14864</guid>
<content:encoded><![CDATA[
<div> CCG method, miscible displacement, heterogeneous porous media, finite volume, discontinuous Galerkin<br />
<br />
Summary: The paper introduces the cell centered Galerkin (CCG) method for solving miscible displacement problems in porous media. This approach combines finite volume and discontinuous Galerkin methods to achieve an efficient lowest-order approximation, with only one unknown per cell. By utilizing classical DG weak formulations, the CCG method shows comparable accuracy and improved efficiency compared to traditional higher-order interior penalty DG methods. The study also proves that the CCG method produces an inverse-positive matrix for a model Poisson problem in 1D. Computational experiments in 2D and 3D highlight the effectiveness of CCG for highly heterogeneous flow and transport problems, with comparisons to classical DG methods demonstrating its advantages. <br /><br /> <div>
arXiv:2509.14864v1 Announce Type: cross 
Abstract: In this paper we present a cell centered Galerkin (CCG) method applied to miscible displacement problems in heterogeneous porous media. The CCG approach combines concepts from finite volume and discontinuous Galerkin (DG) methods to arrive at an efficient lowest-order approximation (one unknown per cell). We demonstrate that the CCG method can be defined using classical DG weak formulations, only requires one unknown per cell, and is able to deliver comparable accuracy and improved efficiency over traditional higher-order interior penalty DG methods. In addition, we prove that the CCG method for a model Poisson problem gives rise to a inverse-positive matrix in 1D. A plethora of computational experiments in 2D and 3D showcase the effectiveness of the CCG method for highly heterogeneous flow and transport problems in porous media. Comparisons between CCG and classical DG methods are included.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed GCN-LSTM Framework for Long-Term Forecasting of 2D and 3D Microstructure Evolution</title>
<link>https://arxiv.org/abs/2509.15029</link>
<guid>https://arxiv.org/abs/2509.15029</guid>
<content:encoded><![CDATA[
<div> framework, graph convolutional networks, long short-term memory, microstructure evolution, composition-aware <br />
Summary:
This paper introduces a novel framework that combines graph convolutional networks (GCN) and long short-term memory (LSTM) to predict microstructure evolution in 2D and 3D systems effectively. The framework considers the composition of the datasets and operates in latent graph space to efficiently capture composition and morphological changes. By compressing phase-field simulation data with convolutional autoencoders, the model can accurately forecast microstructural evolution in different compositions and dimensions over long time spans. The framework successfully captures spatial and temporal patterns in evolving microstructures, allowing for efficient long-term forecasting post-training. The integration of GCN and LSTM enables robust performance across various evaluation metrics, making it a promising tool for studying microstructure evolution. <br /><br /> <div>
arXiv:2509.15029v1 Announce Type: cross 
Abstract: This paper presents a physics-informed framework that integrates graph convolutional networks (GCN) with long short-term memory (LSTM) architecture to forecast microstructure evolution over long time horizons in both 2D and 3D with remarkable performance across varied metrics. The proposed framework is composition-aware, trained jointly on datasets with different compositions, and operates in latent graph space, which enables the model to capture compositions and morphological dynamics while remaining computationally efficient. Compressing and encoding phase-field simulation data with convolutional autoencoders and operating in Latent graph space facilitates efficient modeling of microstructural evolution across composition, dimensions, and long-term horizons. The framework captures the spatial and temporal patterns of evolving microstructures while enabling long-range forecasting at reduced computational cost after training.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying companies and financial actors exposed to marine tipping points</title>
<link>https://arxiv.org/abs/2411.10307</link>
<guid>https://arxiv.org/abs/2411.10307</guid>
<content:encoded><![CDATA[
<div> Keywords: climate change, marine ecosystems, tipping points, fisheries, investors<br />
<br />
Summary: 
Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Tracking fishing vessels in areas prone to marine regime shifts revealed key countries, companies, and shareholders exposed to tipping risk. Data gaps were acknowledged, but potential challenges and opportunities for these actors if marine ecosystems shift to less productive states were outlined. <div>
arXiv:2411.10307v2 Announce Type: replace 
Abstract: Climate change and other anthropogenic pressures are likely to induce tipping points in marine ecosystems, potentially leading to declines in primary productivity and fisheries. Despite increasing attention to nature-related financial risks and opportunities within the ocean economy, the extent to which these tipping points could affect investors has remained largely unexplored. Here we used satellite data to track fishing vessels operating in areas prone to marine regime shifts, as identified by their loss of resilience and vulnerability to marine heatwaves, and uncovered their corporate beneficial owners and shareholders. Despite some data gaps, we identified key countries, companies, and shareholders exposed to tipping risk. We also outline the potential challenges and opportunities that these actors may face if marine ecosystems shift to less productive states.
]]></content:encoded>
<pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-based deep kernel learning for parameter estimation in high dimensional PDEs</title>
<link>https://arxiv.org/abs/2509.14054</link>
<guid>https://arxiv.org/abs/2509.14054</guid>
<content:encoded><![CDATA[
<div> train, deep kernel learning, Hamiltonian Monte Carlo, parameter inference, uncertainty quantification

Summary: 
This paper proposes a novel two-stage Bayesian framework for inferring parameters of high-dimensional partial differential equations (PDEs). It combines physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to accurately estimate unknown PDE parameters and quantify their uncertainties from sparse observations. In the first stage, a surrogate model is trained using DKL to optimize a neural network feature extractor and provide initial parameter estimates. The second stage uses fixed neural network weights and HMC to sample the joint posterior distribution of kernel hyperparameters and PDE parameters efficiently. Numerical experiments show that the framework accurately estimates parameters, offers reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity. This approach provides a robust and scalable tool for a wide range of scientific and engineering applications.<br /><br />Summary: <div>
arXiv:2509.14054v1 Announce Type: new 
Abstract: Inferring parameters of high-dimensional partial differential equations (PDEs) poses significant computational and inferential challenges, primarily due to the curse of dimensionality and the inherent limitations of traditional numerical methods. This paper introduces a novel two-stage Bayesian framework that synergistically integrates training, physics-based deep kernel learning (DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE parameters and quantify their uncertainties from sparse, exact observations. The first stage leverages physics-based DKL to train a surrogate model, which jointly yields an optimized neural network feature extractor and robust initial estimates for the PDE parameters. In the second stage, with the neural network weights fixed, HMC is employed within a full Bayesian framework to efficiently sample the joint posterior distribution of the kernel hyperparameters and the PDE parameters. Numerical experiments on canonical and high-dimensional inverse PDE problems demonstrate that our framework accurately estimates parameters, provides reliable uncertainty estimates, and effectively addresses challenges of data sparsity and model complexity, offering a robust and scalable tool for diverse scientific and engineering applications.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Cognitive Bias in Social Agents</title>
<link>https://arxiv.org/abs/2509.13588</link>
<guid>https://arxiv.org/abs/2509.13588</guid>
<content:encoded><![CDATA[
<div> cognitive bias, social simulation, agent behavior, CoBRA, HCI toolkit

Summary:
CoBRA is a new toolkit for specifying agent behavior in LLM-based social simulations. Traditional methods using natural language descriptions often result in inconsistent behaviors that do not accurately represent the intended nuances. CoBRA introduces a novel approach by explicitly programming agents' cognitive biases based on social science experiments. It includes a Cognitive Bias Index that quantifies agents' reactions in validated experiments and a Behavioral Regulation Engine to align behaviors with controlled cognitive bias. Evaluation as an HCI toolkit demonstrated CoBRA's ability to precisely program cognitive bias in social agents across different models. This approach enables accurate representation of cognitive biases in social simulations, enhancing the realism and effectiveness of agent behavior programming. <div>
arXiv:2509.13588v1 Announce Type: cross 
Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behaviors through implicit natural language descriptions cannot yield consistent behaviors across models, and the produced agent behaviors do not capture the nuances of the descriptions. In contrast, CoBRA presents a new approach to program agents' cognitive biases explicitly, by grounding agents' expected behaviors using classic social science experiments. CoBRA has two components: (1) Cognitive Bias Index that measures the cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classical social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to demonstrate controlled cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and technical benchmarks. Our results suggest that CoBRA can precisely program the cognitive bias demonstrated in a social agent in a model-agnostic manner.
]]></content:encoded>
<pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Meshing Framework for Digital Twins for Extrusion based Additive Manufacturing</title>
<link>https://arxiv.org/abs/2509.12436</link>
<guid>https://arxiv.org/abs/2509.12436</guid>
<content:encoded><![CDATA[
<div> Keywords: additive manufacturing, computational meshes, finite element analysis, toolpath infill, design optimization<br />
Summary:<br />
Additive manufacturing (AM) allows for the production of complex 3D geometries with unique internal microstructures that influence mechanical properties. A new framework is proposed for creating computational meshes suitable for finite element analysis (FEA) of fine-scale features generated by AM tool paths. This framework enables in-depth numerical simulations to assess the impact of internal microstructures on component properties without the need for physical testing. By analyzing toolpath infill, the framework facilitates design optimization for components such as soft elastomeric lattices. This approach reduces time and resource waste typically associated with trial-and-error design cycles and enables the exploration of unconventional design spaces. Overall, the framework enhances the process-structure-property-performance linkage in AM components, opening up possibilities for innovative design solutions. <br /><br />Summary: <div>
arXiv:2509.12436v1 Announce Type: new 
Abstract: Additive manufacturing (AM) allows for manufacturing of complex three-dimensional geometries not typically realizable with standard subtractive manufacturing practices. The internal microstructure of a 3D printed component can have a significant impact on its mechanical, vibrational, and shock properties and allows for a richer design space when this is controllable. Due to the complex interactions of the internal geometry of an extrusion-based AM component, it is common practice to assume a homogeneous behavior or to perform characterization testing on the specific toolpath configurations. To avoid unnecessary testing or material waste, it is necessary to develop an accurate and consistent numerical simulation framework with relevant boundary value problems that can handle the complicated geometry of internal material microstructure present in AM components. Herein, a framework is proposed to directly create computational meshes suitable for finite element analysis (FEA) of the fine-scale features generated from extrusion-based AM tool paths to maintain a strong process-structure-property-performance linkage. This mesh can be manually or automatically analyzed using standard FEA simulations such as quasi-static preloading, modal analysis, or thermal analysis. The framework allows an in-silico assessment of a target AM geometry where fine-scale features may greatly impact quantities of design interest such as in soft elastomeric lattices where toolpath infill can greatly influence the self contact of a structure in compression, which we will use as a motivating exemplar. This approach greatly reduces the waste of both time and resources consumed through traditional build and test design cycles for non-intuitive design spaces. It also further allows for the exploration of toolpath infill to optimize component properties beyond simple linear properties such as density and stiffness.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News</title>
<link>https://arxiv.org/abs/2509.12519</link>
<guid>https://arxiv.org/abs/2509.12519</guid>
<content:encoded><![CDATA[
<div> financial news, stock prices, information diffusion, historical context, large language models

Summary:
- Financial news is a key driver of stock prices and plays a crucial role in information dissemination in financial markets.
- Each news article may require broader historical context for accurate interpretation, posing challenges in identifying relevant information.
- Historical context significantly improves performance in understanding the market impact of financial news across methods and time horizons.
- An efficient method is proposed that uses large language models to process the main article and small models to encode historical context for improved performance.
- Qualitative and quantitative tests reveal insights into the value of contextualization in model behavior and predictions, leading to substantial improvements in simulated investment performance. <div>
arXiv:2509.12519v1 Announce Type: new 
Abstract: Financial news plays a critical role in the information diffusion process in financial markets and is a known driver of stock prices. However, the information in each news article is not necessarily self-contained, often requiring a broader understanding of the historical news coverage for accurate interpretation. Further, identifying and incorporating the most relevant contextual information presents significant challenges. In this work, we explore the value of historical context in the ability of large language models to understand the market impact of financial news. We find that historical context provides a consistent and significant improvement in performance across methods and time horizons. To this end, we propose an efficient and effective contextualization method that uses a large LM to process the main article, while a small LM encodes the historical context into concise summary embeddings that are then aligned with the large model's representation space. We explore the behavior of the model through multiple qualitative and quantitative interpretability tests and reveal insights into the value of contextualization. Finally, we demonstrate that the value of historical context in model predictions has real-world applications, translating to substantial improvements in simulated investment performance.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Geometric Uncertainty on the Computation of Abdominal Aortic Aneurysm Wall Strain</title>
<link>https://arxiv.org/abs/2509.12550</link>
<guid>https://arxiv.org/abs/2509.12550</guid>
<content:encoded><![CDATA[
<div> geometry, abdominal aortic aneurysm, wall strain, wall stress, computed tomography angiography<br />
<br />
Summary:<br />
Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by the permanent enlargement of the aorta. Current management relies on aneurysm diameter and growth rate, which may not accurately predict rupture risk. This study examined the impact of geometric uncertainty on AAA wall strain calculated from 4D-CTA. Results showed that uncertainties in wall geometry reduce the accuracy of computed strain, with inward bias causing greater deviations than outward bias. Peak strain is more sensitive but less robust, while the 99th percentile strain remains stable. To ensure accurate strain estimation, geometric uncertainty should remain within one wall thickness. This research emphasizes the importance of considering geometric uncertainty in AAA risk assessments and highlights the need for precise image-derived geometry in computational analyses. <div>
arXiv:2509.12550v1 Announce Type: new 
Abstract: Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by permanent enlargement of the aorta, often detected incidentally during imaging for unrelated conditions. Current management relies primarily on aneurysm diameter and growth rate, which may not reliably predict patient-specific rupture risk. Computation of AAA wall stress and strain has the potential to improve individualized risk assessment, but these analyses depend on image-derived geometry, which is subject to segmentation uncertainty and lacks a definitive ground truth for the wall boundary. While the effect of geometric uncertainty on wall stress has been studied, its influence on wall strain remains unclear. In this study, we assessed the impact of geometric uncertainty on AAA wall strain computed using deformable image registration of time-resolved 3D computed tomography angiography (4D-CTA). Controlled perturbations were applied to the wall geometry along the surface normal, parameterized by the standard deviation for random variation and the mean for systematic inward or outward bias, both scaled relative to wall thickness. Results show that uncertainties in AAA wall geometry reduce the accuracy of computed strain, with inward bias (toward the blood lumen and intraluminal thrombus) consistently causing greater deviations than outward bias (toward regions external to the aortic wall). Peak strain is more sensitive but less robust, whereas the 99th percentile strain remains more stable under perturbations. We concluded that, for sufficiently accurate strain estimation, geometric uncertainty should remain within one wall thickness (typically 1.5 mm).
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinSentLLM: Multi-LLM and Structured Semantic Signals for Enhanced Financial Sentiment Forecasting</title>
<link>https://arxiv.org/abs/2509.12638</link>
<guid>https://arxiv.org/abs/2509.12638</guid>
<content:encoded><![CDATA[
<div> Sentiment Analysis, Large Language Models, Financial Signals, Stock Markets, Forecasting <br />
Summary: <br />
The study introduces FinSentLLM, a framework that combines multiple large language models and financial signals for financial sentiment analysis. It outperforms existing models on accuracy and F1-score without the need for extensive retraining. The framework shows a 3-6% improvement over baseline models using the Financial PhraseBank dataset. Additionally, econometric analysis using the DCC-GARCH and Johansen cointegration test demonstrates a significant long-term relationship between financial sentiment and stock market movements. This suggests that sentiment signals can effectively predict equity market dynamics in the long run. <div>
arXiv:2509.12638v1 Announce Type: new 
Abstract: Financial sentiment analysis (FSA) has attracted significant attention, and recent studies increasingly explore large language models (LLMs) for this field. Yet most work evaluates only classification metrics, leaving unclear whether sentiment signals align with market behavior. We propose FinSentLLM, a lightweight multi-LLM framework that integrates an expert panel of sentiment forecasting LLMs, and structured semantic financial signals via a compact meta-classifier. This design captures expert complementarity, semantic reasoning signal, and agreement/divergence patterns without costly retraining, yielding consistent 3-6% gains over strong baselines in accuracy and F1-score on the Financial PhraseBank dataset. In addition, we also provide econometric evidence that financial sentiment and stock markets exhibit statistically significant long-run comovement, applying Dynamic Conditional Correlation GARCH (DCC-GARCH) and the Johansen cointegration test to daily sentiment scores computed from the FNSPID dataset and major stock indices. Together, these results demonstrate that FinSentLLM delivers superior forecasting accuracy for financial sentiment and further establish that sentiment signals are robustly linked to long-run equity market dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cost-Optimization Model for EV Charging Stations Utilizing Solar Energy and Variable Pricing</title>
<link>https://arxiv.org/abs/2509.12214</link>
<guid>https://arxiv.org/abs/2509.12214</guid>
<content:encoded><![CDATA[
<div> framework, electric vehicle charging stations, cost optimization, photovoltaic generation, electricity price uncertainty
Summary:
The paper introduces a cost optimization framework for electric vehicle (EV) charging stations that integrates on-site photovoltaic (PV) generation and considers electricity price uncertainty. The model, formulated as a linear program, ensures the satisfaction of vehicle energy demands, adherence to charging and grid capacity constraints, and minimization of procurement cost. Evaluations based on actual charging data demonstrate average savings of approximately 12% compared to a traditional first-come-first-served approach, with potential peak monthly reductions reaching 19.2%. A sensitivity analysis suggests that a slight increase in nominal cost can significantly mitigate worst-case exposure by 14%. Computational tests confirm the practicality and scalability of the proposed solution by successfully solving instances with up to 50 concurrent EVs in under 5 seconds on a standard laptop. This innovative method offers a grid-friendly and efficient approach for future EV charging operations. 
<br /><br />Summary: <div>
arXiv:2509.12214v1 Announce Type: cross 
Abstract: This paper presents a cost optimization framework for electric vehicle (EV) charging stations that leverages on-site photovoltaic (PV) generation and explicitly accounts for electricity price uncertainty through a Bertsimas--Sim robust formulation. The model is formulated as a linear program that satisfies vehicle energy demands, respects charging and grid capacity constraints, and minimizes procurement cost. Evaluations on real charging data from the Caltech ACN dataset show average savings of about 12\% compared to a first-come--first-served baseline, with peak monthly reductions up to 19.2\%. A lightweight sensitivity analysis indicates that a modest $\sim$5\% increase in nominal cost can reduce worst-case exposure by 14\%. Computational tests confirm real-time feasibility, with instances of up to 50 concurrent EVs solved in under 5 seconds on a standard laptop. The proposed method provides a practical, grid-friendly, and scalable solution for future EV charging operations.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An End to End Edge to Cloud Data and Analytics Strategy</title>
<link>https://arxiv.org/abs/2509.12296</link>
<guid>https://arxiv.org/abs/2509.12296</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, cloud, edge, data strategy, reference architectures

Summary:
Internet of Things (IoT) devices are rapidly increasing, leading to the need for real-time data for critical decision-making. Enterprises are quickly embracing cloud technology, creating a demand for secure and efficient strategies to maximize cloud and edge capabilities. This paper presents an end-to-end secure edge-to-cloud data and analytics strategy. It includes reference architectures for the device layer, edge layer, and cloud layer, facilitating practical implementation. By addressing the exponential growth of IoT devices, leveraging cloud technology, and ensuring secure data transmission, this strategy aims to optimize the utilization of cloud and edge assets for real-time decision-making. The proposed approach not only enhances data security but also enables efficient data analysis, ultimately contributing to the successful implementation of IoT applications in various industries. 

Summary: <div>
arXiv:2509.12296v1 Announce Type: cross 
Abstract: There is an exponential growth of connected Internet of Things (IoT) devices. These have given rise to applications that rely on real time data to make critical decisions quickly. Enterprises today are adopting cloud at a rapid pace. There is a critical need to develop secure and efficient strategy and architectures to best leverage capabilities of cloud and edge assets. This paper provides an end to end secure edge to cloud data and analytics strategy. To enable real life implementation, the paper provides reference architectures for device layer, edge layer and cloud layer.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparison of pipelines for the translation of a low resource language based on transformers</title>
<link>https://arxiv.org/abs/2509.12514</link>
<guid>https://arxiv.org/abs/2509.12514</guid>
<content:encoded><![CDATA[
<div> transformer-based neural networks, machine translation, Bambara language, low-resource translation, language distillation

Summary:
This work compares three pipelines for training transformer-based neural networks to produce machine translators for the Bambara language. The first pipeline focuses on translating French into Bambara using a simple transformer model. The second pipeline fine-tunes instructor models for French-to-Bambara translation, achieving varying results based on dataset specificity. The third pipeline utilizes language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model for language-agnostic embeddings, followed by a BERT extension for translation. Results show the first pipeline achieves the best accuracy overall, particularly on low-resource datasets. The instructor-based models perform better on individual datasets compared to aggregated collections, indicating their efficacy in capturing dataset-specific patterns. <div>
arXiv:2509.12514v1 Announce Type: cross 
Abstract: This work compares three pipelines for training transformer-based neural networks to produce machine translators for Bambara, a Mand\`e language spoken in Africa by about 14,188,850 people. The first pipeline trains a simple transformer to translate sentences from French into Bambara. The second fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures for French-to-Bambara translation. Models from the first two pipelines were trained with different hyperparameter combinations to improve BLEU and chrF scores, evaluated on both test sentences and official Bambara benchmarks. The third pipeline uses language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model, which provides language-agnostic embeddings. A BERT extension is then applied to LaBSE to generate translations. All pipelines were tested on Dokotoro (medical) and Bayelemagaba (mixed domains). Results show that the first pipeline, although simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on Bayelemagaba), consistent with low-resource translation results. On the Yiri dataset, created for this work, it achieves 33.81% BLEU and 41% chrF. Instructor-based models perform better on single datasets than on aggregated collections, suggesting they capture dataset-specific patterns more effectively.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Pipeline for Patient-Specific Modeling of Thoracic Aortic Aneurysm: From Medical Image to Finite Element Analysis</title>
<link>https://arxiv.org/abs/2509.12596</link>
<guid>https://arxiv.org/abs/2509.12596</guid>
<content:encoded><![CDATA[
<div> Keywords: aorta, thoracic aortic aneurysm, finite element analysis, deep learning, patient-specific modeling

Summary: 
The article discusses the significance of thoracic aortic aneurysms (TAAs) as a leading cause of mortality, emphasizing the importance of accurate diagnosis for treatment. It highlights the use of three-dimensional computed tomography (3D CT) imaging for precise evaluation of aortic geometry and stresses on the aortic wall. Deep learning-based image segmentation is recognized as a reliable method for extracting anatomical structures from medical images. The conversion of segmentation masks into structured mesh representation enables accurate finite element analysis (FEA) simulations, with hexahedral meshes commonly used for efficiency and accuracy in aortic biomechanics. Patient-specific modeling allows for detailed assessment of individual anatomical and biomechanical characteristics, supporting personalized treatment strategies. Developing accurate FE models is crucial for establishing a biomechanically based framework to predict the risk of TAA. <div>
arXiv:2509.12596v1 Announce Type: cross 
Abstract: The aorta is the body's largest arterial vessel, serving as the primary pathway for oxygenated blood within the systemic circulation. Aortic aneurysms consistently rank among the top twenty causes of mortality in the United States. Thoracic aortic aneurysm (TAA) arises from abnormal dilation of the thoracic aorta and remains a clinically significant disease, ranking as one of the leading causes of death in adults. A thoracic aortic aneurysm ruptures when the integrity of all aortic wall layers is compromised due to elevated blood pressure. Currently, three-dimensional computed tomography (3D CT) is considered the gold standard for diagnosing TAA. The geometric characteristics of the aorta, which can be quantified from medical imaging, and stresses on the aortic wall, which can be obtained by finite element analysis (FEA), are critical in evaluating the risk of rupture and dissection. Deep learning based image segmentation has emerged as a reliable method for extracting anatomical regions of interest from medical images. Voxel based segmentation masks of anatomical structures are typically converted into structured mesh representation to enable accurate simulation. Hexahedral meshes are commonly used in finite element simulations of the aorta due to their computational efficiency and superior simulation accuracy. Due to anatomical variability, patient specific modeling enables detailed assessment of individual anatomical and biomechanics behaviors, supporting precise simulations, accurate diagnoses, and personalized treatment strategies. Finite element (FE) simulations provide valuable insights into the biomechanical behaviors of tissues and organs in clinical studies. Developing accurate FE models represents a crucial initial step in establishing a patient-specific, biomechanically based framework for predicting the risk of TAA.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry</title>
<link>https://arxiv.org/abs/2509.12679</link>
<guid>https://arxiv.org/abs/2509.12679</guid>
<content:encoded><![CDATA[
<div> transformer-based NQS, neural quantum states, scaling laws, language models, second-quantized quantum chemistry<br />
Summary: <br />
The study investigates scaling laws for neural quantum states (NQS) incorporating language model components. It aims to understand NQS scalability and optimal performance trade-offs. By analyzing transformer-based NQS in second-quantized quantum chemistry applications, the research identifies scaling laws predicting performance based on problem size. The study delves into the relationship between model size, training time, and performance metrics, such as absolute error and V-score. Unlike language models, the relationship between model size and training time in NQS is highly dependent on the loss metric and ansatz used, challenging the linear relationship observed in traditional language models. This research sheds light on the scalability of NQS ansatze and provides insights into performance-resource trade-offs in quantum chemistry applications. <br /> <div>
arXiv:2509.12679v1 Announce Type: cross 
Abstract: Scaling laws have been used to describe how large language model (LLM) performance scales with model size, training data size, or amount of computational resources. Motivated by the fact that neural quantum states (NQS) has increasingly adopted LLM-based components, we seek to understand NQS scaling laws, thereby shedding light on the scalability and optimal performance--resource trade-offs of NQS ansatze. In particular, we identify scaling laws that predict the performance, as measured by absolute error and V-score, for transformer-based NQS as a function of problem size in second-quantized quantum chemistry applications. By performing analogous compute-constrained optimization of the obtained parametric curves, we find that the relationship between model size and training time is highly dependent on loss metric and ansatz, and does not follow the approximately linear relationship found for language models.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs</title>
<link>https://arxiv.org/abs/2509.12730</link>
<guid>https://arxiv.org/abs/2509.12730</guid>
<content:encoded><![CDATA[
<div> Keywords: digital ecosystems, financial sector, graph machine learning, network analysis, financial crime

Summary:
The article discusses the challenges faced by traditional rule-based systems in detecting sophisticated criminal behaviors in digital ecosystems within the financial sector. It proposes an approach that integrates graph machine learning and network analysis to improve the detection of topological patterns in transactional graphs. The key challenge lies in the sparse and unlabeled information in traditional financial datasets, prompting the development of a four-step preprocessing framework to generate weak ground-truth labels for analysis. Graph Autoencoders are then implemented to distinguish among topological patterns, with three variants compared in the analysis. Preliminary results suggest that this method is effective in detecting complex financial crime schemes, presenting a promising alternative to rule-based detection systems.<br /><br />Summary: <div>
arXiv:2509.12730v1 Announce Type: cross 
Abstract: The rise of digital ecosystems has exposed the financial sector to evolving abuse and criminal tactics that share operational knowledge and techniques both within and across different environments (fiat-based, crypto-assets, etc.). Traditional rule-based systems lack the adaptability needed to detect sophisticated or coordinated criminal behaviors (patterns), highlighting the need for strategies that analyze actors' interactions to uncover suspicious activities and extract their modus operandi. For this reason, in this work, we propose an approach that integrates graph machine learning and network analysis to improve the detection of well-known topological patterns within transactional graphs. However, a key challenge lies in the limitations of traditional financial datasets, which often provide sparse, unlabeled information that is difficult to use for graph-based pattern analysis. Therefore, we firstly propose a four-step preprocessing framework that involves (i) extracting graph structures, (ii) considering data temporality to manage large node sets, (iii) detecting communities within, and (iv) applying automatic labeling strategies to generate weak ground-truth labels. Then, once the data is processed, Graph Autoencoders are implemented to distinguish among the well-known topological patterns. Specifically, three different GAE variants are implemented and compared in this analysis. Preliminary results show that this pattern-focused, topology-driven method is effective for detecting complex financial crime schemes, offering a promising alternative to conventional rule-based detection systems.
]]></content:encoded>
<pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics-Informed Machine Learning for Geospatial Modeling of Soil Liquefaction: Global and National Surrogate Models for Simulation and Near-Real-Time Response</title>
<link>https://arxiv.org/abs/2509.10962</link>
<guid>https://arxiv.org/abs/2509.10962</guid>
<content:encoded><![CDATA[
<div> machine learning, soil liquefaction, geospatial information, high performance computing, regional-scale modeling

Summary:<br />
Using machine learning and geospatial information, surrogate models are developed to predict soil liquefaction at regional scales. Global and New Zealand-specific models are trained to mimic geotechnical models, anchored to mechanics and driven by ML for more predictive information. The models are geostatistically updated by subsurface data and precomputed globally for all earthquake scenarios, making them easy to execute and encouraging user adoption. Test applications show the proposed models outperform others significantly, with geostatistical updating further improving performance. Region-specific models may not offer significant advantages over global datasets. These models are ideal for regional-scale liquefaction hazard simulation and near-real-time response, with accompanying variance products indicating the influence of local geotechnical data on predicted liquefaction response. <br />Summary: <div>
arXiv:2509.10962v1 Announce Type: new 
Abstract: Using machine learning (ML), high performance computing, and a large body of geospatial information, we develop surrogate models to predict soil liquefaction across regional scales. Two sets of models - one global and one specific to New Zealand - are trained by learning to mimic geotechnical models at the sites of in-situ tests. Our geospatial approach has conceptual advantages in that predictions: (i) are anchored to mechanics, which encourages more sensible response and scaling across the domains of soil, site, and loading characteristics; (ii) are driven by ML, which allows more predictive information to be used, with greater potential for it to be exploited; (iii) are geostatistically updated by subsurface data, which anchors the predictions to known conditions; and (iv) are precomputed everywhere on earth for all conceivable earthquakes, which allows the models to be executed very easily, thus encouraging user adoption and evaluation. Test applications suggest that: (i) the proposed models outperform others to a statistically significant degree; (ii) the geostatistical updating further improves performance; and (iii) the anticipated advantages of region-specific models may largely be negated by the benefits of learning from larger global datasets. These models are best suited for regional-scale liquefaction hazard simulation and near-real-time response and are accompanied by variance products that convey where, and to what degree, the ML-predicted liquefaction response is influenced by local geotechnical data.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geospatial AI for Liquefaction Hazard and Impact Forecasting: A Demonstrative Study in the U.S. Pacific Northwest</title>
<link>https://arxiv.org/abs/2509.10965</link>
<guid>https://arxiv.org/abs/2509.10965</guid>
<content:encoded><![CDATA[
<div> machine learning, liquefaction hazard, geospatial model, regional-scale, earthquake

Summary:
- The study focuses on predicting liquefaction hazards in the Pacific Northwest region, specifically in Washington and Oregon, using a geospatial model driven by machine learning.
- The model is able to predict the probability of damaging ground deformation for 85 scenario earthquakes, providing high-resolution forecasts across regional scales.
- The model improves upon prior approaches by incorporating mechanics-based predictions, utilizing machine learning for more geospatial information, and anchoring predictions to known subsurface conditions.
- The resulting liquefaction hazard forecasts are made available in a GIS-ready, public repository for various regional-scale applications such as disaster simulations, evacuation route planning, and infrastructure vulnerability assessments.
- These predictions can be utilized for land-use planning, insurance loss modeling, hazard communication, and public investment prioritization to better prepare for and mitigate the consequences of potential large-magnitude earthquakes in the region.

<br /><br />Summary: <div>
arXiv:2509.10965v1 Announce Type: new 
Abstract: Recent large-magnitude earthquakes have demonstrated the damaging consequences of soil liquefaction and reinforced the need to understand and plan for liquefaction hazards at a regional scale. In the United States, the Pacific Northwest is uniquely vulnerable to such consequences given the potential for crustal, intraslab, and subduction zone earthquakes. In this study, the liquefaction hazard is predicted geospatially at high resolution and across regional scales for 85 scenario earthquakes in the states of Washington and Oregon. This is accomplished using an emergent geospatial model that is driven by machine learning, and which predicts the probability of damaging ground deformation by surrogating state-of-practice geotechnical models. The adopted model shows improved performance and has conceptual advantages over prior regional-scale modeling approaches in that predictions (i) are informed by mechanics, (ii) employ more geospatial information using machine learning, and (iii) are geostatistically anchored to known subsurface conditions. The utility of the resulting predictions for the 85 scenarios is then demonstrated via asset and network infrastructure vulnerability assessments. The liquefaction hazard forecasts are published in a GIS-ready, public repository and are suitable for disaster simulations, evacuation route planning, network vulnerability analysis, land-use planning, insurance loss modeling, hazard communication, public investment prioritization, and other regional-scale applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why "AI" Models for Predicting Soil Liquefaction have been Ignored, Plus Some that Shouldn't Be</title>
<link>https://arxiv.org/abs/2509.10966</link>
<guid>https://arxiv.org/abs/2509.10966</guid>
<content:encoded><![CDATA[
<div> AI, soil liquefaction, prediction models, model development, state-of-practice models

Summary:
- AI liquefaction models are increasingly used but often lack comparison to state-of-practice models, deviate from best practices, may not be used effectively, are presented in a complex manner, and are not provided for use.
- Understanding and addressing these issues can improve the direction and perception of AI in liquefaction research.
- While not all prior efforts are without merit, recognizing recurring shortcomings can guide future research.
- Highlighted papers show applications where AI can add value by enabling new modeling approaches and improving predictions of liquefaction phenomena. 

<br /><br />Summary: <div>
arXiv:2509.10966v1 Announce Type: new 
Abstract: Soil liquefaction remains an important and interesting problem that has attracted the development of enumerable prediction models. Increasingly, these models are utilizing algorithmic learning, or "artificial intelligence" (AI). The rapid growth of AI in the liquefaction literature is unsurprising, given its ease of implementation and potential advantages over traditional statistical methods. However, AI liquefaction models have been widely ignored by practitioners and researchers alike; the objective of this paper is to investigate "why?" Through a sample review of 75 publications, we identify several good reasons. Namely, these models frequently: (i) are not compared to state-of-practice models, making it unclear why they should be adopted; (ii) depart from best practices in model development; (iii) use AI in ways that may not be useful; (iv) are presented in ways that overstate their complexity and make them unapproachable; and (v) are discussed but not actually provided, meaning that no one can use the models even if they wanted to. These prevailing problems must be understood, identified, and remedied, but this does not mean that AI itself is problematic, or that all prior efforts have been without merit or utility. Instead, understanding these recurrent shortcomings can help improve the direction and perceptions of this growing body of work. Towards this end, we highlight papers that are generally free from these shortcomings, and which demonstrate applications where AI is more likely to provide value in the near term: permitting new modeling approaches and potentially improving predictions of liquefaction phenomena.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language model-empowered next-generation computer-aided engineering</title>
<link>https://arxiv.org/abs/2509.11447</link>
<guid>https://arxiv.org/abs/2509.11447</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, computer-aided engineering, model order reduction, autonomous collaborators, parametric analysis<br />
<br />Summary: <br />
The article introduces the concept of using large language models (LLMs) as autonomous collaborators in computer-aided engineering (CAE) to automate and optimize workflows. Specifically, it focuses on data-free model order reduction (MOR) for ultra-fast large-scale parametric analysis. By leveraging LLMs, intrusive MOR, a powerful but underused approach, can be made more accessible and practical through automation of derivations, code restructuring, and implementation. An LLM-empowered CAE agent for solving ultra-large-scale space-parameter-time (S-P-T) physical problems is presented, demonstrating the capability to translate natural language prompts into efficient solver implementations and generate novel MOR solvers for unseen cases. This showcases the potential of LLMs in establishing next-generation CAE systems. <div>
arXiv:2509.11447v1 Announce Type: new 
Abstract: Software development has entered a new era where large language models (LLMs) now serve as general-purpose reasoning engines, enabling natural language interaction and transformative applications across diverse domains. This paradigm is now extending into computer-aided engineering (CAE). Recent applications of LLMs in CAE have successfully automated routine tasks, including CAD model generation and FEM simulations. Nevertheless, these contributions, which primarily serve to reduce manual labor, are often insufficient for addressing the significant computational challenges posed by large-scale, high-dimensional systems. To this aim, we first introduce the concept of LLM-empowered CAE agent, where LLMs act as autonomous collaborators that plan, execute, and adapt CAE workflows. Then, we propose an LLM-empowered CAE agent for data-free model order reduction (MOR), a powerful yet underused approach for ultra-fast large-scale parametric analysis due to the intrusive nature and labor-intensive redevelopment of solvers. LLMs can alleviate this barrier by automating derivations, code restructuring, and implementation, making intrusive MOR both practical and broadly accessible. To demonstrate feasibility, we present an LLM-empowered CAE agent for solving ultra-large-scale space-parameter-time (S-P-T) physical problems using Tensor-decomposition-based A Priori Surrogates (TAPS). Our results show that natural language prompts describing parametric partial differential equations (PDEs) can be translated into efficient solver implementations, substantially reducing human effort while producing high-fidelity reduced-order models. Moreover, LLMs can synthesize novel MOR solvers for unseen cases such as nonlinear and high-dimensional parametric problems based on their internal knowledge base. This highlights the potential of LLMs to establish the foundation for next-generation CAE systems.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward lean industry 5.0: a human-centered model for integrating lean and industry 4.0 in an automotive supplier</title>
<link>https://arxiv.org/abs/2509.11658</link>
<guid>https://arxiv.org/abs/2509.11658</guid>
<content:encoded><![CDATA[
<div> lean, Industry 4.0, human-centered, automotive, case study 

Summary: 
This paper presents a human-centered conceptual model that integrates lean principles and Industry 4.0. It fills a gap in research by offering theoretical insights and practical findings through a case study at an advanced automotive supplier. The study emphasizes the importance of a human-centered approach, identifying key enablers and barriers in lean Industry 4.0 implementation. Through a five-phase multi-method approach, the study examines operational, social, and technological perspectives at both group and model site levels. It illustrates effective implementation strategies and showcases how advanced lean tools can be digitized. The case study identifies 26 positive aspects and 10 negative aspects, showcasing their causal relationships. Successful implementation is shown to benefit organizations and employees when supported by appropriate technological knowledge and people skills, paving the way for lean Industry 5.0. <br /><br /> <div>
arXiv:2509.11658v1 Announce Type: new 
Abstract: This paper proposes a human-centered conceptual model integrating lean and Industry 4.0 based on the literature review and validated it through a case study in the context of an advanced automotive first-tier supplier. Addressing a significant gap in existing research on lean Industry 4.0 implementations, the study provides both theoretical insights and practical findings. It emphasizes the importance of a human-centered approach, identifies key enablers and barriers. In the implementation process of the case study, it is considered at group level and model site level through operational, social and technological perspectives in a five-phase multi-method approach. It shows what effective human-centered lean Industry 4.0 implementation look like and how advanced lean tools can be digitized. It highlights 26 positive and 10 negative aspects of the case and their causal relation. With the appropriate internal and external technological knowhow and people skills, it shows how successful implementation can benefit the organization and employees based on the conceptual model that serves as a first step toward lean Industry 5.0.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Very-low-field MRI scanners: from the ideal to the real permanent magnet array</title>
<link>https://arxiv.org/abs/2509.11762</link>
<guid>https://arxiv.org/abs/2509.11762</guid>
<content:encoded><![CDATA[
<div> MRI, very-low-field, permanent magnets, spatial homogeneity, numerical model<br />
Summary:<br />
Very-low-field MRI technology, utilizing permanent magnets for B0 generation, is gaining popularity due to its portable and cost-effective nature. This article explores the importance of magnet performance in achieving spatial homogeneity of the magnetic field. It investigates factors affecting homogeneity and discrepancies between numerical predictions and actual measurements on fabricated magnets. The study also evaluates the impact of different numerical model approximations on results, highlighting the trade-offs between computational efficiency and result reliability. By providing insights into magnet characterization and model assumptions, this research contributes to enhancing the design and performance of low-cost MRI scanners. <div>
arXiv:2509.11762v1 Announce Type: new 
Abstract: Very-low-field MRIs are becoming increasingly popular due to their portability and adaptability to different environments. They are being successfully used for various clinical applications, leading to a paradigm shift in the way imaging care is typically performed. The development of low-cost MRI scanner prototypes began a few years ago, with some interesting and promising open-source projects emerging in both hardware and software design. Using permanent magnets (PMs) to generate the static magnetic field B0 can substantially reduce the manufacturing cost of low-field scanners while achieving satisfactory homogeneity. This article focuses on characterizing magnet performance in terms of B0 spatial homogeneity. Specifically, it investigates its sensitivity to various factors and explores the reasons for discrepancies between numerical expectations and actual measurements on fabricated magnets. The analysis also examines the consequences of using different numerical model approximations, revisiting concepts most frequently used in other design contexts. While these assumptions simplify the numerical model and may improve its performance in terms of computational time, this paper demonstrates that they also impact the reliability of the obtained results.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hetero-EUCLID: Interpretable model discovery for heterogeneous hyperelastic materials using stress-unsupervised learning</title>
<link>https://arxiv.org/abs/2509.11784</link>
<guid>https://arxiv.org/abs/2509.11784</guid>
<content:encoded><![CDATA[
<div> segmentation, parameter identification, hyperelastic behavior, heterogeneous material, Bayesian-EUCLID

Summary:
The computational framework Hetero-EUCLID is proposed for the segmentation and parameter identification of heterogeneous materials' hyperelastic behavior. Leveraging the Bayesian-EUCLID framework, the approach efficiently solves the heterogenized formulation through model selection with sparsity-promoting priors and Monte Carlo Markov Chain sampling. By utilizing experimentally observable data from non-equi-biaxial tension tests, the framework involves residual force-based segmentation and constitutive parameter identification. Validation shows its capability to segment domains and characterize constituent materials on thin square heterogeneous domains, even with noise in displacement data and non-native mesh discretizations. The framework's potential applications include Digital Image/Volume Correlation-based experimental scenarios like aerospace composites and medical conditions such as fibroatheroma, atherosclerosis, or cancer, offering rapid and interpretable model discovery from a single experiment. <br /><br />Summary: <div>
arXiv:2509.11784v1 Announce Type: new 
Abstract: We propose a computational framework, Hetero-EUCLID, for segmentation and parameter identification to characterize the full hyperelastic behavior of all constituents of a heterogeneous material. In this work, we leverage the Bayesian-EUCLID (Efficient Unsupervised Constitutive Law Identification and Discovery) framework to efficiently solve the heterogenized formulation through parsimonious model selection using sparsity-promoting priors and Monte Carlo Markov Chain sampling. We utilize experimentally observable 3D surface displacement and boundary-averaged force data generated from Finite Element simulations of non-equi-biaxial tension tests on heterogeneous specimens. The framework broadly consists of two steps -- residual force-based segmentation, and constitutive parameter identification. We validate and demonstrate the ability of the proposed framework to segment the domain, and characterize the constituent materials on various types of thin square heterogeneous domains. We validate of the framework's ability to segment and characterize materials with various levels of displacement noises and non-native mesh discretizations, i.e, using different meshes for the forward FE simulations and the inverse EUCLID problem. This demonstrates Hetero-EUCLID framework's applicability in Digital Image/Volume Correlation-based experimental scenarios. Furthermore, the proposed framework performs successful segmentation and material characterizations based on data from a single experiment, thereby making it viable for rapid, interpretable model discovery in domains such as aerospace and defense composites and for characterization of selective tissue stiffening in medical conditions such as fibroatheroma, atherosclerosis, or cancer.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical analysis of fluid estimation for source terms in neutral particles simulation</title>
<link>https://arxiv.org/abs/2509.11883</link>
<guid>https://arxiv.org/abs/2509.11883</guid>
<content:encoded><![CDATA[
<div> Keywords: plasma edge simulations, kinetic Monte Carlo, asymptotic-preserving method, convergence analysis, numerical analysis

Summary:
In this study, the authors investigate the efficiency of a kinetic-diffusion Monte Carlo (KDMC) simulation method coupled with a fluid estimation technique in plasma edge simulations. The aim is to address the computational cost associated with high particle collision rates in large-sized reactors like ITER and DEMO. Through numerical analysis, the researchers compare the accuracy of the proposed algorithm with an approximate fluid method and traditional kinetic Monte Carlo method as a reference. Results show that KDMC with the fluid estimation exhibits significantly lower errors than the fluid method in both high- and low-collisional regimes. Additionally, the KDMC method demonstrates a clear speed-up compared to the kinetic Monte Carlo approach. Overall, the study confirms the effectiveness of the KDMC algorithm in improving computational efficiency and accuracy in plasma edge simulations. <br /><br />Summary: <div>
arXiv:2509.11883v1 Announce Type: new 
Abstract: In plasma edge simulations, kinetic Monte Carlo (MC) is often used to simulate neutral particles and estimate source terms. For large-sized reactors, like ITER and DEMO, high particle collision rates lead to a substantial computational cost for such schemes. To address this challenge, an asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method and a corresponding fluid estimation technique have been proposed in the literature. In this work, we perform numerical analysis on the convergence of KDMC with the fluid estimation. To do so, we compare the accuracy of the analyzed algorithm with the accuracy of an approximate fluid method using the kinetic MC method as a reference. In a one-dimensional test case, KDMC with the fluid estimation achieves at least one order of magnitude lower errors than the fluid method for both high- and low-collisional regimes. Moreover, KDMC with the fluid estimation outperforms the kinetic MC method with a clear speed-up. Overall, our analysis confirms the effectiveness of the discussed algorithm.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval</title>
<link>https://arxiv.org/abs/2509.12042</link>
<guid>https://arxiv.org/abs/2509.12042</guid>
<content:encoded><![CDATA[
<div> keywords: financial disclosures, retrieval framework, FinGEAR, FLAM, 10-K filings

Summary:
FinGEAR is a retrieval framework designed specifically for financial documents, addressing challenges such as complex regulatory language and document structure. It incorporates the finance lexicon for Item-level guidance (FLAM), dual hierarchical indices for within-Item search, and a two-stage cross-encoder reranker. This framework aligns retrieval with the structure and terminology of financial disclosures, resulting in improved precision, recall, F1 score, and relevancy compared to existing models. FinGEAR outperforms flat RAG models by up to 56.7%, graph-based RAGs by 12.5%, and prior tree-based systems by 217.6%. It also enhances downstream answer accuracy when used with a fixed reader. By combining section hierarchy and domain lexicon signals, FinGEAR enhances retrieval fidelity, making it a valuable tool for high-stakes financial analysis.

<br /><br />Summary: <div>
arXiv:2509.12042v1 Announce Type: new 
Abstract: Financial disclosures such as 10-K filings present challenging retrieval problems due to their length, regulatory section hierarchy, and domain-specific language, which standard retrieval-augmented generation (RAG) models underuse. We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a retrieval framework tailored to financial documents. FinGEAR combines a finance lexicon for Item-level guidance (FLAM), dual hierarchical indices for within-Item search (Summary Tree and Question Tree), and a two-stage cross-encoder reranker. This design aligns retrieval with disclosure structure and terminology, enabling fine-grained, query-aware context selection. Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR delivers consistent gains in precision, recall, F1, and relevancy, improving F1 by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over prior tree-based systems, while also increasing downstream answer accuracy with a fixed reader. By jointly modeling section hierarchy and domain lexicon signals, FinGEAR improves retrieval fidelity and provides a practical foundation for high-stakes financial analysis.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing MacPherson Suspension Architectures using Bayesian Optimization</title>
<link>https://arxiv.org/abs/2206.09022</link>
<guid>https://arxiv.org/abs/2206.09022</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, engineering design, compliance, target specifications, discipline model
Summary: 
The article introduces a Bayesian optimization system for enhancing the traditional manual engineering design process. By optimizing compliance with target specifications directly and without requiring gradient information, the system aims to automate and expedite the design process. The method focuses on computing a generalized inverse of a high-dimensional non-linear function, enabling efficient optimization in a scalable manner. Additionally, the proposed two-tier convergence criterion ensures either convergence to an optimal solution satisfying all specified design criteria or convergence to a minimum-norm solution. The system's effectiveness is demonstrated through a vehicle chassis design problem in an industry context, utilizing a commercial discipline model. The results highlight the system's general applicability, scalability, and efficiency, showcasing the straightforward implementation of novel convergence criteria within popular Bayesian optimization software packages. 
<br /><br />Summary: <div>
arXiv:2206.09022v1 Announce Type: cross 
Abstract: Engineering design is traditionally performed by hand: an expert makes design proposals based on past experience, and these proposals are then tested for compliance with certain target specifications. Testing for compliance is performed first by computer simulation using what is called a discipline model. Such a model can be implemented by a finite element analysis, multibody systems approach, etc. Designs passing this simulation are then considered for physical prototyping. The overall process may take months, and is a significant cost in practice. We have developed a Bayesian optimization system for partially automating this process by directly optimizing compliance with the target specification with respect to the design parameters. The proposed method is a general framework for computing a generalized inverse of a high-dimensional non-linear function that does not require e.g. gradient information, which is often unavailable from discipline models. We furthermore develop a two-tier convergence criterion based on (i) convergence to a solution optimally satisfying all specified design criteria, or (ii) convergence to a minimum-norm solution. We demonstrate the proposed approach on a vehicle chassis design problem motivated by an industry setting using a state-of-the-art commercial discipline model. We show that the proposed approach is general, scalable, and efficient, and that the novel convergence criteria can be implemented straightforwardly based on existing concepts and subroutines in popular Bayesian optimization software packages.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs</title>
<link>https://arxiv.org/abs/2509.10504</link>
<guid>https://arxiv.org/abs/2509.10504</guid>
<content:encoded><![CDATA[
<div> Reframe retrosynthesis planning, worst-path optimization, tree-structured Markov Decision Processes, Interactive Retrosynthesis Planning, self-imitation learning <br />
Summary: 
This paper proposes a new approach to retrosynthesis planning by treating it as a worst-path optimization problem within tree-structured Markov Decision Processes, ensuring a unique optimal solution and offering improvement guarantees. The Interactive Retrosynthesis Planning (InterRetro) method is introduced, which interacts with the tree MDP, learns a value function for worst-path outcomes, and improves its policy through self-imitation learning. Empirical results show that InterRetro achieves state-of-the-art performance by solving 100% of targets on the Retro*-190 benchmark, shortening synthetic routes by 4.9%, and exhibiting promising performance with only 10% of the training data. This approach represents a significant advancement in computational retrosynthesis planning. <br /><br />Summary: <div>
arXiv:2509.10504v1 Announce Type: cross 
Abstract: Retrosynthesis planning aims to decompose target molecules into available building blocks, forming a synthesis tree where each internal node represents an intermediate compound and each leaf ideally corresponds to a purchasable reactant. However, this tree becomes invalid if any leaf node is not a valid building block, making the planning process vulnerable to the "weakest link" in the synthetic route. Existing methods often optimise for average performance across branches, failing to account for this worst-case sensitivity. In this paper, we reframe retrosynthesis as a worst-path optimisation problem within tree-structured Markov Decision Processes (MDPs). We prove that this formulation admits a unique optimal solution and offers monotonic improvement guarantees. Building on this insight, we introduce Interactive Retrosynthesis Planning (InterRetro), a method that interacts with the tree MDP, learns a value function for worst-path outcomes, and improves its policy through self-imitation, preferentially reinforcing past decisions with high estimated advantage. Empirically, InterRetro achieves state-of-the-art results, solving 100% of targets on the Retro*-190 benchmark, shortening synthetic routes by 4.9%, and achieving promising performance using only 10% of the training data - representing a significant advance in computational retrosynthesis planning.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective</title>
<link>https://arxiv.org/abs/2509.10506</link>
<guid>https://arxiv.org/abs/2509.10506</guid>
<content:encoded><![CDATA[
<div> boosting, retail demand forecasting, interpretable learning, feature attention, supply chain management <br />
Summary: <br />
The article discusses the challenges faced in forecasting product demand in retail supply chains and introduces a new framework called AttnBoost. AttnBoost integrates feature-level attention into the boosting process to improve predictive accuracy and explainability in the presence of noisy and heterogeneous data. By dynamically adjusting feature importance through an attention mechanism, AttnBoost can focus on high-impact variables like promotions, pricing, and seasonal trends. The model outperforms traditional machine learning and deep tabular models on a large-scale retail sales dataset, providing valuable insights for supply chain managers. An ablation study confirms the effectiveness of the attention module in reducing overfitting and enhancing interpretability. The results highlight the potential of attention-guided boosting for scalable and interpretable AI in real-world forecasting applications. <div>
arXiv:2509.10506v1 Announce Type: cross 
Abstract: Forecasting product demand in retail supply chains presents a complex challenge due to noisy, heterogeneous features and rapidly shifting consumer behavior. While traditional gradient boosting decision trees (GBDT) offer strong predictive performance on structured data, they often lack adaptive mechanisms to identify and emphasize the most relevant features under changing conditions. In this work, we propose AttnBoost, an interpretable learning framework that integrates feature-level attention into the boosting process to enhance both predictive accuracy and explainability. Specifically, the model dynamically adjusts feature importance during each boosting round via a lightweight attention mechanism, allowing it to focus on high-impact variables such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a large-scale retail sales dataset and demonstrate that it outperforms standard machine learning and deep tabular models, while also providing actionable insights for supply chain managers. An ablation study confirms the utility of the attention module in mitigating overfitting and improving interpretability. Our results suggest that attention-guided boosting represents a promising direction for interpretable and scalable AI in real-world forecasting applications.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Temporal Fusion Transformers for Cryptocurrency Price Prediction</title>
<link>https://arxiv.org/abs/2509.10542</link>
<guid>https://arxiv.org/abs/2509.10542</guid>
<content:encoded><![CDATA[
<div> Transformers, short-term prediction, cryptocurrency market, adaptive modeling, price forecasting

Summary: 
This paper introduces an adaptive Temporal Fusion Transformer (TFT) approach for precise short-term price prediction in the cryptocurrency market. By leveraging dynamic subseries lengths and pattern-based categorization, the model addresses the market's non-stationary nature and extreme volatility. A novel segmentation method is proposed, where subseries end at relative maxima, capturing significant upward movements and filtering noise. The fixed-length pattern ending each subseries determines the category assigned to the subsequent variable-length subseries, allowing for specialized prediction models for each category. Experimental results on ETH-USDT 10-minute data demonstrate that the adaptive approach outperforms baseline models in prediction accuracy and simulated trading profitability. The combination of adaptive segmentation and pattern-conditioned forecasting enhances robust and responsive cryptocurrency price prediction. 

<br /><br />Summary: <div>
arXiv:2509.10542v1 Announce Type: cross 
Abstract: Precise short-term price prediction in the highly volatile cryptocurrency market is critical for informed trading strategies. Although Temporal Fusion Transformers (TFTs) have shown potential, their direct use often struggles in the face of the market's non-stationary nature and extreme volatility. This paper introduces an adaptive TFT modeling approach leveraging dynamic subseries lengths and pattern-based categorization to enhance short-term forecasting. We propose a novel segmentation method where subseries end at relative maxima, identified when the price increase from the preceding minimum surpasses a threshold, thus capturing significant upward movements, which act as key markers for the end of a growth phase, while potentially filtering the noise. Crucially, the fixed-length pattern ending each subseries determines the category assigned to the subsequent variable-length subseries, grouping typical market responses that follow similar preceding conditions. A distinct TFT model trained for each category is specialized in predicting the evolution of these subsequent subseries based on their initial steps after the preceding peak. Experimental results on ETH-USDT 10-minute data over a two-month test period demonstrate that our adaptive approach significantly outperforms baseline fixed-length TFT and LSTM models in prediction accuracy and simulated trading profitability. Our combination of adaptive segmentation and pattern-conditioned forecasting enables more robust and responsive cryptocurrency price prediction.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COVID-BLUeS -- A Prospective Study on the Value of AI in Lung Ultrasound Analysis</title>
<link>https://arxiv.org/abs/2509.10556</link>
<guid>https://arxiv.org/abs/2509.10556</guid>
<content:encoded><![CDATA[
<div> Keywords: lung ultrasound, Artificial intelligence, COVID-19, severity, multi-modal models<br />
Summary:<br />
In a study at Maastricht University Medical Centre, lung ultrasound (LUS) data from COVID-19 suspects were analyzed using Artificial intelligence (AI) models for detecting and assessing pulmonary infections. The severity of lung involvement in COVID-19 positive and negative patients was similar according to human annotators. AI models showed 65% accuracy in detecting COVID-19 without specific training, improving to 79% with targeted training. Multi-modal models combining images and CBC data outperformed image-only models. However, the performance of AI models was limited due to heterogeneous LUS datasets, frame-based processing ignoring video-level information, and a lack of focus on multi-modal models. The dataset used in the study has been made publicly available to aid future research efforts. <div>
arXiv:2509.10556v1 Announce Type: cross 
Abstract: As a lightweight and non-invasive imaging technique, lung ultrasound (LUS) has gained importance for assessing lung pathologies. The use of Artificial intelligence (AI) in medical decision support systems is promising due to the time- and expertise-intensive interpretation, however, due to the poor quality of existing data used for training AI models, their usability for real-world applications remains unclear. In a prospective study, we analyze data from 63 COVID-19 suspects (33 positive) collected at Maastricht University Medical Centre. Ultrasound recordings at six body locations were acquired following the BLUE protocol and manually labeled for severity of lung involvement. Several AI models were applied and trained for detection and severity of pulmonary infection. The severity of the lung infection, as assigned by human annotators based on the LUS videos, is not significantly different between COVID-19 positive and negative patients (p = 0.89). Nevertheless, the predictions of image-based AI models identify a COVID-19 infection with 65% accuracy when applied zero-shot (i.e., trained on other datasets), and up to 79% with targeted training, whereas the accuracy based on human annotations is at most 65%. Multi-modal models combining images and CBC improve significantly over image-only models. Although our analysis generally supports the value of AI in LUS assessment, the evaluated models fall short of the performance expected from previous work. We find this is due to 1) the heterogeneity of LUS datasets, limiting the generalization ability to new data, 2) the frame-based processing of AI models ignoring video-level information, and 3) lack of work on multi-modal models that can extract the most relevant information from video-, image- and variable-based inputs. To aid future research, we publish the dataset at: https://github.com/NinaWie/COVID-BLUES.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations</title>
<link>https://arxiv.org/abs/2509.10659</link>
<guid>https://arxiv.org/abs/2509.10659</guid>
<content:encoded><![CDATA[
<div> segmentation, hierarchical network, mesh-based, graph neural networks, PDE simulations

Summary:
M4GN is a hierarchical network for mesh-based graph neural networks, addressing challenges of high cost and over-smoothing on large meshes. It utilizes a hybrid segmentation strategy to create contiguous segments of nodes that respect mesh topology and geometry. The segments are encoded by a permutation-invariant aggregator to capture local dynamics efficiently. M4GN incorporates a micro-level GNN and a macro-level transformer to balance accuracy and efficiency. Results show an improvement in prediction accuracy of up to 56% with up to 22% faster inference compared to state-of-the-art baselines. <div>
arXiv:2509.10659v1 Announce Type: cross 
Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for PDE simulations, yet their deep message passing incurs high cost and over-smoothing on large, long-range meshes; hierarchical GNNs shorten propagation paths but still face two key obstacles: (i) building coarse graphs that respect mesh topology, geometry, and physical discontinuities, and (ii) maintaining fine-scale accuracy without sacrificing the speed gained from coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric hierarchical network. M4GN begins with a hybrid segmentation strategy that pairs a fast graph partitioner with a superpixel-style refinement guided by modal-decomposition features, producing contiguous segments of dynamically consistent nodes. These segments are encoded by a permutation-invariant aggregator, avoiding the order sensitivity and quadratic cost of aggregation approaches used in prior works. The resulting information bridges a micro-level GNN, which captures local dynamics, and a macro-level transformer that reasons efficiently across segments, achieving a principled balance between accuracy and efficiency. Evaluated on multiple representative benchmark datasets, M4GN improves prediction accuracy by up to 56% while achieving up to 22% faster inference than state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data</title>
<link>https://arxiv.org/abs/2509.11053</link>
<guid>https://arxiv.org/abs/2509.11053</guid>
<content:encoded><![CDATA[
<div> conditional consistent latent representation, generative adversarial network, contrastive learning, 1D fourier convolution neural network, bearing fault diagnosis

Summary:<br />
- The article proposes a novel framework, DAC-FCF, for bearing fault diagnosis using limited data by addressing data scarcity and model limitations.
- It introduces a Conditional Consistent Latent Representation Generative Adversarial Network (CCLR-GAN) to generate diverse data and a contrastive learning mechanism for better modeling relationships between training samples.
- The framework utilizes a 1D Fourier Convolution Neural Network (1D-FCNN) to extract global features from complex vibration signals.
- Experimental results on the CWRU dataset and a self-collected test bench show that DAC-FCF outperforms baselines by up to 32% and 10% respectively.
- Ablation experiments confirm the effectiveness of the proposed components, demonstrating the promising potential of DAC-FCF for bearing fault diagnosis under limited data.<br /><br />Summary: <div>
arXiv:2509.11053v1 Announce Type: cross 
Abstract: In the area of bearing fault diagnosis, deep learning (DL) methods have been widely used recently. However, due to the high cost or privacy concerns, high-quality labeled data are scarce in real world scenarios. While few-shot learning has shown promise in addressing data scarcity, existing methods still face significant limitations in this domain. Traditional data augmentation techniques often suffer from mode collapse and generate low-quality samples that fail to capture the diversity of bearing fault patterns. Moreover, conventional convolutional neural networks (CNNs) with local receptive fields makes them inadequate for extracting global features from complex vibration signals. Additionally, existing methods fail to model the intricate relationships between limited training samples. To solve these problems, we propose an advanced data augmentation and contrastive fourier convolution framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a novel conditional consistent latent representation and reconstruction generative adversarial network (CCLR-GAN) is proposed to generate more diverse data. Secondly, a contrastive learning based joint optimization mechanism is utilized to better model the relations between the available training data. Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF achieves significant improvements, outperforming baselines by up to 32\% on case western reserve university (CWRU) dataset and 10\% on a self-collected test bench. Extensive ablation experiments prove the effectiveness of the proposed components. Thus, the proposed DAC-FCF offers a promising solution for bearing fault diagnosis under limited data.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations</title>
<link>https://arxiv.org/abs/2509.11376</link>
<guid>https://arxiv.org/abs/2509.11376</guid>
<content:encoded><![CDATA[
<div> framework, multimodal data fusion, reservoir analysis, AI models, operational efficiency

Summary:
The study presents an integrated framework utilizing cutting-edge AI models and advanced prompt engineering techniques for reservoir management in the petroleum industry. The framework combines large language models with multimodal data fusion to analyze seismic interpretations, well logs, and production data for real-time decision support. Field validation across 15 reservoir environments shows high accuracy in reservoir characterization, production forecasting, and well placement optimization. The system achieves rapid response times and high safety reliability with significant cost reductions compared to traditional methods. Few-shot learning and prompt optimization improve field adaptation time and reasoning quality. Real-time data processing includes anomaly detection and reduces environmental incidents. The research highlights the practical integration of AI technologies with domain expertise to enhance operational efficiency, safety, and economic performance. 

<br /><br />Summary: <div>
arXiv:2509.11376v1 Announce Type: cross 
Abstract: The petroleum industry faces unprecedented challenges in reservoir management, requiring rapid integration of complex multimodal datasets for real-time decision support. This study presents a novel integrated framework combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data fusion for comprehensive reservoir analysis. The framework implements domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum engineering documents, chain-of-thought reasoning, and few-shot learning for rapid field adaptation. Multimodal integration processes seismic interpretations, well logs, and production data through specialized AI models with vision transformers. Field validation across 15 diverse reservoir environments demonstrates exceptional performance: 94.2% reservoir characterization accuracy, 87.6% production forecasting precision, and 91.4% well placement optimization success rate. The system achieves sub-second response times while maintaining 96.2% safety reliability with no high-risk incidents during evaluation. Economic analysis reveals 62-78% cost reductions (mean 72%) relative to traditional methods with 8-month payback period. Few-shot learning reduces field adaptation time by 72%, while automated prompt optimization achieves 89% improvement in reasoning quality. The framework processed real-time data streams with 96.2% anomaly detection accuracy and reduced environmental incidents by 45%. We provide detailed experimental protocols, baseline comparisons, ablation studies, and statistical significance testing to ensure reproducibility. This research demonstrates practical integration of cutting-edge AI technologies with petroleum domain expertise for enhanced operational efficiency, safety, and economic performance.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.11420</link>
<guid>https://arxiv.org/abs/2509.11420</guid>
<content:encoded><![CDATA[
<div> Trading-R1, reasoning LLMs, financial decision making, risk-sensitive, supervised fine-tuning<br />
<br />
Summary: 
Trading-R1 is a financially-aware model designed to improve reasoning and decision-making in finance, addressing the need for interpretable and trustworthy AI in the market. It incorporates strategic thinking and planning, aligned with trading principles, through a comprehensive thesis composition and volatility-adjusted decision-making process. The model is trained on a diverse dataset and shows improved risk-adjusted returns and lower drawdowns compared to other models. By generating evidence-based investment theses, Trading-R1 supports structured and interpretable trading decisions. The system's approach combines supervised fine-tuning and reinforcement learning, following a three-stage curriculum. This innovative model fills a gap in applying reasoning LLMs to risk-sensitive financial tasks, providing a valuable tool for traders and analysts. Trading-R1 Terminal is available for access on GitHub, offering a practical application for financial professionals. <br /><br /> <div>
arXiv:2509.11420v1 Announce Type: cross 
Abstract: Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at https://github.com/TauricResearch/Trading-R1.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Requirements for Early Quantum Advantage and Quantum Utility in the Capacitated Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2509.11469</link>
<guid>https://arxiv.org/abs/2509.11469</guid>
<content:encoded><![CDATA[
<div> framework, Capacitated Vehicle Routing Problem, early quantum advantage, NISQ hardware, encoding <br />
Summary: 
This study introduces a framework for assessing the potential for early quantum advantage in solving the Capacitated Vehicle Routing Problem (CVRP). The analysis suggests that achieving quantum advantage on noisy intermediate scale quantum (NISQ) hardware is unlikely, even with the most qubit-efficient encoding methods. Through closed-form resource calculations and device benchmarks, key figures of merit such as the quantum feasibility point, qubit-feasibility line, and gate-feasibility line are identified to evaluate the feasibility of solving CVRP on quantum devices. A comparison of direct QUBO mapping and a space-efficient HOBO encoding reveals significant differences in resource requirements. The framework highlights that CVRP instances might need innovative problem decomposition techniques to leverage quantum devices effectively. Additionally, benchmarking on early-advantage instances like Golden-5 shows that HOBO circuits outperform QUBO encodings in terms of qubit utilization. <div>
arXiv:2509.11469v1 Announce Type: cross 
Abstract: We introduce a transparent, encoding-agnostic framework for determining when the Capacitated Vehicle Routing Problem (CVRP) can achieve early quantum advantage. Our analysis shows this is unlikely on noisy intermediate scale quantum (NISQ) hardware even in best case scenarios that use the most qubit-efficient direct encodings. Closed-form resource counts, combined with recent device benchmarks, yield three decisive go/no-go figures of merit: the quantum feasibility point and the qubit- and gate-feasibility lines, which place any CVRP instance on a single decision diagram. Contrasting a direct QUBO mapping with a space-efficient higher-order (HOBO) encoding reveals a large gap. Applied to early-advantage benchmarks such as Golden-5, our diagram shows that HOBO circuits require only 7,685 qubits, whereas comparable QUBO encodings still exceed 200,000 qubits. In addition to identifying candidate instances for early quantum advantage in CVRP, the framework provides a unifying go/no-go metric that ingests any CVRP encoding together with any hardware profile and highlights when quantum devices could challenge classical heuristics. Quantum advantage in CVRP would likely require innovative problem decomposition techniques.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMLNet: A Knowledge-Based Multi-Agent Framework to Generate and Detect Realistic Money Laundering Transactions</title>
<link>https://arxiv.org/abs/2509.11595</link>
<guid>https://arxiv.org/abs/2509.11595</guid>
<content:encoded><![CDATA[
<div> transaction datasets, AML research, AMLNet, synthetic transactions, detection ensemble<br />
<br />
Summary: A new framework called AMLNet has been introduced to address the lack of publicly shareable transaction datasets for anti-money laundering (AML) research. AMLNet consists of a regulation-aware transaction generator producing over 1 million synthetic transactions covering various money laundering phases and typologies. The generated transactions have a high regulatory alignment and technical fidelity score. The detection ensemble within AMLNet achieves high performance on internal test partitions and shows adaptability to external datasets. The framework enables multi-dimensional evaluation and the release of the dataset for reproducible and regulation-conscious AML experimentation. <div>
arXiv:2509.11595v1 Announce Type: cross 
Abstract: Anti-money laundering (AML) research is constrained by the lack of publicly shareable, regulation-aligned transaction datasets. We present AMLNet, a knowledge-based multi-agent framework with two coordinated units: a regulation-aware transaction generator and an ensemble detection pipeline. The generator produces 1,090,173 synthetic transactions (approximately 0.16\% laundering-positive) spanning core laundering phases (placement, layering, integration) and advanced typologies (e.g., structuring, adaptive threshold behavior). Regulatory alignment reaches 75\% based on AUSTRAC rule coverage (Section 4.2), while a composite technical fidelity score of 0.75 summarizes temporal, structural, and behavioral realism components (Section 4.4). The detection ensemble achieves F1 0.90 (precision 0.84, recall 0.97) on the internal test partitions of AMLNet and adapts to the external SynthAML dataset, indicating architectural generalizability across different synthetic generation paradigms. We provide multi-dimensional evaluation (regulatory, temporal, network, behavioral) and release the dataset (Version 1.0, https://doi.org/10.5281/zenodo.16736515), to advance reproducible and regulation-conscious AML experimentation.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing</title>
<link>https://arxiv.org/abs/2402.16445</link>
<guid>https://arxiv.org/abs/2402.16445</guid>
<content:encoded><![CDATA[
<div> Keywords: Protein Language Models, Protein Engineering, ProLLaMA, Evolutionary Protein Generation Framework, Multitask Model

Summary:
ProLLaMA is a multitask protein language model that addresses the limitations of current Protein Language Models in both Protein Language Understanding and Generation. It is enhanced by the Evolutionary Protein Generation Framework (EPGF) and trained on a comprehensive dataset with superfamily annotations. ProLLaMA excels in both unconditional and controllable protein generation tasks, displaying superior structural quality metrics. It also demonstrates strong understanding capabilities with a high exact match rate in superfamily prediction. The EPGF significantly improves the biological viability of generated sequences, as shown by enhanced biophysical scores and structural metrics. This model bridges the gap between PLU and PLG, boosting progress in protein engineering. The project code is available on GitHub for further exploration and development. <br /><br />Summary: <div>
arXiv:2402.16445v3 Announce Type: replace 
Abstract: Recent advances in Protein Language Models (PLMs) have transformed protein engineering, yet unlike their counterparts in Natural Language Processing (NLP), current PLMs exhibit a fundamental limitation: they excel in either Protein Language Understanding (PLU) or Protein Language Generation (PLG), but rarely both. This fragmentation hinders progress in protein engineering. To bridge this gap, we introduce ProLLaMA, a multitask protein language model enhanced by the Evolutionary Protein Generation Framework (EPGF). We construct a comprehensive instruction dataset containing approximately 13 million samples with over 11,000 superfamily annotations to facilitate better modeling of sequence-function landscapes. We leverage a two-stage training approach to develop ProLLaMA, a multitask LLM with protein domain expertise. Our EPGF addresses the mismatch between statistic language modeling and biological constraints through three innovations: a multi-dimensional interpretable scorer, hierarchical efficient decoding, and a probabilistic-biophysical joint selection mechanism. Extensive experiments demonstrate that ProLLaMA excels in both unconditional and controllable protein generation tasks, achieving superior structural quality metrics compared to existing PLMs. Additionally, ProLLaMA demonstrates strong understanding capabilities with a 67.1% exact match rate in superfamily prediction. EPGF significantly enhances the biological viability of generated sequences, as evidenced by improved biophysical scores (+4.3%) and structural metrics (+14.5%). The project is available at https://github.com/PKU-YuanGroup/ProLLaMA.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Machine Learning Models for Predicting the Next Targets of Activist Funds</title>
<link>https://arxiv.org/abs/2404.16169</link>
<guid>https://arxiv.org/abs/2404.16169</guid>
<content:encoded><![CDATA[
<div> activist investment, predictive model, machine learning, Shapley value, corporate governance<br />
<br />
Summary: This research introduces a predictive model to identify potential targets of activist investment funds, essential for companies to reduce intervention risks, activist funds to make optimal investments, and investors to capitalize on stock price gains. Evaluating 123 model configurations using data from the Russell 3000 index, the best model achieved an AUC-ROC of 0.782, showcasing its ability to predict activist fund targets effectively. The Shapley value method was employed to determine key factors influencing a company's likelihood of being targeted, shedding light on the dynamic mechanisms behind activist fund target selection. These findings provide valuable insights for proactive corporate governance and informed investment strategies, contributing to a deeper understanding of the factors driving activist investment decisions. <br /><br /> <div>
arXiv:2404.16169v3 Announce Type: replace 
Abstract: This research presents a predictive model to identify potential targets of activist investment funds--entities that acquire significant corporate stakes to influence strategic and operational decisions, ultimately enhancing shareholder value. Predicting such targets is crucial for companies aiming to mitigate intervention risks, activist funds seeking optimal investments, and investors looking to leverage potential stock price gains. Using data from the Russell 3000 index from 2016 to 2022, we evaluated 123 model configurations incorporating diverse imputation, oversampling, and machine learning techniques. Our best model achieved an AUC-ROC of 0.782, demonstrating its capability to effectively predict activist fund targets. To enhance interpretability, we employed the Shapley value method to identify key factors influencing a company's likelihood of being targeted, highlighting the dynamic mechanisms underlying activist fund target selection. These insights offer a powerful tool for proactive corporate governance and informed investment strategies, advancing understanding of the mechanisms driving activist investment decisions.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plastic Arbor: a modern simulation framework for synaptic plasticity -- from single synapses to networks of morphological neurons</title>
<link>https://arxiv.org/abs/2411.16445</link>
<guid>https://arxiv.org/abs/2411.16445</guid>
<content:encoded><![CDATA[
<div> Keywords: Arbor, neuronal networks, synaptic plasticity, computational modeling, dendritic structures 

Summary: 
Arbor is a software library for simulating large-scale networks of biological neurons with detailed morphological structures. It supports multi-core CPU and GPU systems, combining customizable neuronal and synaptic mechanisms for efficient simulation. The library has been extended to model a variety of spike-driven plasticity paradigms, from single-synapse dynamics to large recurrent networks. By comparing with other simulators, it is shown that Arbor allows simulating plastic networks of multi-compartment neurons at minimal runtime cost. The extension also demonstrates high efficiency in terms of runtime and memory usage. Using this framework, the impact of dendritic structures on network dynamics over hours is investigated, revealing a relationship between dendritic tree length and information storage efficiency. This extended Arbor framework provides a valuable tool for future studies on the influence of synaptic plasticity in large networks, particularly in conjunction with neuronal morphology.<br /><br />Summary: <div>
arXiv:2411.16445v3 Announce Type: replace 
Abstract: Arbor is a software library designed for efficient simulation of large-scale networks of biological neurons with detailed morphological structures. It combines customizable neuronal and synaptic mechanisms with high-performance computing, supporting multi-core CPU and GPU systems. In humans and other animals, synaptic plasticity processes play a vital role in cognitive functions, including learning and memory. Recent studies have shown that intracellular molecular processes in dendrites significantly influence single-neuron dynamics. However, for understanding how the complex interplay between dendrites and synaptic processes influences network dynamics, computational modeling is required.
  To enable the modeling of large-scale networks of morphologically detailed neurons with diverse plasticity processes, we have extended the Arbor library to support simulations of a large variety of spike-driven plasticity paradigms. To showcase the features of the extended framework, we present examples of computational models, beginning with single-synapse dynamics, progressing to multi-synapse rules, and finally scaling up to large recurrent networks. While cross-validating our implementations by comparison with other simulators, we show that Arbor allows simulating plastic networks of multi-compartment neurons at nearly no additional cost in runtime compared to point-neuron simulations. In addition, we demonstrate that Arbor is highly efficient in terms of runtime and memory use as compared to other simulators. Using the extended framework, as an example, we investigate the impact of dendritic structures on network dynamics across a timescale of several hours, finding a relation between the length of dendritic trees and the ability of the network to efficiently store information. By our extension of Arbor, we aim to provide a valuable tool that will support future studies on the impact of synaptic plasticity, especially, in conjunction with neuronal morphology, in large networks.
]]></content:encoded>
<pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superstructure Optimization with Embedded Neural Networks for Sustainable Aviation Fuel Production</title>
<link>https://arxiv.org/abs/2509.09796</link>
<guid>https://arxiv.org/abs/2509.09796</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-objective optimization, sustainable aviation fuel, artificial neural networks, Fischer-Tropsch kerosene production, carbon emissions constraints

Summary: 
This study introduces a novel framework for sustainable aviation fuel (SAF) production that combines artificial neural networks (ANNs) with mathematical optimization techniques. By integrating ANNs into a mixed-integer quadratically constrained programming (MIQCP) formulation, the framework allows for simultaneous optimization of discrete process choices and continuous operating parameters. The application of this framework to Fischer-Tropsch kerosene production reveals that configurations minimizing costs under unconstrained CO2 emissions favor fossil-based autothermal reforming (ATR). However, imposing carbon emission constraints leads to the integration of biomass gasification and direct air capture coupled with carbon sequestration (DAC-CS) for reduced net emissions at higher production costs. Hybrid configurations with flexible process parameters, enabled by embedded ANNs, outperform fixed setups and achieve cost savings of up to 20%. Sensitivity analyses show the impact of process conditions on economic and environmental performance, emphasizing the importance of process adaptability in SAF production. 

<br /><br />Summary: <div>
arXiv:2509.09796v1 Announce Type: new 
Abstract: This study presents a multi-objective optimization framework for sustainable aviation fuel (SAF) production, integrating artificial neural networks (ANNs) within a mixed-integer quadratically constrained programming (MIQCP) formulation. By embedding data-driven surrogate models into the mathematical optimization structure, the proposed methodology addresses key limitations of conventional superstructure-based approaches, enabling simultaneous optimization of discrete process choices and continuous operating parameters. The framework captures variable input and output stream compositions, facilitating the joint optimization of target product composition and system design. Application to Fischer-Tropsch (FT) kerosene production demonstrates that cost-minimizing configurations under unconstrained CO2 emissions are dominated by the fossil-based autothermal reforming (ATR) route. Imposing carbon emission constraints necessitates the integration of biomass gasification and direct air capture coupled with carbon sequestration (DAC-CS), resulting in substantially reduced net emissions but higher production costs. At the zero-emission limit, hybrid configurations combining ATR and biomass gasification achieve the lowest costs (~2.38 \$/kg-kerosene), followed closely by biomass gasification-only (~2.43 \$/kg), both of which outperform the ATR-only pathway with DAC-CS (~2.65 \$/kg). In contrast, DAC-only systems relying exclusively on atmospheric CO2 and water electrolysis are prohibitively expensive (~10.8 \$/kg). The results highlight the critical role of process adaptability: configurations exploiting flexible process parameters, facilitated by embedded ANNs, consistently outperform fixed setups, achieving up to 20% cost savings. Sensitivity analyses elucidate the influence of process conditions, such as FT reactor pressure and gasification temperature, on economic and environmental performance.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fraud detection and risk assessment of online payment transactions on e-commerce platforms based on LLM and GCN frameworks</title>
<link>https://arxiv.org/abs/2509.09928</link>
<guid>https://arxiv.org/abs/2509.09928</guid>
<content:encoded><![CDATA[
<div> Keywords: e-commerce, fraud detection, Large Language Models, Graph Convolutional Networks, online payment

Summary: 
This study presents a novel fraud detection framework for e-commerce online payment transactions that combines Large Language Models (LLM) with Graph Convolutional Networks (GCN). A dataset of 2,840,000 transactions involving consumers and merchants was used to train the model, addressing the imbalanced nature of fraud instances in the data. The model represents consumers and merchants as nodes and transactions as edges in a heterogeneous graph, enabling the GCN to learn complex behavioral patterns. By integrating semantic features from GPT-4o and Tabformer with structural features, the model achieves an accuracy of 0.98 in fraud detection, balancing precision and sensitivity effectively. This framework offers a scalable, real-time solution for securing online payment environments and demonstrates the potential of graph-based deep learning in financial fraud prevention.

<br /><br />Summary: <div>
arXiv:2509.09928v1 Announce Type: new 
Abstract: With the rapid growth of e-commerce, online payment fraud has become increasingly complex, posing serious threats to financial security and consumer trust. Traditional detection methods often struggle to capture the intricate relational structures inherent in transactional data. This study presents a novel fraud detection framework that combines Large Language Models (LLM) with Graph Convolutional Networks (GCN) to effectively identify fraudulent activities in e-commerce online payment transactions. A dataset of 2,840,000 transactions was collected over 14 days from major platforms such as Amazon, involving approximately 2,000 U.S.-based consumers and 30 merchants. With fewer than 6000 fraudulent instances, the dataset represents a highly imbalanced scenario. Consumers and merchants were modeled as nodes and transactions as edges to form a heterogeneous graph, upon which a GCN was applied to learn complex behavioral patterns. Semantic features extracted via GPT-4o and Tabformer were integrated with structural features to enhance detection performance. Experimental results demonstrate that the proposed model achieves an accuracy of 0.98, effectively balancing precision and sensitivity in fraud detection. This framework offers a scalable and real-time solution for securing online payment environments and provides a promising direction for applying graph-based deep learning in financial fraud prevention.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading</title>
<link>https://arxiv.org/abs/2509.09995</link>
<guid>https://arxiv.org/abs/2509.09995</guid>
<content:encoded><![CDATA[
<div> Language Models, Financial Reasoning, High-Frequency Trading, Multi-Agent Framework, Algorithmic Trading 
<br />
Summary:
QuantAgent is a novel multi-agent Large Language Model (LLM) framework designed specifically for high-frequency algorithmic trading. It consists of four specialized agents - Indicator, Pattern, Trend, and Risk - each equipped with tools and reasoning capabilities tailored for short-term market dynamics. In evaluations across various financial instruments, QuantAgent outperformed neural and rule-based baselines in terms of predictive accuracy and cumulative return over 4-hour trading intervals. By combining structured financial priors with language-native reasoning, QuantAgent demonstrates the potential for creating real-time decision systems in high-frequency financial markets. <div>
arXiv:2509.09995v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks, leveraging fundamental and sentiment-based inputs for strategic decision-making. However, such systems are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT requires rapid, risk-aware decisions based on structured, short-horizon signals, including technical indicators, chart patterns, and trend-based features, distinct from the long-term semantic reasoning typical of traditional financial LLM applications. To this end, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents, Indicator, Pattern, Trend, and Risk, each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. In zero-shot evaluations across ten financial instruments, including Bitcoin and Nasdaq futures, QuantAgent demonstrates superior performance in both predictive accuracy and cumulative return over 4-hour trading intervals, outperforming strong neural and rule-based baselines. Our findings suggest that combining structured financial priors with language-native reasoning unlocks new potential for traceable, real-time decision systems in high-frequency financial markets.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Freight Rail Electrification: A Framework for Charge Station Selection and Battery Charge/Swap Scheduling</title>
<link>https://arxiv.org/abs/2509.10157</link>
<guid>https://arxiv.org/abs/2509.10157</guid>
<content:encoded><![CDATA[
<div> Keywords: battery electric freight trains, charging infrastructure, charge scheduling, optimization, algorithms <br />
Summary: <br />
Battery electric freight trains play a vital role in achieving decarbonization goals through zero-emission transportation options. The study focuses on developing an efficient strategy for the adoption of battery electric freight trains, involving the optimal design of charging infrastructure and charge scheduling for each train. The model allows for flexibility by enabling batteries to be either charged or swapped at deployed stations, with each train able to carry multiple batteries. The problem is formulated as a mixed integer linear programming model. Three algorithms are proposed to solve the optimization problem, including a Rectangle Piecewise Linear Approximation technique, a Fixed Algorithm heuristic, and a Benders Decomposition algorithm. Computational experiments show that the Benders Decomposition algorithm outperforms the other two algorithms in terms of objective function value, with the Rectangle Piecewise Linear Approximation technique closely following. The Fixed Algorithm provides the least optimal solution. <div>
arXiv:2509.10157v1 Announce Type: new 
Abstract: Battery electric freight trains are crucial for decarbonization by providing zero-emission transportation alternatives. The proper adoption of battery electric freight trains depends on an efficient battery electrification strategy, involving both infrastructure setup and charge scheduling. The study presents a comprehensive model for the optimal design of charging infrastructure and charge scheduling for each train. To provide more refueling flexibility, we allow batteries to be either charged or swapped in a deployed station, and each train can carry multiple batteries. This problem is formulated as a mixed integer linear programming model. To obtain real-time solutions for a large scale network, we develop three algorithms to solve the optimization problem: (1) a Rectangle Piecewise Linear Approximation technique, (2) a Fixed Algorithm heuristic, and (3) Benders Decomposition algorithm. In computational experiments, we use the three proposed algorithms to solve instances with up to 25 stations. Statistical analysis verifies that Benders Decomposition outperforms the other two algorithms with respect to the objective function value, closely followed by the Rectangle Piecewise Linear Approximation technique, and the Fixed Algorithm provides the least optimal solution.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Flow Separation Control Strategies in 3D Wings via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.10185</link>
<guid>https://arxiv.org/abs/2509.10185</guid>
<content:encoded><![CDATA[
<div> keywords: deep reinforcement learning, active flow control, SD7003 wing, aerodynamic efficiency, turbulence <br />
Summary:
In this study, deep reinforcement learning (DRL) is utilized to optimize active flow control (AFC) on a three-dimensional SD7003 wing at specific aerodynamic conditions. The uncontrolled baseline case exhibits significant flow separation and a turbulent wake. Through the use of a GPU-accelerated CFD solver and multi-agent training, DRL successfully identifies control strategies that significantly enhance lift (79%), reduce drag (65%), and improve aerodynamic efficiency (408%). Flow visualizations validate the reattachment of the separated shear layer, highlighting the potential of DRL in tackling complex and turbulent flows. This research showcases the effectiveness of DRL in improving aerodynamic performance by discovering optimized control strategies in challenging flow conditions. <br /><br />Summary: <div>
arXiv:2509.10185v1 Announce Type: new 
Abstract: In this work, deep reinforcement learning (DRL) is applied to active flow control (AFC) over a threedimensional SD7003 wing at a Reynolds number of Re = 60,000 and angle of attack of AoA = 14 degrees. In the uncontrolled baseline case, the flow exhibits massive separation and a fully turbulent wake. Using a GPU-accelerated CFD solver and multi-agent training, DRL discovers control strategies that enhance lift (79%), reduce drag (65%), and improve aerodynamic efficiency (408%). Flow visualizations confirm reattachment of the separated shear layer, demonstrating the potential of DRL for complex and turbulent flows.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Active Flow Control around a Three-Dimensional Flow-Separated Wing at Re = 1,000</title>
<link>https://arxiv.org/abs/2509.10195</link>
<guid>https://arxiv.org/abs/2509.10195</guid>
<content:encoded><![CDATA[
<div> Keywords: deep reinforcement learning, active flow control, NACA0012 wing, computational fluid dynamics, aerodynamic performance

Summary:<br /><br />
This study investigates the application of deep reinforcement learning (DRL) for active flow control (AFC) to mitigate flow separation on wings at high angles of attack. The DRL agent autonomously adjusts the flow over a three-dimensional NACA0012 wing section at Re = 1,000 and AoA = 20 degrees by analyzing real-time flow data and utilizing a reward function focused on enhancing aerodynamic performance. By integrating the GPU-accelerated computational fluid dynamics (CFD) solver SOD2D with the TF-Agents DRL library through a Redis in-memory database, the framework enables efficient training. This research showcases the potential of DRL in addressing intricate aerodynamic challenges and pushing the boundaries of conventional AFC techniques. The study underscores the effectiveness of DRL in optimizing control actions to improve aerodynamic performance on wing sections, thereby advancing the field of flow control in aerodynamics. <div>
arXiv:2509.10195v1 Announce Type: new 
Abstract: This study explores the use of deep reinforcement learning (DRL) for active flow control (AFC) to reduce flow separation on wings at high angles of attack. Concretely, here the DRL agent controls the flow over the three-dimensional NACA0012 wing section at the Reynolds number Re = 1,000 and angle of attack AoA = 20 degrees, autonomously identifying optimal control actions through real-time flow data and a reward function focused on improving aerodynamic performance. The framework integrates the GPU-accelerated computational fluid dynamics (CFD) solver SOD2D with the TF-Agents DRL library via a Redis in-memory database, enabling rapid training. This work builds on previous DRL flow-control studies, demonstrating DRL potential to address complex aerodynamic challenges and push the boundaries of traditional AFC methods.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TubeBEND: A Real-World Dataset for Geometry Prediction in Rotary Draw Bending</title>
<link>https://arxiv.org/abs/2509.10272</link>
<guid>https://arxiv.org/abs/2509.10272</guid>
<content:encoded><![CDATA[
<div> Dataset, TubeBEND, rotary tube bending processes, machine learning, signal analysis<br />
<br />
Summary: <br />
This paper introduces TubeBEND, a dataset consisting of 318 rotary tube bending processes curated by experts. The dataset aims to solve the industrial challenge of predicting the geometry of a first-stage bend to optimize machine clamping molds for the second-stage bend in two-stage rotary draw bending. It includes criteria such as final bent angle and cross-sectional deformation of the tube. The dataset enables the development and testing of machine learning models to predict tube geometry, aiding machine operators in optimizing springback and deformation. By recording process parameters like tool movements and forces, the dataset provides detailed information on their effects on tube geometry. The goal is to explore solutions that leverage experimental process variables in machine learning algorithms to replace traditional trial-and-error or simulation-based methods. The dataset is publicly available for further research and improvement of data-driven approaches in the domain. <div>
arXiv:2509.10272v1 Announce Type: new 
Abstract: This paper presents TubeBEND, a real-world dataset comprising 318 rotary tube bending processes, which were collected and sorted by experts from various fields to evaluate machine learning and signal analysis methods. The dataset addresses the industrial challenge of predicting the geometry of a first-stage bend, which can be beneficial for designing machine clamping molds for the second-stage bend in two-stage rotary draw bending. Some geometry criteria, such as the tube's final bent angle (or springback) and its cross-sectional deformation, are being recorded in this dataset. This dataset gives us the possibility to build and test machine learning models that can predict the geometry and help the machine operators with a better machine setup to optimize the tube's springback and deformation. Moreover, by recording some process parameters, such as tool movements and forces or torques applied to them, we deliver detailed information about their impacts on the final tube geometry. The focus of our work is to discover solutions that can replace traditional methods, such as trial-and-error or simulation-based predictions, by including experimental process variables in ML algorithms. Our dataset is publicly available at https://github.com/zeyneddinoz/tubebend and https://zenodo.org/records/16614082 as a benchmark to improve data-driven methods in this field.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs</title>
<link>https://arxiv.org/abs/2509.09727</link>
<guid>https://arxiv.org/abs/2509.09727</guid>
<content:encoded><![CDATA[
<div> framework, financial education, question answering, multi-agent, domain-specific 

Summary: 
The article introduces a multi-agent framework for financial question answering in education. Existing large language models often struggle with the nuanced reasoning required in finance. The proposed framework includes a Base Generator, Evidence Retriever, and Expert Reviewer agent to enhance domain-specific QA. By leveraging retrieval-augmented generation and prompting strategies, the framework improves answer accuracy by 6.6-8.3% over baselines. The Gemini-2.0-Flash model shows the highest performance. Additionally, the method enables GPT-4o-mini to achieve performance comparable to FinGPT-mt_Llama3-8B_LoRA. The results suggest a cost-effective approach to enhancing financial QA and provide insights for future research in multi-agent financial language model systems. 

<br /><br />Summary: <div>
arXiv:2509.09727v1 Announce Type: cross 
Abstract: Question answering (QA) plays a central role in financial education, yet existing large language model (LLM) approaches often fail to capture the nuanced and specialized reasoning required for financial problem-solving. The financial domain demands multistep quantitative reasoning, familiarity with domain-specific terminology, and comprehension of real-world scenarios. We present a multi-agent framework that leverages role-based prompting to enhance performance on domain-specific QA. Our framework comprises a Base Generator, an Evidence Retriever, and an Expert Reviewer agent that work in a single-pass iteration to produce a refined answer. We evaluated our framework on a set of 3,532 expert-designed finance education questions from Study.com, an online learning platform. We leverage retrieval-augmented generation (RAG) for contextual evidence from 6 finance textbooks and prompting strategies for a domain-expert reviewer. Our experiments indicate that critique-based refinement improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines, with the highest performance from Gemini-2.0-Flash. Furthermore, our method enables GPT-4o-mini to achieve performance comparable to the finance-tuned FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to enhancing financial QA and offer insights for further research in multi-agent financial LLM systems.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing understanding and clinical applications of cerebral autoregulation: A novel integrated numerical framework</title>
<link>https://arxiv.org/abs/2509.10295</link>
<guid>https://arxiv.org/abs/2509.10295</guid>
<content:encoded><![CDATA[
<div> cerebral autoregulation, numerical algorithm, cerebral blood flow, hemodynamic parameters, personalized model 
Summary: 
This study introduces a novel numerical algorithm that incorporates key factors driving cerebral autoregulation (CA) to regulate cerebral blood flow (CBF). The algorithm utilizes partial and ordinary differential equations to capture spatial and temporal distributions of arterial pressure, oxygen, and carbon dioxide levels in the cerebral vasculature. Validation with two datasets confirms its reliability in simulating the regulatory effects of CA on CBF under various physiological conditions. By integrating with a personalized multi-dimensional model, this framework enhances our understanding of CA and offers potential for developing hemodynamic-based therapeutic strategies for cerebrovascular disorders. <div>
arXiv:2509.10295v1 Announce Type: cross 
Abstract: Cerebral autoregulation (CA) is a fundamental mechanism that modulates cerebrovascular resistance, primarily by regulating the diameter of small cerebral vessels to maintain stable cerebral blood flow (CBF) in response to fluctuations in systemic arterial pressure. However, the clinical understanding of CA remains limited due to the intricate structure of the cerebral vasculature and the challenges in accurately quantifying the hemodynamic and physiological parameters that govern this autoregulatory process. Method: In this study, we introduced a novel numerical algorithm that employs three partial differential equations and one ordinary differential equation to capture both the spatial and temporal distributions of key CA-driving factors, including the arterial pressure (P) and the partial pressures of oxygen (PO_2) and carbon dioxide (PCO_2) within the cerebral vasculature, together with a Windkessel model in turn to regulate the CBF based on the calculated P, PO_2, and PCO_2. This algorithm was sequentially integrated with our previously developed personalized 0D-1D multi-dimensional model to account for the patient-specific effects. Results: The integrated framework was rigorously validated using two independent datasets, demonstrating its high reliability and accuracy in capturing the regulatory effects of CA on CBF across a range of physiological conditions. Conclusion: This work significantly advances our understanding of CA and provides a promising foundation for developing hemodynamic-based therapeutic strategies aimed at improving clinical outcomes in patients with cerebrovascular disorders.
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetries in stochastic homogenization and acclimatizations for the RVE method</title>
<link>https://arxiv.org/abs/2509.08977</link>
<guid>https://arxiv.org/abs/2509.08977</guid>
<content:encoded><![CDATA[
<div> symmetry, microstructure, effective tensor, thermal conductivity, RVE method
<br />
Summary:
The study explores the impact of symmetry in a random microstructure on effective tensor and fluctuations in thermal conductivity. It investigates methods to enforce symmetries in postprocessing using orthogonal projectors. In the context of the Representative Volume Element (RVE) method, invariance conditions for effective tensor and fluctuations under different microstructure symmetry groups are established. It is found that the symmetry of the RVE cell type can disrupt ensemble symmetry, affecting effective property approximation. Strategies are introduced to enforce expected symmetries, reducing errors and improving accuracy. Theoretical arguments support the use of projections for unbiased variance reduction and exact symmetry enforcement. Large-scale simulations confirm the effectiveness of symmetry-projection techniques, especially in fiber-reinforced composites of industrial size. <div>
arXiv:2509.08977v1 Announce Type: new 
Abstract: We investigate the implications of a given symmetry of a random microstructure on the obtained effective tensor and its fluctuation in the context of thermal conductivity, and study strategies for enforcing these symmetries in postprocessing via orthogonal projectors. Within the framework of the representative volume element (RVE) method, we establish the invariance conditions for the effective tensor and its fluctuation under different symmetry groups of the microstructure. Interestingly, the symmetry of the considered cell type in the RVE method may break the ensemble symmetry and compromise the approximation of the effective properties. To rectify this issue, we introduce dedicated techniques which permit to enforce the expected symmetries in postprocessing and study the implications on the bounds for the effective properties as well as the total, the random and the systematic errors. We provide theoretical arguments that suitable projections lead to unbiased variance-reduction strategies which furthermore enforce the expected symmetries exactly. Through large-scale FFT-based homogenization simulations, we study the symmetry structure of the estimated effective conductivities and their fluctuations. Moreover, we demonstrate the power of the symmetry-projection techniques for fiber-reinforced composite microstructures of industrial scale.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Isogeometric Topology Optimization Based on Topological Derivatives</title>
<link>https://arxiv.org/abs/2509.09236</link>
<guid>https://arxiv.org/abs/2509.09236</guid>
<content:encoded><![CDATA[
<div> Topology optimization, isogeometric approach, topological derivatives, level-set method, immersed isogeometric framework <br />
<br />
Summary: In this work, an isogeometric approach to topology optimization driven by topological derivatives is proposed. This approach allows for seamless geometry updates without the need for remeshing. The combination of a level-set method and an immersed isogeometric framework enables topological modifications without the requirement of defining initial holes. The influence of higher-degree basis functions in both the level-set representation and solution approximation is investigated, showing that using higher-degree basis functions for the solution improves accuracy, while linear basis functions are sufficient for the level-set function representation. Two numerical examples are presented to demonstrate the effectiveness of the proposed approach. <div>
arXiv:2509.09236v1 Announce Type: cross 
Abstract: Topology optimization is a valuable tool in engineering, facilitating the design of optimized structures. However, topological changes often require a remeshing step, which can become challenging. In this work, we propose an isogeometric approach to topology optimization driven by topological derivatives. The combination of a level-set method together with an immersed isogeometric framework allows seamless geometry updates without the necessity of remeshing. At the same time, topological derivatives provide topological modifications without the need to define initial holes [7]. We investigate the influence of higher-degree basis functions in both the level-set representation and the approximation of the solution. Two numerical examples demonstrate the proposed approach, showing that employing higher-degree basis functions for approximating the solution improves accuracy, while linear basis functions remain sufficient for the level-set function representation.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified RIME algorithm with covariance learning and diversity enhancement for numerical optimization</title>
<link>https://arxiv.org/abs/2509.09529</link>
<guid>https://arxiv.org/abs/2509.09529</guid>
<content:encoded><![CDATA[
<div> Covariance learning, diversity enhancement, metaheuristic algorithm, optimization, RIME.<br />
Summary:<br />
The modified RIME algorithm with covariance learning and diversity enhancement (MRIME-CD) addresses the shortcomings of the RIME algorithm by introducing three key strategies. Firstly, a covariance learning strategy increases population diversity and balances exploitation and exploration abilities. Secondly, an average bootstrapping strategy guides population search in the early stage for better global search abilities. Lastly, a new stagnation indicator and stochastic covariance learning enhance the ability to escape local optima. Validation on test sets shows that MRIME-CD improves solution accuracy, convergence speed, and stability compared to basic RIME. The algorithm outperforms in terms of performance and demonstrates its effectiveness in various experiments. <br />Summary: <div>
arXiv:2509.09529v1 Announce Type: cross 
Abstract: Metaheuristics are widely applied for their ability to provide more efficient solutions. The RIME algorithm is a recently proposed physical-based metaheuristic algorithm with certain advantages. However, it suffers from rapid loss of population diversity during optimization and is prone to fall into local optima, leading to unbalanced exploitation and exploration. To address the shortcomings of RIME, this paper proposes a modified RIME with covariance learning and diversity enhancement (MRIME-CD). The algorithm applies three strategies to improve the optimization capability. First, a covariance learning strategy is introduced in the soft-rime search stage to increase the population diversity and balance the over-exploitation ability of RIME through the bootstrapping effect of dominant populations. Second, in order to moderate the tendency of RIME population to approach the optimal individual in the early search stage, an average bootstrapping strategy is introduced into the hard-rime puncture mechanism, which guides the population search through the weighted position of the dominant populations, thus enhancing the global search ability of RIME in the early stage. Finally, a new stagnation indicator is proposed, and a stochastic covariance learning strategy is used to update the stagnant individuals in the population when the algorithm gets stagnant, thus enhancing the ability to jump out of the local optimal solution. The proposed MRIME-CD algorithm is subjected to a series of validations on the CEC2017 test set, the CEC2022 test set, and the experimental results are analyzed using the Friedman test, the Wilcoxon rank sum test, and the Kruskal Wallis test. The results show that MRIME-CD can effectively improve the performance of basic RIME and has obvious superiorities in terms of solution accuracy, convergence speed and stability.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An improved educational competition optimizer with multi-covariance learning operators for global optimization problems</title>
<link>https://arxiv.org/abs/2509.09552</link>
<guid>https://arxiv.org/abs/2509.09552</guid>
<content:encoded><![CDATA[
<div> Keywords: educational competition optimizer, metaheuristic algorithm, multi-covariance learning operators, optimization problems, constrained optimization

Summary:
The study introduces an enhanced version of the educational competition optimizer (IECO-MCO) that utilizes multi-covariance learning operators to improve performance in tackling complex optimization problems. Three distinct covariance learning operators are introduced in IECO to balance exploitation and exploration effectively, preventing premature convergence. Experimental results using benchmark functions from CEC 2017 and CEC 2022 test suites show that IECO-MCO outperforms basic ECO and other algorithms in terms of convergence speed, stability, and avoiding local optima. Statistical analyses support the superiority of IECO-MCO. The algorithm demonstrates practical applicability in solving constrained optimization problems, showcasing its robustness and effectiveness in real-world scenarios.<br /><br />Summary: <div>
arXiv:2509.09552v1 Announce Type: cross 
Abstract: The educational competition optimizer is a recently introduced metaheuristic algorithm inspired by human behavior, originating from the dynamics of educational competition within society. Nonetheless, ECO faces constraints due to an imbalance between exploitation and exploration, rendering it susceptible to local optima and demonstrating restricted effectiveness in addressing complex optimization problems. To address these limitations, this study presents an enhanced educational competition optimizer (IECO-MCO) utilizing multi-covariance learning operators. In IECO, three distinct covariance learning operators are introduced to improve the performance of ECO. Each operator effectively balances exploitation and exploration while preventing premature convergence of the population. The effectiveness of IECO is assessed through benchmark functions derived from the CEC 2017 and CEC 2022 test suites, and its performance is compared with various basic and improved algorithms across different categories. The results demonstrate that IECO-MCO surpasses the basic ECO and other competing algorithms in convergence speed, stability, and the capability to avoid local optima. Furthermore, statistical analyses, including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test, are conducted to validate the superiority of IECO-MCO over the compared algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test suites. Additionally, the practical applicability of the proposed IECO-MCO algorithm is verified by solving constrained optimization problems. The experimental outcomes demonstrate the superior performance of IECO-MCO in tackling intricate optimization problems, underscoring its robustness and practical effectiveness in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Intraday Risk Factor Collections via Hierarchical Reinforcement Learning based on Transferred Options</title>
<link>https://arxiv.org/abs/2501.07274</link>
<guid>https://arxiv.org/abs/2501.07274</guid>
<content:encoded><![CDATA[
<div> Keywords: risk factors, stock return volatility, genetic programming, Hierarchical Proximal Policy Optimization, transfer learning

Summary:
The traditional risk factors used to measure and predict stock return volatility often lag behind market dynamics. Statistical models like PCA and factor analysis struggle to capture hidden nonlinear relationships. Genetic programming (GP) can identify nonlinear factors but lacks mechanisms for evaluating factor quality and results in complex formulas. In response to these challenges, the authors propose a Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor generation and evaluation. HPPO utilizes two PPO models: a high-level policy assigns weights to stock features, and a low-level policy identifies latent nonlinear relationships. The Pearson correlation between generated factors and return volatility serves as the reward signal. Transfer learning is employed to pre-train the high-level policy on historical data and fine-tune it with the latest data. Experimental results demonstrate that the HPPO-TO algorithm achieves a 25% excess return in HFT markets across China (CSI 300/800), India (Nifty 100), and the US (S&amp;P 500). <div>
arXiv:2501.07274v3 Announce Type: replace 
Abstract: Traditional risk factors like beta, size/value, and momentum often lag behind market dynamics in measuring and predicting stock return volatility. Statistical models like PCA and factor analysis fail to capture hidden nonlinear relationships. Genetic programming (GP) can identify nonlinear factors but often lacks mechanisms for evaluating factor quality, and the resulting formulas are complex. To address these challenges, we propose a Hierarchical Proximal Policy Optimization (HPPO) framework for automated factor generation and evaluation. HPPO uses two PPO models: a high-level policy assigns weights to stock features, and a low-level policy identifies latent nonlinear relationships. The Pearson correlation between generated factors and return volatility serves as the reward signal. Transfer learning pre-trains the high-level policy on large-scale historical data, fine-tuning it with the latest data to adapt to new features and shifts. Experiments show the HPPO-TO algorithm achieves a 25\% excess return in HFT markets across China (CSI 300/800), India (Nifty 100), and the US (S\&amp;P 500). Code and data are available at https://github.com/wencyxu/HRL-HF_risk_factor_set.
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Physics-Data Enrichments to Represent Uncertainty in Reduced Gas-Surface Chemistry Models for Hypersonic Flight</title>
<link>https://arxiv.org/abs/2509.08137</link>
<guid>https://arxiv.org/abs/2509.08137</guid>
<content:encoded><![CDATA[
<div> Reaction products, thermal protection system, ablation modeling, gas-surface chemistry, data-driven enrichments

Summary:
- During hypersonic flight, reactions with air deplete a re-entry vehicle's thermal protection system (TPS).
- Accurate ablation models are crucial for assessing TPS performance.
- New finite-rate gas-surface chemistry models are improving TPS ablation modeling.
- Model reductions may be necessary for computational tractability, but can lead to discrepancies in predicted carbon monoxide production.
- Hybrid physics-based and data-driven enrichments are developed to enhance predictive capability and quantify uncertainties in low-fidelity models.
- The enrichments significantly improve accuracy with the addition of only three reactions.<br /><br />Summary: <div>
arXiv:2509.08137v1 Announce Type: new 
Abstract: During hypersonic flight, air reacts with a planetary re-entry vehicle's thermal protection system (TPS), creating reaction products that deplete the TPS. Reliable assessment of TPS performance depends on accurate ablation models. New finite-rate gas-surface chemistry models are advancing state-of-the-art in TPS ablation modeling, but model reductions that omit chemical species and reactions may be necessary in some cases for computational tractability. This work develops hybrid physics-based and data-driven enrichments to improve the predictive capability and quantify uncertainties in such low-fidelity models while maintaining computational tractability. We focus on discrepancies in predicted carbon monoxide production that arise because the low-fidelity model tracks only a subset of reactions. To address this, we embed targeted enrichments into the low-fidelity model to capture the influence of omitted reactions. Numerical results show that the hybrid enrichments significantly improve predictive accuracy while requiring the addition of only three reactions.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving contact problems using Fiber Monte Carlo</title>
<link>https://arxiv.org/abs/2509.08609</link>
<guid>https://arxiv.org/abs/2509.08609</guid>
<content:encoded><![CDATA[
<div> contact algorithm, computational modeling, contact forces, geometric descriptors, finite element method <br />
Summary: 
This work presents a novel contact algorithm that computes contact forces by calculating the gradient of an energy function with respect to geometric descriptors. The algorithm, inspired by the Fiber Monte Carlo method, accurately computes contact forces for bodies with complex geometries, independent of mesh conformity. It eliminates the need for master-slave identification and projection iterations, making it easy to incorporate into existing numerical solvers like the finite element method. Various numerical examples demonstrate the algorithm's efficiency in handling a wide range of contact scenarios, from small-deformation static contact to large-deformation dynamic contact with nonlinear material behavior. Examples include Hertzian contact for small-deformation verification, contact between different-shaped bodies, contact with hyperelastic materials, and dynamic collision cases to examine transient behavior. <div>
arXiv:2509.08609v1 Announce Type: new 
Abstract: Computational modeling of contact is fundamental to many engineering applications, yet accurately and efficiently solving complex contact problems remains challenging. In this work, we propose a new contact algorithm that computes contact forces by taking the gradient of an energy function of the contact volume (overlap) with respect to the geometry descriptors. While elegant in concept, evaluating this gradient is non-trivial due to the arbitrary geometry of the contact region. Inspired by the recently proposed Fiber Monte Carlo (FMC) method, we develop an algorithm that accurately computes contact forces based on the overlap volume between bodies with complex geometries. Our computational framework operates independently of mesh conformity, eliminating the need for master-slave identification and projection iterations, thus handling arbitrary discretizations. Moreover, by removing explicit complementarity constraints, the method retains a simple structure that can be easily incorporated into existing numerical solvers, such as the finite element method. In this paper, numerical examples cover a wide range of contact scenarios, from classical small-deformation static contact to complex large-deformation dynamic contact in both two- and three-dimensional settings with nonlinear material behavior. These cases include Hertzian contact for small-deformation verification; contact between wedge- and cone-shaped bodies to assess pressure and displacement predictions at non-smooth boundaries; contact involving Neo-Hookean hyperelastic materials for evaluating nonlinear responses under finite deformation; and dynamic collision cases to examine transient behavior.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying model prediction sensitivity to model-form uncertainty</title>
<link>https://arxiv.org/abs/2509.08708</link>
<guid>https://arxiv.org/abs/2509.08708</guid>
<content:encoded><![CDATA[
<div> Model-form uncertainty, Physics-based model, Uncertainty quantification, Sensitivity analysis, Model assumptions<br />
<br />
Summary: 
Model-form uncertainty (MFU) in physics-based model development is a significant source of uncertainty. A novel method is proposed to quantify the importance of uncertainties associated with model assumptions. By using parameterized modifications to assumptions (MFU representations) and grouped variance-based sensitivity analysis, the importance of assumptions can be measured. This approach can be applied even without calibration data. However, if calibration data is available, it can inform the MFU representation. The method is shown to be effective even when there is dependence between parameters, which often occurs during calibration. This method can help prioritize resources and efforts to reduce error in model predictions by understanding the importance of model assumptions relative to other sources of uncertainty. <div>
arXiv:2509.08708v1 Announce Type: new 
Abstract: Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration).
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.08207</link>
<guid>https://arxiv.org/abs/2509.08207</guid>
<content:encoded><![CDATA[
<div> Keywords: Aurora, Exascale supercomputer, Intel Xeon Data Center GPU Max Series, High Bandwidth Memory, DAOS <br />
Summary: <br />
Aurora, the Exascale supercomputer from Argonne National Laboratory, showcases advanced technologies like Intel Xeon Data Center GPU Max Series and High Bandwidth Memory. It also features the Distributed Asynchronous Object Storage (DAOS) and utilizes Intel's oneAPI programming environment. The node architecture, HPE Slingshot interconnect, and software ecosystem of Aurora are explored in detail in this paper. Standard benchmark performance and applications readiness are highlighted through the Early Science Program and the Exascale Computing Project. The integration of Intel's Data Center GPU Max Series on each compute node provides enhanced computational power, while the innovative DAOS storage solution offers high-speed data access. This comprehensive analysis demonstrates Aurora's potential to revolutionize scientific discovery through cutting-edge technologies and infrastructure. <br /> <div>
arXiv:2509.08207v1 Announce Type: cross 
Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer, designed to accelerate scientific discovery with cutting-edge architectural innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named Ponte Vecchio) on each compute node. Aurora also integrates the Distributed Asynchronous Object Storage (DAOS), a novel exascale storage solution, and leverages Intel's oneAPI programming environment. This paper presents an in-depth exploration of Aurora's node architecture, the HPE Slingshot interconnect, the supporting software ecosystem, and DAOS. We provide insights into standard benchmark performance and applications readiness efforts via Aurora's Early Science Program and the Exascale Computing Project.
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic AI Workflow to Simplify Parameter Estimation of Complex Differential Equation Systems</title>
<link>https://arxiv.org/abs/2509.07283</link>
<guid>https://arxiv.org/abs/2509.07283</guid>
<content:encoded><![CDATA[
<div> Parameter identification, Ordinary Differential Equation models, AI workflow, calibration pipeline, optimization. 
<br /> 
Summary: 
An AI workflow is introduced to streamline the parameter identification process for mechanistic Ordinary Differential Equation models. The system translates a human-readable specification into a parallel and differentiable calibration pipeline. Users input an XML description and fill in a Python code skeleton, with the AI agent handling validation and remediation of common issues. Python callables are converted to JAX functions for efficient compilation and parallelization. The workflow includes global exploration of the parameter space followed by gradient-based refinement. This approach provides a reproducible workflow that simplifies advanced calibration while maintaining expert involvement. The open-source implementation allows for quick progression from problem statement to fitted models with minimal boilerplate. <div>
arXiv:2509.07283v1 Announce Type: new 
Abstract: Parameter identification for mechanistic Ordinary Differential Equation (ODE) models underpins prediction and control in several applications, yet remains a labor-intensive and brittle process: datasets are noisy and partial, models can be stiff or misspecified, and differentiable implementations demand framework expertise. An agentic AI workflow is presented that converts a lightweight, human-readable specification into a compiled, parallel, and differentiable calibration pipeline. Users supply an XML description of the problem and fill in a Python code skeleton; the agent automatically validates consistency between spec and code, and auto-remediates common pathologies. It transforms Python callables into pure JAX functions for efficient just-in-time compilation and parallelization. The system then orchestrates a two-stage search comprising global exploration of the parameter space followed by gradient-based refinement. The result is an AD-native, reproducible workflow that lowers the barrier to advanced calibration while preserving expert control. An open-source implementation with a documented API and examples is released, enabling rapid movement from problem statement to fitted, auditable models with minimal boilerplate.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Data-Driven Framework for Efficient Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.07303</link>
<guid>https://arxiv.org/abs/2509.07303</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific discovery, data-driven, formula discovery, dimensional constraints, symbolic regression

Summary:
Scientific discovery is crucial for progress across disciplines, but identifying physical laws from datasets can be challenging and resource-intensive. This study introduces a novel approach, FIND, which utilizes the Buckingham $\Pi$ theorem and Taylor's theorem to create a unified representation of formulas. FIND focuses on determining the structure of latent formulas first, then streamlines parameter identification by adhering to dimensional constraints and simplicity in formulas. Strategic optimization techniques are employed to minimize search iterations, with complex outcomes refined using symbolic regression. Validation across 11 datasets showcases FIND's effectiveness in discovering physical laws, dimensionless numbers, partial differential equations, and critical system parameters in fields like astronomy, physics, chemistry, and electronics. The results position FIND as a versatile and powerful tool for advancing data-driven scientific discovery in diverse domains. 

<br /><br />Summary: <div>
arXiv:2509.07303v1 Announce Type: new 
Abstract: Scientific discovery drives progress across disciplines, from fundamental physics to industrial applications. However, identifying physical laws automatically from gathered datasets requires identifying the structure and parameters of the formula underlying the data, which involves navigating a vast search space and consuming substantial computational resources. To address these issues, we build on the Buckingham $\Pi$ theorem and Taylor's theorem to create a unified representation of diverse formulas, which introduces latent variables to form a two-stage structure. To minimize the search space, we initially focus on determining the structure of the latent formula, including the relevant contributing inputs, the count of latent variables, and their interconnections. Following this, the process of parameter identification is expedited by enforcing dimensional constraints for physical relevance, favoring simplicity in the formulas, and employing strategic optimization techniques. Any overly complex outcomes are refined using symbolic regression for a compact form. These general strategic techniques drastically reduce search iterations from hundreds of millions to just tens, significantly enhancing the efficiency of data-driven formula discovery. We performed comprehensive validation to demonstrate FIND's effectiveness in discovering physical laws, dimensionless numbers, partial differential equations, and uniform critical system parameters across various fields, including astronomy, physics, chemistry, and electronics. The excellent performances across 11 distinct datasets position FIND as a powerful and versatile tool for advancing data-driven scientific discovery in multiple domains.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Driven Hierarchical Sampling for Unbalanced Continual Malware Detection with Time-Series Update-Based Retrieval</title>
<link>https://arxiv.org/abs/2509.07532</link>
<guid>https://arxiv.org/abs/2509.07532</guid>
<content:encoded><![CDATA[
<div> Hierarchical balanced sampler, uncertainty-guided continual learning, dynamic class balancing, vector retrieval mechanism, Android malware detection <br />
Summary: <br />
- The study addresses challenges in Android malware detection due to concept drift and class imbalance.
- Existing replay-based methods have bias issues as they prioritize the benign class, leading to overfitting.
- A novel uncertainty-guided continual learning framework is proposed to balance benign and malicious samples and select high-information instances.
- The framework incorporates a vector retrieval mechanism using historical malware embeddings to identify evolved variants.
- Experimental results show significant performance improvement over state-of-the-art methods, achieving high true positive rate and mean accuracy, making it effective for sustainable Android malware detection. <div>
arXiv:2509.07532v1 Announce Type: new 
Abstract: Android malware detection continues to face persistent challenges stemming from long-term concept drift and class imbalance, as evolving malicious behaviors and shifting usage patterns dynamically reshape feature distributions. Although continual learning (CL) mitigates drift, existing replay-based methods suffer from inherent bias. Specifically, their reliance on classifier uncertainty for sample selection disproportionately prioritizes the dominant benign class, causing overfitting and reduced generalization to evolving malware. To address these limitations, we propose a novel uncertainty-guided CL framework. First, we introduce a hierarchical balanced sampler that employs a dual-phase uncertainty strategy to dynamically balance benign and malicious samples while simultaneously selecting high-information, high-uncertainty instances within each class. This mechanism ensures class equilibrium across both replay and incremental data, thereby enhancing adaptability to emerging threats. Second, we augment the framework with a vector retrieval mechanism that exploits historical malware embeddings to identify evolved variants via similarity-based retrieval, thereby complementing classifier updates. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods under strict low-label conditions (50 labels per phase). It achieves a true positive rate (TPR) of 92.95\% and a mean accuracy (mACC) of 94.26\%, which validates its efficacy for sustainable Android malware detection.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
<link>https://arxiv.org/abs/2509.07627</link>
<guid>https://arxiv.org/abs/2509.07627</guid>
<content:encoded><![CDATA[
<div> Framework, TCR design, epitope-specific, gene-aware Transformer, multi-architecture, LSMTCR <br />
Summary: <br />
The article introduces LSMTCR, a multi-architecture framework for designing full-length, epitope-specific TCRs. It separates specificity from constraint learning to generate paired TCRs conditioned on epitopes. By utilizing a diffusion-enhanced BERT encoder and conditional GPT decoders, LSMTCR can generate chain-specific CDR3 sequences with high predicted binding and diversity. The gene-aware Transformer ensures immunogenetic fidelity by predicting V/J usage for complete TCR assembly. LSMTCR outperforms baselines in predicted binding, grammar fidelity, and diversity on various datasets. Transfer learning improves predicted binding, length realism, and diversity for TCR generation. Full-length TCR assembly from known or de novo CDR3s maintains k-mer spectra and achieves high pTM/ipTM scores in paired co-modelling with epitopes. LSMTCR enables the generation of diverse, gene-contextualized TCR designs solely from epitope input, facilitating high-throughput screening and iterative optimization. <br /> <div>
arXiv:2509.07627v1 Announce Type: new 
Abstract: Designing full-length, epitope-specific TCR {\alpha}\b{eta} remains challenging due to vast sequence space, data biases and incomplete modeling of immunogenetic constraints. We present LSMTCR, a scalable multi-architecture framework that separates specificity from constraint learning to enable de novo, epitope-conditioned generation of paired, full-length TCRs. A diffusion-enhanced BERT encoder learns time-conditioned epitope representations; conditional GPT decoders, pretrained on CDR3\b{eta} and transferred to CDR3{\alpha}, generate chain-specific CDR3s under cross-modal conditioning with temperature-controlled diversity; and a gene-aware Transformer assembles complete {\alpha}/\b{eta} sequences by predicting V/J usage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and our curated dataset, LSMTCR achieves higher predicted binding than baselines on most datasets, more faithfully recovers positional and length grammars, and delivers superior, temperature-tunable diversity. For {\alpha}-chain generation, transfer learning improves predicted binding, length realism and diversity over representative methods. Full-length assembly from known or de novo CDR3s preserves k-mer spectra, yields low edit distances to references, and, in paired {\alpha}/\b{eta} co-modelling with epitope, attains higher pTM/ipTM than single-chain settings. LSMTCR outputs diverse, gene-contextualized, full-length TCR designs from epitope input alone, enabling high-throughput screening and iterative optimization.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized eigenvalue stabilization for immersed explicit dynamics</title>
<link>https://arxiv.org/abs/2509.07632</link>
<guid>https://arxiv.org/abs/2509.07632</guid>
<content:encoded><![CDATA[
<div> Stabilization, Immersed finite element discretizations, Generalized eigenvalue stabilization, Spectral basis functions, Finite cell method <br />
Summary: <br />
This article proposes a Generalized Eigenvalue Stabilization (GEVS) strategy for element mass matrices of cut elements in explicit time integration for immersed finite element discretizations. The use of spectral basis functions and the Finite Cell Method (FCM) ensures high-order convergence and definiteness of system matrices. The GEVS approach can be applied to various immersed boundary finite element methods. Numerical experiments show that the stabilization strategy achieves optimal convergence rates and restores critical time step sizes of boundary-conforming discretizations. It is effective even with weakly enforced Dirichlet boundary conditions using Nitsche's method or penalty formulations. <div>
arXiv:2509.07632v1 Announce Type: new 
Abstract: Explicit time integration for immersed finite element discretizations severely suffers from the influence of poorly cut elements. In this contribution, we propose a generalized eigenvalue stabilization (GEVS) strategy for the element mass matrices of cut elements to cure their adverse impact on the critical time step size of the global system. We use spectral basis functions, specifically $C^0$ continuous Lagrangian interpolation polynomials defined on Gauss-Lobatto-Legendre (GLL) points, which, in combination with its associated GLL quadrature rule, yield high-order convergent diagonal mass matrices for uncut elements. Moreover, considering cut elements, we combine the proposed GEVS approach with the finite cell method (FCM) to guarantee definiteness of the system matrices. However, the proposed GEVS stabilization can directly be applied to other immersed boundary finite element methods. Numerical experiments demonstrate that the stabilization strategy achieves optimal convergence rates and recovers critical time step sizes of equivalent boundary-conforming discretizations. This also holds in the presence of weakly enforced Dirichlet boundary conditions using either Nitsche's method or penalty formulations.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards automatizing detection and quantification of intestinal metaplasia: a multi-expert comparative study</title>
<link>https://arxiv.org/abs/2509.06991</link>
<guid>https://arxiv.org/abs/2509.06991</guid>
<content:encoded><![CDATA[
<div> deep learning, gastric cancer, intestinal metaplasia, risk assessment, pathologists <br />
Summary: 
- The study introduces an automated method utilizing deep learning models to detect and quantify intestinal metaplasia in gastric samples for gastric cancer risk assessment.
- Deep learning models achieved high performance in classifying intestinal metaplasia, outperforming experienced pathologists.
- The best-performing model demonstrated F1-Score of 0.80 and AUC of 0.91 in classifying intestinal metaplasia.
- Pathologists' inter-observer agreement ranged from 0.61 to 0.75, while agreement between pathologists and the deep learning model ranged from 0.37 to 0.54.
- Deep learning models offer potential for more precise and reproducible detection and quantification of intestinal metaplasia, highlighting the variability in risk assessment when visually estimating intestinal metaplasia percentage. <br /> 
Summary: <div>
arXiv:2509.06991v1 Announce Type: cross 
Abstract: Current gastric cancer risk systems are prone to errors since they evaluate a visual estimation of intestinal metaplasia percentages to assign a risk. This study presents an automated method to detect and quantify intestinal metaplasia using deep learning models as well as a comparative analysis with visual estimations of three experienced pathologists. Gastric samples were collected from two different cohorts: 149 asymptomatic volunteers from a region with a high prevalence of GCa in Colombia and 56 patients from a third-level hospital. Deep learning models were selected and trained to classify intestinal metaplasia, and predictions were used to estimate the percentage of intestinal metaplasia and assign the risk score. Results were compared with independent blinded assessments performed by three experienced pathologists. The best-performing deep learning architecture classified intestinal metaplasia with F1-Score of 0.80 +- 0.01 and AUC of 0.91 +- 0.01. Among pathologists, inter-observer agreement by a Fleiss's Kappa score ranged from 0.61 to 0.75. In comparison, agreement between the pathologists and the best-performing model ranged from 0.37 to 0.54. Deep learning models show potential to detect and quantify the percentage of intestinal metaplasia with greater precision and reproducibility than experienced pathologists. Likewise, estimated risk shows high inter-observer variability when visually assigning the intestinal metaplasia percentage.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-strategy improved gazelle optimization algorithm for solving numerical optimization and engineering applications</title>
<link>https://arxiv.org/abs/2509.07211</link>
<guid>https://arxiv.org/abs/2509.07211</guid>
<content:encoded><![CDATA[
<div> iteration-based updating framework, adaptive parameter tuning, dominant population-based restart strategy, exploration, exploitation

Summary:
The article introduces a multi-strategy improved gazelle optimization algorithm (MSIGOA) to address the limitations of the basic GOA. The proposed algorithm utilizes an iteration-based updating framework to balance exploration and exploitation, enhancing convergence speed. Adaptive parameter tuning strategies improve applicability, while a dominant population-based restart strategy helps escape local optima. Evaluation on benchmark test sets shows MSIGOA outperforms basic GOA and other advanced algorithms, with a significant improvement in exploration and exploitation capabilities. Results indicate that MSIGOA performs well on various functions, demonstrating superiority over other methods. The algorithm's effectiveness is further confirmed through engineering design optimization problems, highlighting its extensibility for practical applications. The study showcases the potential of MSIGOA in enhancing optimization processes and solving complex problems efficiently and effectively. 

<br /><br />Summary: <div>
arXiv:2509.07211v1 Announce Type: cross 
Abstract: Aiming at the shortcomings of the gazelle optimization algorithm, such as the imbalance between exploration and exploitation and the insufficient information exchange within the population, this paper proposes a multi-strategy improved gazelle optimization algorithm (MSIGOA). To address these issues, MSIGOA proposes an iteration-based updating framework that switches between exploitation and exploration according to the optimization process, which effectively enhances the balance between local exploitation and global exploration in the optimization process and improves the convergence speed. Two adaptive parameter tuning strategies improve the applicability of the algorithm and promote a smoother optimization process. The dominant population-based restart strategy enhances the algorithms ability to escape from local optima and avoid its premature convergence. These enhancements significantly improve the exploration and exploitation capabilities of MSIGOA, bringing superior convergence and efficiency in dealing with complex problems. In this paper, the parameter sensitivity, strategy effectiveness, convergence and stability of the proposed method are evaluated on two benchmark test sets including CEC2017 and CEC2022. Test results and statistical tests show that MSIGOA outperforms basic GOA and other advanced algorithms. On the CEC2017 and CEC2022 test sets, the proportion of functions where MSIGOA is not worse than GOA is 92.2% and 83.3%, respectively, and the proportion of functions where MSIGOA is not worse than other algorithms is 88.57% and 87.5%, respectively. Finally, the extensibility of MSIGAO is further verified by several engineering design optimization problems.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyHexTop: a compact Python code for topology optimization using hexagonal elements</title>
<link>https://arxiv.org/abs/2310.01968</link>
<guid>https://arxiv.org/abs/2310.01968</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, Python, Hexagonal Elements, Educational, Compliance Minimization

Summary:
Python-based code "PyHexTop" is introduced as an alternative to MATLAB for topology optimization, featuring hexagonal elements for design domains without checkerboard issues. Developed from the MATLAB code "HoneyTop90," it utilizes NumPy and SciPy libraries, making it easy to understand for beginners in the field. The code focuses on compliance minimization with volume constraints, demonstrated with the Messerschmitt-Bolkow-Blohm beam problem and other variations. "PyHexTop" is a valuable educational tool shared openly for learning and exploration in topology optimization. <div>
arXiv:2310.01968v4 Announce Type: replace 
Abstract: Python serves as an open-source and cost-effective alternative to the MATLAB programming language. This paper introduces a concise topology optimization Python code, named ``\texttt{PyHexTop}," primarily intended for educational purposes. Code employs hexagonal elements to parameterize design domains as such elements provide checkerboard-free optimized design naturally. \texttt{PyHexTop} is developed based on the ``\texttt{HoneyTop90}" MATLAB code~\cite{kumar2023honeytop90} and uses the \texttt{NumPy} and \texttt{SciPy} libraries. Code is straightforward and easily comprehensible, proving a helpful tool that can help people new in the topology optimization field to learn and explore. \texttt{PyHexTop} is specifically tailored to address compliance minimization with specified volume constraints. The paper provides a detailed explanation of the code for solving the Messerschmitt-Bolkow-Blohm beam and extensions to solve problems different problems. The code is publicly shared at: https://github.com/PrabhatIn/PyHexTop
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOaCNN: Adaptive Convolutional Neural Network for Multidisciplinary Topology Optimization</title>
<link>https://arxiv.org/abs/2310.02069</link>
<guid>https://arxiv.org/abs/2310.02069</guid>
<content:encoded><![CDATA[
<div> Keywords: adaptive convolutional neural network, topology optimization, encoder-decoder networks, dense layers, compliance minimization problems

Summary:<br />
This paper introduces an adaptive convolutional neural network (CNN) architecture designed to automate a variety of topology optimization (TO) problems with different physical constraints. The network utilizes encoder-decoder networks with dense layers, incorporating an adaptive layer to capture complex geometric features. Trained on datasets from three open-source TO codes with varying physics, the CNN demonstrates robustness and success in compliance minimization tasks involving constant and design-dependent loads, as well as material bulk modulus optimization. Able to generate optimized designs quickly based on user input of volume fraction, the architecture closely matches results obtained from existing TO codes with minimal errors in performance and volume fraction. <div>
arXiv:2310.02069v2 Announce Type: replace 
Abstract: This paper presents an adaptive convolutional neural network (CNN) architecture that can automate diverse topology optimization (TO) problems having different underlying physics. The architecture uses the encoder-decoder networks with dense layers in the middle which includes an additional adaptive layer to capture complex geometrical features. The network is trained using the dataset obtained from the three open-source TO codes involving different physics. The robustness and success of the presented adaptive CNN are demonstrated on compliance minimization problems with constant and design-dependent loads and material bulk modulus optimization. The architecture takes the user's input of the volume fraction. It instantly generates optimized designs resembling their counterparts obtained via open-source TO codes with negligible performance and volume fraction error.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanding Hardware-Efficiently Manipulable Hilbert Space via Hamiltonian Embedding</title>
<link>https://arxiv.org/abs/2401.08550</link>
<guid>https://arxiv.org/abs/2401.08550</guid>
<content:encoded><![CDATA[
<div> Quantum, Sparse Hamiltonian, Simulation, Hamiltonian Embedding, Quantum Applications <br />
Summary: 
The paper introduces the concept of Hamiltonian embedding as a technique for efficient quantum simulation of sparse Hamiltonians. This approach involves embedding the sparse Hamiltonian into a larger and more structured quantum system to enable more efficient simulation using hardware-efficient operations. Through a systematic study, the researchers demonstrate significant savings in computational resources, making it possible to implement quantum walks on complex graphs, quantum spatial search, and simulate real-space Schrdinger equations on current quantum platforms. The technique enhances the implementability of quantum advantages in the NISQ era by expanding the possibilities for quantum algorithms design. This advancement paves the way for practical implementation of quantum applications that depend on efficient sparse Hamiltonian simulation, offering a promising outlook for near-term quantum computing technology. <br /><br /> <div>
arXiv:2401.08550v2 Announce Type: replace-cross 
Abstract: Many promising quantum applications depend on the efficient quantum simulation of an exponentially large sparse Hamiltonian, a task known as sparse Hamiltonian simulation, which is fundamentally important in quantum computation. Although several theoretically appealing quantum algorithms have been proposed for this task, they typically require a black-box query model of the sparse Hamiltonian, rendering them impractical for near-term implementation on quantum devices.
  In this paper, we propose a technique named Hamiltonian embedding. This technique simulates a desired sparse Hamiltonian by embedding it into the evolution of a larger and more structured quantum system, allowing for more efficient simulation through hardware-efficient operations. We conduct a systematic study of this new technique and demonstrate significant savings in computational resources for implementing prominent quantum applications. As a result, we can now experimentally realize quantum walks on complicated graphs (e.g., binary trees, glued-tree graphs), quantum spatial search, and the simulation of real-space Schr\"odinger equations on current trapped-ion and neutral-atom platforms. Given the fundamental role of Hamiltonian evolution in the design of quantum algorithms, our technique markedly expands the horizon of implementable quantum advantages in the NISQ era.
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Newton to Einstein: Axiom-Based Discovery via Game Design</title>
<link>https://arxiv.org/abs/2509.05448</link>
<guid>https://arxiv.org/abs/2509.05448</guid>
<content:encoded><![CDATA[
<div> machine learning, scientific discovery, axiom-based reasoning, rule-evolving system, interpretability

Summary: 
This position paper advocates for a shift in machine learning for scientific discovery from inductive pattern recognition to axiom-based reasoning. The proposed framework treats scientific inquiry as a rule-evolving system, where agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike traditional machine learning approaches, this method allows for the discovery of new theoretical structures through systematic rule adaptation. Preliminary experiments in logic-based games demonstrate the feasibility of this approach by showing that agents can evolve axioms to solve previously unsolvable problems. This framework provides a basis for developing machine learning systems capable of creative, interpretable, and theory-driven discovery. <div>
arXiv:2509.05448v1 Announce Type: new 
Abstract: This position paper argues that machine learning for scientific discovery should shift from inductive pattern recognition to axiom-based reasoning. We propose a game design framework in which scientific inquiry is recast as a rule-evolving system: agents operate within environments governed by axioms and modify them to explain outlier observations. Unlike conventional ML approaches that operate within fixed assumptions, our method enables the discovery of new theoretical structures through systematic rule adaptation. We demonstrate the feasibility of this approach through preliminary experiments in logic-based games, showing that agents can evolve axioms that solve previously unsolvable problems. This framework offers a foundation for building machine learning systems capable of creative, interpretable, and theory-driven discovery.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUIDe: Generative and Uncertainty-Informed Inverse Design for On-Demand Nonlinear Functional Responses</title>
<link>https://arxiv.org/abs/2509.05641</link>
<guid>https://arxiv.org/abs/2509.05641</guid>
<content:encoded><![CDATA[
<div> Machine learning, generative models, inverse design, nonlinear systems, probabilistic modeling <br />
Summary: The article discusses the challenges of inverse design problems in engineering, particularly when dealing with nonlinear system responses. Traditional methods such as deep generative models and optimization-based approaches may yield unreliable solutions or incomplete coverage of the solution space. To overcome this, the Generative and Uncertainty-informed Inverse Design (GUIDe) framework is proposed, leveraging probabilistic machine learning and statistical inference. Unlike traditional inverse models, GUIDe uses a design-to-response strategy to generate designs with targeted nonlinear behaviors. By predicting each design's nonlinear functional response and evaluating the confidence that a design will meet the target, GUIDe enables the discovery of diverse feasible solutions, even for out-of-distribution targets. The method is validated through the design of interface properties for nacre-inspired composites to achieve target stress-strain responses. <div>
arXiv:2509.05641v1 Announce Type: new 
Abstract: Inverse design problems are pervasive in engineering, particularly when dealing with nonlinear system responses, such as in mechanical behavior or spectral analysis. The inherent intractability, non-existence, or non-uniqueness of their solutions, and the need for swift exploration of the solution space necessitate the adoption of machine learning and data-driven approaches, such as deep generative models. Here, we show that both deep generative model-based and optimization-based methods can yield unreliable solutions or incomplete coverage of the solution space. To address this, we propose the Generative and Uncertainty-informed Inverse Design (GUIDe) framework, leveraging probabilistic machine learning, statistical inference, and Markov chain Monte Carlo sampling to generate designs with targeted nonlinear behaviors. Unlike inverse models that directly map response to design, i.e., response $\mapsto$ design, we employ a design $\mapsto$ response strategy: a forward model that predicts each design's nonlinear functional response allows GUIDe to evaluate the confidence that a design will meet the target, conditioned on a target response with a user-specified tolerance level. Then, solutions are generated by sampling the solution space based on the confidence. We validate the method by designing the interface properties for nacre-inspired composites to achieve target stress-strain responses. Results show that GUIDe enables the discovery of diverse feasible solutions, including those outside the training data range, even for out-of-distribution targets.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-based Topology Optimization</title>
<link>https://arxiv.org/abs/2509.05800</link>
<guid>https://arxiv.org/abs/2509.05800</guid>
<content:encoded><![CDATA[
<div> Transformer-based machine learning model, topology optimization, transfer learning, FFT encoding, auxiliary loss functions

Summary:
The study introduces a transformer-based machine learning model for topology optimization that incorporates critical boundary and loading conditions using a class token mechanism. Transfer learning and FFT encoding are utilized to enhance performance on dynamic datasets. The model includes auxiliary loss functions to improve the realism and manufacturability of generated designs. Performance evaluation shows that the model approaches the fidelity of diffusion-based models while eliminating the need for iterations. It achieves low compliance error, volume fraction error, floating material percentage, and load discrepancy error, demonstrating its potential for real-time, high-fidelity topology generation. <div>
arXiv:2509.05800v1 Announce Type: new 
Abstract: Topology optimization enables the design of highly efficient and complex structures, but conventional iterative methods, such as SIMP-based approaches, often suffer from high computational costs and sensitivity to initial conditions. Although machine learning methods have recently shown promise for accelerating topology generation, existing models either remain iterative or struggle to match ground-truth performance. In this work, we propose a transformer-based machine learning model for topology optimization that embeds critical boundary and loading conditions directly into the tokenized domain representation via a class token mechanism. We implement this model on static and dynamic datasets, using transfer learning and FFT encoding of dynamic loads to improve our performance on the dynamic dataset. Auxiliary loss functions are introduced to promote the realism and manufacturability of the generated designs. We conduct a comprehensive evaluation of the model's performance, including compliance error, volume fraction error, floating material percentage, and load discrepancy error, and benchmark it against state-of-the-art non-iterative and iterative generative models. Our results demonstrate that the proposed model approaches the fidelity of diffusion-based models while remaining iteration-free, offering a significant step toward real-time, high-fidelity topology generation.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distortion Minimization in Reverse Engineering for Additive Manufacturing: An Integrated 3D Scanning and Simulation Framework</title>
<link>https://arxiv.org/abs/2509.05857</link>
<guid>https://arxiv.org/abs/2509.05857</guid>
<content:encoded><![CDATA[
<div> Keywords: reverse engineering, additive manufacturing, 3D scanning, process simulation, geometric distortions

Summary:
This paper introduces a new framework for reverse engineering additively manufactured components, focusing on minimizing distortions. The framework combines 3D scanning with process simulation to predict geometric distortions and minimize errors between predicted and measured dimensions. The approach was demonstrated on Inconel-718 components produced using laser powder bed fusion additive manufacturing. The framework generates compensated STL and parametric CAD models, eliminating the need for experimental adjustments. The CAD-based method showed better accuracy, with an average absolute percent error of 0.087% between predicted and measured dimensions. This integrated approach offers a promising solution for reverse engineering and additive manufacturing processes, particularly for parts with complex geometries and high process-induced distortions. <div>
arXiv:2509.05857v1 Announce Type: new 
Abstract: Reverse engineering can be used to derive a 3D model of an existing physical part when such a model is not readily available. For parts that will be fabricated with subtractive and formative manufacturing processes, existing reverse engineering techniques can be readily applied, but parts produced with additive manufacturing can present new challenges due to the high level of process-induced distortions and unique part attributes. This paper introduces an integrated 3D scanning and process simulation data-driven framework to minimize distortions of reverse-engineered additively manufactured components. This framework employs iterative finite element simulations to predict geometric distortions to minimize errors between the predicted and measured geometrical deviations of the key dimensional characteristics of the part. The effectiveness of this approach is then demonstrated by reverse engineering two Inconel-718 components manufactured using laser powder bed fusion additive manufacturing. This paper presents a remanufacturing process that combines reverse engineering and additive manufacturing, leveraging geometric feature-based part compensation through process simulation. Our approach can generate both compensated STL and parametric CAD models, eliminating laborious experimentation during reverse engineering. We evaluate the merits of STL-based and CAD-based approaches by quantifying the errors induced at the different steps of the proposed approach and analyzing the influence of varying part geometries. Using the proposed CAD-based method, the average absolute percent error between simulation-predicted distorted dimensions and actual measured dimensions of the manufactured parts was 0.087%, with better accuracy than the STL-based method.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anticipating AMOC transitions via deep learning</title>
<link>https://arxiv.org/abs/2509.06450</link>
<guid>https://arxiv.org/abs/2509.06450</guid>
<content:encoded><![CDATA[
<div> AMOC, abrupt transitions, Earth system, convolutional neural network, early warning indicators
Summary: Key components of the Earth system, such as the Atlantic Meridional Overturning Circulation (AMOC), can undergo abrupt and potentially irreversible transitions when external forcing exceeds critical thresholds. This study explores the challenges of predicting such transitions, which can be induced by bifurcations, critical forcing rates, and noise. Traditional early warning indicators based on critical slowing down are unreliable in the stochastic regime of these transitions. To address this limitation, a convolutional neural network (CNN)-based approach is developed to identify statistical differences between transitioning and non-transitioning trajectories within ensemble simulations. This CNN-based indicator allows for real-time prediction of transition probabilities for individual trajectories prior to tipping points. The results demonstrate the effectiveness of this approach in providing early warnings for abrupt transitions of Earth system components, highlighting the importance of identifying safe operating spaces and early warning indicators under uncertainty. 
<br /><br />Summary: <div>
arXiv:2509.06450v1 Announce Type: new 
Abstract: Key components of the Earth system can undergo abrupt and potentially irreversible transitions when the magnitude or rate of external forcing exceeds critical thresholds. In this study, we use the example of the Atlantic Meridional Overturning Circulation (AMOC) to demonstrate the challenges associated with anticipating such transitions when the system is susceptible to bifurcation-induced, rate-induced, and noise-induced tipping. Using a calibrated AMOC box model, we conduct large ensemble simulations and show that transition behavior is inherently probabilistic: under identical freshwater forcing scenarios, some ensemble members exhibit transitions while others do not. In this stochastic regime, traditional early warning indicators based on critical slowing down are unreliable in predicting impending transitions. To address this limitation, we develop a convolutional neural network (CNN)-based approach that identifies higher-order statistical differences between transitioning and non-transitioning trajectories within the ensemble realizations. This method enables the real-time prediction of transition probabilities for individual trajectories prior to the onset of tipping. Our results show that the CNN-based indicator provides effective early warnings in a system where transitions can be induced by bifurcations, critical forcing rates, and noise. These findings underscore the potential in identifying safe operating spaces and early warning indicators for abrupt transitions of Earth system components under uncertainty.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reusable Surrogate Models for Distillation Columns</title>
<link>https://arxiv.org/abs/2509.06638</link>
<guid>https://arxiv.org/abs/2509.06638</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, chemical process engineering, distillation columns, ML-fueled modelfluid representation, entrainer distillation<br />
Summary:<br />
This article introduces a novel approach to surrogate modeling in chemical process engineering, aiming to create reusable models for distillation columns. By implementing a new ML-fueled modelfluid representation, the researchers were able to generate a vast dataset of over $1,000,000 samples, allowing the surrogate model to generalize across various column specifications and chemical compositions. The model's accuracy was validated, and it was successfully applied in a case study on entrainer distillation, where it efficiently screened and ranked candidate entrainers. This approach has the potential to significantly reduce computational efforts in optimization tasks compared to traditional flowsheet simulators, marking a paradigm shift towards more versatile and efficient surrogate models in chemical engineering.<br /> 
Summary: <div>
arXiv:2509.06638v1 Announce Type: new 
Abstract: Surrogate modeling is a powerful methodology in chemical process engineering, frequently employed to accelerate optimization tasks where traditional flowsheet simulators are computationally prohibitive. However, the state-of-the-art is dominated by surrogate models trained for a narrow range of fixed chemical systems and operating conditions, limiting their reusability. This work introduces a paradigm shift towards reusable surrogates by developing a single model for distillation columns that generalizes across a vast design space. The key enabler is a novel ML-fueled modelfluid representation which allows for the generation of datasets of more than $1,000,000$ samples. This allows the surrogate to generalize not only over column specifications but also over the entire chemical space of homogeneous ternary vapor-liquid mixtures. We validate the model's accuracy and demonstrate its practical utility in a case study on entrainer distillation, where it successfully screens and ranks candidate entrainers, significantly reducing the computational effort compared to rigorous optimization.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing</title>
<link>https://arxiv.org/abs/2509.05794</link>
<guid>https://arxiv.org/abs/2509.05794</guid>
<content:encoded><![CDATA[
<div> microservices, stateful services, migration, Kubernetes, optimization
Summary:<br />
- The paper addresses the challenge of migrating stateful microservices in Kubernetes, proposing an optimized scheme that integrates the MS2M framework with Kubernetes' FCC feature.
- Key enhancements include support for migrating StatefulSet-managed Pods and a Threshold-Based Cutoff Mechanism to manage high message rates.
- Evaluation results show that MS2M for individual Pods significantly reduces downtime by 96.986% compared to cold migration methods.
- The StatefulSet approach offers greater flexibility in managing stateful services within Kubernetes.
- The insights provided offer practical strategies for optimizing stateful microservice migration in cloud-native environments.<br />Summary: <div>
arXiv:2509.05794v1 Announce Type: cross 
Abstract: The widespread adoption of microservices architecture in modern software systems has emphasized the need for efficient management of distributed services. While stateless microservices enable straightforward migration, stateful microservices introduce added complexity due to the need to preserve in-memory state during migration. However, most container orchestrators, including Kubernetes, lack native support for live stateful service migration. This paper proposes an optimized migration scheme for stateful services in Kubernetes by integrating the Message-based Stateful Microservice Migration (MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC) feature. Key enhancements include support for migrating StatefulSet-managed Pods and the introduction of a Threshold-Based Cutoff Mechanism to handle high incoming message rates. Evaluation results demonstrate that MS2M for individual Pods reduces downtime by 96.986% compared to cold migration methods, while the StatefulSet approach provides greater flexibility in managing stateful services. These insights provide practical strategies for optimizing stateful microservice migration in cloud-native environments.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection</title>
<link>https://arxiv.org/abs/2509.06035</link>
<guid>https://arxiv.org/abs/2509.06035</guid>
<content:encoded><![CDATA[
<div> detection, transmission lines, UAVs, small defects, power grids  
TinyDef-DETR is a framework for small-defect detection in transmission lines using UAVs. It addresses challenges such as detail loss, weak boundary sensitivity, and lack of global context integration. The method includes a stride-free space-to-depth module for downsampling, edge-enhanced convolution for boundary awareness, and dual-domain multi-scale attention for global and local information capture. It also uses a Focaler-Wise-SIoU regression loss for improved localization of small objects. TinyDef-DETR outperforms competitive baselines in precision and recall, especially for small objects, with minimal computational overhead. Validation on the VisDrone benchmark confirms the approach's generalization capability for power grid inspections. Overall, the integration of detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression offers an efficient solution for UAV-based small-defect inspection in power grids.<br /><br />Summary: <div>
arXiv:2509.06035v1 Announce Type: cross 
Abstract: Automated inspection of transmission lines using UAVs is hindered by the difficulty of detecting small and ambiguous defects against complex backgrounds. Conventional detectors often suffer from detail loss due to strided downsampling, weak boundary sensitivity in lightweight backbones, and insufficient integration of global context with local cues. To address these challenges, we propose TinyDef-DETR, a DETR-based framework designed for small-defect detection. The method introduces a stride-free space-to-depth module for lossless downsampling, an edge-enhanced convolution for boundary-aware feature extraction, a cross-stage dual-domain multi-scale attention module to jointly capture global and local information, and a Focaler-Wise-SIoU regression loss to improve localization of small objects. Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR achieves substantial improvements in both precision and recall compared to competitive baselines, with particularly notable gains on small-object subsets, while incurring only modest computational overhead. Further validation on the VisDrone benchmark confirms the generalization capability of the proposed approach. Overall, the results indicate that integrating detail-preserving downsampling, edge-sensitive representations, dual-domain attention, and difficulty-adaptive regression provides a practical and efficient solution for UAV-based small-defect inspection in power grids.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon</title>
<link>https://arxiv.org/abs/2509.06227</link>
<guid>https://arxiv.org/abs/2509.06227</guid>
<content:encoded><![CDATA[
<div> Skill, ENSO forecasting, Deep learning, Ensemble members, Climate science  
Summary:  
- The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO) remains a challenge in climate science.  
- Operational systems often use the mean of all ensemble members assuming equal skill, but a subset of members show higher skill levels.  
- Study using a state-of-the-art ENSO forecast system found Top-5 subsets with significantly higher correlation and lower RMSE compared to the mean.  
- Improvement in skill is most pronounced at extreme lead times, crucial transition periods like SON and DJF, and season-dependent months like JJA and MJJ.  
- Identification of high-quality ensemble members could enhance forecasting skill and provide clues for future investigations.  

Summary: <div>
arXiv:2509.06227v1 Announce Type: cross 
Abstract: The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO) is still one of the biggest challenges in climate science. While it is true that short-to medium-range performance has been improved significantly using the advances in deep learning, statistical dynamical hybrids, most operational systems still use the simple mean of all ensemble members, implicitly assuming equal skill across members. In this study, we demonstrate, through a strictly a-posteriori evaluation , for any large enough ensemble of ENSO forecasts, there is a subset of members whose skill is substantially higher than that of the ensemble mean. Using a state-of-the-art ENSO forecast system cross-validated against the 1986-2017 observed Nino3.4 index, we identify two Top-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on highest Pearson correlation. Generally across all leads, these outstanding members show higher correlation and lower RMSE, with the advantage rising enormously with lead time. Whereas at short leads (1 month) raises the mean correlation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\deg}C or by 23.3% compared to the All-40 mean, at extreme leads (23 months) the correlation is raised by +0.43 (+172%) and RMSE by 0.18 {\deg}C or by 22.5% decrease. The enhancements are largest during crucial ENSO transition periods such as SON and DJF, when accurate amplitude and phase forecasting is of greatest socio-economic benefit, and furthermore season-dependent e.g., mid-year months such as JJA and MJJ have incredibly large RMSE reductions. This study provides a solid foundation for further investigations to identify reliable clues for detecting high-quality ensemble members, thereby enhancing forecasting skill.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
<link>https://arxiv.org/abs/2509.06465</link>
<guid>https://arxiv.org/abs/2509.06465</guid>
<content:encoded><![CDATA[
<div> Keywords: Antibody binding site prediction, CAME-AB, multimodal representation, cross-modal reasoning, contrastive learning.

Summary: 
Antibody binding site prediction is crucial for computational immunology and antibody design. Existing methods often lack in representation and fail to identify antibody-specific binding sites. In this study, a novel framework called CAME-AB is introduced, which integrates multiple biologically grounded modalities to create a robust multimodal representation for antibody binding site prediction. The framework includes raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs. An adaptive modality fusion module dynamically weighs each modality for cross-modal reasoning, while a Transformer encoder with a Mixture-of-Experts module enhances feature specialization. Supervised contrastive learning shapes the latent space geometry for improved performance. Experimental results on antibody-antigen datasets show that CAME-AB outperforms existing baselines on various metrics. Ablation studies confirm the effectiveness of each architectural component and the benefits of multimodal feature integration. <div>
arXiv:2509.06465v1 Announce Type: cross 
Abstract: Antibody binding site prediction plays a pivotal role in computational immunology and therapeutic antibody design. Existing sequence or structure methods rely on single-view features and fail to identify antibody-specific binding sites on the antigens-a dual limitation in representation and prediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention framework with a Mixture-of-Experts (MoE) backbone for robust antibody binding site prediction. CAME-AB integrates five biologically grounded modalities, including raw amino acid encodings, BLOSUM substitution profiles, pretrained language model embeddings, structure-aware features, and GCN-refined biochemical graphs-into a unified multimodal representation. To enhance adaptive cross-modal reasoning, we propose an adaptive modality fusion module that learns to dynamically weight each modality based on its global relevance and input-specific contribution. A Transformer encoder combined with an MoE module further promotes feature specialization and capacity expansion. We additionally incorporate a supervised contrastive learning objective to explicitly shape the latent space geometry, encouraging intra-class compactness and inter-class separability. To improve optimization stability and generalization, we apply stochastic weight averaging during training. Extensive experiments on benchmark antibody-antigen datasets demonstrate that CAME-AB consistently outperforms strong baselines on multiple metrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further validate the effectiveness of each architectural component and the benefit of multimodal feature integration. The model implementation details and the codes are available on https://anonymous.4open.science/r/CAME-AB-C525
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A machine-learned expression for the excess Gibbs energy</title>
<link>https://arxiv.org/abs/2509.06484</link>
<guid>https://arxiv.org/abs/2509.06484</guid>
<content:encoded><![CDATA[
<div> neural network, Gibbs energy, thermodynamic properties, liquid mixtures, multi-component mixtures <br />
<br />Summary: 
HANNA, a neural network model, was developed to predict the excess Gibbs energy of multi-component liquid mixtures based on molecular structures. Physical laws were integrated as constraints to ensure thermodynamic consistency in predictions. The model was trained on a comprehensive experimental dataset and included a novel solver to incorporate liquid-liquid equilibrium data. A geometric projection method enabled accurate extrapolations to multi-component mixtures without the need for additional parameters. HANNA significantly outperformed existing methods in terms of accuracy and scope. The trained model and code are publicly available, with an interactive interface provided on the MLPROP website. <div>
arXiv:2509.06484v1 Announce Type: cross 
Abstract: The excess Gibbs energy plays a central role in chemical engineering and chemistry, providing a basis for modeling the thermodynamic properties of liquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures solely from the molecular structures of their components is a long-standing challenge. In this work, we address this challenge by integrating physical laws as hard constraints within a flexible neural network. The resulting model, HANNA, was trained end-to-end on an extensive experimental dataset for binary mixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent predictions. A novel surrogate solver developed in this work enabled the inclusion of liquid-liquid equilibrium data in the training process. Furthermore, a geometric projection method was applied to enable robust extrapolations to multi-component mixtures, without requiring additional parameters. We demonstrate that HANNA delivers excellent predictions, clearly outperforming state-of-the-art benchmark methods in accuracy and scope. The trained model and corresponding code are openly available, and an interactive interface is provided on our website, MLPROP.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Parallel Solver with Multiphysics Finite Element Method for Poroelasticity Coupled with Elasticity Model</title>
<link>https://arxiv.org/abs/2509.06673</link>
<guid>https://arxiv.org/abs/2509.06673</guid>
<content:encoded><![CDATA[
<div> pressure, poroelasticity, elasticity, Lagrange multiplier, parallel solver

Summary: 
This paper presents a parallel solver for the quasi-static linear poroelasticity and linear elasticity model in the Lagrange multiplier framework. The model is reformulated as a coupling of nearly incompressible elasticity and unsteady advection-diffusion equations, with new variables introduced to ensure normal stress continuity. Variational formulations and finite element methods are employed for each subdomain, with a parallel solver using the FETI method and FETI preconditioner for efficiency. Numerical tests demonstrate computational efficiency and convergence error order, and the model is validated using Barry-Mercer's model as a benchmark. The results show no oscillations in computed pressure, affirming the effectiveness of the proposed parallel solver for solving poroelasticity and elasticity models. 

<br /><br />Summary: <div>
arXiv:2509.06673v1 Announce Type: cross 
Abstract: In this paper, we propose a parallel solver for solving the quasi-static linear poroelasticity coupled with linear elasticity model in the Lagrange multiplier framework. Firstly, we reformulate the model into a coupling of the nearly incompressible elasticity and an unsteady affection-diffusion equations by setting new variable ``elastic pressure" and ``volumetric fluid content". And we introduce a Lagrange multiplier to guarantee the normal stress continuity on the interface. Then, we give the variational formulations in each subdomain and choose the $\boldsymbol{P}_k$-$P_1$-$P_1$ mixed finite element tuple for poroelasticity subdomain, and $\boldsymbol{P}_k$-$P_1$ finite element pair ($k=1,2$) for elasticity subdomain and the backward Euler scheme for time. Also, we propose a parallel solver for solving the fully discrete scheme at each time step-- the FETI method with a classical FETI preconditioner for solving the Lagrange multiplier and calculating the subproblems in each subdomain in parallel. And we show several numerical tests to validate the computational efficiency and the convergence error order, and we consider Barry-Mercer's model as the benchmark test to show that there no oscillation in the computed pressure. Finally, we draw conclusions to summarize the main results of this paper.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A simple and efficient hybrid discretization approach to alleviate membrane locking in isogeometric thin shells</title>
<link>https://arxiv.org/abs/2312.16944</link>
<guid>https://arxiv.org/abs/2312.16944</guid>
<content:encoded><![CDATA[
<div> membrane locking, isogeometric finite element, Kirchhoff-Love shells, hybrid discretization, stress recovery  
Summary:  
This work introduces a hybrid discretization technique to address membrane locking in isogeometric finite element models for Kirchhoff-Love shells. The method, compatible with existing isogeometric finite element codes, combines isogeometric and Lagrange-based surface discretizations without increasing the tangent matrix bandwidth or requiring additional degrees of freedom or static condensation. It proves effective for both linear and nonlinear problems, with simplified stress recovery. By incorporating quadratic NURBS surfaces, the approach significantly improves accuracy in membrane stresses relative to traditional methods. Rigorous analysis on various benchmark problems confirms the efficacy of the proposed technique in alleviating or eliminating membrane locking, suggesting potential extensions to other discretization types and constraints. <div>
arXiv:2312.16944v2 Announce Type: replace 
Abstract: This work presents a new hybrid discretization approach to alleviate membrane locking in isogeometric finite element formulations for Kirchhoff-Love shells. The approach is simple, and requires no additional dofs and no static condensation. It does not increase the bandwidth of the tangent matrix and is effective for both linear and nonlinear problems. It combines isogeometric surface discretizations with classical Lagrange-based surface discretizations, and can thus be run with existing isogeometric finite element codes. Also, the stresses can be recovered straightforwardly. The effectiveness of the proposed approach in alleviating, if not eliminating, membrane locking is demonstrated through the rigorous study of the convergence behavior of several classical benchmark problems. Accuracy gains are particularly large in the membrane stresses. The approach is formulated here for quadratic NURBS, but an extension to other discretization types can be anticipated. The same applies to other constraints and associated locking phenomena.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection-based model-order reduction via graph autoencoders suited for unstructured meshes</title>
<link>https://arxiv.org/abs/2407.13669</link>
<guid>https://arxiv.org/abs/2407.13669</guid>
<content:encoded><![CDATA[
<div> Graph autoencoder, projection-based model-order reduction, nonlinear manifold least-squares Petrov-Galerkin projection scheme, advection-dominated flows, unstructured meshes<br />
<br />
Summary: <br />
This paper introduces a graph autoencoder architecture, GD-LSPG, for projection-based model-order reduction in advection-dominated flow simulations on unstructured meshes. The architecture combines reduced graph hierarchy generation and message passing operations to emulate the filtering process of CNNs, allowing for improved flexibility and interpretability. GD-LSPG's latent state variables offer interpretable mode shapes similar to proper orthogonal decomposition modes. The framework is demonstrated on a one-dimensional Burgers' model with a structured mesh and two test cases for two-dimensional Euler equations using an unstructured mesh, showcasing its flexibility and accuracy compared to traditional affine projections. This approach provides a significant enhancement in accuracy for low-dimensional latent spaces and outperforms CNN-based autoencoders, making it a promising tool for model reduction in computational fluid dynamics. <br /> <div>
arXiv:2407.13669v4 Announce Type: replace 
Abstract: This paper presents the development of a graph autoencoder architecture capable of performing projection-based model-order reduction (PMOR) using a nonlinear manifold least-squares Petrov-Galerkin (LSPG) projection scheme. The architecture is particularly useful for advection-dominated flows modeled by unstructured meshes, as it provides a robust nonlinear mapping that can be leveraged in a PMOR setting. The presented graph autoencoder is constructed with a two-part process that consists of (1) generating a hierarchy of reduced graphs to emulate the compressive abilities of convolutional neural networks (CNNs) and (2) training a message passing operation at each step in the hierarchy of reduced graphs to emulate the filtering process of a CNN. The resulting framework provides improved flexibility over traditional CNN-based autoencoders because it is readily extendable to unstructured meshes. We provide an analysis of the interpretability of the graph autoencoder's latent state variables, where we find that the Jacobian of the decoder for the proposed graph autoencoder provides interpretable mode shapes akin to traditional proper orthogonal decomposition modes. To highlight the capabilities of the proposed framework, which is named geometric deep least-squares Petrov-Galerkin (GD-LSPG), we benchmark the method on a one-dimensional Burgers' model with a structured mesh and demonstrate the flexibility of GD-LSPG by deploying it on two test cases for two-dimensional Euler equations that use an unstructured mesh. The proposed framework is more flexible than using a traditional CNN-based autoencoder and provides considerable improvement in accuracy for very low-dimensional latent spaces in comparison with traditional affine projections.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed-precision numerics in scientific applications: survey and perspectives</title>
<link>https://arxiv.org/abs/2412.19322</link>
<guid>https://arxiv.org/abs/2412.19322</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, mixed-precision, scientific applications, algorithmic innovations, computational science
<br />
Summary: 
This article discusses the utilization of mixed-precision computations in artificial intelligence (AI) workloads and scientific applications. It highlights the potential for significant performance improvements up to 8x compared to double-precision in compute-intensive tasks. The survey covers various scientific domains like fluid dynamics, weather and climate, quantum chemistry, and computational genomics that have started implementing mixed-precision strategies. State-of-the-art algorithmic techniques such as iterative refinement and adaptive precision solvers are examined for their implications on accuracy, performance, and resource utilization. The article also discusses the emerging software ecosystem supporting mixed-precision methods at scale. Overall, the survey emphasizes the transformative impact of mixed-precision numerics in computational science, emphasizing the importance of aligning algorithms with evolving hardware capabilities. 
<br /><br /> <div>
arXiv:2412.19322v3 Announce Type: replace 
Abstract: The explosive demand for artificial intelligence (AI) workloads has led to a significant increase in silicon area dedicated to lower-precision computations on recent high-performance computing hardware designs. However, mixed-precision capabilities, which can achieve performance improvements of 8x compared to double-precision in extreme compute-intensive workloads, remain largely untapped in most scientific applications. A growing number of efforts have shown that mixed-precision algorithmic innovations can deliver superior performance without sacrificing accuracy. These developments should prompt computational scientists to seriously consider whether their scientific modeling and simulation applications could benefit from the acceleration offered by new hardware and mixed-precision algorithms. In this survey, we (1) review progress across diverse scientific domains -- including fluid dynamics, weather and climate, quantum chemistry, and computational genomics -- that have begun adopting mixed-precision strategies; (2) examine state-of-the-art algorithmic techniques such as iterative refinement, splitting and emulation schemes, and adaptive precision solvers; (3) assess their implications for accuracy, performance, and resource utilization; and (4) survey the emerging software ecosystem that enables mixed-precision methods at scale. We conclude with perspectives and recommendations on cross-cutting opportunities, domain-specific challenges, and the role of co-design between application scientists, numerical analysts and computer scientists. Collectively, this survey underscores that mixed-precision numerics can reshape computational science by aligning algorithms with the evolving landscape of hardware capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical optimization of aviation decarbonization scenarios: balancing traffic and emissions with maturing energy carriers and aircraft technology</title>
<link>https://arxiv.org/abs/2503.22435</link>
<guid>https://arxiv.org/abs/2503.22435</guid>
<content:encoded><![CDATA[
<div> Aviation, emissions, decarbonization, transportation, mitigation
<br />
Summary:<br />
- The article focuses on the role of aviation emissions in long-term climate mitigation in transportation.
- Low-carbon energy carriers and new aircraft deployment are modeled as technology-centered decarbonization policies.
- Supply constraints in targeted market segments are considered as demand-side policies.
- Shared Socioeconomic Pathways (SSPs) are used to estimate trend traffic demand and limit sectoral consumption of electricity and biomass.
- Emissions peak by 2040 in all scenarios, but meeting Paris Agreement goals requires targeted demand management or additional low-carbon energy supply.
- Gradient-based optimization in a multidisciplinary framework efficiently addresses nonlinear, high-dimensional problems while reducing implementation effort. 
<br />Summary: <div>
arXiv:2503.22435v2 Announce Type: replace 
Abstract: Despite being considered a hard-to-abate sector, aviation's emissions will play an important role in long-term climate mitigation of transportation. The introduction of low-carbon energy carriers and the deployment of new aircraft in the current fleet are modeled as technology-centered decarbonization policies, while supply constraints in targeted market segments are modeled as demand-side policies. Shared Socioeconomic Pathways (SSPs) are used to estimate trend traffic demand and to limit the sectoral consumption of electricity and biomass. Mitigation scenarios are formulated as optimization problems, and three applications are demonstrated: no-policy baselines, single-policy optimization, and scenario-robust policies. Results show that the choice of energy carrier is highly dependent on assumptions regarding aircraft technology and the background energy system. Across all SSP-based scenarios, emissions peak by around 2040, but achieving alignment with the Paris Agreement requires either targeted demand management or additional low-carbon energy supply. The use of gradient-based optimization within a multidisciplinary framework enables the efficient resolution of these nonlinear, high-dimensional problems while reducing implementation effort.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust blue-green urban flood risk management optimised with a genetic algorithm for multiple rainstorm return periods</title>
<link>https://arxiv.org/abs/2502.12174</link>
<guid>https://arxiv.org/abs/2502.12174</guid>
<content:encoded><![CDATA[
<div> optimisation, Blue-Green Infrastructure, flood risk, return periods, multi-objective

Summary:
This study introduces a novel methodology for optimising Blue-Green Infrastructure (BGI) designs to enhance flood risk management. By incorporating multiple return periods (10, 20, 30, 50, and 100 years) into a multi-objective optimisation framework, the study aims to improve the robustness of BGI schemes. Utilising a Non-dominated Sorting Genetic Algorithm II (NSGA-II) with a hydrodynamic model, the design process considers direct damage cost (DDC) and expected annual damage (EAD) as risk objective functions. Results highlight that a BGI design optimised for a single 100-year return period may not perform well for other return periods, indicating the importance of considering various storm magnitudes. The study demonstrates that a composite return period approach leads to improved performance metrics across all return periods, enhancing resilience to future climate extremes. This paradigm shift towards multi-return period-based designs in flood risk management can enhance adaptability and resilience in the face of changing climate conditions.

<br /><br />Summary: <div>
arXiv:2502.12174v2 Announce Type: replace-cross 
Abstract: Flood risk managers seek to optimise Blue-Green Infrastructure (BGI) designs to maximise return on investment. Current systems often use optimisation algorithms and detailed flood models to maximise benefit-cost ratios for single rainstorm return periods. However, these schemes may lack robustness in mitigating flood risks across different storm magnitudes. For example, a BGI scheme optimised for a 100-year return period may differ from one optimised for a 10-year return period. This study introduces a novel methodology incorporating five return periods (T = 10, 20, 30, 50, and 100 years) into a multi-objective BGI optimisation framework. The framework combines a Non-dominated Sorting Genetic Algorithm II (NSGA-II) with a fully distributed hydrodynamic model to optimise the spatial placement and combined size of BGI features. For the first time, direct damage cost (DDC) and expected annual damage (EAD), calculated for various building types, are used as risk objective functions, transforming a many-objective problem into a multi-objective one. Performance metrics such as Median Risk Difference (MedRD), Maximum Risk Difference (MaxRD), and Area Under Pareto Front (AUPF) reveal that a 100-year optimised BGI design performs poorly when evaluated for other return periods, particularly shorter ones. In contrast, a BGI design optimised using composite return periods enhances performance metrics across all return periods, with the greatest improvements observed in MedRD (22%) and AUPF (73%) for the 20-year return period, and MaxRD (23%) for the 50-year return period. Furthermore, climate uplift stress testing confirms the robustness of the proposed design to future rainfall extremes. This study advocates a paradigm shift in flood risk management, moving from single maximum to multiple rainstorm return period-based designs to enhance resilience and adaptability to future climate extremes.
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phase-field and lip-field approaches for fracture with extreme mesh deformation (X-Mesh): a one-dimensional study</title>
<link>https://arxiv.org/abs/2509.04971</link>
<guid>https://arxiv.org/abs/2509.04971</guid>
<content:encoded><![CDATA[
<div> Keywords: fracture, phase-field, lip-field, variational mesh study, X-Mesh

Summary:
The study examines a one-dimensional fracture problem using the phase-field or lip-field approach, focusing on optimizing incremental potential in relation to displacement and damage fields, as well as nodal coordinates of the mesh. Through variational mesh analysis, the research shows that as damage increases, the most damaged element decreases in size until it reaches zero, providing an accurate representation of the bar breaking. The optimized solution proves to be more precise than fixed mesh solutions. This work contributes to exploring the possibilities of extreme meshes in computational mechanics within the X-Mesh framework. 

<br /><br />Summary: <div>
arXiv:2509.04971v1 Announce Type: new 
Abstract: We consider a one-dimensional fracture problem modelled using either the phase-field or lip-field approach. In both cases, we optimise the incremental potential with respect to the displacement and damage fields and the nodal coordinates of the mesh. This is thus a variational mesh study. We observe that, as the damage reaches its maximum value, the optimisation drives the most damaged element to zero size as the damage reaches its maximum value. This peculiar element provides a precise displacement jump representation as the bar breaks. The overall solution is also shown to be much more accurate than the fixed mesh solution. This work forms part of an exploration into the capabilities of extreme meshes in computational mechanics (X-Mesh).
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Inverse Rosenblatt Transport for Structural Reliability Analysis</title>
<link>https://arxiv.org/abs/2509.05061</link>
<guid>https://arxiv.org/abs/2509.05061</guid>
<content:encoded><![CDATA[
<div> probability estimation, reliability analysis, solid mechanics, deep learning, high-dimensional spaces

Summary:
The article investigates the Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to efficiently estimate the probability of failure in high-dimensional settings. The framework scales linearly in input dimension and provides a compact and reusable surrogate of the target distribution. Through experimentation on various analytical and numerical examples, DIRT demonstrates lower estimator variance and accurate estimation of rare event probabilities compared to established methods like Bayesian updating with Subset Simulation (BUS-SuS). This research addresses the challenge of accurate failure probability estimation in engineering systems, particularly in high-dimensional settings with rare events. <div>
arXiv:2509.05061v1 Announce Type: new 
Abstract: Accurately estimating the probability of failure in engineering systems under uncertainty is a fundamental challenge, particularly in high-dimensional settings and for rare events. Conventional reliability analysis methods often become computationally intractable or exhibit high estimator variance when applied to problems with hundreds of uncertain parameters or highly concentrated failure regions. In this work, we investigate the use of the recently proposed Deep Inverse Rosenblatt Transport (DIRT) framework for reliability analysis in solid mechanics. DIRT combines a TT decomposition with an inverse Rosenblatt transformation to construct a low-rank approximation of the posterior distribution, enabling efficient sampling and probability estimation in high-dimensional spaces. By representing the optimal importance density in the TT format, DIRT scales linearly in the input dimension while maintaining a compact, reusable surrogate of the target distribution. We demonstrate the effectiveness of the DIRT framework on three analytical reliability problems and one numerical example with dimensionality ranging from 2 to 250. Compared to established methods such as Bayesian updating with Subset Simulation (BUS-SuS), DIRT seems to lower the estimator variance while accurately capturing rare event probabilities for the benchmark problems of this study.
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Evaluation of Derivatives of Green's Functions Using Recurrences</title>
<link>https://arxiv.org/abs/2509.03687</link>
<guid>https://arxiv.org/abs/2509.03687</guid>
<content:encoded><![CDATA[
<div> Green's functions, derivatives, symbolic-numerical procedures, quadrature by expansion, higher-order accuracy. 
Summary: This article presents hybrid symbolic-numerical methods for efficiently computing higher-order derivatives of Green's functions, crucial in fast multipole methods and Barnes-Hut algorithms. The proposed procedures achieve an O(n) cost for computing n derivatives, offering significant computational savings. These methods are applicable to radially symmetric Green's functions and are general, requiring only knowledge of the relevant PDE. The algorithm guarantees controlled error levels, enhancing reliability. Additionally, the article introduces a rotation-based approach for target-specific evaluation in the Cartesian setting within the method of quadrature by expansion, which significantly reduces computational expenses compared to traditional symbolic methods. Numerical experiments validate the accuracy and efficiency of the proposed techniques. <div>
arXiv:2509.03687v1 Announce Type: new 
Abstract: High-order derivatives of Green's functions are a key ingredient in Taylor-based fast multipole methods, Barnes-Hut $n$-body algorithms, and quadrature by expansion (QBX). In these settings, derivatives underpin either the formation, evaluation, and/or translation of Taylor expansions.
  In this article, we provide hybrid symbolic-numerical procedures that generate recurrences to attain an $O(n)$ cost for the the computation of $n$ derivatives (i.e. $O(1)$ per derivative) for arbitrary radially symmetric Green's functions. These procedures are general--only requiring knowledge of the PDE that the Green's function solves. We show that the algorithm has controlled, theoretically-understood error.
  We apply these methods to the method of quadrature by expansion, a method for the evaluation of singular layer potentials, which requires higher-order derivatives of Green's functions. In doing so, we contribute a new rotation-based method for target-specific QBX evaluation in the Cartesian setting that attains dramatically lower cost than existing symbolic approaches.
  Numerical experiments support our claims of accuracy and cost.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dynamic Energy-Based Hysteresis Model for Pulsed-Operated Fast-Ramping Magnets</title>
<link>https://arxiv.org/abs/2509.04115</link>
<guid>https://arxiv.org/abs/2509.04115</guid>
<content:encoded><![CDATA[
<div> Keywords: ferromagnetic yokes, fast-ramping magnets, hysteresis description, eddy-current model, normal-conducting bending magnet<br />
<br />
Summary: 
This paper presents a dynamic ferromagnetic model that combines energy-based hysteresis description and a thin-sheet eddy-current model in the time domain. The study addresses the challenges of accurately analyzing fast-ramping magnets due to their strongly nonlinear behavior. Existing approaches often oversimplify the analysis using anhysteretic material descriptions and after-the-fact loss formulae. By utilizing a more comprehensive model, the research aims to provide a more precise calculation of losses and magnetic fields. The model's effectiveness was demonstrated through its application in analyzing a normal-conducting bending magnet. This work sheds light on the importance of considering dynamic ferromagnetic behavior in the numerical analysis of magnets, offering a more accurate representation of their performance. <div>
arXiv:2509.04115v1 Announce Type: new 
Abstract: Due to the strongly nonlinear behavior of ferromagnetic yokes, the numerical analysis of fast-ramping magnets is highly cumbersome and, therefore, in practice overly simplified by means of anhysteretic material descriptions and a posteriori loss formulae. This paper establishes the use of a dynamic ferromagnetic model combining a preconditioned energy-based hysteresis description and a thin-sheet eddy-current model in time-domain. The model was successfully employed in the analysis of a normal-conducting bending magnet in order to precisely calculate losses and fields.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COBRA: Multimodal Sensing Deep Learning Framework for Remote Chronic Obesity Management via Wrist-Worn Activity Monitoring</title>
<link>https://arxiv.org/abs/2509.04210</link>
<guid>https://arxiv.org/abs/2509.04210</guid>
<content:encoded><![CDATA[
<div> Keywords: Chronic obesity management, Deep learning, Behavioral monitoring, Multimodal sensors, Digital health systems

Summary: 
COBRA (Chronic Obesity Behavioral Recognition Architecture) is a novel deep learning framework designed for objective monitoring of energy balance behaviors in individuals with chronic obesity. It utilizes wrist-worn multimodal sensors and a hybrid D-Net architecture that incorporates spatial modeling, self-attention mechanisms, and temporal processing to classify daily activities related to obesity. Validation on the WISDM-Smart dataset shows high accuracy and outperformance of state-of-the-art baselines. The framework's optimal preprocessing strategy includes spectral-temporal feature extraction, enabling robust generalizability with low demographic variance. COBRA's success in accurately categorizing activities such as Food Intake, Physical Activity, Sedentary Behavior, and Daily Living showcases its potential for scalable deployment in personalized obesity interventions and continuous lifestyle monitoring.<br /><br />Summary: <div>
arXiv:2509.04210v1 Announce Type: new 
Abstract: Chronic obesity management requires continuous monitoring of energy balance behaviors, yet traditional self-reported methods suffer from significant underreporting and recall bias, and difficulty in integration with modern digital health systems. This study presents COBRA (Chronic Obesity Behavioral Recognition Architecture), a novel deep learning framework for objective behavioral monitoring using wrist-worn multimodal sensors. COBRA integrates a hybrid D-Net architecture combining U-Net spatial modeling, multi-head self-attention mechanisms, and BiLSTM temporal processing to classify daily activities into four obesity-relevant categories: Food Intake, Physical Activity, Sedentary Behavior, and Daily Living. Validated on the WISDM-Smart dataset with 51 subjects performing 18 activities, COBRA's optimal preprocessing strategy combines spectral-temporal feature extraction, achieving high performance across multiple architectures. D-Net demonstrates 96.86% overall accuracy with category-specific F1-scores of 98.55% (Physical Activity), 95.53% (Food Intake), 94.63% (Sedentary Behavior), and 98.68% (Daily Living), outperforming state-of-the-art baselines by 1.18% in accuracy. The framework shows robust generalizability with low demographic variance (<3%), enabling scalable deployment for personalized obesity interventions and continuous lifestyle monitoring.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and scalable deep Maxwell solvers using multilevel iterative methods</title>
<link>https://arxiv.org/abs/2509.03622</link>
<guid>https://arxiv.org/abs/2509.03622</guid>
<content:encoded><![CDATA[
<div> surrogate PDE solvers, neural networks, subdomain neural operator model, iterative algorithms, multilevel domain decomposition <br /> 
Summary: 
This article explores the use of neural networks as surrogate solvers for partial differential equations, addressing challenges in accuracy and scalability. By combining neural network surrogates with iterative algorithms, the study demonstrates the accurate solution of PDE problems with varying scales, resolutions, and boundary conditions. A subdomain neural operator model is developed to handle arbitrary Robin-type boundary conditions, serving as a flexible preconditioner for solving subdomain problems iteratively. The model also supports the construction of global coarse spaces for efficient large-scale PDE problem solving through multilevel domain decomposition. Using two-dimensional Maxwell's equations as a test case, a single neural network is trained to simulate diverse problem scenarios with varying sizes, resolutions, wavelengths, and media distributions. The study showcases the platform's effectiveness in accurately designing multi-wavelength nanophotonic devices through inverse design techniques.  <br /><br />Summary: <div>
arXiv:2509.03622v1 Announce Type: cross 
Abstract: Neural networks have promise as surrogate partial differential equation (PDE) solvers, but it remains a challenge to use these concepts to solve problems with high accuracy and scalability. In this work, we show that neural network surrogates can combine with iterative algorithms to accurately solve PDE problems featuring different scales, resolutions, and boundary conditions. We develop a subdomain neural operator model that supports arbitrary Robin-type boundary condition inputs, and we show that it can be utilized as a flexible preconditioner to iteratively solve subdomain problems with bounded accuracy. We further show that our subdomain models can facilitate the construction of global coarse spaces to enable accelerated, large scale PDE problem solving based on iterative multilevel domain decomposition. With two-dimensional Maxwell's equations as a model system, we train a single network to simulate large scale problems with different sizes, resolutions, wavelengths, and dielectric media distribution. We further demonstrate the utility of our platform in performing the accurate inverse design of multi-wavelength nanophotonic devices. Our work presents a promising path to building accurate and scalable multi-physics surrogate solvers for large practical problems.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Linear Time Quantum Algorithm for Pairwise Sequence Alignment</title>
<link>https://arxiv.org/abs/2307.04479</link>
<guid>https://arxiv.org/abs/2307.04479</guid>
<content:encoded><![CDATA[
<div> Sequence Alignment, Quantum Algorithm, DNA sequences, Grover's search algorithm, optimal alignment

Summary: 
The paper introduces a Quantum Algorithm for sequence alignment, specifically for DNA sequences. By mapping the problem into a path-searching 2D graph and using a proposed oracle for profit calculation, the algorithm is able to find the optimal alignment in linear time. This surpasses classical deterministic algorithms in efficiency. By utilizing Grover's search algorithm, the proposed approach provides quadratic speeding up for unstructured search problems, ensuring optimal solutions deterministically. This is a significant improvement over existing randomized algorithms that often produce sub-optimal alignments. The quantum algorithm not only aligns sequences accurately but also guarantees finding the optimal solution, making it a promising tool for bioinformatics research. <br /><br />Summary: <div>
arXiv:2307.04479v2 Announce Type: replace-cross 
Abstract: Sequence Alignment is the process of aligning biological sequences in order to identify similarities between multiple sequences. In this paper, a Quantum Algorithm for finding the optimal alignment between DNA sequences has been demonstrated which works by mapping the sequence alignment problem into a path-searching problem through a 2D graph. The transition, which converges to a fixed path on the graph, is based on a proposed oracle for profit calculation. By implementing Grover's search algorithm, our proposed approach is able to align a pair of sequences and figure out the optimal alignment within linear time, which hasn't been attained by any classical deterministic algorithm. In addition to that, the proposed algorithm is capable of quadratic speeding up to any unstructured search problem by finding out the optimal paths accurately in a deterministic manner, in contrast to existing randomized algorithms that frequently sort out the sub-optimal alignments, therefore, don't always guarantee of finding out the optimal solutions.
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use of Physicochemical Modification Methods for Producing Traditional and Nanomodified Polymeric Composites with Improved Operational Properties</title>
<link>https://arxiv.org/abs/2509.02572</link>
<guid>https://arxiv.org/abs/2509.02572</guid>
<content:encoded><![CDATA[
<div> surface modification, thermoplastic composite materials, interfacial interaction, ultrasonic processing, nanocomposites <br />
Summary: Various physical and physicochemical methods for modifying components of thermoplastic composite materials were analyzed. Improving the surface properties of fillers and the interaction between components of the composite is crucial for enhancing the reliability of the composite. Research focused on modifying the surface of reinforcing fibrous fillers and liquid polymer binders to improve contact properties within the composite. The effectiveness of low-frequency ultrasonic processing in enhancing interfacial interaction was highlighted. Cluster formation and physicochemical modification of epoxy polymers filled with dispersed fillers were discussed, with emphasis on ultrasonic cavitation for deagglomeration and nanoparticle distribution in nanocomposites. Experimental results showed improved technological and physicomechanical properties of sonicated epoxy matrices. The article also briefly touched on biological modifications of polymer components for functional applications. <br /><br />Summary: <div>
arXiv:2509.02572v1 Announce Type: new 
Abstract: Various aspects of the methods of physical and physicochemical modification of components of filled thermoplastic composite materials are analyzed, aimed at improving the surface properties of the fillers and the technological properties of the polymer matrix during their interaction. It is noted that the improvement of the interfacial interaction of the components of polymer reactoplastic composites, including adhesive strength, is a key factor for improving the reliability of the cured filled composite. As a promising area of research, a modification of the surface of the reinforcing fibrous filler and the technological characteristics of the liquid polymer binder, aimed at increasing their contact properties in the composite, was chosen. The effectiveness of the physical method of modifying the components of composites in the form of low-frequency ultrasonic processing is described. The peculiarities of cluster formation and physicochemical modification of epoxy polymers filled with dispersed fillers are analyzed. Attention is focused on the effectiveness of ultrasonic processing in the cavitation mode for deagglomeration and uniform distribution of nanoparticles in a liquid medium during the creation of nanocomposites. Experimentally confirmed is the improvement of the technological properties of liquid epoxy polymers, modified by ultrasound, used for the impregnation of oriented fibrous fillers, as well as the improvement of the physicomechanical properties of the sonicated epoxy matrices. Some issues of biological modifications of components of polymers for functional application are briefly reviewed.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Use ADAS Data to Predict Near-Miss Events: A Group-Based Zero-Inflated Poisson Approach</title>
<link>https://arxiv.org/abs/2509.02614</link>
<guid>https://arxiv.org/abs/2509.02614</guid>
<content:encoded><![CDATA[
<div> telematics, driving behavior, risk evaluation, usage-based insurance, zero-inflated Poisson framework  
Summary:  
The article discusses the utilization of driving behavior big data and telematics to understand how people drive and its applications in risk evaluation and insurance pricing. Traditional statistical models struggle to accurately analyze sparse and zero-inflated near-miss events captured by telematics. The study proposes zero-inflated Poisson frameworks that learn latent behavior groups and fit offset-based count models to improve weekly risk predictions. Using a naturalistic dataset from a fleet of commercial drivers, the results show significant improvements over prior models, with better calibration and lower information criteria values. Sensitivity analyses on the EM-based grouping demonstrate robust and interpretable gains, supporting context-aware ratemaking and fairer premiums by recognizing heterogeneous driving styles.<br /><br />Summary: <div>
arXiv:2509.02614v1 Announce Type: cross 
Abstract: Driving behavior big data leverages multi-sensor telematics to understand how people drive and powers applications such as risk evaluation, insurance pricing, and targeted intervention. Usage-based insurance (UBI) built on these data has become mainstream. Telematics-captured near-miss events (NMEs) provide a timely alternative to claim-based risk, but weekly NMEs are sparse, highly zero-inflated, and behaviorally heterogeneous even after exposure normalization. Analyzing multi-sensor telematics and ADAS warnings, we show that the traditional statistical models underfit the dataset. We address these challenges by proposing a set of zero-inflated Poisson (ZIP) frameworks that learn latent behavior groups and fit offset-based count models via EM to yield calibrated, interpretable weekly risk predictions. Using a naturalistic dataset from a fleet of 354 commercial drivers over a year, during which the drivers completed 287,511 trips and logged 8,142,896 km in total, our results show consistent improvements over baselines and prior telematics models, with lower AIC/BIC values in-sample and better calibration out-of-sample. We also conducted sensitivity analyses on the EM-based grouping for the number of clusters, finding that the gains were robust and interpretable. Practically, this supports context-aware ratemaking on a weekly basis and fairer premiums by recognizing heterogeneous driving styles.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Differentiation of Agent-Based Models</title>
<link>https://arxiv.org/abs/2509.03303</link>
<guid>https://arxiv.org/abs/2509.03303</guid>
<content:encoded><![CDATA[
<div> Agent-based models, automatic differentiation, computational burden, parameter calibration, variational inference <br />
<br />
Summary: <br />
Agent-based models (ABMs) are used to simulate complex systems by modeling individual agents and their interactions. However, the large number of agents in such systems requires significant computational resources and calibration of numerous parameters, limiting their widespread adoption. This paper demonstrates that automatic differentiation (AD) techniques can help alleviate these computational challenges by providing gradients of the simulator, making tasks like calibration and sensitivity analysis more efficient. By applying variational inference (VI) techniques enabled by AD, the study shows improved performance and computational savings in calibrating three different ABMs: Axtell's firm model, Sugarscape, and the SIR epidemiological model. This approach enhances the practicality and scalability of ABMs for studying complex systems. <br /> <div>
arXiv:2509.03303v1 Announce Type: cross 
Abstract: Agent-based models (ABMs) simulate complex systems by capturing the bottom-up interactions of individual agents comprising the system. Many complex systems of interest, such as epidemics or financial markets, involve thousands or even millions of agents. Consequently, ABMs often become computationally demanding and rely on the calibration of numerous free parameters, which has significantly hindered their widespread adoption. In this paper, we demonstrate that automatic differentiation (AD) techniques can effectively alleviate these computational burdens. By applying AD to ABMs, the gradients of the simulator become readily available, greatly facilitating essential tasks such as calibration and sensitivity analysis. Specifically, we show how AD enables variational inference (VI) techniques for efficient parameter calibration. Our experiments demonstrate substantial performance improvements and computational savings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape; and the SIR epidemiological model. Our approach thus significantly enhances the practicality and scalability of ABMs for studying complex systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems</title>
<link>https://arxiv.org/abs/2509.03340</link>
<guid>https://arxiv.org/abs/2509.03340</guid>
<content:encoded><![CDATA[
<div> machine learning, bifurcation phenomena, symmetries, flow matching, multistability 

Summary: 
This study addresses the challenge of capturing multiple stable solutions in nonlinear dynamical systems due to symmetry breaking and bifurcation phenomena. The proposed generative framework based on flow matching allows modeling the full probability distribution over bifurcation outcomes, enabling direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. A symmetric matching strategy aligns predicted and target outputs under group actions, facilitating accurate learning in equivariant settings. The method is validated on various systems, demonstrating superior performance in capturing multimodal distributions and symmetry-breaking bifurcations compared to non-probabilistic and variational approaches. Overall, flow matching offers a principled and scalable solution for modeling multistability in high-dimensional systems. <div>
arXiv:2509.03340v1 Announce Type: cross 
Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple coexisting stable solutions, particularly in the presence of symmetry breaking. Deterministic machine learning models struggle to capture this multiplicity, averaging over solutions and failing to represent lower-symmetry outcomes. In this work, we propose a generative framework based on flow matching to model the full probability distribution over bifurcation outcomes. Our method enables direct sampling of multiple valid solutions while preserving system symmetries through equivariant modeling. We introduce a symmetric matching strategy that aligns predicted and target outputs under group actions, allowing accurate learning in equivariant settings. We validate our approach on a range of systems, from toy models to complex physical problems such as buckling beams and the Allen-Cahn equation. Our results demonstrate that flow matching significantly outperforms non-probabilistic and variational methods in capturing multimodal distributions and symmetry-breaking bifurcations, offering a principled and scalable solution for modeling multistability in high-dimensional systems.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Differentiable Boundary Element Solver for Hydrodynamic Sensitivity Analysis of Wave-Structure Interactions</title>
<link>https://arxiv.org/abs/2501.06988</link>
<guid>https://arxiv.org/abs/2501.06988</guid>
<content:encoded><![CDATA[
<div> wave-structure interactions, marine structures, boundary element method, linear potential flow theory, automatic differentiation

Summary:
Accurately predicting wave-structure interactions in marine structures is crucial for effective design and analysis. Current solvers using the boundary element method (BEM) rely on linear potential flow theory but lack the ability to provide sensitivities for system-level applications like design optimization. To address this limitation, a fully differentiable BEM solver has been developed, capable of estimating sensitivities. This advancement allows for precise estimation of wave-structure interaction sensitivity, which is crucial for optimizing designs. By incorporating automatic differentiation (AD) into BEM solvers, a more comprehensive understanding of wave-structure interactions can be achieved, enabling better design and analysis of marine structures. <div>
arXiv:2501.06988v2 Announce Type: replace 
Abstract: Accurately predicting wave-structure interactions is critical for the effective design and analysis of marine structures. This is typically achieved using solvers that employ the boundary element method (BEM), which relies on linear potential flow theory. Precise estimation of the sensitivity of these interactions is equally important for system-level applications such as design optimization. Current BEM solvers are unable to provide these sensitivities as they do not support automatic differentiation (AD). To address these challenges, we have developed a fully differentiable BEM solver
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Data Encoding and Variational Algorithms: A Framework for Hybrid Quantum Classical Machine Learning</title>
<link>https://arxiv.org/abs/2502.11951</link>
<guid>https://arxiv.org/abs/2502.11951</guid>
<content:encoded><![CDATA[
<div> QML, Quantum Machine Learning, classical data pipelines, hybrid quantum-classical models, CQ paradigm
<br />
Summary:
The article discusses the development of Quantum Machine Learning (QML) and its integration of quantum mechanics with classical machine learning. It proposes a broad architecture connecting classical data pipelines with quantum algorithms, emphasizing hybrid quantum-classical models for scalable quantum benefits. The Classical-Quantum (CQ) paradigm is highlighted, using classical encoding strategies to compress information into Hilbert space representations. Variational quantum circuits are explored to overcome device constraints. Experimental comparisons show that small quantum circuits can approximate probabilistic inference with competitive accuracy and robustness to noisy data. The article provides a roadmap for implementing quantum kernels, variational algorithms, and hybrid feedback loops in practice for optimization, computer vision, and medical diagnostics. It emphasizes the importance of strong data encoding and adaptive error protection in moving QML from theory to practice. 
<br /> <div>
arXiv:2502.11951v2 Announce Type: replace 
Abstract: The development of quantum computers has been the stimulus that enables the realization of Quantum Machine Learning (QML), an area that integrates the calculational framework of quantum mechanics with the adaptive properties of classical machine learning. This article suggests a broad architecture that allows the connection between classical data pipelines and quantum algorithms, hybrid quantum-classical models emerge as a promising route to scalable and near-term quantum benefit. At the core of this paradigm lies the Classical-Quantum (CQ) paradigm, in which the qubit states of high-dimensional classical data are encoded using sophisticated classical encoding strategies which encode the data in terms of amplitude and angle of rotation, along with superposition mapping. These techniques allow compression of information exponentially into Hilbert space representations, which, together with reduced sample complexity, allows greater feature expressivity. We also examine variational quantum circuits, quantum gates expressed as trainable variables that run with classical optimizers to overcome decoherence, noise, and gate-depth constraints of the existing Noisy Intermediate-Scale Quantum (NISQ) devices. Experimental comparisons with a Quantum Naive Bayes classifier prove that even small quantum circuits can approximate probabilistic inference with competitive accuracy compared to classical benchmarks, and have much better robustness to noisy data distributionsThis model does not only explain the algorithmic and architectural design of QML, it also offers a roadmap to the implementation of quantum kernels, variational algorithms, and hybrid feedback loops into practice, including optimization, computer vision, and medical diagnostics. The results support the idea that hybrid architectures with strong data encoding and adaptive error protection are key to moving QML out of theory to practice.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Metrics to Meaning: Time to Rethink Evaluation in Human-AI Collaborative Design</title>
<link>https://arxiv.org/abs/2402.07911</link>
<guid>https://arxiv.org/abs/2402.07911</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, human-AI collaboration, interactive design, evaluation methods, user engagement

Summary:
The study explores the interaction between humans and AI systems in the context of creative design. By analyzing the engagement of participants in a co-creative system called The Genetic Car Designer, the research highlights the importance of evaluating human-AI collaborative systems in a multidimensional manner. The system, which employs an interactive evolutionary algorithm, showed that exposure to design suggestions generated by the AI system led to enhanced cognitive and behavioral engagement, resulting in higher-quality design outcomes. The findings suggest that conventional evaluation methods focused solely on behavioral and design metrics may not capture the full extent of user engagement. It is proposed that evaluating human-AI systems holistically, considering emotional, behavioral, and cognitive states of the designer, is crucial for a comprehensive understanding of the user experience and the role of intelligent systems in creative design processes. <div>
arXiv:2402.07911v2 Announce Type: replace-cross 
Abstract: As AI systems increasingly shape decision making in creative design contexts, understanding how humans engage with these tools has become a critical challenge for interactive intelligent systems research. This paper contributes a challenge to rethink how to evaluate human--AI collaborative systems, advocating for a more nuanced and multidimensional approach. Findings from one of the largest field studies to date (n = 808) of a human--AI co-creative system, The Genetic Car Designer, complemented by a controlled lab study (n = 12) are presented. The system is based on an interactive evolutionary algorithm where participants were tasked with designing a simple two dimensional representation of a car. Participants were exposed to galleries of design suggestions generated by an intelligent system, MAP--Elites, and a random control. Results indicate that exposure to galleries generated by MAP--Elites significantly enhanced both cognitive and behavioural engagement, leading to higher-quality design outcomes. Crucially for the wider community, the analysis reveals that conventional evaluation methods, which often focus on solely behavioural and design quality metrics, fail to capture the full spectrum of user engagement. By considering the human--AI design process as a changing emotional, behavioural and cognitive state of the designer, we propose evaluating human--AI systems holistically and considering intelligent systems as a core part of the user experience -- not simply a back end tool.
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel manifolds: nonlinear-augmentation dimensionality reduction using reproducing kernel Hilbert spaces</title>
<link>https://arxiv.org/abs/2509.00224</link>
<guid>https://arxiv.org/abs/2509.00224</guid>
<content:encoded><![CDATA[
<div> Kernel methods, Nonlinear dimensionality reduction, Quadratic manifold, Feature map, Reproducing kernel Hilbert space <br />
<br />
Summary: 
This paper introduces a novel approach to dimensionality reduction called kernel methods-based nonlinear-augmentation dimensionality reduction as an extension of quadratic manifold (QM) dimensionality reduction. The method involves augmenting linear dimensionality reduction with a nonlinear correction term in the reconstruction map to improve accuracy. Unlike previous methods that use least-squares optimal polynomial correction terms, this approach learns an optimal nonlinear correction from a reproducing kernel Hilbert space defined by the user. It allows for the imposition of various nonlinear structures on the correction term, including polynomial and radial basis function-based structures. The method is computationally efficient and exhibits decreasing error as the latent space dimension increases. Comparisons with other dimensionality reduction techniques like proper orthogonal decomposition and existing QM approaches demonstrate the effectiveness of the proposed method on various datasets. <div>
arXiv:2509.00224v1 Announce Type: new 
Abstract: This paper generalizes recent advances on quadratic manifold (QM) dimensionality reduction by developing kernel methods-based nonlinear-augmentation dimensionality reduction. QMs, and more generally feature map-based nonlinear corrections, augment linear dimensionality reduction with a nonlinear correction term in the reconstruction map to overcome approximation accuracy limitations of purely linear approaches. While feature map-based approaches typically learn a least-squares optimal polynomial correction term, we generalize this approach by learning an optimal nonlinear correction from a user-defined reproducing kernel Hilbert space. Our approach allows one to impose arbitrary nonlinear structure on the correction term, including polynomial structure, and includes feature map and radial basis function-based corrections as special cases. Furthermore, our method has relatively low training cost and has monotonically decreasing error as the latent space dimension increases. We compare our approach to proper orthogonal decomposition and several recent QM approaches on data from several example problems.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I-FENN with DeepONets: accelerating simulations in coupled multiphysics problems</title>
<link>https://arxiv.org/abs/2509.00604</link>
<guid>https://arxiv.org/abs/2509.00604</guid>
<content:encoded><![CDATA[
<div> Keywords: DeepONet, Finite Element Method, multiphysics simulations, thermoelasticity, poroelasticity

Summary: 
This article introduces a novel framework that integrates DeepONet with the Finite Element Method to address coupled thermoelasticity and poroelasticity problems efficiently. The framework, called I-FENN, employs neural networks as PDE solvers within FEM, resulting in a hybrid staggered solver. By decoupling multiphysics interactions, the framework reduces computational costs while maintaining flexibility across various scenarios. The modified DeepONet architecture allows for multiple inputs and an efficient strategy for enforcing boundary conditions on distinct boundaries. Numerical examples demonstrate the computational efficiency, accuracy, and generalization capabilities of the proposed work, even under unseen loading conditions. The computational savings increase with model complexity while maintaining high accuracy levels in challenging regions of the domain. Overall, the framework shows promise in tackling high-dimensional, large-scale coupled multiphysics simulations. 

Summary: <div>
arXiv:2509.00604v1 Announce Type: new 
Abstract: Coupled multiphysics simulations for high-dimensional, large-scale problems can be prohibitively expensive due to their computational demands. This article presents a novel framework integrating a deep operator network (DeepONet) with the Finite Element Method (FEM) to address coupled thermoelasticity and poroelasticity problems. This integration occurs within the context of I-FENN, a framework where neural networks are directly employed as PDE solvers within FEM, resulting in a hybrid staggered solver. In this setup, the mechanical field is computed using FEM, while the other coupled field is predicted using a neural network (NN). By decoupling multiphysics interactions, the hybrid framework reduces computational cost by simplifying calculations and reducing the FEM unknowns, while maintaining flexibility across unseen scenarios. The proposed work introduces a new I-FENN architecture with extended generalizability due to the DeepONets ability to efficiently address several combinations of natural boundary conditions and body loads. A modified DeepONet architecture is introduced to accommodate multiple inputs, along with a streamlined strategy for enforcing boundary conditions on distinct boundaries. We showcase the applicability and merits of the proposed work through numerical examples covering thermoelasticity and poroelasticity problems, demonstrating computational efficiency, accuracy, and generalization capabilities. In all examples, the test cases involve unseen loading conditions. The computational savings scale with the model complexity while preserving an accuracy of more than 95\% in the non-trivial regions of the domain.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new definition of peridynamic damage for thermo-mechanical fracture modeling</title>
<link>https://arxiv.org/abs/2509.01079</link>
<guid>https://arxiv.org/abs/2509.01079</guid>
<content:encoded><![CDATA[
<div> Thermo-mechanical fracture modeling, thermal failure, heat conduction, continuum mechanics, peridynamic model<br />
Summary: A novel thermo-mechanical fracture modeling approach is proposed, combining classical continuum mechanics and peridynamic models to address thermal failure issues. The model incorporates a CCM/PD alternating solution for accurate calculations using finite element discretization. A new definition of PD damage is introduced, considering both the number and distribution of broken bonds for capturing damage in various directions. Validation against analytical solutions and simulations of crack propagation demonstrate the model's effectiveness in simulating complex thermal fractures and understanding initiation and propagation mechanisms. <div>
arXiv:2509.01079v1 Announce Type: new 
Abstract: A thermo-mechanical fracture modeling is proposed to address thermal failure issues, where the temperature field is calculated by a heat conduction model based on classical continuum mechanics (CCM), while the deformation field with discontinuities is calculated by the peridynamic (PD) model. The model is calculated by a CCM/PD alternating solution based on the finite element discretization, which ensures the calculation accuracy and facilitates engineering applications. The original PD model defines damage solely based on the number of broken bonds in the vicinity of the material point, neglecting the distribution of these bonds. To address this limitation, a new definition of the PD damage accounting for both the number of broken bonds and their specific distribution is proposed. As a result, damage in various directions can be captured, enabling more realistic thermal fracture simulations based on a unified mesh discretization. The effectiveness of the proposed model is validated by comparing numerical examples with analytical solutions. Moreover, simulation results of quasi-static and dynamic crack propagation demonstrate the model's ability to aid in understanding the initiation and propagation mechanisms of complex thermal fractures.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAMS: Residual-based adversarial-gradient moving sample method for scientific machine learning in solving partial differential equations</title>
<link>https://arxiv.org/abs/2509.01234</link>
<guid>https://arxiv.org/abs/2509.01234</guid>
<content:encoded><![CDATA[
<div> Neural networks, PDEs, PINNs, SciML, RAMS 
Summary: 
Physics-informed neural networks (PINNs) and neural operators are powerful tools for solving PDEs. Increasing the training sample size enhances network performance but increases computational costs. To address this trade-off, the residual-based adversarial-gradient moving sample (RAMS) method is proposed. RAMS moves samples based on the adversarial gradient direction to maximize the PDE residual, improving sampling efficiency for high-dimensional problems. It can be integrated into existing sampling methods. Extensive experiments show RAMS's effectiveness in PINNs for high-dimensional PDEs and operator learning tasks, making it the first efficient adaptive sampling approach for operator learning in the SciML field. <br /><br />Summary: <div>
arXiv:2509.01234v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) and neural operators, two leading scientific machine learning (SciML) paradigms, have emerged as powerful tools for solving partial differential equations (PDEs). Although increasing the training sample size generally enhances network performance, it also increases computational costs for physics-informed or data-driven training. To address this trade-off, different sampling strategies have been developed to sample more points in regions with high PDE residuals. However, existing sampling methods are computationally demanding for high-dimensional problems, such as high-dimensional PDEs or operator learning tasks. Here, we propose a residual-based adversarial-gradient moving sample (RAMS) method, which moves samples according to the adversarial gradient direction to maximize the PDE residual via gradient-based optimization. RAMS can be easily integrated into existing sampling methods. Extensive experiments, ranging from PINN applied to high-dimensional PDEs to physics-informed and data-driven operator learning problems, have been conducted to demonstrate the effectiveness of RAMS. Notably, RAMS represents the first efficient adaptive sampling approach for operator learning, marking a significant advancement in the SciML field.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A continuum multi-species biofilm model with a novel interaction scheme</title>
<link>https://arxiv.org/abs/2509.01274</link>
<guid>https://arxiv.org/abs/2509.01274</guid>
<content:encoded><![CDATA[
<div> biofilms, microorganisms, mathematical modeling, biofilm interactions, antibiotic agents 

Summary:
This article introduces a comprehensive multi-species continuum-based biofilm model that aims to understand the interactions between different species of microorganisms within biofilms. The model, derived using Hamilton's principle of stationary action, can replicate various biofilm interactions with an arbitrary number of species and incorporates the effects of nutrient sources and antibiotic agents on biofilm behavior. By combining mathematical modeling with in vitro and in vivo experiments, researchers can gain more insights into biofilm dynamics while reducing costs. The model demonstrates good quantitative agreement with biofilm behavior, showcasing its potential utility for researchers looking to study biofilm systems. <div>
arXiv:2509.01274v1 Announce Type: new 
Abstract: Biofilms are complex structures which are inhabited by numerous amount of different species of microorganisms. Due to their ubiquity, they influence human life on an everyday basis. It is therefore important to understand the interactions between different biofilm components and reactions to outside conditions. For this purpose, mathematical models and in silico experiments have proven themselves to be fundamental. In combination with in vitro and in vivo experiments, they can give more insights and focus researchers' attention, reducing costs in the process. In this work, a comprehensive multi-species continuum-based biofilm model is presented. This model is capable of replicating a variety of different biofilm interactions with an arbitrary number of species, while still being comprehensive to encourage usage by researchers less familiar with mathematical modeling. In addition to a nutrient source, antibiotic agents and their effect on the biofilm can also be depicted. The model is derived using Hamilton's principle of stationary action, ensuring thermodynamic consistency automatically. The results show good quantitative agreement with biofilm behavior.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Alpha Weighting with PPO: Enhancing Prompt-Based LLM-Generated Alphas in Quant Trading</title>
<link>https://arxiv.org/abs/2509.01393</link>
<guid>https://arxiv.org/abs/2509.01393</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, proximal policy optimization, large language model, formulaic alphas, stock trading strategies

Summary:
This paper presents a novel approach using reinforcement learning with Proximal Policy Optimization (PPO) to dynamically optimize the weights of multiple large language model-generated formulaic alphas for stock trading strategies. The study leverages a deepseek-r1-distill-llama-70b model to generate fifty alphas for five major stocks and applies PPO to adjust their weights in real time. The experimental results show that the PPO-optimized strategy outperforms an equal-weighted alpha portfolio and traditional benchmarks like Nikkei 225, S&amp;P 500, and Hang Seng Index. This highlights the significance of using reinforcement learning in the allocation of alpha weights and demonstrates the potential of combining large language model-generated signals with adaptive optimization for robust financial forecasting and trading. <div>
arXiv:2509.01393v1 Announce Type: new 
Abstract: This paper proposes a reinforcement learning framework that employs Proximal Policy Optimization (PPO) to dynamically optimize the weights of multiple large language model (LLM)-generated formulaic alphas for stock trading strategies. Formulaic alphas are mathematically defined trading signals derived from price, volume, sentiment, and other data. Although recent studies have shown that LLMs can generate diverse and effective alphas, a critical challenge lies in how to adaptively integrate them under varying market conditions. To address this gap, we leverage the deepseek-r1-distill-llama-70b model to generate fifty alphas for five major stocks: Apple, HSBC, Pepsi, Toyota, and Tencent, and then use PPO to adjust their weights in real time. Experimental results demonstrate that the PPO-optimized strategy achieves strong returns and high Sharpe ratios across most stocks, outperforming both an equal-weighted alpha portfolio and traditional benchmarks such as the Nikkei 225, S&amp;P 500, and Hang Seng Index. The findings highlight the importance of reinforcement learning in the allocation of alpha weights and show the potential of combining LLM-generated signals with adaptive optimization for robust financial forecasting and trading.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisit of Two-dimensional CEM on Crack Branching: from Single Crack-tip Tracking to Multiple Crack-tips Tracking</title>
<link>https://arxiv.org/abs/2509.01827</link>
<guid>https://arxiv.org/abs/2509.01827</guid>
<content:encoded><![CDATA[
<div> algorithm, crack tracking, fracture energy release rate, crack branching, GPU acceleration
Summary:
The article introduces the Multiple Crack-tips Tracking algorithm in two-dimensional Crack Element Model (MCT-2D-CEM) for predicting complex crack patterns in dynamic fracturing problems. The algorithm is developed to model advancements like crack branching and fragmentation. It utilizes a fracture energy release rate formulation for split elementary topology and includes benchmark examples to demonstrate its efficiency. The MCT-2D-CEM can also handle single crack propagation while introducing extra micro-cracks. The use of GPU acceleration in two-dimensional simulations ensures high computational efficiency, consistency, and accuracy. Overall, the algorithm offers a comprehensive solution for tracking and predicting advanced crack patterns in dynamic fracturing scenarios. 
<br /><br />Summary: <div>
arXiv:2509.01827v1 Announce Type: new 
Abstract: In this work, a Multiple Crack-tips Tracking algorithm in two-dimensional Crack Element Model (MCT-2D-CEM) is developed, aiming at modeling and predicting advanced and complicated crack patterns in two-dimensional dynamic fracturing problems, such as crack branching and fragmentation. Based on the developed fracture energy release rate formulation of split elementary topology, the Multiple Crack-tips Tracking algorithm is proposed and a series of benchmark examples are provided to validate effectiveness and efficiency in modeling crack branching and fragmentation. Besides, the proposed MCR-2D-CEM can still model single crack propagation but extra micro-cracks are introduced. GPU acceleration is employed in all two-dimensional simulations, providing high computational efficiency, consistency, and accuracy.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Fluid Dynamics Optimization of F1 Front Wing using Physics Informed Neural Networks</title>
<link>https://arxiv.org/abs/2509.01963</link>
<guid>https://arxiv.org/abs/2509.01963</guid>
<content:encoded><![CDATA[
<div> Neural Network, Computational Fluid Dynamics, Formula 1, Aerodynamics, Physics-Informed<br />
<br />
Summary: 
A Physics-Informed Neural Network (PINN) is proposed for fast prediction of Formula 1 front wing aerodynamic coefficients in response to new FIA regulations. The hybrid loss function combines CFD data with fluid dynamics principles to ensure accurate predictions while reducing computational time. With high R-squared values for drag and lift coefficient prediction, the PINN model offers F1 teams an efficient tool for design space exploration within the budget and time constraints. The physics-informed framework maintains adherence to fundamental aerodynamic principles, making it a valuable asset for aerodynamic development in the F1 industry. <div>
arXiv:2509.01963v1 Announce Type: new 
Abstract: In response to recent FIA regulations reducing Formula 1 team wind tunnel hours (from 320 hours for last-place teams to 200 hours for championship leaders) and strict budget caps of 135 million USD per year, more efficient aerodynamic development tools are needed by teams. Conventional computational fluid dynamics (CFD) simulations, though offering high fidelity results, require large computational resources with typical simulation durations of 8-24 hours per configuration analysis. This article proposes a Physics-Informed Neural Network (PINN) for the fast prediction of Formula 1 front wing aerodynamic coefficients. The suggested methodology combines CFD simulation data from SimScale with first principles of fluid dynamics through a hybrid loss function that constrains both data fidelity and physical adherence based on Navier-Stokes equations. Training on force and moment data from 12 aerodynamic features, the PINN model records coefficient of determination (R-squared) values of 0.968 for drag coefficient and 0.981 for lift coefficient prediction while lowering computational time. The physics-informed framework guarantees that predictions remain adherent to fundamental aerodynamic principles, offering F1 teams an efficient tool for the fast exploration of design space within regulatory constraints.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoder-based non-intrusive model order reduction in continuum mechanics</title>
<link>https://arxiv.org/abs/2509.02237</link>
<guid>https://arxiv.org/abs/2509.02237</guid>
<content:encoded><![CDATA[
<div> Autoencoder, reduced-order modeling, continuum mechanics, deep learning, surrogate models

Summary:

This article introduces a non-intrusive framework for reduced-order modeling in continuum mechanics using Autoencoder-based techniques. The framework comprises three stages: compression of high-dimensional finite element solutions into a latent space, mapping problem parameters to latent codes through regression networks, and reconstructing full-field solutions from input parameters. Two key extensions, including a force-augmented variant and a multi-field architecture, enhance the framework's capabilities, allowing for accurate predictions in complex scenarios such as thermo-mechanical coupling. The proposed method is validated on various benchmark problems and demonstrates accurate reconstructions of high-fidelity solutions without intruding on the original model. By combining deep learning with dimensionality reduction, this approach offers an efficient and extensible solution for building surrogate models in continuum mechanics. The publicly available implementation offers a foundation for integrating data-driven model order reduction into various applications, including uncertainty quantification, optimization, and digital twins.<br /><br />Summary: <div>
arXiv:2509.02237v1 Announce Type: new 
Abstract: We propose a non-intrusive, Autoencoder-based framework for reduced-order modeling in continuum mechanics. Our method integrates three stages: (i) an unsupervised Autoencoder compresses high-dimensional finite element solutions into a compact latent space, (ii) a supervised regression network maps problem parameters to latent codes, and (iii) an end-to-end surrogate reconstructs full-field solutions directly from input parameters.
  To overcome limitations of existing approaches, we propose two key extensions: a force-augmented variant that jointly predicts displacement fields and reaction forces at Neumann boundaries, and a multi-field architecture that enables coupled field predictions, such as in thermo-mechanical systems. The framework is validated on nonlinear benchmark problems involving heterogeneous composites, anisotropic elasticity with geometric variation, and thermo-mechanical coupling. Across all cases, it achieves accurate reconstructions of high-fidelity solutions while remaining fully non-intrusive.
  These results highlight the potential of combining deep learning with dimensionality reduction to build efficient and extensible surrogate models. Our publicly available implementation provides a foundation for integrating data-driven model order reduction into uncertainty quantification, optimization, and digital twin applications.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning-Fueled Modelfluid for Flowsheet Optimization</title>
<link>https://arxiv.org/abs/2509.02242</link>
<guid>https://arxiv.org/abs/2509.02242</guid>
<content:encoded><![CDATA[
<div> machine learning, process optimization, thermodynamic data, distillation, entrainer selection

Summary:
This article discusses the use of machine learning techniques to predict thermodynamic mixture properties for process optimization in chemical engineering. The authors present a novel modelfluid representation that integrates machine learning predicted data directly into flowsheet optimization, specifically tailored for distillation processes. The approach is built on physically interpretable features derived from vapor-liquid equilibrium phenomena, ensuring compatibility with existing simulation tools and optimization methods. The study demonstrates the accuracy and efficiency of this machine learning-fueled modelfluid by applying it to the problem of entrainer selection for azeotropic separation. Results show that the framework successfully identifies optimal entrainers with high fidelity compared to traditional models. This work provides a practical way to incorporate large-scale property prediction into process design, overcoming limitations of traditional thermodynamic models and complex equations of state.

<br /><br />Summary: <div>
arXiv:2509.02242v1 Announce Type: new 
Abstract: Process optimization in chemical engineering may be hindered by the limited availability of reliable thermodynamic data for fluid mixtures.
  Remarkable progress is being made in predicting thermodynamic mixture properties by machine learning techniques. The vast information provided by these prediction methods enables new possibilities in process optimization.
  This work introduces a novel modelfluid representation that is designed to seamlessly integrate these ML-predicted data directly into flowsheet optimization. Tailored for distillation, our approach is built on physically interpretable and continuous features derived from core vapor liquid equilibrium phenomena. This ensures compatibility with existing simulation tools and gradient-based optimization. We demonstrate the power and accuracy of this ML-fueled modelfluid by applying it to the problem of entrainer selection for an azeotropic separation. The results show that our framework successfully identifies optimal, thermodynamically consistent entrainers with high fidelity compared to conventional models.
  Ultimately, this work provides a practical pathway to incorporate large-scale property prediction into efficient process design and optimization, overcoming the limitations of both traditional thermodynamic models and complex molecular-based equations of state.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Electromechanical computational model of the human stomach</title>
<link>https://arxiv.org/abs/2509.02486</link>
<guid>https://arxiv.org/abs/2509.02486</guid>
<content:encoded><![CDATA[
<div> Keywords: gastric electromechanics, computational framework, peristalsis, motility disorders, organ scale

Summary:
The article presents a new computational framework for modeling human gastric electromechanics to study digestion and related motility disorders. It addresses the limitations of existing approaches by incorporating spatial heterogeneity, anisotropic deformations, and active-strain dynamics. The framework combines a rotation-free shell formulation with a constrained mixture material model, allowing for realistic simulation of gastric peristalsis. It can reproduce key features of gastric motility such as slow-wave entrainment, conduction velocity gradients, and peristaltic contractions. This new tool enables robust simulations of the entire stomach at the organ scale, offering promise for in-depth studies of both normal physiology and pathological conditions affecting gastric motility. <div>
arXiv:2509.02486v1 Announce Type: new 
Abstract: The stomach plays a central role in digestion through coordinated muscle contractions, known as gastric peristalsis, driven by slow-wave electrophysiology. Understanding this process is critical for treating motility disorders such as gastroparesis, dyspepsia, and gastroesophageal reflux disease. Computer simulations can be a valuable tool to deepen our understanding of these disorders and help to develop new therapies. However, existing approaches often neglect spatial heterogeneity, fail to capture large anisotropic deformations, or rely on computationally expensive three-dimensional formulations. We present here a computational framework of human gastric electromechanics, that combines a nonlinear, rotation-free shell formulation with a constrained mixture material model. The formulation incorporates active-strain, constituent-specific prestress, and spatially non-uniform parameter fields. Numerical examples demonstrate that the framework can reproduce characteristic features of gastric motility, including slow-wave entrainment, conduction velocity gradients, and large peristaltic contractions with physiologically realistic amplitudes. The proposed framework enables robust electromechanical simulations of the whole stomach at the organ scale. It thus provides a promising basis for future in silico studies of both physiological function and pathological motility disorders.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized Renewable Energy Planning MDP for Socially-Equitable Electricity Coverage in the US</title>
<link>https://arxiv.org/abs/2509.00008</link>
<guid>https://arxiv.org/abs/2509.00008</guid>
<content:encoded><![CDATA[
<div> optimization, renewable energy, social equity, electricity distribution, clean energy 

Summary: 
The study introduces a Markov Decision Process framework to optimize renewable energy allocation in the electricity distribution system while addressing social equity concerns. By considering budget constraints, energy demand variability, and social vulnerability indicators across major U.S. cities, the model evaluates policy alternatives for achieving equitable clean energy transitions. Numerical experiments show that an equity-focused approach can increase renewable energy penetration by 32.9% and reduce underserved low-income populations by 55% compared to conventional methods. The expert policy performed the best, while the Monte Carlo Tree Search baseline showed competitive performance with lower budget utilization. This study highlights that fair distribution of clean energy resources is possible without compromising system performance, offering a pathway to integrate social equity considerations with climate goals and provide inclusive access to clean power infrastructure. 

<br /><br />Summary: <div>
arXiv:2509.00008v1 Announce Type: cross 
Abstract: Traditional power grid infrastructure presents significant barriers to renewable energy integration and perpetuates energy access inequities, with low-income communities experiencing disproportionately longer power outages. This study develops a Markov Decision Process (MDP) framework to optimize renewable energy allocation while explicitly addressing social equity concerns in electricity distribution. The model incorporates budget constraints, energy demand variability, and social vulnerability indicators across eight major U.S. cities to evaluate policy alternatives for equitable clean energy transitions. Numerical experiments compare the MDP-based approach against baseline policies including random allocation, greedy renewable expansion, and expert heuristics. Results demonstrate that equity-focused optimization can achieve 32.9% renewable energy penetration while reducing underserved low-income populations by 55% compared to conventional approaches. The expert policy achieved the highest reward, while the Monte Carlo Tree Search baseline provided competitive performance with significantly lower budget utilization, demonstrating that fair distribution of clean energy resources is achievable without sacrificing overall system performance and providing ways for integrating social equity considerations with climate goals and inclusive access to clean power infrastructure.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Parameter Fields in Multi-Physics PDEs from Scarce Measurements</title>
<link>https://arxiv.org/abs/2509.00203</link>
<guid>https://arxiv.org/abs/2509.00203</guid>
<content:encoded><![CDATA[
<div> parameterized partial differential equations, parameter estimation methods, sparse measurements, Neptune, parameter fields<br />
<br />
Summary: Neptune is a novel method for inferring parameter fields from sparse measurements of system responses in parameterized partial differential equations. It addresses challenges in accurately estimating parameters with nonlinear and spatiotemporal variations. Neptune outperforms existing methods like sparse identification and PINNs by reducing parameter estimation errors significantly and improving dynamic response prediction accuracy. It achieves reliable parameter inference from a small number of observations and exhibits superior extrapolation capabilities compared to PINNs. Neptune's performance in various physical and biomedical problems makes it a promising tool for applications in engineering and healthcare, offering data-efficient and robust parameter estimation for complex systems. <div>
arXiv:2509.00203v1 Announce Type: cross 
Abstract: Parameterized partial differential equations (PDEs) underpin the mathematical modeling of complex systems in diverse domains, including engineering, healthcare, and physics. A central challenge in using PDEs for real-world applications is to accurately infer the parameters, particularly when the parameters exhibit non-linear and spatiotemporal variations. Existing parameter estimation methods, such as sparse identification and physics-informed neural networks (PINNs), struggle in such cases, especially with nonlinear dynamics, multiphysics interactions, or limited observations of the system response. To address these challenges, we introduce Neptune, a general-purpose method capable of inferring parameter fields from sparse measurements of system responses. Neptune employs independent coordinate neural networks to continuously represent each parameter field in physical space or in state variables. Across various physical and biomedical problems, where direct parameter measurements are prohibitively expensive or unattainable, Neptune significantly outperforms existing methods, achieving robust parameter estimation from as few as 50 observations, reducing parameter estimation errors by two orders of magnitude and dynamic response prediction errors by a factor of ten compared to PINNs. Furthermore, Neptune exhibits superior extrapolation capabilities, enabling accurate predictions in regimes beyond training data where PINN fail. By facilitating reliable and data-efficient parameter inference, Neptune promises broad transformative impacts in engineering, healthcare, and beyond.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stabilization techniques for immersogeometric analysis of plate and shell problems in explicit dynamics</title>
<link>https://arxiv.org/abs/2509.00522</link>
<guid>https://arxiv.org/abs/2509.00522</guid>
<content:encoded><![CDATA[
<div> Finite element, plate, shell, immersed finite element, lumped mass matrix <br />
<br />
Summary: <br />
The article discusses the challenges faced in dynamic analyses of slender structures using finite element plate and shell formulations due to high order partial differential equations and badly cut elements in immersed finite element discretizations. The critical time step in explicit dynamics is constrained, and lumping the mass matrix, while increasing the critical time step, can lead to spurious oscillations. The authors extend their previous work to enable stable immersogeometric analysis of plate and shell problems with lumped mass matrices by using polynomial extensions. This technique restores accuracy comparable to boundary-fitted discretizations, providing a solution to the issues faced in dynamic analysis of slender structures. <div>
arXiv:2509.00522v1 Announce Type: cross 
Abstract: Finite element plate and shell formulations are ubiquitous in structural analysis for modeling all kinds of slender structures, both for static and dynamic analyses. The latter are particularly challenging as the high order nature of the underlying partial differential equations and the slenderness of the structures all impose a stringent constraint on the critical time step in explicit dynamics. Unfortunately, badly cut elements in immersed finite element discretizations further aggravate the issue. While lumping the mass matrix often increases the critical time step, it might also trigger spurious oscillations in the approximate solution thereby compromising the numerical solution. In this article, we extend our previous work in \cite{voet2025stabilization} to allow stable immersogeometric analysis of plate and shell problems with lumped mass matrices. This technique is based on polynomial extensions and restores a level of accuracy comparable to boundary-fitted discretizations.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures</title>
<link>https://arxiv.org/abs/2509.00633</link>
<guid>https://arxiv.org/abs/2509.00633</guid>
<content:encoded><![CDATA[
<div> thermal vulnerabilities, memory wall, HBM architectures, performance degradation attacks, thermal wave<br />
<br />
Summary: 3D-stacked High Bandwidth Memory (HBM) architectures address the memory wall challenge but are vulnerable to thermal attacks due to vertical adjacency. Adversaries could exploit this by injecting intense heat pulses from adjacent memory banks, creating a convergent thermal wave that delays victim applications. These attacks do not access out-of-range memory, bypassing security tests and memory management policies. Detection is difficult as the attack mimics legitimate workloads. <div>
arXiv:2509.00633v1 Announce Type: cross 
Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance memory interactions to address the well-known performance challenge, namely the memory wall. However, these architectures are susceptible to thermal vulnerabilities due to the inherent vertical adjacency that occurs during the manufacturing process of HBM architectures. We anticipate that adversaries may exploit the intense vertical and lateral adjacency to design and develop thermal performance degradation attacks on the memory banks that host data/instructions from victim applications. In such attacks, the adversary manages to inject short and intense heat pulses from vertically and/or laterally adjacent memory banks, creating a convergent thermal wave that maximizes impact and delays the victim application from accessing its data/instructions. As the attacking application does not access any out-of-range memory locations, it can bypass both design-time security tests and the operating system's memory management policies. In other words, since the attack mimics legitimate workloads, it will be challenging to detect.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resting-state fMRI Analysis using Quantum Time-series Transformer</title>
<link>https://arxiv.org/abs/2509.00711</link>
<guid>https://arxiv.org/abs/2509.00711</guid>
<content:encoded><![CDATA[
<div> fMRI, Quantum Time-series Transformer, neural biomarkers, computational complexity, interpretability analysis <br />
Summary:<br />
The article introduces a Quantum Time-series Transformer, a novel quantum-enhanced transformer architecture for analyzing resting-state fMRI data. It addresses the limitations of classical transformer models by reducing computational complexity, parameter counts, and data requirements. The Quantum Time-series Transformer outperforms traditional models in predictive performance, especially in small-sample scenarios, and reliably identifies neural biomarkers related to ADHD. By leveraging quantum techniques like Linear Combination of Unitaries and Quantum Singular Value Transformation, this approach shows promise in efficiently modeling complex brain dynamics and enhancing clinical interpretability in computational neuroscience. <div>
arXiv:2509.00711v1 Announce Type: cross 
Abstract: Resting-state functional magnetic resonance imaging (fMRI) has emerged as a pivotal tool for revealing intrinsic brain network connectivity and identifying neural biomarkers of neuropsychiatric conditions. However, classical self-attention transformer models--despite their formidable representational power--struggle with quadratic complexity, large parameter counts, and substantial data requirements. To address these barriers, we introduce a Quantum Time-series Transformer, a novel quantum-enhanced transformer architecture leveraging Linear Combination of Unitaries and Quantum Singular Value Transformation. Unlike classical transformers, Quantum Time-series Transformer operates with polylogarithmic computational complexity, markedly reducing training overhead and enabling robust performance even with fewer parameters and limited sample sizes. Empirical evaluation on the largest-scale fMRI datasets from the Adolescent Brain Cognitive Development Study and the UK Biobank demonstrates that Quantum Time-series Transformer achieves comparable or superior predictive performance compared to state-of-the-art classical transformer models, with especially pronounced gains in small-sample scenarios. Interpretability analyses using SHapley Additive exPlanations further reveal that Quantum Time-series Transformer reliably identifies clinically meaningful neural biomarkers of attention-deficit/hyperactivity disorder (ADHD). These findings underscore the promise of quantum-enhanced transformers in advancing computational neuroscience by more efficiently modeling complex spatio-temporal dynamics and improving clinical interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Modeling for Personalized Transcranial Electrical Stimulation: Theory, Tools, and Applications</title>
<link>https://arxiv.org/abs/2509.01192</link>
<guid>https://arxiv.org/abs/2509.01192</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized tES, computational modeling, individualized stimulation optimization, head modeling, optimization algorithms<br />
Summary:<br />
The review focuses on the advancements in personalized transcranial electrical stimulation (tES) through computational modeling. It emphasizes the importance of individualized stimulation optimization due to the significant variability in brain anatomy and physiology among individuals. The review systematically examines recent developments in forward and inverse modeling techniques to simulate personalized electric fields and optimize stimulation parameters. It discusses the progress in constructing subject-specific head conductor models, utilizing optimization algorithms, and integrating multimodal brain data. Recent advancements have led to dynamic and individualized stimulation planning, moving away from traditional trial-and-error approaches. The review highlights the challenges, opportunities, and future directions in achieving precision neuromodulation in research and clinical settings. <div>
arXiv:2509.01192v1 Announce Type: cross 
Abstract: Objective. Personalized transcranial electrical stimulation (tES) has gained growing attention due to the substantial inter-individual variability in brain anatomy and physiology. While previous reviews have discussed the physiological mechanisms and clinical applications of tES, there remains a critical gap in up-to-date syntheses focused on the computational modeling frameworks that enable individualized stimulation optimization. Approach. This review presents a comprehensive overview of recent advances in computational techniques supporting personalized tES. We systematically examine developments in forward modeling for simulating individualized electric fields, as well as inverse modeling approaches for optimizing stimulation parameters. We critically evaluate progress in head modeling pipelines, optimization algorithms, and the integration of multimodal brain data. Main results. Recent advances have substantially accelerated the construction of subject-specific head conductor models and expanded the landscape of optimization methods, including multi-objective optimization and brain network-informed optimization. These advances allow for dynamic and individualized stimulation planning, moving beyond empirical trial-and-error approaches.Significance. By integrating the latest developments in computational modeling for personalized tES, this review highlights current challenges, emerging opportunities, and future directions for achieving precision neuromodulation in both research and clinical contexts.
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Bifurcation Handling in Physics-Based Reduced-Order Vascular Hemodynamic Models</title>
<link>https://arxiv.org/abs/2508.21165</link>
<guid>https://arxiv.org/abs/2508.21165</guid>
<content:encoded><![CDATA[
<div> machine learning, cardiovascular flows, reduced-order models, bifurcation coefficients, numerical framework

Summary:
- The study presents a numerical framework that combines machine learning-predicted bifurcation coefficients with zero-dimensional (0D) hemodynamic reduced-order models (ROMs) to enhance accuracy while maintaining computational efficiency.
- A resistor-resistor-inductor (RRI) model utilizing neural networks is developed to predict pressure-flow relationships based on bifurcation geometry, incorporating linear and quadratic resistances and inductive effects.
- Non-dimensionalization is used to reduce training data requirements, and a priori flow split prediction is employed for better bifurcation characterization.
- The RRI model is integrated into a 0D model using an optimization-based solution strategy, demonstrating significant accuracy improvements, especially at high Reynolds numbers and in complex vascular networks.
- The enhanced 0D models enable real-time hemodynamic modeling for clinical decision support, uncertainty quantification, and digital twins in cardiovascular biomedical engineering.

<br /><br />Summary: <div>
arXiv:2508.21165v1 Announce Type: new 
Abstract: Three-dimensional (3D) finite-element simulations of cardiovascular flows provide high-fidelity predictions to support cardiovascular medicine, but their high computational cost limits clinical practicality. Reduced-order models (ROMs) offer computationally efficient alternatives but suffer reduced accuracy, particularly at vessel bifurcations where complex flow physics are inadequately captured by standard Poiseuille flow assumptions. We present an enhanced numerical framework that integrates machine learning-predicted bifurcation coefficients into zero-dimensional (0D) hemodynamic ROMs to improve accuracy while maintaining computational efficiency. We develop a resistor-resistor-inductor (RRI) model that uses neural networks to predict pressure-flow relationships from bifurcation geometry, incorporating linear and quadratic resistances along with inductive effects. The method employs non-dimensionalization to reduce training data requirements and apriori flow split prediction for improved bifurcation characterization. We incorporate the RRI model into a 0D model using an optimization-based solution strategy. We validate the approach in isolated bifurcations and vascular trees, across Reynolds numbers from 0 to 5,500, defining ROM accuracy by comparison to 3D finite element simulation. Results demonstrate substantial accuracy improvements: averaged across all trees and Reynolds numbers, the RRI method reduces inlet pressure errors from 54 mmHg (45%) for standard 0D models to 25 mmHg (17%), while a simplified resistor-inductor (RI) variant achieves 31 mmHg (26%) error. The enhanced 0D models show particular effectiveness at high Reynolds numbers and in extensive vascular networks. This hybrid numerical approach enables accurate, real-time hemodynamic modeling for clinical decision support, uncertainty quantification, and digital twins in cardiovascular biomedical engineering.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A hyperreduced manifold learning approach to nonlinear model order reduction for the homogenisation of hyperelastic RVEs</title>
<link>https://arxiv.org/abs/2508.21527</link>
<guid>https://arxiv.org/abs/2508.21527</guid>
<content:encoded><![CDATA[
<div> Keywords: graph-based manifold learning, nonlinear Galerkin-reduction, model order reduction, hyperreduction methods, online computational costs <br />
Summary: 
The article presents a graph-based manifold learning scheme for nonlinear Galerkin-reduction in quasi-static solid mechanical problems. This approach allows for the creation of nonlinear approximation spaces that closely represent nonlinear solution manifolds. By integrating hyperreduction methods, the scheme significantly reduces online computational costs while maintaining high accuracy. The algorithmic complexity is independent of the original system size, and improvements to the local online linearization scheme enhance performance and robustness. In an example problem, the model order reduction scheme accelerates computations by over two orders of magnitude with minimal training data and negligible loss of accuracy. The approach outperforms alternative methods in the accuracy-runtime trade-off, showcasing its efficiency and effectiveness in reducing computational costs in nonlinear solid mechanics problems. <br /><br />Summary: <div>
arXiv:2508.21527v1 Announce Type: new 
Abstract: In a recent work, we proposed a graph-based manifold learning scheme for the nonlinear Galerkin-reduction of quasi-static solid mechanical problems [1]. The resulting nonlinear approximation spaces can closely and flexibly represent nonlinear solution manifolds. The present work discusses how this nonlinear model order reduction (MOR) approach can be employed to reduce online computational costs by multiple orders of magnitude while retaining high levels of accuracy. We integrate two popular hyperreduction methods into the nonlinear MOR framework and discuss how we achieve an algorithmic complexity which is independent from the original system size. Furthermore, improvements are made to the local online linearisation scheme for the sake of performance and robustness. On an example RVE problem, the MOR scheme accelerates computations by more than two orders of magnitude with little training data and negligible loss of accuracy. Additionally, the algorithm Pareto-dominates alternative approaches in the trade-off between accuracy and runtime on the considered example.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIPPO: A Graph-based, Iterative, Printing-Path Optimization Slicer for Architected Lattices</title>
<link>https://arxiv.org/abs/2508.21694</link>
<guid>https://arxiv.org/abs/2508.21694</guid>
<content:encoded><![CDATA[
<div> Graph-based, Iterative, Printing-Path Optimization, lattice structures, 3D printing, mechanical properties, open-source slicing platform, fabrication

Summary:
GIPPO is an open-source slicing platform that optimizes printing trajectories for complex lattice designs using a modified version of Prim's algorithm. It improves shape fidelity, reduces local thickness deviations, eliminates missing struts, and minimizes excess material deposition. GIPPO outperforms conventional slicing software in fabricating architected lattice structures made of thermoplastic polyurethane through fused deposition modeling. The optimization of printing paths directly affects the mechanical responses of the structures under different loading conditions. GIPPO accommodates planar and non-planar printing geometries and allows for the fabrication of objects with varying infill patterns per layer. This platform addresses limitations in commercial slicing software and enables high-fidelity fabrication of intricate architected materials. 

<br /><br />Summary: <div>
arXiv:2508.21694v1 Announce Type: new 
Abstract: Architected materials of significant geometric complexity offer exceptional mechanical properties that often surpass those of their constituent materials. However, their fabrication through extrusion-based 3D printing remains hindered by suboptimal printing trajectories, which is inherent to commercial slicing software. They produce multiple non-continuous paths that compromise fabrication time, shape fidelity, and structural integrity, particularly for thin-walled lattice structures. To address this issue, we introduce GIPPO (Graph-based, Iterative, Printing-Path Optimization), an open-source slicing platform that transforms complex lattice designs into optimized printing trajectories. Lattices are converted to graph networks to derive the optimal printing trajectories through a modified version of Prim's algorithm. The resulting paths are translated back to Euclidean coordinates and exported as a ready-to-use G-code. We validated GIPPO's performance against conventional slicing software across six architected lattice geometries fabricated from thermoplastic polyurethane using fused deposition modeling. GIPPO-optimized constructs demonstrated superior shape fidelity with reduced local thickness deviations, no missing struts, and minimized excess material deposition compared to conventionally printed controls. Mechanical testing revealed that printing path optimization directly influences both uniaxial and out-of-plane mechanical responses, with different optimization strategies yielding distinct performance characteristics suited to specific loading conditions. Moreover, the platform accommodates both planar and non-planar printing geometries and enables fabrication of objects with varying infill patterns per layer. Our work addresses critical limitations in commercial slicing software and opens new opportunities for high-fidelity fabrication of complex architected materials.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Financial Brain Scan of the LLM</title>
<link>https://arxiv.org/abs/2508.21285</link>
<guid>https://arxiv.org/abs/2508.21285</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, economic forecasts, sentiment, risk-averse, biases

Summary:
Using advanced computer science techniques, researchers can analyze large language models (LLMs) to understand the concepts guiding their economic forecasts. This method allows for the identification of key factors such as sentiment, technical analysis, and timing without compromising performance. The approach also enables researchers to manipulate the models to be more or less risk-averse, optimistic, or pessimistic, providing opportunities to correct or simulate biases in the forecasts. Importantly, this method is transparent, easy to implement, and replicable, making it a valuable tool for empirical research in the social sciences.<br /><br />Summary: <div>
arXiv:2508.21285v1 Announce Type: cross 
Abstract: Emerging techniques in computer science make it possible to "brain scan" large language models (LLMs), identify the plain-English concepts that guide their reasoning, and steer them while holding other factors constant. We show that this approach can map LLM-generated economic forecasts to concepts such as sentiment, technical analysis, and timing, and compute their relative importance without reducing performance. We also show that models can be steered to be more or less risk-averse, optimistic, or pessimistic, which allows researchers to correct or simulate biases. The method is transparent, lightweight, and replicable for empirical research in the social sciences.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality</title>
<link>https://arxiv.org/abs/2508.21736</link>
<guid>https://arxiv.org/abs/2508.21736</guid>
<content:encoded><![CDATA[
<div> Keywords: Microbiomes, mathematical modeling, spatiotemporal data, virtual reality, data analysis

Summary: <br /><br />Microbiomes play a crucial role in the human body, but studying them experimentally is challenging, leading to more research in mathematical modeling. Current tools for simulating microbial communities lack interactive functionalities and are complex to use. To address these limitations, a user-friendly tool called MicroLabVR has been developed. It transfers spatial data into virtual reality (VR) and allows users to explore spatiotemporal simulation data interactively. Users can import datasets containing population growth, substance concentration development, and metabolic flux distribution data. This tool aims to improve data analysis by enabling the exploration of microbiome data in their spatial context. <div>
arXiv:2508.21736v1 Announce Type: cross 
Abstract: Microbiomes are a vital part of the human body, engaging in tasks like food digestion and immune defense. Their structure and function must be understood in order to promote host health and facilitate swift recovery during disease. Due to the difficulties in experimentally studying these systems in situ, more research is being conducted in the field of mathematical modeling. Visualizing spatiotemporal data is challenging, and current tools that simulate microbial communities' spatial and temporal development often only provide limited functionalities, often requiring expert knowledge to generate useful results. To overcome these limitations, we provide a user-friendly tool to interactively explore spatiotemporal simulation data, called MicroLabVR, which transfers spatial data into virtual reality (VR) while following guidelines to enhance user experience (UX). With MicroLabVR, users can import CSV datasets containing population growth, substance concentration development, and metabolic flux distribution data. The implemented visualization methods allow users to evaluate the dataset in a VR environment interactively. MicroLabVR aims to improve data analysis for the user by allowing the exploration of microbiome data in their spatial context.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Hypergraph Diffusion for Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2501.18850</link>
<guid>https://arxiv.org/abs/2501.18850</guid>
<content:encoded><![CDATA[
<div> Keywords: Crystal Structure Prediction, Generative Models, Hypergraphs, EH-Diff, Symmetry-preserving Properties

Summary: 
Crystal Structure Prediction (CSP) is a challenging task crucial for developing new materials. Traditional graph-based models struggle to capture complex high-order interactions in crystal structures. This study introduces a novel approach using hypergraphs to represent crystal structures, allowing for the modeling of multi-way atomic interactions. The Equivariant Hypergraph Diffusion Model (EH-Diff) is proposed as a generative model that leverages the symmetry-preserving properties of hypergraphs to accurately predict crystal structures. Experimental results on benchmark datasets show that EH-Diff outperforms existing CSP methods with just one sample. This approach offers an efficient and accurate method for crystal structure prediction, emphasizing the importance of symmetry and high-order relationships in accurately characterizing crystal structures.<br /><br />Summary: <div>
arXiv:2501.18850v2 Announce Type: replace 
Abstract: Crystal Structure Prediction (CSP) remains a fundamental challenge with significant implications for the development of new materials and the advancement of various scientific disciplines. Recent developments have shown that generative models, particularly diffusion models, hold great promise for CSP. However, traditional graph-based representations, where atomic bonds are modeled as pairwise graph edges, fail to fully capture the intricate high-order interactions essential for accurately representing crystal structures. In this work, we propose a novel approach that utilizes hypergraphs to represent crystal structures, providing a more expressive abstraction for modeling multi-way atomic interactions. By adopting hypergraphs, we can effectively capture complex high-order relationships and symmetries, such as permutation and periodic translation invariance, which are crucial for characterizing crystal structures. In this work, we propose the \textbf{E}quivariant \textbf{H}ypergraph \textbf{Diff}usion Model (\textbf{EH-Diff}), a generative model designed to take advantage of the symmetry-preserving properties of hypergraphs. EH-Diff exploits these features to offer an efficient and accurate method for predicting crystal structures with a strong theoretical justification to preserve invariance properties. Empirically, we conduct extensive experiments on four benchmark datasets, and the results demonstrate that EH-Diff outperforms state-of-the-art CSP methods with only one sample.
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mass conservation analysis of extrusion-based 3D printing simulations based on the level-set method</title>
<link>https://arxiv.org/abs/2508.20617</link>
<guid>https://arxiv.org/abs/2508.20617</guid>
<content:encoded><![CDATA[
<div> conservative level-set method, mass conservation, extrusion-based printing, numerical simulations, cross-sectional area<br />
Summary:<br />
The article investigates the mass conservation properties of the conservative level-set method in extrusion-based 3D printing. It focuses on tracking evolving material boundaries accurately to avoid mismatches between the extruded and simulated shapes. The study analyzes the impact of level set parameters on mass conservation accuracy, specifically looking at the cross-sectional area of deposited strands. Results show that reducing reinitialization and interface thickness parameters decreases errors in cross-sectional area calculations but may increase computational costs. Selecting an appropriate interface thickness can also reduce strong mesh requirements. Comparing simulated cross-sectional areas with ideal areas from a mass balance at steady state indicates good agreement, validating the method's accuracy. The research contributes valuable insights into improving mass conservation in extrusion-based 3D printing simulations. <br /><br />Summary: <div>
arXiv:2508.20617v1 Announce Type: new 
Abstract: Numerical simulations of extrusion-based printing require tracking evolving material bound- aries, a challenging task due to possible topological changes and mass conservation issues. Inaccurate conservation of mass can lead to a mismatch between the extruded and simulated shapes, and generally to unreliable predictions of the actual ink behavior. This work investigates the mass conservation properties of the conservative level-set method in extrusion-based 3D printing applications. We analyze the effects of the level set parameters on the accuracy of mass conservation using the cross-sectional area of the deposited strand. We compare the cross- sectional areas obtained in the simulation with the ideal areas obtained from a mass balance when the system reaches a steady-state condition. The numerical results indicate that reducing the reinitialization and the interface thickness parameters decreases the errors in the cross-sectional area obtained. However, the reductions in error tend to decline and could lead to excessive computational cost. Furthermore, we also found that the typical strong mesh requirements can be lessened by selecting an adequate interface thickness. Finally, we obtained the cross-sectional areas from simulations with different printing settings and found that they show good agreement with the simulated and experimental data published in previous work.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can News Predict the Direction of Oil Price Volatility? A Language Model Approach with SHAP Explanations</title>
<link>https://arxiv.org/abs/2508.20707</link>
<guid>https://arxiv.org/abs/2508.20707</guid>
<content:encoded><![CDATA[
<div> Keywords: financial markets, crude oil, news analysis, predictive modeling, sentiment analysis 

Summary:<br /><br />Financial markets are influenced by news, sentiment, and economic indicators, impacting asset price fluctuations. This study focuses on crude oil price volatility prediction using news data exclusively, comparing it to traditional market data methods. Utilizing a decade-long Eikon dataset, an ensemble learning framework incorporating sentiment analysis techniques and language models is developed. The model's performance is compared to the HAR model through the McNemar test, with raw news count being a significant predictor. FastText emerges as the most effective embedding technique for forecasting price movements. SHAP-based interpretation at the word level reveals evolving predictive drivers during different market regimes. Pre-pandemic factors included supply-demand and economic terms, early pandemic emphasized uncertainty and macroeconomic instability, post-shock focused on long-term recovery indicators, and war-period considered geopolitical and regional oil market disruptions. These findings highlight the potential of news-driven features and explainable NLP in financial forecasting. <div>
arXiv:2508.20707v1 Announce Type: new 
Abstract: Financial markets can be highly sensitive to news, investor sentiment, and economic indicators, leading to important asset price fluctuations. In this study we focus on crude oil, due to its crucial role in commodity markets and the global economy. Specifically, we are interested in understanding the directional changes of oil price volatility, and for this purpose we investigate whether news alone -- without incorporating traditional market data -- can effectively predict the direction of oil price movements. Using a decade-long dataset from Eikon (2014-2024), we develop an ensemble learning framework to extract predictive signals from financial news. Our approach leverages diverse sentiment analysis techniques and modern language models, including FastText, FinBERT, Gemini, and LLaMA, to capture market sentiment and textual patterns. We benchmark our model against the Heterogeneous Autoregressive (HAR) model and assess statistical significance using the McNemar test. While most sentiment-based indicators do not consistently outperform HAR, the raw news count emerges as a robust predictor. Among embedding techniques, FastText proves most effective for forecasting directional movements. Furthermore, SHAP-based interpretation at the word level reveals evolving predictive drivers across market regimes: pre-pandemic emphasis on supply-demand and economic terms; early pandemic focus on uncertainty and macroeconomic instability; post-shock attention to long-term recovery indicators; and war-period sensitivity to geopolitical and regional oil market disruptions. These findings highlight the predictive power of news-driven features and the value of explainable NLP in financial forecasting.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-consistent clustering analysis for homogenisation of heterogeneous plates</title>
<link>https://arxiv.org/abs/2508.20446</link>
<guid>https://arxiv.org/abs/2508.20446</guid>
<content:encoded><![CDATA[
<div> plate structures, reduced-order model, periodic micro-structures, self-consistent clustering analysis, Lippmann-Schwinger equation

Summary: 
This study presents a novel reduced-order model for plate structures with periodic micro-structures. By combining self-consistent clustering analysis (SCA) with the Lippmann-Schwinger equation, the model allows for fast multiscale homogenization of heterogeneous plates. A plate-specific SCA scheme is developed, incorporating an offline-online strategy utilizing Green's functions and k-means data compression, as well as an online self-consistent update leveraging the weak sensitivity of the reference medium. The framework is applicable to both linear and nonlinear problems in classical plate theory and first-order shear deformation theory, demonstrating its accuracy on various plate configurations. The proposed model matches the precision of FFT-based direct numerical simulation while significantly reducing computational cost. Examples include linear isotropic perforated plates, woven composites, and nonlinear elasto-plastic perforated plates with damage. This innovative approach enables efficient analysis of complex plate structures with periodic micro-structures. <div>
arXiv:2508.20446v1 Announce Type: cross 
Abstract: This work introduces a reduced-order model for plate structures with periodic micro-structures by coupling self-consistent clustering analysis (SCA) with the Lippmann-Schwinger equation, enabling rapid multiscale homogenisation of heterogeneous plates. A plate-specific SCA scheme is derived for the first time and features two key elements: (i) an offline-online strategy that combines Green's functions with k-means data compression, and (ii) an online self-consistent update that exploits the weak sensitivity of the reference medium. The framework handles both linear and nonlinear problems in classical plate theory and first-order shear deformation theory, and its performance is verified on linear isotropic perforated plates and woven composites, as well as on non-linear elasto-plastic perforated plates and woven composites with damage. Across all cases the proposed model matches the accuracy of FFT-based direct numerical simulation while reducing computational cost by over an order of magnitude.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Epistemic Support-Point Filter (ESPF): A Bounded Possibilistic Framework for Ordinal State Estimation</title>
<link>https://arxiv.org/abs/2508.20806</link>
<guid>https://arxiv.org/abs/2508.20806</guid>
<content:encoded><![CDATA[
<div> epistemic uncertainty, Epistemic Support-Point Filter, possibility theory, ordinal logic, Choquet integral <br /> 
Summary: The Epistemic Support-Point Filter (ESPF) introduces a non-Bayesian filtering framework that addresses the limitations of traditional state estimation methods. ESPF is grounded in possibility theory and emphasizes epistemic humility by redefining belief evolution using compatibility-weighted support updates and surprisal-aware pruning. It adapts belief support through adaptive dispersion via sparse grid quadrature and employs the Choquet integral for multi-model inference. ESPF does not seek a posterior distribution but maintains a structured region of plausibility, updating using ordinal logic. This approach allows for dynamic contraction or expansion of belief support based on information structure without requiring prior statistical calibration. The framework supports robust estimation in sparse or adversarial sensing environments where priors are unavailable, misleading, or epistemically unjustified. <br /> 
Summary: <div>
arXiv:2508.20806v1 Announce Type: cross 
Abstract: Traditional state estimation methods rely on probabilistic assumptions that often collapse epistemic uncertainty into scalar beliefs, risking overconfidence in sparse or adversarial sensing environments. We introduce the Epistemic Support-Point Filter (ESPF), a novel non-Bayesian filtering framework fully grounded in possibility theory and epistemic humility. ESPF redefines the evolution of belief over state space using compatibility-weighted support updates, surprisalaware pruning, and adaptive dispersion via sparse grid quadrature. Unlike conventional filters, ESPF does not seek a posterior distribution, but rather maintains a structured region of plausibility or non-rejection, updated using ordinal logic rather than integration. For multi-model inference, we employ the Choquet integral to fuse competing hypotheses based on a dynamic epistemic capacity function, generalizing classical winner-take-all strategies. The result is an inference engine capable of dynamically contracting or expanding belief support in direct response to information structure, without requiring prior statistical calibration. This work presents a foundational shift in how inference, evidence, and ignorance are reconciled, supporting robust estimation where priors are unavailable, misleading, or epistemically unjustified.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scale-invariant Monte Carlo and multilevel Monte Carlo estimation of mean and variance: An application to simulation of linear elastic bone tissue</title>
<link>https://arxiv.org/abs/2106.13723</link>
<guid>https://arxiv.org/abs/2106.13723</guid>
<content:encoded><![CDATA[
<div> scale-invariant, error estimators, Monte Carlo, multilevel Monte Carlo, mechanical simulation

Summary:
The article introduces novel scale-invariant error estimators for Monte Carlo and multilevel Monte Carlo methods used in estimating mean and variance. These estimators optimize computation costs across different grid levels for linear transformations of the quantity of interest, remaining robust to distribution variations. The proposed algorithms are demonstrated in a mechanical simulation of linear elastic bone tissue, incorporating material uncertainty with heterogeneity and random anisotropy in the constitutive law. The new error estimators are fully dimensionless and offer improved efficiency in estimating mean and variance, making them suitable for a wide range of applications where accurate estimation of uncertainty is essential.<br /><br />Summary: <div>
arXiv:2106.13723v3 Announce Type: replace-cross 
Abstract: We propose novel scale-invariant error estimators for the Monte Carlo and multilevel Monte Carlo estimation of mean and variance. For any linear transformation of the distribution of the quantity of interest, the computation cost across grid levels is optimized using a normalized error estimate, which is not only fully dimensionless but also remains robust to variation in characteristics of the distribution. We demonstrate the effectiveness of the algorithms through application to a mechanical simulation of linear elastic bone tissue, where material uncertainty incorporating both heterogeneity and random anisotropy is considered in the constitutive law.
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infrastructure-enabled risk assessment of hazardous road conditions on rural roads during inclement weather</title>
<link>https://arxiv.org/abs/2508.19444</link>
<guid>https://arxiv.org/abs/2508.19444</guid>
<content:encoded><![CDATA[
<div> Keywords: Rural roadways, Commercial Motor Vehicle drivers, hazardous conditions, roadway hazard risk assessment, safe advisory speeds

Summary: 
The study addresses the lack of real-time reporting of hazardous conditions on rural roadways and limited infrastructure, increasing the risk of crashes for Commercial Motor Vehicle (CMV) drivers. The framework presented quantifies the probability and severity of crash occurrences due to specific roadway hazards, providing a comprehensive approach to assess combined driving risks. A synthetic dataset was used for a case study, confirming the coherence of the risk profile generated by the combined ProbabilitySeverity scoring. The results validate the practicality of the risk assessment approach and suggest implementing graduated safety measures in real-world roadway operations.<br /><br />Summary: <div>
arXiv:2508.19444v1 Announce Type: new 
Abstract: Rural roadways often expose Commercial Motor Vehicle (CMV) drivers to hazardous conditions, such as heavy fog, rain, snow, black ice, and flash floods, many of which remain unreported in real time. This lack of timely information, coupled with limited infrastructure in rural areas, significantly increases the risk of crashes. Although various sensing technologies exist to monitor individual hazards like low visibility or surface friction, they rarely assess the combined driving risk posed by multiple simultaneous hazards, nor do they provide actionable recommendations such as safe advisory speeds. To address this critical gap, in this study, we present a roadway hazard risk assessment framework that provides an approach to quantify the probability and severity of crash occurrences due to specific roadway hazards. To evaluate this framework, we presented a case study by constructing a synthetic "year-long" dataset that encompasses every possible pairing of road surface and visibility conditions. Our analysis confirms that the combined ProbabilitySeverity scoring yields a coherent, stepwise risk profile across all hazard scenarios. These results validate the practicality of our risk assessment approach and provide a foundation for deploying graduated safety measures in real-world roadway operations.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An assessment of estimation models and investment gaps for the deployment of high-speed broadband networks in NUTS3 regions to meet the objectives of the European Gigabit Society</title>
<link>https://arxiv.org/abs/2508.19921</link>
<guid>https://arxiv.org/abs/2508.19921</guid>
<content:encoded><![CDATA[
<div> European Union, high speed broadband networks, investment, European Gigabit Society, estimation model <br />
Summary: 
This paper examines the deployment of high speed broadband networks in the European Union, specifically focusing on the investment required to achieve the targets set by the European Commission for 2025 as part of the European Gigabit Society. The analysis includes assessing the availability and adoption of high capacity fixed and wireless networks in urban and rural areas. The estimation model used in the study incorporates data at the local level to determine the investment gap for each EGS objective. Three scenarios based on technology mixes are considered. The paper compares its methodology with existing literature and provides a dynamic view of the investment gap evolution from 2017 to 2019. The analysis proves the usefulness of the estimation models in evaluating the investment needed for high speed broadband infrastructure in the EU. <br /><br /> <div>
arXiv:2508.19921v1 Announce Type: new 
Abstract: This paper analyses the deployment of high speed broadband networks in the European Union (EU). Its aim is to assess the investment required to meet the targets set by the European Commission (EC) for 2025, within the framework of the European Gigabit Society (EGS). This plan aims to ensure the availability and take up of very high capacity fixed and wireless networks, in both urban and rural areas, among households and the main socioeconomic drivers. The estimation model presented here uses a methodology supported by data at the local (NUTS3) level to give a bottom up estimation of the investment gap for each of the EGS objectives, using three different scenarios depending on the mix of wired and wireless technologies offered. The methodology and estimation model used in the paper are examined against other examples and assumptions available in the literature. We also offer a dynamic perspective on the analysis of the evolution of this investment gap over the years 2017 2019, which includes an assessment of the usefulness of these estimation models.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-field decomposed hyper-reduced order modeling of damage-plasticity simulations</title>
<link>https://arxiv.org/abs/2508.19957</link>
<guid>https://arxiv.org/abs/2508.19957</guid>
<content:encoded><![CDATA[
<div> DEIM, ECSW, multi-field decomposed approach, hyper-reduced order modeling, gradient-extended damage-plasticity simulations <br />
Summary: <br />
This paper introduces a new approach for hyper-reduced order modeling to address the limitations of traditional model reduction techniques in gradient-extended damage-plasticity simulations. The method involves extending the discrete empirical interpolation method (DEIM) and the energy-conserving sampling and weighting method (ECSW) to accommodate the multi-field nature of the problem. By applying these methods, stable reduced order simulations are achieved while significantly reducing computational costs compared to full-order simulations. Through two numerical examples, the proposed approaches' performance and limitations are demonstrated. The decomposed ECSW method proves to have higher accuracy and lower computational cost than the decomposed DEIM method, showcasing its superiority in hyper-reduced order modeling for complex simulations. <div>
arXiv:2508.19957v1 Announce Type: new 
Abstract: This paper presents a multi-field decomposed approach for hyper-reduced order modeling to overcome the limitations of traditional model reduction techniques for gradient-extended damage-plasticity simulations. The discrete empirical interpolation method (DEIM) and the energy-conserving sampling and weighting method (ECSW) are extended to account for the multi-field nature of the problem. Both methods yield stable reduced order simulations, while significantly reducing the computational cost compared to full-order simulations. Two numerical examples are presented to demonstrate the performance and limitations of the proposed approaches. The decomposed ECSW method has overall higher accuracy and lower computational cost than the decomposed DEIM method.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From stand-up to start-up: exploring entrepreneurship competences and STEM womens intention</title>
<link>https://arxiv.org/abs/2508.20091</link>
<guid>https://arxiv.org/abs/2508.20091</guid>
<content:encoded><![CDATA[
<div> STEM, entrepreneurship competencies, intention, gender differences, European Commission

Summary: 
The study examines the relationship between entrepreneurship competencies and intention among potential STEM entrepreneurs. Contrary to the assumption, there is no significant difference in entrepreneurship intention between men and women. Gender does not act as a moderating factor in the relationship between competencies and intention. The analysis, based on the Entrepreneurship Competences Framework by the European Commission, highlights a positive correlation between competencies and entrepreneurship intention. Self-perceived competences show minor variations based on gender. These findings debunk the belief that women have lower rates of entrepreneurship intention due to perceived lack of competence. The study's results provide valuable insights for entrepreneurship education and business creation initiatives. <div>
arXiv:2508.20091v1 Announce Type: new 
Abstract: This study seeks to explore the relationship between entrepreneurship competencies and intention (EI) of a sample of potential STEM entrepreneurs in order to assess the conventional assumption on women exhibiting lower rates of entrepreneurship intention than men and that the lack of competence perceived is a higher barrier to be an entrepreneur for them. The model used for the analysis takes as reference the Entrepreneurship Competences Framework (EntreComp) proposed by the European Commission (EC) as a common guide to inspire entrepreneurship education. Data gathering is based on a structured questionnaire. The conducted analysis uses Students t test means comparison and factor analysis to define the model of competences, and a multiple regression model to study the relationship between competences and skill factors in EI. Findings do not validate the hypothesis that women have fewer entrepreneurship intentions than men. Also, slight differences on the self-perceived competences are obtained by gender. In addition, the study confirms the hypothesis of a positive relationship between competences and EI, but here gender is not a moderating factor. Results are expected to contribute to the entrepreneurship competences debate and provide useful insights of application in entrepreneurship education with orientation towards the business creation.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Epistemic Trade-Off: An Analysis of the Operational Breakdown and Ontological Limits of "Certainty-Scope" in AI</title>
<link>https://arxiv.org/abs/2508.19304</link>
<guid>https://arxiv.org/abs/2508.19304</guid>
<content:encoded><![CDATA[
<div> Keywords: Floridi's conjecture, artificial intelligence, certainty, scope, operationalization

Summary:<br /><br />
Floridi's conjecture on the trade-off between certainty and scope in artificial intelligence systems is discussed in this paper. The conjecture, while conceptually sound, faces challenges in practical implementation due to its reliance on incomputable constructs and its assumption of AI systems as self-contained entities. This hinders its ability to inform the design, deployment, and governance of real-world AI systems in complex human-centric domains. The paper argues that these limitations prevent the conjecture from being actionable and verifiable in real-world scenarios. The authors propose a re-framing of Floridi's epistemic challenge to address the epistemic burdens of AI within dynamic socio-technical environments, aiming to bridge the gap between theoretical insights and practical applications in AI engineering and regulation. <div>
arXiv:2508.19304v1 Announce Type: cross 
Abstract: Floridi's conjecture offers a compelling intuition about the fundamental trade-off between certainty and scope in artificial intelligence (AI) systems. This exploration remains crucial, not merely as a philosophical exercise, but as a potential compass for guiding AI investments, particularly in safety-critical industrial domains where the level of attention will surely be higher in the future. However, while intellectually coherent, its formalization ultimately freezes this insight into a suspended epistemic truth, resisting operationalization within real-world systems. This paper is a result of an analysis arguing that the conjecture's ambition to provide insights to engineering design and regulatory decision-making is constrained by two critical factors: first, its reliance on incomputable constructs - rendering it practically unactionable and unverifiable; second, its underlying ontological assumption of AI systems as self-contained epistemic entities - separating it from the intricate and dynamic socio-technical environments in which knowledge is co-constructed. We conclude that this dual breakdown - an epistemic closure deficit and an embeddedness bypass - prevents the conjecture from transitioning into a computable and actionable framework suitable for informing the design, deployment, and governance of real-world AI hybrid systems. In response, we propose a contribution to the framing of Floridi's epistemic challenge, addressing the inherent epistemic burdens of AI within complex human-centric domains.
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network-Based Topology Optimization for Self-Supporting Structures in Additive Manufacturing</title>
<link>https://arxiv.org/abs/2508.19169</link>
<guid>https://arxiv.org/abs/2508.19169</guid>
<content:encoded><![CDATA[
<div> machine learning, topology optimization, self-supporting structures, additive manufacturing, stress constraints
<br />
The paper introduces a novel machine learning-based framework for optimizing the topology of self-supporting structures for additive manufacturing. The framework utilizes a graph neural network (GNN) to predict material distributions over a finite element mesh, ensuring printability through an integrated AM filter. By minimizing structural compliance under volume and stress constraints, the framework generates stress-constrained manufacturable designs in various loading conditions. The stress constraint is enforced using a differentiable p-norm aggregation of von Mises stress to enhance mechanical reliability. The approach features a fully differentiable architecture, eliminating the need for explicit sensitivity derivation in the optimization loop. Numerical experiments demonstrate the efficacy of the framework in producing high-performance designs suitable for additive manufacturing with reduced post-processing requirements.
<br /><br />Summary: <div>
arXiv:2508.19169v1 Announce Type: new 
Abstract: This paper presents a machine learning-based framework for topology optimization of self-supporting structures, specifically tailored for additive manufacturing (AM). By employing a graph neural network (GNN) that acts as a neural field over the finite element mesh, the framework effectively learns and predicts continuous material distributions. An integrated AM filter ensures printability by eliminating unsupported overhangs, while the optimization process minimizes structural compliance under volume and stress constraints. The stress constraint is enforced using a differentiable p-norm aggregation of von Mises stress, promoting mechanical reliability in the optimized designs. A key advantage of the approach lies in its fully differentiable architecture, which leverages automatic differentiation throughout the optimization loop--eliminating the need for explicit sensitivity derivation for both the filter and the stress constraint. Numerical experiments demonstrate the ability of the framework to generate stress-constrained manufacturable topologies under various loading and boundary conditions, offering a practical pathway toward AM-ready high-performance designs with reduced post-processing requirements.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance</title>
<link>https://arxiv.org/abs/2508.19138</link>
<guid>https://arxiv.org/abs/2508.19138</guid>
<content:encoded><![CDATA[
<div> nanoscale electronic devices, nanoribbon field-effect transistors, NEGF, DFT, electron-electron interactions<br />
<br />
Summary:<br />
Designing nanoscale electronic devices like nanoribbon field-effect transistors (NRFETs) requires advanced quantum mechanical modeling tools. Current approaches combine NEGF and DFT, but with ultra-small device dimensions, electron-electron interactions become crucial. The NEGF+GW scheme presented here extends existing solvers to handle NRFET geometries with dimensions comparable to experiments. The QuaTrEx package utilizes a novel domain decomposition scheme, can handle devices with up to 84,480 atoms, and scales efficiently on supercomputers like Alps and Frontier. It achieves exascale FP64 performance on 42,240 atoms, reaching 1.15 Eflop/s. <div>
arXiv:2508.19138v1 Announce Type: cross 
Abstract: Designing nanoscale electronic devices such as the currently manufactured nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools capturing all relevant quantum mechanical effects. State-of-the-art approaches combine the non-equilibrium Green's function (NEGF) formalism and density functional theory (DFT). However, as device dimensions do not exceed a few nanometers anymore, electrons are confined in ultra-small volumes, giving rise to strong electron-electron interactions. To account for these critical effects, DFT+NEGF solvers should be extended with the GW approximation, which massively increases their computational intensity. Here, we present the first implementation of the NEGF+GW scheme capable of handling NRFET geometries with dimensions comparable to experiments. This package, called QuaTrEx, makes use of a novel spatial domain decomposition scheme, can treat devices made of up to 84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80% weak scaling efficiency), and sustains an exascale FP64 performance on 42,240 atoms (1.15 Eflop/s).
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOFLUX: A Differentiable Topology Optimization Framework for Multiphysics Fluidic Problems</title>
<link>https://arxiv.org/abs/2508.17564</link>
<guid>https://arxiv.org/abs/2508.17564</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, fluidic devices, automatic differentiation, JAX library, TOFLUX

Summary:
TOFLUX is a new framework for fluid devices that utilizes automatic differentiation for efficient design optimization. The complexity of fluid-based systems, with multiphysics nonlinear interactions, often hinders researchers, but TOFLUX aims to simplify the process. By using the JAX library, the framework enables rapid exploration of various objectives and constraints, even in challenging scenarios like thermo-fluidic coupling and fluid-structure interaction. The integration with neural networks and machine learning enhances scientific computing capabilities. TOFLUX provides a foundational resource to accelerate research and innovation in fluid-based Topology Optimization. The accompanying software can be accessed on GitHub at github.com/UW-ERSL/TOFLUX. <br /><br />Summary: <div>
arXiv:2508.17564v1 Announce Type: new 
Abstract: Topology Optimization (TO) holds the promise of designing next-generation compact and efficient fluidic devices. However, the inherent complexity of fluid-based TO systems, characterized by multiphysics nonlinear interactions, poses substantial barriers to entry for researchers.
  Beyond the inherent intricacies of forward simulation models, design optimization is further complicated by the difficulty of computing sensitivities, i.e., gradients. Manual derivation and implementation of sensitivities are often laborious and prone to errors, particularly for non-trivial objectives, constraints, and material models. An alternative solution is automatic differentiation (AD). Although AD has been previously demonstrated for simpler TO problems, extending its use to complex nonlinear multiphysics systems, specifically in fluidic optimization, is key to reducing the entry barrier.
  To this end, we introduce TOFLUX, a TO framework for fluid devices leveraging the JAX library for high-performance automatic differentiation. The flexibility afforded by AD enables the rapid exploration and evaluation of various objectives and constraints. We illustrate this capability through challenging examples encompassing thermo-fluidic coupling, fluid-structure interaction, and non-Newtonian flows. Additionally, we demonstrate the seamless integration of our framework with neural networks and machine learning methodologies, enabling modern approaches to scientific computing. Ultimately, the framework aims to provide a foundational resource to accelerate research and innovation in fluid-based TO. The software accompanying this educational paper can be accessed at github.com/UW-ERSL/TOFLUX.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing the exploration-exploitation trade-off in active learning for surrogate model-based reliability analysis via multi-objective optimization</title>
<link>https://arxiv.org/abs/2508.18170</link>
<guid>https://arxiv.org/abs/2508.18170</guid>
<content:encoded><![CDATA[
<div> active learning, reliability assessment, surrogate model, multi-objective optimization, sample acquisition

Summary: 
The article introduces a new approach for reliability assessment of engineering systems by using active learning to iteratively refine a surrogate model. This approach aims to reduce the number of expensive simulations by balancing exploration and exploitation through a multi-objective optimization (MOO) formulation. Traditional strategies like U and Expected Feasibility Function (EFF) are compared with the MOO approach, which explicitly considers the trade-off between exploration and exploitation. The MOO framework provides a unifying perspective and allows for the selection of samples based on a quantifiable exploration-exploitation trade-off. Different sample selection strategies, such as knee point and compromise solution, are evaluated across benchmark limit-state functions. Results show that the MOO approach is generally effective, with an adaptive strategy maintaining high reliability estimates and low relative errors. <div>
arXiv:2508.18170v1 Announce Type: new 
Abstract: Reliability assessment of engineering systems is often hindered by the need to evaluate limit-state functions through computationally expensive simulations, rendering standard sampling impractical. An effective solution is to approximate the limit-state function with a surrogate model iteratively refined through active learning, thereby reducing the number of expensive simulations. At each iteration, an acquisition strategy selects the next sample by balancing two competing goals: exploration, to reduce global predictive uncertainty, and exploitation, to improve accuracy near the failure boundary. Classical strategies, such as the U-function and the Expected Feasibility Function (EFF), implicitly condense exploration and exploitation into a scalar score derived from the surrogate predictive mean and variance, concealing the trade-off and biasing sampling. We introduce a multi-objective optimization (MOO) formulation for sample acquisition in reliability analysis, where exploration and exploitation are explicit, competing objectives. Within our framework, U and EFF correspond to specific Pareto-optimal solutions, providing a unifying perspective that connects classical and Pareto-based approaches. Solving the MOO problem discards dominated candidates, yielding a compact Pareto set, with samples representing a quantifiable exploration-exploitation trade-off. To select samples from the Pareto set, we adopt the knee point and the compromise solution, and further propose a strategy that adjusts the trade-off according to reliability estimates. Across benchmark limit-state functions, we assess the sample efficiency and active learning performance of all strategies. Results show that U and EFF exhibit case-dependent performance, knee and compromise are generally effective, and the adaptive strategy is robust, consistently reaching strict targets and maintaining relative errors below 0.1%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience</title>
<link>https://arxiv.org/abs/2508.16836</link>
<guid>https://arxiv.org/abs/2508.16836</guid>
<content:encoded><![CDATA[
<div> Neural symbolic approach, complex networks, resilience prediction, physical information method, industrial chain<br />
<br />
Summary: 
This paper introduces a novel physically informative neural symbolic approach for predicting the resilience of complex networks, focusing on industrial chains. The approach integrates physical entity dynamics with spatiotemporal network evolution to enhance predictive accuracy. By jointly learning physical symbol dynamics and network topology, the model demonstrates superior prediction capabilities for industrial chain resilience. The experimental results showcase the effectiveness of the proposed approach in accurately and effectively predicting the elasticity of industrial chains. This advancement is crucial for sustainable development and has significant implications for the industry. <div>
arXiv:2508.16836v1 Announce Type: cross 
Abstract: Industrial chain plays an increasingly important role in the sustainable development of national economy. However, as a typical complex network, data-driven deep learning is still in its infancy in describing and analyzing the resilience of complex networks, and its core is the lack of a theoretical framework to describe the system dynamics. In this paper, we propose a physically informative neural symbolic approach to describe the evolutionary dynamics of complex networks for resilient prediction. The core idea is to learn the dynamics of the activity state of physical entities and integrate it into the multi-layer spatiotemporal co-evolution network, and use the physical information method to realize the joint learning of physical symbol dynamics and spatiotemporal co-evolution topology, so as to predict the industrial chain resilience. The experimental results show that the model can obtain better results and predict the elasticity of the industry chain more accurately and effectively, which has certain practical significance for the development of the industry.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decoupled LOB Representation Framework for Multilevel Manipulation Detection with Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.17086</link>
<guid>https://arxiv.org/abs/2508.17086</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial markets, Trade-based manipulation, Spoofing, Limit Order Book, Anomaly detection 

Summary: 
Financial markets are essential for global economic stability but are often undermined by trade-based manipulation (TBM), including deceptive strategies like spoofing. Detecting these anomalies in the rich information of the Limit Order Book (LOB) is challenging due to its high dimensionality and noise. To address this, a representation learning framework combining a cascaded LOB representation pipeline with supervised contrastive learning is proposed. Extensive experiments show improved detection performance across various models, with Transformer-based architectures achieving state-of-the-art results. Systematic analyses and ablation studies are conducted to investigate multilevel anomalies and the contributions of key components, providing insights into representation learning and anomaly detection for complex sequential data. The code for the framework will be released later at the provided URL. 

<br /><br />Summary: <div>
arXiv:2508.17086v1 Announce Type: cross 
Abstract: Financial markets are critical to global economic stability, yet trade-based manipulation (TBM) often undermines their fairness. Spoofing, a particularly deceptive TBM strategy, exhibits multilevel anomaly patterns that have not been adequately modeled. These patterns are usually concealed within the rich, hierarchical information of the Limit Order Book (LOB), which is challenging to leverage due to high dimensionality and noise. To address this, we propose a representation learning framework combining a cascaded LOB representation pipeline with supervised contrastive learning. Extensive experiments demonstrate that our framework consistently improves detection performance across diverse models, with Transformer-based architectures achieving state-of-the-art results. In addition, we conduct systematic analyses and ablation studies to investigate multilevel anomalies and the contributions of key components, offering broader insights into representation learning and anomaly detection for complex sequential data. Our code will be released later at this URL.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Easy Acceleration with Distributed Arrays</title>
<link>https://arxiv.org/abs/2508.17493</link>
<guid>https://arxiv.org/abs/2508.17493</guid>
<content:encoded><![CDATA[
<div> Keywords: high level programming languages, GPU accelerators, distributed arrays, memory bandwidth, scalability<br />
<br />
Summary: <br />
High level programming languages and GPU accelerators play a crucial role in enabling a wide range of applications. To achieve scalable vertical, horizontal, and temporal performance, effective abstractions are needed. Distributed arrays serve as one such abstraction, enabling high level programming to achieve highly scalable performance by deriving parallelism from data locality. Using the STREAM memory bandwidth benchmark on various hardware, this paper demonstrates scalable performance within and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across multiple nodes showed linear performance. The study also compared hardware improvements for memory bandwidth over decades, showing significant increases in CPU core, CPU node, and GPU node bandwidth. Finally, running on hundreds of MIT SuperCloud nodes simultaneously achieved a sustained bandwidth of over 1 PB/s. <br /><br />Summary: <div>
arXiv:2508.17493v1 Announce Type: cross 
Abstract: High level programming languages and GPU accelerators are powerful enablers for a wide range of applications. Achieving scalable vertical (within a compute node), horizontal (across compute nodes), and temporal (over different generations of hardware) performance while retaining productivity requires effective abstractions. Distributed arrays are one such abstraction that enables high level programming to achieve highly scalable performance. Distributed arrays achieve this performance by deriving parallelism from data locality, which naturally leads to high memory bandwidth efficiency. This paper explores distributed array performance using the STREAM memory bandwidth benchmark on a variety of hardware. Scalable performance is demonstrated within and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across multiple nodes was linear. The hardware used spans decades and allows a direct comparison of hardware improvements for memory bandwidth over this time range; showing a 10x increase in CPU core bandwidth over 20 years, 100x increase in CPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5 years. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a sustained bandwidth $>$1 PB/s.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boltzina: Efficient and Accurate Virtual Screening via Docking-Guided Binding Prediction with Boltz-2</title>
<link>https://arxiv.org/abs/2508.17555</link>
<guid>https://arxiv.org/abs/2508.17555</guid>
<content:encoded><![CDATA[
<div> high-accuracy, computational efficiency, virtual screening, drug discovery, molecular docking<br />
Summary:
Boltzina is introduced as a novel framework in structure-based drug discovery to enhance the computational efficiency of high-accuracy binding affinity prediction. By omitting the structure prediction step and directly predicting affinity from AutoDock Vina docking poses, Boltzina achieves improved screening performance compared to traditional methods like AutoDock Vina and GNINA. While Boltzina falls slightly below Boltz-2 in accuracy, it offers significant speed enhancements, up to 11.8 times faster, through optimized iterations and batch processing. The study explores multi-pose selection strategies and proposes a two-stage screening approach combining Boltzina and Boltz-2 for increased accuracy and efficiency tailored to specific application requirements. This work marks the first successful integration of Boltz-2's accurate predictions into practical-scale screening, providing a comprehensive pipeline that balances accuracy and efficiency in computational biology.<br /><br /> <div>
arXiv:2508.17555v1 Announce Type: cross 
Abstract: In structure-based drug discovery, virtual screening using conventional molecular docking methods can be performed rapidly but suffers from limitations in prediction accuracy. Recently, Boltz-2 was proposed, achieving extremely high accuracy in binding affinity prediction, but requiring approximately 20 seconds per compound per GPU, making it difficult to apply to large-scale screening of hundreds of thousands to millions of compounds. This study proposes Boltzina, a novel framework that leverages Boltz-2's high accuracy while significantly improving computational efficiency. Boltzina achieves both accuracy and speed by omitting the rate-limiting structure prediction from Boltz-2's architecture and directly predicting affinity from AutoDock Vina docking poses. We evaluate on eight assays from the MF-PCBA dataset and show that while Boltzina performs below Boltz-2, it provides significantly higher screening performance compared to AutoDock Vina and GNINA. Additionally, Boltzina achieved up to 11.8$\times$ faster through reduced recycling iterations and batch processing. Furthermore, we investigated multi-pose selection strategies and two-stage screening combining Boltzina and Boltz-2, presenting optimization methods for accuracy and efficiency according to application requirements. This study represents the first attempt to apply Boltz-2's high-accuracy predictions to practical-scale screening, offering a pipeline that combines both accuracy and efficiency in computational biology. The Boltzina is available on github; https://github.com/ohuelab/boltzina.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation</title>
<link>https://arxiv.org/abs/2508.17568</link>
<guid>https://arxiv.org/abs/2508.17568</guid>
<content:encoded><![CDATA[
<div> Keywords: Metamaterials, MetaDSL, MetaDB, MetaBench, structure-representation-property relationships 

Summary: 
MetaDSL is introduced as a domain-specific language for capturing diverse metamaterial designs in a human-readable and machine-parsable form. MetaDB serves as a repository with a vast collection of parameterized MetaDSL programs and their derivatives, providing detailed information on geometry, renderings, and elastic properties. MetaBench offers benchmark suites for testing vision-language metamaterial assistants' core capabilities like structure reconstruction, inverse design driven by properties, and performance prediction. The study establishes baselines by fine-tuning advanced vision-language models and deploying an omni-model within an interactive CAD-like interface. Through case studies, the framework demonstrates a significant advancement in integrated design and comprehension of structure-representation-property relationships. 

<br /><br />Summary: <div>
arXiv:2508.17568v1 Announce Type: cross 
Abstract: Metamaterials are micro-architected structures whose geometry imparts highly tunable-often counter-intuitive-bulk properties. Yet their design is difficult because of geometric complexity and a non-trivial mapping from architecture to behaviour. We address these challenges with three complementary contributions. (i) MetaDSL: a compact, semantically rich domain-specific language that captures diverse metamaterial designs in a form that is both human-readable and machine-parsable. (ii) MetaDB: a curated repository of more than 150,000 parameterized MetaDSL programs together with their derivatives-three-dimensional geometry, multi-view renderings, and simulated elastic properties. (iii) MetaBench: benchmark suites that test three core capabilities of vision-language metamaterial assistants-structure reconstruction, property-driven inverse design, and performance prediction. We establish baselines by fine-tuning state-of-the-art vision-language models and deploy an omni-model within an interactive, CAD-like interface. Case studies show that our framework provides a strong first step toward integrated design and understanding of structure-representation-property relationships.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral-Prior Guided Multistage Physics-Informed Neural Networks for Highly Accurate PDE Solutions</title>
<link>https://arxiv.org/abs/2508.17902</link>
<guid>https://arxiv.org/abs/2508.17902</guid>
<content:encoded><![CDATA[
<div> PINNs, physics-informed neural networks, spectral prior, multistage strategy, accuracy improvement <br />
Summary:<br /> 
- This paper introduces two methods, SI-MSPINNs and RFF-MSPINNs, to enhance the accuracy of Physics-Informed Neural Networks (PINNs) by incorporating spectral information.
- SI-MSPINNs extract dominant spectral patterns to guide network initialization and use a multistage strategy to optimize resolution accuracy.
- RFF-MSPINNs combine random Fourier features with spectral weighting to prioritize learning high-energy physical modes based on residual power spectral density.
- Experimental verification on the Burgers equation and Helmholtz equation demonstrates significant accuracy improvements compared to traditional PINNs. 
- The proposed methods offer a practical solution to enhance the accuracy and performance of PINNs for solving high-dimensional problems efficiently. 
Summary: <div>
arXiv:2508.17902v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINNs) are becoming a popular method for solving PDEs, due to their mesh-free nature and their ability to handle high-dimensional problems where traditional numerical solvers often struggle. Despite their promise, the practical application of PINNs is still constrained by several fac- tors, a primary one being their often-limited accuracy. This paper is dedicated to enhancing the accuracy of PINNs by introducing spectral-prior guided multistage strategy. We propose two methods: Spectrum- Informed Multistage Physics-Informed Neural Networks (SI-MSPINNs) and Multistage Physics-Informed Neural Networks with Spectrum Weighted Random Fourier Features (RFF-MSPINNs). The SI-MSPINNs integrate the core mechanism of Spectrum-Informed Multistage Neural Network (SI-MSNNs) and PINNs, in which we extract the Dominant Spectral Pattern (DSP) of residuals by the discrete Fourier transform. This DSP guides the network initialization to alleviate spectral bias, and gradually optimizes the resolution accuracy using a multistage strategy. The RFF-MSPINNs combines random Fourier features with spectral weighting methods, dynamically adjusting the frequency sampling distribution based on the residual power spectral density, allowing the network to prioritize learning high-energy physical modes. Through experimental verification of the Burgers equation and the Helmholtz equation, we show that both models significantly improve the accuracy of the original PINNs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermodynamically Consistent Hybrid and Permutation-Invariant Neural Yield Functions for Anisotropic Plasticity</title>
<link>https://arxiv.org/abs/2508.15923</link>
<guid>https://arxiv.org/abs/2508.15923</guid>
<content:encoded><![CDATA[
<div> architecturally-constrained neural networks, plastic anisotropy, yield criteria, anisotropic yield function, data-driven frameworks
<br />
Summary:
The study addresses the challenge of modeling plastic anisotropy in metals by utilizing architecturally-constrained neural networks. Two data-driven frameworks are developed: one that combines the Hill yield criterion with Input Convex Neural Networks for anisotropic yield function representation, and another that uses a permutation-invariant input convex neural network to embed anisotropy through linear stress transformations. Calibration on an Al-7079 extrusion experimental dataset shows that the permutation-invariant input convex neural network frameworks outperform existing methods in terms of generalization capabilities. These frameworks accurately predict yield loci and Lankford ratios with minimal data, demonstrating the potential for rapid and thermodynamically consistent constitutive models for advanced forming simulations and microstructure-informed design in the future. 
<br /> <div>
arXiv:2508.15923v1 Announce Type: new 
Abstract: Plastic anisotropy in metals remains challenging to model. This is partly because conventional phenomenological yield criteria struggle to combine a highly descriptive, flexible representation with constraints, such as convexity, dictated by thermodynamic consistency. To address this gap, we employ architecturally-constrained neural networks and develop two data-driven frameworks: (i) a hybrid model that augments the Hill yield criterion with an Input Convex Neural Network (ICNN) to get an anisotropic yield function representation in the six-dimensional stress space and (ii) a permutation-invariant input convex neural network (PI-ICNN) that learns an isotropic yield function representation in the principal stress space and embeds anisotropy through linear stress transformations. We calibrate the proposed frameworks on a sparse Al-7079 extrusion experimental dataset comprising 12 uniaxial samples with measured yield stresses and Lankford ratios. To test the robustness of each framework, nine datasets were generated using k-fold cross-validation. These datasets were then used to quantitatively compare Hill-48, Yld2004-18p, pure ICNNs, the hybrid approach, and the PI-ICNN frameworks. While ICNNs and hybrid approaches can almost perfectly fit the training data, they exhibit significant over-fitting, resulting in high validation and test losses. In contrast, both PI-ICNN frameworks demonstrate better generalization capabilities, even outperforming Yld2004-18p on the validation and test data. These results demonstrate that PI-ICNNs unify physics-based constraints with the flexibility of neural networks, enabling the accurate prediction of both yield loci and Lankford ratios from minimal data. The approach opens a path toward rapid, thermodynamically consistent constitutive models for advanced forming simulations and future exploration of coupled hardening or microstructure-informed design.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise, Adaptation, and Strategy: Assessing LLM Fidelity in Decision-Making</title>
<link>https://arxiv.org/abs/2508.15926</link>
<guid>https://arxiv.org/abs/2508.15926</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, social science simulations, decision-making, variability, adaptability <br />
Summary: 
Large language models (LLMs) are being used in social science simulations, but their ability to simulate human decision-making variability and adaptability is not well understood. A new evaluation framework with progressive interventions was proposed to examine LLM agents' adaptability under different levels of external guidance and human-derived noise. The framework was validated on two classic economics tasks, highlighting behavioral gaps between LLMs and humans. By default, LLMs tend to converge on stable and conservative strategies that differ from human behaviors. Risk-framed instructions influence LLM behavior but do not capture human-like diversity. Incorporating human data through in-context learning helps narrow the gap but still falls short of replicating human subjects' strategic variability. These results underscore the need for more realistic evaluations of LLMs in dynamic decision-making tasks, providing guidance for their application in synthetic data for social science research. <br /><br /> <div>
arXiv:2508.15926v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in social science simulations. While their performance on reasoning and optimization tasks has been extensively evaluated, less attention has been paid to their ability to simulate human decision-making's variability and adaptability. We propose a process-oriented evaluation framework with progressive interventions (Intrinsicality, Instruction, and Imitation) to examine how LLM agents adapt under different levels of external guidance and human-derived noise. We validate the framework on two classic economics tasks, irrationality in the second-price auction and decision bias in the newsvendor problem, showing behavioral gaps between LLMs and humans.
  We find that LLMs, by default, converge on stable and conservative strategies that diverge from observed human behaviors. Risk-framed instructions impact LLM behavior predictably but do not replicate human-like diversity. Incorporating human data through in-context learning narrows the gap but fails to reach human subjects' strategic variability. These results highlight a persistent alignment gap in behavioral fidelity and suggest that future LLM evaluations should consider more process-level realism. We present a process-oriented approach for assessing LLMs in dynamic decision-making tasks, offering guidance for their application in synthetic data for social science research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUEENS: An Open-Source Python Framework for Solver-Independent Analyses of Large-Scale Computational Models</title>
<link>https://arxiv.org/abs/2508.16316</link>
<guid>https://arxiv.org/abs/2508.16316</guid>
<content:encoded><![CDATA[
<div> Keywords: QUEENS, uncertainty quantification, simulation management, distributed computing, Bayesian analysis

Summary:
QUEENS is a Python framework designed to facilitate the analysis of large-scale computational models, specifically patient-specific digital twins of diseased human organs. It aims to streamline simulation management with arbitrary solvers on distributed systems, offering a range of state-of-the-art algorithms for convergence studies, optimization, uncertainty quantification, and Bayesian inverse analysis. The framework supports both deterministic and probabilistic analysis, featuring multi-fidelity uncertainty quantification and Bayesian analysis. With a modular architecture, QUEENS allows researchers to easily switch between different types of analyses and build sophisticated algorithms. The open-source repository for QUEENS is available on GitHub, providing researchers with access to cutting-edge research in probabilistic machine learning and efficient analysis methods. <div>
arXiv:2508.16316v1 Announce Type: new 
Abstract: A growing challenge in research and industrial engineering applications is the need for repeated, systematic analysis of large-scale computational models, for example, patient-specific digital twins of diseased human organs: The analysis requires efficient implementation, data, resource management, and parallelization, possibly on distributed systems. To tackle these challenges and save many researchers from annoying, time-consuming tasks, we present QUEENS (Quantification of Uncertain Effects in Engineering Systems), an open-source Python framework for composing and managing simulation analyses with arbitrary (physics-based) solvers on distributed computing infrastructures. Besides simulation management capabilities, QUEENS offers a comprehensive collection of efficiently implemented state-of-the-art algorithms ranging from routines for convergence studies and common optimization algorithms to more advanced sampling algorithms for uncertainty quantification and Bayesian inverse analysis. Additionally, we provide our latest cutting-edge research in multi-fidelity uncertainty quantification, efficient multi-fidelity Bayesian inverse analysis, and probabilistic machine learning. QUEENS adopts a Bayesian, probabilistic mindset but equally supports standard deterministic analysis without requiring prior knowledge of probability theory. The modular architecture allows rapid switching between common types of analyses and facilitates building sophisticated hierarchical algorithms. Encouraging natural incremental steps and scaling towards complexity allows researchers to consider the big picture while building towards it through smaller, manageable steps. The open-source repository is available at https://github.com/queens-py/queens.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Linear to Hierarchical: Evolving Tree-structured Thoughts for Efficient Alpha Mining</title>
<link>https://arxiv.org/abs/2508.16334</link>
<guid>https://arxiv.org/abs/2508.16334</guid>
<content:encoded><![CDATA[
<div> Alpha mining, Large Language Models, Tree-structured thought Evolution, hierarchical reasoning, automatic quantitative investment <br />
Summary: 

This paper introduces Tree-structured thought Evolution (TreEvo) as a solution to alpha mining using Large Language Models (LLMs). The goal is to automatically discover signals that predict asset returns without depending on handcrafted features or arithmetic operators. TreEvo evolves hierarchical reasoning ideas solely at the thought level, addressing the hierarchical tree structures of alphas. Experiments on real-market datasets show that TreEvo can generate better alphas in less computational time and with fewer expert efforts compared to traditional methods. The tree-structured thoughts and compatible evolutionary operators play a crucial role in achieving this improvement, demonstrating the importance of considering hierarchical structures in alpha mining. <br /><br />Summary: <div>
arXiv:2508.16334v1 Announce Type: new 
Abstract: Alpha mining, which discovers signals that predict asset returns, has long been attractive for automatic quantitative investment. This problem is typically formulated as a tree-based symbolic regression with handcrafted market data features and arithmetic operators. Unfortunately, existing symbolic methods are concerned with computational inefficiency and dependence on prior knowledge. Recent implementation of Large Language Models (LLMs) show that they can automatically generate executable codes for various tasks efficiently, thus can be considered as a new promising way for alpha mining. Specifically, LLMs-driven methods evolve a set of heuristics, including thoughts and codes, where the thoughts are usually represented as plain-text prompts of codes. Unfortunately, trivially adopting them in alpha mining ignores the fact that alphas are with hierarchical tree structures. This paper introduces Tree-structured thought Evolution (TreEvo), which evolves hierarchical reasoning ideas solely at the thought level. Experiments on four real-market datasets demonstrate that TreEvo can obtain better alphas with much less computational time and human expert efforts. And this superiority hardly holds without the tree-structured thoughts and the compatible evolutionary operators.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-optimized replacement strategies for water electrolysis systems affected by degradation</title>
<link>https://arxiv.org/abs/2508.16370</link>
<guid>https://arxiv.org/abs/2508.16370</guid>
<content:encoded><![CDATA[
<div> renewable hydrogen, electrolyzer stacks, degradation modeling, levelized cost, optimization<br />
<br />
Summary:<br />
A study was conducted to analyze the economics of green hydrogen production using water electrolysis systems. The focus was on minimizing the degradation of electrolyzer stacks to reduce project costs and increase stack lifetime. A linear optimization approach was used to calculate the levelized cost of hydrogen based on varying degradation thresholds, determining the optimal time for stack replacement. The study considered uncertainties such as degradation scale, load-dependency of degradation and energy demand, and electrolyzer costs. The findings showed that the optimal time for stack replacement could differ by up to 9 years depending on degradation scale. Understanding the impact of degradation is crucial for reducing project costs and supporting the growth of the hydrogen market. <div>
arXiv:2508.16370v1 Announce Type: new 
Abstract: A key factor in reducing the cost of green hydrogen production projects using water electrolysis systems is to minimize the degradation of the electrolyzer stacks, as this impacts the lifetime of the stacks and therefore the frequency of their replacement. To create a better understanding of the economics of stack degradation, we present a linear optimization approach minimizing the costs of a green hydrogen supply chain including an electrolyzer with degradation modeling. By calculating the levelized cost of hydrogen depending on a variable degradation threshold, the cost optimal time for stack replacement can be identified. We further study how this optimal time of replacement is affected by uncertainties such as the degradation scale, the load-dependency of both degradation and energy demand, and the costs of the electrolyzer. The variation of the identified major uncertainty degradation scale results in a difference of up to 9 years regarding the cost optimal time for stack replacement, respectively lifetime of the stacks. Therefore, a better understanding of the degradation impact is imperative for project cost reductions, which in turn would support a proceeding hydrogen market ramp-up.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment-Aware Mean-Variance Portfolio Optimization for Cryptocurrencies</title>
<link>https://arxiv.org/abs/2508.16378</link>
<guid>https://arxiv.org/abs/2508.16378</guid>
<content:encoded><![CDATA[
<div> Keywords: cryptocurrency, portfolio optimization, technical indicators, sentiment analysis, investment decision-making

Summary:
This paper introduces a dynamic cryptocurrency portfolio optimization strategy that combines technical indicators and sentiment analysis for better investment decision-making. The method uses the RSI and SMA to capture market momentum and sentiment scores from news articles with the VADER model. Google Gemini is employed to verify sentiment scores and make investment decisions. These signals are integrated into expected return estimates for mean-variance optimization with asset weight constraints. The strategy is tested through a rolling-window backtest on cryptocurrency market data, outperforming benchmarks of Bitcoin and an equal-weighted portfolio in cumulative return and Sharpe ratio. However, it also shows higher short-term downside risk. The results demonstrate the potential of integrating sentiment and technical signals to enhance cryptocurrency portfolio performance while emphasizing the importance of managing risk exposure in volatile markets.<br /><br />Summary: <div>
arXiv:2508.16378v1 Announce Type: new 
Abstract: This paper presents a dynamic cryptocurrency portfolio optimization strategy that integrates technical indicators and sentiment analysis to enhance investment decision-making. The proposed method employs the 14-day Relative Strength Index (RSI) and 14-day Simple Moving Average (SMA) to capture market momentum, while sentiment scores are extracted from news articles using the VADER (Valence Aware Dictionary and sEntiment Reasoner) model, with compound scores quantifying overall market tone. The large language model Google Gemini is used to further verify the sentiment scores predicted by VADER and give investment decisions. These technical indicator and sentiment signals are incorporated into the expected return estimates before applying mean-variance optimization with constraints on asset weights. The strategy is evaluated through a rolling-window backtest over cryptocurrency market data, with Bitcoin (BTC) and an equal-weighted portfolio of selected cryptocurrencies serving as benchmarks. Experimental results show that the proposed approach achieves a cumulative return of 38.72, substantially exceeding Bitcoin's 8.85 and the equal-weighted portfolio's 21.65 over the same period, and delivers a higher Sharpe ratio (1.1093 vs. 0.8853 and 1.0194, respectively). However, the strategy exhibits a larger maximum drawdown (-18.52%) compared to Bitcoin (-4.48%) and the equal-weighted portfolio (-11.02%), indicating higher short-term downside risk. These results highlight the potential of combining sentiment and technical signals to improve cryptocurrency portfolio performance, while also emphasizing the need to address risk exposure in volatile markets.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain</title>
<link>https://arxiv.org/abs/2508.15776</link>
<guid>https://arxiv.org/abs/2508.15776</guid>
<content:encoded><![CDATA[
<div> Keywords: pharmaceutical supply chain, cybersecurity, zero trust architecture, data protection, resilience

Summary:
The pharmaceutical supply chain is facing increasing cybersecurity challenges that threaten patient safety and operational continuity. This paper explores the potential of zero trust architecture in enhancing security and resilience in this critical ecosystem. By implementing principles such as continuous verification, least-privilege access, and data-centric security, organizations can strengthen security measures and protect sensitive data. Real-world case studies demonstrate the successful implementation of zero trust in pharmaceutical supply chains. One crucial area where zero trust can be effectively applied is in managing narcotics and high-health-risk drugs to ensure drug safety throughout the production process. By adopting zero trust principles, the pharmaceutical industry can safeguard its supply chain from evolving cyber threats, guaranteeing the reliability of critical medical operations.<br /><br />Summary: <div>
arXiv:2508.15776v1 Announce Type: cross 
Abstract: The pharmaceutical supply chain faces escalating cybersecurity challenges threatening patient safety and operational continuity. This paper examines the transformative potential of zero trust architecture for enhancing security and resilience within this critical ecosystem. We explore the challenges posed by data breaches, counterfeiting, and disruptions and introduce the principles of continuous verification, least-privilege access, and data-centric security inherent in zero trust. Real-world case studies illustrate successful implementations. Benefits include heightened security, data protection, and adaptable resilience. As recognized by researchers and industrialists, a reliable drug tracing system is crucial for ensuring drug safety throughout the pharmaceutical production process. One of the most pivotal domains within the pharmaceutical industry and its associated supply chains where zero trust can be effectively implemented is in the management of narcotics, high-health-risk drugs, and abusable substances. By embracing zero trust, the pharmaceutical industry fortifies its supply chain against constantly changing cyber threats, ensuring the trustworthiness of critical medical operations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms</title>
<link>https://arxiv.org/abs/2508.15963</link>
<guid>https://arxiv.org/abs/2508.15963</guid>
<content:encoded><![CDATA[
<div> Measurement System, Wheel Flange Wear, Machine Learning Algorithm, Infinite Impulse Response Filter, Rail Safety<br />
Summary:<br /> 
This paper presents an innovative onboard measurement system for monitoring wheel flange wear depth in railway systems. The system uses displacement and temperature sensors to accurately measure wear depth and surrounding temperature fluctuations. Machine learning algorithms based on regression models are trained dynamically using collected data, achieving an accuracy of 96.5%. An infinite impulse response filter (IIR) is designed to mitigate vehicle dynamics and sensor noise, further enhancing accuracy to 98.2%. The system also integrates with Internet of Things devices for real-time monitoring of wheel flange wear and track conditions. Overall, this advanced monitoring system ensures increased safety and efficiency in railway operations. <div>
arXiv:2508.15963v1 Announce Type: cross 
Abstract: Rail and wheel interaction functionality is pivotal to the railway system safety, requiring accurate measurement systems for optimal safety monitoring operation. This paper introduces an innovative onboard measurement system for monitoring wheel flange wear depth, utilizing displacement and temperature sensors. Laboratory experiments are conducted to emulate wheel flange wear depth and surrounding temperature fluctuations in different periods of time. Employing collected data, the training of machine learning algorithms that are based on regression models, is dynamically automated. Further experimentation results, using standards procedures, validate the system's efficacy. To enhance accuracy, an infinite impulse response filter (IIR) that mitigates vehicle dynamics and sensor noise is designed. Filter parameters were computed based on specifications derived from a Fast Fourier Transform analysis of locomotive simulations and emulation experiments data. The results show that the dynamic machine learning algorithm effectively counter sensor nonlinear response to temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime. The real-time noise reduction via IIR filter enhances the accuracy up to 98.2 %. Integrated with railway communication embedded systems such as Internet of Things devices, this advanced monitoring system offers unparalleled real-time insights into wheel flange wear and track irregular conditions that cause it, ensuring heightened safety and efficiency in railway systems operations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Scattering Matrix Synthesis: Independent Region Decomposition for Hybrid Antenna--Scatterer Systems</title>
<link>https://arxiv.org/abs/2503.17616</link>
<guid>https://arxiv.org/abs/2503.17616</guid>
<content:encoded><![CDATA[
<div> Keywords: generalized scattering matrix, hybrid electromagnetic systems, vector spherical wavefunctions, modular region decomposition, efficient analysis <br />
Summary: 
This paper introduces a unified formulation for synthesizing the generalized scattering matrix (GS-matrix) of hybrid electromagnetic systems consisting of various antennas and scatterers. The method utilizes a modular region decomposition framework to analyze electromagnetic interactions between separate structures, assuming they are separable by a plane. By using the addition theorem of vector spherical wavefunctions (VSWFs), a compact matrix representation is created to combine the GS- and S-matrices of individual components for the overall system response. This approach extends previous methods for multiple scattering or antenna array analysis, making it suitable for configurations where substructures can be repositioned or reused. Numerical examples demonstrate the accuracy and flexibility of the method, including cases with closely spaced components and rotational variations in substructure layout. <div>
arXiv:2503.17616v2 Announce Type: replace 
Abstract: This paper presents a unified formulation for synthesizing the generalized scattering matrix (GS-matrix) of hybrid electromagnetic systems comprising arbitrary numbers of antennas and scatterers. The proposed method provides a modular region decomposition framework that enables efficient analysis of electromagnetic interactions between distinct structures, under the relaxed geometric condition that the constituents are separable by a plane. By leveraging the addition theorem of vector spherical wavefunctions (VSWFs), a compact matrix representation is derived to assemble the GS- and S-matrices of individual components into the overall system response. This formulation generalizes and extends prior methods developed for either multiple scattering or antenna array analysis, and is particularly suited to configurations where substructures may be repositioned or reused. Numerical examples are provided to validate the accuracy and versatility of the method, including scenarios involving tightly spaced components and rotational variations in substructure layout.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa</title>
<link>https://arxiv.org/abs/2508.15110</link>
<guid>https://arxiv.org/abs/2508.15110</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, insurance sector, African insurance market, inclusive solutions
<br />
Summary: 
Artificial Intelligence, particularly Large Language Models (LLMs) and agentic AI, offer transformative potential in the insurance sector. Rapid performance improvements, open-source access, and decreasing deployment costs present unique opportunities and challenges for insurers. There is a need to address the complexity of LLM and agentic AI frameworks in the insurance industry. In the African insurance market, critical gaps exist, but there are also local efforts, players, and partnership opportunities that can be leveraged. It is essential for actuaries, insurers, regulators, and tech leaders to collaborate in creating inclusive, sustainable, and equitable AI strategies and solutions that cater to the specific needs of Africans.
<br /><br />Summary: <div>
arXiv:2508.15110v1 Announce Type: new 
Abstract: In this work, we highlight the transformative potential of Artificial Intelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in the insurance sector. We consider and emphasize the unique opportunities, challenges, and potential pathways in insurance amid rapid performance improvements, increased open-source access, decreasing deployment costs, and the complexity of LLM or agentic AI frameworks. To bring it closer to home, we identify critical gaps in the African insurance market and highlight key local efforts, players, and partnership opportunities. Finally, we call upon actuaries, insurers, regulators, and tech leaders to a collaborative effort aimed at creating inclusive, sustainable, and equitable AI strategies and solutions: by and for Africans.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating profitable price bounds for prescriptive price optimization</title>
<link>https://arxiv.org/abs/2508.15248</link>
<guid>https://arxiv.org/abs/2508.15248</guid>
<content:encoded><![CDATA[
<div> bootstrap procedure, confidence intervals, Nelder-Mead simplex method, black-box optimization, total revenue

Summary:
The article discusses the importance of pricing in maximizing business profits, focusing on prescriptive price optimization. Two methods for estimating price bounds in prescriptive price optimization are proposed: one using the bootstrap procedure to estimate confidence intervals for optimal prices, and the other employing the Nelder-Mead simplex method for black-box price bounds optimization. Experimental results with synthetic price-demand datasets show that these methods successfully narrow down the price range while maintaining high revenues, especially with a small number of items or low demand noise levels. Additionally, the comparative advantage of these methods increases as more data accumulates. <div>
arXiv:2508.15248v1 Announce Type: cross 
Abstract: Pricing of products and services, which has a significant impact on consumer demand, is one of the most important factors in maximizing business profits. Prescriptive price optimization is a prominent data-driven pricing methodology consisting of two phases: demand forecasting and price optimization. In the practice of prescriptive price optimization, the price of each item is typically set within a predetermined range defined by lower and upper bounds. Narrow price ranges can lead to missed opportunities, while wide price ranges run the risk of proposing unrealistic prices; therefore, determining profitable price bounds while maintaining the reliability of the suggested prices is a critical challenge that directly affects the effectiveness of prescriptive price optimization. We propose two methods for estimating price bounds in prescriptive price optimization so that future total revenue derived from the optimized prices will be maximized. Our first method for price bounds estimation uses the bootstrap procedure to estimate confidence intervals for optimal prices. Our second method uses the Nelder--Mead simplex method for black-box price bounds optimization that maximizes total revenue estimated through $K$-fold cross-validation. Experimental results with synthetic price--demand datasets demonstrate that our methods successfully narrowed down the price range while maintaining high revenues, particularly when the number of items was small or the demand noise level was low. Moreover, as more data accumulated, the comparative advantage of our methods further increased.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEAS: Hierarchical Evolutionary Agent Simulation Framework for Cross-Scale Modeling and Multi-Objective Search</title>
<link>https://arxiv.org/abs/2508.15555</link>
<guid>https://arxiv.org/abs/2508.15555</guid>
<content:encoded><![CDATA[
<div> Framework, Agent-based modeling, Evolutionary optimization, Python, Hierarchical

Summary:
Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that integrates layered agent-based modeling with evolutionary optimization and tournament evaluation. HEAS organizes models as hierarchies of processes scheduled in layers, facilitating explicit cross-scale couplings. The framework provides a compact API for simulating, optimizing, and evaluating single- and multi-objective evolution, with PyTorch policy integration. HEAS standardizes evaluation metrics, persists data, and offers plotting tools for analysis. It emphasizes separating mechanism from orchestration, enabling easy composition and swapping of components. The framework is versatile and applicable for forward simulation, optimization, and comparisons across studies. Two example applications demonstrate the utility of HEAS in ecological systems and enterprise decision-making scenarios. HEAS serves as a reliable foundation for interdisciplinary, multi-level investigations, delivering reproducible results. 

<br /><br />Summary: <div>
arXiv:2508.15555v1 Announce Type: cross 
Abstract: Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that unifies layered agent-based modeling with evolutionary optimization and tournament evaluation in a single, reproducible workflow. HEAS represents models as hierarchies of lightweight processes ("streams") scheduled in deterministic layers that read and write a shared context, making cross-scale couplings explicit and auditable. A compact API and CLI-simulate, optimize, evaluate-expose single- and multi-objective evolution, PyTorch policy integration via parameter flattening/unflattening, and general tournament tooling with user-defined scoring and voting rules. The framework standardizes evaluation through uniform per-step and episode metrics, persists seeds, logbooks, and hall-of-fame archives, and provides plotting helpers for traces, Pareto fronts, and comparative outcomes, reducing glue code and improving comparability across studies. HEAS emphasizes separation of mechanism from orchestration, allowing exogenous drivers, endogenous agents, and aggregators to be composed and swapped without refactoring, while the same model can be used for forward simulation, optimization, or systematic comparison. We illustrate usage with two compact examples-an ecological system and an enterprise decision-making setting. HEAS offers a practical foundation for cross-disciplinary, multi-level inquiry, yielding reliable, reproducible results.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliability comparison of vessel trajectory prediction models via Probability of Detection</title>
<link>https://arxiv.org/abs/2508.14198</link>
<guid>https://arxiv.org/abs/2508.14198</guid>
<content:encoded><![CDATA[
<div> deep learning, vessel trajectory prediction, traffic complexity, model performance, reliability analysis

Summary:<br />
This study examines vessel trajectory prediction using deep learning approaches, focusing on evaluating model performance in various traffic complexities. Unlike previous models, this research considers specific traffic situations and assesses reliability through a probability of detection analysis. The models are tested on different traffic scenarios, with performance metrics and reliability estimates calculated for each category. The results provide insights into the strengths and limitations of the prediction approaches and their reliability in ensuring safe forecasts over different prediction horizons. By understanding these aspects, future developments can lead to more reliable vessel trajectory prediction methods, ultimately enhancing safety and efficiency in inland waterway navigation. <br /> <div>
arXiv:2508.14198v1 Announce Type: cross 
Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on the evaluation of different deep learning-based approaches. The objective is to assess model performance in diverse traffic complexities and compare the reliability of the approaches. While previous VTP models overlook the specific traffic situation complexity and lack reliability assessments, this research uses a probability of detection analysis to quantify model reliability in varying traffic scenarios, thus going beyond common error distribution analyses. All models are evaluated on test samples categorized according to their traffic situation during the prediction horizon, with performance metrics and reliability estimates obtained for each category. The results of this comprehensive evaluation provide a deeper understanding of the strengths and weaknesses of the different prediction approaches, along with their reliability in terms of the prediction horizon lengths for which safe forecasts can be guaranteed. These findings can inform the development of more reliable vessel trajectory prediction approaches, enhancing safety and efficiency in future inland waterways navigation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel geometric predictive algorithm for assessing Compressive Elastic Modulus in MEX additive processes, based on part nonlinearities and layers stiffness,validated with PETG and PLA materials</title>
<link>https://arxiv.org/abs/2508.13164</link>
<guid>https://arxiv.org/abs/2508.13164</guid>
<content:encoded><![CDATA[
<div> Algorithm, plastic materials, MEX, elastic modulus, compressive loads

Summary:
The paper introduces a new predictive algorithm developed by researchers for determining the elastic modulus and mechanical behavior of plastic materials manufactured using MEX under compressive loads. This algorithm requires input of the compressive elastic modulus of the material filament and MEX manufacturing parameters. It calculates layer stiffness based on the number of holes in the projected area and has been validated using PETG and PLA materials on test specimens and a variable topology case study. The algorithm is applicable to various print patterns and manufacturing directions, offering versatility for different plastic polymers suitable for MEX. It eliminates the need for costly mechanical analysis software or extensive experimental validations for complex component geometries under uniaxial compression loads. 

<br /><br />Summary: <div>
arXiv:2508.13164v1 Announce Type: new 
Abstract: The paper presents an innovative methodology based on the use of a new predictive algorithm created by the researchers capable of obtaining the elastic modulus of a plastic material manufactured with MEX and its mechanical behaviour in the elastic zone under compressive loads. The predictive algorithm only needs as input the compressive elastic modulus of the isotropic plastic material filament and the manufacturing parameters of the MEX process. The smart developed algorithm calculates the stiffness of each layer considering the number of holes in the projected area. The innovative predictive algorithm has been experimentally and numerically validated using PETG Polyethylene Terephthalate Glycol material and PLA Polylactic Acid on test specimens and on a case study of variable topology. The predictive algorithm is valid for each print pattern and manufacturing direction. The new algorithm improves the existing state of the art significantly since this algorithm extends its utility to most plastic polymer materials suitable for MEX 3D printing, provided that the mechanical and elastic properties of the filament are known. Its versatility extends to complex component geometries subjected to uniaxial compression loads, eliminating the need for mechanical analysis software or expensive experimental validations.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating Financial Large Language Models</title>
<link>https://arxiv.org/abs/2508.13491</link>
<guid>https://arxiv.org/abs/2508.13491</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, FinCDM, CPA examination, Financial skills, Cognitive diagnosis evaluation

Summary:
FinCDM introduces a new cognitive diagnosis evaluation framework tailored for financial Large Language Models (LLMs). It allows for the evaluation of LLMs at the knowledge-skill level, uncovering hidden knowledge gaps and identifying under-tested areas such as tax and regulatory reasoning. The framework is supported by CPA-QKA, a dataset derived from the Certified Public Accountant examination, providing comprehensive coverage of real-world accounting and financial skills. The dataset is rigorously annotated by domain experts for fine-grained knowledge labels. Through extensive experiments on various LLMs, FinCDM reveals behavioral clusters among models, enabling interpretable, skill-aware diagnosis for more targeted model development. The approach supports trustworthy and targeted model development in the financial domain and aims to improve the overall performance and understanding of LLMs in high-stakes applications.<br /><br />Summary: <div>
arXiv:2508.13491v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown promise for financial applications, yet their suitability for this high-stakes domain remains largely unproven due to inadequacies in existing benchmarks. Existing benchmarks solely rely on score-level evaluation, summarizing performance with a single score that obscures the nuanced understanding of what models truly know and their precise limitations. They also rely on datasets that cover only a narrow subset of financial concepts, while overlooking other essentials for real-world applications. To address these gaps, we introduce FinCDM, the first cognitive diagnosis evaluation framework tailored for financial LLMs, enabling the evaluation of LLMs at the knowledge-skill level, identifying what financial skills and knowledge they have or lack based on their response patterns across skill-tagged tasks, rather than a single aggregated number. We construct CPA-QKA, the first cognitively informed financial evaluation dataset derived from the Certified Public Accountant (CPA) examination, with comprehensive coverage of real-world accounting and financial skills. It is rigorously annotated by domain experts, who author, validate, and annotate questions with high inter-annotator agreement and fine-grained knowledge labels. Our extensive experiments on 30 proprietary, open-source, and domain-specific LLMs show that FinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax and regulatory reasoning overlooked by traditional benchmarks, and uncovers behavioral clusters among models. FinCDM introduces a new paradigm for financial LLM evaluation by enabling interpretable, skill-aware diagnosis that supports more trustworthy and targeted model development, and all datasets and evaluation scripts will be publicly released to support further research.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Discovery of Multi-Dimensional Breakage Population Balance Equations</title>
<link>https://arxiv.org/abs/2508.13763</link>
<guid>https://arxiv.org/abs/2508.13763</guid>
<content:encoded><![CDATA[
<div> Sparse regression, multi-dimensional breakage, population balance equation, data-driven, Dynamic Mode Decomposition

Summary: 
The article introduces the Multi-Dimensional Breakage Population Balance Equation Identification (mPBE ID) algorithm, which aims to discover multi-dimensional breakage population balance equations directly from data. Current inverse solution techniques are limited to one-dimensional cases and require prior system knowledge, constraining their applicability. The mPBE-ID incorporates breakage-informed constrained sparse regression, constructs candidate library functions based on Dynamic Mode Decomposition (DMD) insights, and handles noisy/limited data through ensembling. The DMD is crucial for identifying dominant breakage dynamics and guiding the inclusion of candidate terms. The algorithm successfully discovers various forms of mPBE, even with noisy and limited data, offering a foundational framework for future extensions to generalize the discovery of multi-dimensional PBEs for high-dimensional particulate phenomena.<br /><br />Summary: <div>
arXiv:2508.13763v1 Announce Type: new 
Abstract: Multi-dimensional breakage is a ubiquitous phenomenon in natural systems, yet the systematic discovery of underlying governing equations remains a long-standing challenge. Current inverse solution techniques are restricted to one-dimensional cases and typically depend on the availability of a priori system knowledge, thus limiting their applicability. By leveraging advances in data-driven sparse regression techniques, we develop the Multi-Dimensional Breakage Population Balance Equation Identification (mPBE ID) algorithm for discovering multi-dimensional breakage population balance equations (mPBEs) directly from data. Our mPBE-ID enables tractable identification of mPBEs by incorporating several key strategies, namely, a breakage-informed constrained sparse regression, targeted candidate library functions construction via insights from Dynamic Mode Decomposition (DMD), and robust handling of noisy/limited data through ensembling (bagging/bragging). Notably, we demonstrate how the DMD is indispensable for distilling dominant breakage dynamics which can then be used to facilitate the systematic inclusion of candidate library terms. We showcase the ability of the mPBE-ID to discover different forms of mPBE (including those with discontinuous stoichiometric kernels) even when tested against noisy and limited data. We anticipate that the mPBE-ID will serve as a foundational framework for future extensions to generalize the discovery of multi-dimensional PBEs for various high-dimensional particulate phenomena.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Modelling of Infrastructure Asset Performance Deterioration -- a bounded gamma process approach</title>
<link>https://arxiv.org/abs/2508.13359</link>
<guid>https://arxiv.org/abs/2508.13359</guid>
<content:encoded><![CDATA[
<div> flexible deterioration model, infrastructure asset management systems, gamma process, bounded transformed gamma process, infrastructure performance deterioration

Summary: 
The article discusses the importance of a flexible deterioration model in infrastructure asset management systems and introduces a new bounded transformed gamma process (BTGP) model. This model is compared to a bounded nonstationary gamma process (BNGP) model in terms of deterioration modelling and asset management decision-making. An empirical study using real-world bridge condition data showcases the flexibility and significance of the proposed BTGP model. The BTGP model is deeply rooted in traditional regression modeling, providing a more flexible approach to characterizing different deterioration patterns in infrastructure systems. This study highlights the advantages of the BTGP model over existing alternatives and emphasizes its potential for improving infrastructure asset management practices. <div>
arXiv:2508.13359v1 Announce Type: cross 
Abstract: Infrastructure asset management systems require a flexible deterioration model that can handle various degradation patterns in a unified way. Owing to its appealing monotonic sample paths, independent increments and mathematical tractability, gamma process has been widely employed as an infrastructure performance deterioration model. This model was recently enhanced by introducing an upper bound to satisfy a practical modelling need that many infrastructure performance deterioration processes are constrained by physical or managerial limits. Several bounded transformed gamma process (BTGP) alternatives had been proposed; however, they lacked due flexibility to characterize different deterioration patterns. This paper proposed a new BTGP model that is deeply grounded upon the traditional regression modelling tradition in infrastructure asset management systems. Qualitative and quantitative comparisons were carried out between the proposed BTGP and a bounded nonstationary gamma process (BNGP) model from both deterioration modelling and asset management decision-making perspectives. An empirical study using the real-world historical bridge condition data was performed to examine the flexibility of the BTGP against the BNGP and six other BTGP alternatives. The results confirmed the flexibility and significance of the proposed BTGP model for infrastructure systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persistence is All You Need -- A Topological Lens on Microstructural Characterization</title>
<link>https://arxiv.org/abs/2508.11967</link>
<guid>https://arxiv.org/abs/2508.11967</guid>
<content:encoded><![CDATA[
<div> Keywords: microstructure, materials, energy engineering, computational topology, deep neural network

Summary:
This study presents a novel approach to accurately design materials for energy and chemical engineering technologies by extracting key microstructural descriptors. By combining computational topology with assembly-learning-based regression, the researchers created a workflow that successfully predicted eight important microstructural features. Using a dataset of synthetic three-dimensional microstructures and a deep neural network trained on persistence images, the model achieved high accuracy in predicting the descriptors. The results showed an average R^2 of ~0.84 and Pearson r of ~0.92 in an independent test set, demonstrating both precision and generality of the approach. This unified and scalable tool offers a rapid characterization method for functional porous materials, potentially leading to improved design and performance in various applications. 

<br /><br />Summary: <div>
arXiv:2508.11967v1 Announce Type: new 
Abstract: The microstructure critically governs the properties of materials used in energy and chemical engineering technologies, from catalysts and filters to thermal insulators and sensors. Therefore, accurate design is based on quantitative descriptors of microstructural features. Here we show that eight key descriptors can be extracted by a single workflow that fuses computational topology with assembly-learning-based regression. First, 1312 synthetic three-dimensional microstructures were generated and evaluated using established algorithms, and a labeled data set of ground-truth parameters was built. Converting every structure into a persistence image allowed us to train a deep neural network that predicts the eight descriptors. In an independent test set, the model achieved on average R^2 ~ 0.84 and Pearson r ~ 0.92, demonstrating both precision and generality. The approach provides a unified and scalable tool for rapid characterization of functional porous materials.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Porous Convection in the Discrete Exterior Calculus with Geometric Multigrid</title>
<link>https://arxiv.org/abs/2508.12501</link>
<guid>https://arxiv.org/abs/2508.12501</guid>
<content:encoded><![CDATA[
<div> DEC, Discrete Exterior Calculus, Decapodes.jl, CombinatorialSpaces.jl, porous convection, geometric multigrid solver

Summary:
The article introduces the use of Discrete Exterior Calculus (DEC) in solving porous convection equations through the Decapodes.jl embedded domain-specific language. This approach is implemented using CombinatorialSpaces.jl, a Julia library that applies DEC over simplicial complexes and includes a geometric multigrid solver for maps between subdivided simplicial complexes. The study showcases numerical results of multigrid solvers for both the Poisson problem and porous convection problem, serving as a standalone solver or a preconditioner for open-source Julia iterative methods libraries. The DEC framework ensures the preservation of properties from the exterior calculus, making it a robust choice for solving multiphysics problems with efficiency and accuracy. <div>
arXiv:2508.12501v1 Announce Type: new 
Abstract: The discrete exterior calculus (DEC) defines a family of discretized differential operators which preserve certain desirable properties from the exterior calculus. We formulate and solve the porous convection equations in the DEC via the Decapodes.jl embedded domain-specific language (eDSL) for multiphysics problems discretized via CombinatorialSpaces.jl. CombinatorialSpaces.jl is an open-source Julia library which implements the DEC over simplicial complexes, and now offers a geometric multigrid solver over maps between subdivided simplicial complexes. We demonstrate numerical results of multigrid solvers for the Poisson problem and porous convection problem, both as a standalone solver and as a preconditioner for open-source Julia iterative methods libraries.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensured Energy: A simulation game to elicit preferences around Swiss energy transition pathways</title>
<link>https://arxiv.org/abs/2508.12799</link>
<guid>https://arxiv.org/abs/2508.12799</guid>
<content:encoded><![CDATA[
<div> Keywords: Paris Agreement, energy transition, serious game, public acceptance, sustainability <br />
Summary: 
The article discusses the analysis of Switzerland's energy and climate strategy towards achieving the objectives set in the 2015 Paris Agreement. Researchers examine different scenarios for transitioning towards renewable energy sources and assessing the impacts on society, environment, and economy. To gauge public acceptance of energy policies, a population survey was complemented with an online serious game that simulates the current and future energy provision, allowing players to make informed decisions. The game successfully attracted participants from various societal groups, highlighting the challenge of balancing complexity and entertainment. This approach provides valuable insights into public opinion and offers a more engaging way for stakeholders and policymakers to understand and address the challenges of transitioning to a sustainable energy future. <br /><br />Summary: <div>
arXiv:2508.12799v1 Announce Type: new 
Abstract: The 2015 Paris Agreement on global warming specifies national objectives for the reduction of greenhouse gas emissions. In support of Switzerland's energy and climate strategy for 2050, researchers investigate scenarios for the transition of energy systems towards a higher share of renewables, assessing their social, environmental and economic impact. Their results guide stakeholders and policy makers in designing resilient and sustainable systems. Political scientists use surveys to quantify public acceptance of energy policy, but the complexity and long time horizon of the subject creates difficulties, both for researchers in posing contextually relevant questions, and for respondents in assimilating enough information to give meaningful answers. A population survey was therefore augmented with an online serious game in which players experience an accurate simulation of current and future energy provision and manage transition towards a sustainable future. This interactive environment allows better informed and engaged decisions, and provides richer information on public opinion. In this paper we motivate and describe the design of the game and report initial findings on player characteristics and engagement. We show that a serious game can successfully attract participants from diverse societal groups and highlight the challenge of balancing complexity and entertainment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising diffusion models for inverse design of inflatable structures with programmable deformations</title>
<link>https://arxiv.org/abs/2508.13097</link>
<guid>https://arxiv.org/abs/2508.13097</guid>
<content:encoded><![CDATA[
<div> Keywords: Programmable structures, Inflatable structures, Generative design, Denoising diffusion probabilistic models, Pressure-driven actuation

Summary:
- The article discusses the inverse design of elastic structures undergoing large, nonlinear deformations under pressure-driven actuation.
- A generative design framework based on denoising diffusion probabilistic models (DDPMs) is presented for designing programmable structures.
- The method formulates the inverse design as a conditional generation task, using geometric descriptors of target deformed states as inputs.
- The framework quickly produces diverse undeformed configurations that achieve desired deformations when inflated, enabling parallel exploration of design candidates.
- Numerical experiments show the framework can accommodate complex constraints and efficiently explore viable design options. 

<br /><br />Summary: <div>
arXiv:2508.13097v1 Announce Type: new 
Abstract: Programmable structures are systems whose undeformed geometries and material property distributions are deliberately designed to achieve prescribed deformed configurations under specific loading conditions. Inflatable structures are a prominent example, using internal pressurization to realize large, nonlinear deformations in applications ranging from soft robotics and deployable aerospace systems to biomedical devices and adaptive architecture. We present a generative design framework based on denoising diffusion probabilistic models (DDPMs) for the inverse design of elastic structures undergoing large, nonlinear deformations under pressure-driven actuation. The method formulates the inverse design as a conditional generation task, using geometric descriptors of target deformed states as inputs and outputting image-based representations of the undeformed configuration. Representing these configurations as simple images is achieved by establishing a pre- and postprocessing pipeline that involves a fixed image processing, simulation setup, and descriptor extraction methods. Numerical experiments with scalar and higher-dimensional descriptors show that the framework can quickly produce diverse undeformed configurations that achieve the desired deformations when inflated, enabling parallel exploration of viable design candidates while accommodating complex constraints.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites</title>
<link>https://arxiv.org/abs/2508.12029</link>
<guid>https://arxiv.org/abs/2508.12029</guid>
<content:encoded><![CDATA[
<div> antibody-binding sites, epitopes, vaccine design, antigen sequences, convolutional neural networks 

Summary:
- Accurate prediction of antibody-binding sites is critical for various applications in immunology.
- In silico methods have limitations in predicting conformational epitopes effectively.
- A new conformer-based model leveraging CNNs and Transformers is proposed for epitope prediction.
- CNNs enhance the prediction of linear epitopes, while Transformers improve conformational epitope prediction.
- Experimental results show the model outperforms existing baselines in various performance metrics. 

<br /><br />Summary: <div>
arXiv:2508.12029v1 Announce Type: cross 
Abstract: Accurate prediction of antibody-binding sites (epitopes) on antigens is crucial for vaccine design, immunodiagnostics, therapeutic antibody development, antibody engineering, research into autoimmune and allergic diseases, and for advancing our understanding of immune responses. Despite in silico methods that have been proposed to predict both linear (continuous) and conformational (discontinuous) epitopes, they consistently underperform in predicting conformational epitopes. In this work, we propose a conformer-based model trained on antigen sequences derived from 1,080 antigen-antibody complexes, leveraging convolutional neural networks (CNNs) to extract local features and Transformers to capture long-range dependencies within antigen sequences. Ablation studies demonstrate that CNN enhances the prediction of linear epitopes, and the Transformer module improves the prediction of conformational epitopes. Experimental results show that our model outperforms existing baselines in terms of PCC, ROC-AUC, PR-AUC, and F1 scores on conformational epitopes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems</title>
<link>https://arxiv.org/abs/2508.12569</link>
<guid>https://arxiv.org/abs/2508.12569</guid>
<content:encoded><![CDATA[
<div> framework, metriplectic, entropy, self-supervised learning, stochastic <br />
Summary: 
The article discusses a framework for machine learning coarse-grained dynamics of multiscale systems using the metriplectic bracket formalism. This framework preserves properties such as dissipative, history-dependent, and stochastic emergent physics, ensuring conservation laws and fluctuation-dissipation balance. A novel self-supervised learning strategy is introduced to identify emergent structural variables when labels are unavailable. The method is validated on benchmark systems and applied to challenging examples like coarse-graining star polymers and learning models from high-speed video of colloidal suspensions. Open-source implementations in PyTorch and LAMMPS are provided, enabling large-scale inference and applicability to various particle-based systems. <br /> <div>
arXiv:2508.12569v1 Announce Type: cross 
Abstract: Multiscale systems are ubiquitous in science and technology, but are notoriously challenging to simulate as short spatiotemporal scales must be appropriately linked to emergent bulk physics. When expensive high-dimensional dynamical systems are coarse-grained into low-dimensional models, the entropic loss of information leads to emergent physics which are dissipative, history-dependent, and stochastic. To machine learn coarse-grained dynamics from time-series observations of particle trajectories, we propose a framework using the metriplectic bracket formalism that preserves these properties by construction; most notably, the framework guarantees discrete notions of the first and second laws of thermodynamics, conservation of momentum, and a discrete fluctuation-dissipation balance crucial for capturing non-equilibrium statistics. We introduce the mathematical framework abstractly before specializing to a particle discretization. As labels are generally unavailable for entropic state variables, we introduce a novel self-supervised learning strategy to identify emergent structural variables. We validate the method on benchmark systems and demonstrate its utility on two challenging examples: (1) coarse-graining star polymers at challenging levels of coarse-graining while preserving non-equilibrium statistics, and (2) learning models from high-speed video of colloidal suspensions that capture coupling between local rearrangement events and emergent stochastic dynamics. We provide open-source implementations in both PyTorch and LAMMPS, enabling large-scale inference and extensibility to diverse particle-based systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shapley Values: Paired-Sampling Approximations</title>
<link>https://arxiv.org/abs/2508.12947</link>
<guid>https://arxiv.org/abs/2508.12947</guid>
<content:encoded><![CDATA[
<div> Shapley values, cooperative game theory, machine learning, sampling approximations, KernelSHAP, PermutationSHAP <br />
<br />
Summary: <br />
Shapley values, originally from cooperative game theory, are widely used in explaining machine learning predictions by assigning credit to input components based on their contribution. This study provides novel contributions by proving the asymptotic normality of sampling approximations like KernelSHAP and PermutationSHAP. Paired-sampling approaches offer exact results for interactions of maximal order two. Additionally, the paired-sampling PermutationSHAP exhibits the additive recovery property, while the kernel counterpart does not. These findings enhance the understanding and computation of Shapley values in explaining prediction outcomes. <div>
arXiv:2508.12947v1 Announce Type: cross 
Abstract: Originally introduced in cooperative game theory, Shapley values have become a very popular tool to explain machine learning predictions. Based on Shapley's fairness axioms, every input (feature component) gets a credit how it contributes to an output (prediction). These credits are then used to explain the prediction. The only limitation in computing the Shapley values (credits) for many different predictions is of computational nature. There are two popular sampling approximations, sampling KernelSHAP and sampling PermutationSHAP. Our first novel contributions are asymptotic normality results for these sampling approximations. Next, we show that the paired-sampling approaches provide exact results in case of interactions being of maximal order two. Furthermore, the paired-sampling PermutationSHAP possesses the additive recovery property, whereas its kernel counterpart does not.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Analysis in Frequency Domain: A Survey of Open Challenges, Opportunities and Benchmarks</title>
<link>https://arxiv.org/abs/2504.07099</link>
<guid>https://arxiv.org/abs/2504.07099</guid>
<content:encoded><![CDATA[
<div> Keywords: Frequency-domain analysis, Spectral methods, Causal structure preservation, Uncertainty quantification, Geometric deep learning<br />
Summary: 
Frequency-domain analysis is a powerful paradigm for time series analysis, offering advantages over traditional approaches. This survey covers classical Fourier analysis to modern neural operators, highlighting three key challenges: preserving causal structure during spectral transformations, quantifying uncertainty in learned frequency representations, and analyzing non-Euclidean data structures with topology awareness. Over 100 studies were reviewed to develop a unified taxonomy bridging conventional spectral techniques with machine learning approaches. The survey identifies knowledge gaps in geometric deep learning and quantum-enhanced spectral analysis. It provides a systematic framework for method selection and implementation and charts promising directions for future research in this rapidly evolving domain. <div>
arXiv:2504.07099v3 Announce Type: replace 
Abstract: Frequency-domain analysis has emerged as a powerful paradigm for time series analysis, offering unique advantages over traditional time-domain approaches while introducing new theoretical and practical challenges. This survey provides a comprehensive examination of spectral methods from classical Fourier analysis to modern neural operators, systematically summarizing three open challenges in current research: (1) causal structure preservation during spectral transformations, (2) uncertainty quantification in learned frequency representations, and (3) topology-aware analysis for non-Euclidean data structures. Through rigorous reviewing of over 100 studies, we develop a unified taxonomy that bridges conventional spectral techniques with cutting-edge machine learning approaches, while establishing standardized benchmarks for performance evaluation. Our work identifies key knowledge gaps in the field, particularly in geometric deep learning and quantum-enhanced spectral analysis. The survey offers practitioners a systematic framework for method selection and implementation, while charting promising directions for future research in this rapidly evolving domain.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantformer: from attention to profit with a quantitative transformer trading strategy</title>
<link>https://arxiv.org/abs/2404.00424</link>
<guid>https://arxiv.org/abs/2404.00424</guid>
<content:encoded><![CDATA[
<div> Transformer, Quantformer, investment factors, sentiment analysis, quantitative trading<br />
<br />
Summary:<br />
Quantitative trading faces challenges in capturing market variables for profit. Quantformer, a neural network based on transformer, uses transfer learning from sentiment analysis to build investment factors. It excels in modeling complex data relationships and forecasting stock trends accurately. With data from 2010 to 2023, Quantformer outperforms other quantitative strategies in predicting stock trends. Its innovative use of transformer-like models combined with market sentiment information enhances trading signal accuracy, promising advancements in quantitative trading strategies. <div>
arXiv:2404.00424v3 Announce Type: replace-cross 
Abstract: In traditional quantitative trading practice, navigating the complicated and dynamic financial market presents a persistent challenge. Fully capturing various market variables, including long-term information, as well as essential signals that may lead to profit remains a difficult task for learning algorithms. In order to tackle this challenge, this paper introduces quantformer, an enhanced neural network architecture based on transformer, to build investment factors. By transfer learning from sentiment analysis, quantformer not only exploits its original inherent advantages in capturing long-range dependencies and modeling complex data relationships, but is also able to solve tasks with numerical inputs and accurately forecast future returns over a given period. This work collects more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market from 2010 to 2023. The results of this study demonstrate the model's superior performance in predicting stock trends compared with other 100-factor-based quantitative strategies. Notably, the model's innovative use of transformer-like model to establish factors, in conjunction with market sentiment information, has been shown to enhance the accuracy of trading signals significantly, thereby offering promising implications for the future of quantitative trading strategies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News</title>
<link>https://arxiv.org/abs/2508.10927</link>
<guid>https://arxiv.org/abs/2508.10927</guid>
<content:encoded><![CDATA[
<div> Keywords: company risk factors, news articles, machine learning models, supply chain, regulations<br />
Summary:<br />
- Importance of identifying company risk factors in financial market for investors and overall well-being.
- Computational framework developed to automatically extract risk factors from news articles.
- Schema comprising seven aspects such as supply chain, regulations, competitions.
- Experiment shows fine-tuned pre-trained language models performing better in identifying risk factors compared to zero-shot and few-shot prompting LLMs.
- Analysis of over 277K Bloomberg news articles demonstrates insight into company and industry operations through identification of risk factors.
<br /><br />Summary: Identifying company risk factors is crucial for investors and financial market stability. A computational framework was created to extract these factors from news articles, focusing on aspects like supply chain and regulations. Fine-tuned language models outperformed zero-shot and few-shot models in identifying risk factors. Analysis of Bloomberg news articles revealed valuable insights into company and industry operations through this approach. <div>
arXiv:2508.10927v1 Announce Type: cross 
Abstract: Identifying risks associated with a company is important to investors and the well-being of the overall financial market. In this study, we build a computational framework to automatically extract company risk factors from news articles. Our newly proposed schema comprises seven distinct aspects, such as supply chain, regulations, and competitions. We sample and annotate 744 news articles and benchmark various machine learning models. While large language models have achieved huge progress in various types of NLP tasks, our experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs (e.g. LLaMA-2) can only achieve moderate to low performances in identifying risk factors. And fine-tuned pre-trained language models are performing better on most of the risk factors. Using this model, we analyze over 277K Bloomberg news articles and demonstrate that identifying risk factors from news could provide extensive insight into the operations of companies and industries.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressive Meta-Learning</title>
<link>https://arxiv.org/abs/2508.11090</link>
<guid>https://arxiv.org/abs/2508.11090</guid>
<content:encoded><![CDATA[
<div> Keywords: compressive learning, parameter-learning, neural networks, meta-learning, data structure<br />
<br />
Summary: 
The article discusses the need for fast and efficient parameter-learning techniques due to the exponential growth of new datasets. Compressive learning is introduced as a framework that utilizes random, non-linear features to project large-scale databases onto compact representations, enabling efficient processing without access to the original samples. However, current compressive learning methods lack data structure exploitation. The proposed Compressive Meta-Learning framework meta-learns both encoding and decoding stages using neural networks, offering faster and more accurate systems. Various applications of the framework, such as compressive PCA, compressive ridge regression, and compressive k-means, are explored. This approach shows promise for improving the efficiency and privacy-friendliness of parameter learning in the face of growing dataset sizes. <br /><br />Summary: <div>
arXiv:2508.11090v1 Announce Type: cross 
Abstract: The rapid expansion in the size of new datasets has created a need for fast and efficient parameter-learning techniques. Compressive learning is a framework that enables efficient processing by using random, non-linear features to project large-scale databases onto compact, information-preserving representations whose dimensionality is independent of the number of samples and can be easily stored, transferred, and processed. These database-level summaries are then used to decode parameters of interest from the underlying data distribution without requiring access to the original samples, offering an efficient and privacy-friendly learning framework. However, both the encoding and decoding techniques are typically randomized and data-independent, failing to exploit the underlying structure of the data. In this work, we propose a framework that meta-learns both the encoding and decoding stages of compressive learning methods by using neural networks that provide faster and more accurate systems than the current state-of-the-art approaches. To demonstrate the potential of the presented Compressive Meta-Learning framework, we explore multiple applications -- including neural network-based compressive PCA, compressive ridge regression, compressive k-means, and autoencoders.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Banking 2.0: The Stablecoin Banking Revolution -- How Digital Assets Are Reshaping Global Finance</title>
<link>https://arxiv.org/abs/2508.11395</link>
<guid>https://arxiv.org/abs/2508.11395</guid>
<content:encoded><![CDATA[
<div> inflection point, stablecoins, "Banking 2.0", institutional adoption, macroeconomic imbalances <br />
<br />
Summary: The article discusses how stablecoins are revolutionizing the global financial system by seamlessly integrating cryptocurrency innovation with traditional banking infrastructure. It highlights the significance of stablecoins in addressing vulnerabilities in modern fiat currencies and tackling macroeconomic imbalances such as the inflation-productivity gap. The increasing institutional adoption of stablecoins, as evidenced by U.S. legislation and initiatives from major industry players like JPMorgan and PayPal, underscores their transformative potential. Stablecoins offer enhanced stability, reduced fraud risk, and facilitate unified global transactions that transcend national boundaries. By providing more robust and diversified backing mechanisms, stablecoins pave the way for a more interconnected international financial system while enabling deregulation and efficiency gains. The article provides real-world examples and current market data to support the argument that stablecoins are poised to reshape banking as we know it. <div>
arXiv:2508.11395v1 Announce Type: cross 
Abstract: The global financial system stands at an inflection point. Stablecoins represent the most significant evolution in banking since the abandonment of the gold standard, positioned to enable "Banking 2.0" by seamlessly integrating cryptocurrency innovation with traditional finance infrastructure. This transformation rivals artificial intelligence as the next major disruptor in the financial sector. Modern fiat currencies derive value entirely from institutional trust rather than physical backing, creating vulnerabilities that stablecoins address through enhanced stability, reduced fraud risk, and unified global transactions that transcend national boundaries. Recent developments demonstrate accelerating institutional adoption: landmark U.S. legislation including the GENIUS Act of 2025, strategic industry pivots from major players like JPMorgan's crypto-backed loan initiatives, and PayPal's comprehensive "Pay with Crypto" service. Widespread stablecoin implementation addresses critical macroeconomic imbalances, particularly the inflation-productivity gap plaguing modern monetary systems, through more robust and diversified backing mechanisms. Furthermore, stablecoins facilitate deregulation and efficiency gains, paving the way for a more interconnected international financial system. This whitepaper comprehensively explores how stablecoins are poised to reshape banking, supported by real-world examples, current market data, and analysis of their transformative potential.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models</title>
<link>https://arxiv.org/abs/2508.11542</link>
<guid>https://arxiv.org/abs/2508.11542</guid>
<content:encoded><![CDATA[
<div> approach, Operator Inference, reduced-order models, dynamic systems, snapshot data  
Summary:  
This paper presents a data-driven, nested Operator Inference (OpInf) approach for learning physics-informed reduced-order models (ROMs) from snapshot data of high-dimensional dynamical systems. The approach utilizes a hierarchy within the reduced space to iteratively construct initial guesses prioritizing interactions of dominant modes. The initial guess for any target reduced dimension yields a ROM with smaller or equal snapshot reconstruction error compared to standard OpInf. The nested OpInf algorithm supports warm-starting from previous models, allowing for dynamic basis and model form updates. Demonstrations on a cubic heat conduction problem showed nested OpInf achieved significantly smaller errors than standard OpInf with comparable offline time. Application to a large-scale Greenland ice sheet model produced a ROM with an average error of 3% and a computational speed-up factor exceeding 19,000. <div>
arXiv:2508.11542v1 Announce Type: cross 
Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach for learning physics-informed reduced-order models (ROMs) from snapshot data of high-dimensional dynamical systems. The approach exploits the inherent hierarchy within the reduced space to iteratively construct initial guesses for the OpInf learning problem that prioritize the interactions of the dominant modes. The initial guess computed for any target reduced dimension corresponds to a ROM with provably smaller or equal snapshot reconstruction error than with standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from previously learned models, enabling versatile application scenarios involving dynamic basis and model form updates. We demonstrate the performance of our algorithm on a cubic heat conduction problem, with nested OpInf achieving a four times smaller error than standard OpInf at a comparable offline time. Further, we apply nested OpInf to a large-scale, parameterized model of the Greenland ice sheet where, despite model form approximation errors, it learns a ROM with, on average, 3% error and computational speed-up factor above 19,000.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarAT: Carbon Atom Tracing across Industrial Chemical Value Chains via Chemistry Language Models</title>
<link>https://arxiv.org/abs/2508.10216</link>
<guid>https://arxiv.org/abs/2508.10216</guid>
<content:encoded><![CDATA[
<div> Carbon Atom Tracker, biogenic carbon content, sustainability reporting, industrial value chains, sustainability.

Summary:
The chemical industry is focusing on sustainability and reducing carbon footprints. The Together for Sustainability consortium will soon require reporting of biogenic carbon content (BCC) in chemical products. Carbon-14 is impractical for continuous monitoring, so a new automated methodology called CarAT has been developed. CarAT uses Enterprise Resource Planning data to calculate BCC across industrial value chains by mapping carbon atoms in chemical reactions and applying a linear program. The methodology was validated on a toluene diisocyanate value chain with different scenarios. Results were visualized with Sankey diagrams, showing the flow of carbon attributes. CarAT enables real-time BCC calculation, supports compliance with reporting mandates, and facilitates the transition to sustainable manufacturing by tracking carbon sources transparently and empowering data-driven decisions. <br /><br />Summary: <div>
arXiv:2508.10216v1 Announce Type: new 
Abstract: The chemical industry is increasingly prioritising sustainability, with a focus on reducing carbon footprints to achieve net zero. By 2026, the Together for Sustainability (TfS) consortium will require reporting of biogenic carbon content (BCC) in chemical products, posing a challenge as BCC depends on feedstocks, value chain configuration, and process-specific variables. While carbon-14 isotope analysis can measure BCC, it is impractical for continuous industrial monitoring. This work presents CarAT (Carbon Atom Tracker), an automated methodology for calculating BCC across industrial value chains, enabling dynamic and accurate sustainability reporting. The approach leverages existing Enterprise Resource Planning data in three stages: (1) preparing value chain data, (2) performing atom mapping in chemical reactions using chemistry language models, and (3) applying a linear program to calculate BCC given known inlet compositions. The methodology is validated on a 27-node industrial toluene diisocyanate value chain. Three scenarios are analysed: a base case with fossil feedstocks, a case incorporating a renewable feedstock, and a butanediol value chain with a recycle stream. Results are visualised with Sankey diagrams showing the flow of carbon attributes across the value chain. The key contribution is a scalable, automated method for real-time BCC calculation under changing industrial conditions. CarAT supports compliance with upcoming reporting mandates and advances carbon neutrality goals by enabling systematic fossil-to-biogenic substitution. Through transparent, auditable tracking of carbon sources in production networks, it empowers data-driven decisions to accelerate the transition to sustainable manufacturing.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOBACO: Topology Optimization via Band-limited Coordinate Networks for Compositionally Graded Alloys</title>
<link>https://arxiv.org/abs/2508.10320</link>
<guid>https://arxiv.org/abs/2508.10320</guid>
<content:encoded><![CDATA[
<div> Keywords: Compositionally Graded Alloys, Additive Manufacturing, Topology Optimization, Coordinate Neural Network, Spatial Gradation<br />
Summary: <br />
Compositionally Graded Alloys (CGAs) offer design flexibility through spatial composition variations for stronger and lighter components. Advances in additive manufacturing (AM) have made CGA fabrication feasible, but manufacturing constraints on spatial gradation exist. This paper presents a topology optimization (TO) framework for optimized CGA designs with controlled compositional gradation. A band-limited coordinate neural network represents the composition distribution, ensuring compliance with gradation limits without explicit constraints. The approach benefits from TO advantages like mesh independence and high-resolution design extraction. Demonstrations in elastic and thermo-elastic TO examples showcase the framework's effectiveness. <div>
arXiv:2508.10320v1 Announce Type: new 
Abstract: Compositionally Graded Alloys (CGAs) offer unprecedented design flexibility by enabling spatial variations in composition; tailoring material properties to local loading conditions. This flexibility leads to components that are stronger, lighter, and more cost-effective than traditional monolithic counterparts. The fabrication of CGAs have become increasingly feasible owing to recent advancements in additive manufacturing (AM), particularly in multi-material printing and improved precision in material deposition. However, AM of CGAs requires imposition of manufacturing constraints; in particular limits on the maximum spatial gradation of composition.
  This paper introduces a topology optimization (TO) based framework for designing optimized CGA components with controlled compositional gradation. In particular, we represent the constrained composition distribution using a band-limited coordinate neural network. By regulating the network's bandwidth, we ensure implicit compliance with gradation limits, eliminating the need for explicit constraints. The proposed approach also benefits from the inherent advantages of TO using coordinate networks, including mesh independence, high-resolution design extraction, and end-to-end differentiability. The effectiveness of our framework is demonstrated through various elastic and thermo-elastic TO examples.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chem3DLLM: 3D Multimodal Large Language Models for Chemistry</title>
<link>https://arxiv.org/abs/2508.10696</link>
<guid>https://arxiv.org/abs/2508.10696</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D molecular structures, Chem3DLLM, multimodal, protein-conditioned, drug discovery

Summary:<br /><br />Chem3DLLM is introduced as a unified protein-conditioned multimodal large language model to generate 3D molecular structures. It addresses challenges faced by autoregressive-based language models in handling 3D molecular conformation. The model utilizes a reversible text encoding technique for 3D molecular structures that enables integration with protein pocket features. Reinforcement learning with stability-based rewards is employed to optimize chemical validity, and a lightweight protein embedding projector is incorporated for end-to-end training. Experimental results demonstrate state-of-the-art performance in structure-based drug design with a Vina score of -7.21, showcasing the efficacy of the unified multimodal approach for practical drug discovery applications. <div>
arXiv:2508.10696v1 Announce Type: new 
Abstract: In the real world, a molecule is a 3D geometric structure. Compared to 1D SMILES sequences and 2D molecular graphs, 3D molecules represent the most informative molecular modality. Despite the rapid progress of autoregressive-based language models, they cannot handle the generation of 3D molecular conformation due to several challenges: 1) 3D molecular structures are incompatible with LLMs' discrete token space, 2) integrating heterogeneous inputs like proteins, ligands, and text remains difficult within a unified model, and 3) LLMs lack essential scientific priors, hindering the enforcement of physical and chemical constraints during generation. To tackle these issues, we present Chem3DLLM, a unified protein-conditioned multimodal large language model. Our approach designs a novel reversible text encoding for 3D molecular structures using run-length compression, achieving 3x size reduction while preserving complete structural information. This enables seamless integration of molecular geometry with protein pocket features in a single LLM architecture. We employ reinforcement learning with stability-based rewards to optimize chemical validity and incorporate a lightweight protein embedding projector for end-to-end training. Experimental results on structure-based drug design demonstrate state-of-the-art performance with a Vina score of -7.21, validating our unified multimodal approach for practical drug discovery applications.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model</title>
<link>https://arxiv.org/abs/2508.10492</link>
<guid>https://arxiv.org/abs/2508.10492</guid>
<content:encoded><![CDATA[
<div> AI, clinical diagnosis, full-process, DxDirector-7B, deep thinking<br />
Summary: In the article, the authors propose a paradigm shift in which AI, specifically the DxDirector-7B large language model, takes on the primary role in driving the full-process clinical diagnosis, with human physicians as assistants. This model is equipped with advanced deep thinking capabilities and establishes an accountability framework for misdiagnoses. DxDirector-7B outperforms existing medical and general-purpose language models in accuracy and significantly reduces physician workload. It is evaluated across rare, complex, and real-world cases, showing potential to serve as a viable substitute for medical specialists. The shift towards AI driving the diagnostic process marks a new era in healthcare, with the potential to enhance diagnostic efficiency and reduce physicians' workload. <div>
arXiv:2508.10492v1 Announce Type: cross 
Abstract: Full-process clinical diagnosis in the real world encompasses the entire diagnostic workflow that begins with only an ambiguous chief complaint. While artificial intelligence (AI), particularly large language models (LLMs), is transforming clinical diagnosis, its role remains largely as an assistant to physicians. This AI-assisted working pattern makes AI can only answer specific medical questions at certain parts within the diagnostic process, but lack the ability to drive the entire diagnostic process starting from an ambiguous complaint, which still relies heavily on human physicians. This gap limits AI's ability to fully reduce physicians' workload and enhance diagnostic efficiency. To address this, we propose a paradigm shift that reverses the relationship between physicians and AI: repositioning AI as the primary director, with physicians serving as its assistants. So we present DxDirector-7B, an LLM endowed with advanced deep thinking capabilities, enabling it to drive the full-process diagnosis with minimal physician involvement. Furthermore, DxDirector-7B establishes a robust accountability framework for misdiagnoses, delineating responsibility between AI and human physicians. In evaluations across rare, complex, and real-world cases under full-process diagnosis setting, DxDirector-7B not only achieves significant superior diagnostic accuracy but also substantially reduces physician workload than state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained analyses across multiple clinical departments and tasks validate its efficacy, with expert evaluations indicating its potential to serve as a viable substitute for medical specialists. These findings mark a new era where AI, traditionally a physicians' assistant, now drives the entire diagnostic process to drastically reduce physicians' workload, indicating an efficient and accurate diagnostic solution.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Sensing for Solder Layer Degradation and Temperature Monitoring in IGBT Modules</title>
<link>https://arxiv.org/abs/2508.10515</link>
<guid>https://arxiv.org/abs/2508.10515</guid>
<content:encoded><![CDATA[
<div> solder degradation, IGBT modules, machine learning, virtual sensing, temperature estimation
Summary:
Machine learning-based virtual sensing is explored in this study to monitor solder degradation in IGBT modules, crucial for power electronic system reliability. With limited physical sensors, accurate estimation of degraded solder area (1.17% error) and surface temperature (max 4.56% relative error) is achieved. This approach offers a promising alternative to direct measurement of internal component degradation indicators, overcoming physical inaccessibility challenges in harsh environments. <div>
arXiv:2508.10515v1 Announce Type: cross 
Abstract: Monitoring the degradation state of Insulated Gate Bipolar Transistor (IGBT) modules is essential for ensuring the reliability and longevity of power electronic systems, especially in safety-critical and high-performance applications. However, direct measurement of key degradation indicators - such as junction temperature, solder fatigue or delamination - remains challenging due to the physical inaccessibility of internal components and the harsh environment. In this context, machine learning-based virtual sensing offers a promising alternative by bridging the gap from feasible sensor placement to the relevant but inaccessible locations. This paper explores the feasibility of estimating the degradation state of solder layers, and the corresponding full temperature maps based on a limited number of physical sensors. Based on synthetic data of a specific degradation mode, we obtain a high accuracy in the estimation of the degraded solder area (1.17% mean absolute error), and are able to reproduce the surface temperature of the IGBT with a maximum relative error of 4.56% (corresponding to an average relative error of 0.37%).
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Deep Contrast Source Inversion: A Unified Framework for Inverse Scattering Problems</title>
<link>https://arxiv.org/abs/2508.10555</link>
<guid>https://arxiv.org/abs/2508.10555</guid>
<content:encoded><![CDATA[
<div> inverse scattering problems, electromagnetic imaging, medical diagnostics, deep learning, contrast source inversion

Summary:
This paper proposes a physics-informed deep contrast source inversion framework (DeepCSI) for accurate medium reconstruction in inverse scattering problems. The approach utilizes a residual multilayer perceptron (ResMLP) to model current distributions under different transmitter excitations, linearizing the problem and reducing computational costs. By treating medium parameters as learnable tensors and employing a hybrid loss function, DeepCSI enables joint optimization of network parameters and medium properties. The framework is capable of handling diverse measurement scenarios, including phase-less and multi-frequency observation, offering simplicity and universal modeling capabilities compared to traditional methods. Simulations and experiments show that DeepCSI outperforms conventional contrast source inversion methods, providing high-precision and robust reconstruction in complex inverse scattering problems. <div>
arXiv:2508.10555v1 Announce Type: cross 
Abstract: Inverse scattering problems are critical in electromagnetic imaging and medical diagnostics but are challenged by their nonlinearity and diverse measurement scenarios. This paper proposes a physics-informed deep contrast source inversion framework (DeepCSI) for fast and accurate medium reconstruction across various measurement conditions. Inspired by contrast source inversion (CSI) and neural operator methods, a residual multilayer perceptron (ResMLP) is employed to model current distributions in the region of interest under different transmitter excitations, effectively linearizing the nonlinear inverse scattering problem and significantly reducing the computational cost of traditional full-waveform inversion. By modeling medium parameters as learnable tensors and utilizing a hybrid loss function that integrates state equation loss, data equation loss, and total variation regularization, DeepCSI establishes a fully differentiable framework for joint optimization of network parameters and medium properties. Compared with conventional methods, DeepCSI offers advantages in terms of simplicity and universal modeling capabilities for diverse measurement scenarios, including phase-less and multi-frequency observation. Simulations and experiments demonstrate that DeepCSI achieves high-precision, robust reconstruction under full-data, phaseless data, and multifrequency conditions, outperforming traditional CSI methods and providing an efficient and universal solution for complex inverse scattering problems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impacts of DEM Type and Resolution on Deep Learning-Based Flood Inundation Mapping</title>
<link>https://arxiv.org/abs/2309.13360</link>
<guid>https://arxiv.org/abs/2309.13360</guid>
<content:encoded><![CDATA[
<div> Keywords: flood mapping, deep learning, DEM resolution, flood prediction accuracy, data-scarce regions 

Summary:
- The study examines how DEM type and resolution impact flood prediction accuracy using a deep learning method.
- Synthetic hydrographs are used as training input with water depth data from a hydrodynamic model as target data.
- DSMs and DTMs derived from a 1 m LIDAR-based DTM were compared at resolutions from 15 to 30 m in the city of Carlisle, UK.
- Using a 30 m DTM outperformed a 30 m DSM by 21% in flood depth prediction accuracy during peak stages.
- Increasing DTM resolution to 15 m resulted in a minimum 50% increase in RMSE and a 20% increase in fit index across all flood stages.
- Coarser resolution DEMs may impact accuracy, but even a slight improvement in data resolution in data-scarce regions can enhance flood risk management.

<br /><br />Summary: <div>
arXiv:2309.13360v4 Announce Type: replace 
Abstract: The increasing availability of hydrological and physiographic spatiotemporal data has boosted machine learning's role in rapid flood mapping. Yet, data scarcity, especially high-resolution DEMs, challenges regions with limited access. This paper examines how DEM type and resolution affect flood prediction accuracy, utilizing a cutting-edge deep learning (DL) method called 1D convolutional neural network (CNN). It utilizes synthetic hydrographs as training input and water depth data obtained from LISFLOOD-FP, a 2D hydrodynamic model, as target data. This study investigates digital surface models (DSMs) and digital terrain models (DTMs) derived from a 1 m LIDAR-based DTM, with resolutions from 15 to 30 m. The methodology is applied and assessed in an established benchmark, the city of Carlisle, UK. The models' performance is then evaluated and compared against an observed flood event using RMSE, Bias, and Fit indices. Leveraging the insights gained from this region, the paper discusses the applicability of the methodology to address the challenges encountered in a data-scarce flood-prone region, exemplified by Pakistan. Results indicated that utilizing a 30 m DTM outperformed a 30 m DSM in terms of flood depth prediction accuracy by about 21% during the flood peak stage, highlighting the superior performance of DTM at lower resolutions. Increasing the resolution of DTM to 15 m resulted in a minimum 50% increase in RMSE and a 20% increase in fit index across all flood stages. The findings emphasize that while a coarser resolution DEM may impact the accuracy of machine learning models, it remains a viable option for rapid flood prediction. However, even a slight improvement in data resolution in data-scarce regions would provide significant added value, ultimately enhancing flood risk management.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations</title>
<link>https://arxiv.org/abs/2501.02409</link>
<guid>https://arxiv.org/abs/2501.02409</guid>
<content:encoded><![CDATA[
<div> neural ODEs, gene regulatory network, perturbations, trajectory prediction, cell state<br />
Summary:<br />
The article focuses on developing PerturbODE, a framework that uses neural ordinary differential equations (ODEs) to model cell state trajectories and infer gene regulatory networks (GRNs) from large-scale perturbation datasets. By incorporating biologically informative neural ODEs, PerturbODE addresses the limitations of existing GRN inference models in terms of expressivity and scalability. It aims to capture causal gene regulatory relationships and account for the dynamic nature of biological processes such as cellular differentiation. The efficacy of PerturbODE is demonstrated through trajectory prediction and GRN inference on simulated and real over-expression datasets. The framework shows promising results in accurately predicting cell state trajectories under perturbations and deriving causal GRNs. <div>
arXiv:2501.02409v3 Announce Type: replace-cross 
Abstract: Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Topology Optimisation of Time-dependent Thermal Conduction Using Space-Time Finite Elements and a Parallel Space-Time Multigrid Preconditioner</title>
<link>https://arxiv.org/abs/2508.09589</link>
<guid>https://arxiv.org/abs/2508.09589</guid>
<content:encoded><![CDATA[
<div> Keywords: space-time topology optimisation, thermal conduction, finite element method, parallel computing, scalability

Summary:
This paper introduces a novel space-time topology optimization framework for time-dependent thermal conduction problems. Time is treated as an additional spatial dimension, and the governing equations are discretized using a stabilised continuous Galerkin space-time finite element method. A parallel-in-time method is implemented, demonstrating excellent scalability on a distributed-memory supercomputer. The framework offers up to 52x speed-up compared to traditional time-stepping approaches, with only moderate increases in total computational cost. Validation on benchmark problems with varying designs and material properties show the flexibility of the method. The proposed space-time method proves to be a promising approach for large-scale time-dependent topology optimization in thermal applications. 

<br /><br />Summary: <div>
arXiv:2508.09589v1 Announce Type: new 
Abstract: This paper presents a novel space-time topology optimisation framework for time-dependent thermal conduction problems, aiming to significantly reduce the time-to-solution. By treating time as an additional spatial dimension, we discretise the governing equations using a stabilised continuous Galerkin space-time finite element method. The resulting large all-at-once system is solved using an iterative Krylov solver preconditioned with a parallel space-time multigrid method employing a semi-coarsening strategy. Implemented in a fully parallel computing framework, the method yields a parallel-in-time method that demonstrates excellent scalability on a distributed-memory supercomputer, solving problems up to 4.2 billion degrees of freedom. Comparative studies show up to 52x speed-up over traditional time-stepping approaches, with only moderate increases in total computational cost in terms of core-hours. The framework is validated on benchmark problems with both time-constant and time-varying designs, and its flexibility is demonstrated through variations in material properties. These results establish the proposed space-time method as a promising approach for large-scale time-dependent topology optimisation in thermal applications.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisFinEval: A Scenario-Driven Chinese Multimodal Benchmark for Holistic Financial Understanding</title>
<link>https://arxiv.org/abs/2508.09641</link>
<guid>https://arxiv.org/abs/2508.09641</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, financial analysis, VisFinEval, Chinese benchmark, error analysis

Summary: 
Multimodal large language models (MLLMs) are promising for automating complex financial analysis. VisFinEval is a large-scale Chinese benchmark that covers various financial tasks using different image modalities. The benchmark includes 15,848 annotated question-answer pairs organized into three financial scenario depths. 21 state-of-the-art MLLMs were evaluated in a zero-shot setting, with the top model achieving 76.3% overall accuracy. However, it still trails behind financial experts. An error analysis revealed six recurring failure modes, indicating areas for future research such as cross-modal misalignment and lapses in business-process reasoning. VisFinEval aims to advance the development of domain-tailored MLLMs capable of integrating textual and visual financial information seamlessly.

<br /><br />Summary: <div>
arXiv:2508.09641v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) hold great promise for automating complex financial analysis. To comprehensively evaluate their capabilities, we introduce VisFinEval, the first large-scale Chinese benchmark that spans the full front-middle-back office lifecycle of financial tasks. VisFinEval comprises 15,848 annotated question-answer pairs drawn from eight common financial image modalities (e.g., K-line charts, financial statements, official seals), organized into three hierarchical scenario depths: Financial Knowledge & Data Analysis, Financial Analysis & Decision Support, and Financial Risk Control & Asset Optimization. We evaluate 21 state-of-the-art MLLMs in a zero-shot setting. The top model, Qwen-VL-max, achieves an overall accuracy of 76.3%, outperforming non-expert humans but trailing financial experts by over 14 percentage points. Our error analysis uncovers six recurring failure modes-including cross-modal misalignment, hallucinations, and lapses in business-process reasoning-that highlight critical avenues for future research. VisFinEval aims to accelerate the development of robust, domain-tailored MLLMs capable of seamlessly integrating textual and visual financial information. The data and the code are available at https://github.com/SUFE-AIFLM-Lab/VisFinEval.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetuning Large Language Model as an Effective Symbolic Regressor</title>
<link>https://arxiv.org/abs/2508.09897</link>
<guid>https://arxiv.org/abs/2508.09897</guid>
<content:encoded><![CDATA[
<div> Keywords: Symbolic Regression, Large Language Models, Fine-tuning, SymbArena, SymbolicChat 

Summary: 
The article introduces the concept of Symbolic Regression (SR) and highlights the limitations of current Large Language Models (LLMs) in solving SR tasks efficiently. To address this, the authors propose fine-tuning LLMs for enhanced SR capability. However, the lack of dedicated datasets for SR-focused fine-tuning poses a challenge. To overcome this, the authors introduce SymbArena, a benchmark comprising a diverse set of equations for LLM training. SymbArena also introduces a new metric for evaluating form-level consistency in SR tasks. Through experiments, the authors demonstrate the effectiveness of SymbolicChat, a new LLM-based SR model that outperforms traditional numerical methods in both numerical precision and symbolic form accuracy. SymbolicChat achieves significant improvements in R2 score and form-level consistency score compared to other LLM baselines. This research paves the way for utilizing LLMs in SR tasks more effectively. 

<br /><br />Summary: <div>
arXiv:2508.09897v1 Announce Type: new 
Abstract: Deriving governing equations from observational data, known as Symbolic Regression (SR), is a cornerstone of scientific discovery. Large Language Models (LLMs) have shown promise in this task by leveraging their vast cross-disciplinary scientific knowledge. However, existing LLM-based methods primarily rely on direct inference or prompt engineering, often requiring excessive inference iterations to converge on correct formulas or failing to treating complex equation targets. These limitations in effectiveness and generalization stem from an inherent tension between pre-trained LLMs' proficiency in approximate reasoning and the high-precision demands of SR tasks. To bridge this gap, we propose to fine-tune LLMs for enhanced SR capability. Yet, the absence of dedicated datasets for SR-oriented fine-tuning remains a critical barrier. We thus introduce SymbArena, specifically engineered to optimize LLMs for SR. This benchmark comprises 148,102 diverse equations formulated as corpora of 1.83 billion tokens for LLM utilization, enabling effective training and inference. Further, SymbArena proposes a heuristics metric to precisely quantify form-level consistency, going beyond existing SR numerical-oriented evaluation strategies. With this benchmark, we explore mainstream LLM fine-tuning techniques for SR tasks and establish SymbolicChat, a simple yet effective LLM-based SR strong baseline. Experimental results validate SymbolicChat as the first LLM to exceed traditional numerical methods in both numerical precision and symbolic form accuracy, outperforming the second-best LLM baseline with improvements of 2-fold gains in R2 score and 8.37% in form-level consistency score.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction</title>
<link>https://arxiv.org/abs/2508.09227</link>
<guid>https://arxiv.org/abs/2508.09227</guid>
<content:encoded><![CDATA[
<div> Graph Attention Network, Recurrent Neural Network, Trajectory Prediction, Intelligent Transportation Systems, Task Corrector

Summary:
The proposed GSMT model integrates a Graph Attention Network (GAT) with a Recurrent Neural Network (RNN) for accurate trajectory prediction of buses in urban environments. A task corrector refines predictions by clustering historical trajectories and identifying distinct motion patterns. GSMT fuses dynamic bus and static station data through embedded networks for prediction and utilizes the corrector for further refinement. The approach allows for multi-node trajectory prediction in dense urban traffic conditions. Experimental results on a Kuala Lumpur dataset show superior performance compared to existing methods in both short-term and long-term prediction tasks. The GSMT model offers enhanced accuracy and efficiency in trajectory prediction for buses, especially in regions with limited multimodal data access.<br /><br />Summary: <div>
arXiv:2508.09227v1 Announce Type: cross 
Abstract: Accurate trajectory prediction for buses is crucial in intelligent transportation systems, particularly within urban environments. In developing regions where access to multimodal data is limited, relying solely on onboard GPS data remains indispensable despite inherent challenges. To address this problem, we propose GSMT, a hybrid model that integrates a Graph Attention Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and incorporates a task corrector capable of extracting complex behavioral patterns from large-scale trajectory data. The task corrector clusters historical trajectories to identify distinct motion patterns and fine-tunes the predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus information and static station information through embedded hybrid networks to perform trajectory prediction, and applies the task corrector for secondary refinement after the initial predictions are generated. This two-stage approach enables multi-node trajectory prediction among buses operating in dense urban traffic environments under complex conditions. Experiments conducted on a real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method significantly outperforms existing approaches, achieving superior performance in both short-term and long-term trajectory prediction tasks.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</title>
<link>https://arxiv.org/abs/2508.09811</link>
<guid>https://arxiv.org/abs/2508.09811</guid>
<content:encoded><![CDATA[
<div> 3D scene geometry, appearance, physical information, dynamic multi-view videos, physics-informed losses

Summary:
The paper introduces a new framework named TRACE for modeling complex dynamic 3D scenes solely from multi-view videos without human labels. By treating each 3D point as a rigid particle with size and orientation, the method learns a translation rotation dynamics system for individual particles, accurately estimating physical parameters governing their motion. This approach enables the modeling of complex motion physics without requiring additional labels. Extensive experiments on various datasets demonstrate superior performance in future frame extrapolation compared to existing methods. Additionally, the framework allows for easy segmentation of multiple objects or parts by clustering the learned physical parameters. The method showcases remarkable capabilities in capturing 3D scene geometry, appearance, and physical information from dynamic multi-view videos, showcasing its potential for various applications in computer vision and scene understanding. 

<br /><br />Summary: <div>
arXiv:2508.09811v1 Announce Type: cross 
Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural nets, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. We propose a new framework named TRACE to model the motion physics of complex dynamic 3D scenes. The key novelty of our method is that, by formulating each 3D point as a rigid particle with size and orientation in space, we directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters.
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Volume Element: Existence and Extent in Cracked Heterogeneous Medium</title>
<link>https://arxiv.org/abs/2508.08320</link>
<guid>https://arxiv.org/abs/2508.08320</guid>
<content:encoded><![CDATA[
<div> microscale failure, composites, multiscale modelling, representative volume element, periodic boundary conditions

Summary:
This paper addresses the growing demand for composites in engineering by focusing on microscale failure and improving composites through multiscale modelling. It aims to enhance the representativeness of volume elements by reducing mesh and size sensitivities in representative volume element (RVE) modelling. A technique is proposed to equalize fracture energy in computational analysis with real phenomena to mitigate mesh sensitivity. Modified periodic boundary conditions (MPBCs) are introduced to reduce size dependency in RVE modelling, validated through analysis of 1200 RVE samples under transverse loading. The study also examines factors influencing damage initiation in 2D composite RVEs, finding that the arrangement of closely spaced fibers can promote damage in the region between them. <div>
arXiv:2508.08320v1 Announce Type: new 
Abstract: Acknowledging the ever-increasing demand for composites in the engineering industry, this paper focuses on the failure of composites at the microscale and augmenting the use of multiscale modelling techniques to make them better for various applications. This work aims to increase the representativeness of the volume element by attenuating the mesh and size sensitivities in representative volume element (RVE) modelling. A technique to alleviate mesh sensitivity in RVE modelling is proposed, which equalises the fracture energy observed from computational analysis with the real phenomenon, thereby keeping the response independent of the bandwidth of strain localisation. Based on the hypothesis that ensuring periodicity of strain, in addition to displacement periodicity across the domain boundary and supplementing the capability of periodic boundary conditions (PBCs) to attenuate the size dependency in RVE modelling, a set of modified PBCs (MPBCs) are formulated. One thousand two hundred RVE samples falling into combinations of five fibre volume fractions and four RVE sizes are analysed under transverse loading, and the ability of MPBCs to attenuate the effect of RVE size on the precision of material response, particularly in the inelastic regime, is verified. This work also focuses on various factors affecting damage initiation in 2D composite RVEs. The arrangement of a pair of fibres with their members placed close to each other, such that the angle between the direction of loading and an imaginary line drawn between their centres is less, is observed to make the region between them more favourable to damage.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</title>
<link>https://arxiv.org/abs/2508.08325</link>
<guid>https://arxiv.org/abs/2508.08325</guid>
<content:encoded><![CDATA[
<div> algorithm, pricing, advertising, competition, collusion
<br />
Summary:<br />
The study examines the impact of AI learning algorithms on product pricing and advertising decisions in online marketplaces. It investigates concerns about tacit collusion among sellers using these algorithms. The research shows that under certain conditions, algorithms can lead to mutually beneficial outcomes for consumers, sellers, and platforms by coordinating on lower prices. This coordination is achieved through lower advertising costs, resulting in decreased prices. Analysis of a large dataset from Amazon.com reveals varying consumer search costs across different product keywords. The study also identifies a negative relationship between consumer search costs and algorithm usage, indicating beneficial collusion. In terms of platform response, profit increases are seen through commission adjustments rather than reserve price changes. Overall, the findings suggest that competing learning algorithms may not have harmful effects and can assist in decision-making for sellers, platforms, and policymakers. 
 <div>
arXiv:2508.08325v1 Announce Type: cross 
Abstract: Online sellers have been adopting AI learning algorithms to automatically make product pricing and advertising decisions on e-commerce platforms. When sellers compete using such algorithms, one concern is that of tacit collusion - the algorithms learn to coordinate on higher than competitive. We empirically investigate whether these concerns are valid when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. Our empirical strategy is to analyze competition with multi-agent reinforcement learning, which we calibrate to a large-scale dataset collected from Amazon.com products. Our first contribution is to find conditions under which learning algorithms can facilitate win-win-win outcomes that are beneficial for consumers, sellers, and even the platform, when consumers have high search costs. In these cases the algorithms learn to coordinate on prices that are lower than competitive prices. The intuition is that the algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices. Our second contribution is an analysis of a large-scale, high-frequency keyword-product dataset for more than 2 million products on Amazon.com. Our estimates of consumer search costs show a wide range of costs for different product keywords. We generate an algorithm usage and find a negative interaction between the estimated consumer search costs and the algorithm usage index, providing empirical evidence of beneficial collusion. Finally, we analyze the platform's strategic response. We find that reserve price adjustments will not increase profits for the platform, but commission adjustments will. Our analyses help alleviate some worries about the potentially harmful effects of competing learning algorithms, and can help sellers, platforms and policymakers to decide on whether to adopt or regulate such algorithms.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Can Understand Spectra: A Multimodal Model for Molecular Structure Elucidation</title>
<link>https://arxiv.org/abs/2508.08441</link>
<guid>https://arxiv.org/abs/2508.08441</guid>
<content:encoded><![CDATA[
<div> Keywords: Structure elucidation, spectroscopic modalities, SpectraLLM, multi-modal reasoning, molecular structure

Summary: 
SpectraLLM is a novel language model designed to support multi-modal spectroscopic joint reasoning for structure elucidation. It can process single or multiple spectroscopic inputs and perform end-to-end structure elucidation by integrating continuous and discrete spectroscopic modalities. SpectraLLM learns to uncover substructural patterns that are consistent and complementary across spectra, enabling precise molecular structure elucidation. Pretrained and fine-tuned in small molecule domain, SpectraLLM achieves state-of-the-art performance on standardized chemical datasets. The model demonstrates strong robustness and generalization, even for single-spectrum inference, with its multi-modal reasoning capability further improving structural prediction accuracy.<br /><br />Summary: <div>
arXiv:2508.08441v1 Announce Type: cross 
Abstract: Structure elucidation is a fundamental technique for understanding the microscopic composition of matter and is widely applied across various disciplines in the natural sciences and engineering. However, existing methods often rely heavily on prior databases or known structural information, making it difficult to resolve unknown structures. In addition, complex structures typically require the joint analysis of multiple spectroscopic modalities. This process heavily depends on expert domain knowledge and is often accompanied by high costs in terms of both time and instrumentation. To address these challenges, we propose SpectraLLM, the first large language model designed to support multi-modal spectroscopic joint reasoning. SpectraLLM is capable of processing either single or multiple spectroscopic inputs and performing end-to-end structure elucidation. By integrating continuous and discrete spectroscopic modalities into a shared semantic space, SpectraLLM learns to uncover substructural patterns that are consistent and complementary across spectra, enabling precise molecular structure elucidation. We pretrain and fine-tune SpectraLLM in the domain of small molecules, and evaluate it on six standardized, publicly available chemical datasets. The model achieves state-of-the-art performance, significantly outperforming existing approaches trained on single modalities. Notably, SpectraLLM demonstrates strong robustness and generalization even for single-spectrum inference, while its multi-modal reasoning capability further improves the accuracy of structural prediction.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Projection-based multifidelity linear regression for data-scarce applications</title>
<link>https://arxiv.org/abs/2508.08517</link>
<guid>https://arxiv.org/abs/2508.08517</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, multifidelity methods, linear regression, high-dimensional outputs, data-limited applications

Summary:
Surrogate modeling for systems with high-dimensional outputs is challenging when training data are limited. This study introduces multifidelity methods for multiple-input multiple-output linear regression in such data-limited scenarios. Two projection-based approaches are proposed using principal component basis vectors for dimensionality reduction. These approaches combine high-fidelity and low-fidelity data through direct data augmentation and data augmentation with explicit linear corrections. The regression model is trained using weighted least squares with fidelity-specific weights, exploring various weighting schemes. The methods are applied to approximating the surface pressure field of a hypersonic vehicle, showing 3% - 12% improvement in median accuracy compared to single-fidelity methods with comparable computational cost in a low-data regime of up to ten high-fidelity samples.

<br /><br />Summary: <div>
arXiv:2508.08517v1 Announce Type: cross 
Abstract: Surrogate modeling for systems with high-dimensional quantities of interest remains challenging, particularly when training data are costly to acquire. This work develops multifidelity methods for multiple-input multiple-output linear regression targeting data-limited applications with high-dimensional outputs. Multifidelity methods integrate many inexpensive low-fidelity model evaluations with limited, costly high-fidelity evaluations. We introduce two projection-based multifidelity linear regression approaches that leverage principal component basis vectors for dimensionality reduction and combine multifidelity data through: (i) a direct data augmentation using low-fidelity data, and (ii) a data augmentation incorporating explicit linear corrections between low-fidelity and high-fidelity data. The data augmentation approaches combine high-fidelity and low-fidelity data into a unified training set and train the linear regression model through weighted least squares with fidelity-specific weights. Various weighting schemes and their impact on regression accuracy are explored. The proposed multifidelity linear regression methods are demonstrated on approximating the surface pressure field of a hypersonic vehicle in flight. In a low-data regime of no more than ten high-fidelity samples, multifidelity linear regression achieves approximately 3% - 12% improvement in median accuracy compared to single-fidelity methods with comparable computational cost.
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Portable Multi-GPU Solver for Collisional Plasmas with Coulombic Interactions</title>
<link>https://arxiv.org/abs/2508.06771</link>
<guid>https://arxiv.org/abs/2508.06771</guid>
<content:encoded><![CDATA[
<div> Keywords: particle-in-cell methods, low-temperature plasmas, GPU acceleration, collisions, Python-based HPC tools

Summary: 
Particle-in-cell (PIC) methods for low-temperature plasmas (LTPs) are studied in this work, focusing on GPU acceleration of algorithms for velocity-space interactions, particularly collisions involving electrons with neutrals, ions, and electrons. The research explores both algorithmic analysis and the feasibility of implementing algorithms using Python-based HPC tools, specifically PyKokkos. Common PIC kernels are discussed, and performance results for NVIDIA Volta V100 and AMD MI250X GPUs are presented, with the MI250X showing slightly faster performance overall but being more sensitive to register pressure. Scaling results for a distributed memory implementation on up to 16 MPI ranks are also reported. This study contributes valuable insights into optimizing PIC methods for LTP simulations and highlights the potential of GPU acceleration in enhancing computational efficiency for studying plasma physics phenomena. 

<br /><br />Summary: <div>
arXiv:2508.06771v1 Announce Type: new 
Abstract: We study parallel particle-in-cell (PIC) methods for low-temperature plasmas (LTPs), which discretize kinetic formulations that capture the time evolution of the probability density function of particles as a function of position and velocity. We use a kinetic description for electrons and a fluid approximation for heavy species. In this paper, we focus on GPU acceleration of algorithms for velocity-space interactions and in particular, collisions of electrons with neutrals, ions, and electrons. Our work has two thrusts. The first is algorithmic exploration and analysis. The second is examining the viability of rapid-prototyping implementations using Python-based HPC tools, in particular PyKokkos. We discuss several common PIC kernels and present performance results on NVIDIA Volta V100 and AMD MI250X GPUs. Overall, the MI250X is slightly faster for most kernels but shows more sensitivity to register pressure. We also report scaling results for a distributed memory implementation on up to 16 MPI ranks.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of association rule mining to assess forest species distribution in Italy considering abiotic and biotic factors</title>
<link>https://arxiv.org/abs/2508.07076</link>
<guid>https://arxiv.org/abs/2508.07076</guid>
<content:encoded><![CDATA[
<div> Association Rule Mining, biodiversity monitoring, forest community composition, ecological rules, Picea abies<br />
<br />Summary: The study explores the relationships among co-occurring plant species and environmental conditions using Association Rule Mining in forest biodiversity monitoring. By analyzing data from 6,784 plots in Italy, the Frequent Pattern Growth algorithm identified ecological rules, such as the strong correlation between temperature seasonality and precipitation seasonality with Picea abies. This tree species showed a specific association with cold, seasonal environments, indicating its ecological specificity. Some plant species acted as community "hubs," suggesting ties to particular environmental or biotic conditions. The findings provide valuable insights for future research in similar environmental settings, emphasizing the importance of accessible ecological data in understanding forest dynamics and biodiversity conservation.<br /> <div>
arXiv:2508.07076v1 Announce Type: new 
Abstract: Biodiversity monitoring represents a pressing global priority, and assessing forest community composition plays a crucial role due to its influence on ecosystem functions. The spatial distribution of forest species becomes essential for understanding biodiversity dynamics, territorial planning, aiding nature conservation and enhancing ecosystem resilience amid global change. Association Rule Mining, commonly applied to other scientific contexts, is now innovatively adopted in the ecological field to explore the relationships among co-occurring plant species and extract hidden interpretable patterns, also with abiotic and biotic conditions. Multiple heterogeneous data sources were integrated through data preprocessing into a unique dataset, including georeferenced information about 151 plant species monitored within 6,784 plots across Italy and several bioclimatic indices, soil-related factors, and variables from earth observations. The Frequent Pattern Growth algorithm, used for association rule mining, provided interesting and encouraging findings, suggesting ecological rules among plant species and environmental conditions. Indeed, temperature seasonality between 650-700 and precipitation seasonality between 45-50 resulted very correlated with Picea abies (confidence = 90.9%, lift = 7.13). Patterns detected for Picea abies highlighted its ecological specificity, indicating a strong association with cold, highly seasonal environments, and particular plant communities. Some species appeared acting as community "hubs", frequently co-occurring with other species, suggesting ties to specific environmental or biotic conditions. These findings represent a valuable resource for future research, especially in regions with similar environmental settings and when prior ecological knowledge exists, also underlining the importance of publicly accessible, high-quality ecological data.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harmonic balance-automatic differentiation method: an out-of-the-box and efficient solver for general nonlinear dynamics simulation</title>
<link>https://arxiv.org/abs/2508.07309</link>
<guid>https://arxiv.org/abs/2508.07309</guid>
<content:encoded><![CDATA[
<div> method, Harmonic Balance-Alternating Frequency-Time domain, high-dimensional complex systems, Automatic Differentiation, computational efficiency

Summary:
The paper introduces the Harmonic Balance-Automatic Differentiation (HB-AD) method, which aims to enhance the dynamic response analysis of nonlinear systems, particularly in high-dimensional complex systems. HB-AD integrates Automatic Differentiation (AD) with the harmonic balance framework to eliminate manual derivations of Jacobian matrices, making it more efficient and accurate. The implementation of HB-AD leverages deep learning frameworks for parallel computing and CUDA acceleration, combined with arc-length continuation for high efficiency. Computational experiments on rotor systems demonstrate HB-AD's capability in handling complex nonlinear expressions with automated Jacobian calculations. Compared to traditional methods, HB-AD achieves significantly higher computational efficiency, making it a valuable tool for the dynamic characterization of high-dimensional engineering systems. <div>
arXiv:2508.07309v1 Announce Type: new 
Abstract: The Harmonic Balance-Alternating Frequency-Time domain (HB-AFT) method is extensively employed for dynamic response analysis of nonlinear systems. However, its application to high-dimensional complex systems is constrained by the manual derivation of Jacobian matrices during Newton-Raphson iterations, which become computationally intractable or error-prone for intricate nonlinearities. The Harmonic Balance-Automatic Differentiation (HB-AD) method is proposed to address this limitation, in which AD is integrated with the harmonic balance framework. This approach eliminates all manual derivations by leveraging AD to compute exact Jacobians numerically, enabling generic and efficient analysis of high-dimensional complex nonlinear systems. The implementation utilizes advanced deep learning frameworks for native parallel computing and CUDA acceleration, and combines AD with arc-length continuation, establishing an out-of-the-box and high efficiency computational architecture. Users need only supply the system's dynamic equations, HB-AD then autonomously trace the complete panorama of periodic responses -- including stable/unstable solution branches. Computational experiments on a rotor system with squeeze-film damper (SFD) demonstrate HB-AD's capability in handling complex nonlinear expressions with automated Jacobian calculations. For a high-dimensional aero-engine rotor-bearing-casing system with complex bearing nonlinearities, HB-AD achieves 17-fold higher efficiency than traditional HB-AFT and 144-fold acceleration over the Newmark method. The HB-AD method is a synergistic merger of computational mechanics and machine learning primitives, delivers an easy to use, general-purpose, high efficiency platform for high-fidelity dynamic characterization of high-dimensional engineering systems.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cardiotensor: A Python Library for Orientation Analysis and Tractography in 3D Cardiac Imaging</title>
<link>https://arxiv.org/abs/2508.07476</link>
<guid>https://arxiv.org/abs/2508.07476</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiotensor, 3D cardiomyocyte orientation, structure tensor analysis, high-performance computing, tractography

Summary:
cardiotensor is a new open-source Python package designed to quantify 3D cardiomyocyte orientation in high-resolution imaging datasets of the human heart. It utilizes structure tensor analysis to extract directional metrics such as helical angle, intrusion angle, and fractional anisotropy. The package supports large teravoxel-scale datasets and is optimized for high-performance computing environments. In addition to providing detailed structural mapping of cardiac tissue, cardiotensor also includes tractography functionality to reconstruct continuous cardiomyocyte trajectories, allowing for multi-scale myoaggregate visualization down to the myocyte level. These capabilities enable the assessment of anatomical continuity and regional organization in the heart, providing valuable insights into its microstructural architecture. 

<br /><br />Summary: <div>
arXiv:2508.07476v1 Announce Type: new 
Abstract: Understanding the architecture of the human heart requires analysis of its microstructural organization across scales. With the advent of high-resolution imaging techniques such as synchrotron-based tomography, it has become possible to visualize entire hearts at micron-scale resolution. However, translating these large, complex volumetric datasets into interpretable, quantitative descriptors of cardiac organization remains a major challenge. Here we present cardiotensor, an open-source Python package designed to quantify 3D cardiomyocyte orientation in whole- or partial-heart imaging datasets. It provides efficient, scalable implementations of structure tensor analysis, enabling extraction of directional metrics such as helical angle (HA), intrusion angle (IA), and fractional anisotropy (FA). The package supports datasets reaching teravoxel-scale and is optimized for high-performance computing environments, including parallel and chunk-based processing pipelines. In addition, cardiotensor includes tractography functionality to reconstruct continuous cardiomyocyte trajectories. This enables multi-scale myoaggregate visualization down to the myocyte level, depending on resolution. These capabilities enable detailed structural mapping of cardiac tissue, supporting the assessment of anatomical continuity and regional organization.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Material Fingerprinting: A shortcut to material model discovery without solving optimization problems</title>
<link>https://arxiv.org/abs/2508.07831</link>
<guid>https://arxiv.org/abs/2508.07831</guid>
<content:encoded><![CDATA[
<div> fingerprinting, mechanical material models, hyperelastic materials, pattern recognition algorithm, experimental setups  
Summary:  
Material Fingerprinting is introduced as a novel method for rapidly discovering mechanical material models without solving complex optimization problems. The method is based on the assumption that each material has a unique response under standardized experimental conditions, creating a "fingerprint" that encodes mechanical characteristics. By establishing a database of fingerprints and corresponding models, unseen materials can be quickly characterized by matching their fingerprints. The study demonstrates that Material Fingerprinting is effective for model discovery in experiments with homogeneous and heterogeneous deformation fields, avoiding the challenges of non-convex optimization. This approach is shown to be applicable across different experimental setups and material behaviors, providing a versatile framework for rapid material model identification. This innovation holds promise for future developments in material characterization.  
<br /><br />Summary: <div>
arXiv:2508.07831v1 Announce Type: new 
Abstract: We propose Material Fingerprinting, a new method for the rapid discovery of mechanical material models from direct or indirect data that avoids solving potentially non-convex optimization problems. The core assumption of Material Fingerprinting is that each material exhibits a unique response when subjected to a standardized experimental setup. We can interpret this response as the material's fingerprint, essentially a unique identifier that encodes all pertinent information about the material's mechanical characteristics. Consequently, once we have established a database containing fingerprints and their corresponding mechanical models during an offline phase, we can rapidly characterize an unseen material in an online phase. This is accomplished by measuring its fingerprint and employing a pattern recognition algorithm to identify the best matching fingerprint in the database. In our study, we explore this concept in the context of hyperelastic materials, demonstrating the applicability of Material Fingerprinting across different experimental setups. Initially, we examine Material Fingerprinting through experiments involving homogeneous deformation fields, which provide direct strain-stress data pairs. We then extend this concept to experiments involving complexly shaped specimens with heterogeneous deformation fields, which provide indirect displacement and reaction force measurements. We show that, in both cases, Material Fingerprinting is an efficient tool for model discovery, bypassing the challenges of potentially non-convex optimization. We believe that Material Fingerprinting provides a powerful and generalizable framework for rapid material model identification across a wide range of experimental designs and material behaviors, paving the way for numerous future developments.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Engineering Student Perceptions of Introductory CS Courses in an Indian Context</title>
<link>https://arxiv.org/abs/2508.06563</link>
<guid>https://arxiv.org/abs/2508.06563</guid>
<content:encoded><![CDATA[
<div> Keywords: student perceptions, assessment practices, computer science education, inclusive learning, programming course<br />
Summary: 
- The study focuses on engineering students' perceptions of assessment practices in an introductory computer science course and its associated lab in an Indian engineering institute.
- A survey involving 318 first-year students revealed that lab assignments were seen as effective, while exams and projects were viewed as authentic and skill-enhancing.
- Instructors played a significant role in shaping course content, and teaching assistants were found to be approachable and helpful despite some inconsistencies.
- Variations in academic performance and assessment perceptions were noted based on factors like prior programming experience, technology familiarity, gender, and academic branch.
- The study challenges common assumptions in grade modeling as the performance data did not follow a Gaussian distribution. A comparative analysis with European cohorts highlighted universal patterns and contextual differences, offering insights for designing inclusive assessment strategies in programming education.<br /><br />Summary: <div>
arXiv:2508.06563v1 Announce Type: cross 
Abstract: Understanding student perceptions of assessment is vital for designing inclusive and effective learning environments, especially in technical education. This study explores engineering students' perceptions of assessment practices in an introductory computer science/ programming course, and its associated laboratory within an Indian engineering institute context. A total of 318 first-year Bachelor of Technology students participated in a weekly 25-statement Likert-scale survey conducted over nine weeks. Using descriptive statistics and non-parametric tests (Mann-Whitney U and Kruskal-Wallis), the analysis reveals that students largely perceive lab assignments as effective learning activities and view exams and projects as authentic and skill-enhancing. Students appreciated the role of instructors in shaping course content and found teaching assistants to be approachable and helpful, despite some inconsistencies. The study also finds significant variations in students' academic performance and assessment perceptions based on prior programming experience, technology familiarity, gender, and academic branch. Notably, the performance data did not follow a Gaussian distribution, challenging common assumptions in grade modeling. A comparative analysis with European cohorts highlights both universal patterns and contextual differences, offering valuable insights for designing inclusive and equitable assessment strategies in programming education.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery Learning accelerates battery design evaluation</title>
<link>https://arxiv.org/abs/2508.06985</link>
<guid>https://arxiv.org/abs/2508.06985</guid>
<content:encoded><![CDATA[
<div> Discovery Learning, batteries, machine learning, lifetime prediction, rapid feedback<br />
<br />
Summary: <br />
The article introduces Discovery Learning (DL), a scientific machine-learning approach that combines active learning, physics-guided learning, and zero-shot learning to efficiently evaluate novel battery designs. DL leverages historical data to predict battery lifetime for untested material-design combinations without the need for additional data labeling or extensive prototyping. Testing DL on a set of large-format lithium-ion pouch cells demonstrates its effectiveness in predicting cycle life with a 7.2% test error, leading to significant time and energy savings compared to traditional industrial practices. This approach showcases the potential of leveraging past designs to accelerate the development of next-generation battery technologies, making data-driven modeling more efficient and aiding scientific discovery and engineering innovation. <div>
arXiv:2508.06985v1 Announce Type: cross 
Abstract: Fast and reliable validation of novel designs in complex physical systems such as batteries is critical to accelerating technological innovation. However, battery research and development remain bottlenecked by the prohibitively high time and energy costs required to evaluate numerous new design candidates, particularly in battery prototyping and life testing. Despite recent progress in data-driven battery lifetime prediction, existing methods require labeled data of target designs to improve accuracy and cannot make reliable predictions until after prototyping, thus falling far short of the efficiency needed to enable rapid feedback for battery design. Here, we introduce Discovery Learning (DL), a scientific machine-learning paradigm that integrates active learning, physics-guided learning, and zero-shot learning into a human-like reasoning loop, drawing inspiration from learning theories in educational psychology. DL can learn from historical battery designs and actively reduce the need for prototyping, thus enabling rapid lifetime evaluation for unobserved material-design combinations without requiring additional data labeling. To test DL, we present 123 industrial-grade large-format lithium-ion pouch cells, spanning eight material-design combinations and diverse cycling protocols. Trained solely on public datasets of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting the average cycle life under unknown device variability. This results in savings of 98% in time and 95% in energy compared to industrial practices. This work highlights the potential of uncovering insights from historical designs to inform and accelerate the development of next-generation battery technologies. DL represents a key advance toward efficient data-driven modeling and helps realize the promise of machine learning for accelerating scientific discovery and engineering innovation.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Enhanced Time-Series Forecasting via Large Language Models</title>
<link>https://arxiv.org/abs/2508.07697</link>
<guid>https://arxiv.org/abs/2508.07697</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, large language models, semantic enhancement, anomalous characteristics, Transformer-based models

Summary: 
- The article proposes a Semantic-Enhanced Large Language Model (SE-LLM) that integrates the periodicity and anomalies of time series data into a semantic space to enhance token embedding.
- By enhancing token interpretability for LLMs, SE-LLM effectively bridges the gap between linguistic knowledge structures and time series data patterns, improving semantic representation.
- A plugin module embedded within self-attention is introduced to enable LLMs to capture both long-term dependencies and short-term anomalies in time series data, enhancing their adaptability to temporal sequence analysis.
- The approach freezes the LLM model and reduces the dimensionality of token sequences, leading to significant computational efficiency gains.
- Experimental results demonstrate that SE-LLM outperforms existing state-of-the-art methods for time series forecasting. 

<br /><br />Summary: <div>
arXiv:2508.07697v1 Announce Type: cross 
Abstract: Time series forecasting plays a significant role in finance, energy, meteorology, and IoT applications. Recent studies have leveraged the generalization capabilities of large language models (LLMs) to adapt to time series forecasting, achieving promising performance. However, existing studies focus on token-level modal alignment, instead of bridging the intrinsic modality gap between linguistic knowledge structures and time series data patterns, greatly limiting the semantic representation. To address this issue, we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent periodicity and anomalous characteristics of time series to embed into the semantic space to enhance the token embedding. This process enhances the interpretability of tokens for LLMs, thereby activating the potential of LLMs for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel at capturing long-range dependencies but are weak at modeling short-term anomalies in time-series data. Hence, we propose a plugin module embedded within self-attention that models long-term and short-term dependencies to effectively adapt LLMs to time-series analysis. Our approach freezes the LLM and reduces the sequence dimensionality of tokens, greatly reducing computational consumption. Experiments demonstrate the superiority performance of our SE-LLM against the state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lagrangian method for solving the spherical shallow water equations using power diagrams</title>
<link>https://arxiv.org/abs/2508.08129</link>
<guid>https://arxiv.org/abs/2508.08129</guid>
<content:encoded><![CDATA[
<div> Eulerian viewpoint, Lagrangian viewpoint, spherical power cells, optimal transport problem, momentum conservation <br />
Summary: 
The article presents a new Lagrangian method for simulating the atmosphere using spherical power cells. Mass conservation is ensured through solving an optimal transport problem, while a semi-implicit time stepping procedure is used for time advancement. Artificial viscosity is not required for stabilization. The efficiency of computing spherical Voronoi diagrams is demonstrated, with calculations of 100 million sites completed in under 2 minutes. The new method is evaluated on benchmark tests, showing comparable momentum and energy conservation to the latest Lagrangian approach for the spherical shallow water equations. The study suggests that this Lagrangian approach can offer a competitive alternative to Eulerian simulations for global atmospheric simulations. <br /> <div>
arXiv:2508.08129v1 Announce Type: cross 
Abstract: Numerical simulations of the air in the atmosphere and water in the oceans are essential for numerical weather prediction. The state-of-the-art for performing these fluid simulations relies on an Eulerian viewpoint, in which the fluid domain is discretized into a mesh, and the governing equations describe the fluid motion as it passes through each cell of the mesh. However, it is unclear whether a Lagrangian viewpoint, in which the fluid is discretized by a collection of particles, can outperform Eulerian simulations in global atmospheric simulations. To date, Lagrangian approaches have shown promise, but tend to produce smoother solutions. In this work, a new Lagrangian method is developed to simulate the atmosphere in which particles are represented with spherical power cells. We introduce an efficient algorithm for computing these cells which are then used to discretize the spherical shallow water equations. Mass conservation is enforced by solving a semi-discrete optimal transport problem and a semi-implicit time stepping procedure is used to advance the solution in time. We note that, in contrast to previous work, artificial viscosity is not needed to stabilize the simulation. The performance of the spherical Voronoi diagram calculation is first assessed, which shows that spherical Voronoi diagrams of 100 million sites can be computed in under 2 minutes on a single machine. The new simulation method is then evaluated on standard benchmark test cases, which shows that momentum and energy conservation of this new method is comparable to the latest Lagrangian approach for simulating the spherical shallow water equations.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Organizations, teams, and job mobility: A social microdynamics approach</title>
<link>https://arxiv.org/abs/2503.24117</link>
<guid>https://arxiv.org/abs/2503.24117</guid>
<content:encoded><![CDATA[
<div> social connections, worker mobility, organizational teams, job reunions, job change

Summary:
Workers in large organizations are influenced by preferring to reunite with past coworkers when changing jobs, with a significant percentage of job moves leading to worker reunions. The study introduces a new framework to describe organizations as composites of teams with specific tasks and social connections. The importance of worker reunions in determining job moves is highlighted, surpassing labor supply and demand considerations. Time spent together and team size are factors influencing the likelihood of reunions, indicating the role of familiarity and trust in job mobility. The study underscores the significance of teams structures and social ties in shaping internal job change within large organizations. <br /><br />Summary: <div>
arXiv:2503.24117v2 Announce Type: replace 
Abstract: Most of the modeling approaches used to understand organizational worker mobility are highly stylized, using idealizations such as structureless organizations, indistinguishable workers, and a lack of social bonding of the workers. In this article, aided by a decade of precise, temporally resolved data of a large civilian organization of the US Army in which employees can change jobs in a similar way to many private organizations, we introduce a new framework to describe organizations as composites of teams within which individuals perform specific tasks and where social connections develop. By tracking the personnel composition of organizational teams, we find that workers who change jobs are highly influenced by preferring to reunite with past coworkers. In this organization, 34% of all moves across temporally stable teams (and 32% of the totality of moves) lead to worker reunions, percentages that have not been reported and are well-above intuitive expectation. To assess the importance of worker reunions in determining job moves, we compare them to labor supply and demand with or without occupational specialization. The comparison shows that the most consistent information about job change is provided by reunions. We find that the greater the time workers spend together or the smaller the team they share both increase their likelihood to reunite, supporting the notion of increased familiarity and trust behind such reunions and the dominant role of social capital in the evolution of large organizations. Our study of this organization supports the idea that to correctly forecast job mobility inside large organizations, their teams structures and the social ties formed in those teams play a key role in shaping internal job change.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection</title>
<link>https://arxiv.org/abs/2403.06534</link>
<guid>https://arxiv.org/abs/2403.06534</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic Aperture Radar, object detection, dataset, SAR, Multi-Stage with Filter Augmentation<br />
<br />
Summary: 
This research introduces a new benchmark dataset, SARDet-100K, for large-scale Synthetic Aperture Radar (SAR) object detection. The dataset is a combination of 10 existing SAR detection datasets and is the first of its kind with multi-class objects at a COCO-level scale. The study identifies a challenge in SAR object detection related to the disparities between pretraining on RGB datasets and finetuning on SAR datasets. In response, a novel pretraining framework called Multi-Stage with Filter Augmentation (MSFA) is proposed to address these gaps in data domain and model structure. The MSFA method significantly improves the performance of SAR object detection models and demonstrates versatility across various models. By providing the SARDet-100K dataset and open-source code, this work aims to advance research in SAR object detection. <div>
arXiv:2403.06534v3 Announce Type: replace-cross 
Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at https://github.com/zcablii/SARDet_100K.
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Sioux Falls Network: Insights from Path-Driven Higher-Order Network Analysis</title>
<link>https://arxiv.org/abs/2508.06234</link>
<guid>https://arxiv.org/abs/2508.06234</guid>
<content:encoded><![CDATA[
<div> Keywords: Benchmark scenarios, Higher-order network models, Mobility behavior, Structural complexity, Path diversity

Summary: 
The study introduces a mathematical framework based on higher-order network models to evaluate benchmark scenarios in transportation research, focusing on the Sioux Falls scenario. The framework aims to quantify the representativeness of benchmark networks by assessing structural and functional patterns. Results show that the classical Sioux Falls network has limited path diversity, rapid structural fragmentation, and weak alignment with empirical routing behavior. Higher-order network models are proposed as a way to bridge the gap between simulation-based and real-world mobility analysis, providing more accurate and generalizable insights in transportation research. This study highlights the importance of considering memory-aware network representations to improve the fidelity of benchmark scenarios in evaluating routing algorithms, infrastructure interventions, and new technologies in transportation research. 

Summary:<br /><br />Keywords: Benchmark scenarios, Higher-order network models, Mobility behavior, Structural complexity, Path diversity<br /><br />The study examines the representativeness of benchmark networks in transportation research using a mathematical framework based on higher-order network models. The analysis focuses on the widely used Sioux Falls scenario, revealing limited path diversity, rapid structural fragmentation, and weak alignment with empirical routing behavior in the classical Sioux Falls network. The study suggests that higher-order network models can enhance the accuracy and generalizability of simulation results, bridging the gap between simulation-based and real-world mobility analysis. By considering memory-aware network representations, researchers can improve the fidelity of benchmark scenarios and gain more meaningful insights in evaluating routing algorithms, infrastructure interventions, and new technologies in transportation research. <div>
arXiv:2508.06234v1 Announce Type: new 
Abstract: Benchmark scenarios are widely used in transportation research to evaluate routing algorithms, simulate infrastructure interventions, and test new technologies under controlled conditions. However, the structural and behavioral fidelity of these benchmarks remains largely unquantified, raising concerns about the external validity of simulation results. In this study, we introduce a mathematical framework based on higher-order network models to evaluate the representativeness of benchmark networks, focusing on the widely used Sioux Falls scenario. Higher-order network models encode empirical and simulated trajectory data into memory-aware network representations, which we use to quantify sequential dependencies in mobility behavior and assess how well benchmark networks capture real-world structural and functional patterns. Applying this framework to the Sioux Falls network, as well as real-world trajectory data, we quantify structural complexity, optimal memory length, link prediction accuracy, and centrality alignment. Our results show and statistically quantify that the classical Sioux Falls network exhibits limited path diversity, rapid structural fragmentation at higher orders, and weak alignment with empirical routing behavior. These results illustrate the potential of higher-order network models to bridge the gap between simulation-based and real-world mobility analysis, providing a robust foundation for more accurate and generalizable insights in transportation research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading</title>
<link>https://arxiv.org/abs/2508.06312</link>
<guid>https://arxiv.org/abs/2508.06312</guid>
<content:encoded><![CDATA[
<div> factor mining, quantitative trading, Large Language Models, automated, alpha discovery

Summary: 
Chain-of-Alpha introduces a novel framework for automated alpha mining in quantitative trading using Large Language Models (LLMs). The method utilizes a dual-chain architecture comprising a Factor Generation Chain and a Factor Optimization Chain to iteratively generate, evaluate, and refine alpha factors without human intervention. By leveraging market data, backtest feedback, and prior optimization knowledge, Chain-of-Alpha offers a high degree of automation, generality, and efficiency in alpha discovery. The framework outperforms existing baselines in real-world A-share benchmarks, highlighting its scalability and promising potential for LLM-driven quantitative research. <div>
arXiv:2508.06312v1 Announce Type: new 
Abstract: Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Farm Economics and Landscape Ecology for Global Sustainability through Hierarchical and Bayesian Optimization</title>
<link>https://arxiv.org/abs/2508.06386</link>
<guid>https://arxiv.org/abs/2508.06386</guid>
<content:encoded><![CDATA[
<div> Optimization, Agricultural landscapes, Biodiversity, Connectivity, Agri-environmental policies
Summary: 
The article introduces a novel hierarchical optimization framework addressing the challenge of sustaining food production while reversing biodiversity loss in agricultural landscapes. The framework consists of an Ecological Intensification (EI) model determining optimal allocation of land to margin and habitat interventions at the farm level, an Ecological Connectivity (EC) model arranging interventions across the landscape to enhance connectivity while maintaining profitability, and a Bayesian Optimization (BO) approach translating spatial outcomes into policy instruments. By applying this framework to a Canadian agricultural landscape, the study demonstrates improved connectivity under economic constraints. The approach aligns farm incentives with biodiversity goals, offering an effective tool for developing economically viable and ecologically sound agri-environmental policies. 
<br /><br />Summary: <div>
arXiv:2508.06386v1 Announce Type: new 
Abstract: Agricultural landscapes face the dual challenge of sustaining food production while reversing biodiversity loss. Agri-environmental policies often fall short of delivering ecological functions such as landscape connectivity, in part due to a persistent disconnect between farm-level economic decisions and landscape-scale spatial planning. We introduce a novel hierarchical optimization framework that bridges this gap. First, an Ecological Intensification (EI) model determines the economically optimal allocation of land to margin and habitat interventions at the individual farm level. These farm-specific intervention levels are then passed to an Ecological Connectivity (EC) model, which spatially arranges them across the landscape to maximize connectivity while preserving farm-level profitability. Finally, we introduce a Bayesian Optimization (BO) approach that translates these spatial outcomes into simple, cost effective, and scalable policy instruments, such as subsidies and eco-premiums, using non-spatial, farm-level policy parameters. Applying the framework to a Canadian agricultural landscape, we demonstrate how it enhances connectivity under real-world economic constraints. Our approach provides a globally relevant tool for aligning farm incentives with biodiversity goals, advancing the development of agri-environmental policies that are economically viable and ecologically effective.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Collocation Point Strategies For Physics Informed Neural Networks via the QR Discrete Empirical Interpolation Method</title>
<link>https://arxiv.org/abs/2501.07700</link>
<guid>https://arxiv.org/abs/2501.07700</guid>
<content:encoded><![CDATA[
<div> adaptive collocation point selection, physics-informed neural networks, partial differential equations, QR Discrete Empirical Interpolation Method, adaptive mesh refinement <br />
<br />
Summary: 
This article explores the impact of collocation point sampling on the performance of Physics-Informed Neural Networks (PINNs) for solving problems related to partial differential equations (PDEs). Traditional fixed sampling methods can be limited in capturing high-gradient regions, impacting the accuracy of PINNs for complex PDEs. The proposed adaptive collocation point selection strategies leverage the QR Discrete Empirical Interpolation Method (QR-DEIM) to dynamically update collocation points during training. Results on benchmark PDEs showcase that the QR-DEIM-based approaches enhance PINN accuracy compared to existing methods. This research opens up a promising pathway for improving the efficiency and effectiveness of adaptive collocation point strategies in the context of PINNs. <br /> <div>
arXiv:2501.07700v4 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have gained significant attention for solving forward and inverse problems related to partial differential equations (PDEs). While advancements in loss functions and network architectures have improved PINN accuracy, the impact of collocation point sampling on their performance remains underexplored. Fixed sampling methods, such as uniform random sampling and equispaced grids, can fail to capture critical regions with high solution gradients, limiting their effectiveness for complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from traditional numerical methods, address this by dynamically updating collocation points during training but may overlook residual dynamics between updates, potentially losing valuable information. To overcome this limitation, we propose two adaptive collocation point selection strategies utilizing the QR Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling technique for efficiently approximating nonlinear functions. Our results on benchmark PDEs demonstrate that our QR-DEIM-based approaches improve PINN accuracy compared to existing methods, offering a promising direction for adaptive collocation point strategies.
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriceFM: Foundation Model for Probabilistic Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2508.04875</link>
<guid>https://arxiv.org/abs/2508.04875</guid>
<content:encoded><![CDATA[
<div> spatiotemporal forecasting, electricity price, Europe, deep learning, graph-based

Summary:
The paper introduces PriceFM, a spatiotemporal foundation model for electricity price forecasting in Europe that leverages graph-based inductive biases to capture spatial interdependencies across interconnected power markets. The model utilizes a comprehensive dataset spanning 24 European countries and 38 regions over a three-year period. PriceFM is designed for multi-region, multi-timestep, and multi-quantile probabilistic forecasting, outperforming competitive baselines in extensive experiments. The study highlights the importance of incorporating spatial context in electricity market forecasting. The dataset and code are available on GitHub for further research and development. <br /><br />Summary: <div>
arXiv:2508.04875v1 Announce Type: new 
Abstract: Electricity price forecasting in Europe presents unique challenges due to the continent's increasingly integrated and physically interconnected power market. While recent advances in deep learning and foundation models have led to substantial improvements in general time series forecasting, most existing approaches fail to capture the complex spatial interdependencies and uncertainty inherent in electricity markets. In this paper, we address these limitations by introducing a comprehensive and up-to-date dataset across 24 European countries (38 regions), spanning from 2022-01-01 to 2025-01-01. Building on this groundwork, we propose PriceFM, a spatiotemporal foundation model that integrates graph-based inductive biases to capture spatial interdependencies across interconnected electricity markets. The model is designed for multi-region, multi-timestep, and multi-quantile probabilistic electricity price forecasting. Extensive experiments and ablation studies confirm the model's effectiveness, consistently outperforming competitive baselines and highlighting the importance of spatial context in electricity markets. The dataset and code can be found at https://github.com/runyao-yu/PriceFM.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment-Aware Stock Price Prediction with Transformer and LLM-Generated Formulaic Alpha</title>
<link>https://arxiv.org/abs/2508.04975</link>
<guid>https://arxiv.org/abs/2508.04975</guid>
<content:encoded><![CDATA[
<div> Keywords: alpha decay, large language models, stock price prediction, financial data, interpretability

Summary:
Automating the generation of alpha decay strategies in trading and quantitative analysis is now possible through the integration of large language models (LLMs) with Transformer models. This novel framework utilizes structured inputs like historical stock features, technical indicators, and sentiment scores to generate diverse and adaptive alphas. These formulaic alphas serve as high-level features capturing complex dependencies within financial data. The alphas, not directly used for trading, are inputted into prediction models like Transformer, LSTM, TCN, SVR, and Random Forest to forecast future stock prices. Experimental results show that LLM-generated alphas significantly enhance predictive accuracy. Additionally, the natural language reasoning provided by the LLM enhances interpretability and transparency in financial decision-making. <br /><br />Summary: <div>
arXiv:2508.04975v1 Announce Type: new 
Abstract: Traditionally, traders and quantitative analysts address alpha decay by manually crafting formulaic alphas, mathematical expressions that identify patterns or signals in financial data, through domain expertise and trial-and-error. This process is often time-consuming and difficult to scale. With recent advances in large language models (LLMs), it is now possible to automate the generation of such alphas by leveraging the reasoning capabilities of LLMs. This paper introduces a novel framework that integrates a prompt-based LLM with a Transformer model for stock price prediction. The LLM first generates diverse and adaptive alphas using structured inputs such as historical stock features (Close, Open, High, Low, Volume), technical indicators, sentiment scores of both target and related companies. These alphas, instead of being used directly for trading, are treated as high-level features that capture complex dependencies within the financial data. To evaluate the effectiveness of these LLM-generated formulaic alphas, the alpha features are then fed into prediction models such as Transformer, LSTM, TCN, SVR, and Random Forest to forecast future stock prices. Experimental results demonstrate that the LLM-generated alphas significantly improve predictive accuracy. Moreover, the accompanying natural language reasoning provided by the LLM enhances the interpretability and transparency of the predictions, supporting more informed financial decision-making.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fuzzy Decisions on Fluid Instabilities: Autoencoder-Based Reconstruction meets Rule-Based Anomaly Classification</title>
<link>https://arxiv.org/abs/2508.05418</link>
<guid>https://arxiv.org/abs/2508.05418</guid>
<content:encoded><![CDATA[
<div> Keywords: shockwave classification, shadowgraph imaging, hybrid framework, unsupervised autoencoder, fuzzy inference system

Summary: 
This study introduces a novel approach for shockwave classification in shadowgraph imaging, addressing the challenges posed by limited labeled data and complex flow structures. The hybrid framework combines unsupervised autoencoder models with a fuzzy inference system to generate and interpret anomaly maps. Among the methods evaluated, the hybrid $\beta$-VAE autoencoder with a fuzzy rule-based system proves to be the most effective in capturing coherent shock features and integrating spatial context for enhanced anomaly classification. This approach allows for interpretable, unsupervised classification of flow disruptions, paving the way for real-time, physics-informed diagnostics in experimental and industrial fluid applications. The proposed methodology holds promise for improving understanding and analysis of shockwave phenomena in various fluid dynamics scenarios. 

<br /><br />Summary: <div>
arXiv:2508.05418v1 Announce Type: new 
Abstract: Shockwave classification in shadowgraph imaging is challenging due to limited labeled data and complex flow structures. This study presents a hybrid framework that combines unsupervised autoencoder models with a fuzzy inference system to generate and interpret anomaly maps. Among the evaluated methods, the hybrid $\beta$-VAE autoencoder with a fuzzy rule-based system most effectively captured coherent shock features, integrating spatial context to enhance anomaly classification. The resulting approach enables interpretable, unsupervised classification of flow disruptions and lays the groundwork for real-time, physics-informed diagnostics in experimental and industrial fluid applications.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorising SME Bank Transactions with Machine Learning and Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2508.05425</link>
<guid>https://arxiv.org/abs/2508.05425</guid>
<content:encoded><![CDATA[
<div> Financial, Small and Medium Enterprises, Cash flow lending, Synthetic data generation, Classification model<br />
Summary:<br />
This article explores the challenges faced by Small and Medium Enterprises (SMEs) in securing traditional financing due to information asymmetries. Cash flow lending is proposed as an alternative, but its effectiveness relies on accurate modeling of transaction-level data. The main obstacle in analyzing SME transactions is the unstructured nature of textual descriptions, characterized by abbreviations and imbalanced label distributions. To address these challenges, the authors propose a bank categorization pipeline leveraging synthetic data generation to enrich transaction datasets. Their approach consists of a synthetic data generation module, a fine-tuned classification model, and a calibration methodology. Experimental results show the model achieves high accuracy and robust generalization across different SMEs and transaction types, making it suitable for practical deployment in cash-flow lending applications. This framework offers a practical solution to data challenges in SME lending contexts, addressing scarcity, noise, and imbalance. <br /> <div>
arXiv:2508.05425v1 Announce Type: new 
Abstract: Despite their significant economic contributions, Small and Medium Enterprises (SMEs) face persistent barriers to securing traditional financing due to information asymmetries. Cash flow lending has emerged as a promising alternative, but its effectiveness depends on accurate modelling of transaction-level data. The main challenge in SME transaction analysis lies in the unstructured nature of textual descriptions, characterised by extreme abbreviations, limited context, and imbalanced label distributions. While consumer transaction descriptions often show significant commonalities across individuals, SME transaction descriptions are typically nonstandard and inconsistent across businesses and industries. To address some of these challenges, we propose a bank categorisation pipeline that leverages synthetic data generation to augment existing transaction data sets. Our approach comprises three core components: (1) a synthetic data generation module that replicates transaction properties while preserving context and semantic meaning; (2) a fine-tuned classification model trained on this enriched dataset; and (3) a calibration methodology that aligns model outputs with real-world label distributions. Experimental results demonstrate that our approach achieves 73.49% (+-5.09) standard accuracy on held-out data, with high-confidence predictions reaching 90.36% (+-6.52) accuracy. The model exhibits robust generalisation across different types of SMEs and transactions, which makes it suitable for practical deployment in cash-flow lending applications. By addressing core data challenges, namely, scarcity, noise, and imbalance, our framework provides a practical solution to build robust classification systems in data-sparse SME lending contexts.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deconstructing the Crystal Ball: From Ad-Hoc Prediction to Principled Startup Evaluation with the SAISE Framework</title>
<link>https://arxiv.org/abs/2508.05491</link>
<guid>https://arxiv.org/abs/2508.05491</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, startup evaluation, systematic literature review, prediction models, SAISE Framework

Summary: 
The article discusses the integration of Artificial Intelligence (AI) into startup evaluation and highlights the fragmented nature of existing academic research in this field. It points out the inconsistencies in definitions of success, lack of theoretical foundations, and insufficient validation methods in current predictive models. The study includes a systematic literature review of 57 empirical studies to understand the features, algorithms, data sources, and evaluation practices used in AI-driven startup prediction. The research identifies key weaknesses in the field, such as fragmented definition of success, lack of theory-driven feature engineering, inadequate model validation, and limited focus on data ethics and explainability. In response to these findings, the authors propose the Systematic AI-driven Startup Evaluation (SAISE) Framework, a five-stage roadmap aimed at guiding researchers towards a more principled and rigorous evaluation methodology. By emphasizing problem definition, data synthesis, feature engineering, validation, and interpretation, the SAISE framework aims to enhance the comparability, robustness, and practical relevance of research in this rapidly evolving domain.

Summary: <br /><br /> <div>
arXiv:2508.05491v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) into startup evaluation represents a significant technological shift, yet the academic research underpinning this transition remains methodologically fragmented. Existing studies often employ ad-hoc approaches, leading to a body of work with inconsistent definitions of success, atheoretical features, and a lack of rigorous validation. This fragmentation severely limits the comparability, reliability, and practical utility of current predictive models.
  To address this critical gap, this paper presents a comprehensive systematic literature review of 57 empirical studies. We deconstruct the current state-of-the-art by systematically mapping the features, algorithms, data sources, and evaluation practices that define the AI-driven startup prediction landscape. Our synthesis reveals a field defined by a central paradox: a strong convergence on a common toolkit -- venture databases and tree-based ensembles -- but a stark divergence in methodological rigor. We identify four foundational weaknesses: a fragmented definition of "success," a divide between theory-informed and data-driven feature engineering, a chasm between common and best-practice model validation, and a nascent approach to data ethics and explainability.
  In response to these findings, our primary contribution is the proposal of the Systematic AI-driven Startup Evaluation (SAISE) Framework. This novel, five-stage prescriptive roadmap is designed to guide researchers from ad-hoc prediction toward principled evaluation. By mandating a coherent, end-to-end methodology that emphasizes stage-aware problem definition, theory-informed data synthesis, principled feature engineering, rigorous validation, and risk-aware interpretation, the SAISE framework provides a new standard for conducting more comparable, robust, and practically relevant research in this rapidly maturing domain
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Space Diffusion for Topology Optimization</title>
<link>https://arxiv.org/abs/2508.05624</link>
<guid>https://arxiv.org/abs/2508.05624</guid>
<content:encoded><![CDATA[
<div> latent diffusion models, variational autoencoders, topology optimization, material distribution, generative process

Summary:<br />
This study introduces a novel framework that combines latent diffusion models (LDMs) with variational autoencoders (VAEs) for efficient topology optimization. The method allows for fast generation of optimized structures by conditioning the generative process on physically meaningful fields such as von Mises stress, strain energy density, volume fraction, and loading information. To improve design quality, auxiliary loss functions are introduced to penalize floating material, load imbalance, and volume fraction deviation, promoting realistic and manufacturable designs. Numerical experiments on a synthetic dataset demonstrate the framework's superior performance in compliance accuracy, volume control, and structural connectivity compared to existing diffusion-based methods. This approach offers a scalable alternative to traditional gradient-based methods, addressing issues of scalability and dimensionality in topology optimization. <br /><br /> <div>
arXiv:2508.05624v1 Announce Type: new 
Abstract: Topology optimization enables the automated design of efficient structures by optimally distributing material within a defined domain. However, traditional gradient-based methods often scale poorly with increasing resolution and dimensionality due to the need for repeated finite element analyses and sensitivity evaluations. In this work, we propose a novel framework that combines latent diffusion models (LDMs) with variational autoencoders (VAEs) to enable fast, conditional generation of optimized topologies. Unlike prior approaches, our method conditions the generative process on physically meaningful fields, specifically von Mises stress, strain energy density, volume fraction, and loading information, embedded as dense input channels. To further guide the generation process, we introduce auxiliary loss functions that penalize floating material, load imbalance, and volume fraction deviation, thereby encouraging physically realistic and manufacturable designs. Numerical experiments on a large synthetic dataset demonstrate that our VAE-LDM framework outperforms existing diffusion-based methods in compliance accuracy, volume control, and structural connectivity, providing a robust and scalable alternative to conventional
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle</title>
<link>https://arxiv.org/abs/2508.04755</link>
<guid>https://arxiv.org/abs/2508.04755</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, dynamic treatment regimes, large language models, implicit prior knowledge, clinical heuristics<br />
Summary: This study evaluates the use of large language models (LLMs) as dynamic insulin dosing agents in Type 1 diabetes treatment. LLMs demonstrate comparable performance to neural network-based agents when provided with carefully designed prompts. However, LLMs show limitations such as aggressive dosing due to reasoning errors like arithmetic hallucination and temporal misinterpretation. Explicit reasoning about latent states does not significantly improve performance. The study suggests cautious integration of LLMs into clinical workflows, emphasizing the need for targeted prompt engineering and validation. Hybrid approaches combining linguistic reasoning with structured physiological modeling may offer more effective decision-support systems.<br /><br />Summary: <div>
arXiv:2508.04755v1 Announce Type: cross 
Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold promise for automating complex clinical decision-making, yet their practical deployment remains hindered by the intensive engineering required to inject clinical knowledge and ensure patient safety. Recent advancements in large language models (LLMs) suggest a complementary approach, where implicit prior knowledge and clinical heuristics are naturally embedded through linguistic prompts without requiring environment-specific training. In this study, we rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in silico Type 1 diabetes simulator, comparing their zero-shot inference performance against small neural network-based RL agents (SRAs) explicitly trained for the task. Our results indicate that carefully designed zero-shot prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or superior clinical performance relative to extensively trained SRAs, particularly in stable patient cohorts. However, LLMs exhibit notable limitations, such as overly aggressive insulin dosing when prompted with chain-of-thought (CoT) reasoning, highlighting critical failure modes including arithmetic hallucination, temporal misinterpretation, and inconsistent clinical logic. Incorporating explicit reasoning about latent clinical states (e.g., meals) yielded minimal performance gains, underscoring the current model's limitations in capturing complex, hidden physiological dynamics solely through textual inference. Our findings advocate for cautious yet optimistic integration of LLMs into clinical workflows, emphasising the necessity of targeted prompt engineering, careful validation, and potentially hybrid approaches that combine linguistic reasoning with structured physiological modelling to achieve safe, robust, and clinically effective decision-support systems.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations</title>
<link>https://arxiv.org/abs/2508.05020</link>
<guid>https://arxiv.org/abs/2508.05020</guid>
<content:encoded><![CDATA[
<div> AMR, Regent, Legion programming model, high-order solvers, compressible flows<br />
Summary:<br />
This study focuses on developing an adaptive mesh refinement (AMR) numerical solver using Regent, a high-level programming language designed for the Legion programming model. The implementation addresses challenges such as dynamic data structures for patch refinement/coarsening, mesh validity enforcement, and reducing task launch overhead through task fusion. Experimental results demonstrate significant speedups achieved with task fusion and automated GPU kernel generation using simple annotations. The approach is validated through simulations of two compressible flow problems governed by the Euler equations. Overall, the study showcases the effectiveness of using Regent and AMR for high-order solvers in scientific applications. <div>
arXiv:2508.05020v1 Announce Type: cross 
Abstract: High-order solvers for compressible flows are vital in scientific applications. Adaptive mesh refinement (AMR) is a key technique for reducing computational cost by concentrating resolution in regions of interest. In this work, we develop an AMR-based numerical solver using Regent, a high-level programming language for the Legion programming model. We address several challenges associated with implementing AMR in Regent. These include dynamic data structures for patch refinement/coarsening, mesh validity enforcement, and reducing task launch overhead via task fusion. Experimental results show that task fusion achieves 18x speedup, while automated GPU kernel generation via simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate our approach through simulations of two canonical compressible flow problems governed by the Euler equations.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Echo State Networks for Bitcoin Time Series Prediction</title>
<link>https://arxiv.org/abs/2508.05416</link>
<guid>https://arxiv.org/abs/2508.05416</guid>
<content:encoded><![CDATA[
<div> Keywords: stock prices, cryptocurrency prices, Echo State Networks, chaos analysis, machine learning methods

Summary:
Forecasting stock and cryptocurrency prices is a challenging task due to their high volatility and non-stationarity, influenced by various factors such as economic changes and market sentiment. This study investigates the use of Echo State Networks (ESNs) for predicting cryptocurrency prices, particularly during periods of extreme volatility. Results show that ESNs outperform other machine learning methods significantly, especially during chaotic periods, as reflected in the Lyapunov exponent analysis. The research demonstrates the robustness of ESNs during turbulent market conditions and their superior performance compared to Boosting and Nave methods. The findings suggest that ESNs are well-suited for capturing nonlinear patterns in dynamic data and can be effective tools for short-term forecasting of both stock and cryptocurrency prices. 

Summary: <div>
arXiv:2508.05416v1 Announce Type: cross 
Abstract: Forecasting stock and cryptocurrency prices is challenging due to high volatility and non-stationarity, influenced by factors like economic changes and market sentiment. Previous research shows that Echo State Networks (ESNs) can effectively model short-term stock market movements, capturing nonlinear patterns in dynamic data. To the best of our knowledge, this work is among the first to explore ESNs for cryptocurrency forecasting, especially during extreme volatility. We also conduct chaos analysis through the Lyapunov exponent in chaotic periods and show that our approach outperforms existing machine learning methods by a significant margin. Our findings are consistent with the Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods and excel under high chaos compared to Boosting and Na\"ive methods.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Variance Efficient Collaborative Filtering for Stock Recommendation</title>
<link>https://arxiv.org/abs/2306.06590</link>
<guid>https://arxiv.org/abs/2306.06590</guid>
<content:encoded><![CDATA[
<div> efficient collaborative filtering, stock recommendations, mean-variance, portfolio theory, personalized recommendations 
Summary:
The article introduces a novel mean-variance efficient collaborative filtering (MVECF) model for personalized stock recommendations that consider both user preferences and the risk-return characteristics of stocks. Traditional recommendation methods often overlook user preferences and focus solely on high-return stocks or diversified portfolios. The MVECF model aims to optimize the trade-off between risk and return by incorporating uncertainties in stock prices through regularization techniques. By enhancing the mean-variance efficiency of suggested portfolios, the MVECF model demonstrates improved performance while maintaining high average precision and recall. The model is designed for computational efficiency and can easily integrate with graph-based ranking models, making it a valuable tool for financial services in the era of FinTech. <br /><br />Summary: <div>
arXiv:2306.06590v2 Announce Type: replace-cross 
Abstract: The rise of FinTech has transformed financial services onto online platforms, yet stock investment recommender systems have received limited attention compared to other industries. Personalized stock recommendations can significantly impact customer engagement and satisfaction within the industry. However, traditional investment recommendations focus on high-return stocks or highly diversified portfolios based on the modern portfolio theory, often neglecting user preferences. On the other hand, collaborative filtering (CF) methods also may not be directly applicable to stock recommendations, because it is inappropriate to just recommend stocks that users like. The key is to optimally blend users preference with the portfolio theory. However, research on stock recommendations within the recommender system domain remains comparatively limited, and no existing model considers both the preference of users and the risk-return characteristics of stocks. In this regard, we propose a mean-variance efficient collaborative filtering (MVECF) model for stock recommendations that consider both aspects. Our model is specifically designed to improve the pareto optimality (mean-variance efficiency) in a trade-off between the risk (variance of return) and return (mean return) by systemically handling uncertainties in stock prices. Such improvements are incorporated into the MVECF model using regularization, and the model is restructured to fit into the ordinary matrix factorization scheme to boost computational efficiency. Experiments on real-world fund holdings data show that our model can increase the mean-variance efficiency of suggested portfolios while sacrificing just a small amount of mean average precision and recall. Finally, we further show MVECF is easily applicable to the state-of-the-art graph-based ranking models.
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tunable Plasmonic Absorption in Metal-Dielectric Multilayers via FDTD Simulations and an Explainable Machine Learning Approach</title>
<link>https://arxiv.org/abs/2508.04014</link>
<guid>https://arxiv.org/abs/2508.04014</guid>
<content:encoded><![CDATA[
<div> plasmonic devices, nanophotonics, machine learning, absorption behavior, multilayer systems
Summary:
- The study combines finite-difference time-domain simulations with machine learning to predict absorbed power behavior in multilayer plasmonic stacks.
- Varying Au and Ag thicknesses across a spectral range, spatial absorption maps and power metrics are generated.
- A multilayer perceptron and convolutional neural network models absorption behavior with high accuracy.
- Plasmonic layer thickness and excitation wavelength are identified as dominant contributors to absorption.
- Gold exhibits broader and sustained absorption compared to silver, with efficiency peaking between 450 and 850 nm.
<br /><br />Summary: <div>
arXiv:2508.04014v1 Announce Type: new 
Abstract: Plasmonic devices, fundamental to modern nanophotonics, exploit resonant interactions between light and free electrons in metals to achieve enhanced light trapping and electromagnetic field confinement. However, modeling their complex, nonlinear optical responses remains computationally intensive. In this work, we combine finite-difference time-domain simulations with machine learning to simulate and predict absorbed power behavior in multilayer plasmonic stacks composed of SiO2, gold, silver, and indium tin oxide. By varying Au and Ag thicknesses (10-50nm) across a spectral range of 300-1500nm, we generate spatial absorption maps and integrated power metrics from full-wave solutions to Maxwell's equations. A multilayer perceptron models global absorption behavior with a mean absolute error of 0.0953, while a convolutional neural network predicts spatial absorption distributions with an MAE of 0.0101. SHapley Additive exPlanations identify plasmonic layer thickness and excitation wavelength as dominant contributors to absorption, which peaks between 450 and 850~nm. Gold demonstrates broader and more sustained absorption compared to silver, although both metals exhibit reduced efficiency outside the resonance window. This integrated FDTD-ML framework offers a fast, explainable, and accurate approach for investigating tunable plasmonic behavior in multilayer systems, with applications in optical sensing, photovoltaics, and nanophotonic device design.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A GPU-Accelerated Three-Dimensional Crack Element Method for Transient Dynamic Fracture Simulation</title>
<link>https://arxiv.org/abs/2508.04076</link>
<guid>https://arxiv.org/abs/2508.04076</guid>
<content:encoded><![CDATA[
<div> Crack Element Method, dynamic crack propagation, quasi-brittle materials, element-splitting algorithm, fracture energy release rate<br />
Summary: <br />
This work introduces a novel three-dimensional Crack Element Method (CEM) for efficiently modeling transient dynamic crack propagation in quasi-brittle materials. The CEM features an advanced element-splitting algorithm that allows for element-wise crack growth and branching. A new formulation for calculating the fracture energy release rate in three dimensions is developed based on the evolving topology of split elements. The proposed 3D CEM is demonstrated to accurately simulate both single crack propagation and complex crack branching scenarios through a series of benchmark examples. Additionally, all three-dimensional simulations are GPU-accelerated, ensuring high levels of computational efficiency, consistency, and accuracy. <div>
arXiv:2508.04076v1 Announce Type: new 
Abstract: This work presents a novel three-dimensional Crack Element Method (CEM) designed to model transient dynamic crack propagation in quasi-brittle materials efficiently. CEM introduces an advanced element-splitting algorithm that enables element-wise crack growth, including crack branching. Based on the evolving topology of split elements, an original formulation for computing the fracture energy release rate in three dimensions is derived. A series of benchmark examples is conducted to demonstrate that the proposed 3D CEM accurately simulates both single crack propagation and complex crack branching scenarios. Furthermore, all three-dimensional simulations are GPU-accelerated, achieving high levels of computational efficiency, consistency, and accuracy.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convolutional autoencoders for the reconstruction of three-dimensional interfacial multiphase flows</title>
<link>https://arxiv.org/abs/2508.04084</link>
<guid>https://arxiv.org/abs/2508.04084</guid>
<content:encoded><![CDATA[
<div> Keywords: autoencoders, reduced-order modeling, multiphase flows, convolutional architecture, interface representation

Summary:<br />
This study explores the use of autoencoders for reduced-order modeling of three-dimensional multiphase flows. The accuracy of reconstructing multiphase flow volume and mass fractions using a standard convolutional architecture is investigated, considering different interface representations such as diffuse, sharp, and level set. The research utilizes synthetic data with complex interface topologies and high-resolution simulation data of multiphase homogeneous isotropic turbulence for training and validation purposes. The findings establish best practices for reducing the dimensionality of multiphase flows with autoencoders, paving the way for separate training of accurate reconstruction and temporal or input/output models on the lower-dimensional latent space. This presents significant implications for the multiphase flow community and beyond, enabling advancements in modeling and understanding complex fluid dynamics efficiently. 

<br /><br />Summary: <div>
arXiv:2508.04084v1 Announce Type: new 
Abstract: In this work, we perform a comprehensive investigation of autoencoders for reduced-order modeling of three-dimensional multiphase flows. Focusing on the accuracy of reconstructing multiphase flow volume/mass fractions with a standard convolutional architecture, we examine the advantages and disadvantages of different interface representation choices (diffuse, sharp, level set). We use a combination of synthetic data with non-trivial interface topologies and high-resolution simulation data of multiphase homogeneous isotropic turbulence for training and validation. This study clarifies the best practices for reducing the dimensionality of multiphase flows via autoencoders. Consequently, this paves the path for uncoupling the training of autoencoders for accurate reconstruction and the training of temporal or input/output models such as neural operators (e.g., FNOs, DeepONets) and neural ODEs on the lower-dimensional latent space given by the autoencoders. As such, the implications of this study are significant and of interest to the multiphase flow community and beyond.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generic Framework for Optimization in Blockchain Simulators</title>
<link>https://arxiv.org/abs/2508.04157</link>
<guid>https://arxiv.org/abs/2508.04157</guid>
<content:encoded><![CDATA[
<div> Keywords: blockchain, simulation, optimization, warm starting technique, concurrent multiprocessing<br />
Summary: 
The paper introduces the Generic Framework for Optimization in Blockchain Simulators (GFOBS), a tool created to standardize and optimize blockchain simulations. GFOBS is designed to support various optimization algorithms, variables, and objectives, catering to a wide range of blockchain research needs. The key contributions of the paper include the development of GFOBS as a versatile tool, an innovative optimization method utilizing warm starting technique, and a novel concurrent multiprocessing technique for simultaneous simulation processes. These advancements aim to enhance the efficiency, replicability, and standardization of blockchain simulation experiments. The authors address the challenge of diverse and non-standardized simulation parameters that hinder the replicability and comparability of research methodologies in the rapidly evolving blockchain technology landscape. GFOBS provides a flexible platform for researchers to conduct blockchain simulations more effectively and efficiently. 

<br /><br />Summary: <div>
arXiv:2508.04157v1 Announce Type: new 
Abstract: As blockchain technology rapidly evolves, researchers face a significant challenge due to diverse and non-standardized simulation parameters, which hinder the replicability and comparability of research methodologies. This paper introduces a Generic Framework for Optimization in Blockchain Simulators (GFOBS), a comprehensive and adaptable solution designed to standardize and optimize blockchain simulations. GFOBS provides a flexible platform that supports various optimization algorithms, variables, and objectives, thereby catering to a wide range of blockchain research needs. The paper's key contributions are threefold: the development of GFOBS as a versatile tool for blockchain simulation optimization; the introduction of an innovative optimization method using warm starting technique; and the proposition of a novel concurrent multiprocessing technique for simultaneous simulation processes. These advancements collectively enhance the efficiency, replicability, and standardization of blockchain simulation experiments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Method-Based Reasoning for Large Language Models: Extraction, Reuse, and Continuous Improvement</title>
<link>https://arxiv.org/abs/2508.04289</link>
<guid>https://arxiv.org/abs/2508.04289</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, reasoning process, method-based model, continual learning, logical consistency

Summary: 
Large language models (LLMs) have shown impressive capabilities in various language tasks but are limited by their reliance on statistical patterns. To address this, a method-based model is proposed, augmenting LLMs with explicit procedures extracted from training data, responses, and user interactions. These methods are stored externally, ranked based on feedback, and retrieved to guide the LLM's response to new queries. The model enables continual learning, method reuse, and logical consistency beyond token prediction. Experimental results show improved factual verification and generalization in complex prompts. Furthermore, newly learned methods can surpass earlier ones through user-driven refinement. The approach shows promise in enhancing the reasoning abilities of LLMs by incorporating reusable procedures to handle novel problems and improve logical reasoning. 

<br /><br />Summary: <div>
arXiv:2508.04289v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown impressive capabilities across a wide range of language tasks. However, their reasoning process is primarily guided by statistical patterns in training data, which limits their ability to handle novel problems and perform consistent logical reasoning. In this paper, we propose a method-based model that enhances LLMs with explicit, reusable procedures extracted from training content, generated responses, and user interactions. Each method is represented as a pair consisting of a problem and its corresponding solution, stored externally and ranked based on feedback. When a new query is received, the system retrieves and applies the most relevant methods to guide the LLM's response. Our model enables continual learning, method reuse, and logical consistency beyond next-token prediction. Experimental results demonstrate that the system improves factual verification and generalization in complex prompts, and that newly learned methods can outperform earlier ones through user-driven refinement.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extreme Event Precursor Prediction in Turbulent Dynamical Systems via CNN-Augmented Recurrence Analysis</title>
<link>https://arxiv.org/abs/2508.04301</link>
<guid>https://arxiv.org/abs/2508.04301</guid>
<content:encoded><![CDATA[
<div> framework, predict, precursors, extreme events, turbulent dynamical systems 

Summary:
- A general framework is presented to predict precursors to extreme events in turbulent dynamical systems.
- The approach combines phase-space reconstruction techniques with recurrence matrices and convolutional neural networks.
- Three testbed systems were evaluated: a triad turbulent interaction model, a stochastic anisotropic turbulent flow, and the Kolmogorov flow.
- The method offers a threshold-free classification strategy, efficient training with a small number of recurrence matrices, and generalizability to unseen systems.
- Results show robust predictive performance with detection rates of 96% for the triad model, 96% for the anisotropic turbulent flow, and 93% for the Kolmogorov flow, with varying mean lead times. 

<br /><br />Summary: <div>
arXiv:2508.04301v1 Announce Type: new 
Abstract: We present a general framework to predict precursors to extreme events in turbulent dynamical systems. The approach combines phase-space reconstruction techniques with recurrence matrices and convolutional neural networks to identify precursors to extreme events. We evaluate the framework across three distinct testbed systems: a triad turbulent interaction model, a prototype stochastic anisotropic turbulent flow, and the Kolmogorov flow. This method offers three key advantages: (1) a threshold-free classification strategy that eliminates subjective parameter tuning, (2) efficient training using only $\mathcal{O}(100)$ recurrence matrices, and (3) ability to generalize to unseen systems. The results demonstrate robust predictive performance across all test systems: 96\% detection rate for the triad model with a mean lead time of 1.8 time units, 96\% for the anisotropic turbulent flow with a mean lead time of 6.1 time units, and 93\% for the Kolmogorov flow with a mean lead time of 22.7 units.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation</title>
<link>https://arxiv.org/abs/2508.04306</link>
<guid>https://arxiv.org/abs/2508.04306</guid>
<content:encoded><![CDATA[
<div> Keywords: literature reviews, large language models, automated systems, Multi-Agent Taskforce Collaboration, benchmark dataset 

Summary: 
The article discusses the importance of literature reviews in scientific research and the role of large language models (LLMs) in automating the literature review process. The Multi-Agent Taskforce Collaboration (MATC) framework is proposed to address challenges of compounding errors in the review workflow. MATC consists of a manager agent and four executor agents for different tasks. Three collaboration paradigms are introduced to organize agents effectively and mitigate errors. Experimental results show that MATC outperforms existing benchmarks and a new dataset with diverse topics is introduced for literature review generation. The framework aims to improve the faithfulness and quality of automated literature reviews. <div>
arXiv:2508.04306v1 Announce Type: new 
Abstract: Literature reviews play an important role in scientific research. Recent advances in large language models (LLMs) have boosted the development of automated systems for the entire literature review workflow, from retrieval to manuscript drafting. However, a key challenge is that mistakes made in early stages can propagate and amplify in subsequent steps, leading to compounding errors that undermine the faithfulness of the final review. To tackle this issue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework, which consists of a manager agent and four executor agents for literature searching, outline generation, fact localization, and manuscript drafting. We propose three novel collaboration paradigms, forming exploration, exploitation, and experience taskforces, to effectively organize agents and mitigate compounding errors both between and within executor agents. Experimental results show that MATC achieves state-of-the-art performance on existing benchmarks. We further propose a new benchmark dataset featuring more diverse topics for faithful literature review generation.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing Large Language Models with PCA Without Performance Loss</title>
<link>https://arxiv.org/abs/2508.04307</link>
<guid>https://arxiv.org/abs/2508.04307</guid>
<content:encoded><![CDATA[
<div> Keywords: Principal Component Analysis, extreme compression, neural models, transformer, token sequences

Summary:<br />
- The study demonstrates that applying Principal Component Analysis (PCA) in a structured manner to polar-transformed images or token sequences allows for extreme compression of neural models while maintaining performance.
- A one-layer classifier trained on PCA-compressed polar MNIST achieves over 98 percent accuracy with only 840 parameters.
- A two-layer transformer utilizing 70-dimensional PCA-reduced MiniLM embeddings achieves 76.62 percent accuracy on the 20 Newsgroups dataset with just 81000 parameters.
- A decoder-only transformer generates coherent token sequences from 70-dimensional PCA embeddings, preserving over 97 percent cosine similarity with full MiniLM representations while using less than 17 percent of the parameter count of GPT-2.
- These findings underscore the effectiveness of PCA-based input compression as a strategy to align model capacity with information content, facilitating the development of lightweight architectures across various modalities.

<br /><br />Summary: <div>
arXiv:2508.04307v1 Announce Type: new 
Abstract: We demonstrate that Principal Component Analysis (PCA), when applied in a structured manner, either to polar-transformed images or segment-wise to token sequences, enables extreme compression of neural models without sacrificing performance. Across three case studies, we show that a one-layer classifier trained on PCA-compressed polar MNIST achieves over 98 percent accuracy using only 840 parameters. A two-layer transformer trained on 70-dimensional PCA-reduced MiniLM embeddings reaches 76.62 percent accuracy on the 20 Newsgroups dataset with just 81000 parameters. A decoder-only transformer generates coherent token sequences from 70-dimensional PCA embeddings while preserving over 97 percent cosine similarity with full MiniLM representations, using less than 17 percent of the parameter count of GPT-2. These results highlight PCA-based input compression as a general and effective strategy for aligning model capacity with information content, enabling lightweight architectures across multiple modalities.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Simulation and Experiment: A Self-Supervised Domain Adaptation Framework for Concrete Damage Classification</title>
<link>https://arxiv.org/abs/2508.04538</link>
<guid>https://arxiv.org/abs/2508.04538</guid>
<content:encoded><![CDATA[
<div> Keywords: concrete degradation, coda wave signals, domain adaptation, ultrasonic wave propagation simulations, neural networks

Summary: 
The study introduces a self-supervised domain adaptation framework for accurate concrete damage classification using coda wave signals. A virtual testing platform is developed for generating large-scale labeled synthetic data to reduce reliance on costly experimental labeling. The framework integrates domain adversarial training, minimum class confusion loss, and the BYOL strategy to bridge the domain gap between simulation and experimental data. Extensive experiments demonstrate notable performance improvements, with an accuracy of 0.7762 and a macro F1 score of 0.7713, outperforming baseline methods and domain adaptation techniques. The framework exhibits high robustness and minimal additional computational cost, showcasing its practical potential for structural health monitoring applications. 

<br /><br />Summary: <div>
arXiv:2508.04538v1 Announce Type: new 
Abstract: Reliable assessment of concrete degradation is critical for ensuring structural safety and longevity of engineering structures. This study proposes a self-supervised domain adaptation framework for robust concrete damage classification using coda wave signals. To support this framework, an advanced virtual testing platform is developed, combining multiscale modeling of concrete degradation with ultrasonic wave propagation simulations. This setup enables the generation of large-scale labeled synthetic data under controlled conditions, reducing the dependency on costly and time-consuming experimental labeling. However, neural networks trained solely on synthetic data often suffer from degraded performance when applied to experimental data due to domain shifts. To bridge this domain gap, the proposed framework integrates domain adversarial training, minimum class confusion loss, and the Bootstrap Your Own Latent (BYOL) strategy. These components work jointly to facilitate effective knowledge transfer from the labeled simulation domain to the unlabeled experimental domain, achieving accurate and reliable damage classification in concrete. Extensive experiments demonstrate that the proposed method achieves notable performance improvements, reaching an accuracy of 0.7762 and a macro F1 score of 0.7713, outperforming both the plain 1D CNN baseline and six representative domain adaptation techniques. Moreover, the method exhibits high robustness across training runs and introduces only minimal additional computational cost. These findings highlight the practical potential of the proposed simulation-driven and label-efficient framework for real-world applications in structural health monitoring.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoK: Stablecoins for Digital Transformation -- Design, Metrics, and Application with Real World Asset Tokenization as a Case Study</title>
<link>https://arxiv.org/abs/2508.02403</link>
<guid>https://arxiv.org/abs/2508.02403</guid>
<content:encoded><![CDATA[
<div> taxonomy, stablecoin systems, performance evaluation framework, digital systems, Real World Asset tokenization

Summary: 
This study addresses the fragmented academic research on stablecoins by providing a unified taxonomy based on custodial structure, stabilization mechanism, and governance. It also introduces a comprehensive performance evaluation framework tailored to diverse stakeholder needs and offers transparency through an open-source benchmarking pipeline. Additionally, a case study on Real World Asset tokenization demonstrates how stablecoins function as programmable monetary infrastructure in cross-border digital systems. By combining conceptual theory with empirical tools, the paper contributes to the development of trusted, inclusive, and transparent digital monetary infrastructure. This research aims to bridge the gap between economics, law, and computer science in the study of stablecoins and provide a solid foundation for future research in this area. <br /><br />Summary: <div>
arXiv:2508.02403v1 Announce Type: cross 
Abstract: Stablecoins have become a foundational component of the digital asset ecosystem, with their market capitalization exceeding 230 billion USD as of May 2025. As fiat-referenced and programmable assets, stablecoins provide low-latency, globally interoperable infrastructure for payments, decentralized finance, DeFi, and tokenized commerce. Their accelerated adoption has prompted extensive regulatory engagement, exemplified by the European Union's Markets in Crypto-assets Regulation, MiCA, the US Guiding and Establishing National Innovation for US Stablecoins Act, GENIUS Act, and Hong Kong's Stablecoins Bill. Despite this momentum, academic research remains fragmented across economics, law, and computer science, lacking a unified framework for design, evaluation, and application. This study addresses that gap through a multi-method research design. First, it synthesizes cross-disciplinary literature to construct a taxonomy of stablecoin systems based on custodial structure, stabilization mechanism, and governance. Second, it develops a performance evaluation framework tailored to diverse stakeholder needs, supported by an open-source benchmarking pipeline to ensure transparency and reproducibility. Third, a case study on Real World Asset tokenization illustrates how stablecoins operate as programmable monetary infrastructure in cross-border digital systems. By integrating conceptual theory with empirical tools, the paper contributes: a unified taxonomy for stablecoin design; a stakeholder-oriented performance evaluation framework; an empirical case linking stablecoins to sectoral transformation; and reproducible methods and datasets to inform future research. These contributions support the development of trusted, inclusive, and transparent digital monetary infrastructure.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification</title>
<link>https://arxiv.org/abs/2508.03750</link>
<guid>https://arxiv.org/abs/2508.03750</guid>
<content:encoded><![CDATA[
<div> Keywords: glaucoma, risk prediction, multimodal, interpretability, XGBoost

Summary: 
GlaBoost is a novel multimodal gradient boosting framework designed for early and accurate detection of glaucoma. It integrates structured clinical features, fundus image embeddings, and textual descriptions for glaucoma risk prediction. GlaBoost utilizes pretrained convolutional encoders for visual representation extraction from retinal fundus photos and transformer-based language models for encoding neuroretinal rim assessments. By combining these heterogeneous signals with manually assessed risk scores and ophthalmic indicators, GlaBoost creates a unified feature space for classification using an enhanced XGBoost model. Experimental results on a real-world dataset show that GlaBoost outperforms baseline models with a validation accuracy of 98.71%. Feature importance analysis highlights the significant contributions of cup-to-disc ratio, rim pallor, and textual embeddings in model decisions. GlaBoost offers a transparent and scalable solution for interpretable glaucoma diagnosis and has the potential for extension to other ophthalmic disorders.<br /><br />Summary: <div>
arXiv:2508.03750v1 Announce Type: cross 
Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible vision loss. However, existing methods often rely on unimodal data and lack interpretability, limiting their clinical utility. In this paper, we present GlaBoost, a multimodal gradient boosting framework that integrates structured clinical features, fundus image embeddings, and expert-curated textual descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual representations from retinal fundus photographs using a pretrained convolutional encoder and encodes free-text neuroretinal rim assessments using a transformer-based language model. These heterogeneous signals, combined with manually assessed risk scores and quantitative ophthalmic indicators, are fused into a unified feature space for classification via an enhanced XGBoost model. Experiments conducted on a real-world annotated dataset demonstrate that GlaBoost significantly outperforms baseline models, achieving a validation accuracy of 98.71%. Feature importance analysis reveals clinically consistent patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings contributing most to model decisions. GlaBoost offers a transparent and scalable solution for interpretable glaucoma diagnosis and can be extended to other ophthalmic disorders.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations</title>
<link>https://arxiv.org/abs/2508.03839</link>
<guid>https://arxiv.org/abs/2508.03839</guid>
<content:encoded><![CDATA[
<div> surrogate model, trainable by parts, parameterized nonlinear PDEs, encoder, latent space

Summary: 
The article introduces a new trainable-by-parts surrogate model for solving forward and inverse parameterized nonlinear partial differential equations. The model consists of an encoder to reduce input dimensionality, a neural network to map to the solution space, and a decoder for reconstruction. The key innovation is the independent training of the model components, leading to decreased time and energy requirements compared to existing models like FNO and DeepONet. The model, named VAE-DNN, is evaluated on solving the nonlinear diffusion equation for groundwater flow, demonstrating higher efficiency and accuracy in both forward and inverse solutions. The separable training approach through a variational autoencoder framework enhances the overall performance of the model. <div>
arXiv:2508.03839v1 Announce Type: cross 
Abstract: We propose a trainable-by-parts surrogate model for solving forward and inverse parameterized nonlinear partial differential equations. Like several other surrogate and operator learning models, the proposed approach employs an encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct $h(\bm{x},t)$. The innovative aspect of our model is its ability to train its three components independently. This approach leads to a substantial decrease in both the time and energy required for training when compared to leading operator learning models such as FNO and DeepONet. The separable training is achieved by training the encoder as part of the variational autoencoder (VAE) for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet models for obtaining forward and inverse solutions to the nonlinear diffusion equation governing groundwater flow in an unconfined aquifer. Our findings indicate that VAE-DNN not only demonstrates greater efficiency but also delivers superior accuracy in both forward and inverse solutions compared to the FNO and DeepONet models.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging</title>
<link>https://arxiv.org/abs/2508.04625</link>
<guid>https://arxiv.org/abs/2508.04625</guid>
<content:encoded><![CDATA[
<div> benchmark, financial reasoning, multimodal, numerical reasoning, large language models

Summary:
- FinMMR is a new benchmark designed to evaluate the reasoning abilities of multimodal large language models in financial numerical tasks.
- It introduces multimodality by incorporating images in addition to text-based questions, covering 14 categories in the financial domain.
- The benchmark is comprehensive, spanning 14 financial subdomains such as corporate finance and banking, surpassing existing benchmarks.
- The challenge lies in requiring models to perform precise numerical reasoning by combining financial knowledge with understanding complex financial images and text.
- The best-performing model achieves 53.0% accuracy on difficult problems, indicating the need for advancement in this area.

<br /><br />Summary: <div>
arXiv:2508.04625v1 Announce Type: cross 
Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Deep Reinforcement Learning Algorithms for Portfolio Optimisation</title>
<link>https://arxiv.org/abs/2307.07694</link>
<guid>https://arxiv.org/abs/2307.07694</guid>
<content:encoded><![CDATA[
<div> algorithm, portfolio optimisation, deep reinforcement learning, Kelly criterion, market impact

Summary:
- The study assessed various deep reinforcement learning algorithms for portfolio optimization using simulated data.
- The simulator utilized correlated geometric Brownian motion with the Bertsimas-Lo market impact model to generate data.
- By applying the Kelly criterion as the objective, the optimal policy without market impact was analytically derived, serving as a performance benchmark.
- Off-policy algorithms like DDPG, TD3, and SAC struggled to learn the correct $Q$-function due to noisy rewards, leading to inferior performance.
- On-policy algorithms PPO and A2C, alongside generalised advantage estimation, effectively managed noise and generated policies close to optimal. The clipping variant of PPO was crucial in maintaining policy convergence. In a more complex setting with changing GBM parameters, PPO combined with a hidden Markov model successfully learned and adapted policies to different regimes. However, the algorithms demonstrated high sample complexity, requiring substantial steps for effective learning in real data applications. <br /><br />Summary: <div>
arXiv:2307.07694v3 Announce Type: replace 
Abstract: We evaluate benchmark deep reinforcement learning algorithms on the task of portfolio optimisation using simulated data. The simulator to generate the data is based on correlated geometric Brownian motion with the Bertsimas-Lo market impact model. Using the Kelly criterion (log utility) as the objective, we can analytically derive the optimal policy without market impact as an upper bound to measure performance when including market impact. We find that the off-policy algorithms DDPG, TD3 and SAC are unable to learn the right $Q$-function due to the noisy rewards and therefore perform poorly. The on-policy algorithms PPO and A2C, with the use of generalised advantage estimation, are able to deal with the noise and derive a close to optimal policy. The clipping variant of PPO was found to be important in preventing the policy from deviating from the optimal once converged. In a more challenging environment where we have regime changes in the GBM parameters, we find that PPO, combined with a hidden Markov model to learn and predict the regime context, is able to learn different policies adapted to each regime. Overall, we find that the sample complexity of these algorithms is too high for applications using real data, requiring more than 2m steps to learn a good policy in the simplest setting, which is equivalent to almost 8,000 years of daily prices.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Based Insight into Eco-Choices: Simulating the Fast Fashion Shift</title>
<link>https://arxiv.org/abs/2407.18814</link>
<guid>https://arxiv.org/abs/2407.18814</guid>
<content:encoded><![CDATA[
<div> Keywords: fashion, fast fashion, Spain, consumer behavior, Agent-Based Modeling

Summary: 
The study focuses on the impact of fast fashion on the environment and society, particularly in Spain. It highlights the detrimental effects of fast fashion, such as waste and human rights abuses, while also acknowledging its economic significance. Through Agent-Based Modeling, the research examines individual decision-making processes in purchasing fast fashion and the influence of awareness on sustainable fashion practices. The study emphasizes the role of government interventions in shaping consumer behavior, with campaigns playing a crucial role in driving progress. However, the success of such interventions is dependent on factors like social media influence and public polarization. The research suggests that a balanced approach by the government, along with targeted social media strategies, can lead to more significant shifts in consumer habits towards sustainable fashion choices. <div>
arXiv:2407.18814v2 Announce Type: replace 
Abstract: Fashion is a powerful force in the modern world. It is one of the most accessible means of self-expression, thereby playing a significant role in our society. Yet, it is plagued by well-documented issues of waste and human rights abuses. Fast fashion in particular, characterized by its disposable nature, contributes extensively to environmental degradation and CO$_2$ emissions, surpassing the combined outputs of France, Germany, and the UK, but its economic contributions have somewhat shielded it from criticism. In this paper, we examine the demand for fast fashion, with a focus on Spain. We explore the individual decision-making process involved in choosing to buy fast fashion and the role of awareness regarding working conditions, environmental consequences, and education on sustainable fashion in influencing consumer behavior. By employing Agent-Based Modeling, we investigate the factors influencing garment consumption patterns and how shifts in public opinion can be achieved through peer pressure, social media influence, and government interventions. Our study revealed that government interventions are pivotal, with the state's campaigns setting the overall tone for progress, although its success is conditioned by social media and polarization levels of the population. Importantly, the state does not need to adopt an extremely proactive stance or continue the campaigns indefinitely to achieve optimal results, as excessive interventions yield diminishing returns.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A CFL condition for the finite cell method</title>
<link>https://arxiv.org/abs/2502.13675</link>
<guid>https://arxiv.org/abs/2502.13675</guid>
<content:encoded><![CDATA[
<div> boundary-conforming mesh generation, finite element methods, critical time step size, explicit time integration, finite cell method
<br />
Summary: 
The study focuses on the finite cell method's effect on the critical time step size for explicit time integration in immersed wave propagation simulations. By analyzing a one-degree-of-freedom model, the influence of -stabilization on the maximum eigenvalue and critical time step size for corner and sliver cuts was systematically studied. It was found that the critical time step size does not decrease below a limit even as the cut fraction tends to zero, with the lower bound controlled by . Sliver cuts were identified as more detrimental than corner cuts in higher dimensions. Increasing polynomial degree had minimal impact on degradation. An estimate of the minimum critical time step size as a function of  was derived to propose a modified CFL condition for the finite cell method, validated on a two-dimensional perforated plate example. <div>
arXiv:2502.13675v2 Announce Type: replace 
Abstract: Immersed boundary finite element methods allow the user to bypass the potentially troublesome task of boundary-conforming mesh generation. When combined with explicit time integration, poorly cut elements with little support in the physical domain lead to a severely reduced critical time step size, posing a major challenge for immersed wave propagation simulations. The finite cell method stabilizes cut elements by defining the weak form of the problem also in the fictitious domain, but scaled by a small value $\alpha$. This paper investigates the effect of the finite cell method on the critical time step size for explicit time integration. Starting with an analytical one-degree-of-freedom model, we systematically study the influence of $\alpha$-stabilization on the maximum eigenvalue, and thus on the critical time step size, for corner and sliver cuts. The analysis is complemented by a numerical study of an example with one element and increasing polynomial degree, confirming that the critical time step size does not decrease below a certain limit, even as the cut fraction tends to zero. This lower bound is controlled by the choice of $\alpha$. In higher dimensions, sliver cuts are found to be more detrimental than corner cuts, thus determining the minimum critical time step size. Increasing the polynomial degree has only little effect on this degradation. Based on these observations, we derive an estimate of the minimum critical time step size as a function of $\alpha$, which we use to propose a modified CFL condition for the finite cell method. The validity of this condition is demonstrated on a two-dimensional perforated plate example.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Numerical Errors in Quantitative System Analysis With Decision Diagrams</title>
<link>https://arxiv.org/abs/2508.02673</link>
<guid>https://arxiv.org/abs/2508.02673</guid>
<content:encoded><![CDATA[
<div> DDs, state-space explosion problem, probabilistic systems, quantum systems, floating-point numbers <br />
<br />
Matrix-vector multiplication with multi-terminal binary decision diagrams (MTBDDs) is crucial for computing successor states in probabilistic and quantum systems. This paper delves into the numerical stability of this algorithm, as floating-point computations can introduce errors affecting result correctness and DD compression effectiveness. The study demonstrates that the MTBDD matrix-vector multiplication algorithm can be made numerically stable under specific conditions, though real-world MTBDD implementations often fail to meet these criteria. A case study on quantum circuit simulation reveals varying degrees of numerical errors in practice. The research underscores the importance of addressing numerical stability challenges in DD-based approaches for handling probabilistic and quantum systems effectively. <br /><br />Summary: <div>
arXiv:2508.02673v1 Announce Type: new 
Abstract: Decision diagrams (DDs) are a powerful data structure that is used to tackle the state-space explosion problem, not only for discrete systems, but for probabilistic and quantum systems as well. While many of the DDs used in the probabilistic and quantum domains make use of floating-point numbers, this is not without challenges. Floating-point computations are subject to small rounding errors, which can affect both the correctness of the result and the effectiveness of the DD's compression. In this paper, we investigate the numerical stability, i.e. the robustness of an algorithm to small numerical errors, of matrix-vector multiplication with multi-terminal binary decision diagrams (MTBDDs). Matrix-vector multiplication is of particular interest because it is the function that computes successor states for both probabilistic and quantum systems. We prove that the MTBDD matrix-vector multiplication algorithm can be made numerically stable under certain conditions, although in many practical implementations of MTBDDs these conditions are not met. Additionally, we provide a case study of the numerical errors in the simulation of quantum circuits, which shows that the extent of numerical errors in practice varies greatly between instances.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming the Loss Conditioning Bottleneck in Optimization-Based PDE Solvers: A Novel Well-Conditioned Loss Function</title>
<link>https://arxiv.org/abs/2508.02692</link>
<guid>https://arxiv.org/abs/2508.02692</guid>
<content:encoded><![CDATA[
<div> optimization, PDE solvers, loss function, Stabilized Gradient Residual, convergence

Summary:
The article introduces a new Stabilized Gradient Residual (SGR) loss for optimization-based PDE solvers that aims to address the slow convergence issues associated with the mean squared error (MSE) loss. The SGR loss allows for flexible modulation of the condition number, leading to faster convergence compared to the MSE loss. By systematically benchmarking the performance of the SGR loss in both the ODIL and PINNs frameworks, the study demonstrates significant improvements in convergence speed and optimization stability. The SGR loss outperforms the MSE loss in various numerical experiments on benchmark problems within the ODIL framework and shows consistent better performance within the PINNs framework despite high nonlinearity. These findings emphasize the importance of loss conditioning in the design of more efficient PDE solvers, bridging the performance gap between classical iterative solvers and optimization-based solvers. <br /><br />Summary: <div>
arXiv:2508.02692v1 Announce Type: new 
Abstract: Optimization-based PDE solvers that minimize scalar loss functions have gained increasing attention in recent years. These methods either define the loss directly over discrete variables, as in Optimizing a Discrete Loss (ODIL), or indirectly through a neural network surrogate, as in Physics-Informed Neural Networks (PINNs). However, despite their promise, such methods often converge much more slowly than classical iterative solvers and are commonly regarded as inefficient. This work provides a theoretical insight, attributing the inefficiency to the use of the mean squared error (MSE) loss, which implicitly forms the normal equations, squares the condition number, and severely impairs optimization. To address this, we propose a novel Stabilized Gradient Residual (SGR) loss. By tuning a weight parameter, it flexibly modulates the condition number between the original system and its normal equations, while reducing to the MSE loss in the limiting case. We systematically benchmark the convergence behavior and optimization stability of the SGR loss within both the ODIL framework and PINNs-employing either numerical or automatic differentiation-and compare its performance against classical iterative solvers. Numerical experiments on a range of benchmark problems demonstrate that, within the ODIL framework, the proposed SGR loss achieves orders-of-magnitude faster convergence than the MSE loss. Further validation within the PINNs framework shows that, despite the high nonlinearity of neural networks, SGR consistently outperforms the MSE loss. These theoretical and empirical findings help bridge the performance gap between classical iterative solvers and optimization-based solvers, highlighting the central role of loss conditioning, and provide key insights for the design of more efficient PDE solvers.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using numerical-experimental analysis to evaluate rPET mechanical behavior under compressive stresses and FFF additive manufacturing for new sustainable designs</title>
<link>https://arxiv.org/abs/2508.02728</link>
<guid>https://arxiv.org/abs/2508.02728</guid>
<content:encoded><![CDATA[
<div> Keywords: recycled polymer, compressive stresses, mechanical behavior modeling, sustainable design, numerical-experimental study

Summary:<br />
The study focuses on investigating the mechanical behavior modeling of recycled polyethylene terephthalate (rPET) manufactured using a deposition FFF process under compressive stresses for sustainable designs. Experimental tests revealed that rPET behaves linearly until the elastic limit along manufacturing axes. Numerical analyses based on experimental data validated the design's structural safety and confirmed rPET could be configured as isotropic in simulation software without material modeling modifications. The results support the use of recycled rPET for sustainable product production using MEX technology under compressive stress. Major design companies are now incorporating recycled plastic materials in their designs. The validation results, presented through experimental testing and numerical simulations, demonstrate the feasibility and efficacy of using recycled rPET in real-world applications.<br /><br />Summary: <div>
arXiv:2508.02728v1 Announce Type: new 
Abstract: The purpose of this study is to investigate the numerical-experimental mechanical behavior modeling of the recycled polymer, that is, recyclable polyethylene terephthalate (rPET), manufactured by a deposition FFF process under compressive stresses for new sustainable designs. In all, 42 test specimens were manufactured and analyzed according to the ASTM D695-15 standards. Eight numerical analyzes were performed on a real design manufactured with rPET using Young's compression modulus from the experimental tests. Finally, eight additional experimental tests under uniaxial compression loads were performed on the real sustainable design for validating its mechanical behavior versus computational numerical tests. As a result of the experimental tests, rPET behaves linearly until it reaches the elastic limit, along each manufacturing axis. The results of this study confirmed the design's structural safety by the load scenario and operating boundary conditions. Experimental and numerical results show a difference of 0.001-0.024 mm, allowing for the rPET to be configured as isotropic in numerical simulation software without having to modify its material modeling equations. The results obtained are of great help to industry, designers and researchers because they validate the use of recycled rPET for the ecological production of real-sustainable products using MEX technology under compressive stress and its configuration for numerical simulations. Major design companies are now using recycled plastic materials in their high-end designs. Validation results have been presented on test specimens and real items, comparing experimental material configuration values with numerical results.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fluid--peridynamic structure model of deformation and damage of microchannels</title>
<link>https://arxiv.org/abs/2508.02875</link>
<guid>https://arxiv.org/abs/2508.02875</guid>
<content:encoded><![CDATA[
<div> fluid-structure interaction, microchannels, peridynamic formulation, failure scenarios, wave propagation <br />
<br />
Summary: 
This study investigates fluid-structure interaction in soft-walled microchannels, applying a nonlocal mechanical theory to model the compliant top wall's behavior, including potential failure scenarios. A computational model coupling viscous flow and a peridynamic Euler-Bernoulli beam formulation is developed to analyze steady and time-dependent responses. Through dispersion analysis, the study reveals that increasing nonlocal influence leads to a gradual suppression of phase velocity in wave propagation. The research identifies a dividing curve in a parameter space, based on the Strouhal number and compliance number, distinguishing potential failure scenarios during transient and steady loads. This work lays the foundation for understanding and predicting failure modes in soft-walled microchannels under hydrodynamic forces. <br /> <div>
arXiv:2508.02875v1 Announce Type: new 
Abstract: Soft-walled microchannels arise in many applications, ranging from organ-on-a-chip platforms to soft-robotic actuators. However, despite extensive research on their static and dynamic response, the potential failure of these devices has not been addressed. To this end, we explore fluid--structure interaction in microchannels whose compliant top wall is governed by a nonlocal mechanical theory capable of simulating both deformation and material failure. We develop a one-dimensional model by coupling viscous flow under the lubrication approximation to a state-based peridynamic formulation of an Euler--Bernoulli beam. The peridynamic formulation enables the wall to be modeled as a genuinely nonlocal beam, and the integral form of its equation of motion remains valid whether the deformation field is smooth or contains discontinuities. Through the proposed computational model, we explore the steady and time-dependent behaviors of this fluid--peridynamic structure interaction. We rationalize the wave and damping dynamics observed in the simulations through a dispersion (linearized) analysis of the coupled system, finding that, with increasing nonlocal influence, wave propagation exhibits a clear departure from classical behavior, characterized by a gradual suppression of the phase velocity. The main contribution of our study is to outline the potential failure scenarios of the microchannel's soft wall under the hydrodynamic load of the flow. Specifically, we find a dividing curve in the space spanned by the dimensionless Strouhal number (quantifying unsteady inertia of the beam) and the compliance number (quantifying the strength of the fluid--structure coupling) separating scenarios of potential failure during transient conditions from potential failure at the steady load.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closed-Loop Multi-Agent Framework for Aerodynamics-Aware Automotive Styling Design</title>
<link>https://arxiv.org/abs/2508.03370</link>
<guid>https://arxiv.org/abs/2508.03370</guid>
<content:encoded><![CDATA[
<div> Keywords: automotive design, aerodynamic performance, multi-agent framework, concept generation, rapid engineering validation<br />
Summary:<br />
The article introduces a new approach to automotive exterior design that combines aesthetics with aerodynamic performance using a multi-agent framework driven by LLM. The framework automates the workflow from ambiguous requirements to 3D concept model validation in two stages: conceptual generation and performance validation. In the conceptual generation stage, agents collaborate to interpret design requirements and produce concept sketches and renderings using diffusion models. The renderings are then converted to 3D point clouds for rapid validation using a Drag Prediction Agent based on a lightweight surrogate model. This approach seamlessly integrates creative exploration with engineering validation in an automated system, offering a new paradigm for balancing design creativity with engineering constraints in the early stages of automotive design. <br /><br />Summary: <div>
arXiv:2508.03370v1 Announce Type: new 
Abstract: The core challenge in automotive exterior design is balancing subjective aesthetics with objective aerodynamic performance while dramatically accelerating the development cycle. To address this, we propose a novel, LLM-driven multi-agent framework that automates the end-to-end workflow from ambiguous requirements to 3D concept model performance validation. The workflow is structured in two stages: conceptual generation and performance validation. In the first stage, agents collaborate to interpret fuzzy design requirements, generate concept sketches, and produce photorealistic renderings using diffusion models. In the second stage, the renderings are converted to 3D point clouds, where a Drag Prediction Agent, built upon a lightweight surrogate model, provides near-instantaneous predictions of the drag coefficient and pressure fields, replacing time-consuming CFD simulations. The primary contribution of this work is the seamless integration of creative generation with a rapid engineering validation loop within a unified, automated system, which provides a new paradigm for efficiently balancing creative exploration with engineering constraints in the earliest stages of design.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Incentivize: LLM-Empowered Contract for AIGC Offloading in Teleoperation</title>
<link>https://arxiv.org/abs/2508.03464</link>
<guid>https://arxiv.org/abs/2508.03464</guid>
<content:encoded><![CDATA[
<div> service providers, incentive mechanisms, AI-generated content, online learning, contract design
<br />
Summary:
This paper addresses the challenge of designing incentive mechanisms for edge AI-generated content service providers (ASPs) with information asymmetry. The study focuses on bonus design between a teleoperator and an ASP, where the teleoperator cannot observe the ASP's private settings and actions. The problem is formulated as an online learning contract design issue, divided into ASP's settings inference and contract derivation subproblems. A large language model (LLM)-empowered framework is introduced to tackle the NP-hard setting-inference problem. By leveraging the LLM's domain expertise, the framework refines a seed solver iteratively. Subsequently, the contract derivation problem is addressed using convex optimization techniques to obtain a near-optimal contract. Simulation results on a Unity-based teleoperation platform demonstrate that the proposed method significantly increases the teleoperator's utility compared to benchmarks, while maintaining positive incentives for the ASP. The code for this study is available on GitHub for further exploration. 
 <div>
arXiv:2508.03464v1 Announce Type: new 
Abstract: With the rapid growth in demand for AI-generated content (AIGC), edge AIGC service providers (ASPs) have become indispensable. However, designing incentive mechanisms that motivate ASPs to deliver high-quality AIGC services remains a challenge, especially in the presence of information asymmetry. In this paper, we address bonus design between a teleoperator and an edge ASP when the teleoperator cannot observe the ASP's private settings and chosen actions (diffusion steps). We formulate this as an online learning contract design problem and decompose it into two subproblems: ASP's settings inference and contract derivation. To tackle the NP-hard setting-inference subproblem with unknown variable sizes, we introduce a large language model (LLM)-empowered framework that iteratively refines a naive seed solver using the LLM's domain expertise. Upon obtaining the solution from the LLM-evolved solver, we directly address the contract derivation problem using convex optimization techniques and obtain a near-optimal contract. Simulation results on our Unity-based teleoperation platform show that our method boosts the teleoperator's utility by $5 \sim 40\%$ compared to benchmarks, while preserving positive incentives for the ASP. The code is available at https://github.com/Zijun0819/llm4contract.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Dynamically Changing Domains</title>
<link>https://arxiv.org/abs/2508.02697</link>
<guid>https://arxiv.org/abs/2508.02697</guid>
<content:encoded><![CDATA[
<div> planning, first-order logic, bounded planning, sequential generalized planning, conformant planning
Summary:
In the article, the authors challenge the Domain Closure Assumption commonly made in classical and conformant planning by introducing a dynamic object set that can change during actions. They formulate the planning problem in first-order logic, considering a finite consistent set of fluent literals as the initial theory. By imposing a finite integer bound on plan length and organizing search over grounded action sequences, they ensure soundness and completeness in solving bounded planning problems without DCA. Their approach combines elements of sequential generalized planning and conformant planning, without the use of disjunction over fluent literals. A proof-of-concept implementation of the planner is discussed, showcasing its practical application in scenarios where object sets evolve dynamically. <div>
arXiv:2508.02697v1 Announce Type: cross 
Abstract: In classical planning and conformant planning, it is assumed that there are finitely many named objects given in advance, and only they can participate in actions and in fluents. This is the Domain Closure Assumption (DCA). However, there are practical planning problems where the set of objects changes dynamically as actions are performed; e.g., new objects can be created, old objects can be destroyed. We formulate the planning problem in first-order logic, assume an initial theory is a finite consistent set of fluent literals, discuss when this guarantees that in every situation there are only finitely many possible actions, impose a finite integer bound on the length of the plan, and propose to organize search over sequences of actions that are grounded at planning time. We show the soundness and completeness of our approach. It can be used to solve the bounded planning problems without DCA that belong to the intersection of sequential generalized planning (without sensing actions) and conformant planning, restricted to the case without the disjunction over fluent literals. We discuss a proof-of-the-concept implementation of our planner.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model</title>
<link>https://arxiv.org/abs/2508.02734</link>
<guid>https://arxiv.org/abs/2508.02734</guid>
<content:encoded><![CDATA[
<div> Keywords: Location-Based Service, human mobility, activity sequences, Variable Selection Network, Insertion Transformer

Summary: 
The study addresses the challenge of incomplete trip and activity sequences in Location-Based Service (LBS) data, proposing a novel solution named Variable Selection Network-fused Insertion Transformer (VSNIT). VSNIT combines the flexibility of the Insertion Transformer with the dynamic covariate handling capability of the Variable Selection Network to recover missing segments in activity sequences at the individual level. Results show that VSNIT generates more diverse and realistic activity patterns, effectively restoring disrupted activity transitions. Compared to baseline models, VSNIT outperforms in accuracy and diversity metrics, showcasing its potential to enhance LBS data utility for mobility analysis. The proposed approach offers a promising framework for future research and applications in location-based studies.<br /><br />Summary: <div>
arXiv:2508.02734v1 Announce Type: cross 
Abstract: Location-Based Service (LBS) data provides critical insights into human mobility, yet its sparsity often yields incomplete trip and activity sequences, making accurate inferences about trips and activities difficult. We raise a research problem: Can we use activity sequences derived from high-quality LBS data to recover incomplete activity sequences at the individual level? This study proposes a new solution, the Variable Selection Network-fused Insertion Transformer (VSNIT), integrating the Insertion Transformer's flexible sequence construction with the Variable Selection Network's dynamic covariate handling capability, to recover missing segments in incomplete activity sequences while preserving existing data. The findings show that VSNIT inserts more diverse, realistic activity patterns, more closely matching real-world variability, and restores disrupted activity transitions more effectively aligning with the target. It also performs significantly better than the baseline model across all metrics. These results highlight VSNIT's superior accuracy and diversity in activity sequence recovery tasks, demonstrating its potential to enhance LBS data utility for mobility analysis. This approach offers a promising framework for future location-based research and applications.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditARF: A Framework for Corporate Credit Rating with Annual Report and Financial Feature Integration</title>
<link>https://arxiv.org/abs/2508.02738</link>
<guid>https://arxiv.org/abs/2508.02738</guid>
<content:encoded><![CDATA[
<div> Keywords: corporate credit rating, FinBERT, annual reports, non-financial data, dataset

Summary:
Corporate credit rating is essential for the market economy, maintaining economic order. This paper introduces a framework that combines financial data with features extracted from annual reports using FinBERT. It aims to leverage the value of unstructured text data typically overlooked in credit rating models. The Comprehensive Corporate Rating Dataset (CCRD) incorporates both financial and textual data, enhancing the accuracy of rating predictions by 8-12%. By integrating non-financial data into the credit rating process, the method proposed in this study improves the effectiveness and reliability of corporate credit ratings. <div>
arXiv:2508.02738v1 Announce Type: cross 
Abstract: Corporate credit rating serves as a crucial intermediary service in the market economy, playing a key role in maintaining economic order. Existing credit rating models rely on financial metrics and deep learning. However, they often overlook insights from non-financial data, such as corporate annual reports. To address this, this paper introduces a corporate credit rating framework that integrates financial data with features extracted from annual reports using FinBERT, aiming to fully leverage the potential value of unstructured text data. In addition, we have developed a large-scale dataset, the Comprehensive Corporate Rating Dataset (CCRD), which combines both traditional financial data and textual data from annual reports. The experimental results show that the proposed method improves the accuracy of the rating predictions by 8-12%, significantly improving the effectiveness and reliability of corporate credit ratings.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTBench: Cryptocurrency Time Series Generation Benchmark</title>
<link>https://arxiv.org/abs/2508.02758</link>
<guid>https://arxiv.org/abs/2508.02758</guid>
<content:encoded><![CDATA[
<div> benchmark, cryptocurrency, time series generation, trading, forecasting

Summary:
The article introduces CTBench, a Time Series Generation (TSG) benchmark specifically tailored for cryptocurrency markets. It addresses the limitations of existing TSG methods by considering the unique characteristics of crypto trading, such as 24/7 trading, extreme volatility, and rapid regime shifts. CTBench evaluates TSG models across 13 metrics in key dimensions including forecasting accuracy, rank fidelity, trading performance, risk assessment, and computational efficiency. A dual-task evaluation framework is introduced, measuring both Predictive Utility for forecasting and Statistical Arbitrage for trading signals. The benchmark analyzes eight models from five methodological families across four market regimes, revealing trade-offs between statistical fidelity and real-world profitability. CTBench provides actionable guidance for selecting and deploying TSG models in cryptocurrency analytics and strategy development. <br /><br />Summary: <div>
arXiv:2508.02758v1 Announce Type: cross 
Abstract: Synthetic time series are essential tools for data augmentation, stress testing, and algorithmic prototyping in quantitative finance. However, in cryptocurrency markets, characterized by 24/7 trading, extreme volatility, and rapid regime shifts, existing Time Series Generation (TSG) methods and benchmarks often fall short, jeopardizing practical utility. Most prior work (1) targets non-financial or traditional financial domains, (2) focuses narrowly on classification and forecasting while neglecting crypto-specific complexities, and (3) lacks critical financial evaluations, particularly for trading applications. To address these gaps, we introduce \textsf{CTBench}, the first comprehensive TSG benchmark tailored for the cryptocurrency domain. \textsf{CTBench} curates an open-source dataset from 452 tokens and evaluates TSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy, rank fidelity, trading performance, risk assessment, and computational efficiency. A key innovation is a dual-task evaluation framework: (1) the \emph{Predictive Utility} task measures how well synthetic data preserves temporal and cross-sectional patterns for forecasting, while (2) the \emph{Statistical Arbitrage} task assesses whether reconstructed series support mean-reverting signals for trading. We benchmark eight representative models from five methodological families over four distinct market regimes, uncovering trade-offs between statistical fidelity and real-world profitability. Notably, \textsf{CTBench} offers model ranking analysis and actionable guidance for selecting and deploying TSG models in crypto analytics and strategy development.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatiotemporal wall pressure forecast of a rectangular cylinder with physics-aware DeepUFNet</title>
<link>https://arxiv.org/abs/2508.03183</link>
<guid>https://arxiv.org/abs/2508.03183</guid>
<content:encoded><![CDATA[
<div> deep learning, wall pressure, spatiotemporal, Fourier neural network, forecasting

Summary:
The study introduces the DeepUFNet model to predict spatiotemporal wall pressure generated by fluid flow past a rectangular cylinder. The model combines UNet structure and Fourier neural network while embedding physical high-frequency loss control in model training. Wind tunnel testing provides data for training and testing the DeepUFNet model, demonstrating its accurate forecast of wall pressure information. Results show agreement with experimental data in statistical information, temporal pressure variation, spatial distribution, and spatiotemporal correlation. Incorporating a physical high-frequency loss control coefficient improves the model's performance, particularly in forecasting high-order frequency fluctuation and wall pressure variance. Additionally, the model exhibits a satisfactory extrapolation capability, even with sparse spatial information input. <div>
arXiv:2508.03183v1 Announce Type: cross 
Abstract: The wall pressure is of great importance in understanding the forces and structural responses induced by fluid. Recent works have investigated the potential of deep learning techniques in predicting mean pressure coefficients and fluctuating pressure coefficients, but most of existing deep learning frameworks are limited to predicting a single snapshot using full spatial information. To forecast spatiotemporal wall pressure of flow past a rectangular cylinder, this study develops a physics-aware DeepU-Fourier neural Network (DeepUFNet) deep learning model. DeepUFNet comprises the UNet structure and the Fourier neural network, with physical high-frequency loss control embedded in the model training stage to optimize model performance, where the parameter $\beta$ varies with the development of the training epoch. Wind tunnel testing is performed to collect wall pressures of a two-dimensional rectangular cylinder with a side ratio of 1.5 at an angle of attack of zero using high-frequency pressure scanning, thereby constructing a database for DeepUFNet training and testing. The DeepUFNet model is found to forecast spatiotemporal wall pressure information with high accuracy. The comparison between forecast results and experimental data presents agreement in statistical information, temporal pressure variation, power spectrum density, spatial distribution, and spatiotemporal correlation. It is also found that embedding a physical high-frequency loss control coefficient $\beta$ in the DeepUFNet model can significantly improve model performance in forecasting spatiotemporal wall pressure information, in particular, in forecasting high-order frequency fluctuation and wall pressure variance. Furthermore, the DeepUFNet extrapolation capability is tested with sparse spatial information input, and the model presents a satisfactory extrapolation ability
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A neural network machine-learning approach for characterising hydrogen trapping parameters from TDS experiments</title>
<link>https://arxiv.org/abs/2508.03371</link>
<guid>https://arxiv.org/abs/2508.03371</guid>
<content:encoded><![CDATA[
<div> machine learning, trapping behaviour, thermal desorption spectroscopy, parameter identification, neural network

Summary:
This study presents a novel approach to analyze the hydrogen trapping behavior of metallic alloys using machine learning and Thermal Desorption Spectroscopy (TDS). Traditional methods struggle to extract key trapping parameters accurately, but this work introduces a multi-Neural Network (NN) model trained on synthetic data to predict these parameters directly from experimental TDS spectra. The model consists of two NNs - a classification model to predict trap types and a regression model to determine trap densities and binding energies. Through optimization of architecture, hyperparameters, and data pre-processing, the model demonstrated high predictive accuracy when applied to three tempered martensitic steels with different compositions. The code developed for this model is openly available for use. This innovative approach shows promise in enhancing the efficiency and accuracy of hydrogen trapping analysis in metallic alloys. 

Summary: <div>
arXiv:2508.03371v1 Announce Type: cross 
Abstract: The hydrogen trapping behaviour of metallic alloys is generally characterised using Thermal Desorption Spectroscopy (TDS). However, as an indirect method, extracting key parameters (trap binding energies and densities) remains a significant challenge. To address these limitations, this work introduces a machine learning-based scheme for parameter identification from TDS spectra. A multi-Neural Network (NN) model is developed and trained exclusively on synthetic data to predict trapping parameters directly from experimental data. The model comprises two multi-layer, fully connected, feed-forward NNs trained with backpropagation. The first network (classification model) predicts the number of distinct trap types. The second network (regression model) then predicts the corresponding trap densities and binding energies. The NN architectures, hyperparameters, and data pre-processing were optimised to minimise the amount of training data. The proposed model demonstrated strong predictive capabilities when applied to three tempered martensitic steels of different compositions. The code developed is freely provided.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Memory Non-Binary LDPC Decoding</title>
<link>https://arxiv.org/abs/2508.03567</link>
<guid>https://arxiv.org/abs/2508.03567</guid>
<content:encoded><![CDATA[
<div> near-memory non-binary LDPC decoders, PiM paradigm, parallel processing, data movement bottleneck, UPMEM system<br />
<br />
Summary:
This paper discusses the challenge of data movement bottleneck in parallel processing systems when using low-density parity-check (LDPC) codes for error correction. The processing in-memory (PiM) paradigm is proposed as a solution, focusing on near-memory non-binary LDPC decoders in the UPMEM system. The study introduces a novel efficient solution for PiM-based non-binary LDPC decoders, benchmarked against low-power GPU parallel solutions. The results show that PiM-based non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, proving to be competitive even compared to edge GPUs. This research highlights the importance of addressing the data movement bottleneck in parallel processing systems and demonstrates the effectiveness of PiM-based solutions for LDPC decoding. <br /><br />Summary: <div>
arXiv:2508.03567v1 Announce Type: cross 
Abstract: Low-density parity-check (LDPC) codes are an important feature of several communication and storage applications, offering a flexible and effective method for error correction. These codes are computationally complex and require the exploitation of parallel processing to meet real-time constraints. As advancements in arithmetic and logic unit technology allowed for higher performance of computing systems, memory technology has not kept the same pace of development, creating a data movement bottleneck and affecting parallel processing systems more dramatically. To alleviate the severity of this bottleneck, several solutions have been proposed, namely the processing in-memory (PiM) paradigm that involves the design of compute units to where (or near) the data is stored, utilizing thousands of low-complexity processing units to perform out bit-wise and simple arithmetic operations. This paper presents a novel efficient solution for near-memory non-binary LDPC decoders in the UPMEM system, for the best of our knowledge the first real hardware PiM-based non-binary LDPC decoder that is benchmarked against low-power GPU parallel solutions highly optimized for throughput performance. PiM-based non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, which is even competitive when compared against implementations running in edge GPUs.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA</title>
<link>https://arxiv.org/abs/2508.03590</link>
<guid>https://arxiv.org/abs/2508.03590</guid>
<content:encoded><![CDATA[
<div> Keywords: solar irradiance forecasting, artificial intelligence, high resolution, data assimilation, net-zero energy systems

Summary:
SolarSeer is introduced as an end-to-end artificial intelligence model for accurate solar irradiance forecasting in the Contiguous United States (CONUS). By directly mapping historical satellite observations to future forecasts, SolarSeer eliminates the need for computationally intensive data assimilation and solving complex partial differential equations, making it over 1,500 times faster than traditional numerical weather prediction models. With a resolution of 5-kilometers, SolarSeer outperforms the state-of-the-art High-Resolution Rapid Refresh (HRRR) model by reducing the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also excels in capturing solar irradiance fluctuations and enhances first-order irradiance difference forecasting accuracy. The ultrafast and accurate 24-hour solar irradiance forecasts provided by SolarSeer play a crucial role in supporting the shift towards sustainable, net-zero energy systems.<br /><br />Summary: <div>
arXiv:2508.03590v1 Announce Type: cross 
Abstract: Accurate 24-hour solar irradiance forecasting is essential for the safe and economic operation of solar photovoltaic systems. Traditional numerical weather prediction (NWP) models represent the state-of-the-art in forecasting performance but rely on computationally costly data assimilation and solving complicated partial differential equations (PDEs) that simulate atmospheric physics. Here, we introduce SolarSeer, an end-to-end large artificial intelligence (AI) model for solar irradiance forecasting across the Contiguous United States (CONUS). SolarSeer is designed to directly map the historical satellite observations to future forecasts, eliminating the computational overhead of data assimilation and PDEs solving. This efficiency allows SolarSeer to operate over 1,500 times faster than traditional NWP, generating 24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer resolution in under 3 seconds. Compared with the state-of-the-art NWP in the CONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly reduces the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively captures solar irradiance fluctuations and significantly enhances the first-order irradiance difference forecasting accuracy. SolarSeer's ultrafast, accurate 24-hour solar irradiance forecasts provide strong support for the transition to sustainable, net-zero energy systems.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial Forecasting by Integrating Time Series Patterns and Salient Macroeconomic Announcements</title>
<link>https://arxiv.org/abs/2502.04592</link>
<guid>https://arxiv.org/abs/2502.04592</guid>
<content:encoded><![CDATA[
<div> Dataset, CAMEF, Multi-modal framework, Causal relationships, Counterfactual event augmentation<br />
<br />
Summary:<br />
The article introduces a new framework called CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting) that combines textual and time-series data to forecast the impact of macroeconomic events on financial markets. CAMEF integrates a causal learning mechanism and an event augmentation technique to capture causal relationships between policy texts and historical price data. A new financial dataset with various macroeconomic releases and high-frequency trading data for U.S. financial assets is introduced for analysis. The framework is compared to transformer-based models and ablation studies confirm the effectiveness of the causal learning mechanism and event types in enhancing financial forecasting accuracy. <div>
arXiv:2502.04592v2 Announce Type: replace-cross 
Abstract: Accurately forecasting the impact of macroeconomic events is critical for investors and policymakers. Salient events like monetary policy decisions and employment reports often trigger market movements by shaping expectations of economic growth and risk, thereby establishing causal relationships between events and market behavior. Existing forecasting methods typically focus either on textual analysis or time-series modeling, but fail to capture the multi-modal nature of financial markets and the causal relationship between events and price movements. To address these gaps, we propose CAMEF (Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a multi-modality framework that effectively integrates textual and time-series data with a causal learning mechanism and an LLM-based counterfactual event augmentation technique for causal-enhanced financial forecasting. Our contributions include: (1) a multi-modal framework that captures causal relationships between policy texts and historical price data; (2) a new financial dataset with six types of macroeconomic releases from 2008 to April 2024, and high-frequency real trading data for five key U.S. financial assets; and (3) an LLM-based counterfactual event augmentation strategy. We compare CAMEF to state-of-the-art transformer-based time-series and multi-modal baselines, and perform ablation studies to validate the effectiveness of the causal learning mechanism and event types.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance Agent Benchmark: Benchmarking LLMs on Real-world Financial Research Tasks</title>
<link>https://arxiv.org/abs/2508.00828</link>
<guid>https://arxiv.org/abs/2508.00828</guid>
<content:encoded><![CDATA[
<div> SEC filings, Finance Agent Benchmark, Large Language Models, AI capabilities, Financial analysis <br />
<br />
Summary: 
The article discusses the use of Large Language Models (LLMs) in financial analysis through the creation of the Finance Agent Benchmark. This benchmark consists of real-world finance research problems that require LLMs to analyze SEC filings. Developed with input from industry experts, the benchmark includes 537 questions spanning nine financial task categories. An agent harness provides LLMs with tools like Google Search and EDGAR database access to generate accurate responses. Evaluation results show that current AI capabilities, with the best model achieving only 46.8% accuracy at a cost of $3.79 per query, have limitations that need to be addressed before reliable deployment in high-stakes finance settings. <div>
arXiv:2508.00828v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) technology has emerged as a transformative force in financial analysis and the finance industry, though significant questions remain about the full capabilities of Large Language Model (LLM) agents in this domain. We present the Finance Agent Benchmark, featuring challenging and diverse real-world finance research problems that require LLMs to perform complex analysis using recent SEC filings. We construct the benchmark using a taxonomy of nine financial task categories, developed in consultation with experts from banks, hedge funds, and private equity firms. The dataset includes 537 expert-authored questions covering tasks from information retrieval to complex financial modeling, each validated through a rigorous review process to ensure accuracy and relevance. Moreover, we implement an agentic harness that equips LLMs with tools sufficient to produce accurate responses, including Google Search and EDGAR database access. Overall, the Finance Agent Benchmark provides a comprehensive testbed for measuring the progress of LLM-driven finance agents. Our evaluation reveals significant limitations in current AI capabilities - even the best-performing model (OpenAI o3) achieved only 46.8% accuracy at an average cost of $3.79 per query. This underscores the need for further advancements before reliable deployment in high-stakes finance settings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bike-Bench: A Bicycle Design Benchmark for Generative Models with Objectives and Constraints</title>
<link>https://arxiv.org/abs/2508.00830</link>
<guid>https://arxiv.org/abs/2508.00830</guid>
<content:encoded><![CDATA[
<div> engineering design benchmark, generative models, multi-objective, constraints, Bike-Bench<br />
Summary:<br />
The article introduces Bike-Bench, an engineering design benchmark for assessing generative models in solving real-world problems with multiple objectives and constraints. Bike-Bench evaluates AI models on their ability to generate designs that meet specific performance goals and restrictions across various domains such as aerodynamics, ergonomics, and human usability. The benchmark includes datasets of simulation results, human-rated bicycle assessments, and a large dataset of design variations. Results from experiments show that Language Models (LLMs) and tabular generative models do not perform as well as optimization-based models in terms of validity and optimality scores. This highlights the need for improvement in generative AI approaches for constrained multi-objective engineering design challenges. Bike-Bench aims to drive progress in this area and provides code, data, and resources for further research.<br /> <div>
arXiv:2508.00830v1 Announce Type: new 
Abstract: We introduce Bike-Bench, an engineering design benchmark for evaluating generative models on problems with multiple real-world objectives and constraints. As generative AI's reach continues to grow, evaluating its capability to understand physical laws, human guidelines, and hard constraints grows increasingly important. Engineering product design lies at the intersection of these difficult tasks, providing new challenges for AI capabilities. Bike-Bench evaluates AI models' capability to generate designs that not only resemble the dataset, but meet specific performance objectives and constraints. To do so, Bike-Bench quantifies a variety of human-centered and multiphysics performance characteristics, such as aerodynamics, ergonomics, structural mechanics, human-rated usability, and similarity to subjective text or image prompts. Supporting the benchmark are several datasets of simulation results, a dataset of 10K human-rated bicycle assessments, and a synthetically-generated dataset of 1.4M designs, each with a parametric, CAD/XML, SVG, and PNG representation. Bike-Bench is uniquely configured to evaluate tabular generative models, LLMs, design optimization, and hybrid algorithms side-by-side. Our experiments indicate that LLMs and tabular generative models fall short of optimization and optimization-augmented generative models in both validity and optimality scores, suggesting significant room for improvement. We hope Bike-Bench, a first-of-its-kind benchmark, will help catalyze progress in generative AI for constrained multi-objective engineering design problems. Code, data, and other resources are published at decode.mit.edu/projects/bikebench/.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EngiBench: A Framework for Data-Driven Engineering Design Research</title>
<link>https://arxiv.org/abs/2508.00831</link>
<guid>https://arxiv.org/abs/2508.00831</guid>
<content:encoded><![CDATA[
<div> Keywords: Engineering design optimization, Data-driven, Open-source library, Benchmarking, Machine learning algorithms

Summary: 
EngiBench is an open-source library that provides datasets and benchmarks for data-driven engineering design optimization. It offers a unified API and curated benchmarks in various domains such as aeronautics, heat conduction, and photonics. EngiOpt is another companion library that includes optimization and machine learning algorithms compatible with EngiBench. Both libraries are modular, allowing users to add new algorithms, automate experiment workflows, and use utilities for visualization and performance analysis. Experiments showed that these engineering design problems are challenging for standard machine learning methods due to sensitive and constrained design parameters. This initiative enables fair and reproducible comparisons of algorithms and facilitates faster experimentation in engineering design optimization. 

<br /><br />Summary: <div>
arXiv:2508.00831v1 Announce Type: new 
Abstract: Engineering design optimization seeks to automatically determine the shapes, topologies, or parameters of components that maximize performance under given conditions. This process often depends on physics-based simulations, which are difficult to install, computationally expensive, and require domain-specific expertise. To mitigate these challenges, we introduce EngiBench, the first open-source library and datasets spanning diverse domains for data-driven engineering design. EngiBench provides a unified API and a curated set of benchmarks -- covering aeronautics, heat conduction, photonics, and more -- that enable fair, reproducible comparisons of optimization and machine learning algorithms, such as generative or surrogate models. We also release EngiOpt, a companion library offering a collection of such algorithms compatible with the EngiBench interface. Both libraries are modular, letting users plug in novel algorithms or problems, automate end-to-end experiment workflows, and leverage built-in utilities for visualization, dataset generation, feasibility checks, and performance analysis. We demonstrate their versatility through experiments comparing state-of-the-art techniques across multiple engineering design problems, an undertaking that was previously prohibitively time-consuming to perform. Finally, we show that these problems pose significant challenges for standard machine learning methods due to highly sensitive and constrained design manifolds.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Kernel Bayesian Optimisation for Closed-Loop Electrode Microstructure Design with User-Defined Properties based on GANs</title>
<link>https://arxiv.org/abs/2508.00833</link>
<guid>https://arxiv.org/abs/2508.00833</guid>
<content:encoded><![CDATA[
<div> Keywords: porous electrode microstructures, lithium-ion batteries, deep convolutional Generative Adversarial Network, Gaussian Process Regression, Bayesian optimisation framework

Summary:
In the design of enhanced electrochemical energy storage devices like lithium-ion batteries, generating multiphase porous electrode microstructures with optimized properties is crucial. A closed-loop algorithm has been developed for designing microstructures with tailored properties. This approach involves using a deep convolutional Generative Adversarial Network to create synthetic three-phase three-dimensional images of a battery cathode material. A Gaussian Process Regression model correlates morphological and transport properties, and a deep kernel Bayesian optimization framework optimizes cathode properties based on the generator's latent space. Objective functions are defined for maximizing morphological and transport properties, and the optimized latent space shows correlation with morphological properties. This innovative method allows for efficient generation of visually realistic microstructures with customized properties. <div>
arXiv:2508.00833v1 Announce Type: new 
Abstract: The generation of multiphase porous electrode microstructures with optimum morphological and transport properties is essential in the design of improved electrochemical energy storage devices, such as lithium-ion batteries. Electrode characteristics directly influence battery performance by acting as the main sites where the electrochemical reactions coupled with transport processes occur. This work presents a generation-optimisation closed-loop algorithm for the design of microstructures with tailored properties. A deep convolutional Generative Adversarial Network is used as a deep kernel and employed to generate synthetic three-phase three-dimensional images of a porous lithium-ion battery cathode material. A Gaussian Process Regression uses the latent space of the generator and serves as a surrogate model to correlate the morphological and transport properties of the synthetic microstructures. This surrogate model is integrated into a deep kernel Bayesian optimisation framework, which optimises cathode properties as a function of the latent space of the generator. A set of objective functions were defined to perform the maximisation of morphological properties (e.g., volume fraction, specific surface area) and transport properties (relative diffusivity). We demonstrate the ability to perform simultaneous maximisation of correlated properties (specific surface area and relative diffusivity), as well as constrained optimisation of these properties. This is the maximisation of morphological or transport properties constrained by constant values of the volume fraction of the phase of interest. Visualising the optimised latent space reveals its correlation with morphological properties, enabling the fast generation of visually realistic microstructures with customised properties.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Baseline-free Damage Detection and Localization on Composite Structures with Unsupervised Kolmogorov-Arnold Autoencoder and Guided Waves</title>
<link>https://arxiv.org/abs/2508.01081</link>
<guid>https://arxiv.org/abs/2508.01081</guid>
<content:encoded><![CDATA[
<div> Keywords: Structural health monitoring, damage detection, composite structures, Kolmogorov-Arnold autoencoder, probabilistic elliptical imaging algorithm

Summary:
A novel hybrid baseline-free damage detection and localization framework has been proposed for composite structures. The framework combines an unsupervised Kolmogorov-Arnold autoencoder (KAE) with a modified probabilistic elliptical imaging algorithm (MRAPID). The KAE processes guided wave signals without prior feature extraction, continuously learning and adapting to the baseline model of each structure. The predictions from KAE are then combined with MRAPID to generate a damage probability map. The method was tested on simulated damage data from wind turbine blades and real damage data from composite flat plates, showing effective detection and localization capabilities, including the ability to detect multiple damages. Comparative analysis demonstrated superior performance over classical algorithms and state-of-the-art baseline-free methods, particularly in terms of damage localization accuracy.<br /><br />Summary: <div>
arXiv:2508.01081v1 Announce Type: new 
Abstract: Structural health monitoring (SHM) ensures the safety and longevity of structures such as aerospace equipment and wind power installations. Developing a simple, highly flexible, and scalable SHM method that does not depend on baseline models is significant for ensuring the operational integrity of advanced composite structures. In this regard, a hybrid baseline-free damage detection and localization framework incorporating an unsupervised Kolmogorov-Arnold autoencoder (KAE) and modified probabilistic elliptical imaging algorithm (MRAPID) is proposed for damage detection and localization in composite structures. Specifically, KAE was used to process the guided wave signals (GW) without any prior feature extraction process. The KAE continuously learns and adapts to the baseline model of each structure, learning from the response characteristics of its undamaged state. Then, the predictions from KAE are processed, combined with the MRAPID to generate a damage probability map. The performance of the proposed method for damage detection and localization was verified using the simulated damage data obtained on wind turbine blades and the actual damage data obtained on composite flat plates. The results show that the proposed method can effectively detect and localize damage and can achieve multiple damage localization. In addition, the method outperforms classical damage detection algorithms and state-of-the-art baseline-free damage detection and localization methods in terms of damage localization accuracy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FluidFormer: Transformer with Continuous Convolution for Particle-based Fluid Simulation</title>
<link>https://arxiv.org/abs/2508.01537</link>
<guid>https://arxiv.org/abs/2508.01537</guid>
<content:encoded><![CDATA[
<div> Fluid Attention Block, local-global hierarchy, continuous convolutions, self-attention, Transformer architecture, neural fluid simulation, convolution-based features, attention-based modeling, FluidFormer, stability.

Summary: 
The article introduces a novel approach for fluid simulation using neural networks, emphasizing the importance of global context integration for stabilizing complex simulations. The proposed Fluid Attention Block (FAB) incorporates a local-global hierarchy, combining continuous convolutions for local features with self-attention for capturing global dependencies. This fusion helps suppress error accumulation and model long-range physical phenomena. Additionally, a specialized Transformer architecture is developed for continuous fluid simulation, integrated within a dual-pipeline framework. The method, named FluidFormer, showcases state-of-the-art performance and enhanced stability in various complex fluid scenarios. This innovative approach unifies convolution-based local features with attention-based global context modeling, setting a new standard for neural fluid simulation techniques. <div>
arXiv:2508.01537v1 Announce Type: new 
Abstract: Learning-based fluid simulation networks have been proven as viable alternatives to traditional numerical solvers for the Navier-Stokes equations. Existing neural methods follow Smoothed Particle Hydrodynamics (SPH) frameworks, which inherently rely only on local inter-particle interactions. However, we emphasize that global context integration is also essential for learning-based methods to stabilize complex fluid simulations. We propose the first Fluid Attention Block (FAB) with a local-global hierarchy, where continuous convolutions extract local features while self-attention captures global dependencies. This fusion suppresses the error accumulation and models long-range physical phenomena. Furthermore, we pioneer the first Transformer architecture specifically designed for continuous fluid simulation, seamlessly integrated within a dual-pipeline architecture. Our method establishes a new paradigm for neural fluid simulation by unifying convolution-based local features with attention-based global context modeling. FluidFormer demonstrates state-of-the-art performance, with stronger stability in complex fluid scenarios.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Model Fidelity Evaluation to Support Design Decisions for Complex, Novel Systems of Systems</title>
<link>https://arxiv.org/abs/2508.02456</link>
<guid>https://arxiv.org/abs/2508.02456</guid>
<content:encoded><![CDATA[
<div> trustworthiness, model validation, model fidelity, systems engineering, design process

Summary: 
Model trust is vital in systems design processes where real-world data is scarce, especially for complex systems with emergent behavior. Trustworthy models are crucial for supporting designers in making informed decisions. Model fidelity, defined as the model's adherence to real-world physics, is closely linked to trust and validity, enhancing a designer's ability to rely on physics-based models. The complexity and accuracy of a model's representation of physical phenomena play a significant role in determining its fidelity. Methods for evaluating and selecting models that do not require real-world data are essential challenges in systems engineering. Validating models based on their fidelity to real-world physics helps designers choose the most appropriate model for a given design decision. <div>
arXiv:2508.02456v1 Announce Type: new 
Abstract: Systems design processes are increasingly reliant on simulation models to inform design decisions. A pervasive issue within the systems engineering community is trusting in the models used to make decisions about complex systems. This work presents a method of evaluating the trustworthiness of a model to provide utility to a designer making a decision within a design process. Trusting the results of a model is especially important in design processes where the system is complex, novel, or displays emergent phenomena. Additionally, systems that are in the pre-prototype stages of development often do not have sources of ground truth for validating the models. Developing methods of model validation and trust that do not require real-world data is a key challenge facing systems engineers. Model fidelity in this work refers to the adherence of a model to real-world physics and is closely tied to model trust and model validity. Trust and validity directly support a designer's ability to make decisions using physics-based models. The physics that are captured in a model and the complexity of the mathematical representation of the physics contribute to a model's fidelity, and this work leverages the included physical phenomena to develop a means of selecting the most appropriate for a given design decision.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability</title>
<link>https://arxiv.org/abs/2508.00850</link>
<guid>https://arxiv.org/abs/2508.00850</guid>
<content:encoded><![CDATA[
<div> Keywords: Supertask paradigm, adaptive behavior, cognitive neuroscience, computational psychiatry, serious gaming

Summary: 
Gearshift Fellowship (GF) is a new Supertask paradigm that models adaptive behavior in humans and artificial agents by combining cognitive neuroscience, computational psychiatry, economics, and artificial intelligence. It creates a dynamic, multi-mission environment for assessing mechanisms of adaptive behavior across cognitive and social contexts. GF allows for neurocognitive modeling of individual differences in perceptual decisions, learning, and meta-cognitive levels, making it a flexible testbed for understanding cognitive-affective control processes, learning styles, and motivation shifts. Results from an online study show that GF recovers effects from traditional neuropsychological tasks, uncovers novel patterns in learning across contexts, and maps clinical features onto distinct adaptations. This research paves the way for developing in-game interventions that promote self-efficacy and coping skills for real-world stress. GF aims to accelerate science, transform clinical care, and support individual growth by creating an adaptive ecosystem where humans and machines can co-develop greater flexibility and awareness.<br /><br />Summary: <div>
arXiv:2508.00850v1 Announce Type: cross 
Abstract: How do we learn when to persist, when to let go, and when to shift gears? Gearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed to model how humans and artificial agents adapt to shifting environment demands. Grounded in cognitive neuroscience, computational psychiatry, economics, and artificial intelligence, Supertasks combine computational neurocognitive modeling with serious gaming. This creates a dynamic, multi-mission environment engineered to assess mechanisms of adaptive behavior across cognitive and social contexts. Computational parameters explain behavior and probe mechanisms by controlling the game environment. Unlike traditional tasks, GF enables neurocognitive modeling of individual differences across perceptual decisions, learning, and meta-cognitive levels. This positions GF as a flexible testbed for understanding how cognitive-affective control processes, learning styles, strategy use, and motivational shifts adapt across contexts and over time. It serves as an experimental platform for scientists, a phenotype-to-mechanism intervention for clinicians, and a training tool for players aiming to strengthen self-regulated learning, mood, and stress resilience. Online study (n = 60, ongoing) results show that GF recovers effects from traditional neuropsychological tasks (construct validity), uncovers novel patterns in how learning differs across contexts and how clinical features map onto distinct adaptations. These findings pave the way for developing in-game interventions that foster self-efficacy and agency to cope with real-world stress and uncertainty. GF builds a new adaptive ecosystem designed to accelerate science, transform clinical care, and foster individual growth. It offers a mirror and training ground where humans and machines co-develop together deeper flexibility and awareness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks</title>
<link>https://arxiv.org/abs/2508.00855</link>
<guid>https://arxiv.org/abs/2508.00855</guid>
<content:encoded><![CDATA[
<div> Transformer, Physics-Informed Neural Networks, Generative Adversarial Networks, Residual Guided Training, Partial Differential Equations<br />
<br />
Summary: 
The article introduces a novel Residual Guided Training strategy for Physics-Informed Transformer via Generative Adversarial Networks (GAN) to improve resolving residuals and enforcing temporal causality in modeling nonlinear partial differential equations (PDEs). The proposed framework combines a decoder-only Transformer for capturing temporal correlations with a residual-aware GAN to prioritize high-residual regions. By incorporating a causal penalty term and an adaptive sampling mechanism, the method enhances accuracy in critical spatiotemporal regions. Experimental results on various PDEs like Allen-Cahn, Klein-Gordon, and Navier-Stokes equations demonstrate substantial improvements with up to three orders of magnitude reduction in Mean Squared Error (MSE) compared to conventional methods. This approach effectively bridges the gap between deep learning and physics-driven modeling, offering a robust solution for modeling multiscale and time-dependent PDE systems. <br /><br /> <div>
arXiv:2508.00855v1 Announce Type: cross 
Abstract: Nonlinear partial differential equations (PDEs) are pivotal in modeling complex physical systems, yet traditional Physics-Informed Neural Networks (PINNs) often struggle with unresolved residuals in critical spatiotemporal regions and violations of temporal causality. To address these limitations, we propose a novel Residual Guided Training strategy for Physics-Informed Transformer via Generative Adversarial Networks (GAN). Our framework integrates a decoder-only Transformer to inherently capture temporal correlations through autoregressive processing, coupled with a residual-aware GAN that dynamically identifies and prioritizes high-residual regions. By introducing a causal penalty term and an adaptive sampling mechanism, the method enforces temporal causality while refining accuracy in problematic domains. Extensive numerical experiments on the Allen-Cahn, Klein-Gordon, and Navier-Stokes equations demonstrate significant improvements, achieving relative MSE reductions of up to three orders of magnitude compared to baseline methods. This work bridges the gap between deep learning and physics-driven modeling, offering a robust solution for multiscale and time-dependent PDE systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Fleet Upgrade Decisions with Machine-Learning Enhanced Optimization</title>
<link>https://arxiv.org/abs/2508.00915</link>
<guid>https://arxiv.org/abs/2508.00915</guid>
<content:encoded><![CDATA[
<div> Keywords: Rental-based business models, fleet management, machine learning, optimization, sustainability

Summary:
Rental-based business models and sustainability requirements are driving the need for efficient strategies to manage large machine and vehicle fleets. Traditional fleet optimization methods based on integer programming are computationally expensive, especially for large fleets. This study proposes two approaches for fleet upgrade optimization: an extended integer programming approach and a machine learning-based method. In a real-world automotive industry case study, the machine learning approach demonstrated near-optimal solutions with improved scalability and computational performance compared to the traditional method. This makes it a practical alternative for large-scale fleet management. By integrating machine learning into fleet upgrade decision-making processes, organizations can achieve optimal solutions that balance utility, cost, and sustainability considerations effectively. 

<br /><br />Summary: <div>
arXiv:2508.00915v1 Announce Type: cross 
Abstract: Rental-based business models and increasing sustainability requirements intensify the need for efficient strategies to manage large machine and vehicle fleet renewal and upgrades. Optimized fleet upgrade strategies maximize overall utility, cost, and sustainability. However, conventional fleet optimization does not account for upgrade options and is based on integer programming with exponential runtime scaling, which leads to substantial computational cost when dealing with large fleets and repeated decision-making processes. This contribution firstly suggests an extended integer programming approach that determines optimal renewal and upgrade decisions. The computational burden is addressed by a second, alternative machine learning-based method that transforms the task to a mixed discrete-continuous optimization problem. Both approaches are evaluated in a real-world automotive industry case study, which shows that the machine learning approach achieves near-optimal solutions with significant improvements in the scalability and overall computational performance, thus making it a practical alternative for large-scale fleet management.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reward-Directed Diffusion Framework for Generative Design Optimization</title>
<link>https://arxiv.org/abs/2508.01509</link>
<guid>https://arxiv.org/abs/2508.01509</guid>
<content:encoded><![CDATA[
<div> Diffusion models, reward-directed sampling, high-performance engineering designs, soft value function, Markov decision process <br />
<br />
Summary: This study introduces a generative optimization framework utilizing a fine-tuned diffusion model and reward-directed sampling to create high-performance engineering designs. The framework uses a parametric design representation to generate new parameter sets with improved performance metrics. It employs a soft value function within a Markov decision process to guide the decoding process, reducing computational costs and achieving high-reward designs. Empirical results show significant enhancements in 3D ship hull design and 2D airfoil design, with samples surpassing the training data distribution. The proposed approach leads to a 25 percent reduction in resistance for ship design and a 10 percent improvement in the lift-to-drag ratio for 2D airfoil design. Integration of this framework in the engineering design cycle can boost designer efficiency and overall design performance. <br /> <div>
arXiv:2508.01509v1 Announce Type: cross 
Abstract: This study presents a generative optimization framework that builds on a fine-tuned diffusion model and reward-directed sampling to generate high-performance engineering designs. The framework adopts a parametric representation of the design geometry and produces new parameter sets corresponding to designs with enhanced performance metrics. A key advantage of the reward-directed approach is its suitability for scenarios in which performance metrics rely on costly engineering simulations or surrogate models (e.g. graph-based, ensemble models, or tree-based) are non-differentiable or prohibitively expensive to differentiate. This work introduces the iterative use of a soft value function within a Markov decision process framework to achieve reward-guided decoding in the diffusion model. By incorporating soft-value guidance during both the training and inference phases, the proposed approach reduces computational and memory costs to achieve high-reward designs, even beyond the training data. Empirical results indicate that this iterative reward-directed method substantially improves the ability of the diffusion models to generate samples with reduced resistance in 3D ship hull design and enhanced hydrodynamic performance in 2D airfoil design tasks. The proposed framework generates samples that extend beyond the training data distribution, resulting in a greater 25 percent reduction in resistance for ship design and over 10 percent improvement in the lift-to-drag ratio for the 2D airfoil design. Successful integration of this model into the engineering design life cycle can enhance both designer productivity and overall design performance.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Policy Iteration for Stochastic Optimal Control: A Physics-Informed Approach</title>
<link>https://arxiv.org/abs/2508.01718</link>
<guid>https://arxiv.org/abs/2508.01718</guid>
<content:encoded><![CDATA[
<div> physics-informed neural network, stochastic optimal control, Hamilton-Jacobi-Bellman equation, policy iteration, value function approximation 
Summary:
The proposed Physics-Informed Neural Network Policy Iteration (PINN-PI) framework tackles stochastic optimal control problems through second-order Hamilton-Jacobi-Bellman equations. A neural network is trained at each iteration to approximate the value function by minimizing the residual of a linear PDE based on a fixed policy, ensuring systematic error control. Explicit Lipschitz-type bounds quantify the propagation of value gradient errors to policy updates, enhancing interpretability during training. Extending deterministic PINN approaches to stochastic environments, the method guarantees global exponential convergence under mild conditions. Demonstrated effectiveness on various benchmark problems like stochastic cartpole, pendulum, and high-dimensional linear quadratic regulation (LQR) challenges up to 10D showcases the versatility and reliability of the approach. <div>
arXiv:2508.01718v1 Announce Type: cross 
Abstract: We propose a physics-informed neural network policy iteration (PINN-PI) framework for solving stochastic optimal control problems governed by second-order Hamilton--Jacobi--Bellman (HJB) equations. At each iteration, a neural network is trained to approximate the value function by minimizing the residual of a linear PDE induced by a fixed policy. This linear structure enables systematic $L^2$ error control at each policy evaluation step, and allows us to derive explicit Lipschitz-type bounds that quantify how value gradient errors propagate to the policy updates. This interpretability provides a theoretical basis for evaluating policy quality during training. Our method extends recent deterministic PINN-based approaches to stochastic settings, inheriting the global exponential convergence guarantees of classical policy iteration under mild conditions. We demonstrate the effectiveness of our method on several benchmark problems, including stochastic cartpole, pendulum problems and high-dimensional linear quadratic regulation (LQR) problems in up to 10D.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization</title>
<link>https://arxiv.org/abs/2508.02002</link>
<guid>https://arxiv.org/abs/2508.02002</guid>
<content:encoded><![CDATA[
<div> transformers, diffusers, trajectory generation, auto-bidding, GRAD
<br />
Summary:
GRAD is a novel approach in the field of auto-bidding systems, combining an Action-Mixture-of-Experts module and the Value Estimator of Causal Transformer. This model addresses challenges faced by generative methods in online advertising, such as distribution shift and limited exploration of the action space, by incorporating constraint-aware optimization techniques. GRAD has been successfully implemented at Meituan, a leading online food delivery platform, resulting in a significant increase in platform revenue, Gross Merchandise Value (GMV), and Return on Investment (ROI). The model demonstrates its effectiveness in enhancing the performance of auto-bidding systems by catering to the dynamic and diverse requirements of modern advertisers. <div>
arXiv:2508.02002v1 Announce Type: cross 
Abstract: Modern auto-bidding systems are required to balance overall performance with diverse advertiser goals and real-world constraints, reflecting the dynamic and evolving needs of the industry. Recent advances in conditional generative models, such as transformers and diffusers, have enabled direct trajectory generation tailored to advertiser preferences, offering a promising alternative to traditional Markov Decision Process-based methods. However, these generative methods face significant challenges, such as the distribution shift between offline and online environments, limited exploration of the action space, and the necessity to meet constraints like marginal Cost-per-Mille (CPM) and Return on Investment (ROI). To tackle these challenges, we propose GRAD (Generative Reward-driven Ad-bidding with Mixture-of-Experts), a scalable foundation model for auto-bidding that combines an Action-Mixture-of-Experts module for diverse bidding action exploration with the Value Estimator of Causal Transformer for constraint-aware optimization. Extensive offline and online experiments demonstrate that GRAD significantly enhances platform revenue, highlighting its effectiveness in addressing the evolving and diverse requirements of modern advertisers. Furthermore, GRAD has been implemented in multiple marketing scenarios at Meituan, one of the world's largest online food delivery platforms, leading to a 2.18% increase in Gross Merchandise Value (GMV) and 10.68% increase in ROI.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval</title>
<link>https://arxiv.org/abs/2508.02222</link>
<guid>https://arxiv.org/abs/2508.02222</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hierarchical queries, passage retrieval, indirect positives mining, Financial Passage Retrieval Generated dataset

Summary: 
The paper introduces a bidirectional generation pipeline for constructing passage retrieval datasets using large language models (LLMs). This pipeline generates 3-level hierarchical queries for intra-doc and cross-doc scenarios by disassembling single-doc text and dividing multi-doc titles into clusters based on industry, topic, and time. It incorporates both bottom-up and top-down query generation methods to enhance query expression and relevance. The pipeline includes a direct mapping annotation process and an indirect positives mining method to enrich relevance labels. The Financial Passage Retrieval Generated dataset (FinCPRG) was created from Chinese financial research reports, containing hierarchical queries and rich relevance labels. Evaluations and experiments demonstrated the quality and effectiveness of FinCPRG as a training and benchmarking dataset for passage retrieval tasks. 

Summary: <div>
arXiv:2508.02222v1 Announce Type: cross 
Abstract: In recent years, large language models (LLMs) have demonstrated significant potential in constructing passage retrieval datasets. However, existing methods still face limitations in expressing cross-doc query needs and controlling annotation quality. To address these issues, this paper proposes a bidirectional generation pipeline, which aims to generate 3-level hierarchical queries for both intra-doc and cross-doc scenarios and mine additional relevance labels on top of direct mapping annotation. The pipeline introduces two query generation methods: bottom-up from single-doc text and top-down from multi-doc titles. The bottom-up method uses LLMs to disassemble and generate structured queries at both sentence-level and passage-level simultaneously from intra-doc passages. The top-down approach incorporates three key financial elements--industry, topic, and time--to divide report titles into clusters and prompts LLMs to generate topic-level queries from each cluster. For relevance annotation, our pipeline not only relies on direct mapping annotation from the generation relationship but also implements an indirect positives mining method to enrich the relevant query-passage pairs. Using this pipeline, we constructed a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k Chinese financial research reports, which includes hierarchical queries and rich relevance labels. Through evaluations of mined relevance labels, benchmarking and training experiments, we assessed the quality of FinCPRG and validated its effectiveness as a passage retrieval dataset for both training and benchmarking.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ByteGen: A Tokenizer-Free Generative Model for Orderbook Events in Byte Space</title>
<link>https://arxiv.org/abs/2508.02247</link>
<guid>https://arxiv.org/abs/2508.02247</guid>
<content:encoded><![CDATA[
<div> Generative modeling, high-frequency limit order book, finance, ByteGen, raw byte streams<br />
Summary:<br />
The article introduces ByteGen, a novel generative model for high-frequency limit order book dynamics in finance. It works directly on raw byte streams of market events, eliminating the need for feature engineering and tokenization. ByteGen treats the problem as an autoregressive next-byte prediction task, using a compact 32-byte packed binary format for efficient data representation. The H-Net architecture, a hybrid Mamba-Transformer model, is used to discover the structure of market messages without predefined rules. Trained on CME Bitcoin futures data, ByteGen successfully replicates key characteristics of financial markets, such as realistic price distributions and bursty event timing. This approach shows promise for modeling complex financial systems without the biases of tokenization, achieving competitive performance on market quality metrics. <br /> <div>
arXiv:2508.02247v1 Announce Type: cross 
Abstract: Generative modeling of high-frequency limit order book (LOB) dynamics is a critical yet unsolved challenge in quantitative finance, essential for robust market simulation and strategy backtesting. Existing approaches are often constrained by simplifying stochastic assumptions or, in the case of modern deep learning models like Transformers, rely on tokenization schemes that affect the high-precision, numerical nature of financial data through discretization and binning. To address these limitations, we introduce ByteGen, a novel generative model that operates directly on the raw byte streams of LOB events. Our approach treats the problem as an autoregressive next-byte prediction task, for which we design a compact and efficient 32-byte packed binary format to represent market messages without information loss. The core novelty of our work is the complete elimination of feature engineering and tokenization, enabling the model to learn market dynamics from its most fundamental representation. We achieve this by adapting the H-Net architecture, a hybrid Mamba-Transformer model that uses a dynamic chunking mechanism to discover the inherent structure of market messages without predefined rules. Our primary contributions are: 1) the first end-to-end, byte-level framework for LOB modeling; 2) an efficient packed data representation; and 3) a comprehensive evaluation on high-frequency data. Trained on over 34 million events from CME Bitcoin futures, ByteGen successfully reproduces key stylized facts of financial markets, generating realistic price distributions, heavy-tailed returns, and bursty event timing. Our findings demonstrate that learning directly from byte space is a promising and highly flexible paradigm for modeling complex financial systems, achieving competitive performance on standard market quality metrics without the biases of tokenization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Continuous-Time MILP for Integrated Aircraft Hangar Scheduling and Layout</title>
<link>https://arxiv.org/abs/2508.02640</link>
<guid>https://arxiv.org/abs/2508.02640</guid>
<content:encoded><![CDATA[
<div> continuous-time mixed-integer linear programming, aircraft maintenance hangars, aircraft scheduling, spatial allocation, scalability limitations <br />
<br />
Summary: 
This paper presents a novel continuous-time mixed-integer linear programming model for efficient management of aircraft maintenance hangars. The model addresses the complex decisions involved in aircraft scheduling and spatial allocation, overcoming the scalability limitations of traditional approaches by treating time as a continuous variable. Benchmarking against a heuristic shows the model's superior performance, solving instances with up to 25 aircraft quickly and delivering high-quality solutions for cases with up to 40 aircraft. The model's economic benefits and managerial insights are highlighted, with solutions consistently outperforming the heuristic. A custom-built visualization dashboard demonstrates the practical applicability of the model, showcasing its ability to provide optimized solutions quickly and effectively. <div>
arXiv:2508.02640v1 Announce Type: cross 
Abstract: Efficient management of aircraft maintenance hangars is a critical operational challenge, involving complex, interdependent decisions regarding aircraft scheduling and spatial allocation. This paper introduces a novel continuous-time mixed-integer linear programming (MILP) model to solve this integrated spatio-temporal problem. By treating time as a continuous variable, our formulation overcomes the scalability limitations of traditional discrete-time approaches. The performance of the exact model is benchmarked against a constructive heuristic, and its practical applicability is demonstrated through a custom-built visualization dashboard. Computational results are compelling: the model solves instances with up to 25 aircraft to proven optimality, often in mere seconds, and for large-scale cases of up to 40 aircraft, delivers high-quality solutions within known optimality gaps. In all tested scenarios, the resulting solutions consistently and significantly outperform the heuristic, which highlights the framework's substantial economic benefits and provides valuable managerial insights into the trade-off between solution time and optimality.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A locking-free isogeometric thin shell formulation based on higher order accurate diagonalized strain projection via approximate dual splines</title>
<link>https://arxiv.org/abs/2406.16685</link>
<guid>https://arxiv.org/abs/2406.16685</guid>
<content:encoded><![CDATA[
<div> spline basis functions, Kirchhoff-Love shell formulation, membrane locking, Hellinger-Reissner variational principle, isogeometric discretization <br />
Summary:<br />
The article presents a novel approach for isogeometric discretization of the Kirchhoff-Love shell formulation, addressing membrane locking issues. Independent strains are discretized using spline basis functions one degree lower than displacements to enhance accuracy. Variations of strains are discretized using approximate dual splines to obtain a projection matrix that is diagonalized through row-sum lumping for efficient condensation. This diagonalization simplifies static condensation of strain fields without the need for matrix inversion, maintaining higher-order accuracy with optimal convergence rates. Numerical benchmarks, such as a curved Euler-Bernoulli beam and shell obstacle course examples, demonstrate the approach's numerical properties and performance. <br /> 
Summary: <div>
arXiv:2406.16685v4 Announce Type: replace 
Abstract: We present a novel isogeometric discretization approach for the Kirchhoff-Love shell formulation based on the Hellinger-Reissner variational principle. For mitigating membrane locking, we discretize the independent strains with spline basis functions that are one degree lower than those used for the displacements. To enable computationally efficient condensation of the independent strains, we first discretize the variations of the independent strains with approximate dual splines to obtain a projection matrix that is close to a diagonal matrix. We then diagonalize this strain projection matrix via row-sum lumping. Due to this diagonalization, the static condensation of the independent strain fields becomes computationally inexpensive, as no matrix needs to be inverted. At the same time, our approach maintains higher-order accuracy at optimal rates of convergence. We illustrate the numerical properties and the performance of our approach through numerical benchmarks, including a curved Euler-Bernoulli beam and the examples of the shell obstacle course.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADformer: A Multi-Granularity Spatial-Temporal Transformer for EEG-Based Alzheimer Detection</title>
<link>https://arxiv.org/abs/2409.00032</link>
<guid>https://arxiv.org/abs/2409.00032</guid>
<content:encoded><![CDATA[
<div> EEG, Alzheimer's Disease, ADformer, transformer, spatial-temporal<br />
<br />
Summary: <br />
The article introduces ADformer, a novel spatial-temporal transformer for EEG-based Alzheimer's Disease (AD) detection. Existing approaches for AD detection often suffer from information loss and limited generalizability due to manual feature engineering. ADformer addresses these challenges by capturing both temporal and spatial features from raw EEG signals, enabling end-to-end representation learning. It incorporates multi-granularity embedding strategies and a two-stage intra-inter granularity self-attention mechanism to learn local patterns and global dependencies. ADformer is evaluated on 4 large-scale datasets with 1,713 subjects, achieving superior performance in distinguishing AD from healthy control subjects. The model outperforms existing methods with subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the 4 datasets, showcasing its effectiveness for EEG-based AD detection. <div>
arXiv:2409.00032v2 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) has emerged as a cost-effective and efficient tool to support neurologists in the detection of Alzheimer's Disease (AD). However, most existing approaches rely heavily on manual feature engineering or data transformation. While such techniques may provide benefits when working with small-scale datasets, they often lead to information loss and distortion when applied to large-scale data, ultimately limiting model performance. Moreover, the limited subject scale and demographic diversity of datasets used in prior studies hinder comprehensive evaluation of model robustness and generalizability, thus restricting their applicability in real-world clinical settings. To address these challenges, we propose ADformer, a novel multi-granularity spatial-temporal transformer designed to capture both temporal and spatial features from raw EEG signals, enabling effective end-to-end representation learning. Our model introduces multi-granularity embedding strategies across both spatial and temporal dimensions, leveraging a two-stage intra-inter granularity self-attention mechanism to learn both local patterns within each granularity and global dependencies across granularities. We evaluate ADformer on 4 large-scale datasets comprising a total of 1,713 subjects, representing one of the largest corpora for EEG-based AD detection to date, under a cross-validated, subject-independent setting. Experimental results demonstrate that ADformer consistently outperforms existing methods, achieving subject-level F1 scores of 92.82%, 89.83%, 67.99%, and 83.98% on the 4 datasets, respectively, in distinguishing AD from healthy control (HC) subjects.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Tick-Size Too Small: A General Method for Modelling Small Tick Limit Order Books</title>
<link>https://arxiv.org/abs/2410.08744</link>
<guid>https://arxiv.org/abs/2410.08744</guid>
<content:encoded><![CDATA[
<div> tick-sizes, market agents behavior, Limit Order Book, Hawkes Process model, stylized facts<br />
<br />Summary:
This study examines the impact of tick-sizes on the microstructural properties of assets in the market. By analyzing a variety of assets with different tick-sizes, the researchers identify distinct stylized facts that differentiate between large, medium, and small-tick assets. They propose a Hawkes Process model that effectively captures the characteristics of different tick-size assets, including sparsity, multi-tick level price moves, and the shape of the Limit Order Book. Through simulations, they demonstrate the model's versatility and its ability to transition between large and small-tick assets based on key variables. The study also assesses the model's assumptions, highlights challenges, and suggests potential directions for future research in this area. <div>
arXiv:2410.08744v3 Announce Type: replace-cross 
Abstract: Tick-sizes not only influence the granularity of the price formation process but also affect market agents' behavior. We investigate the disparity in the microstructural properties of the Limit Order Book (LOB) across a basket of assets with different relative tick-sizes. A key contribution of this study is the identification of several stylized facts, which are used to differentiate between large, medium, and small-tick assets, along with clear metrics for their measurement. We provide cross-asset visualizations to illustrate how these attributes vary with relative tick-size. Further, we propose a Hawkes Process model that {\color{black}not only fits well for large-tick assets, but also accounts for }sparsity, multi-tick level price moves, and the shape of the LOB in small-tick assets. Through simulation studies, we demonstrate the {\color{black} versatility} of the model and identify key variables that determine whether a simulated LOB resembles a large-tick or small-tick asset. Our tests show that stylized facts like sparsity, shape, and relative returns distribution can be smoothly transitioned from a large-tick to a small-tick asset using our model. We test this model's assumptions, showcase its challenges and propose questions for further directions in this area of research.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Operator Networks for Bayesian Parameter Estimation in PDEs</title>
<link>https://arxiv.org/abs/2501.10684</link>
<guid>https://arxiv.org/abs/2501.10684</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Operator Networks, Physics-Informed Neural Networks, partial differential equations, uncertainty quantification, surrogate modeling

Summary: 

Deep Operator Networks (DeepONets) and Physics-Informed Neural Networks (PINNs) are combined to solve partial differential equations (PDEs) and estimate their parameters. The framework integrates data-driven learning with physical constraints, achieving robust and accurate solutions in various scenarios. Bayesian training via variational inference enables comprehensive uncertainty quantification for aleatoric and epistemic uncertainties, ensuring reliable predictions even in noisy conditions or when governing equations are missing. This approach proves effective in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. The framework offers a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.<br /><br />Summary: <div>
arXiv:2501.10684v2 Announce Type: replace-cross 
Abstract: We present a novel framework combining Deep Operator Networks (DeepONets) with Physics-Informed Neural Networks (PINNs) to solve partial differential equations (PDEs) and estimate their unknown parameters. By integrating data-driven learning with physical constraints, our method achieves robust and accurate solutions across diverse scenarios. Bayesian training is implemented through variational inference, allowing for comprehensive uncertainty quantification for both aleatoric and epistemic uncertainties. This ensures reliable predictions and parameter estimates even in noisy conditions or when some of the physical equations governing the problem are missing. The framework demonstrates its efficacy in solving forward and inverse problems, including the 1D unsteady heat equation and 2D reaction-diffusion equations, as well as regression tasks with sparse, noisy observations. This approach provides a computationally efficient and generalizable method for addressing uncertainty quantification in PDE surrogate modeling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Practical Finite Element Approach for Simulating Dynamic Crack Growth in Cu/Ultra Low-k Interconnect Structures</title>
<link>https://arxiv.org/abs/2508.00193</link>
<guid>https://arxiv.org/abs/2508.00193</guid>
<content:encoded><![CDATA[
<div> Keywords: finite element modeling, dynamic crack propagation, Crack Element Method, Edge-based Smoothed Finite Element Method, fracture energy release rate <br />
Summary: 
The article introduces the Crack Element Method (CEM) for simulating dynamic crack propagation in 2D structures. CEM utilizes an element-splitting algorithm based on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture crack growth while minimizing poorly shaped elements. A fracture energy release rate formulation is developed using split element topology. Validation on benchmark problems confirms accuracy and robustness. A case study on patterned Cu/Ultra Low-k interconnect structures showcases CEM's applicability.<br /><br />Summary: <div>
arXiv:2508.00193v1 Announce Type: new 
Abstract: This work presents a practical finite element modeling strategy, the Crack Element Method (CEM), for simulating the dynamic crack propagation in two-dimensional structures. The method employs an element-splitting algorithm based on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture the element-wise crack growth while reducing the formation of poorly shaped elements that can compromise numerical accuracy and computational performance. A fracture energy release rate formulation is also developed based on the evolving topology of the split elements. The proposed approach is validated through a series of classical benchmark problems, demonstrating its accuracy and robustness in addressing dynamic fracture scenarios. Finally, the applicability of the CEM is illustrated in a case study involving patterned Cu/Ultra Low-k interconnect structures.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeightFlow: Learning Stochastic Dynamics via Evolving Weight of Neural Network</title>
<link>https://arxiv.org/abs/2508.00451</link>
<guid>https://arxiv.org/abs/2508.00451</guid>
<content:encoded><![CDATA[
<div> Keywords: stochastic dynamics, neural network, weight space, optimal transport, high-dimensional

Summary: 
This article introduces a novel approach, WeightFlow, to model stochastic dynamics directly in the weight space of a neural network. By projecting the evolving probability distribution onto the weight space, WeightFlow connects dynamic optimal transport in measure space to an energy functional in weight space. The neural network weights are constructed into a graph, and their evolution is learned through a graph-controlled differential equation. Experimental results on interdisciplinary datasets show that WeightFlow outperforms state-of-the-art methods by an average of 43.02%. This approach provides an effective and scalable solution for modeling high-dimensional stochastic dynamics. <div>
arXiv:2508.00451v1 Announce Type: new 
Abstract: Modeling stochastic dynamics from discrete observations is a key interdisciplinary challenge. Existing methods often fail to estimate the continuous evolution of probability densities from trajectories or face the curse of dimensionality. To address these limitations, we presents a novel paradigm: modeling dynamics directly in the weight space of a neural network by projecting the evolving probability distribution. We first theoretically establish the connection between dynamic optimal transport in measure space and an equivalent energy functional in weight space. Subsequently, we design WeightFlow, which constructs the neural network weights into a graph and learns its evolution via a graph controlled differential equation. Experiments on interdisciplinary datasets demonstrate that WeightFlow improves performance by an average of 43.02\% over state-of-the-art methods, providing an effective and scalable solution for modeling high-dimensional stochastic dynamics.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources</title>
<link>https://arxiv.org/abs/2508.00654</link>
<guid>https://arxiv.org/abs/2508.00654</guid>
<content:encoded><![CDATA[
<div> management, microscopy, data integration, interoperability, web-based platform
<br />
Summary:<br />
- Managing and integrating large volumes of microscopy data stored across different platforms is a challenge in research. 
- Data types such as bioimages, experimental records, and spectral information are often stored separately, hindering data linkage and alignment with FAIR data management principles. 
- The lack of tools for effectively integrating heterogeneous data sources prompted the development of LEO, a web-based platform. 
- LEO initially linked Electronic Lab Notebooks (ELNs) with OMERO but can now integrate other data sources through a plugin-based architecture. 
- LEO's extensibility makes it a scalable and flexible solution for various microscopy research workflows. 
<br />Summary: <div>
arXiv:2508.00654v1 Announce Type: new 
Abstract: In the interdisciplinary field of microscopy research, managing and integrating large volumes of data stored across disparate platforms remains a major challenge. Data types such as bioimages, experimental records, and spectral information are often maintained in separate repositories, each following different management standards. However, linking these data sources across the research lifecycle is essential to align with the FAIR principles of data management: Findability, Accessibility, Interoperability, and Reusability. Despite this need, there is a notable lack of tools capable of effectively integrating and linking data from heterogeneous sources. To address this gap, we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based platform designed to create and manage links between distributed data systems. LEO was initially developed to link objects between Electronic Lab Notebooks (ELNs) and OMERO, but its functionality has since been extended through a plugin-based architecture, allowing the integration of additional data sources. This extensibility makes LEO a scalable and flexible solution for a wide range of microscopy research workflows.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contact Sensors to Remote Cameras: Quantifying Cardiorespiratory Coupling in High-Altitude Exercise Recovery</title>
<link>https://arxiv.org/abs/2508.00773</link>
<guid>https://arxiv.org/abs/2508.00773</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiorespiratory coupling, high altitude, exercise, remote photoplethysmography, autonomic regulation

Summary: 
Cardiorespiratory coupling (CRC) is the dynamic interaction between the heart and lungs, which is enhanced during physical exercise and associated with improved physiological function. A study examined CRC at high altitudes during rest and post-exercise recovery, revealing significant differences. The analysis showed that recovery involved more frequent yet less stable synchronization between breathing and pulse. The feasibility of non-contact CRC measurement using remote photoplethysmography (rPPG) was explored, showing a strong correlation with oximeter-based metrics. These findings suggest that CRC could serve as a sensitive marker for autonomic regulation and have potential applications in contactless monitoring. Source code for the study is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2508.00773v1 Announce Type: new 
Abstract: Cardiorespiratory coupling (CRC) captures the dynamic interaction between the cardiac and respiratory systems--an interaction strengthened by physical exercise and linked to improved physiological function. We examined CRC at high altitude in two states, rest and post-exercise recovery, and found significant differences (p < 0.05). Quantitative analysis revealed that recovery involved more frequent yet less stable episodes of synchronization between respiration and pulse. Furthermore, we explored the feasibility of non-contact CRC measurement with remote photoplethysmography (rPPG), observing a strong correlation with oximeter-based metrics (Pearson r = 0.96). These findings highlight the potential of CRC as a sensitive marker for autonomic regulation and its future application in contactless monitoring. Source code is available at GitHub: https://github.com/McJackTang/CRC.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>{\tau}-Ring: A Smart Ring Platform for Multimodal Physiological and Behavioral Sensing</title>
<link>https://arxiv.org/abs/2508.00778</link>
<guid>https://arxiv.org/abs/2508.00778</guid>
<content:encoded><![CDATA[
<div> Keyword: Smart rings, wearable, physiological sensing, open-source, reproducible <br />
<br />
Summary: 
The article introduces the {\tau}-Ring platform, designed to address the limitations of proprietary smart rings for continuous physiological and behavioral sensing. The platform offers accessible hardware with multi-channel PPG, IMU, temperature sensing, NFC, and on-board storage. It features adjustable firmware for quick reconfiguration of settings, allowing researchers to customize sampling rates, power modes, and wireless protocols. The platform also includes an open-source Android software suite for real-time streaming and offline logging. These capabilities enable easy acquisition of rich datasets, facilitating research prototyping and standardization. The platform is validated through studies in heart-rate monitoring and ring-based handwriting recognition. Overall, {\tau}-Ring provides a commercial-ready solution that promotes reproducibility in wearable research and accelerates innovation in physiological and behavioral sensing technologies. Source code is available on GitHub for reference and collaboration. <br /><br />Summary: <div>
arXiv:2508.00778v1 Announce Type: new 
Abstract: Smart rings have emerged as uniquely convenient devices for continuous physiological and behavioral sensing, offering unobtrusive, constant access to metrics such as heart rate, motion, and skin temperature. Yet most commercial solutions remain proprietary, hindering reproducibility and slowing innovation in wearable research. We introduce {\tau}-Ring, a commercial-ready platform that bridges this gap through: (i) accessible hardware combining time-synchronized multi-channel PPG, 6-axis IMU, temperature sensing, NFC, and on-board storage; (ii) adjustable firmware that lets researchers rapidly reconfigure sampling rates, power modes, and wireless protocols; and (iii) a fully open-source Android software suite that supports both real-time streaming and 8-hour offline logging. Together, these features enable out-of-the-box, reproducible acquisition of rich physiological and behavioral datasets, accelerating prototyping and standardizing experimentation. We validate the platform with demonstration studies in heart-rate monitoring and ring-based handwriting recognition. Source code is available at GitHub: https://github.com/thuhci/OpenRing.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models</title>
<link>https://arxiv.org/abs/2508.00804</link>
<guid>https://arxiv.org/abs/2508.00804</guid>
<content:encoded><![CDATA[
<div> structured state space models, real-time recurrent learning, online adaptation, linear-recurrent-unit, prediction error 

Summary: 
This paper presents a novel approach for enhancing the predictions of structured state space models (SSMs) using real-time recurrent learning during inference. SSMs are traditionally trained offline and do not adapt to new data during deployment. The proposed method enables online adaptation by continuously updating model parameters based on incoming data. The study evaluated this approach using a small carbon emission dataset from embedded automotive hardware and found that it consistently reduced prediction error during inference. This highlights the potential of the method in dynamic and resource-constrained environments. The use of linear-recurrent-unit SSMs demonstrated the effectiveness of the approach in improving prediction accuracy in real-time scenarios. <div>
arXiv:2508.00804v1 Announce Type: new 
Abstract: This paper introduces a new approach for fine-tuning the predictions of structured state space models (SSMs) at inference time using real-time recurrent learning. While SSMs are known for their efficiency and long-range modeling capabilities, they are typically trained offline and remain static during deployment. Our method enables online adaptation by continuously updating model parameters in response to incoming data. We evaluate our approach for linear-recurrent-unit SSMs using a small carbon emission dataset collected from embedded automotive hardware. Experimental results show that our method consistently reduces prediction error online during inference, demonstrating its potential for dynamic, resource-constrained environments.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weak Values as Geometric Lenses: Deformations of Hilbert Space and the Emergence of superoscillations</title>
<link>https://arxiv.org/abs/2508.00023</link>
<guid>https://arxiv.org/abs/2508.00023</guid>
<content:encoded><![CDATA[
<div> weak measurement, quantum mechanics, signal processing, superoscillations, geometric structure <br />
Summary: 
This paper explores the relationship between weak measurement in quantum mechanics and superoscillations, showing that superoscillations are a natural consequence of the geometric structure underlying weak values. The weak value is described as a ratio of geometric deformation, representing how an observable transforms Hilbert space relative to the standard inner product. This deformation warps quantum states locally, producing oscillations that exceed the global Fourier bandwidth. The weak value is interpreted as a comparison between a deformed sesquilinear form and the standard one, revealing connections to generalized Rayleigh quotients and projective geometry of quantum states. This perspective unifies weak values and superoscillations as manifestations of a single geometric principle. <div>
arXiv:2508.00023v1 Announce Type: cross 
Abstract: The formalism of weak measurement in quantum mechanics has revealed profound connections between measurement theory, quantum foundations, and signal processing. In this paper, we develop a pointer-free derivation of superoscillations, demonstrating that they are a natural and necessary consequence of the geometric structure underlying weak values. We argue that the weak value is best understood as a ratio of geometric deformation, quantifying how an observable transforms the structure of Hilbert space relative to a reference provided by the standard inner product. This deformation acts as a conceptual lens, warping the local structure of quantum states to produce oscillations far exceeding the global Fourier bandwidth. We formalize this by interpreting the weak value as a comparison between a deformed sesquilinear form and the standard one, and explore its deep connections to generalized Rayleigh quotients and the projective geometry of quantum states. This perspective unifies weak values and superoscillations as two facets of a single underlying geometric principle.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis</title>
<link>https://arxiv.org/abs/2508.00381</link>
<guid>https://arxiv.org/abs/2508.00381</guid>
<content:encoded><![CDATA[
<div> marine environment, welding defect detection, neural networks, interpretability, offshore environment
<br />
Summary:<br />
The paper introduces "Adapt-WeldNet", an adaptive framework for welding defect detection in marine and offshore environments. It evaluates pre-trained architectures, transfer learning strategies, and adaptive optimizers to optimize defect detection. A Defect Detection Interpretability Analysis (DDIA) framework enhances system transparency through Explainable AI techniques and domain-specific evaluations by NDE Level II professionals. The Human-in-the-Loop (HITL) approach and Trustworthy AI principles ensure reliability and accountability. By improving performance and interpretability, the system enhances trust, safety, and reliability of welding defect detection, supporting critical operations in challenging environments. <div>
arXiv:2508.00381v1 Announce Type: cross 
Abstract: Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models</title>
<link>https://arxiv.org/abs/2508.00383</link>
<guid>https://arxiv.org/abs/2508.00383</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial transcriptomics, Precision oncology, Vision foundation models, State space models, Colorectal cancer

Summary:
Spatial transcriptomics is a valuable tool for predicting treatment responses in oncology, but its high cost and complexity hinder clinical adoption. Current vision foundation models (VFMs) based on ViT backbones struggle to meet clinical standards. To address this, a hybrid backbone architecture named MVHybrid, combining state space models with ViT, is proposed. Pretrained on colorectal cancer datasets using self-supervised learning, MVHybrid outperforms ViT in predicting gene expression and exhibits superior robustness in leave-one-study-out evaluation. It also performs well in classification, patch retrieval, and survival prediction tasks, showing promise as a next-generation pathology VFM backbone. The code is publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2508.00383v1 Announce Type: cross 
Abstract: Spatial transcriptomics reveals gene expression patterns within tissue context, enabling precision oncology applications such as treatment response prediction, but its high cost and technical complexity limit clinical adoption. Predicting spatial gene expression (biomarkers) from routine histopathology images offers a practical alternative, yet current vision foundation models (VFMs) in pathology based on Vision Transformer (ViT) backbones perform below clinical standards. Given that VFMs are already trained on millions of diverse whole slide images, we hypothesize that architectural innovations beyond ViTs may better capture the low-frequency, subtle morphological patterns correlating with molecular phenotypes. By demonstrating that state space models initialized with negative real eigenvalues exhibit strong low-frequency bias, we introduce $MV_{Hybrid}$, a hybrid backbone architecture combining state space models (SSMs) with ViT. We compare five other different backbone architectures for pathology VFMs, all pretrained on identical colorectal cancer datasets using the DINOv2 self-supervised learning method. We evaluate all pretrained models using both random split and leave-one-study-out (LOSO) settings of the same biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher correlation than the best-performing ViT and shows 43% smaller performance degradation compared to random split in gene expression prediction, demonstrating superior performance and robustness, respectively. Furthermore, $MV_{Hybrid}$ shows equal or better downstream performance in classification, patch retrieval, and survival prediction tasks compared to that of ViT, showing its promise as a next-generation pathology VFM backbone. Our code is publicly available at: https://github.com/deepnoid-ai/MVHybrid.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The time slot allocation problem in liberalised passenger railway markets: a multi-objective approach</title>
<link>https://arxiv.org/abs/2401.12073</link>
<guid>https://arxiv.org/abs/2401.12073</guid>
<content:encoded><![CDATA[
<div> competition, railway market, time slot allocation, multi-objective model, market equilibrium
<br />
Summary:
The article discusses the time slot allocation problem in the European passenger railway market post-liberalization. The Infrastructure Manager assesses bids and allocates resources to Railway Undertakings, influencing market equilibrium. A multi-objective model is proposed for time slot allocation, with the Infrastructure Manager choosing a point from the Pareto front. Two selection criteria are suggested: one based on priorities for time slot allocation to companies, and the other introducing fairness to incentivize competition. The impact of these rules on market equilibrium was evaluated on a high-speed corridor in the Spanish railway network. <div>
arXiv:2401.12073v2 Announce Type: replace 
Abstract: The liberalisation of the European passenger railway markets through the European Directive EU 91/440/EEC states a new scenario where different Railway Undertakings compete with each other in a bidding process for time slots. The infrastructure resources are provided by the Infrastructure Manager, who analyses and assesses the bids received, allocating the resources to each Railway Undertaking. Time slot allocation is a fact that drastically influences the market equilibrium. In this paper, we address the time slot allocation problem within the context of a liberalized passenger railway market as a multi-objective model. The Infrastructure Manager is tasked with selecting a point from the Pareto front as the solution to the time slot allocation problem. We propose two criteria for making this selection: the first one allocates time slots to each company according to a set of priorities, while the second one introduces a criterion of fairness in the treatment of companies to incentive competition. The assessment of the impact of these rules on market equilibrium has been conducted on a liberalized high-speed corridor within the Spanish railway network.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-physics Model of Flow from Coronary Angiography: Insights to Microvascular Function</title>
<link>https://arxiv.org/abs/2412.04798</link>
<guid>https://arxiv.org/abs/2412.04798</guid>
<content:encoded><![CDATA[
<div> coronary microvascular dysfunction, computational fluid dynamics, angiography, contrast intensity profile, lumped parameter model<br />
Summary:<br />
Coronary Microvascular Dysfunction (CMD) causes impaired vasodilation and insufficient blood flow to the myocardium during stress. Invasive wire-based diagnosis techniques like index of microcirculatory resistance (IMR) and coronary flow reserve (CFR) are underutilized due to complexity. A 3D-0D coupled multi-physics computational fluid dynamics (CFD) model was developed to simulate contrast injection during angiography. A contrast intensity profile (CIP) was introduced to describe angiography data dynamics. Sensitivity studies showed that resistance impacts CIP slopes more than capacitance, with higher resistance amplifying the effect. The model offers a tool for interpreting angiographic data, potentially transforming the understanding and utilization of coronary angiography in diagnosing CMD. <br />Summary: <div>
arXiv:2412.04798v2 Announce Type: replace 
Abstract: Coronary Microvascular Dysfunction (CMD) is characterized by impaired vasodilation and can lead to insufficient blood flow to the myocardium during stress or exertion, affecting millions of people globally. Despite their diagnostic value, invasive, wire-based diagnosis techniques of CMD, such as index of microcirculatory resistance (IMR) and coronary flow reserve (CFR), are underutilized due to their complexity and inconsistency. Coronary angiography, one of the most commonly used imaging modalities, offers valuable flow information that assists in diagnosing CMD. However, this information is not fully understood or utilized in current clinical practice. In this study, a 3D-0D coupled multi-physics computational fluid dynamics (CFD) model was developed and calibrated to simulate and study the process of contrast injection and washout during clinical angiography. A contrast intensity profile (CIP) was introduced to describe the dynamics of coronary angiography data. Additionally, sensitivity studies were conducted to evaluate the influence of various coronary lumped parameter model (LPM) parameters on the shapes of CIPs. The results demonstrate that the multi-physics model can be effectively calibrated to produce physiologically meaningful hemodynamic results. Sensitivity studies reveal that resistance has a greater impact on the rising and falling slopes of CIP than capacitance, with higher resistance amplifying this effect. The model and results are presented here. These results are potentially transformative, as they provide a tool for interpreting angiographic data and ultimately extracting information concerning coronary microcirculation.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATH: A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly Detection Approaches for Multivariate Time Series</title>
<link>https://arxiv.org/abs/2411.13951</link>
<guid>https://arxiv.org/abs/2411.13951</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, multivariate time series, automotive powertrain, semi-supervised learning, threshold selection <br />
Summary: <br />
- Benchmarking anomaly detection approaches for multivariate time series is challenging due to the lack of high-quality datasets.
- A diverse, extensive, and non-trivial dataset generated via simulation tools reflecting realistic behavior of an automotive powertrain is proposed.
- The dataset represents a discrete-sequence problem not addressed by previous literature.
- Different versions of the dataset are provided for unsupervised and semi-supervised anomaly detection settings, time series generation, and forecasting.
- Baseline results from deterministic and variational autoencoders, as well as a non-parametric approach, show the importance of robustness to contaminated training data and the influence of the threshold on detection performance. Further work is needed to improve threshold selection methods without requiring labeled data. <br /> <div>
arXiv:2411.13951v5 Announce Type: replace-cross 
Abstract: Benchmarking anomaly detection approaches for multivariate time series is a challenging task due to a lack of high-quality datasets. Current publicly available datasets are too small, not diverse and feature trivial anomalies, which hinders measurable progress in this research area. We propose a solution: a diverse, extensive, and non-trivial dataset generated via state-of-the-art simulation tools that reflects realistic behaviour of an automotive powertrain, including its multivariate, dynamic and variable-state properties. Additionally, our dataset represents a discrete-sequence problem, which remains unaddressed by previously-proposed solutions in literature. To cater for both unsupervised and semi-supervised anomaly detection settings, as well as time series generation and forecasting, we make different versions of the dataset available, where training and test subsets are offered in contaminated and clean versions, depending on the task. We also provide baseline results from a selection of approaches based on deterministic and variational autoencoders, as well as a non-parametric approach. As expected, the baseline experimentation shows that the approaches trained on the semi-supervised version of the dataset outperform their unsupervised counterparts, highlighting a need for approaches more robust to contaminated training data. Furthermore, results show that the threshold used can have a large influence on detection performance, hence more work needs to be invested in methods to find a suitable threshold without the need for labelled data.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Axioms for Model Fidelity Evaluation</title>
<link>https://arxiv.org/abs/2507.23020</link>
<guid>https://arxiv.org/abs/2507.23020</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital engineering, model fidelity, simulation, evaluation framework, ground vehicle model

Summary:
Digital engineering has revolutionized the design process, with simulations playing a crucial role in providing information consistent with reality. The concept of model fidelity, which refers to the similarity between a simulation and reality, is essential in this context. However, existing definitions of model fidelity lack formal rigor, leading to ambiguity in evaluation processes. This paper introduces seven axioms to guide the development of future fidelity evaluation frameworks. By applying these axioms to a ground vehicle model, the study demonstrates their practicality. The axioms serve as a foundation for potential advancements in evaluating model fidelity and can be used as reference points for future research in this area. This work highlights the importance of ensuring the accuracy and reliability of simulations in digital engineering practices. 

<br /><br />Summary: <div>
arXiv:2507.23020v1 Announce Type: new 
Abstract: Digital engineering has transformed the design and development process. However, the utility of digital engineering is fundamentally dependent on the assumption that a simulation provides information consistent with reality. This relationship is described as model fidelity. Despite the widespread use of the term, existing definitions of model fidelity often lack formal rigor in practical application, which leaves ambiguity in how this similarity should be evaluated. This paper presents seven fundamental axioms to aid the development of future fidelity evaluation frameworks. An example of a ground vehicle model is used under an existing fidelity evaluation framework to observe the applicability of these axioms. In addition, these axioms are used as a reference point for considering future opportunities in future work related to model fidelity.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information Bottleneck Asset Pricing Model</title>
<link>https://arxiv.org/abs/2507.23218</link>
<guid>https://arxiv.org/abs/2507.23218</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep neural networks, financial asset pricing, information bottleneck, mutual information, nonlinear relationships 

Summary: 
Deep neural networks (DNNs) are often used in financial asset pricing due to their ability to model complex nonlinear relationships in financial data. However, these models can over-fit to noise in the data, leading to subpar performance. To combat this issue, a new information bottleneck asset pricing model is proposed. This model compresses data with low signal-to-noise ratios by eliminating redundant information while retaining critical information for asset pricing. By imposing constraints of mutual information during the nonlinear mapping process, the model progressively reduces the mutual information between input data and compressed representation while increasing mutual information between the compressed representation and output prediction. This approach ensures that irrelevant information (noise) is filtered out during the modeling of financial nonlinear relationships, ultimately improving asset pricing accuracy. <div>
arXiv:2507.23218v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) have garnered significant attention in financial asset pricing, due to their strong capacity for modeling complex nonlinear relationships within financial data. However, sophisticated models are prone to over-fitting to the noise information in financial data, resulting in inferior performance. To address this issue, we propose an information bottleneck asset pricing model that compresses data with low signal-to-noise ratios to eliminate redundant information and retain the critical information for asset pricing. Our model imposes constraints of mutual information during the nonlinear mapping process. Specifically, we progressively reduce the mutual information between the input data and the compressed representation while increasing the mutual information between the compressed representation and the output prediction. The design ensures that irrelevant information, which is essentially the noise in the data, is forgotten during the modeling of financial nonlinear relationships without affecting the final asset pricing. By leveraging the constraints of the Information bottleneck, our model not only harnesses the nonlinear modeling capabilities of deep networks to capture the intricate relationships within financial data but also ensures that noise information is filtered out during the information compression process.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adjoint-Based Aerodynamic Shape Optimization with a Manifold Constraint Learned by Diffusion Models</title>
<link>https://arxiv.org/abs/2507.23443</link>
<guid>https://arxiv.org/abs/2507.23443</guid>
<content:encoded><![CDATA[
<div> shape optimization, aerodynamics, adjoint method, diffusion model, automatic differentiation

Summary:
This article presents a novel approach to aerodynamic shape optimization using an adjoint-based framework. The framework incorporates a diffusion model trained on existing designs to learn a smooth manifold of aerodynamically viable shapes, which is enforced as an equality constraint in the optimization problem. By computing adjoint gradients of design objectives with respect to the manifold space, the method eliminates the need for ad hoc parameter tuning and variable scaling. The framework integrates seamlessly into existing adjoint-based design workflows with minimal modification and demonstrates superior aerodynamic performance compared to conventional approaches in transonic RANS airfoil design cases. By combining AI-generated priors with adjoint methods, this approach enables robust, high-fidelity aerodynamic shape optimization through automatic differentiation. <div>
arXiv:2507.23443v1 Announce Type: new 
Abstract: We introduce an adjoint-based aerodynamic shape optimization framework that integrates a diffusion model trained on existing designs to learn a smooth manifold of aerodynamically viable shapes. This manifold is enforced as an equality constraint to the shape optimization problem. Central to our method is the computation of adjoint gradients of the design objectives (e.g., drag and lift) with respect to the manifold space. These gradients are derived by first computing shape derivatives with respect to conventional shape design parameters (e.g., Hicks-Henne parameters) and then backpropagating them through the diffusion model to its latent space via automatic differentiation. Our framework preserves mathematical rigor and can be integrated into existing adjoint-based design workflows with minimal modification. Demonstrated on extensive transonic RANS airfoil design cases using off-the-shelf and general-purpose nonlinear optimizers, our approach eliminates ad hoc parameter tuning and variable scaling, maintains robustness across initialization and optimizer choices, and achieves superior aerodynamic performance compared to conventional approaches. This work establishes how AI generated priors integrates effectively with adjoint methods to enable robust, high-fidelity aerodynamic shape optimization through automatic differentiation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis</title>
<link>https://arxiv.org/abs/2507.22936</link>
<guid>https://arxiv.org/abs/2507.22936</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Financial Natural Language Processing, Comparative Evaluation, 10-K Filings, Model Performance

Summary: 
Large Language Models (LLMs) have shown significant capabilities in various Financial Natural Language Processing (FinNLP) tasks. This study compares five leading LLMs, including GPT, Claude, Perplexity, Gemini, and DeepSeek, using 10-K filings from top technology companies. Evaluation methods include human annotation, automated metrics, and model behavior diagnostics. GPT performs best in coherence, semantic alignment, and contextual relevance, followed by Claude and Perplexity. Gemini and DeepSeek exhibit more variability and disagreement. Model outputs differ based on prompts and source material, varying across companies and over time. The study highlights the importance of careful prompt design and data selection in influencing LLM performance in financial analysis.<br /><br />Summary: <div>
arXiv:2507.22936v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide variety of Financial Natural Language Processing (FinNLP) tasks. However, systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis, this study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies. We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity). The results show that GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Machine Learning with Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2507.22959</link>
<guid>https://arxiv.org/abs/2507.22959</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific machine learning, Kolmogorov-Arnold Networks, data encoding, interpretability, nonlinear interactions<br />
Summary:<br />
The article discusses the shift from multilayer perceptrons (MLPs) to Kolmogorov-Arnold Networks (KANs) in scientific machine learning. KANs offer enhanced interpretability, flexibility, and improved capability in capturing complex nonlinear interactions, overcoming the limitations of MLPs. The review categorizes recent progress in KAN-based models from three perspectives: data-driven learning, physics-informed modeling, and deep operator learning. It highlights the advantages of KANs in accuracy, convergence, and spectral representation compared to MLPs. The review also identifies challenges in KAN development such as computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity. Future research directions focus on improving the robustness, scalability, and physical consistency of KAN-based frameworks.<br /><br /> <div>
arXiv:2507.22959v1 Announce Type: cross 
Abstract: The field of scientific machine learning, which originally utilized multilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold Networks (KANs) for data encoding. This shift is driven by the limitations of MLPs, including poor interpretability, fixed activation functions, and difficulty capturing localized or high-frequency features. KANs address these issues with enhanced interpretability and flexibility, enabling more efficient modeling of complex nonlinear interactions and effectively overcoming the constraints associated with conventional MLP architectures. This review categorizes recent progress in KAN-based models across three distinct perspectives: (i) data-driven learning, (ii) physics-informed modeling, and (iii) deep operator learning. Each perspective is examined through the lens of architectural design, training strategies, application efficacy, and comparative evaluation against MLP-based counterparts. By benchmarking KANs against MLPs, we highlight consistent improvements in accuracy, convergence, and spectral representation, clarifying KANs' advantages in capturing complex dynamics while learning more effectively. Finally, this review identifies critical challenges and open research questions in KAN development, particularly regarding computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity. We also outline future research directions aimed at improving the robustness, scalability, and physical consistency of KAN-based frameworks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Readiness for Scientific AI at Scale</title>
<link>https://arxiv.org/abs/2507.23018</link>
<guid>https://arxiv.org/abs/2507.23018</guid>
<content:encoded><![CDATA[
<div> Data Readiness for AI, leadership-scale scientific datasets, foundation models, preprocessing patterns, domain-specific constraints <br />
<br />
Summary: This paper explores how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets for training foundation models. By analyzing workflows in climate, nuclear fusion, bio/health, and materials domains, common preprocessing patterns and domain-specific constraints are identified. A two-dimensional readiness framework is introduced, consisting of Data Readiness Levels and Data Processing Stages tailored to high performance computing environments. This framework highlights challenges in transforming scientific data for scalable AI training, particularly focusing on transformer-based generative models. By incorporating these dimensions, a conceptual maturity matrix is formed to characterize scientific data readiness and guide infrastructure development towards standardized, cross-domain support for scalable and reproducible AI applications in science. <br /> <div>
arXiv:2507.23018v1 Announce Type: cross 
Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets used to train foundation models. We analyze archetypal workflows across four representative domains - climate, nuclear fusion, bio/health, and materials - to identify common preprocessing patterns and domain-specific constraints. We introduce a two-dimensional readiness framework composed of Data Readiness Levels (raw to AI-ready) and Data Processing Stages (ingest to shard), both tailored to high performance computing (HPC) environments. This framework outlines key challenges in transforming scientific data for scalable AI training, emphasizing transformer-based generative models. Together, these dimensions form a conceptual maturity matrix that characterizes scientific data readiness and guides infrastructure development toward standardized, cross-domain support for scalable and reproducible AI for science.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution</title>
<link>https://arxiv.org/abs/2507.23600</link>
<guid>https://arxiv.org/abs/2507.23600</guid>
<content:encoded><![CDATA[
<div> matrix factorization, multivariate curve resolution, deep learning, signal unmixing, generative modeling

Summary:
This study introduces a novel energy-based deep learning solver, EB-gMCR, for signal unmixing analysis. By reformulating multivariate curve resolution as a generative process, EB-gMCR automatically determines the optimal component set for faithful data reconstruction. The solver utilizes a differentiable gating network to select active components and estimate their concentrations, achieving high accuracy even in the presence of noise. Additional chemical priors can be easily incorporated into the framework, allowing for adaptation to various instruments and domains without changing the core learning process. With the ability to handle large datasets and unknown component counts, EB-gMCR provides a practical approach to scalable signal unmixing analysis, particularly in chemical library-driven scenarios. The source code for EB-gMCR is available on GitHub for further exploration and use. <br /><br />Summary: <div>
arXiv:2507.23600v1 Announce Type: cross 
Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely applied in chemical and biological research. Multivariate curve resolution (MCR), a branch of signal unmixing, separates mixed chemical signals into base patterns (components) and their concentrations, playing a key role in understanding composition. Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified component count, usually unknown in real data. As dataset size or component count increases, the scalability and reliability of MF-based MCR face significant challenges. This study reformulates MCR as a generative process (gMCR), and introduces an energy-based deep learning solver, EB-gMCR, that automatically discovers the smallest component set able to reconstruct the data faithfully. EB-gMCR starts from a large candidate pool (e.g., 1024 spectra) and employs a differentiable gating network to retain only active components while estimating their concentrations. On noisy synthetic datasets containing up to 256 latent sources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count within 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near exact component estimation. Additional chemical priors, such as non-negativity or nonlinear mixing, enter as simple plug-in functions, enabling adaptation to other instruments or domains without altering the core learning process. By uniting high-capacity generative modeling and hard component selection, EB-gMCR offers a practical route to large-scale signal unmixing analysis, including chemical library-driven scenarios. The source code is available at https://github.com/b05611038/ebgmcr_solver.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An entropy-stable and kinetic energy-preserving macro-element HDG method for compressible flows</title>
<link>https://arxiv.org/abs/2507.22195</link>
<guid>https://arxiv.org/abs/2507.22195</guid>
<content:encoded><![CDATA[
<div> Efficient, robust simulation, compressible flows, high order numerical framework, macro element HDG method, turbulence simulation<br />
Summary:<br />
This paper presents a new high order numerical framework for simulating compressible flows efficiently and robustly. The approach, called macro element HDG method, embeds continuous Galerkin structure within macro-elements to reduce degrees of freedom and enable highly parallel local solves. By using entropy variables and a flux differencing approach, the method extends its robustness, making it suitable for under resolved or turbulent regimes. The formulations ensure entropy stability and kinetic energy preservation while maintaining high order accuracy. The method's performance is demonstrated on benchmark problems, showing optimal accuracy, improved robustness, and significant speedup compared to standard HDG methods. These advancements mark a significant progress in high order methods for direct numerical simulation (DNS) of compressible flows.<br /> <div>
arXiv:2507.22195v1 Announce Type: new 
Abstract: This paper introduces a high order numerical framework for efficient and robust simulation of compressible flows. To address the inefficiencies of standard hybridized discontinuous Galerkin (HDG) methods in large scale settings, we develop a macro element HDG method that reduces global and local degrees of freedom by embedding continuous Galerkin structure within macro-elements. This formulation supports matrix free implementations and enables highly parallel local solves, leading to substantial performance gains and excellent scalability on modern architectures. To enhance robustness in under resolved or turbulent regimes, we extend the method using entropy variables and a flux differencing approach to construct entropy stable and kinetic energy preserving variants. These formulations satisfy a discrete entropy inequality and improve stability without compromising high order accuracy. We demonstrate the performance of the proposed method on benchmark problems including the inviscid isentropic vortex and the Taylor Green vortex in both inviscid and turbulent regimes. Numerical results confirm optimal accuracy, improved robustness, and up to an order of magnitude speedup over standard HDG methods. These developments mark a significant advancement in high order methods for direct numerical simulation (DNS) of compressible flows.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cycles Protocol: A Peer-to-Peer Electronic Clearing System</title>
<link>https://arxiv.org/abs/2507.22309</link>
<guid>https://arxiv.org/abs/2507.22309</guid>
<content:encoded><![CDATA[
<div> Keywords: financial institutions, liquidity challenges, blockchain communities, decentralized settlement systems, Cycles <br />
<br />
Summary: Financial institutions have historically formed closed clearing clubs to address liquidity challenges, excluding many small enterprises and communities. Blockchain communities offer decentralized settlement systems, but they have yet to significantly impact the real economy. To tackle these issues, Cycles introduces an open, decentralized protocol for clearing, settlement, and issuance. It aims to help firms reduce payment inefficiencies, lower working capital costs, and access diverse assets and liquidity sources. Cycles uses a privacy-preserving multilateral settlement platform based on a graph optimization algorithm, recognizing that liquidity can be found within cycles in the payment network structure. By optimizing settlement flows to reduce debt, Cycles seeks to transform the way firms manage liquidity challenges and improve financial access for all actors in the ecosystem. <br /> <div>
arXiv:2507.22309v1 Announce Type: new 
Abstract: For centuries, financial institutions have responded to liquidity challenges by forming closed, centralized clearing clubs with strict rules and membership that allow them to collaborate on using the least money to discharge the most debt. As closed clubs, much of the general public has been excluded from participation. But the vast majority of private sector actors consists of micro or small firms that are vulnerable to late payments and generally ineligible for bank loans. This low liquidity environment often results in gridlock and leads to insolvency, and it disproportionately impacts small enterprises and communities.
  On the other hand, blockchain communities have developed open, decentralized settlement systems, along with a proliferation of store of value assets and new lending protocols, allowing anyone to permissionlessly transact and access credit. However, these protocols remain used primarily for speculative purposes, and so far have fallen short of the large-scale positive impact on the real economy prophesied by their promoters.
  We address these challenges by introducing Cycles, an open, decentralized clearing, settlement, and issuance protocol. Cycles is designed to enable firms to overcome payment inefficiencies, to reduce their working capital costs, and to leverage diverse assets and liquidity sources, including cryptocurrencies, stablecoins, and lending protocols, in service of clearing more debt with less money. Cycles solves real world liquidity challenges through a privacy-preserving multilateral settlement platform based on a graph optimization algorithm. The design is based on a core insight: liquidity resides within cycles in the payment network's structure and can be accessed via settlement flows optimized to reduce debt.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A holomorphic Kolmogorov-Arnold network framework for solving elliptic problems on arbitrary 2D domains</title>
<link>https://arxiv.org/abs/2507.22678</link>
<guid>https://arxiv.org/abs/2507.22678</guid>
<content:encoded><![CDATA[
<div> holomorphic neural networks, Physics-informed, differential equations, Kolmogorov-Arnold representation, Laurent series theory
Summary:
Physics-informed holomorphic neural networks (PIHNNs) have become efficient surrogate models for differential problems. They embed the problem structure into the network, needing training only for boundary conditions. Introduction of a new holomorphic network architecture, PIHKAN, based on Kolmogorov-Arnold representation, improves accuracy with reduced complexity. Mathematical extensions broaden PIHNNs' applicability to a wider class of elliptic partial differential equations like the Helmholtz equation. A new method using Laurent series theory allows holomorphic networks to be applied to multiply-connected plane domains, removing limitations to simply-connected geometries. This advancement enhances accuracy and computational efficiency in solving two-dimensional differential problems. <br /><br />Summary: <div>
arXiv:2507.22678v1 Announce Type: new 
Abstract: Physics-informed holomorphic neural networks (PIHNNs) have recently emerged as efficient surrogate models for solving differential problems. By embedding the underlying problem structure into the network, PIHNNs require training only to satisfy boundary conditions, often resulting in significantly improved accuracy and computational efficiency compared to traditional physics-informed neural networks (PINNs). In this work, we improve and extend the application of PIHNNs to two-dimensional problems. First, we introduce a novel holomorphic network architecture based on the Kolmogorov-Arnold representation (PIHKAN), which achieves higher accuracy with reduced model complexity. Second, we develop mathematical extensions that broaden the applicability of PIHNNs to a wider class of elliptic partial differential equations, including the Helmholtz equation. Finally, we propose a new method based on Laurent series theory that enables the application of holomorphic networks to multiply-connected plane domains, thereby removing the previous limitation to simply-connected geometries.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep reinforcement learning for efficient exploration of combinatorial structural design spaces</title>
<link>https://arxiv.org/abs/2507.22804</link>
<guid>https://arxiv.org/abs/2507.22804</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, structural design, form finding, sequential decision-making, material efficiency

Summary:
The paper introduces a novel reinforcement learning framework for performance-driven structural design, departing from traditional top-down optimization methods. Structures are modeled as compositions of predefined elements to align with practical constraints like constructability. The framework transforms the design task into a sequential decision-making problem and utilizes a training algorithm inspired by human learning. By applying reinforcement learning to structural design, the method efficiently searches large combinatorial design spaces. Through experimentation on steel braced truss frame cantilever structures, the trained policies consistently generate high-performing and structurally efficient designs based on known engineering principles. The analysis reveals that the agent effectively focuses its search on promising regions of the design space, showcasing transferable structural knowledge. <br /><br />Summary: <div>
arXiv:2507.22804v1 Announce Type: new 
Abstract: This paper proposes a reinforcement learning framework for performance-driven structural design that combines bottom-up design generation with learned strategies to efficiently search large combinatorial design spaces. Motivated by the limitations of conventional top-down approaches such as optimization, the framework instead models structures as compositions of predefined elements, aligning form finding with practical constraints like constructability and component reuse. With the formulation of the design task as a sequential decision-making problem and a human learning inspired training algorithm, the method adapts reinforcement learning for structural design. The framework is demonstrated by designing steel braced truss frame cantilever structures, where trained policies consistently generate distinct, high-performing designs that display structural performance and material efficiency with the use of structural strategies that align with known engineering principles. Further analysis shows that the agent efficiently narrows its search to promising regions of the design space, revealing transferable structural knowledge.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling and simulation of electro-mechanically coupled dielectric elastomers and myocardial tissue using smoothed finite element methods</title>
<link>https://arxiv.org/abs/2507.22838</link>
<guid>https://arxiv.org/abs/2507.22838</guid>
<content:encoded><![CDATA[
<div> tetrahedral meshes, cardiac electro-mechanics, finite element method, di-electric elastomer actuators, smoothed finite element methods 

Summary: 
This study explores the use of smoothed finite element methods (S-FEMs) in cardiac electro-mechanics to address issues with stiffness and volume locking that arise from automatically generated tetrahedral meshes. Four approaches, including standard linear FEM, face-based S-FEM (FS-FEM), node-based S-FEM (NS-FEM), and the hybrid FSNS-FEM, were implemented and evaluated in modeling electrically induced contraction in dielectric elastomers and orthotropic myocardial tissue samples. Results show that FSNS-FEM offers the best balance of accuracy and computational efficiency, closely matching reference data. NS-FEM produces softer results, leading to an overestimation of deformation, while FS-FEM and standard FEM exhibit overly stiff behavior and volume locking. These findings suggest that S-FEMs, particularly FSNS-FEM, hold promise for accurately simulating coupled electro-mechanical behavior in complex biomedical applications.<br /><br /> <div>
arXiv:2507.22838v1 Announce Type: new 
Abstract: Computational modelling offers a cost-effective and time-efficient alternative to experimental studies in biomedical engineering. In cardiac electro-mechanics, finite element method (FEM)-based simulations provide valuable insights into diseased tissue behaviour and the development of assistive systems such as di-electric elastomer actuators. However, the use of automatically generated tetrahedral meshes, commonly applied due to geometric complexity, often leads to numerical issues including overly stiff responses and volume locking, particularly in incompressible materials. Smoothed finite element methods (S-FEMs) offer a promising alternative by softening the stiffness matrix through gradient smoothing over defined smoothing domains. This work extends S-FEM formulations to electro-mechanically coupled problems and compares their performance against standard linear FEM. We implement and evaluate four approaches in the Abaqus environment via custom user elements: standard linear FEM, face-based S-FEM (FS-FEM), node-based S-FEM (NS-FEM), and the hybrid face/node-based S-FEM (FSNS-FEM). Two benchmark problems are studied: the electrically induced contraction of a compressible dielectric elastomer and an incompressible, orthotropic myocardial tissue sample. Reference solutions are obtained using a mesh consisting of higher-order elements. Our results demonstrate that FSNS-FEM provides the best balance between accuracy and computational efficiency, closely matching reference data. NS-FEM produces softer results, which leads to an overestimation of the true deformation. FS-FEM and standard FEM consistently exhibit overly stiff behaviour, with pronounced volume locking in the myocardial case. These findings support the potential of S-FEMs, in particular FSNS-FEM, for accurate simulation of coupled electro-mechanical behaviour in complex biomedical applications.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh based segmentation for automated margin line generation on incisors receiving crown treatment</title>
<link>https://arxiv.org/abs/2507.22859</link>
<guid>https://arxiv.org/abs/2507.22859</guid>
<content:encoded><![CDATA[
<div> Keywords: dental crowns, deep learning, segmentation, margin line, dataset 

Summary: 
- Dental crowns are crucial for restoring damaged teeth, with current design methods relying on manual input for margin line definition.
- A new framework utilizing deep learning was developed to automatically and accurately determine margin lines in dental preparations.
- An ensemble model combined with maximum probability showed the highest success rate in predicting margin lines.
- The quality of the preparation directly affected the accuracy of margin line prediction.
- The study provides the community with the datasets used for training and testing the deep learning model. 

<br /><br />Summary: <div>
arXiv:2507.22859v1 Announce Type: new 
Abstract: Dental crowns are essential dental treatments for restoring damaged or missing teeth of patients. Recent design approaches of dental crowns are carried out using commercial dental design software. Once a scan of a preparation is uploaded to the software, a dental technician needs to manually define a precise margin line on the preparation surface, which constitutes a non-repeatable and inconsistent procedure. This work proposes a new framework to determine margin lines automatically and accurately using deep learning. A dataset of incisor teeth was provided by a collaborating dental laboratory to train a deep learning segmentation model. A mesh-based neural network was modified by changing its input channels and used to segment the prepared tooth into two regions such that the margin line is contained within the boundary faces separating the two regions. Next, k-fold cross-validation was used to train 5 models, and a voting classifier technique was used to combine their results to enhance the segmentation. After that, boundary smoothing and optimization using the graph cut method were applied to refine the segmentation results. Then, boundary faces separating the two regions were selected to represent the margin line faces. A spline was approximated to best fit the centers of the boundary faces to predict the margin line. Our results show that an ensemble model combined with maximum probability predicted the highest number of successful test cases (7 out of 13) based on a maximum distance threshold of 200 m (representing human error) between the predicted and ground truth point clouds. It was also demonstrated that the better the quality of the preparation, the smaller the divergence between the predicted and ground truth margin lines (Spearman's rank correlation coefficient of -0.683). We provide the train and test datasets for the community.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach</title>
<link>https://arxiv.org/abs/2507.22070</link>
<guid>https://arxiv.org/abs/2507.22070</guid>
<content:encoded><![CDATA[
<div> metaclass, Protocol Buffers, test data generation, statistical analysis, enterprise systems <br />
<br />
In this paper, a novel framework for generating test data for enterprise systems using Protocol Buffers is proposed. The framework leverages Python's metaclass system for dynamic type enhancement and statistical analysis of production logs to extract realistic value domains. It combines automatic schema introspection, statistical value distribution analysis, and recursive descent algorithms to handle complex nested data structures. Experimental results show significant reduction in test data preparation time and improvement in test coverage compared to existing approaches. The framework is capable of handling protobuf structures with up to 15 levels of nesting and generating over 100,000 test cases within seconds. This approach addresses the challenges posed by large-scale enterprise systems with intricate hierarchical and graph-like structures, making it a valuable tool for performance testing in such environments. <br /><br />Summary: <div>
arXiv:2507.22070v1 Announce Type: cross 
Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present significant challenges for performance testing, particularly when targeting intermediate business interfaces with complex nested data structures. Traditional test data generation approaches are inadequate for handling the intricate hierarchical and graph-like structures inherent in enterprise protobuf schemas. This paper presents a novel test data generation framework that leverages Python's metaclass system for dynamic type enhancement and statistical analysis of production logs for realistic value domain extraction. Our approach combines automatic schema introspection, statistical value distribution analysis, and recursive descent algorithms for handling deeply nested structures. Experimental evaluation on three real-world enterprise systems demonstrates up to 95\% reduction in test data preparation time and 80\% improvement in test coverage compared to existing approaches. The framework successfully handles protobuf structures with up to 15 levels of nesting and generates comprehensive test suites containing over 100,000 test cases within seconds.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mean-Variance Optimization and Algorithm for Finite-Horizon Markov Decision Processes</title>
<link>https://arxiv.org/abs/2507.22327</link>
<guid>https://arxiv.org/abs/2507.22327</guid>
<content:encoded><![CDATA[
<div> Mean-variance optimization, multi-period, Markov decision processes, pseudo mean, pseudo variance

Summary: 
This paper addresses the challenge of multi-period mean-variance optimization in finite-horizon discrete-time Markov decision processes. By introducing pseudo mean and pseudo variance concepts, the problem is transformed into a bilevel MDP. The bilevel MDP consists of an outer optimization for pseudo mean and an inner MDP with augmented state space. The properties of the bilevel MDP are explored, and an iterative algorithm is proposed to efficiently solve it, converging to a local optimum. Conditions for global optimum convergence are also derived. The approach is applied to multi-period portfolio selection and other scenarios such as queueing control and inventory management. The results align with classic financial engineering findings. This innovative approach presents a new method for mean-variance optimization problems in MDP models with broad applicability. <br /><br />Summary: <div>
arXiv:2507.22327v1 Announce Type: cross 
Abstract: Multi-period mean-variance optimization is a long-standing problem, caused by the failure of dynamic programming principle. This paper studies the mean-variance optimization in a setting of finite-horizon discrete-time Markov decision processes (MDPs), where the objective is to maximize the combined metrics of mean and variance of the accumulated rewards at terminal stage. By introducing the concepts of pseudo mean and pseudo variance, we convert the original mean-variance MDP to a bilevel MDP, where the outer is a single parameter optimization of the pseudo mean and the inner is a standard finite-horizon MDP with an augmented state space by adding an auxiliary state of accumulated rewards. We further study the properties of this bilevel MDP, including the optimality of history-dependent deterministic policies and the piecewise quadratic concavity of the inner MDPs' optimal values with respect to the pseudo mean. To efficiently solve this bilevel MDP, we propose an iterative algorithm that alternatingly updates the inner optimal policy and the outer pseudo mean. We prove that this algorithm converges to a local optimum. We also derive a sufficient condition under which our algorithm converges to the global optimum. Furthermore, we apply this approach to study the mean-variance optimization of multi-period portfolio selection problem, which shows that our approach exactly coincides with the classical result by Li and Ng (2000) in financial engineering. Our approach builds a new avenue to solve mean-variance optimization problems and has wide applicability to any problem modeled by MDPs, which is further demonstrated by examples of mean-variance optimization for queueing control and inventory management.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A surrogate model for topology optimisation of elastic structures via parametric autoencoders</title>
<link>https://arxiv.org/abs/2507.22539</link>
<guid>https://arxiv.org/abs/2507.22539</guid>
<content:encoded><![CDATA[
<div> surrogate-based, topology optimisation, linear elastic structures, parametric loads, boundary conditions <br />
Summary: 
A new surrogate-based algorithm for topology optimization of linear elastic structures under parametric loads and boundary conditions is introduced. The approach involves using a surrogate model to predict quasi-optimal topologies based on system parameters, trained through a feed-forward neural network. This predicted topology serves as an initial guess for a computationally efficient optimization algorithm, allowing for correction of errors and refinement of the design. The method demonstrates superior performance compared to high-fidelity optimizers, reducing the average number of iterations by 53% and maintaining discrepancies below 4% in the optimal functional value. Various architectures are proposed and evaluated for their approximation and generalization capabilities. The quasi-optimal topologies generated by the surrogate model enable efficient and accurate topology optimization even when extrapolating beyond the training and validation domain. <br /> <div>
arXiv:2507.22539v1 Announce Type: cross 
Abstract: A surrogate-based topology optimisation algorithm for linear elastic structures under parametric loads and boundary conditions is proposed. Instead of learning the parametric solution of the state (and adjoint) problems or the optimisation trajectory as a function of the iterations, the proposed approach devises a surrogate version of the entire optimisation pipeline. First, the method predicts a quasi-optimal topology for a given problem configuration as a surrogate model of high-fidelity topologies optimised with the homogenisation method. This is achieved by means of a feed-forward net learning the mapping between the input parameters characterising the system setup and a latent space determined by encoder/decoder blocks reducing the dimensionality of the parametric topology optimisation problem and reconstructing a high-dimensional representation of the topology. Then, the predicted topology is used as an educated initial guess for a computationally efficient algorithm penalising the intermediate values of the design variable, while enforcing the governing equations of the system. This step allows the method to correct potential errors introduced by the surrogate model, eliminate artifacts, and refine the design in order to produce topologies consistent with the underlying physics. Different architectures are proposed and the approximation and generalisation capabilities of the resulting models are numerically evaluated. The quasi-optimal topologies allow to outperform the high-fidelity optimiser by reducing the average number of optimisation iterations by $53\%$ while achieving discrepancies below $4\%$ in the optimal value of the objective functional, even in the challenging scenario of testing the model to extrapolate beyond the training and validation domain.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASCA: LLM based-Multi Agents System for Credit Assessment</title>
<link>https://arxiv.org/abs/2507.22758</link>
<guid>https://arxiv.org/abs/2507.22758</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, agent-based systems, credit assessment, MASCA, hierarchical multi-agent systems

Summary: 
MASCA is introduced as a multi-agent system driven by LLMs to improve credit assessment through a layered architecture approach. The system integrates contrastive learning for risk and reward evaluation and incorporates a signaling game theory perspective for theoretical insights. A detailed bias analysis is included to address fairness concerns in credit assessment. Experimental results show that MASCA outperforms baseline methods, showcasing the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring. <div>
arXiv:2507.22758v1 Announce Type: cross 
Abstract: Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic flux surrogate-based partitioned methods for interface problems</title>
<link>https://arxiv.org/abs/2402.03560</link>
<guid>https://arxiv.org/abs/2402.03560</guid>
<content:encoded><![CDATA[
<div> dynamic mode decomposition, partitioned methods, multiphysics problems, surrogate modeling, parametric PDEs  
Summary:  
Loosely coupled partitioned methods for multiphysics problems are beneficial for code reuse, concurrency, and plug-and-play simulations. However, they can compromise accuracy and stability. This study introduces a data-driven partitioned method for coupled parametric PDEs that improves accuracy without sacrificing performance. By replacing field transfers with a surrogate for interface flux dynamics, the approach uses dynamic mode decomposition on a staggered-in-time state. The offline training phase handles the computational load, while applying the surrogate in the online phase involves a single matrix-vector multiplication. Stability analysis of the scheme is provided, along with numerical results showcasing its effectiveness. <div>
arXiv:2402.03560v2 Announce Type: replace 
Abstract: Loosely coupled partitioned methods for multiphysics problems treat each subproblem as a separate entity and advance them independently in time. In so doing these methods enable code reuse, increase concurrency and provide a convenient framework for plug-and-play multiphysics simulations. However, mathematically loosely coupled schemes are equivalent to a single step of an iterative solution method, which can compromise their accuracy and stability. We present a new data-driven partitioned method for coupled parametric PDEs that can improve upon the accuracy of traditional loosely coupled methods without incurring a performance penalty. To that end, we replace conventional field transfers across the interface by a surrogate for the dynamics of the interface flux exchanged between the subdomains. To develop this surrogate we apply dynamic mode decomposition to a non-standard staggered-in-time state, comprising the interface flux and small solution patches near the interface. The new approach shifts the main computational burden to an offline training phase, whereas application of the surrogate in the online phase amounts to a single matrix-vector multiplication. We provide stability analysis of the surrogate-based partitioned scheme and include numerical results that demonstrate its potential.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Neural Network Training using Dynamic Learning Rate Schedule for PINNs and Image Classification</title>
<link>https://arxiv.org/abs/2507.21749</link>
<guid>https://arxiv.org/abs/2507.21749</guid>
<content:encoded><![CDATA[
<div> dynamic learning rate scheduler, neural networks, training, hyperparameters, optimization

Summary:
The paper introduces a dynamic learning rate scheduler (DLRS) algorithm to address the challenges in training neural networks, particularly in complex problems. The conventional static learning rate approach can lead to tedious training processes and suboptimal results. The DLRS adapts the learning rate based on loss values calculated during training, allowing for more efficient navigation of varying gradients and improved optimization. Experiments on physics-informed neural networks (PINNs) and image classification tasks using multilayer perceptrons and convolutional neural networks show that the DLRS accelerates training and enhances stability. This adaptive approach to learning rate optimization proves to be beneficial in improving the training process and achieving better performance in neural network tasks. <div>
arXiv:2507.21749v1 Announce Type: new 
Abstract: Training neural networks can be challenging, especially as the complexity of the problem increases. Despite using wider or deeper networks, training them can be a tedious process, especially if a wrong choice of the hyperparameter is made. The learning rate is one of such crucial hyperparameters, which is usually kept static during the training process. Learning dynamics in complex systems often requires a more adaptive approach to the learning rate. This adaptability becomes crucial to effectively navigate varying gradients and optimize the learning process during the training process. In this paper, a dynamic learning rate scheduler (DLRS) algorithm is presented that adapts the learning rate based on the loss values calculated during the training process. Experiments are conducted on problems related to physics-informed neural networks (PINNs) and image classification using multilayer perceptrons and convolutional neural networks, respectively. The results demonstrate that the proposed DLRS accelerates training and improves stability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical Knowledge</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, chemistry, reasoning, interpretable outputs, human-AI collaboration <br />
Summary: 
ChemDFM-R is a Chemical Reasoner LLM developed to improve reasoning capabilities in chemistry. The model is trained on a comprehensive dataset of atomized knowledge points to enhance its understanding of fundamental principles. A mix-sourced distillation strategy is used to integrate expert-curated knowledge and general-domain reasoning skills, followed by domain-specific reinforcement learning for enhanced chemical reasoning. ChemDFM-R achieves state-of-the-art performance on diverse chemical benchmarks and provides interpretable, rationale-driven outputs. The model's explicit reasoning chains improve reliability, transparency, and practical utility in real-world human-AI collaboration scenarios. <div>
arXiv:2507.21990v1 Announce Type: new 
Abstract: While large language models (LLMs) have achieved impressive progress, their application in scientific domains such as chemistry remains hindered by shallow domain understanding and limited reasoning capabilities. In this work, we focus on the specific field of chemistry and develop a Chemical Reasoner LLM, ChemDFM-R. We first construct a comprehensive dataset of atomized knowledge points to enhance the model's understanding of the fundamental principles and logical structure of chemistry. Then, we propose a mix-sourced distillation strategy that integrates expert-curated knowledge with general-domain reasoning skills, followed by domain-specific reinforcement learning to enhance chemical reasoning. Experiments on diverse chemical benchmarks demonstrate that ChemDFM-R achieves state-of-the-art performance while providing interpretable, rationale-driven outputs. Further case studies illustrate how explicit reasoning chains significantly improve the reliability, transparency, and practical utility of the model in real-world human-AI collaboration scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2507.07426</link>
<guid>https://arxiv.org/abs/2507.07426</guid>
<content:encoded><![CDATA[
<div> drug repurposing, large language models, structured reasoning, multi-agent collaboration, Monte Carlo Tree Search

Summary:
DrugMCTS is a novel framework that combines Retrieval-Augmented Generation (RAG), multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. It utilizes five specialized agents to retrieve and analyze molecular and protein information, enabling structured and iterative reasoning without domain-specific fine-tuning. The framework outperforms Deepseek-R1 by over 20%, achieving higher recall and robustness on DrugBank and KIBA datasets. The results demonstrate the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in enhancing the application of Large Language Models (LLMs) in drug discovery. <div>
arXiv:2507.07426v2 Announce Type: cross 
Abstract: Recent advances in large language models have demonstrated considerable potential in scientific domains such as drug discovery. However, their effectiveness remains constrained when reasoning extends beyond the knowledge acquired during pretraining. Conventional approaches, such as fine-tuning or retrieval-augmented generation, face limitations in either imposing high computational overhead or failing to fully exploit structured scientific data. To overcome these challenges, we propose DrugMCTS, a novel framework that synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree Search for drug repurposing. The framework employs five specialized agents tasked with retrieving and analyzing molecular and protein information, thereby enabling structured and iterative reasoning. Without requiring domain-specific fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate that DrugMCTS achieves substantially higher recall and robustness compared to both general-purpose LLMs and deep learning baselines. Our results highlight the importance of structured reasoning, agent-based collaboration, and feedback-driven search mechanisms in advancing LLM applications for drug discovery.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bubbleformer: Forecasting Boiling with Transformers</title>
<link>https://arxiv.org/abs/2507.21244</link>
<guid>https://arxiv.org/abs/2507.21244</guid>
<content:encoded><![CDATA[
<div> Transformer-based, spatiotemporal model, boiling dynamics, nucleation, interface evolution, heat transfer <br /> 
Summary: Bubbleformer is a transformer-based spatiotemporal model that accurately forecasts boiling dynamics, including nucleation, interface evolution, and heat transfer, without relying on future input during inference. It integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across various fluid types, geometries, and operating conditions. The model is evaluated using physics-based metrics that assess heat-flux consistency, interface geometry, and mass conservation in chaotic systems. The BubbleML 2.0 dataset accompanying the model includes diverse working fluids and boiling configurations. Bubbleformer achieves new benchmark results in both prediction and forecasting of two-phase boiling flows. <br /> <div>
arXiv:2507.21244v1 Announce Type: cross 
Abstract: Modeling boiling (an inherently chaotic, multiphase process central to energy and thermal systems) remains a significant challenge for neural PDE surrogates. Existing models require future input (e.g., bubble positions) during inference because they fail to learn nucleation from past states, limiting their ability to autonomously forecast boiling dynamics. They also fail to model flow boiling velocity fields, where sharp interface-momentum coupling demands long-range and directional inductive biases. We introduce Bubbleformer, a transformer-based spatiotemporal model that forecasts stable and long-range boiling dynamics including nucleation, interface evolution, and heat transfer without dependence on simulation data during inference. Bubbleformer integrates factorized axial attention, frequency-aware scaling, and conditions on thermophysical parameters to generalize across fluids, geometries, and operating conditions. To evaluate physical fidelity in chaotic systems, we propose interpretable physics-based metrics that evaluate heat-flux consistency, interface geometry, and mass conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans diverse working fluids (cryogens, refrigerants, dielectrics), boiling configurations (pool and flow boiling), flow regimes (bubbly, slug, annular), and boundary conditions. Bubbleformer sets new benchmark results in both prediction and forecasting of two-phase boiling flows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>evoxels: A differentiable physics framework for voxel-based microstructure simulations</title>
<link>https://arxiv.org/abs/2507.21748</link>
<guid>https://arxiv.org/abs/2507.21748</guid>
<content:encoded><![CDATA[
<div> Keywords: materials science, advanced microscopy, predictive simulations, inverse modeling, machine learning

Summary:
Materials science research involves collaboration between experimentalists using advanced microscopy and theorists using computational models to understand the relationship between processing, structure, and properties. Inverse material design, starting from desired performance and working backwards to determine optimal microstructures and manufacturing routes, requires the integration of high-resolution imaging with predictive simulations and data-driven optimization. The evoxels framework, based on a Pythonic voxel-based approach, combines segmented 3D microscopy data with physical simulations, inverse modeling, and machine learning to accelerate discovery and deepen understanding of process-structure-property relationships.<br /><br />Summary: <div>
arXiv:2507.21748v1 Announce Type: cross 
Abstract: Materials science inherently spans disciplines: experimentalists use advanced microscopy to uncover micro- and nanoscale structure, while theorists and computational scientists develop models that link processing, structure, and properties. Bridging these domains is essential for inverse material design where you start from desired performance and work backwards to optimal microstructures and manufacturing routes. Integrating high-resolution imaging with predictive simulations and data-driven optimization accelerates discovery and deepens understanding of process-structure-property relationships. The differentiable physics framework evoxels is based on a fully Pythonic, unified voxel-based approach that integrates segmented 3D microscopy data, physical simulations, inverse modeling, and machine learning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predict Patient Self-reported Race from Skin Histological Images</title>
<link>https://arxiv.org/abs/2507.21912</link>
<guid>https://arxiv.org/abs/2507.21912</guid>
<content:encoded><![CDATA[
<div> Deep learning, Artificial Intelligence, computational pathology, race prediction, bias mitigation  
Summary:   
- The study investigates the use of deep learning models to predict self-reported race from digitized dermatopathology slides.  
- Attention-based mechanisms are applied to uncover race-associated morphological features.  
- Different dataset curation strategies were evaluated to control for confounding factors.  
- White and Black demographic groups showed high prediction performance, while overall performance decreased when considering all groups.  
- Attention analysis revealed the epidermis as a key predictive feature, emphasizing the importance of careful data curation and bias mitigation for equitable AI deployment in pathology.  
<br /><br />Summary: <div>
arXiv:2507.21912v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) has demonstrated success in computational pathology (CPath) for disease detection, biomarker classification, and prognosis prediction. However, its potential to learn unintended demographic biases, particularly those related to social determinants of health, remains understudied. This study investigates whether deep learning models can predict self-reported race from digitized dermatopathology slides and identifies potential morphological shortcuts. Using a multisite dataset with a racially diverse population, we apply an attention-based mechanism to uncover race-associated morphological features. After evaluating three dataset curation strategies to control for confounding factors, the final experiment showed that White and Black demographic groups retained high prediction performance (AUC: 0.799, 0.762), while overall performance dropped to 0.663. Attention analysis revealed the epidermis as a key predictive feature, with significant performance declines when these regions were removed. These findings highlight the need for careful data curation and bias mitigation to ensure equitable AI deployment in pathology. Code available at: https://github.com/sinai-computational-pathology/CPath_SAIF.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falling through the cracks: energy storage along segmented brittle crack fronts</title>
<link>https://arxiv.org/abs/2507.19406</link>
<guid>https://arxiv.org/abs/2507.19406</guid>
<content:encoded><![CDATA[
<div> disjoint crack front, stepped crack, material ligament, 3D measurements, strain energy density
Summary: 
The study focuses on the mechanics of crack propagation in brittle materials, specifically the formation of stepped cracks and material ligaments. Through in-situ 3D measurements using laser scanning, researchers observed the deformation field around stepped cracks and within the ligament feature. They discovered that the ligament concentrates strain energy density, leading to an increase in apparent fracture energy proportional to the strain energy within the ligament. This finding highlights the importance of understanding the role of material ligaments in controlling crack propagation behavior and provides valuable insights into the mechanics of brittle fracture. <div>
arXiv:2507.19406v2 Announce Type: replace 
Abstract: During brittle crack propagation, a smooth crack front curve frequently becomes disjoint, generating a stepped crack and a material ligament that unites the newly formed crack fronts. These universal features fundamentally alter the singular field structure and stability of propagating cracks; however, a quantitative analysis of their mechanics is lacking. Here, we perform in-situ 3D measurements to resolve the deformation field around stepped cracks, and crucially, within the ligament feature. The 3D kinematic data are obtained by scanning a thin laser sheet through the brittle hydrogel samples, while recording the scattered intensity from the embedded tracer particles. We find that the ligament concentrates the strain energy density, and moreover, the apparent fracture energy increases proportionally to the strain energy within the ligament.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFD: A Large-Scale Benchmark for Insider Filing Violation Detection</title>
<link>https://arxiv.org/abs/2507.20162</link>
<guid>https://arxiv.org/abs/2507.20162</guid>
<content:encoded><![CDATA[
<div> Dataset, Insider trading, Form 4 filings, Regulatory compliance, MaBoost <br />
Summary: 
The article introduces the Insider Filing Delay (IFD) dataset, the largest dataset for insider disclosure behavior, covering over one million Form 4 transactions from 2002 to 2025. It addresses the challenge of insider trading violations and delayed disclosures in financial markets by presenting IFD as a benchmark for detecting strategic disclosure violations. The MaBoost framework, a hybrid model combining Mamba-based state space encoder with XGBoost, achieves high accuracy and interpretability in identifying high-risk behavioral patterns. Experiments show that MaBoost outperforms previous approaches, with an F1-score of up to 99.47% under regulatory settings. IFD serves as a realistic and reproducible benchmark for developing AI models in financial compliance, regulatory forensics, and interpretable time-series classification. The dataset and codes are publicly available for further research and analysis. <br /> <div>
arXiv:2507.20162v1 Announce Type: new 
Abstract: Insider trading violations, particularly delayed disclosures of Form 4 filings, remain a persistent challenge for financial market surveillance. Despite regulatory requirements such as the two-business-day rule of the Securities and Exchange Commission (SEC), enforcement is limited by the lack of large-scale, labeled datasets and task-specific benchmarks. In this paper, we introduce Insider Filing Delay (IFD), the first and largest publicly available dataset for insider disclosure behavior, comprising over one million Form 4 transactions spanning two decades (2002-2025), with structured annotations on delay status, insider roles, governance factors, and firm-level financial indicators. IFD enables the first large-scale formulation of strategic disclosure violation detection as a binary classification task grounded in regulatory compliance. To demonstrate the utility of IFD, we propose MaBoost, a hybrid framework combining a Mamba-based state space encoder with XGBoost, achieving high accuracy and interpretability in identifying high-risk behavioral patterns. Experiments across statistical baselines, deep learning models, and large language models confirm that MaBoost outperforms prior approaches, achieving an F1-score of up to 99.47% under constrained regulatory settings. IFD provides a realistic, reproducible, and behavior-rich benchmark for developing AI models in financial compliance, regulatory forensics, and interpretable time-series classification. All data and codes are available: https://github.com/CH-YellowOrange/MaBoost-and-IFD.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Explainable Stock Predictions with Tweets Using Mixture of Experts</title>
<link>https://arxiv.org/abs/2507.20535</link>
<guid>https://arxiv.org/abs/2507.20535</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock price movements, Textual information, LLMs, FTS-Text-MoE model, Financial time series prediction

Summary: 
The FTS-Text-MoE model proposed in this study aims to improve stock price prediction by integrating numerical data with key summaries from news and social media using point embeddings. This model utilizes a Mixture of Experts Transformer decoder to process both data types, reducing computational costs by activating only a subset of model parameters. Multi-resolution prediction heads allow for flexible forecasting of financial time series at different scales. Experimental results demonstrate that FTS-Text-MoE outperforms baseline methods in terms of investment returns and Sharpe ratio, showcasing its superior accuracy and ability to predict future market trends. This approach addresses limitations in prompt-based methods and enhances financial analysis by leveraging factual textual data alongside historical price data. <div>
arXiv:2507.20535v1 Announce Type: new 
Abstract: Stock price movements are influenced by many factors, and alongside historical price data, tex-tual information is a key source. Public news and social media offer valuable insights into market sentiment and emerging events. These sources are fast-paced, diverse, and significantly impact future stock trends. Recently, LLMs have enhanced financial analysis, but prompt-based methods still have limitations, such as input length restrictions and difficulties in predicting sequences of varying lengths. Additionally, most models rely on dense computational layers, which are resource-intensive. To address these challenges, we propose the FTS- Text-MoE model, which combines numerical data with key summaries from news and tweets using point embeddings, boosting prediction accuracy through the integration of factual textual data. The model uses a Mixture of Experts (MoE) Transformer decoder to process both data types. By activating only a subset of model parameters, it reduces computational costs. Furthermore, the model features multi-resolution prediction heads, enabling flexible forecasting of financial time series at different scales. Experimental results show that FTS-Text-MoE outperforms baseline methods in terms of investment returns and Sharpe ratio, demonstrating its superior accuracy and ability to predict future market trends.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exascale Implicit Kinetic Plasma Simulations on El~Capitan for Solving the Micro-Macro Coupling in Magnetospheric Physics</title>
<link>https://arxiv.org/abs/2507.20719</link>
<guid>https://arxiv.org/abs/2507.20719</guid>
<content:encoded><![CDATA[
<div> Keywords: kinetic simulations, magnetospheres, Particle-in-Cell method, AMD Instinct MI300A Accelerated Processing Units, multi-scale coupling <br />
Summary: 
Our study presents fully kinetic, implicit Particle-in-Cell (PIC) simulations of global magnetospheres using El Capitan's AMD Instinct MI300A Accelerated Processing Units. This computational approach addresses the challenge of resolving the multi-scale coupling between microscopic and macroscopic dynamics in planetary magnetospheres. The implicit scheme of iPIC3D allows for larger time steps and grid spacing compared to explicit methods, without compromising accuracy. This capability enables the simulation of magnetospheres while preserving fine-scale electron physics crucial for processes like magnetic reconnection and plasma turbulence. Our innovations in algorithmic and technological aspects, including GPU-optimized kernels and data compression techniques, support the simulation of global-scale dynamics in small-to-medium planetary magnetospheres like Mercury and Ganymede. This advancement extends the reach of fully kinetic PIC codes to systems previously beyond their capabilities.  <br /><br />Summary: <div>
arXiv:2507.20719v1 Announce Type: new 
Abstract: Our fully kinetic, implicit Particle-in-Cell (PIC) simulations of global magnetospheres on up to 32,768 of El Capitan's AMD Instinct MI300A Accelerated Processing Units (APUs) represent an unprecedented computational capability that addresses a fundamental challenge in space physics: resolving the multi-scale coupling between microscopic (electron-scale) and macroscopic (global-scale) dynamics in planetary magnetospheres. The implicit scheme of iPIC3D supports time steps and grid spacing that are up to 10 times larger than those of explicit methods, without sacrificing physical accuracy. This enables the simulation of magnetospheres while preserving fine-scale electron physics, which is critical for key processes such as magnetic reconnection and plasma turbulence. Our algorithmic and technological innovations include GPU-optimized kernels, particle control, and physics-aware data compression using Gaussian Mixture Models. With simulation domains spanning 100-1,000 ion skin depths, we reach the global scale of small-to-medium planetary magnetospheres, such as those of Mercury and Ganymede, which supports fully kinetic treatment of global-scale dynamics in systems previously out of reach for fully kinetic PIC codes.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Programmable Virtual Humans Toward Human Physiologically-Based Drug Discovery</title>
<link>https://arxiv.org/abs/2507.19568</link>
<guid>https://arxiv.org/abs/2507.19568</guid>
<content:encoded><![CDATA[
<div> AI, drug discovery, virtual experiments, programmable virtual humans, translational gap
Summary:
Artificial intelligence in drug discovery has primarily focused on digitizing existing experiments without addressing the challenges of predicting drug effects in humans. Biomedical digital twins, while useful in late-phase development, lack the resolution for early-stage discovery. To overcome this disconnect, programmable virtual humans have emerged as a solution, utilizing AI, high-throughput assays, and omics data to simulate drug actions in the human body. By bridging the translational gap, programmable virtual humans offer a new paradigm for drug discovery centered on human physiology. While offering transformative potential, key opportunities and challenges must be addressed for their realization. <div>
arXiv:2507.19568v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has sparked immense interest in drug discovery, but most current approaches only digitize existing high-throughput experiments. They remain constrained by conventional pipelines. As a result, they do not address the fundamental challenges of predicting drug effects in humans. Similarly, biomedical digital twins, largely grounded in real-world data and mechanistic models, are tailored for late-phase drug development and lack the resolution to model molecular interactions or their systemic consequences, limiting their impact in early-stage discovery. This disconnect between early discovery and late development is one of the main drivers of high failure rates in drug discovery. The true promise of AI lies not in augmenting current experiments but in enabling virtual experiments that are impossible in the real world: testing novel compounds directly in silico in the human body. Recent advances in AI, high-throughput perturbation assays, and single-cell and spatial omics across species now make it possible to construct programmable virtual humans: dynamic, multiscale models that simulate drug actions from molecular to phenotypic levels. By bridging the translational gap, programmable virtual humans offer a transformative path to optimize therapeutic efficacy and safety earlier than ever before. This perspective introduces the concept of programmable virtual humans, explores their roles in a new paradigm of drug discovery centered on human physiology, and outlines key opportunities, challenges, and roadmaps for their realization.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Domain Shift in Multi-source CT-Scan Classification via Input-Space Standardization</title>
<link>https://arxiv.org/abs/2507.19858</link>
<guid>https://arxiv.org/abs/2507.19858</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-source CT-scan, domain shifts, cross-source generalization, input-space standardization, medical imaging

Summary:
Spatial-Slice Feature Learning (SSFL++) and Kernel-Density-based Slice Sampling (KDS) preprocessing pipelines address domain shifts in multi-source CT-scan classification. This study investigates the mechanisms behind the domain robustness of these pipelines and how they manage the trade-off between local discriminability and cross-source generalization. SSFL++ and KDS preprocess the input data, reducing inter-source variance and aligning inputs into a consistent target space to mitigate domain shift. This alignment simplifies the learning task for network optimization and consistently improves performance across different architectures. Experimental validation confirmed the effectiveness of the preprocessing approach, leading to a first-place finish in a competitive challenge. This study highlights the practicality and robustness of input-space standardization in multi-institutional medical imaging.<br /><br />Summary: Input-space standardization through SSFL++ and KDS preprocessing pipelines effectively addresses domain shifts in multi-source CT-scan classification. By reducing inter-source variance and aligning inputs into a consistent target space, these pipelines improve performance across architectures, simplifying the learning task and supporting cross-source generalization. Experimental validation and a first-place finish in a competitive challenge demonstrate the effectiveness and practicality of this preprocessing approach for multi-institutional medical imaging. <div>
arXiv:2507.19858v1 Announce Type: cross 
Abstract: Multi-source CT-scan classification suffers from domain shifts that impair cross-source generalization. While preprocessing pipelines combining Spatial-Slice Feature Learning (SSFL++) and Kernel-Density-based Slice Sampling (KDS) have shown empirical success, the mechanisms underlying their domain robustness remain underexplored. This study analyzes how this input-space standardization manages the trade-off between local discriminability and cross-source generalization. The SSFL++ and KDS pipeline performs spatial and temporal standardization to reduce inter-source variance, effectively mapping disparate inputs into a consistent target space. This preemptive alignment mitigates domain shift and simplifies the learning task for network optimization. Experimental validation demonstrates consistent improvements across architectures, proving the benefits stem from the preprocessing itself. The approach's effectiveness was validated by securing first place in a competitive challenge, supporting input-space standardization as a robust and practical solution for multi-institutional medical imaging.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQUA: A Large Language Model for Aquaculture &amp; Fisheries</title>
<link>https://arxiv.org/abs/2507.20520</link>
<guid>https://arxiv.org/abs/2507.20520</guid>
<content:encoded><![CDATA[
<div> Keywords: Aquaculture, artificial intelligence, large language model, AQUA, AQUADAPT 

Summary: 
Aquaculture is vital for global food security and economies. Challenges like disease outbreaks, inefficient feeding, and hatchery issues persist. Existing machine learning methods lack domain-specific solutions for aquaculture. AQUA, the first large language model tailored for aquaculture, aims to support farmers and researchers. AQUADAPT, an Agentic Framework, generates high-quality synthetic data combining expert knowledge and automated evaluation techniques. This innovation paves the way for LLM-driven advancements in aquaculture research, advisory systems, and decision-making tools. <div>
arXiv:2507.20520v1 Announce Type: cross 
Abstract: Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery</title>
<link>https://arxiv.org/abs/2507.20954</link>
<guid>https://arxiv.org/abs/2507.20954</guid>
<content:encoded><![CDATA[
<div> Python package, SHRED, deep learning strategy, dynamical systems, spatiotemporal data<br />
<br />
Summary: <br />
PySHRED is a Python package that implements SHRED, a deep learning strategy for modeling high-dimensional dynamical systems and spatiotemporal data. The version 1.0 release of PySHRED includes data preprocessors and cutting-edge SHRED methods designed for handling real-world data that may be noisy, multi-scale, parameterized, high-dimensional, and nonlinear. The package is easy to install, well-documented, and includes extensive code examples. It is modularly-structured to support future additions and is released under the MIT license. PySHRED provides extensions for robust sensing, reduced order modeling, and physics discovery, making it a valuable tool for researchers analyzing complex systems. The codebase is available on GitHub, allowing for collaboration and further development in the field of deep learning for dynamical systems. <div>
arXiv:2507.20954v1 Announce Type: cross 
Abstract: SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for modeling high-dimensional dynamical systems and/or spatiotemporal data from dynamical system snapshot observations. PySHRED is a Python package that implements SHRED and several of its major extensions, including for robust sensing, reduced order modeling and physics discovery. In this paper, we introduce the version 1.0 release of PySHRED, which includes data preprocessors and a number of cutting-edge SHRED methods specifically designed to handle real-world data that may be noisy, multi-scale, parameterized, prohibitively high-dimensional, and strongly nonlinear. The package is easy to install, thoroughly-documented, supplemented with extensive code examples, and modularly-structured to support future additions. The entire codebase is released under the MIT license and is available at https://github.com/pyshred-dev/pyshred.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order transmissibility and its linear approximation for in-service crack identification in train wheelset axles</title>
<link>https://arxiv.org/abs/2507.18636</link>
<guid>https://arxiv.org/abs/2507.18636</guid>
<content:encoded><![CDATA[
<div> Keywords: structural health monitoring, crack detection, train wheelset axles, higher-order harmonics, crack identification

Summary: 
Structural health monitoring is a potential method for early crack detection in train wheelset axles. A new crack detection feature called Higher-Order Transmissibility (HOTr) based on higher-order harmonics of breathing crack is proposed. The sensitivity and efficacy of this feature in crack identification are assessed, and a surrogate model based on linear system theory is developed to speed up the crack identification process. The method accurately reproduces the HOTr feature while eliminating the need for iterative solutions of nonlinear equations, reducing computational burden. The results indicate the potential for adoption in in-service damage identification for wheelset axles in near real-time applications.<br /><br />Summary: <div>
arXiv:2507.18636v1 Announce Type: new 
Abstract: In-service structural health monitoring is a so far rarely exploited, yet potent option for early-stage crack detection and identification in train wheelset axles. This procedure is non-trivial to enforce on the basis of a purely data-driven approach and typically requires the adoption of numerical, e.g. finite element-based, simulation schemes of the dynamic behavior of these axles. Damage in this particular case can be formulated as a breathing crack problem, which further complicates simulation by introducing response-dependent nonlinearities into the picture. In this study, first, a new crack detection feature based on higher-order harmonics of the breathing crack is proposed, termed Higher-Order Transmissibility (HOTr), and, secondly, its sensitivity and efficacy are assessed within the context of crack identification. Next, the mentioned feature is approximated via use of linear system theory, delivering a surrogate model which facilitates the computation and speeds up the crack identification procedure. The accuracy of the proposed method in reproducing the delivered HOTr is compared against the nonlinear simulation model. The obtained results suggest that the approximation of the HOTr can significantly reduce the computational burden by eliminating the need for an iterative solution of the governing nonlinear equation of motion, while maintaining a high level of accuracy when compared against the reference model. This implies great potential for adoption in in-service damage identification for wheelset axles, feasibly within a near real-time context.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning coupled Allen-Cahn and Cahn-Hilliard phase-field equations using Physics-informed neural operator(PINO)</title>
<link>https://arxiv.org/abs/2507.18731</link>
<guid>https://arxiv.org/abs/2507.18731</guid>
<content:encoded><![CDATA[
<div> PINOs, phase-field equations, mesoscale microstructural evolution, Physics informed Neural Operators, Al-Cu alloys<br />
<br />
Summary:<br />
Physics informed Neural Operators (PINOs) offer an alternative approach to predict microstructural evolution in materials subjected to periodic boundary conditions. In this study, PINO successfully predicted the growth of $\theta^{\prime}$ precipitates in Al-Cu alloys by solving three coupled physics equations simultaneously, involving two second-order Allen-Cahn equations and one fourth-order Cahn-Hilliard equation. The use of Fourier derivatives, specifically a pseudo-spectral method and Fourier extension, significantly improved the Cahn-Hilliard equation's accuracy. By leveraging the Fourier domain's properties, computing fourth derivatives of the Cahn-Hilliard equation was made more efficient. This research showcases the potential of PINOs in accurately predicting material microstructural evolution with reduced computational cost. <div>
arXiv:2507.18731v1 Announce Type: new 
Abstract: Phase-field equations, mostly solved numerically, are known for capturing the mesoscale microstructural evolution of a material. However, such numerical solvers are computationally expensive as it needs to generate fine mesh systems to solve the complex Partial Differential Equations(PDEs) with good accuracy. Therefore, we propose an alternative approach of predicting the microstructural evolution subjected to periodic boundary conditions using Physics informed Neural Operators (PINOs).
  In this study, we have demonstrated the capability of PINO to predict the growth of $\theta^{\prime}$ precipitates in Al-Cu alloys by learning the operator as well as by solving three coupled physics equations simultaneously. The coupling is of two second-order Allen-Cahn equation and one fourth-order Cahn-Hilliard equation. We also found that using Fourier derivatives(pseudo-spectral method and Fourier extension) instead of Finite Difference Method improved the Cahn-Hilliard equation loss by twelve orders of magnitude. Moreover, since differentiation is equivalent to multiplication in the Fourier domain, unlike Physics informed Neural Networks(PINNs), we can easily compute the fourth derivative of Cahn-Hilliard equation without converting it to coupled second order derivative.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThermoRL:Structure-Aware Reinforcement Learning for Protein Mutation Design to Enhance Thermostability</title>
<link>https://arxiv.org/abs/2507.18816</link>
<guid>https://arxiv.org/abs/2507.18816</guid>
<content:encoded><![CDATA[
<div> Keywords: protein thermostability, mutation design, reinforcement learning, graph neural networks, computational efficiency

Summary: 
ThermoRL is a novel framework utilizing reinforcement learning and graph neural networks to optimize protein thermostability through mutation design. Traditional methods face challenges in balancing sequence variations, structural dynamics, and thermostability. ThermoRL overcomes these limitations by incorporating a hierarchical Q-learning network and a surrogate model for reward feedback, guiding the agent on optimal mutation positions and amino acids. Experimental results demonstrate ThermoRL's ability to outperform baselines in rewards while efficiently filtering out destabilizing mutations and identifying stabilizing mutations aligned with experimental data. The framework's generalizability is highlighted by its accurate detection of key mutation sites in previously unseen proteins. ThermoRL represents a robust alternative to traditional methods for enhancing protein thermostability. 

<br /><br />Summary: <div>
arXiv:2507.18816v1 Announce Type: new 
Abstract: Designing mutations to optimize protein thermostability remains challenging due to the complex relationship between sequence variations, structural dynamics, and thermostability, often assessed by \delta\delta G
  (the change in free energy of unfolding). Existing methods rely on experimental random mutagenesis or prediction models tested with pre-defined datasets, using sequence-based heuristics and treating enzyme design as a one-step process without iterative refinement, which limits design space exploration and restricts discoveries beyond known variations. We present ThermoRL, a framework based on reinforcement learning (RL) that leverages graph neural networks (GNN) to design mutations with enhanced thermostability. It combines a pre-trained GNN-based encoder with a hierarchical Q-learning network and employs a surrogate model for reward feedback, guiding the RL agent on where (the position) and which (mutant amino acid) to apply for enhanced thermostability. Experimental results show that ThermoRL achieves higher or comparable rewards than baselines while maintaining computational efficiency. It filters out destabilizing mutations and identifies stabilizing mutations aligned with experimental data. Moreover, ThermoRL accurately detects key mutation sites in unseen proteins, highlighting its strong generalizability. This RL-guided approach powered by GNN embeddings offers a robust alternative to traditional protein mutation design.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrinityDNA: A Bio-Inspired Foundational Model for Efficient Long-Sequence DNA Modeling</title>
<link>https://arxiv.org/abs/2507.19229</link>
<guid>https://arxiv.org/abs/2507.19229</guid>
<content:encoded><![CDATA[
<div> TrinityDNA, DNA foundational model, long-range dependencies, structural features, Gated Reverse Complement (GRC), multi-scale attention mechanism. 

Summary:
TrinityDNA is a novel DNA foundational model designed to address the challenges of genomic sequence modeling. It integrates biologically informed components like Groove Fusion and GRC to capture DNA's structural features and symmetry. The model also utilizes a multi-scale attention mechanism to attend to varying levels of sequence dependencies. An evolutionary training strategy adapts the model to prokaryotic and eukaryotic genomes. TrinityDNA offers improvements in gene function prediction and regulatory mechanism discovery in genomics applications. It bridges machine learning techniques with biological insights for more effective genomic data analysis. Additionally, a new DNA long-sequence CDS annotation benchmark is introduced for comprehensive evaluations oriented towards practical applications. 

<br /><br />Summary: 
TrinityDNA, a novel DNA foundational model, integrates biologically informed components like Groove Fusion and GRC, along with a multi-scale attention mechanism and evolutionary training strategy, improving gene function prediction and regulatory mechanism discovery in genomics applications. It bridges machine learning techniques with biological insights for more effective genomic data analysis and introduces a DNA long-sequence CDS annotation benchmark for comprehensive evaluations focused on practical applications. <div>
arXiv:2507.19229v1 Announce Type: new 
Abstract: The modeling of genomic sequences presents unique challenges due to their length and structural complexity. Traditional sequence models struggle to capture long-range dependencies and biological features inherent in DNA. In this work, we propose TrinityDNA, a novel DNA foundational model designed to address these challenges. The model integrates biologically informed components, including Groove Fusion for capturing DNA's structural features and Gated Reverse Complement (GRC) to handle the inherent symmetry of DNA sequences. Additionally, we introduce a multi-scale attention mechanism that allows the model to attend to varying levels of sequence dependencies, and an evolutionary training strategy that progressively adapts the model to both prokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and efficient approach to genomic sequence modeling, offering significant improvements in gene function prediction, regulatory mechanism discovery, and other genomics applications. Our model bridges the gap between machine learning techniques and biological insights, paving the way for more effective analysis of genomic data. Additionally, we introduced a new DNA long-sequence CDS annotation benchmark to make evaluations more comprehensive and oriented toward practical applications.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation</title>
<link>https://arxiv.org/abs/2507.19246</link>
<guid>https://arxiv.org/abs/2507.19246</guid>
<content:encoded><![CDATA[
<div> Efficient Uncertainty Quantification, Monte Carlo sampling, Multi-Level Monte Carlo method, Parallel-in-Time integration, computational effort <br />
Summary: <br />
This article introduces a method that combines Multi-Level Monte Carlo sampling with Parallel-in-Time integration to improve the efficiency of Uncertainty Quantification in high-dimensional uncertainty scenarios. While the Multi-Level Monte Carlo method reduces computational effort, it struggles to decrease time to solution in highly parallel computing environments. By leveraging Parallel-in-Time integration for select samples, the proposed method accelerates computation without sacrificing accuracy. Results from numerical examples show a significant speedup of 12-45% compared to Multi-Level Monte Carlo sampling, with a modest increase of 15-18% in total computational effort. The study delves into the tradeoff between time-to-solution and computational effort, presenting theoretical considerations and practical implications for this combined approach. <div>
arXiv:2507.19246v1 Announce Type: new 
Abstract: While generally considered computationally expensive, Uncertainty Quantification using Monte Carlo sampling remains beneficial for applications with uncertainties of high dimension. As an extension of the naive Monte Carlo method, the Multi-Level Monte Carlo method reduces the overall computational effort, but is unable to reduce the time to solution in a sufficiently parallel computing environment. In this work, we propose a Uncertainty Quantification method combining Multi-Level Monte Carlo sampling and Parallel-in-Time integration for select samples, exploiting remaining parallel computing capacity to accelerate the computation. While effective at reducing the time-to-solution, Parallel-in-Time integration methods greatly increase the total computational effort. We investigate the tradeoff between time-to-solution and total computational effort of the combined method, starting from theoretical considerations and comparing our findings to two numerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte Carlo sampling is observed, with an increase of 15 - 18% in computational effort.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning electromagnetic fields based on finite element basis functions</title>
<link>https://arxiv.org/abs/2507.19255</link>
<guid>https://arxiv.org/abs/2507.19255</guid>
<content:encoded><![CDATA[
<div> Keywords: parametric surrogate models, electric machines, isogeometric analysis, proper orthogonal decomposition, deep learning

Summary: 
Parametric surrogate models are essential for efficient design optimization and operational monitoring of electric machines. This study introduces a novel approach that combines isogeometric analysis, proper orthogonal decomposition, and deep learning to predict spline basis coefficients rapidly and accurately. By directly learning these coefficients, the method enables efficient and physically consistent predictions, particularly for parametric nonlinear magnetostatic models of permanent magnet synchronous machines. The effectiveness of this approach is demonstrated in the study, showcasing its potential for enhancing the design and analysis of electric machines. <br /><br />Summary: <div>
arXiv:2507.19255v1 Announce Type: new 
Abstract: Parametric surrogate models of electric machines are widely used for efficient design optimization and operational monitoring. Addressing geometry variations, spline-based computer-aided design representations play a pivotal role. In this study, we propose a novel approach that combines isogeometric analysis, proper orthogonal decomposition and deep learning to enable rapid and physically consistent predictions by directly learning spline basis coefficients. The effectiveness of this method is demonstrated using a parametric nonlinear magnetostatic model of a permanent magnet synchronous machine.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel multi-thickness topology optimization method for balancing structural performance and manufacturability</title>
<link>https://arxiv.org/abs/2507.19388</link>
<guid>https://arxiv.org/abs/2507.19388</guid>
<content:encoded><![CDATA[
<div> Keywords: Topology optimization, multi-thickness, density-based, manufacturability, compliance

Summary:
This paper introduces a novel multi-thickness, density-based topology optimization method that aims to strike a balance between structural performance and manufacturability. By guiding the design towards a predefined set of discrete allowable thicknesses through a multilevel penalization scheme and smoothed Heaviside projection, the method transitions designs from truss-like structures to high-performance sheet-like structures as the number of allowable thickness levels increases. The approach, validated on standard benchmarks, achieves compliance values within 2% of fully unpenalized optimization while outperforming standard SIMP results. The method eliminates impractically thin regions and features, making designs suitable for both additive manufacturing and conventional fabrication. This approach maximizes both performance and manufacturability, addressing the trade-off commonly encountered in two-dimensional topology optimization.<br /><br />Summary: <div>
arXiv:2507.19388v1 Announce Type: new 
Abstract: Topology optimization (TO) in two dimensions often presents a trade-off between structural performance and manufacturability, with unpenalized (variable-thickness) methods yielding superior but complex designs, and penalized (SIMP) methods producing simpler, truss-like structures with compromised performance. This paper introduces a multi-thickness, density-based topology optimization method designed to bridge this gap. The proposed approach guides the design towards a predefined set of discrete, allowable thicknesses by employing a novel multilevel penalization scheme and a multilevel smoothed Heaviside projection. A continuation strategy for the penalization and projection parameters, combined with an adaptive mesh refinement technique, ensures robust convergence and high-resolution geometric features. The method is validated on standard cantilever and MBB beam benchmarks. Results demonstrate that as the number of allowable thicknesses increases, the designs systematically transition from conventional truss-like structures to high-performance, sheet-like structures. Notably, designs with as few as three discrete thickness levels achieve compliance values within 2\% of those from fully unpenalized, variable-thickness optimization, while significantly outperforming standard SIMP results. The method inherently eliminates impractically thin regions and features, both in the out-of-plane and in-plane directions and produces designs well-suited for both additive manufacturing and conventional fabrication using standard-thickness stock materials, thus maximizing both performance and manufacturability.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance as Extended Biology: Reciprocity as the Cognitive Substrate of Financial Behavior</title>
<link>https://arxiv.org/abs/2506.00099</link>
<guid>https://arxiv.org/abs/2506.00099</guid>
<content:encoded><![CDATA[
<div> reciprocity, financial behaviors, credit, insurance, trade, artificial intelligence
Summary:
- The article argues that financial behaviors such as credit, insurance, trade, and token exchange are not products of institutional design but extensions of reciprocity.
- Reciprocity is considered the foundational logic of early human societies, governing the circulation of goods and maintenance of long-term cooperation.
- Trade is seen as the canonical form of reciprocity, involving simultaneous, symmetric, and partner-contingent interactions.
- The four core financial functions mentioned - credit, insurance, token exchange, and investment - are reconstructed as expressions of the underlying principle of reciprocity under different conditions.
- By focusing on the minimal dynamics of reciprocal interaction, the framework shifts the focus from institutional engineering to behavioral computation, providing a new foundation for modeling decentralized financial behavior in both human and artificial agents. 

<br /><br />Summary: <div>
arXiv:2506.00099v2 Announce Type: cross 
Abstract: A central challenge in economics and artificial intelligence is explaining how financial behaviors-such as credit, insurance, and trade-emerge without formal institutions. We argue that these functions are not products of institutional design, but structured extensions of a single behavioral substrate: reciprocity. Far from being a derived strategy, reciprocity served as the foundational logic of early human societies-governing the circulation of goods, regulation of obligation, and maintenance of long-term cooperation well before markets, money, or formal rules. Trade, commonly regarded as the origin of financial systems, is reframed here as the canonical form of reciprocity: simultaneous, symmetric, and partner-contingent. Building on this logic, we reconstruct four core financial functions-credit, insurance, token exchange, and investment-as expressions of the same underlying principle under varying conditions. By grounding financial behavior in minimal, simulateable dynamics of reciprocal interaction, this framework shifts the focus from institutional engineering to behavioral computation-offering a new foundation for modeling decentralized financial behavior in both human and artificial agents.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable inverse design of optical multilayer thin films based on extended neural adjoint and regression activation mapping</title>
<link>https://arxiv.org/abs/2507.18644</link>
<guid>https://arxiv.org/abs/2507.18644</guid>
<content:encoded><![CDATA[
<div> Neural adjoint, optical multilayer thin films, inverse design, material loss function, interpretability<br />
<br />
Summary:
The article introduces an Extended Neural Adjoint (ENA) framework for the inverse design of optical multilayer thin films (OMTs). It meets six key criteria: accuracy, efficiency, diversity, scalability, flexibility, and interpretability. The ENA framework incorporates a material loss function in the neural adjoint method to explore different material configurations of OMTs. The forward neural network architecture (F-RAM) enhances scalability and interpretability by visualizing feature importance. Ablation studies show that the material loss improves accuracy and diversity of OMT solutions. Compared to a residual network-based method (Res-GLOnet), the ENA achieves higher accuracy and better diversity in inverse design. The interpretability of the ENA method is demonstrated by consistent feature importance distributions across OMT structures with similar optical properties. The flexibility of the ENA method is showcased by imposing constraints on the initial layer of OMTs. <br /><br />Summary: <div>
arXiv:2507.18644v1 Announce Type: cross 
Abstract: We propose an extended neural adjoint (ENA) framework, which meets six key criteria for artificial intelligence-assisted inverse design of optical multilayer thin films (OMTs): accuracy, efficiency, diversity, scalability, flexibility, and interpretability. To enhance the scalability of the existing neural adjoint method, we present a novel forward neural network architecture for OMTs and introduce a material loss function into the existing neural adjoint loss function, facilitating the exploration of material configurations of OMTs. Furthermore, we present the detailed formulation of the regression activation mapping for the presented forward neural network architecture (F-RAM), a feature visualization method aimed at improving interpretability. We validated the efficacy of the material loss by conducting an ablation study, where each component of the loss function is systematically removed and evaluated. The results indicated that the inclusion of the material loss significantly improves accuracy and diversity. To substantiate the performance of the ENA-based inverse design, we compared it against the residual network-based global optimization network (Res-GLOnet). The ENA yielded the OMT solutions of an inverse design with higher accuracy and better diversity compared to the Res-GLOnet. To demonstrate the interpretability, we applied F-RAM to diverse OMT structures with similar optical properties, obtained by the proposed ENA method. We showed that distributions of feature importance for various OMT structures exhibiting analogous optical properties are consistent, despite variations in material configurations, layer number, and thicknesses. Furthermore, we demonstrate the flexibility of the ENA method by restricting the initial layer of OMTs to SiO2 and 100 nm.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report</title>
<link>https://arxiv.org/abs/2507.19402</link>
<guid>https://arxiv.org/abs/2507.19402</guid>
<content:encoded><![CDATA[
<div> Keywords: financial transactions, fraud detection, classical machine learning, quantum machine learning, hybrid models
Summary: 
The report investigates and compares classical, quantum, and hybrid machine learning models for detecting fraudulent financial activities using a comprehensive behavioural feature engineering framework. Classical models such as Random Forest outperformed quantum models on the IBM Anti-Money Laundering dataset, achieving high accuracy and F-measure. The proposed FD4QC architecture offers a classical-first, quantum-enhanced approach for real-world deployment. While classical models showed better performance in this study, the Quantum Support Vector Machine (QSVM) demonstrated promise with high precision and low false-positive rates. The results highlight the current limitations of quantum machine learning in financial fraud detection and suggest avenues for future research. 
<br /><br />Summary: <div>
arXiv:2507.19402v1 Announce Type: cross 
Abstract: The increasing complexity and volume of financial transactions pose significant challenges to traditional fraud detection systems. This technical report investigates and compares the efficacy of classical, quantum, and quantum-hybrid machine learning models for the binary classification of fraudulent financial activities.
  As of our methodology, first, we develop a comprehensive behavioural feature engineering framework to transform raw transactional data into a rich, descriptive feature set. Second, we implement and evaluate a range of models on the IBM Anti-Money Laundering (AML) dataset. The classical baseline models include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These are compared against three hybrid classic quantum algorithms architectures: a Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC), and a Hybrid Quantum Neural Network (HQNN).
  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a practical, API-driven system architecture designed for real-world deployment, featuring a classical-first, quantum-enhanced philosophy with robust fallback mechanisms.
  Our results demonstrate that classical tree-based models, particularly \textit{Random Forest}, significantly outperform the quantum counterparts in the current setup, achieving high accuracy (\(97.34\%\)) and F-measure (\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise, delivering high precision (\(77.15\%\)) and a low false-positive rate (\(1.36\%\)), albeit with lower recall and significant computational overhead.
  This report provides a benchmark for a real-world financial application, highlights the current limitations of quantum machine learning in this domain, and outlines promising directions for future research.
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Digital Twin Technologies in Predictive Maintenance: Enabling Transferability via Sim-to-Real and Real-to-Sim Transfer</title>
<link>https://arxiv.org/abs/2507.18449</link>
<guid>https://arxiv.org/abs/2507.18449</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet of Things, Artificial Intelligence, Digital Twins, Reality Gap Analysis, Simulation Model

Summary:
The article discusses the development of Digital Twins (DTs) in the context of the Internet of Things (IoT) and Artificial Intelligence. It emphasizes the importance of standardized frameworks for transitioning DTs from academia to industry. The focus is on transferability, particularly the transfer of knowledge between simulations and real-world operations, known as sim-to-real and real-to-sim transfer. The concept of the "reality gap," the difference between simulated predictions and actual outcomes, is explored. The authors propose integrating a Reality Gap Analysis (RGA) module into existing DT frameworks to manage sim-to-real and real-to-sim transfers effectively. Data pipelines connect the RGA module with historical repositories and simulation models to facilitate bidirectional knowledge transfer. A case study on a pedestrian bridge at Carnegie Mellon University demonstrates the performance of this approach, showcasing efficient bidirectional knowledge transfer without compromising effectiveness.<br /><br />Summary: The article highlights the importance of standardizing DT frameworks for industry adoption and emphasizes the need for bidirectional knowledge transfer between simulations and real-world operations. The integration of a Reality Gap Analysis module enhances this transferability, addressing the challenge of the "reality gap" discrepancy. The proposed approach, demonstrated through a case study, showcases efficient transfer capabilities without compromising effectiveness. <div>
arXiv:2507.18449v1 Announce Type: new 
Abstract: The advancement of the Internet of Things (IoT) and Artificial Intelligence has catalyzed the evolution of Digital Twins (DTs) from conceptual ideas to more implementable realities. Yet, transitioning from academia to industry is complex due to the absence of standardized frameworks. This paper builds upon the authors' previously established functional and informational requirements supporting standardized DT development, focusing on a crucial aspect: transferability. While existing DT research primarily centers on asset transfer, the significance of "sim-to-real transfer" and "real-to-sim transfer"--transferring knowledge between simulations and real-world operations--is vital for comprehensive lifecycle management in DTs. A key challenge in this process is calibrating the "reality gap," the discrepancy between simulated predictions and actual outcomes. Our research investigates the impact of integrating a single Reality Gap Analysis (RGA) module into an existing DT framework to effectively manage both sim-to-real and real-to-sim transfers. This integration is facilitated by data pipelines that connect the RGA module with the existing components of the DT framework, including the historical repository and the simulation model. A case study on a pedestrian bridge at Carnegie Mellon University showcases the performance of different levels of integration of our approach with an existing framework. With full implementation of an RGA module and a complete data pipeline, our approach is capable of bidirectional knowledge transfer between simulations and real-world operations without compromising efficiency.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents</title>
<link>https://arxiv.org/abs/2507.18067</link>
<guid>https://arxiv.org/abs/2507.18067</guid>
<content:encoded><![CDATA[
<div> supervised deep learning framework, neural operators, PDEs, downscaling models, Copernicus ocean current data <br />
Summary: 
This paper introduces a supervised deep learning framework utilizing neural operators to solve partial differential equations (PDEs) and provide high-resolution solutions for ocean current data. The proposed method aims to address the limitations of available satellite products by downscaling models and predicting solutions at arbitrary resolutions. By applying this approach to Copernicus ocean data, the study demonstrates the ability to enhance spatial granularity for detailed local analyses crucial in oceanography. Additionally, the model's versatility allows it to model surrogate PDEs and predict solutions at varying resolutions, irrespective of the input resolution. Evaluation on real-world Copernicus data and synthetic Navier-Stokes simulation datasets showcases the effectiveness of this approach in generating accurate and detailed current data, with potential applications in coastal management, environmental monitoring, and maritime safety. <br /> <div>
arXiv:2507.18067v1 Announce Type: cross 
Abstract: Accurate modeling of physical systems governed by partial differential equations is a central challenge in scientific computing. In oceanography, high-resolution current data are critical for coastal management, environmental monitoring, and maritime safety. However, available satellite products, such as Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and global ocean models, often lack the spatial granularity required for detailed local analyses. In this work, we (a) introduce a supervised deep learning framework based on neural operators for solving PDEs and providing arbitrary resolution solutions, and (b) propose downscaling models with an application to Copernicus ocean current data. Additionally, our method can model surrogate PDEs and predict solutions at arbitrary resolution, regardless of the input resolution. We evaluated our model on real-world Copernicus ocean current data and synthetic Navier-Stokes simulation datasets.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On zero-order consistency residue and background pressure for the conservative SPH fluid dynamics</title>
<link>https://arxiv.org/abs/2507.18210</link>
<guid>https://arxiv.org/abs/2507.18210</guid>
<content:encoded><![CDATA[
<div> SPH method, smoothed particle hydrodynamics, zero-order consistency issue, numerical dissipation, gravity-driven flow<br />
<br />
Summary: 
The study examines the zero-order consistency issue in conservative smoothed particle hydrodynamics (SPH) and its impact on flow simulation in pressure-driven channels and gravity-driven free-surface flows. It identifies the common root cause of non-physical numerical damping in these scenarios as the zero-order gradient consistency residue. The background pressure exacerbates this issue, leading to excessive numerical dissipation. The study conducts theoretical analysis and numerical experiments to understand and mitigate this residue effect, testing sensitive factors like water depth, input dynamic pressure, channel length, resolution, and outlet pressure. The reverse kernel gradient correction technique is effective but has limitations in reducing the residue effect. The study highlights the necessity of correction schemes, especially in scenarios with high background pressure, as demonstrated in the FDA nozzle engineering benchmark. <div>
arXiv:2507.18210v1 Announce Type: cross 
Abstract: As one of the major challenges for the conservative smoothed particle hydrodynamics (SPH) method, the zero-order consistency issue, although thought to be mitigated by the particle regularization scheme, such as the transport velocity formulation, significantly damps the flow in a long channel for both laminar and turbulent simulations. Building on this finding, this paper not only thoroughly analyzes the damping reason in this pressure-driven channel flow, but also relates this problem with the excessive numerical dissipation in the gravity-driven free-surface flow. The common root cause of the non-physical numerical damping in the two typical flow scenarios, the zero-order gradient consistency residue, is exposed. The adverse influence of the background pressure on the residue for the two scenarios is revealed and discussed. To comprehensively understand the behavior of the residue and mitigate its potential adverse effects, we conduct both theoretical analysis and numerical experiments focusing on the key sensitive factors. For studying the residue-induced non-physical energy dissipation in the gravity-driven free-surface flow, the water depth and input dynamic pressure in the inviscid standing wave case are tested. To investigate the velocity loss in the pressure-driven channel flow, we examine the effects of the channel length, resolution, and outlet pressure. The state-of-the-art reverse kernel gradient correction technique is introduced for the two typical flows, and proved to be effective in reducing the residue effect, but we find its correction capability is fundamentally limited. Finally, the FDA nozzle, an engineering benchmark, is tested to demonstrate the residue influence in a complex geometry, highlighting the necessity of correction schemes in scenarios with unavoidable high background pressure.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A stabilized Two-Step Formulation of Maxwell's Equations in the time-domain</title>
<link>https://arxiv.org/abs/2507.18235</link>
<guid>https://arxiv.org/abs/2507.18235</guid>
<content:encoded><![CDATA[
<div> time-domain, Maxwell's equations, Galerkin discretization, low-frequency instability, numerical stability

Summary:
This study presents a novel approach to simulating electromagnetic fields across broad frequency ranges by extending a stabilized two-step formulation of Maxwell's equations to the time-domain. Utilizing a Galerkin discretization in space, the researchers apply two time-discretization schemes tailored to the first- and second-order partial differential equations. To combat low-frequency instabilities, a generalized tree-cotree gauge is incorporated to eliminate the singularity of the curl-curl operator, ensuring robustness even in the static limit. Numerical experimentation on various 3D problems demonstrates the method's stability, accuracy, and its ability to handle nonlinear, temperature-dependent materials. The results affirm the reliability and applicability of this approach in simulating electromagnetic phenomena with diverse frequency characteristics. 

<br /><br />Summary: <div>
arXiv:2507.18235v1 Announce Type: cross 
Abstract: Simulating electromagnetic fields across broad frequency ranges is challenging due to numerical instabilities at low frequencies. This work extends a stabilized two-step formulation of Maxwell's equations to the time-domain. Using a Galerkin discretization in space, we apply two different time-discretization schemes that are tailored to the first- and second-order in time partial differential equations of the two-step solution procedure used here. To address the low-frequency instability, we incorporate a generalized tree-cotree gauge that removes the singularity of the curl-curl operator, ensuring robustness even in the static limit. Numerical results on academic and application-oriented 3D problems confirm stability, accuracy, and the method's applicability to nonlinear, temperature-dependent materials.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Select2Drive: Pragmatic Communications for Real-Time Collaborative Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.12040</link>
<guid>https://arxiv.org/abs/2501.12040</guid>
<content:encoded><![CDATA[
<div> keyword: Vehicle-to-everything communications, autonomous driving, collaborative perception, decision-making, Select2Drive <br />
Summary: <br />
Vehicle-to-everything communications play a crucial role in advancing autonomous driving, with the emergence of PragComm as a promising paradigm. The Select2Drive framework is proposed to optimize limited computational and communication resources for collaborative driving. It introduces distributed predictive perception to reduce latency and enhance decision-making efficiency. By prioritizing critical regions in communication using area-of-importance-based PragComm, Select2Drive boosts both communication efficiency and decision-making efficacy. Empirical evaluations on V2Xverse and real-world DAIR-V2X demonstrate significant improvements in offline perception tasks and driving performance, especially in dense traffic scenarios. Select2Drive showcases a promising approach to enhance collaborative driving through efficient resource utilization and improved decision-making processes. <br /> <div>
arXiv:2501.12040v2 Announce Type: replace 
Abstract: Vehicle-to-everything communications-assisted autonomous driving has witnessed remarkable advancements in recent years, with pragmatic communications (PragComm) emerging as a promising paradigm for real-time collaboration among vehicles and other agents. Simultaneously, extensive research has explored the interplay between collaborative perception and decision-making in end-to-end driving frameworks. In this work, we revisit the collaborative driving problem and propose the Select2Drive framework to optimize the utilization of limited computational and communication resources. Particularly, to mitigate cumulative latency in perception and decision-making, Select2Drive introduces distributed predictive perception by formulating an active prediction paradigm and simplifying high-dimensional semantic feature prediction into a computation cost-efficient, motion-aware reconstruction. Given the ``less is more" principle that an over-broadened perceptual horizon possibly confuses the decision module rather than contributing to it, Select2Drive utilizes area-of-importance-based PragComm to prioritize the communications of critical regions, thus boosting both communication efficiency and decision-making efficacy. Empirical evaluations on the V2Xverse and real-world DAIR-V2X demonstrate that Select2Drive achieves a $2.60$\% and $1.99$\% improvement in offline perception tasks under limited bandwidth (resp., pose error conditions). Moreover, it delivers at most $8.35$\% and $2.65$\% enhancement in closed-loop driving scores and route completion rates, particularly in scenarios characterized by dense traffic and high-speed dynamics.
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding</title>
<link>https://arxiv.org/abs/2507.17353</link>
<guid>https://arxiv.org/abs/2507.17353</guid>
<content:encoded><![CDATA[
<div> dataset, multimodal, road damage, vision language model, infrastructure monitoring <br />
<br /> Summary: RoadBench is introduced as a multimodal benchmark for road damage understanding, combining high-resolution images with textual descriptions for richer context. The RoadCLIP vision language model enhances CLIP with disease-aware positional encoding and road-condition priors for improved road damage recognition. A data generation pipeline using GPT expands the dataset, improving data diversity without manual annotation. Experimental results show RoadCLIP outperforms vision-only models by 19.2%, demonstrating the benefits of combining visual and textual information for enhanced road condition analysis. This work sets new benchmarks for the field and advances infrastructure monitoring through multimodal learning. <div>
arXiv:2507.17353v1 Announce Type: new 
Abstract: Accurate road damage detection is crucial for timely infrastructure maintenance and public safety, but existing vision-only datasets and models lack the rich contextual understanding that textual information can provide. To address this limitation, we introduce RoadBench, the first multimodal benchmark for comprehensive road damage understanding. This dataset pairs high resolution images of road damages with detailed textual descriptions, providing a richer context for model training. We also present RoadCLIP, a novel vision language model that builds upon CLIP by integrating domain specific enhancements. It includes a disease aware positional encoding that captures spatial patterns of road defects and a mechanism for injecting road-condition priors to refine the model's understanding of road damages. We further employ a GPT driven data generation pipeline to expand the image to text pairs in RoadBench, greatly increasing data diversity without exhaustive manual annotation. Experiments demonstrate that RoadCLIP achieves state of the art performance on road damage recognition tasks, significantly outperforming existing vision-only models by 19.2%. These results highlight the advantages of integrating visual and textual information for enhanced road condition analysis, setting new benchmarks for the field and paving the way for more effective infrastructure monitoring through multimodal learning.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Driven Retrosynthesis Prediction with Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.17448</link>
<guid>https://arxiv.org/abs/2507.17448</guid>
<content:encoded><![CDATA[
<div> Reversal planning; organic synthesis; drug discovery; AI-driven advancements; RetroDFM-R <br />
Summary: RetroDFM-R is a reasoning-based large language model (LLM) developed for chemical retrosynthesis to address limitations in existing methods. Through reinforcement learning guided by chemically verifiable rewards, RetroDFM-R improves prediction accuracy and explainability. It outperforms state-of-the-art methods with a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Human assessments confirm the model's chemical plausibility and practicality. RetroDFM-R accurately predicts multistep retrosynthetic routes for drug molecules and perovskite materials from the literature. The model's reasoning process provides interpretable insights, enhancing trust and practical value in real-world retrosynthesis applications.<br /><br />Summary: RetroDFM-R, a reasoning-based large language model, enhances prediction accuracy and explainability in chemical retrosynthesis. It outperforms existing methods on benchmarks and accurately predicts multistep routes for various compounds. Human assessments confirm its utility and the practical value of its reasoning process. <div>
arXiv:2507.17448v1 Announce Type: new 
Abstract: Retrosynthesis planning, essential in organic synthesis and drug discovery, has greatly benefited from recent AI-driven advancements. Nevertheless, existing methods frequently face limitations in both applicability and explainability. Traditional graph-based and sequence-to-sequence models often lack generalized chemical knowledge, leading to predictions that are neither consistently accurate nor easily explainable. To address these challenges, we introduce RetroDFM-R, a reasoning-based large language model (LLM) designed specifically for chemical retrosynthesis. Leveraging large-scale reinforcement learning guided by chemically verifiable rewards, RetroDFM-R significantly enhances prediction accuracy and explainability. Comprehensive evaluations demonstrate that RetroDFM-R significantly outperforms state-of-the-art methods, achieving a top-1 accuracy of 65.0% on the USPTO-50K benchmark. Double-blind human assessments further validate the chemical plausibility and practical utility of RetroDFM-R's predictions. RetroDFM-R also accurately predicts multistep retrosynthetic routes reported in the literature for both real-world drug molecules and perovskite materials. Crucially, the model's explicit reasoning process provides human-interpretable insights, thereby enhancing trust and practical value in real-world retrosynthesis applications.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous Sustainability Assessment via Multimodal AI Agents</title>
<link>https://arxiv.org/abs/2507.17012</link>
<guid>https://arxiv.org/abs/2507.17012</guid>
<content:encoded><![CDATA[
<div> AI agents, life cycle assessment, sustainability, carbon emissions, electronic devices
Summary: 
- AI agents are introduced to streamline the process of calculating cradle-to-gate carbon emissions for electronic devices, reducing expert time while maintaining accuracy.
- A method is developed to estimate environmental impacts by comparing products with similar descriptions, providing quick and accurate results.
- A data-driven approach is implemented to generate emission factors, improving accuracy compared to traditional methods.
- The scalability and implications of this approach for future LCA workflows are analyzed.
- The innovative use of AI and data abstraction tools addresses data availability gaps and enhances the efficiency of sustainability assessments in product manufacturing. <br /><br /> <div>
arXiv:2507.17012v1 Announce Type: cross 
Abstract: Interest in sustainability information has surged in recent years. However, the data required for a life cycle assessment (LCA) that maps the materials and processes from product manufacturing to disposal into environmental impacts (EI) are often unavailable. Here we reimagine conventional LCA by introducing multimodal AI agents that emulate interactions between LCA experts and stakeholders like product managers and engineers to calculate the cradle-to-gate (production) carbon emissions of electronic devices. The AI agents iteratively generate a detailed life-cycle inventory leveraging a custom data abstraction and software tools that extract information from online text and images from repair communities and government certifications. This approach reduces weeks or months of expert time to under one minute and closes data availability gaps while yielding carbon footprint estimates within 19% of expert LCAs with zero proprietary data. Additionally, we develop a method to directly estimate EI by comparing an input to a cluster of products with similar descriptions and known carbon footprints. This runs in 3 ms on a laptop with a MAPE of 12.28% on electronic products. Further, we develop a data-driven method to generate emission factors. We use the properties of an unknown material to represent it as a weighted sum of emission factors for similar materials. Compared to human experts picking the closest LCA database entry, this improves MAPE by 120.26%. We analyze the data and compute scaling of this approach and discuss its implications for future LCA workflows.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image memorability predicts social media virality and externally-associated commenting</title>
<link>https://arxiv.org/abs/2409.14659</link>
<guid>https://arxiv.org/abs/2409.14659</guid>
<content:encoded><![CDATA[
<div> memorability, viral potential, social media, neural network, image categorization  
Summary:  
Memorable images play a crucial role in predicting viral potential on social media platforms. The study utilized neural network ResMem to assess image memorability and correlated it with virality metrics using Reddit image posts. Results showed that memorable images consistently received more comments, even after accounting for image categories. Semantic analysis indicated that memorable images elicited neutral-affect comments, suggesting a unique pathway to virality compared to emotional content. Visual consistency analysis revealed that memorable posts stimulated diverse comments from external sources. Analyses of ResMem's layers highlighted the importance of semantic distinctiveness in both memorability and virality, independent of image category effects. This study sheds light on the link between memorability and social media engagement, emphasizing the role of visual features and human cognitive interactions in online content dissemination.  
<br /><br />Summary: <div>
arXiv:2409.14659v2 Announce Type: replace-cross 
Abstract: Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images are consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional contents. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found that semantic distinctiveness was key to both memorability and virality even after accounting for image category effects. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting CFD Surrogates through Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2507.16069</link>
<guid>https://arxiv.org/abs/2507.16069</guid>
<content:encoded><![CDATA[
<div> Keywords: surrogate models, computational fluid dynamics, interpretability, sparse autoencoders, latent features

Summary: 
Surrogate models are increasingly being used as alternatives to high-fidelity CFD solvers, but their opaque latent representations limit their adoption in safety-critical or regulated environments. This study introduces a posthoc interpretability framework for graph-based surrogate models in CFD, utilizing sparse autoencoders (SAEs). By extracting an overcomplete basis in the node embedding space of a pretrained surrogate, the method generates a dictionary of interpretable latent features. This approach allows for the identification of monosemantic concepts aligned with physical phenomena like vorticity and flow structures. By enhancing explainability and trustworthiness in CFD applications, this model-agnostic pathway provides a valuable tool for improving the transparency of surrogate models. 

<br /><br />Summary: <div>
arXiv:2507.16069v1 Announce Type: new 
Abstract: Learning-based surrogate models have become a practical alternative to high-fidelity CFD solvers, but their latent representations remain opaque and hinder adoption in safety-critical or regulation-bound settings. This work introduces a posthoc interpretability framework for graph-based surrogate models used in computational fluid dynamics (CFD) by leveraging sparse autoencoders (SAEs). By obtaining an overcomplete basis in the node embedding space of a pretrained surrogate, the method extracts a dictionary of interpretable latent features. The approach enables the identification of monosemantic concepts aligned with physical phenomena such as vorticity or flow structures, offering a model-agnostic pathway to enhance explainability and trustworthiness in CFD applications.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational design of personalized drugs via robust optimization under uncertainty</title>
<link>https://arxiv.org/abs/2507.16470</link>
<guid>https://arxiv.org/abs/2507.16470</guid>
<content:encoded><![CDATA[
<div> Keywords: drug composition, release profile, inverse design, topology optimization, uncertainty-aware drug design

Summary: 
This study introduces a computational inverse design approach for optimizing drug composition to achieve a specific release profile necessary for effective disease treatment. The method, based on topology optimization, considers drug material parameters and shape to determine the desired drug composition. The Noyes-Whitney model is used to govern drug release, with robust topology optimization incorporating random material parameters through the stochastic reduced-order method (SROM). Unlike Monte Carlo methods, SROM reduces computational requirements while accurately predicting release profiles. Application of the method to designing drugs with various target release profiles demonstrates close alignment between designed and target profiles. Moreover, SROM-based drug designs exhibit lower uncertainty in release profiles, indicating the effectiveness of this strategy for uncertainty-aware drug design. <div>
arXiv:2507.16470v1 Announce Type: new 
Abstract: Effective disease treatment often requires precise control of the release of the active pharmaceutical ingredient (API). In this work, we present a computational inverse design approach to determine the optimal drug composition that yields a target release profile. We assume that the drug release is governed by the Noyes-Whitney model, meaning that dissolution occurs at the surface of the drug. Our inverse design method is based on topology optimization. The method optimizes the drug composition based on the target release profile, considering the drug material parameters and the shape of the final drug. Our method is non-parametric and applicable to arbitrary drug shapes. The inverse design method is complemented by robust topology optimization, which accounts for the random drug material parameters. We use the stochastic reduced-order method (SROM) to propagate the uncertainty in the dissolution model. Unlike Monte Carlo methods, SROM requires fewer samples and improves computational performance. We apply our method to designing drugs with several target release profiles. The numerical results indicate that the release profiles of the designed drugs closely resemble the target profiles. The SROM-based drug designs exhibit less uncertainty in their release profiles, suggesting that our method is a convincing approach for uncertainty-aware drug design.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-objective Portfolio Optimization Via Gradient Descent</title>
<link>https://arxiv.org/abs/2507.16717</link>
<guid>https://arxiv.org/abs/2507.16717</guid>
<content:encoded><![CDATA[
<div> Keywords: portfolio optimization, multi-objective, gradient descent, automatic differentiation, constraints<br />
Summary:<br />
The article introduces a new approach to portfolio optimization called multi-objective portfolio optimization (MPO) using gradient descent with automatic differentiation. Traditional methods often struggle with scalability and flexibility in complex scenarios. The MPO framework can handle various optimization objectives, constraints, and scenarios, such as minimizing risk measures or maximizing Sharpe ratio while considering constraints like tracking error limits or asset group restrictions. The authors conducted experiments on six scenarios, showing that their method outperforms standard solvers like CVXPY and SKFOLIO in terms of performance and flexibility. The framework aims to be a practical tool for researchers and practitioners dealing with advanced portfolio optimization problems in real-world conditions.<br /><br />Summary: <div>
arXiv:2507.16717v1 Announce Type: new 
Abstract: Traditional approaches to portfolio optimization, often rooted in Modern Portfolio Theory and solved via quadratic programming or evolutionary algorithms, struggle with scalability or flexibility, especially in scenarios involving complex constraints, large datasets and/or multiple conflicting objectives. To address these challenges, we introduce a benchmark framework for multi-objective portfolio optimization (MPO) using gradient descent with automatic differentiation. Our method supports any optimization objective, such as minimizing risk measures (e.g., CVaR) or maximizing Sharpe ratio, along with realistic constraints, such as tracking error limits, UCITS regulations, or asset group restrictions. We have evaluated our framework across six experimental scenarios, from single-objective setups to complex multi-objective cases, and have compared its performance against standard solvers like CVXPY and SKFOLIO. Our results show that our method achieves competitive performance while offering enhanced flexibility for modeling multiple objectives and constraints. We aim to provide a practical and extensible tool for researchers and practitioners exploring advanced portfolio optimization problems in real-world conditions.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking CO$_2$ Storage Simulations: Results from the 11th Society of Petroleum Engineers Comparative Solution Project</title>
<link>https://arxiv.org/abs/2507.15861</link>
<guid>https://arxiv.org/abs/2507.15861</guid>
<content:encoded><![CDATA[
<div> benchmark, simulation tools, geological carbon dioxide storage, quantitative analysis, grid resolution

Summary:<br />
The 11th Society of Petroleum Engineers Comparative Solution Project (SPE11) benchmarked simulation tools for geological carbon dioxide storage, with 18 valid results included in the study. Qualitative variation in results was related to thermal effects, convective mixing, and facies discontinuities, with grid resolution playing a significant role. The quantitative analysis revealed that unreported variations due to human choices during the simulation process were just as impactful as reported computational choices. The study highlights the need for comprehensive documentation and transparency in reporting simulation results for accurate comparative analysis. <div>
arXiv:2507.15861v1 Announce Type: cross 
Abstract: The 11th Society of Petroleum Engineers Comparative Solution Project (shortened SPE11 herein) benchmarked simulation tools for geological carbon dioxide (CO$_2$) storage. A total of 45 groups from leading research institutions and industry across the globe signed up to participate, with 18 ultimately contributing valid results that were included in the comparative study reported here.
  This paper summarizes the SPE11. A comprehensive introduction and qualitative discussion of the submitted data are provided, together with an overview of online resources for accessing the full depth of data. A global metric for analyzing the relative distance between submissions is proposed and used to conduct a quantitative analysis of the submissions. This analysis attempts to statistically resolve the key aspects influencing the variability between submissions.
  The study shows that the major qualitative variation between the submitted results is related to thermal effects, dissolution-driven convective mixing, and resolution of facies discontinuities. Moreover, a strong dependence on grid resolution is observed across all three versions of the SPE11. However, our quantitative analysis suggests that the observed variations are predominantly influenced by factors not documented in the technical responses provided by the participants. We therefore identify that unreported variations due to human choices within the process of setting up, conducting, and reporting on the simulations underlying each SPE11 submission are at least as impactful as the computational choices reported.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making</title>
<link>https://arxiv.org/abs/2507.14542</link>
<guid>https://arxiv.org/abs/2507.14542</guid>
<content:encoded><![CDATA[
<div> Keywords: High-frequency oscillations, intracranial Electroencephalography, machine learning, variational autoencoder, epilepsy treatment

Summary:<br />
- High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are important in localizing the epileptogenic zone in epilepsy treatment.
- Traditional rule-based detectors for HFOs have low precision, leading to false positives that require manual review.
- Supervised machine learning approaches for classification depend on labeled datasets, which are hard to acquire and label consistently.
- The Self-Supervised to Label Discovery (SS2LD) framework uses a variational autoencoder (VAE) to refine candidate events from legacy detectors into precise pathological HFOs.
- SS2LD outperforms state-of-the-art methods in identifying pathological HFOs, offering a scalable, label-efficient, and clinically effective strategy using legacy detectors.<br /> 
Summary: <div>
arXiv:2507.14542v1 Announce Type: new 
Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography (iEEG) are critical biomarkers for localizing the epileptogenic zone in epilepsy treatment. However, traditional rule-based detectors for HFOs suffer from unsatisfactory precision, producing false positives that require time-consuming manual review. Supervised machine learning approaches have been used to classify the detection results, yet they typically depend on labeled datasets, which are difficult to acquire due to the need for specialized expertise. Moreover, accurate labeling of HFOs is challenging due to low inter-rater reliability and inconsistent annotation practices across institutions. The lack of a clear consensus on what constitutes a pathological HFO further challenges supervised refinement approaches. To address this, we leverage the insight that legacy detectors reliably capture clinically relevant signals despite their relatively high false positive rates. We thus propose the Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of candidate events generated by legacy detectors into a precise set of pathological HFOs. SS2LD employs a variational autoencoder (VAE) for morphological pre-training to learn meaningful latent representation of the detected events. These representations are clustered to derive weak supervision for pathological events. A classifier then uses this supervision to refine detection boundaries, trained on real and VAE-augmented data. Evaluated on large multi-institutional interictal iEEG datasets, SS2LD outperforms state-of-the-art methods. SS2LD offers a scalable, label-efficient, and clinically effective strategy to identify pathological HFOs using legacy detectors.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Generative Models in Condition and Structural Health Monitoring: Opportunities, Limitations and Future Outlook</title>
<link>https://arxiv.org/abs/2507.15026</link>
<guid>https://arxiv.org/abs/2507.15026</guid>
<content:encoded><![CDATA[
<div> Keywords: Condition monitoring, Structural health monitoring, Deep generative models, Fault diagnosis, Anomaly detection

Summary: 
Condition and structural health monitoring (CM/SHM) are crucial for predictive maintenance in various industrial sectors. Conventional deep learning models face challenges such as operational variability, imbalanced datasets, and multimodal sensory data. Deep generative models (DGMs) like autoregressive models and generative adversarial networks offer solutions by generating data samples, reconstructing system states, and handling multimodal data. This review compares DGMs with traditional models in CM/SHM, focusing on tasks like data imbalance, domain adaptation, and fault diagnosis. Limitations of DGMs include explainability issues, computational inefficiencies, and the need for parameter-efficient strategies. Future research can explore zero-shot learning, multimodal generalization, hybrid architectures combining DGMs with physics knowledge, and reinforcement learning with DGMs for industrial scenarios. <div>
arXiv:2507.15026v1 Announce Type: new 
Abstract: Condition and structural health monitoring (CM/SHM) is a pivotal component of predictive maintenance (PdM) strategies across diverse industrial sectors, including mechanical rotating machinery, airplane composite wings, offshore wind turbines, and civil engineering structures. Conventional deep learning models, while effective in fault diagnosis and anomaly detection through supervised feature extraction and rule-based data augmentation, often struggle with operational variability, imbalanced or scarce fault datasets, and multimodal sensory data from complex systems. Deep generative models (DGMs) in this regard, including autoregressive models, variational autoencoders, generative adversarial networks, diffusion-based models, and emerging large language models, offer transformative capabilities by synthesizing high-fidelity data samples, reconstructing latent system states, and modeling complex multimodal data streams. This review systematically examines state-of-the-art DGM applications in CM/SHM systems, emphasizing their role in addressing key challenges: data imbalance and imputation, domain adaptation and generalization, multimodal data fusion, and downstream fault diagnosis and anomaly detection tasks, with rigorous comparison among signal processing, conventional machine learning or deep learning models, and DGMs. We also analyze current limitations of DGMs, including challenges of explainable and trustworthy models, computational inefficiencies for edge deployment, and the need for parameter-efficient fine-tuning strategies. Future research directions can focus on zero-shot and few-shot learning, robust multimodal generalization, hybrid architectures integrating DGMs with physics knowledge, and reinforcement learning with DGMs to enhance robustness and accuracy in industrial scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Configurational-force-driven adaptive refinement and coarsening in topology optimization</title>
<link>https://arxiv.org/abs/2507.15570</link>
<guid>https://arxiv.org/abs/2507.15570</guid>
<content:encoded><![CDATA[
<div> Keywords: topology optimization, configurational forces, mesh adaptivity, Eshelby stress, multi-level refinement

Summary:
Topology optimization often requires solving numerous linear equation systems, leading to high computational costs due to fine mesh requirements. A multi-level adaptive refinement and coarsening strategy based on configurational forces is proposed to address this challenge. Configurational forces, derived from Eshelby stress, predict configurational changes like crack propagation. By utilizing configurational forces for refinement, a high-resolution structure is achieved along design boundaries and stress-critical regions, while multilevel coarsening reduces computational effort. This approach is particularly advantageous in stress-sensitive problems where preventing stress failure is crucial. Ultimately, the use of configurational forces for mesh adaptivity in topology optimization results in geometrically well-defined structures with significantly reduced computational costs. 

<br /><br />Summary: <div>
arXiv:2507.15570v1 Announce Type: new 
Abstract: The iterative nature of topology optimization, especially in combination with nonlinear state problems, often requires the solution of thousands of linear equation systems. Furthermore, due to the pixelated design representation, the use of a fine mesh is essential to obtain geometrically well-defined structures and to accurately compute response quantities such as the von Mises stress. Therefore, the computational cost of solving a fine-mesh topology optimization problem quickly adds up. To address this challenge, we consider a multi-level adaptive refinement and coarsening strategy based on configurational forces. Configurational forces based on the Eshelby stress predict configurational changes such as crack propagation or dislocation motion. Due to a relaxation in the calculation of (Eshelby) stresses with respect to the design variables, discrete configurational forces increase not only in highly stressed regions, but also in grey transition regions (design boundaries). For this reason they are an ideal criterion for mesh adaptivity in topology optimization, especially when avoiding stress failure is a priority. By using configurational forces for refinement, we obtain a high-resolution structure where the refined mesh is present along the design boundaries as well as in stress-critical regions. At the same time, multilevel coarsening using the same criterion drastically minimizes the computational effort.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers</title>
<link>https://arxiv.org/abs/2507.15753</link>
<guid>https://arxiv.org/abs/2507.15753</guid>
<content:encoded><![CDATA[
<div> diffusion transformers, metamaterials, generative framework, 3D geometries, mechanical objectives<br />
<br />
Summary:<br />
A new generative framework called DiffuMeta has been developed to enable the inverse design of three-dimensional metamaterials. It uses diffusion transformers and a novel algebraic language representation to encode complex 3D geometries as mathematical sentences. This approach allows for the precise control of stress-strain responses in shell structures, considering factors like buckling and contact. DiffuMeta can generate diverse solutions to address the one-to-many mapping challenge and simultaneously optimize multiple mechanical objectives, including nonlinear responses. Experimental validation has shown the effectiveness of DiffuMeta in designing metamaterials with tailored properties, demonstrating its potential for accelerating the design process of complex structures. <div>
arXiv:2507.15753v1 Announce Type: new 
Abstract: Generative machine learning models have revolutionized material discovery by capturing complex structure-property relationships, yet extending these approaches to the inverse design of three-dimensional metamaterials remains limited by computational complexity and underexplored design spaces due to the lack of expressive representations. Here, we present DiffuMeta, a generative framework integrating diffusion transformers with a novel algebraic language representation, encoding 3D geometries as mathematical sentences. This compact, unified parameterization spans diverse topologies while enabling direct application of transformers to structural design. DiffuMeta leverages diffusion models to generate novel shell structures with precisely targeted stress-strain responses under large deformations, accounting for buckling and contact while addressing the inherent one-to-many mapping by producing diverse solutions. Uniquely, our approach enables simultaneous control over multiple mechanical objectives, including linear and nonlinear responses beyond training domains. Experimental validation of fabricated structures further confirms the efficacy of our approach for accelerated design of metamaterials and structures with tailored properties.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Physics Discovery through Fully Differentiable Finite Element-Based Machine Learning</title>
<link>https://arxiv.org/abs/2507.15787</link>
<guid>https://arxiv.org/abs/2507.15787</guid>
<content:encoded><![CDATA[
<div> finite element-based machine learning, surrogate-modelling, PDE solutions, unknown physics, trainable operators <br />
Summary: 
The article introduces a novel approach called fully differentiable finite element-based machine learning (FEBML) to address limitations in existing surrogate-modelling methods for problems involving unknown or incomplete relationships in PDEs. FEBML embeds trainable operators for unknown physics within a state-of-the-art FEM solver, enabling end-to-end differentiation. It represents unknown operators as an encode-process-decode pipeline over finite-element degrees of freedom, ensuring learned physics respects the variational structure. The versatility of FEBML is demonstrated by successfully recovering nonlinear stress-strain laws from laboratory tests, applying the learned model to new mechanical scenarios without retraining, and identifying temperature-dependent conductivity in transient heat flow. This new framework offers promise for improving accuracy and generality in modelling complex scientific and engineering problems. <br /> <div>
arXiv:2507.15787v1 Announce Type: new 
Abstract: Although many problems in science and engineering are modelled by well-established PDEs, they often involve unknown or incomplete relationships, such as material constitutive laws or thermal response, that limit accuracy and generality. Existing surrogate-modelling approaches directly approximate PDE solutions but remain tied to a specific geometry, boundary conditions, and set of physical constraints. To address these limitations, we introduce a fully differentiable finite element-based machine learning (FEBML) framework that embeds trainable operators for unknown physics within a state-of-the-art, general FEM solver, enabling true end-to-end differentiation. At its core, FEBML represents each unknown operator as an encode-process-decode pipeline over finite-element degrees of freedom: field values are projected to nodal coefficients, transformed by a neural network, and then lifted back to a continuous FE function, ensuring the learned physics respects the variational structure. We demonstrate its versatility by recovering nonlinear stress-strain laws from laboratory tests, applying the learned model to a new mechanical scenario without retraining, and identifying temperature-dependent conductivity in transient heat flow.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions</title>
<link>https://arxiv.org/abs/2507.14245</link>
<guid>https://arxiv.org/abs/2507.14245</guid>
<content:encoded><![CDATA[
<div> Keywords: nanomaterials, proteins, AI, NanoPro-3M, multimodal representation learning <br />
Summary: 
NanoPro-3M dataset is introduced, containing over 3.2 million samples and 37,000 unique proteins, aiming to enhance understanding of nanomaterial-protein interactions. NanoProFormer model is proposed to predict these interactions with strong generalization abilities, handling missing features and unseen entities. The model outperforms single-modality approaches by utilizing multimodal representation learning, identifying crucial factors influencing corona formation. It showcases applicability to various tasks via zero-shot inference and fine-tuning. This work lays the groundwork for accurate and generalized predictions of nanomaterial-protein interactions, reducing reliance on experimental data and speeding up in vitro applications. <br /><br />Summary: <div>
arXiv:2507.14245v1 Announce Type: cross 
Abstract: Unlocking the potential of nanomaterials in medicine and environmental science hinges on understanding their interactions with proteins, a complex decision space where AI is poised to make a transformative impact. However, progress has been hindered by limited datasets and the restricted generalizability of existing models. Here, we propose NanoPro-3M, the largest nanomaterial-protein interaction dataset to date, comprising over 3.2 million samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer, a foundational model that predicts nanomaterial-protein affinities through multimodal representation learning, demonstrating strong generalization, handling missing features, and unseen nanomaterials or proteins. We show that multimodal modeling significantly outperforms single-modality approaches and identifies key determinants of corona formation. Furthermore, we demonstrate its applicability to a range of downstream tasks through zero-shot inference and fine-tuning. Together, this work establishes a solid foundation for high-performance and generalized prediction of nanomaterial-protein interaction endpoints, reducing experimental reliance and accelerating various in vitro applications.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Large Language Models know about materials?</title>
<link>https://arxiv.org/abs/2507.14586</link>
<guid>https://arxiv.org/abs/2507.14586</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Mechanical Engineering, Materials Science, Periodic Table of Elements, Benchmark 

Summary: 
Large Language Models (LLMs) are being used in the fields of mechanical engineering and materials science to facilitate step-wise reasoning through the Processing-Structure-Property-Performance chain. While current LLMs are trained on a wide range of internet data, much of this data is non-scientific. To ensure LLMs can provide accurate information about materials, it is important to evaluate their intrinsic knowledge. In this study, the researchers focus on the example of the Periodic Table of Elements to assess LLMs' ability to generate factually correct output. By analyzing the vocabulary and tokenization used in different LLM models, the study highlights the uniqueness of material fingerprints and identifies the need for specialized models in various stages of the PSPP chain. This work serves as a benchmark for determining which steps in the material science process LLMs are suitable for and where specialized models may be necessary. 

<br /><br />Summary: <div>
arXiv:2507.14586v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly applied in the fields of mechanical engineering and materials science. As models that establish connections through the interface of language, LLMs can be applied for step-wise reasoning through the Processing-Structure-Property-Performance chain of material science and engineering. Current LLMs are built for adequately representing a dataset, which is the most part of the accessible internet. However, the internet mostly contains non-scientific content. If LLMs should be applied for engineering purposes, it is valuable to investigate models for their intrinsic knowledge -- here: the capacity to generate correct information about materials. In the current work, for the example of the Periodic Table of Elements, we highlight the role of vocabulary and tokenization for the uniqueness of material fingerprints, and the LLMs' capabilities of generating factually correct output of different state-of-the-art open models. This leads to a material knowledge benchmark for an informed choice, for which steps in the PSPP chain LLMs are applicable, and where specialized models are required.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators</title>
<link>https://arxiv.org/abs/2507.14652</link>
<guid>https://arxiv.org/abs/2507.14652</guid>
<content:encoded><![CDATA[
<div> Hamiltonian Monte Carlo, Bayesian neural networks, variational inference, stochastic gradient MCMC, uncertainty estimation <br />
Summary: 
This paper introduces a hybrid approach that combines variational inference and Hamiltonian Monte Carlo (HMC) methods to efficiently estimate uncertainties in neural networks. By initially training with variational inference and identifying non-contributory parameters, the dimension of the parameter space is reduced, allowing for faster and more accurate HMC inference. This approach is demonstrated on deep neural networks and operator networks, showing effectiveness in learning surrogates for complex physical systems. The method enables inference for large networks with tens to hundreds of thousands of parameters, showcasing its efficiency and accuracy in quantifying uncertainties. Additionally, the approach is applied to model an operator mapping in hypersonic flow, illustrating its capability in learning complex relations between input conditions and output data. <br /> <div>
arXiv:2507.14652v1 Announce Type: cross 
Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample from the posterior distribution in Bayesian inference. However, HMC techniques are computationally demanding for Bayesian neural networks due to the high dimensionality of the network's parameter space and the non-convexity of their posterior distributions. Therefore, various approximation techniques, such as variational inference (VI) or stochastic gradient MCMC, are often employed to infer the posterior distribution of the network parameters. Such approximations introduce inaccuracies in the inferred distributions, resulting in unreliable uncertainty estimates. In this work, we propose a hybrid approach that combines inexpensive VI and accurate HMC methods to efficiently and accurately quantify uncertainties in neural networks and neural operators. The proposed approach leverages an initial VI training on the full network. We examine the influence of individual parameters on the prediction uncertainty, which shows that a large proportion of the parameters do not contribute substantially to uncertainty in the network predictions. This information is then used to significantly reduce the dimension of the parameter space, and HMC is performed only for the subset of network parameters that strongly influence prediction uncertainties. This yields a framework for accelerating the full batch HMC for posterior inference in neural networks. We demonstrate the efficiency and accuracy of the proposed framework on deep neural networks and operator networks, showing that inference can be performed for large networks with tens to hundreds of thousands of parameters. We show that this method can effectively learn surrogates for complex physical systems by modeling the operator that maps from upstream conditions to wall-pressure data on a cone in hypersonic flow.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Profiling and Address Role Inference in Tokenized U.S. Treasuries</title>
<link>https://arxiv.org/abs/2507.14808</link>
<guid>https://arxiv.org/abs/2507.14808</guid>
<content:encoded><![CDATA[
<div> Tokenized U.S. Treasuries, real-world assets, blockchain networks, transaction-level behavior, functional dissection<br />
<br />
Summary: Tokenized U.S. Treasuries, a subset of real-world assets, are yield-bearing instruments collateralized by sovereign debt deployed on various blockchain networks. This study delves into the transaction-level behavior of U.S. Treasury-backed RWA tokens (e.g., BUIDL, BENJI, USDY) across multiple chains, identifying core functional primitives like issuance, redemption, transfer, and bridge activity. The analysis reveals a distinction in behavior between institutional and retail users. A curvature-aware representation learning framework utilizing Poincar embeddings and liquidity-based graph features is introduced for address-level economic role modeling. The method surpasses baseline models in role inference and extends to anomaly detection and wallet classification in broader blockchain transaction networks. These findings offer insights into functional diversity and participant roles in tokenized Treasuries on a transaction-specific level, enhancing understanding of on-chain financialization.<br /><br /> <div>
arXiv:2507.14808v1 Announce Type: cross 
Abstract: Tokenized U.S. Treasuries have emerged as a prominent subclass of real-world assets (RWAs), offering cryptographically enforced, yield-bearing instruments collateralized by sovereign debt and deployed across multiple blockchain networks. While the market has expanded rapidly, empirical analyses of transaction-level behaviour remain limited. This paper conducts a quantitative, function-level dissection of U.S. Treasury-backed RWA tokens including BUIDL, BENJI, and USDY, across multi-chain: mostly Ethereum and Layer-2s. We analyze decoded contract calls to isolate core functional primitives such as issuance, redemption, transfer, and bridge activity, revealing segmentation in behaviour between institutional actors and retail users. To model address-level economic roles, we introduce a curvature-aware representation learning framework using Poincar\'e embeddings and liquidity-based graph features. Our method outperforms baseline models on our RWA Treasury dataset in role inference and generalizes to downstream tasks such as anomaly detection and wallet classification in broader blockchain transaction networks. These findings provide a structured understanding of functional heterogeneity and participant roles in tokenized Treasury in a transaction-level perspective, contributing new empirical evidence to the study of on-chain financialization.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering</title>
<link>https://arxiv.org/abs/2507.15003</link>
<guid>https://arxiv.org/abs/2507.15003</guid>
<content:encoded><![CDATA[
<div> Keywords: AI teammates, autonomous coding agents, software engineering, dataset, collaboration

Summary:<br /><br />
The paper introduces AIDev, a large-scale dataset capturing the operation of autonomous coding agents in software development. It includes data from over 456,000 pull requests by five leading agents across 61,000 repositories and 47,000 developers. AIDev provides rich metadata on pull requests, authorship, review timelines, code changes, and integration outcomes. The dataset allows for research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. While autonomous agents show faster code submission compared to humans, their pull requests are accepted less frequently, indicating a trust and utility gap. The dataset highlights that even though agents accelerate the submission process, the code they generate is structurally simpler. AIDev is intended to be a living resource for the software engineering and AI communities, aiming to support research into AI-native workflows and symbiotic human-AI collaboration. The dataset is publicly available for further analysis and exploration.<br /><br />Summary: <div>
arXiv:2507.15003v1 Announce Type: cross 
Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI teammates: autonomous, goal-driven systems collaborating with human developers. Among these, autonomous coding agents are especially transformative, now actively initiating, reviewing, and evolving code at scale. This paper introduces AIDev, the first large-scale dataset capturing how such agents operate in the wild. Spanning over 456,000 pull requests by five leading agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across 61,000 repositories and 47,000 developers, AIDev provides an unprecedented empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software engineering, AIDev offers structured, open data to support research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. The dataset includes rich metadata on PRs, authorship, review timelines, code changes, and integration outcomes--enabling exploration beyond synthetic benchmarks like SWE-bench. For instance, although agents often outperform humans in speed, their PRs are accepted less frequently, revealing a trust and utility gap. Furthermore, while agents accelerate code submission--one developer submitted as many PRs in three days as they had in three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev enables a new generation of research into AI-native workflows and supports building the next wave of symbiotic human-AI collaboration. The dataset is publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperelastic nature of the Hoek-Brown criterion</title>
<link>https://arxiv.org/abs/2507.15813</link>
<guid>https://arxiv.org/abs/2507.15813</guid>
<content:encoded><![CDATA[
<div> hyperbolic elasticity, elasto-plastic model, yield criterion, plasticity, finite element simulations
Summary:
The article presents a new nonlinear elasto-plastic model that incorporates hyperbolic elasticity resulting from an invariant yield criterion on the plasticity level. This model combines nonlinear elastic behavior with plasticity adhering to the associated flow rule. It highlights the connection between a linear yield criterion on the thermodynamic force of plasticity and a quadratic yield criterion in stress space, indicating a relationship between different yield criteria. Comparisons between linear and hyperbolic elasticity in the context of the Drucker-Prager yield criterion show the nonlinear case exhibiting dilatancy saturation in triaxial compression tests. Structural finite element simulations are conducted to demonstrate the practicality of the proposed model, showcasing its numerical applicability in various scenarios. <div>
arXiv:2507.15813v1 Announce Type: cross 
Abstract: We propose a nonlinear elasto-plastic model, for which a specific class of hyperbolic elasticity arises as a straight consequence of the yield criterion invariance on the plasticity level. We superimpose this nonlinear elastic (or hyperelastic) behavior with plasticity obeying the associated flow rule. Interestingly, we find that a linear yield criterion on the thermodynamical force associated with plasticity results in a quadratic yield criterion in the stress space. This suggests a specific hyperelastic connection between Mohr-Coulomb and Hoek-Brown (or alternatively between Drucker-Prager and Pan-Hudson) yield criteria. We compare the elasto-plastic responses of standard tests for the Drucker-Prager yield criterion using either linear or the suggested hyperbolic elasticity. Notably, the nonlinear case stands out due to dilatancy saturation observed during cyclic loading in the triaxial compression test. We conclude this study with structural finite element simulations that clearly demonstrate the numerical applicability of the proposed model.
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection</title>
<link>https://arxiv.org/abs/2507.13459</link>
<guid>https://arxiv.org/abs/2507.13459</guid>
<content:encoded><![CDATA[
<div> Graph neural network, surrogate modeling, contact mechanics, soft deformable bodies, computational cost 
Summary: 
The article introduces a graph neural network architecture for surrogate modeling in nonlinear boundary value problems in mechanics, focusing on contact between soft deformable bodies. This approach incorporates continuous collision detection and sufficient conditions for contact, improving generalization of the network. The framework is tested on soft tissue mechanics problems, such as predicting the closed state of a bioprosthetic aortic valve, demonstrating better generalization with additional contact terms in the loss function. The network can handle complex contact scenarios with varying reference geometries but comes with high computational costs during training. Despite this, the implementation results in significant speedups for inference, showing up to a thousand-fold improvement on benchmark problems. Overall, the graph neural network offers promising advancements in efficiently solving contact mechanics problems involving soft deformable bodies. 
<br /><br />Summary: <div>
arXiv:2507.13459v1 Announce Type: new 
Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems in mechanics are helpful in a broad range of engineering applications. However, effective surrogate modeling of applications involving the contact of deformable bodies, especially in the context of varying geometries, is still an open issue. In particular, existing methods are confined to rigid body contact or, at best, contact between rigid and soft objects with well-defined contact planes. Furthermore, they employ contact or collision detection filters that serve as a rapid test but use only the necessary and not sufficient conditions for detection. In this work, we present a graph neural network architecture that utilizes continuous collision detection and, for the first time, incorporates sufficient conditions designed for contact between soft deformable bodies. We test its performance on two benchmarks, including a problem in soft tissue mechanics of predicting the closed state of a bioprosthetic aortic valve. We find a regularizing effect on adding additional contact terms to the loss function, leading to better generalization of the network. These benefits hold for simple contact at similar planes and element normal angles, and complex contact at differing planes and element normal angles. We also demonstrate that the framework can handle varying reference geometries. However, such benefits come with high computational costs during training, resulting in a trade-off that may not always be favorable. We quantify the training cost and the resulting inference speedups on various hardware architectures. Importantly, our graph neural network implementation results in up to a thousand-fold speedup for our benchmark problems at inference.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems</title>
<link>https://arxiv.org/abs/2507.14043</link>
<guid>https://arxiv.org/abs/2507.14043</guid>
<content:encoded><![CDATA[
<div> Algorithm, Snake Optimizer, Multi-strategy, Levy flight, UAV path planning

Summary:
The study introduces the Multi-strategy Improved Snake Optimizer (MISO) to enhance the Snake Optimizer algorithm by addressing issues such as slow convergence and local optima traps. MISO incorporates adaptive random disturbance and Levy flight strategies to prevent local optima trapping and improve global optimum exploration. A unique position update strategy combining elite leadership and Brownian motion accelerates convergence speed while maintaining precision. Experimental validation against 30 CEC2017 test functions and the CEC2022 test suite showcases MISO's effectiveness compared to 11 popular algorithms. Application of MISO to UAV 3D path planning and engineering design problems demonstrates superior solution quality and stability, highlighting its potential for practical use.

<br /><br />Summary: <div>
arXiv:2507.14043v1 Announce Type: cross 
Abstract: Metaheuristic algorithms have gained widespread application across various fields owing to their ability to generate diverse solutions. One such algorithm is the Snake Optimizer (SO), a progressive optimization approach. However, SO suffers from the issues of slow convergence speed and susceptibility to local optima. In light of these shortcomings, we propose a novel Multi-strategy Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random disturbance strategy based on sine function to alleviate the risk of getting trapped in a local optimum. Secondly, we introduce adaptive Levy flight strategy based on scale factor and leader and endow the male snake leader with flight capability, which makes it easier for the algorithm to leap out of the local optimum and find the global optimum. More importantly, we put forward a position update strategy combining elite leadership and Brownian motion, effectively accelerating the convergence speed while ensuring precision. Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test functions and the CEC2022 test suite, comparing it with 11 popular algorithms across different dimensions to validate its effectiveness. Moreover, Unmanned Aerial Vehicle (UAV) has been widely used in various fields due to its advantages of low cost, high mobility and easy operation. However, the UAV path planning problem is crucial for flight safety and efficiency, and there are still challenges in establishing and optimizing the path model. Therefore, we apply MISO to the UAV 3D path planning problem as well as 6 engineering design problems to assess its feasibility in practical applications. The experimental results demonstrate that MISO exceeds other competitive algorithms in terms of solution quality and stability, establishing its strong potential for application.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Paradigm Shift to Assembly-like Finite Element Model Updating</title>
<link>https://arxiv.org/abs/2502.02592</link>
<guid>https://arxiv.org/abs/2502.02592</guid>
<content:encoded><![CDATA[
<div> Validation, Finite Element Model, Aircraft, Computational Efficiency, Assembly-Like Approach
<br />
Summary: 
The article introduces a new assembly-like approach for updating finite element models in aeronautics, crucial for developing aircraft with modern flexible wings. This method updates the model as parts are assembled, offering significant computational efficiency by requiring 20% fewer iterations and fewer parameters compared to the traditional one-shot approach. The proposed approach maintains fidelity to the global method while reducing computational burden, making it a promising technique for complex structures. <div>
arXiv:2502.02592v2 Announce Type: replace 
Abstract: In general, there is a mismatch between a finite element model of a structure and its real behaviour. In aeronautics, this mismatch must be small because finite element models are a fundamental part of the development of an aircraft and of increasing importance with the trend to more flexible wings in modern designs. Finite element model updating can be computationally expensive for complex structures and surrogate models can be employed to reduce the computational burden. A novel approach for finite element model updating, namely assembly-like, is proposed and validated using real experimental data. The assembly-like model updating framework implies that the model is updated as parts are assembled. Benchmarking against the classical global, or one-shot, approach demonstrates that the proposed method is more computationally efficient since it takes 20% fewer iterations to obtain convergence, also using fewer parameters for the model evaluations. Despite the increase in computational performance, the new approach retains the fidelity of the global approach.
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vector-level Feedforward Control of LPBF Melt Pool Area Using a Physics-Based Thermal Model</title>
<link>https://arxiv.org/abs/2507.12557</link>
<guid>https://arxiv.org/abs/2507.12557</guid>
<content:encoded><![CDATA[
<div> Keywords: Laser powder bed fusion, additive manufacturing, feedforward control, melt pool area, part quality <br />
Summary: 
A new feedforward control framework for regulating melt pool area in Laser Powder Bed Fusion (LPBF) additive manufacturing is proposed. The framework combines a thermal model and a melt pool model to efficiently predict and optimize melt pool area, reducing geometric inaccuracies and porosity in metal parts. Calibration of the models using minimal experiments allows for accurate control of laser power scheduling. The framework was successfully validated on complex 3D geometries made of Inconel 718 and 316L stainless steel, showing significant improvements in part quality metrics. By proactively compensating for thermal effects, the approach demonstrates enhanced part quality while remaining computationally efficient and adaptable to different materials and machines. Overall, the vector-level feedforward control framework presents a promising method for improving the quality of LPBF-produced parts. <br /><br /> <div>
arXiv:2507.12557v1 Announce Type: new 
Abstract: Laser powder bed fusion (LPBF) is an additive manufacturing technique that has gained popularity thanks to its ability to produce geometrically complex, fully dense metal parts. However, these parts are prone to internal defects and geometric inaccuracies, stemming in part from variations in the melt pool. This paper proposes a novel vector-level feedforward control framework for regulating melt pool area in LPBF. By decoupling part-scale thermal behavior from small-scale melt pool physics, the controller provides a scale-agnostic prediction of melt pool area and efficient optimization over it. This is done by operating on two coupled lightweight models: a finite-difference thermal model that efficiently captures vector-level temperature fields and a reduced-order, analytical melt pool model. Each model is calibrated separately with minimal single-track and 2D experiments, and the framework is validated on a complex 3D geometry in both Inconel 718 and 316L stainless steel. Results showed that feedforward vector-level laser power scheduling reduced geometric inaccuracy in key dimensions by 62%, overall porosity by 16.5%, and photodiode variation by 6.8% on average. Overall, this modular, data-efficient approach demonstrates that proactively compensating for known thermal effects can significantly improve part quality while remaining computationally efficient and readily extensible to other materials and machines.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDS-Net: A novel framework for few-shot photovoltaic power prediction with interpretable dynamic selection and feature information fusion</title>
<link>https://arxiv.org/abs/2507.12745</link>
<guid>https://arxiv.org/abs/2507.12745</guid>
<content:encoded><![CDATA[
<div> transfer learning, PV power stations, feature selection, interpretable dynamic selection network, prediction

Summary:
The article introduces a novel interpretable dynamic selection network (IDS-Net) for accurate few-shot prediction in PV power stations. The framework includes pre-training on a large dataset, feature selection using the ReliefF algorithm, and outlier correction with the Hampel Identifier. The IDS-Net model incorporates interpretable weights and adaptive selection outcomes for accurate predictions. An end-to-end adaptive transfer learning strategy is designed for final prediction results on the target dataset. The framework's effectiveness and generalization are demonstrated using two PV power datasets from Hebei province, China. <div>
arXiv:2507.12745v1 Announce Type: new 
Abstract: With the growing demand for renewable energy, countries are accelerating the construction of photovoltaic (PV) power stations. However, accurately forecasting power data for newly constructed PV stations is extremely challenging due to limited data availability. To this end, we propose a novel interpretable dynamic selection network (IDS-Net) based on feature information fusion to achieve accurate few-shot prediction. This transfer learning framework primarily consists of two parts. In the first stage, we pre-train on the large dataset, utilizing Maximum Mean Discrepancy (MMD) to select the source domain dataset most similar to the target domain data distribution. Subsequently, the ReliefF algorithm is utilized for feature selection, reducing the influence of feature redundancy. Then, the Hampel Identifier (HI) is used for training dataset outlier correction. In the IDS-Net model, we first obtain the initial extracted features from a pool of predictive models. Following this, two separate weighting channels are utilized to determine the interpretable weights for each sub-model and the adaptive selection outcomes, respectively. Subsequently, the extracted feature results from each sub-model are multiplied by their corresponding weights and then summed to obtain the weighted extracted features. Then, we perform cross-embedding on the additional features and fuse them with the extracted weighted features. This fused information is then passed through the MLP (Multi-Layer Perceptron) layer to obtain predictions. In the second stage, we design an end-to-end adaptive transfer learning strategy to obtain the final prediction results on the target dataset. We validate the transfer learning process using two PV power datasets from Hebei province, China, to demonstrate the effectiveness and generalization of our framework and transfer learning strategy.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-Enhanced Reinforcement Learning with LSTM Forecasting Signals for Optimizing Fintech Trading Decisions</title>
<link>https://arxiv.org/abs/2507.12835</link>
<guid>https://arxiv.org/abs/2507.12835</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, quantum circuits, financial systems, LSTM predictions, performance<br />
<br />
Summary: <br />
Financial trading environments are complex and dynamic, posing challenges for traditional reinforcement learning methods. This study introduces a novel approach by integrating quantum circuits into a reinforcement learning framework customized for financial systems. The comparison between classical A3C and quantum A3C algorithms, alongside incorporating LSTM-based predictions of economic trends, reveals that quantum models with predictive signals outperform traditional methods. The experiments conducted in a Gymnasium-compatible trading environment demonstrate the superior performance and stability of quantum models, even with shallow quantum circuit depth, in noisy financial conditions. This research highlights the potential of quantum reinforcement learning in tackling the complexities of financial markets and improving trading strategies. <div>
arXiv:2507.12835v1 Announce Type: new 
Abstract: Financial trading environments are characterized by high volatility, numerous macroeconomic signals, and dynamically shifting market regimes, where traditional reinforcement learning methods often fail to deliver breakthrough performance. In this study, we design a reinforcement learning framework tailored for financial systems by integrating quantum circuits. We compare (1) the performance of classical A3C versus quantum A3C algorithms, and (2) the impact of incorporating LSTM-based predictions of the following week's economic trends on learning outcomes. The experimental framework adopts a custom Gymnasium-compatible trading environment, simulating discrete trading actions and evaluating rewards based on portfolio feedback. Experimental results show that quantum models - especially when combined with predictive signals - demonstrate superior performance and stability under noisy financial conditions, even with shallow quantum circuit depth.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentar-DeepFinance-300K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization</title>
<link>https://arxiv.org/abs/2507.12901</link>
<guid>https://arxiv.org/abs/2507.12901</guid>
<content:encoded><![CDATA[
<div> Dataset, financial reasoning, language models, CoT synthesis, knowledge space

Summary:
Agentar-DeepFinance-300K is a new large-scale financial reasoning dataset created with a focus on optimizing chain-of-thought (CoT) synthesis for robust financial reasoning. The dataset is generated using a Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) pipeline to ensure comprehensive and deep financial reasoning trajectories. The researchers also conducted a systematic investigation called CoT Cube to analyze critical factors affecting the effectiveness of CoT, such as necessity, length, and synthesizer. Models trained on Agentar-DeepFinance-300K show significant improvements on financial benchmarks, highlighting the importance of well-designed CoT construction in financial reasoning models. The dataset is publicly available to advance research in financial reasoning models.
<br /><br />Summary: <div>
arXiv:2507.12901v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have demonstrated remarkable general reasoning capabilities, holding significant potential for applications in the financial domain, a field that requires robust and reliable reasoning. It has been demonstrated that distilling high-quality chain-of-thought (CoT) rationales from advanced general reasoning models offers a promising and efficient path to the financial reasoning model. However, existing CoT synthesis methods suffer from shallow CoT sampling, leaving the question of how to construct a well-designed knowledge space for finance reasoning unexplored. In this paper, we present \textbf{Agentar-DeepFinance-300K }, a large-scale financial reasoning dataset characterized by its systematic CoT synthesis optimization. We first introduce a comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and deep financial reasoning trajectories. Furthermore, a systematic investigation, termed CoT Cube, is conducted to analyze critical factors that influence CoT effectiveness, such as necessity, length and synthesizer, yielding valuable insights for high-quality financial CoT construction. Experiments demonstrate that models trained on our Agentar-DeepFinance-300K achieve significant improvements on financial benchmarks. We publicly release Agentar-DeepFinance-300K , hoping to advance the research in financial reasoning models.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To What Extent Can Public Equity Indices Statistically Hedge Real Purchasing Power Loss in Compounded Structural Emerging-Market Crises? An Explainable ML-Based Assessment</title>
<link>https://arxiv.org/abs/2507.13055</link>
<guid>https://arxiv.org/abs/2507.13055</guid>
<content:encoded><![CDATA[
<div> Keywords: local public equity indices, real purchasing power loss, macro-financial collapses, emerging markets, tail dependence copula analysis

Summary: 
This study examines the effectiveness of local public equity indices in hedging real purchasing power loss during macro-financial collapses in emerging markets. Using non-linear real return calculations and advanced statistical analysis techniques, the research focuses on collapse episodes in Turkey (2018), Nigeria (2020), and Pakistan (2021). The findings highlight the limitations of using equity-based protection during simultaneous macroeconomic and monetary dislocations. The study challenges traditional notions of equity pricing theory for inflation and devaluation hedge effectiveness and emphasizes the need for context-sensitive strategies in times of compounded macro-financial distress. The analysis underscores the importance of considering tail risk and the potential breakdown of equity-based protection mechanisms during crises. Contextual factors and crisis triggers play a significant role in shaping the effectiveness of equity indices in preserving purchasing power during economic turmoil. <div>
arXiv:2507.13055v1 Announce Type: new 
Abstract: This study investigates the extent to which local public equity indices can statistically hedge real purchasing power loss during compounded structural macro-financial collapses in emerging markets. We employ a non-linear multiplicative real return calculations consistent with Fisher-parity logics for both domestic and foreign investors with a principled quantile regression, tail dependence copula analysis, and Shapley Additive Explanations (SHAP) to assess the explanatory power of macro variables. The analysis focuses on three recent and data-accessible exemplary collapse episodes: Turkey (2018), Nigeria (2020), and Pakistan (2021). Such cases, selected to align with post-2018 improvements in data standardization and crisis comparability, span varied monetary regimes and crisis triggers. Our tail-focused modeling reveals a systematic breakdown in public-equity-based purchasing power protection precisely during simultaneous macroeconomic and monetary dislocations when such protection is most needed. The findings call into question conventional inflation and devaluation hedge presumptions in equity pricing theory, emphasizing the limitations of equity-based protection and the need for context-sensitive strategies during compounded macro-financial distress.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding</title>
<link>https://arxiv.org/abs/2507.12482</link>
<guid>https://arxiv.org/abs/2507.12482</guid>
<content:encoded><![CDATA[
<div> retrieval, code generation, autonomous code understanding, debugging, software maintenance <br />
Summary: 
The article introduces Kodezi Chronos, a new architecture for autonomous code understanding that can operate across ultra-long contexts without fixed window limits. It leverages a multi-level embedding memory engine to efficiently reason over millions of lines of code, supporting tasks like repository-scale comprehension and real-time self-healing actions. A novel benchmark, the Multi Random Retrieval, evaluates the models ability to resolve distant associations across code artifacts, outperforming prior models by 23% in bug detection. Chronos reduces debugging cycles by up to 40% compared to traditional approaches. By integrating with IDEs and CI/CD workflows, Chronos enhances code reliability and productivity, reducing manual effort and advancing towards self-sustaining software ecosystems. <br /><br />Summary: <div>
arXiv:2507.12482v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have advanced code generation and software automation, but are fundamentally constrained by limited inference-time context and lack of explicit code structure reasoning. We introduce Kodezi Chronos, a next-generation architecture for autonomous code understanding, debugging, and maintenance, designed to operate across ultra-long contexts comprising entire codebases, histories, and documentation, all without fixed window limits. Kodezi Chronos leverages a multi-level embedding memory engine, combining vector and graph-based indexing with continuous code-aware retrieval. This enables efficient and accurate reasoning over millions of lines of code, supporting repository-scale comprehension, multi-file refactoring, and real-time self-healing actions. Our evaluation introduces a novel Multi Random Retrieval benchmark, specifically tailored to the software engineering domain. Unlike classical retrieval benchmarks, this method requires the model to resolve arbitrarily distant and obfuscated associations across code artifacts, simulating realistic tasks such as variable tracing, dependency migration, and semantic bug localization. Chronos outperforms prior LLMs and code models, demonstrating a 23% improvement in real-world bug detection and reducing debugging cycles by up to 40% compared to traditional sequence-based approaches. By natively interfacing with IDEs and CI/CD workflows, Chronos enables seamless, autonomous software maintenance, elevating code reliability and productivity while reducing manual effort. These results mark a critical advance toward self-sustaining, continuously optimized software ecosystems.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RONOM: Reduced-Order Neural Operator Modeling</title>
<link>https://arxiv.org/abs/2507.12814</link>
<guid>https://arxiv.org/abs/2507.12814</guid>
<content:encoded><![CDATA[
<div> Reduced-order modeling, neural operators, time-dependent partial differential equations, discretization error bound, spatial super-resolution.<br />
Summary:<br />
The article introduces Reduced-Order Neural Operator Modeling (RONOM), combining concepts from reduced-order modeling (ROM) and operator learning. It addresses the computational intensity of time-dependent partial differential equations in many-query scenarios. RONOM bridges the gap between ROM and neural operator approaches, providing insights into discretization convergence and robustness. The framework offers a discretization error bound analogous to ROM for rigorous numerical error estimation. Comparing RONOM to existing neural operators in solving PDEs, results show RONOM's standard vector-to-vector neural networks achieve comparable input generalization and superior performance in spatial super-resolution and discretization robustness. Additionally, RONOM offers novel insights into temporal super-resolution scenarios. <div>
arXiv:2507.12814v1 Announce Type: cross 
Abstract: Time-dependent partial differential equations are ubiquitous in physics-based modeling, but they remain computationally intensive in many-query scenarios, such as real-time forecasting, optimal control, and uncertainty quantification. Reduced-order modeling (ROM) addresses these challenges by constructing a low-dimensional surrogate model but relies on a fixed discretization, which limits flexibility across varying meshes during evaluation. Operator learning approaches, such as neural operators, offer an alternative by parameterizing mappings between infinite-dimensional function spaces, enabling adaptation to data across different resolutions. Whereas ROM provides rigorous numerical error estimates, neural operator learning largely focuses on discretization convergence and invariance without quantifying the error between the infinite-dimensional and the discretized operators. This work introduces the reduced-order neural operator modeling (RONOM) framework, which bridges concepts from ROM and operator learning. We establish a discretization error bound analogous to those in ROM, and get insights into RONOM's discretization convergence and discretization robustness. Moreover, two numerical examples are presented that compare RONOM to existing neural operators for solving partial differential equations. The results demonstrate that RONOM using standard vector-to-vector neural networks achieves comparable performance in input generalization and superior performance in both spatial super-resolution and discretization robustness, while also offering novel insights into temporal super-resolution scenarios.
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying data needs in surrogate modeling for flow fields in 2D stirred tanks with physics-informed neural networks (PINNs)</title>
<link>https://arxiv.org/abs/2507.11640</link>
<guid>https://arxiv.org/abs/2507.11640</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Surrogate Models, Stirred Tanks, Computational Fluid Dynamics, Data Requirements<br />
<br />Summary:
Physics-informed neural networks (PINNs) are proposed as a solution to develop efficient surrogate models for flow fields in stirred tanks. This study investigates the data requirements for developing these models and compares them with classical neural networks and boundary-informed neural networks (BINNs). The results show that PINNs can achieve accurate predictions with as few as six data points, demonstrating their effectiveness in reducing data requirements. Surrogate models can achieve prediction errors of around 3% across a range of Reynolds numbers, with an approximation of velocity profile leading to errors of 2.5%. This study highlights the potential of PINNs in efficiently modeling flow fields in stirred tanks, even with limited or approximate datasets. <div>
arXiv:2507.11640v1 Announce Type: new 
Abstract: Stirred tanks are vital in chemical and biotechnological processes, particularly as bioreactors. Although computational fluid dynamics (CFD) is widely used to model the flow in stirred tanks, its high computational cost$-$especially in multi-query scenarios for process design and optimization$-$drives the need for efficient data-driven surrogate models. However, acquiring sufficiently large datasets can be costly. Physics-informed neural networks (PINNs) offer a promising solution to reduce data requirements while maintaining accuracy by embedding underlying physics into neural network (NN) training. This study quantifies the data requirements of vanilla PINNs for developing surrogate models of a flow field in a 2D stirred tank. We compare these requirements with classical supervised neural networks and boundary-informed neural networks (BINNs). Our findings demonstrate that surrogate models can achieve prediction errors around 3% across Reynolds numbers from 50 to 5000 using as few as six datapoints. Moreover, employing an approximation of the velocity profile in place of real data labels leads to prediction errors of around 2.5%. These results indicate that even with limited or approximate datasets, PINNs can be effectively trained to deliver high accuracy comparable to high-fidelity data.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MNO : A Multi-modal Neural Operator for Parametric Nonlinear BVPs</title>
<link>https://arxiv.org/abs/2507.11870</link>
<guid>https://arxiv.org/abs/2507.11870</guid>
<content:encoded><![CDATA[
<div> Multi-parameter, Nonlinear, Boundary value problems, Multimodal Neural Operator, Generalized FMM<br />
Summary:<br />
The article introduces a novel Multimodal Neural Operator (MNO) architecture for learning solution operators for multi-parameter nonlinear boundary value problems (BVPs). Unlike traditional neural operators that map PDE coefficients or source terms independently, the MNO architecture can map multiple parameters, including PDE coefficients, source terms, and boundary conditions, to the solution space in a unified manner. Inspired by the Fast Multipole Method (FMM), the MNO consists of three key components: a Generalized FMM (GFMM) block, a Unimodal Neural Operator (UNO) for single parameter mappings, and a multimodal fusion mechanism. Experiment results demonstrate the MNO's capability to handle variations in PDE coefficients and source or boundary terms simultaneously. <div>
arXiv:2507.11870v1 Announce Type: new 
Abstract: We introduce a novel Multimodal Neural Operator (MNO) architecture designed to learn solution operators for multi-parameter nonlinear boundary value problems (BVPs). Traditional neural operators primarily map either the PDE coefficients or source terms independently to the solution, limiting their flexibility and applicability. In contrast, our proposed MNO architecture generalizes these approaches by mapping multiple parameters including PDE coefficients, source terms, and boundary conditions to the solution space in a unified manner. Our MNO is motivated by the hierarchical nested bases of the Fast Multipole Method (FMM) and is constructed systematically through three key components: a parameter efficient Generalized FMM (GFMM) block, a Unimodal Neural Operator (UNO) built upon GFMM blocks for single parameter mappings, and most importantly, a multimodal fusion mechanism extending these components to learn the joint map. We demonstrate the multimodal generalization capacity of our approach on both linear and nonlinear BVPs. Our experiments show that the network effectively handles simultaneous variations in PDE coefficients and source or boundary terms.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Fourier Neural Operators for Micromechanics</title>
<link>https://arxiv.org/abs/2507.12233</link>
<guid>https://arxiv.org/abs/2507.12233</guid>
<content:encoded><![CDATA[
<div> Fourier Neural Operators, Micromechanics, Homogenization, Deep Learning, Fast Fourier Transform <br />
Summary: <br />
The article discusses the use of Fourier Neural Operators (FNOs) in solving cell problems in micromechanics. Traditional computational frameworks outperform deep-learning frameworks in this area, and the potential of machine-learning approaches for micromechanics is unclear. The study shows that FNOs, empowered by insights from fast Fourier transform (FFT) methods, can accurately predict solutions to cell problems with arbitrary stiffness distribution, subject to a material-contrast constraint. This approach does not require restrictions on material properties, number of phases, or geometry of interfaces between materials. The fidelity provided by FNOs is sharp and uniform, with explicit guarantees of accuracy. Furthermore, an FNO explicitly constructed without training demonstrates the universal approximation property, with memory requirements and runtimes comparable to classical FFT solvers. This work aims to bridge the gap between FFT-based methods and FNOs, potentially facilitating collaboration between the two approaches. <br /> <div>
arXiv:2507.12233v1 Announce Type: new 
Abstract: \noindent Solving cell problems in homogenization is hard, and available deep-learning frameworks fail to match the speed and generality of traditional computational frameworks. More to the point, it is generally unclear what to expect of machine-learning approaches, let alone single out which approaches are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs) for micromechanics, empowering them by insights from computational micromechanics methods based on the fast Fourier transform (FFT). We construct an FNO surrogate mimicking the basic scheme foundational for FFT-based methods and show that the resulting operator predicts solutions to cell problems with \emph{arbitrary} stiffness distribution only subject to a material-contrast constraint up to a desired accuracy. In particular, there are no restrictions on the material symmetry like isotropy, on the number of phases and on the geometry of the interfaces between materials. Also, the provided fidelity is sharp and uniform, providing explicit guarantees leveraging our physical empowerment of FNOs. To show the desired universal approximation property, we construct an FNO explicitly that requires no training to begin with. Still, the obtained neural operator complies with the same memory requirements as the basic scheme and comes with runtimes proportional to classical FFT solvers. In particular, large-scale problems with more than 100 million voxels are readily handled. The goal of this work is to underline the potential of FNOs for solving micromechanical problems, linking FFT-based methods to FNOs. This connection is expected to provide a fruitful exchange between both worlds.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Identification of Nonlinear Dynamics with Conformal Prediction</title>
<link>https://arxiv.org/abs/2507.11739</link>
<guid>https://arxiv.org/abs/2507.11739</guid>
<content:encoded><![CDATA[
<div> Sparse Identification of Nonlinear Dynamics, SINDy, uncertainty quantification, Conformal Prediction, Ensemble-SINDy<br />
<br />
Summary: 
The article investigates the integration of Conformal Prediction with Ensemble-SINDy (E-SINDy) for uncertainty quantification in nonlinear dynamical system models. The study focuses on three key applications: quantifying uncertainty in time series prediction, model selection based on library feature importance, and assessing the uncertainty of identified model coefficients using feature conformal prediction. Results show that the integration of Conformal Prediction with E-SINDy can reliably achieve target coverage for time series forecasting, effectively quantify feature importance in model selection, and produce robust uncertainty intervals for model coefficients, even under non-Gaussian noise conditions. The study demonstrates the versatility and reliability of the approach in various scenarios, including stochastic predator-prey dynamics and chaotic dynamical systems. <div>
arXiv:2507.11739v1 Announce Type: cross 
Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for discovering nonlinear dynamical system models from data. Quantifying uncertainty in SINDy models is essential for assessing their reliability, particularly in safety-critical applications. While various uncertainty quantification methods exist for SINDy, including Bayesian and ensemble approaches, this work explores the integration of Conformal Prediction, a framework that can provide valid prediction intervals with coverage guarantees based on minimal assumptions like data exchangeability. We introduce three applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1) quantifying uncertainty in time series prediction, (2) model selection based on library feature importance, and (3) quantifying the uncertainty of identified model coefficients using feature conformal prediction. We demonstrate the three applications on stochastic predator-prey dynamics and several chaotic dynamical systems. We show that conformal prediction methods integrated with E-SINDy can reliably achieve desired target coverage for time series forecasting, effectively quantify feature importance, and produce more robust uncertainty intervals for model coefficients, even under non-Gaussian noise, compared to standard E-SINDy coefficient estimates.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Purity: Defense Paradigm For Chain-of-Thought Attack</title>
<link>https://arxiv.org/abs/2507.12314</link>
<guid>https://arxiv.org/abs/2507.12314</guid>
<content:encoded><![CDATA[
<div> Large Reasoning Models, Chain-of-Thought Attack, security threats, reinforcement learning, Thought Purity<br />
Summary:<br />
The article discusses the vulnerability of reinforcement learning-trained Large Reasoning Models to Chain-of-Thought Attack (CoTA) due to backdoor prompt attacks. CoTA exploits prompt controllability, compromising both safety and task performance. To address this, the authors propose a defense mechanism called Thought Purity (TP) with three key components: a safety-optimized data processing pipeline, reinforcement learning-enhanced rule constraints, and adaptive monitoring metrics. TP aims to strengthen resistance to malicious content while maintaining operational efficacy. This approach offers the first comprehensive defense against CoTA vulnerabilities in reasoning systems aligned with reinforcement learning, enhancing the security-functionality balance in future AI architectures. <br /><br /> <div>
arXiv:2507.12314v1 Announce Type: cross 
Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g., Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large Language Models (LLMs) domain, their susceptibility to security threats remains a critical vulnerability. This weakness is particularly evident in Chain-of-Thought (CoT) generation processes, where adversarial methods like backdoor prompt attacks can systematically subvert the model's core reasoning mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this vulnerability through exploiting prompt controllability, simultaneously degrading both CoT safety and task performance with low-cost interventions. To address this compounded security-performance vulnerability, we propose Thought Purity (TP): a defense paradigm that systematically strengthens resistance to malicious content while preserving operational efficacy. Our solution achieves this through three synergistic components: (1) a safety-optimized data processing pipeline (2) reinforcement learning-enhanced rule constraints (3) adaptive monitoring metrics. Our approach establishes the first comprehensive defense mechanism against CoTA vulnerabilities in reinforcement learning-aligned reasoning systems, significantly advancing the security-functionality equilibrium for next-generation AI architectures.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data</title>
<link>https://arxiv.org/abs/2507.12425</link>
<guid>https://arxiv.org/abs/2507.12425</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Enterprise Data, Dense Embeddings, Metadata-aware Filtering

Summary: 
The study introduces an advanced Retrieval-Augmented Generation (RAG) framework for enterprise data that combines hybrid retrieval strategies using dense embeddings and BM25, along with metadata-aware filtering and cross-encoder reranking. By applying semantic chunking and retaining tabular data structures, the framework ensures textual coherence and maintains the integrity of data. Experiments on enterprise datasets demonstrate significant improvements in Precision@5, Recall@5, and Mean Reciprocal Rank. Qualitative evaluations also show higher scores in Faithfulness, Completeness, and Relevance. The framework delivers accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work involves extending the framework to handle multimodal data and integrating agent-based retrieval. The source code will be made available on GitHub at the provided link.<br /><br />Summary: <div>
arXiv:2507.12425v1 Announce Type: cross 
Abstract: Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Algorithms and Implementations for Computing the Minimum Distance of Quantum Codes</title>
<link>https://arxiv.org/abs/2408.10743</link>
<guid>https://arxiv.org/abs/2408.10743</guid>
<content:encoded><![CDATA[
<div> stabilizer quantum code, symplectic distance, fast algorithms, computational time, shared-memory parallel architectures  
Summary: The article introduces three new fast algorithms and implementations for computing the symplectic distance of stabilizer quantum codes. The distance of a stabilizer quantum code is crucial for error detection and correction. The new algorithms, based on the Brouwer-Zimmermann algorithm, outperform current state-of-the-art implementations on various processors, showing significant improvements in computational time, especially in highly demanding cases. The study demonstrates superior performance on single-core processors, multicore processors, and shared-memory multiprocessors, with scalability observed on shared-memory parallel architectures. These advancements in computing the symplectic distance offer a substantial leap forward in the efficiency and speed of error detection and correction in stabilizer quantum codes. <br /><br />Summary: <div>
arXiv:2408.10743v2 Announce Type: replace-cross 
Abstract: The distance of a stabilizer quantum code is a very important feature since it determines the number of errors that can be detected and corrected. We present three new fast algorithms and implementations for computing the symplectic distance of the associated classical code. Our new algorithms are based on the Brouwer-Zimmermann algorithm. Our experimental study shows that these new implementations are much faster than current state-of-the-art licensed implementations on single-core processors, multicore processors, and shared-memory multiprocessors. In the most computationally-demanding cases, the performance gain in the computational time can be larger than one order of magnitude. The experimental study also shows a good scalability on shared-memory parallel architectures.
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Three-dimensional SPH modeling of brittle fracture under hydrodynamic loading</title>
<link>https://arxiv.org/abs/2507.10553</link>
<guid>https://arxiv.org/abs/2507.10553</guid>
<content:encoded><![CDATA[
<div> Modeling fluid-structure interactions, three-dimensional SPH computational framework, weakly compressible SPH, pseudo-spring-based SPH solver, structural deformation, structural failure<br />
<br />
Summary:<br />
A three-dimensional computational framework using SPH for fluid-structure interactions is introduced. The model integrates weakly compressible SPH with a pseudo-spring-based solver to simulate fluid flow and deformable structures while capturing solid boundaries and fluid-structure interfaces without contact forces. Pressure calculations in the fluid phase are enhanced by the $\delta$-SPH technique, and structural damage is modeled using a pseudo-spring approach with limited particle interactions. The framework accurately simulates detailed fracture patterns without complex crack-tracking algorithms. It has shown effectiveness compared to existing models and experimental data, providing insights into the effects of hydrodynamic events on structural integrity.<br /><br /> <div>
arXiv:2507.10553v1 Announce Type: new 
Abstract: A three-dimensional SPH computational framework is presented for modeling fluid-structure interactions with structural deformation and failure. We combine weakly compressible SPH with a pseudo-spring-based SPH solver to capture the fluid flow and deformable structures. A unified modeling approach captures the solid boundaries and fluid-structure interfaces without penalty-based contact force. The $\delta$-SPH technique improves the pressure calculations in the fluid phase, while structural damage is modeled using a pseudo-spring approach, with particle interactions limited to its neighbors. The present framework can capture the three-dimensional crack surfaces in structures without any computationally intensive crack-tracking algorithm or visibility criteria. The framework has been proven effective against existing models and experimental data, demonstrating high accuracy and robustness in simulating detailed fracture patterns and offering insights into the impact of hydrodynamic events on structural integrity.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Multiple Time-Stepping Method for 3-Body Interactions in High Performance Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2507.11172</link>
<guid>https://arxiv.org/abs/2507.11172</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular dynamics, two-body interactions, three-body interactions, r-RESPA algorithm, High Performance Computing

Summary:
This study focuses on improving the efficiency of molecular dynamics (MD) simulations by incorporating two-body and three-body interactions. Traditional two-body potentials may not fully capture the complexity of molecular systems, necessitating the inclusion of three-body interactions. However, three-body interactions are computationally expensive due to their cubic complexity class. The r-RESPA algorithm is utilized to reduce the number of three-body interaction calculations, enhancing efficiency. The study explores this method in the context of High Performance Computing (HPC) methods for parallelizing calculations. It introduces a communication-reducing distributed-memory parallel method and a novel shared-memory parallel cutoff method implemented in the particle simulation library AutoPas. The results and methods discussed offer insights into potential advancements in MD simulation efficiency. 

<br /><br />Summary: <div>
arXiv:2507.11172v1 Announce Type: new 
Abstract: Understanding the complex behavior of molecular systems is fundamental to fields such as physics, materials science, and biology. Molecular dynamics (MD) simulations are crucial tools for studying atomic-level dynamics. This work focuses on improving the efficiency of MD simulations involving two-body and three-body interactions. Traditional two-body potentials often can not fully capture the complexity of molecular systems, making the inclusion of three-body interactions important. However, these interactions are in a cubic complexity class, compared to a quadratic one for two-body interactions, and therefore are computationally expensive, even when a cutoff distance is applied. One way to improve efficiency is to use the r-RESPA multiple time-stepping algorithm to reduce the number of three-body interaction calculations. In this work, we investigate this method in the context of High Performance Computing (HPC) methods that parallelize the calculations. In particular, we investigate a communication-reducing distributed-memory parallel method from literature and present a novel shared-memory parallel cutoff method, implemented in the particle simulation library AutoPas. The results and methods are discussed, providing insights into potential advancements in MD simulation efficiency.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models</title>
<link>https://arxiv.org/abs/2507.11191</link>
<guid>https://arxiv.org/abs/2507.11191</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, data-driven optimization, manufacturing systems, machine learning, metaheuristic<br />
<br />Summary: 
This study introduces a data-driven methodology for optimizing complex manufacturing systems using historical process data. By utilizing machine learning models to create surrogate models, the approach, Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models, is tailored to the specifics of the industry. The method is applied to an extrusion process in tire manufacturing to optimize initialization parameters and reduce waste and production time. Results show a 65% reduction in setup time and a decrease in material waste compared to historical configurations, demonstrating the superiority of the surrogate-based optimization approach. This research underscores the advantages of combining data-driven modeling with metaheuristic optimization for industrial processes lacking explicit formulations. <br /><br /> <div>
arXiv:2507.11191v1 Announce Type: new 
Abstract: The optimization of industrial processes remains a critical challenge, particularly when no mathematical formulation of objective functions or constraints is available. This study addresses this issue by proposing a surrogate-based, data-driven methodology for optimizing complex real-world manufacturing systems using only historical process data. Machine learning models are employed to approximate system behavior and construct surrogate models, which are integrated into a tailored metaheuristic approach: Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models, an adapted version of Differential Evolution suited to the characteristics of the studied process. The methodology is applied to an extrusion process in the tire manufacturing industry, with the goal of optimizing initialization parameters to reduce waste and production time. Results show that the surrogate-based optimization approach outperforms historical best configurations, achieving a 65\% reduction in initialization and setup time, while also significantly minimizing material waste. These findings highlight the potential of combining data-driven modeling and metaheuristic optimization for industrial processes where explicit formulations are unavailable.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tax-Efficient Model Predictive Control Policy for Retirement Funding</title>
<link>https://arxiv.org/abs/2507.10603</link>
<guid>https://arxiv.org/abs/2507.10603</guid>
<content:encoded><![CDATA[
<div> taxes, retirement funding, inflation, investment returns, bequest
<br />
Summary: 
The article presents a retirement funding policy that addresses the challenge of managing a retiree's savings to ensure constant post-tax inflation-adjusted consumption throughout their lifetime. The policy involves two main steps. Firstly, a simplified planning problem is formulated as a convex optimization problem to maximize the bequest while maintaining a constant inflation-adjusted consumption target. This allows for quick and reliable solution of the planning problem. Secondly, a model predictive control (MPC) retirement policy is developed based on the annual update-plan-act cycle. The MPC policy takes into account uncertain factors such as investment returns, inflation, changes in life expectancy, external income, liabilities, and tax rules and rates. The effectiveness of the MPC retirement policy is demonstrated through Monte Carlo simulation, showcasing its ability to adapt to changing circumstances and provide a reliable approach to retirement funding. <div>
arXiv:2507.10603v1 Announce Type: cross 
Abstract: The retirement funding problem addresses the question of how to manage a retiree's savings to provide her with a constant post-tax inflation adjusted consumption throughout her lifetime. This consists of choosing withdrawals and transfers from and between several accounts with different tax treatments, taking into account basic rules such as required minimum distributions and limits on Roth conversions, additional income, liabilities, taxes, and the bequest when the retiree dies. We develop a retirement funding policy in two steps. In the first step, we consider a simplified planning problem in which various future quantities, such as the retiree's remaining lifetime, future investment returns, and future inflation, are known. Using a simplified model of taxes, we pose this planning problem as a convex optimization problem, where we maximize the bequest subject to providing a constant inflation adjusted consumption target. Since this problem is convex, it can be solved quickly and reliably. We leverage this planning method to form a retirement funding policy that determines the actions to take each year, based on information known at that time. Each year the retiree forms a new plan for the future years, using the current account values and life expectancy, and optionally, updated information such as changes in tax rates or rules. The retiree then carries out the actions from the first year of the current plan. This update-plan-act cycle is repeated each year, a general policy called model predictive control (MPC). The MPC retirement policy reacts to the effects of uncertain investment returns and inflation, changes in the retiree's expected lifetime or external income and liabilities, and changes in tax rules and rates. We demonstrate the effectiveness of the MPC retirement policy using Monte Carlo simulation.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong</title>
<link>https://arxiv.org/abs/2507.11502</link>
<guid>https://arxiv.org/abs/2507.11502</guid>
<content:encoded><![CDATA[
<div> foundational sovereign large language model, Hong Kong, multilingual, value-aligned, AI infrastructure<br />
Summary: This paper introduces HKGAI-V1, a large language model tailored for Hong Kong's unique multilingual and socio-legal environment. Developed using the DeepSeek architecture, the model is aligned with regional norms through full parameter fine-tuning. Integrated with a retrieval-augmented generation system, it provides timely and factual information access. The paper showcases two key achievements: the successful development of HKGAI-V1, outperforming general-purpose models in handling culturally sensitive queries, and the creation of the Adversarial HK Value Benchmark for evaluating model alignment with local ethical and legal standards. This work presents a replicable blueprint for developing regionally focused AI systems with a strong emphasis on local identity and values.<br /> <div>
arXiv:2507.11502v1 Announce Type: cross 
Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign large language model (LLM), developed as part of an initiative to establish value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing the region's unique multilingual environment (Cantonese, Mandarin, and English), its distinct socio-legal context under the "one country, two systems" framework, and specific local cultural and value considerations, the model is built upon the DeepSeek architecture and systematically aligned with regional norms through a multifaceted full parameter fine-tuning process. It is further integrated with a retrieval-augmented generation (RAG) system to ensure timely and factually grounded information access. The core contribution lies in the design and implementation of a comprehensive, region-specific AI alignment and safety framework, demonstrated through two key achievements: 1) The successful development of HKGAI-V1 itself - which outper-forms general-purpose models in handling Hong Kong-specific culturally sensitive queries, and embodies a "governance-embedded" approach to digital sovereignty - empowers Hong Kong to exercise control over AI applications in critical sectors including public services, legal systems, and edu-cation. 2) The development of the proprietary Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment with local ethical and legal stand-ards under challenging conditions. By documenting these achievements, the paper provides not only a technological artifact but also a replicable blueprint for developing advanced, regionally focused AI systems deeply rooted in their local identities.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering</title>
<link>https://arxiv.org/abs/2507.11527</link>
<guid>https://arxiv.org/abs/2507.11527</guid>
<content:encoded><![CDATA[
<div> benchmark, Large Language Model, Civil Engineering, technical drawing revision, automation agents

Summary:
DrafterBench is a comprehensive benchmark designed for evaluating Large Language Model (LLM) agents in the context of technical drawing revision in Civil Engineering. It consists of twelve types of tasks derived from real-world drawing files, encompassing 46 customized functions/tools and a total of 1920 tasks. The benchmark aims to rigorously test AI agents' abilities in interpreting complex instructions, leveraging prior knowledge, and adapting to varying instruction quality. It evaluates capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. Detailed analysis of task accuracy and error statistics is provided to gain deeper insights into agent proficiency and improvement areas for integrating LLMs in engineering applications. The benchmark is open-source and available for access, facilitating the assessment of automation agents from an industrial perspective. 

<br /><br />Summary: <div>
arXiv:2507.11527v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Irrotational Contact Fields</title>
<link>https://arxiv.org/abs/2312.03908</link>
<guid>https://arxiv.org/abs/2312.03908</guid>
<content:encoded><![CDATA[
<div> Framework, convex approximations, complex contact models, Coulomb's law, maximum dissipation

Summary: The article introduces a framework for generating convex approximations of complex contact models, incorporating validated models like Hunt & Crossley and Coulomb's law of friction. The approach is robust across a wide range of stiffness values, suitable for compliant surfaces and rigid approximations. The approximations are evaluated across various test cases, with detailed properties and limitations outlined. The implementation in the open-source robotics toolkit, Drake, provides a fully differentiable solution, allowing for computation of gradients for complex geometric models while reusing contact resolution factorizations. The hybrid approach enables robust simulation of robotic tasks at interactive rates, accurately resolving stiction and contact transitions, thus supporting effective sim-to-real transfer. <div>
arXiv:2312.03908v3 Announce Type: replace-cross 
Abstract: We present a framework for generating convex approximations of complex contact models, incorporating experimentally validated models like Hunt & Crossley coupled with Coulomb's law of friction alongside the principle of maximum dissipation. Our approach is robust across a wide range of stiffness values, making it suitable for both compliant surfaces and rigid approximations. We evaluate these approximations across a wide variety of test cases, detailing properties and limitations. We implement a fully differentiable solution in the open-source robotics toolkit, Drake. Our novel hybrid approach enables computation of gradients for complex geometric models while reusing factorizations from contact resolution. We demonstrate robust simulation of robotic tasks at interactive rates, with accurately resolved stiction and contact transitions, supporting effective sim-to-real transfer.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Parameter Inference and Uncertainty Quantification for a Computational Pulmonary Hemodynamics Model Using Gaussian Processes</title>
<link>https://arxiv.org/abs/2502.14251</link>
<guid>https://arxiv.org/abs/2502.14251</guid>
<content:encoded><![CDATA[
<div> Keywords: subject-specific modeling, cardiovascular research, chronic thromboembolic pulmonary hypertension, microvascular disease, Gaussian process emulators 

Summary: 
Subject-specific modeling in cardiovascular research is crucial for personalized treatment guidance beyond current clinical diagnostics. This study utilized a one-dimensional fluid dynamics model informed by experimental data from a dog model of chronic thromboembolic pulmonary hypertension (CTEPH). The model incorporated measurements from multiple subjects under both baseline and CTEPH conditions, allowing for the assessment of microvascular disease severity. By modeling each lung separately to account for heterogeneity in CTEPH, the study identified distinct parameter shifts reflecting heterogeneous microvascular adaptation. Gaussian process emulators were used to accelerate model calibration, enabling the estimation of microvascular parameters and their uncertainties efficiently. The study demonstrated strong correlations between model parameter changes and disease severity, particularly in the lung with more advanced disease. This framework provides a rapid, uncertainty-aware method for evaluating microvascular dysfunction in CTEPH and may inform targeted treatment strategies with clinical applicability.<br /><br />Summary: <div>
arXiv:2502.14251v2 Announce Type: replace-cross 
Abstract: Subject-specific modeling is a powerful tool in cardiovascular research, providing insights beyond the reach of current clinical diagnostics. Limitations in available clinical data require the incorporation of uncertainty into models to improve guidance for personalized treatments. However, for clinical relevance, such modeling must be computationally efficient. In this study, we used a one-dimensional (1D) fluid dynamics model informed by experimental data from a dog model of chronic thromboembolic pulmonary hypertension (CTEPH), incorporating measurements from multiple subjects under both baseline and CTEPH conditions. Surgical intervention can alleviate CTEPH, yet patients with microvascular disease (e.g., remodeling and narrowing of small vessels) often exhibit persistent pulmonary hypertension, highlighting the importance of assessing microvascular disease severity. Thus, each lung was modeled separately to account for the heterogeneous nature of CTEPH, allowing us to explore lung-specific microvascular narrowing and resistance. We compared inferred parameters between baseline and CTEPH and examined their correlation with clinical markers of disease severity. To accelerate model calibration, we employed Gaussian process (GP) emulators, enabling the estimation of microvascular parameters and their uncertainties within a clinically feasible timeframe. Our results demonstrated that CTEPH leads to heterogeneous microvascular adaptation, reflected in distinct parameter shifts. Notably, the changes in model parameters strongly correlated with disease severity, especially in the lung previously reported to have more advanced disease. This framework provides a rapid, uncertainty-aware method for evaluating microvascular dysfunction in CTEPH and may support more targeted treatment strategies within a timeframe suitable for clinical application.
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets</title>
<link>https://arxiv.org/abs/2507.09255</link>
<guid>https://arxiv.org/abs/2507.09255</guid>
<content:encoded><![CDATA[
<div> simulation platform, large language models, financial decision-making, StockSim, open-source
Summary:
StockSim is an open-source simulation platform designed for evaluating large language models (LLMs) in realistic financial decision-making scenarios. It offers a comprehensive system that accurately models market dynamics and supports various simulation modes with different levels of detail. By incorporating real-world factors like latency and order-book microstructure, StockSim allows for more insightful assessment of LLM-based trading agents. Its extensible agent framework supports diverse trading strategies and multi-agent coordination, making it a valuable tool for NLP research on reasoning under uncertainty and sequential decision-making.
<br /><br />Summary: <div>
arXiv:2507.09255v1 Announce Type: new 
Abstract: We present StockSim, an open-source simulation platform for systematic evaluation of large language models (LLMs) in realistic financial decision-making scenarios. Unlike previous toolkits that offer limited scope, StockSim delivers a comprehensive system that fully models market dynamics and supports diverse simulation modes of varying granularity. It incorporates critical real-world factors, such as latency, slippage, and order-book microstructure, that were previously neglected, enabling more faithful and insightful assessment of LLM-based trading agents. An extensible, role-based agent framework supports heterogeneous trading strategies and multi-agent coordination, making StockSim a uniquely capable testbed for NLP research on reasoning under uncertainty and sequential decision-making. We open-source all our code at https: //github.com/harrypapa2002/StockSim.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoWarp: An automatically differentiable and GPU-accelerated implicit MPM framework for geomechanics based on NVIDIA Warp</title>
<link>https://arxiv.org/abs/2507.09435</link>
<guid>https://arxiv.org/abs/2507.09435</guid>
<content:encoded><![CDATA[
<div> Keywords: material point method, implicit formulation, geomechanics, GPU parallelism, automatic differentiation

Summary:
GeoWarp is introduced as an implicit material point method (MPM) framework for geomechanics. It utilizes GPU parallelism and automatic differentiation to compute Jacobian matrices without manual derivation, overcoming the limitations of explicit MPM formulations. The framework includes a sparse Jacobian construction algorithm that takes advantage of localized particle-grid interactions inherent in MPM. GeoWarp is verified through examples in large-deformation elastoplasticity and coupled poromechanics, showcasing its robustness, scalability, and extensibility for differentiable implicit MPM simulation in computational geomechanics. By leveraging GPU parallelism and automatic differentiation, GeoWarp provides a powerful tool for simulating large-deformation and history-dependent behavior in geomechanical systems, making it a valuable asset in the field of computational geomechanics. 

<br /><br />Summary: <div>
arXiv:2507.09435v1 Announce Type: new 
Abstract: The material point method (MPM), a hybrid Lagrangian-Eulerian particle method, is increasingly used to simulate large-deformation and history-dependent behavior of geomaterials. While explicit time integration dominates current MPM implementations due to its algorithmic simplicity, such schemes are unsuitable for quasi-static and long-term processes typical in geomechanics. Implicit MPM formulations are free of these limitations but remain less adopted, largely due to the difficulty of computing the Jacobian matrix required for Newton-type solvers, especially when consistent tangent operators should be derived for complex constitutive models. In this paper, we introduce GeoWarp -- an implicit MPM framework for geomechanics built on NVIDIA Warp -- that exploits GPU parallelism and reverse-mode automatic differentiation to compute Jacobians without manual derivation. To enhance efficiency, we develop a sparse Jacobian construction algorithm that leverages the localized particle-grid interactions intrinsic to MPM. The framework is verified through forward and inverse examples in large-deformation elastoplasticity and coupled poromechanics. Results demonstrate that GeoWarp provides a robust, scalable, and extensible platform for differentiable implicit MPM simulation in computational geomechanics.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EV-STLLM: Electric vehicle charging forecasting based on spatio-temporal large language models with multi-frequency and multi-scale information fusion</title>
<link>https://arxiv.org/abs/2507.09527</link>
<guid>https://arxiv.org/abs/2507.09527</guid>
<content:encoded><![CDATA[
<div> VMD, ICEEMDAN, FIG, ReliefF, EV-STLLM <br />
Summary: <br />
The paper proposes a novel EV spatio-temporal large language model (EV-STLLM) for accurate prediction of electric vehicle (EV) charging demand and station occupancy. The framework consists of two modules: a data processing module using VMD and ICEEMDAN for data denoising and multi-frequency decomposition, FIG for extracting multi-scale information, and ReliefF for feature selection; a forecasting module using EV-STLLM for direct prediction. The model integrates adjacency matrices from regional station networks and spatio-temporal-frequency embedding information to capture data characteristics. The PFGA module maintains sequential feature modeling capabilities and incorporates EV domain knowledge. Experiments on real-world data from Shenzhen show superior accuracy compared to existing methods. <div>
arXiv:2507.09527v1 Announce Type: new 
Abstract: With the proliferation of electric vehicles (EVs), accurate charging demand and station occupancy forecasting are critical for optimizing urban energy and the profit of EVs aggregator. Existing approaches in this field usually struggle to capture the complex spatio-temporal dependencies in EV charging behaviors, and their limited model parameters hinder their ability to learn complex data distribution representations from large datasets. To this end, we propose a novel EV spatio-temporal large language model (EV-STLLM) for accurate prediction. Our proposed framework is divided into two modules. In the data processing module, we utilize variational mode decomposition (VMD) for data denoising, and improved complete ensemble empirical mode decomposition with adaptive noise (ICEEMDAN) for data multi-frequency decomposition. Fuzzy information granulation (FIG) for extracting multi-scale information. Additionally, ReliefF is used for feature selection to mitigate redundancy. In the forecasting module, the EV-STLLM is used to directly achieve EV charging and occupancy forecasting. Firstly, we fully capture the intrinsic spatio-temporal characteristics of the data by integrating adjacency matrices derived from the regional stations network and spatio-temporal-frequency embedding information. Then, the partially frozen graph attention (PFGA) module is utilized to maintain the sequential feature modeling capabilities of the pre-trained large model while incorporating EV domain knowledge. Extensive experiments using real-world data from Shenzhen, China, demonstrate that our proposed framework can achieve superior accuracy and robustness compared to the state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed machine learning surrogate for scalable simulation of thermal histories during wire-arc directed energy deposition</title>
<link>https://arxiv.org/abs/2507.09591</link>
<guid>https://arxiv.org/abs/2507.09591</guid>
<content:encoded><![CDATA[
<div> large-scale structural engineering, wire-arc directed energy deposition, finite element method, physics-informed neural networks, metal additive manufacturing <br /> 
Summary: <br /> 
The study focuses on wire-arc directed energy deposition (DED) for large-scale structural engineering applications. Traditional finite element method (FEM) simulations for thermal history prediction during deposition are computationally intensive. Physics-informed neural networks (PINNs) offer an alternative by combining physical knowledge with machine learning. The study investigates the scalability of PINNs, emphasizing efficient collocation points sampling to reduce computational time. Results show that PINNs can significantly decrease effort by up to 98.6% while maintaining accuracy and providing "super-resolution." The research suggests future enhancements for PINN performance in the context of metal additive manufacturing. <br /> <div>
arXiv:2507.09591v1 Announce Type: new 
Abstract: Wire-arc directed energy deposition (DED) has emerged as a promising additive manufacturing (AM) technology for large-scale structural engineering applications. However, the complex thermal dynamics inherent to the process present challenges in ensuring structural integrity and mechanical properties of fabricated thick walls and plates. While finite element method (FEM) simulations have been conventionally employed to predict thermal history during deposition, their computational demand remains prohibitively high for actual large-scale applications. Given the necessity of multiple repetitive simulations for heat management and the determination of an optimal printing strategy, FEM simulation quickly becomes entirely infeasible. Instead, advancements have been made in using trained neural networks as surrogate models for rapid prediction. However, traditional data-driven approaches necessitate large amounts of relevant and verifiable external data, during the training and validation of the neural network. Regarding large-scale wire-arc DED, none of these data sources are readily available in quantities sufficient for an accurate surrogate. The introduction of physics-informed neural networks (PINNs) has opened up an alternative simulation strategy by leveraging the existing physical knowledge of the phenomena with advanced machine learning methods. Despite their theoretical advantages, PINNs have seen limited application in the context of large-scale wire-arc DED for structural engineering. This study investigates the scalability of PINNs, focusing on efficient collocation points sampling, a critical factor controlling both the training time and model performance. Results show PINNs can reduce computational time and effort by up to 98.6%, while maintaining the desired accuracy and offering "super-resolution". Future directions for enhancing PINN performance in metal AM are discussed.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Matters Most? A Quantitative Meta-Analysis of AI-Based Predictors for Startup Success</title>
<link>https://arxiv.org/abs/2507.09675</link>
<guid>https://arxiv.org/abs/2507.09675</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, startup success, predictor importance, meta-analysis, context

Summary: 
- The study conducts a meta-analysis to synthesize predictor importance in AI-based startup evaluation, analyzing 13 empirical studies and identifying 58 unique predictors.
- The most powerful predictors for startup success are Firm Characteristics, Investor Structure, Digital and Social Traction, and Funding History.
- Predictor importance varies depending on the startup's goals and stage, with context influencing the hierarchy of factors.
- Factors predicting near-term funding milestones focus on immediate deal context, while long-term exits prioritize firm and investor characteristics.
- The study suggests a potential "convenience bias" in the literature, where predictor importance may be linked to data accessibility.
<br /><br />Summary: <div>
arXiv:2507.09675v1 Announce Type: new 
Abstract: Background: Predicting startup success with machine learning is a rapidly growing field, yet findings on key predictors are often fragmented and context-specific. This makes it difficult to discern robust patterns and highlights a need for a systematic synthesis of the evidence.
  Methods: This study conducts a quantitative meta-analysis to synthesize the literature on predictor importance in AI-based startup evaluation. We performed a systematic review to identify a final sample of 13 empirical studies that report rankable feature importance. From these papers, we extracted and categorized 58 unique predictors, synthesizing their importance using a Weighted Importance Score (WIS) that balances a feature's average rank with its frequency of appearance. We also conducted a moderator analysis to investigate how predictor importance changes with context (e.g., success definition).
  Results: Our aggregate analysis reveals that the most consistently powerful predictors are a quartet of foundational attributes: Firm Characteristics (e.g., age, location), Investor Structure (e.g., investor quality), Digital and Social Traction (e.g., online momentum), and Funding History. The moderator analysis further reveals that this hierarchy is highly context-dependent. For instance, predicting near-term funding milestones elevates the importance of the deal's immediate context, while predicting long-term exits prioritizes fundamental firm and investor characteristics.
  Conclusion: The factors that best predict startup success are not universal but are contingent on the startup's goals, stage, and the data used for evaluation. Our findings point to a potential "convenience bias" in the literature, where predictor importance may be tied to data accessibility. We conclude by underscoring the need for standardized reporting practices to enable more robust, cumulative knowledge building in the field.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legendre Polynomials and Their Use for Karhunen-Lo\`eve Expansion</title>
<link>https://arxiv.org/abs/2507.09825</link>
<guid>https://arxiv.org/abs/2507.09825</guid>
<content:encoded><![CDATA[
<div> Legendre polynomials, recurrence relation, Gaussian random fields, Karhunen-Love expansions, computational framework<br />
<br />
Summary:<br />
This paper presents a pedagogical review of the derivation of the three-term recurrence relation for Legendre polynomials, aimed at undergraduate students. It also introduces a computational framework for Karhunen-Love expansions of isotropic Gaussian random fields on hyper-rectangular domains. The framework utilizes Legendre polynomials and associated Gaussian quadrature techniques, ensuring efficiency in higher spatial dimensions. The approach approximates a covariance kernel using a non-negative mixture of squared-exponentials and employs a separable kernel for efficient Legendre-Galerkin discretization. Structural properties like even/odd parity structure in submatrices and a Duffy-type transformation for assembly reduce memory usage and arithmetic cost. The paper includes algorithms and numerical experiments in an open-source repository, reproducing all figures and tables provided in the work. <div>
arXiv:2507.09825v1 Announce Type: new 
Abstract: This paper makes two main contributions. First, we present a pedagogical review of the derivation of the three-term recurrence relation for Legendre polynomials, without relying on the classical Legendre differential equation, Rodrigues' formula, or generating functions. This exposition is designed to be accessible to undergraduate students.
  Second, we develop a computational framework for Karhunen-Lo\`eve expansions of isotropic Gaussian random fields on hyper-rectangular domains. The framework leverages Legendre polynomials and their associated Gaussian quadrature, and it remains efficient even in higher spatial dimensions.
  A covariance kernel is first approximated by a non-negative mixture of squared-exponentials, obtained via a Newton-optimized fit with a theoretically informed initialization. The resulting separable kernel enables a Legendre-Galerkin discretization in the form of a Kronecker product over single dimensions, with submatrices that exhibit even/odd parity structure. For assembly, we introduce a Duffy-type transformation followed by quadrature. These structural properties significantly reduce both memory usage and arithmetic cost compared to naive approaches. All algorithms and numerical experiments are provided in an open-source repository that reproduces every figure and table in this work.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-smooth optimization meets automated material model discovery</title>
<link>https://arxiv.org/abs/2507.10196</link>
<guid>https://arxiv.org/abs/2507.10196</guid>
<content:encoded><![CDATA[
<div> Automated material model discovery, Non-smooth L1-norm regularization, Minimization algorithms, Sparse regression problem, Regularization path computation<br />
<br />
Summary: 
This study explores the minimization of functions with non-smooth L1-norm regularization for automated material model discovery. It investigates the minimization of functions involving a metric quantifying the model-data mismatch and a regularization parameter determining solution sparsity. The study covers cases where the metric function is quadratic or non-quadratic, proposing efficient algorithms for solving the minimization problem and computing the entire regularization path. Algorithms discussed include coordinate descent, LARS for determining critical regularization values, proximal gradient method ISTA for non-quadratic scenarios, and a pathwise extension of ISTA. These algorithms are applied to discover hyperelastic material models from tension and shear data, showcasing their effectiveness in automated material model discovery in mechanics.
<br /> <div>
arXiv:2507.10196v1 Announce Type: new 
Abstract: Automated material model discovery disrupts the tedious and time-consuming cycle of iteratively calibrating and modifying manually designed models. Non-smooth L1-norm regularization is the backbone of automated model discovery; however, the current literature on automated material model discovery offers limited insights into the robust and efficient minimization of non-smooth objective functions. In this work, we examine the minimization of functions of the form f(w) + a ||w||_1, where w are the material model parameters, f is a metric that quantifies the mismatch between the material model and the observed data, and a is a regularization parameter that determines the sparsity of the solution. We investigate both the straightforward case where f is quadratic and the more complex scenario where it is non-quadratic or even non-convex. Importantly, we do not only focus on methods that solve the sparse regression problem for a given value of the regularization parameter a, but propose methods to efficiently compute the entire regularization path, facilitating the selection of a suitable a. Specifically, we present four algorithms and discuss their roles for automated material model discovery in mechanics: First, we recapitulate a well-known coordinate descent algorithm that solves the minimization problem assuming that f is quadratic for a given value of a, also known as the LASSO. Second, we discuss the algorithm LARS, which automatically determines the critical values of a, at which material parameters in w are set to zero. Third, we propose to use the proximal gradient method ISTA for automated material model discovery if f is not quadratic, and fourth, we suggest a pathwise extension of ISTA for computing the regularization path. We demonstrate the applicability of all algorithms for the discovery of hyperelastic material models from uniaxial tension and simple shear data.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios</title>
<link>https://arxiv.org/abs/2507.10448</link>
<guid>https://arxiv.org/abs/2507.10448</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial report generation, LLM models, multi-agent collaborative system, real financial scenarios, human evaluation

Summary:
Financial report generation tasks are complex and require extensive data analysis across various areas of finance. Existing Language Model models are limited in their ability to comprehensively analyze real financial scenarios. To address this, the FinTeam system was developed with a collaborative workflow involving four specialized agents: document analyzer, analyst, accountant, and consultant. Trained on specific financial expertise datasets, these agents work together to produce comprehensive financial reports. Evaluation results show that FinTeam outperformed baseline models like GPT-4o and Xuanyuan, achieving a 62.00% acceptance rate. The system also demonstrated an average improvement of 7.43% on FinCUGE and a 2.06% accuracy boost on FinEval. The project code is available on GitHub for further exploration. 

<br /><br />Summary: 
- Financial report generation tasks are complex and require extensive data analysis.
- The FinTeam system utilizes a collaborative workflow with specialized agents trained on specific financial expertise.
- Evaluation results show that FinTeam outperformed baseline models and achieved a high acceptance rate.
- The system demonstrated improvements on key financial evaluation metrics.
- The project code is available on GitHub for further exploration. <div>
arXiv:2507.10448v1 Announce Type: new 
Abstract: Financial report generation tasks range from macro- to micro-economics analysis, also requiring extensive data analysis. Existing LLM models are usually fine-tuned on simple QA tasks and cannot comprehensively analyze real financial scenarios. Given the complexity, financial companies often distribute tasks among departments. Inspired by this, we propose FinTeam, a financial multi-agent collaborative system, with a workflow with four LLM agents: document analyzer, analyst, accountant, and consultant. We train these agents with specific financial expertise using constructed datasets. We evaluate FinTeam on comprehensive financial tasks constructed from real online investment forums, including macroeconomic, industry, and company analysis. The human evaluation shows that by combining agents, the financial reports generate from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43% average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project is available at https://github.com/FudanDISC/DISC-FinLLM/.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Central Bank Digital Currencies: A Survey</title>
<link>https://arxiv.org/abs/2507.08880</link>
<guid>https://arxiv.org/abs/2507.08880</guid>
<content:encoded><![CDATA[
<div> Keywords: Central Bank Digital Currencies, CBDC design taxonomy, ledger technology, consensus mechanisms, CBDC ecosystem

Summary:
Central banks are exploring the implementation of Central Bank Digital Currencies (CBDCs) due to advancements in digital payment technologies. A review of 135 research papers from 2018 to 2025 examines CBDC design taxonomy and ecosystem frameworks. The study refines key architectural elements, investigates ledger technologies, consensus mechanisms, offline payments, and digital wallet integration. A comparative analysis of 26 existing CBDC systems across system architecture, ledger technology, access model, and application domain reveals trends like a two-tier architecture, distributed ledger technology (DLT), and token-based access model. There is a growing focus on using CBDCs for cross-border payments to improve efficiency. Recommendations for future research are provided. 

<br /><br />Summary: <div>
arXiv:2507.08880v1 Announce Type: cross 
Abstract: With the advancement of digital payment technologies, central banks worldwide have increasingly begun to explore the implementation of Central Bank Digital Currencies (CBDCs). This paper presents a comprehensive review of the latest developments in CBDC system design and implementation. By analyzing 135 research papers published between 2018 and 2025, the study provides an in-depth examination of CBDC design taxonomy and ecosystem frameworks. Grounded in the CBDC Design Pyramid, the paper refines and expands key architectural elements by thoroughly investigating innovations in ledger technologies, the selection of consensus mechanisms, and challenges associated with offline payments and digital wallet integration. Furthermore, it conceptualizes a CBDC ecosystem. A detailed comparative analysis of 26 existing CBDC systems is conducted across four dimensions: system architecture, ledger technology, access model, and application domain. The findings reveal that the most common configuration consists of a two-tier architecture, distributed ledger technology (DLT), and a token-based access model. However, no dominant trend has emerged regarding application domains. Notably, recent research shows a growing focus on leveraging CBDCs for cross-border payments to resolve inefficiencies and structural delays in current systems. Finally, the paper offers several forward-looking recommendations for future research.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components</title>
<link>https://arxiv.org/abs/2507.09443</link>
<guid>https://arxiv.org/abs/2507.09443</guid>
<content:encoded><![CDATA[
<div> Predictive Maintenance, Nuclear Power Plants, Convolutional Neural Network, Computational Thermomechanical Model, Fuel Rod<br />
Summary:<br />
Proactive maintenance strategies, like Predictive Maintenance, are crucial for Nuclear Power Plants as they help in reducing downtime caused by unexpected component failures. This study explores the use of a Convolutional Neural Network combined with a computational thermomechanical model to estimate the temperature, stress, and strain of a Pressurized Water Reactor fuel rod during operation using limited temperature measurements. The datasets for training, validation, and testing were generated through simulations involving a nuclear fuel performance code and a Thermal-Hydraulics Module. The CNN was trained for over 1,000 epochs and showed accurate temperature distribution predictions, further used in a thermomechanical model to determine stress and strain distribution within the fuel rod. This methodology has the potential to aid in the development of Predictive Maintenance tools for real-time monitoring of nuclear reactors. <br /> <div>
arXiv:2507.09443v1 Announce Type: cross 
Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play an important role in the operation of Nuclear Power Plants (NPPs), particularly due to their capacity to reduce offline time by preventing unexpected shutdowns caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN) architecture combined with a computational thermomechanical model to calculate the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel rod during operation. This estimation relies on a limited number of temperature measurements from the cladding's outer surface. This methodology can potentially aid in developing PdM tools for nuclear reactors by enabling real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled simulations involving BISON, a finite element-based nuclear fuel performance code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven simulations, varying the peak linear heat generation rates. Of these, eight were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting, achieving highly accurate temperature distribution predictions. These were then used in a thermomechanical model to determine the stress and strain distribution within the fuel rod.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When the Weak Becomes Strong: Effective Observables via Time-Symmetric Quantum Selection</title>
<link>https://arxiv.org/abs/2507.09716</link>
<guid>https://arxiv.org/abs/2507.09716</guid>
<content:encoded><![CDATA[
<div> weak values, time-symmetric quantum mechanics, sequential composition, state-conditioned observable, interference information

Summary: In this study, the authors explore the sequential composition of weak values within time-symmetric quantum mechanics. They analyze the combination of forward and reverse weak measurements, showing that their product corresponds to the normalized expectation value of a state-conditioned observable. This observable encodes interference information, especially when the initial state is a superposition. The concept extends to mixed states by using a density matrix, connecting to generalized quantum measurements. Practical applications in quantum computing include error detection and the inference of weak value phases through strong measurements in the case of pure states. <div>
arXiv:2507.09716v1 Announce Type: cross 
Abstract: We investigate the sequential composition of weak values in the framework of time-symmetric quantum mechanics. Specifically, we consider a forward'' weak measurement from a preselected state $\ket{\psi}$ to a post-selected state $\ket{\phi}$, followed by a reverse'' weak measurement. We show that the product of these two weak values corresponds to the normalized expectation value of a strong, state-conditioned observable $B = A P_\psi A$, where $P_\psi = \ket{\psi}\bra{\psi}$ is the projector onto the preselected state. Analyzing the structure of $B$, we demonstrate how it encodes interference information, particularly when $\ket{\psi}$ is a superposition rather than an eigenstate of $A$. This formulation extends naturally to mixed states by replacing $P_\psi$ with a generic density matrix $\rho$, linking the construction to the formalism of generalized quantum measurements. We illustrate practical applications in quantum information, including state-specific error witnessing in quantum computing, and show how the phase of a weak value can be inferred via strong measurements in the pure-state case.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Coincidence of Wants Mechanism for Swap Trade Execution in Decentralized Exchanges</title>
<link>https://arxiv.org/abs/2507.10149</link>
<guid>https://arxiv.org/abs/2507.10149</guid>
<content:encoded><![CDATA[
<div> cycle, Coincidence of Wants, decentralized exchange, asset matrix, liquidity providing

Summary:
The article presents a novel framework for identifying and completing Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators. Unlike existing auction based systems like CoWSwap, this approach uses an asset matrix formulation to verify feasibility, utilize oracle prices, and adhere to formal conservation laws. It also introduces bridging orders to execute slippage-free and capital preserving swap orders, offering a delta-neutral strategy for liquidity providing market makers. By leveraging graph traversal and imbalance correction, the algorithm efficiently discovers CoW cycles in real-world Arbitrum swap data and can seamlessly insert synthetic orders for atomic cycle closure. This structured CoW cycle execution demonstrates the potential for enhanced liquidity management and improved order processing in decentralized exchange ecosystems. <div>
arXiv:2507.10149v1 Announce Type: cross 
Abstract: We propose a mathematically rigorous framework for identifying and completing Coincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators. Unlike existing auction based systems such as CoWSwap, our approach introduces an asset matrix formulation that not only verifies feasibility using oracle prices and formal conservation laws but also completes partial CoW cycles of swap orders that are discovered using graph traversal and are settled using imbalance correction. We define bridging orders and show that the resulting execution is slippage free and capital preserving for LPs. Applied to real world Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW cycles and supports the insertion of synthetic orders for atomic cycle closure. This work can be thought of as the detailing of a potential delta-neutral strategy by liquidity providing market makers: a structured CoW cycle execution.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model for Composite Microstructures: Reconstruction, Stiffness, and Nonlinear Behavior Prediction</title>
<link>https://arxiv.org/abs/2411.06565</link>
<guid>https://arxiv.org/abs/2411.06565</guid>
<content:encoded><![CDATA[
<div> Keywords: Material Masked Autoencoder, self-supervised learning, Vision Transformer, microstructural features, composite images <br />
Summary: 
The Material Masked Autoencoder (MMAE) is introduced as a self-supervised Vision Transformer pre-trained on a large dataset of short-fiber composite images. The MMAE captures essential microstructural features and has broad applicability across tasks. Through fine-tuning on limited data, the MMAE can predict homogenized stiffness components effectively. The MMAE combined with an interaction-based material network (IMN) allows for inferring physically interpretable parameters and enables the extrapolation of nonlinear stress-strain responses. These results demonstrate the potential of microstructure foundation models in dealing with complex systems like 3D composites and experimental datasets. The MMAE represents a promising approach for advancing research in material science and lays the foundation for future extensions to more intricate systems. <br /><br />Summary: <div>
arXiv:2411.06565v4 Announce Type: replace 
Abstract: We present the Material Masked Autoencoder (MMAE), a self-supervised Vision Transformer pretrained on a large corpus of short-fiber composite images via masked image reconstruction. The pretrained MMAE learns latent representations that capture essential microstructural features and are broadly transferable across tasks. We demonstrate two key applications: (i) predicting homogenized stiffness components through fine-tuning on limited data, and (ii) inferring physically interpretable parameters by coupling MMAE with an interaction-based material network (IMN), thereby enabling extrapolation of nonlinear stress-strain responses. These results highlight the promise of microstructure foundation models and lay the groundwork for future extensions to more complex systems, such as 3D composites and experimental datasets.
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EP-GAT: Energy-based Parallel Graph Attention Neural Network for Stock Trend Classification</title>
<link>https://arxiv.org/abs/2507.08184</link>
<guid>https://arxiv.org/abs/2507.08184</guid>
<content:encoded><![CDATA[
arXiv:2507.08184v1 Announce Type: new 
Abstract: Graph neural networks have shown remarkable performance in forecasting stock movements, which arises from learning complex inter-dependencies between stocks and intra-dynamics of stocks. Existing approaches based on graph neural networks typically rely on static or manually defined factors to model changing inter-dependencies between stocks. Furthermore, these works often struggle to preserve hierarchical features within stocks. To bridge these gaps, this work presents the Energy-based Parallel Graph Attention Neural Network, a novel approach for predicting future movements for multiple stocks. First, it generates a dynamic stock graph with the energy difference between stocks and Boltzmann distribution, capturing evolving inter-dependencies between stocks. Then, a parallel graph attention mechanism is proposed to preserve the hierarchical intra-stock dynamics. Extensive experiments on five real-world datasets are conducted to validate the proposed approach, spanning from the US stock markets (NASDAQ, NYSE, SP) and UK stock markets (FTSE, LSE). The experimental results demonstrate that EP-GAT consistently outperforms competitive five baselines on test periods across various metrics. The ablation studies and hyperparameter sensitivity analysis further validate the effectiveness of each module in the proposed method.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models</title>
<link>https://arxiv.org/abs/2507.08030</link>
<guid>https://arxiv.org/abs/2507.08030</guid>
<content:encoded><![CDATA[
arXiv:2507.08030v1 Announce Type: cross 
Abstract: Generative AI models, including large language models (LLMs) and vision-language models (VLMs), are increasingly used to interpret medical images and answer clinical questions. Their responses often include inaccuracies; therefore, safety measures like medical disclaimers are critical to remind users that AI outputs are not professionally vetted or a substitute for medical advice. This study evaluated the presence of disclaimers in LLM and VLM outputs across model generations from 2022 to 2025. Using 500 mammograms, 500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs were screened for disclaimer phrases. Medical disclaimer presence in LLM and VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023 to 1.05% in 2025, respectively. By 2025, the majority of models displayed no disclaimers. As public models become more capable and authoritative, disclaimers must be implemented as a safeguard adapting to the clinical context of each output.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk Improves Trading Decisions</title>
<link>https://arxiv.org/abs/2507.08584</link>
<guid>https://arxiv.org/abs/2507.08584</guid>
<content:encoded><![CDATA[
arXiv:2507.08584v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in agentic frameworks, in which prompts trigger complex tool-based analysis in pursuit of a goal. While these frameworks have shown promise across multiple domains including in finance, they typically lack a principled model-building step, relying instead on sentiment- or trend-based analysis. We address this gap by developing an agentic system that uses LLMs to iteratively discover stochastic differential equations for financial time series. These models generate risk metrics which inform daily trading decisions. We evaluate our system in both traditional backtests and using a market simulator, which introduces synthetic but causally plausible price paths and news events. We find that model-informed trading strategies outperform standard LLM-based agents, improving Sharpe ratios across multiple equities. Our results show that combining LLMs with agentic model discovery enhances market risk estimation and enables more profitable trading decisions.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating diversion and treatment policies for opioid use disorder</title>
<link>https://arxiv.org/abs/2311.05076</link>
<guid>https://arxiv.org/abs/2311.05076</guid>
<content:encoded><![CDATA[
arXiv:2311.05076v4 Announce Type: replace 
Abstract: The United States (US) opioid crisis contributed to 81,806 fatalities in 2022. It has strained hospitals, treatment facilities, and law enforcement agencies due to the enormous resources and procedures needed to respond to the crisis. As a result, many individuals who use opioids never receive or finish the treatment they need and instead have many interactions with hospitals or the criminal justice system. This paper introduces a discrete event simulation model that evaluates three opioid use disorder treatment policies: arrest diversion, re-entry case management, and overdose diversion. Publicly available data from 2011 to 2019 in Dane County, Wisconsin, was used to forecast opioid-related outcomes through 2032. Through analyzing a variety of policy-mix implementations, the study offers a versatile framework for evaluating policies at various implementation levels. The results demonstrate that treatment policies that create new pathways and programming by utilizing treatment services and successfully divert at least 20% of eligible individuals can lead to more opioid-resilient communities. The benefits increase when more policies are enacted and/or offered to more individuals, with the largest impact from overdose diversion, followed by re-entry case management, and the smallest impact from arrest diversion. The statistically significant 10-year cumulative total reduction in societal costs from 2023 through 2032 ranges from $39 M (USD) to $584 M (USD), excluding implementation costs of policies. To reverse the opioid crisis within a community, treatment policies may need to be combined with other strategies, such as harm reduction, supply reduction, and use prevention.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing wheel loader performance -- an end-to-end approach</title>
<link>https://arxiv.org/abs/2501.06583</link>
<guid>https://arxiv.org/abs/2501.06583</guid>
<content:encoded><![CDATA[
arXiv:2501.06583v3 Announce Type: replace 
Abstract: Wheel loaders in mines and construction sites repeatedly load soil from a pile to load receivers. Automating this task presents a challenging planning problem since each loading's performance depends on the pile state, which depends on previous loadings. We investigate an end-to-end optimization approach considering future loading outcomes and transportation costs between the pile and load receivers. To predict the evolution of the pile state and the loading performance, we use world models that leverage deep neural networks trained on numerous simulated loading cycles. A look-ahead tree search optimizes the sequence of loading actions by evaluating the performance of thousands of action candidates, which expand into subsequent action candidates under the predicted pile states recursively. Test results demonstrate that, over a horizon of 15 sequential loadings, the look-ahead tree search is 6% more efficient than a greedy strategy, which always selects the action that maximizes the current single loading performance, and 14% more efficient than using a fixed loading controller optimized for the nominal case.
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pandora's Box Problem with Sequential Inspections</title>
<link>https://arxiv.org/abs/2507.07508</link>
<guid>https://arxiv.org/abs/2507.07508</guid>
<content:encoded><![CDATA[
<div> optimization, Pandora's box, tradeoff, information acquisition, cost efficiency

Summary:
The study explores a generalization of the Pandora's box problem in economic theory, where an agent can choose to fully open boxes at a certain fee or partially open them at a reduced cost. The research establishes a hardness result and utilizes stochastic optimization techniques to analyze the model comprehensively. Structural properties of the optimal policy are identified, providing insights into decision-making. Problem relaxations and near-optimal solutions are derived, and the optimal policy is characterized in special cases. An extensive numerical study comparing various policies reveals that threshold-based policies extending the Pandora's box optimal solution can effectively guide search decisions. The study contributes to understanding the tradeoff between information acquisition and cost efficiency in decision-making scenarios. <br /><br />Summary: <div>
arXiv:2507.07508v1 Announce Type: new 
Abstract: The Pandora's box problem (Weitzman 1979) is a core model in economic theory that captures an agent's (Pandora's) search for the best alternative (box). We study an important generalization of the problem where the agent can either fully open boxes for a certain fee to reveal their exact values or partially open them at a reduced cost. This introduces a new tradeoff between information acquisition and cost efficiency. We establish a hardness result and employ an array of techniques in stochastic optimization to provide a comprehensive analysis of this model. This includes (1) the identification of structural properties of the optimal policy that provide insights about optimal decisions; (2) the derivation of problem relaxations and provably near-optimal solutions; (3) the characterization of the optimal policy in special yet non-trivial cases; and (4) an extensive numerical study that compares the performance of various policies, and which provides additional insights about the optimal policy. Throughout, we show that intuitive threshold-based policies that extend the Pandora's box optimal solution can effectively guide search decisions.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics</title>
<link>https://arxiv.org/abs/2507.07830</link>
<guid>https://arxiv.org/abs/2507.07830</guid>
<content:encoded><![CDATA[
<div> modal reference spaces, model-order reduction framework, meshless SPH method, proper orthogonal decomposition, Galerkin POD, Adjoint Petrov-Galerkin

Summary:
The paper introduces a model-order reduction framework for meshless weakly compressible smoothed particle hydrodynamics (SPH). It proposes modal reference spaces to handle the challenges of discovering low-dimensional subspaces in SPH simulations. By projecting SPH snapshot data onto a reference space, low dimensionality of field quantities can be identified through modal decomposition techniques like proper orthogonal decomposition (POD). These modal quantities are then mapped back to the meshless SPH space during online prediction using scattered data interpolation. The framework is based on the meshless Galerkin POD and Adjoint Petrov-Galerkin projection model-order reduction formulations. Testing on various numerical experiments demonstrates good agreement in reconstructed and predictive velocity fields. However, the pressure field shows sensitivity to projection error due to weakly-compressible assumptions in SPH, which can be mitigated using nonlinear approximations like the APG approach. Overall, the proposed meshless model-order reduction framework shows promise in reducing computational costs for SPH simulations. 

<br /><br />Summary: <div>
arXiv:2507.07830v1 Announce Type: new 
Abstract: This work proposes a model-order reduction framework for the meshless weakly compressible smoothed particle hydrodynamics (SPH) method. The proposed framework introduces the concept of modal reference spaces to overcome the challenges of discovering low-dimensional subspaces from unstructured, dynamic, and mixing numerical topology that is often seen in SPH simulations. The proposed modal reference spaces enable a low-dimensional representation of the SPH field equations while maintaining their inherent meshless qualities. Modal reference spaces are constructed by projecting SPH snapshot data onto a reference space where low-dimensionality of field quantities can be discovered via traditional modal decomposition techniques (e.g., the proper orthogonal decomposition (POD)). Modal quantities are mapped back to the meshless SPH space via scattered data interpolation during the online predictive stage. The proposed model-order reduction framework is cast into the \emph{meshless} Galerkin POD (GPOD) and the Adjoint Petrov--Galerkin (APG) projection model-order reduction (PMOR) formulation. The PMORs are tested on three numerical experiments: 1) the Taylor--Green vortex; 2) lid-driven cavity; and 3) flow past an open cavity. Results show good agreement in reconstructed and predictive velocity fields, which showcase the ability of the proposed framework to evolve the unstructured, dynamic, and mixing SPH field equations in a low-dimensional subspace. Results also show that the pressure field is sensitive to the projection error due to the stiff weakly-compressible assumption made in the current SPH framework, but can be alleviated through nonlinear approximations, such as the APG approach. Ultimately, the presented meshless model-order reduction framework marks a step toward enabling drastic cost savings of SPH simulations.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning Enhanced Multi-Factor Quantitative Trading: A Cross-Sectional Portfolio Optimization Approach with Bias Correction</title>
<link>https://arxiv.org/abs/2507.07107</link>
<guid>https://arxiv.org/abs/2507.07107</guid>
<content:encoded><![CDATA[
<div> alpha discovery, bias correction, factor computation, portfolio optimization, machine learning

Summary:
This paper introduces a machine learning framework for quantitative trading that emphasizes factor engineering, real-time computation optimization, and portfolio construction. The framework combines multi-factor alpha discovery with bias correction techniques using PyTorch-accelerated factor computation. It processes 500-1000 factors from open-source alpha101 extensions and market microstructure signals. Key innovations include tensor-based factor computation acceleration, data augmentation using geometric Brownian motion, and cross-sectional neutralization strategies. Empirical validation on Chinese A-share markets from 2010-2024 showcases annualized returns of 20% with Sharpe ratios exceeding 2.0, surpassing traditional approaches. The study underscores the significance of bias correction in factor construction and the considerable impact of cross-sectional portfolio optimization on strategy performance. Experimental implementations and code are accessible on GitHub at: https://github.com/initial-d/ml-quant-trading

<br /><br />Summary: <div>
arXiv:2507.07107v1 Announce Type: cross 
Abstract: This paper presents a comprehensive machine learning framework for quantitative trading that achieves superior risk-adjusted returns through systematic factor engineering, real-time computation optimization, and cross-sectional portfolio construction. Our approach integrates multi-factor alpha discovery with bias correction techniques, leveraging PyTorch-accelerated factor computation and advanced portfolio optimization. The system processes 500-1000 factors derived from open-source alpha101 extensions and proprietary market microstructure signals. Key innovations include tensor-based factor computation acceleration, geometric Brownian motion data augmentation, and cross-sectional neutralization strategies. Empirical validation on Chinese A-share markets (2010-2024) demonstrates annualized returns of $20\%$ with Sharpe ratios exceeding 2.0, significantly outperforming traditional approaches. Our analysis reveals the critical importance of bias correction in factor construction and the substantial impact of cross-sectional portfolio optimization on strategy performance. Code and experimental implementations are available at: https://github.com/initial-d/ml-quant-trading
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable ADER-DG Transport Method with Polynomial Order Independent CFL Limit</title>
<link>https://arxiv.org/abs/2507.07304</link>
<guid>https://arxiv.org/abs/2507.07304</guid>
<content:encoded><![CDATA[
<div> Discontinuous Galerkin methods, Locally Implicit, Globally Explicit, ADER-DG scheme, Element-width based CFL condition, Transport-dominated problems <br />
Summary:<br />
This paper introduces a novel locally implicit, globally explicit ADER-DG scheme for transport-dominated problems. The method overcomes the restrictive time step constraints seen in high-order DG methods by using an element-width based CFL condition, allowing for a maximum stable time step independent of polynomial degree. By solving element-local implicit problems at each time step, the method effectively captures the domain of dependence and remains stable for CFL numbers up to $1/\sqrt{d}$ in $d$ spatial dimensions. Rigorous stability proofs in one dimension and von Neumann stability analysis in higher dimensions validate the method's accuracy and convergence. Numerical experiments on linear and nonlinear test cases further demonstrate the effectiveness of the proposed approach. <br /> <div>
arXiv:2507.07304v1 Announce Type: cross 
Abstract: Discontinuous Galerkin (DG) methods are known to suffer from increasingly restrictive time step constraints as the polynomial order increases, limiting their efficiency at high orders. In this paper, we introduce a novel locally implicit, but globally explicit ADER-DG scheme designed for transport-dominated problems. The method achieves a maximum stable time step governed by an element-width based CFL condition that is independent of the polynomial degree. By solving a set of element-local implicit problems at each time step, our approach more effectively captures the domain of dependence. As a result, our method remains stable for CFL numbers up to $1/\sqrt{d}$ in $d$ spatial dimensions. We provide a rigorous stability proof in one dimension, and extend the analysis to two and three dimensions using a semi-analytical von Neumann stability analysis. The accuracy and convergence of the method are demonstrated through numerical experiments on both linear and nonlinear test cases.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Plausibility-Validity Gap by Fine-Tuning a Reasoning-Enhanced LLM for Chemical Synthesis and Discovery</title>
<link>https://arxiv.org/abs/2507.07328</link>
<guid>https://arxiv.org/abs/2507.07328</guid>
<content:encoded><![CDATA[
<div> chemistry, language models, plausibility-validity gap, Low-Rank Adaptation, dual-domain dataset

Summary:
The paper addresses the challenge of factually invalid information generated by Large Language Models (LLMs) in specialized domains like chemistry, known as the "plausibility-validity gap." A specialized scientific assistant was developed by fine-tuning the Magistral Small model using Low-Rank Adaptation (LoRA) and a dual-domain dataset curated from various sources. The evaluation showed improved format adherence, chemical validity of generated molecules, and feasibility of proposed synthesis routes. The model exhibited a hierarchical learning pattern, with syntactic correctness learned more easily than chemical possibility and synthesis feasibility. While competitive with human experts in areas like chemical creativity and reasoning, limitations such as errors in stereochemistry, static knowledge cutoff, and occasional reference hallucination were identified. This work establishes a framework for transforming generalist LLMs into specialized tools for chemical research, while also highlighting areas for future enhancement. 

<br /><br />Summary: <div>
arXiv:2507.07328v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often generate scientifically plausible but factually invalid information, a challenge we term the "plausibility-validity gap," particularly in specialized domains like chemistry. This paper presents a systematic methodology to bridge this gap by developing a specialized scientific assistant. We utilized the Magistral Small model, noted for its integrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation (LoRA). A key component of our approach was the creation of a "dual-domain dataset," a comprehensive corpus curated from various sources encompassing both molecular properties and chemical reactions, which was standardized to ensure quality. Our evaluation demonstrates that the fine-tuned model achieves significant improvements over the baseline model in format adherence, chemical validity of generated molecules, and the feasibility of proposed synthesis routes. The results indicate a hierarchical learning pattern, where syntactic correctness is learned more readily than chemical possibility and synthesis feasibility. While a comparative analysis with human experts revealed competitive performance in areas like chemical creativity and reasoning, it also highlighted key limitations, including persistent errors in stereochemistry, a static knowledge cutoff, and occasional reference hallucination. This work establishes a viable framework for adapting generalist LLMs into reliable, specialized tools for chemical research, while also delineating critical areas for future improvement.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computationally Efficient Information-Driven Optical Design with Interchanging Optimization</title>
<link>https://arxiv.org/abs/2507.07789</link>
<guid>https://arxiv.org/abs/2507.07789</guid>
<content:encoded><![CDATA[
<div> IDEAL-IO, imaging systems, information content, optical design, computational decoding<br />
<br />
Summary:<br />
Recent work has shown that evaluating imaging systems based on the information content of their measurements alone can simplify optical design. However, the IDEAL method for automating this process faces challenges such as high memory usage, long runtimes, and a potentially mismatched objective function. To address these issues, IDEAL-IO was introduced, which separates density estimation from optical parameter optimization. By alternating between fitting models to measurements and updating optical parameters using fixed models, IDEAL-IO reduces runtime and memory usage while allowing for more expressive density models. This approach was successfully validated on various imaging applications, demonstrating the practicality and scalability of information-driven optimization for real-world imaging system design. <br /> <div>
arXiv:2507.07789v1 Announce Type: cross 
Abstract: Recent work has demonstrated that imaging systems can be evaluated through the information content of their measurements alone, enabling application-agnostic optical design that avoids computational decoding challenges. Information-Driven Encoder Analysis Learning (IDEAL) was proposed to automate this process through gradient-based. In this work, we study IDEAL across diverse imaging systems and find that it suffers from high memory usage, long runtimes, and a potentially mismatched objective function due to end-to-end differentiability requirements. We introduce IDEAL with Interchanging Optimization (IDEAL-IO), a method that decouples density estimation from optical parameter optimization by alternating between fitting models to current measurements and updating optical parameters using fixed models for information estimation. This approach reduces runtime and memory usage by up to 6x while enabling more expressive density models that guide optimization toward superior designs. We validate our method on diffractive optics, lensless imaging, and snapshot 3D microscopy applications, establishing information-theoretic optimization as a practical, scalable strategy for real-world imaging system design.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development and Real-World Application of Commercial Motor Vehicle Safety Enforcement Dashboards</title>
<link>https://arxiv.org/abs/2507.06351</link>
<guid>https://arxiv.org/abs/2507.06351</guid>
<content:encoded><![CDATA[
<div> Keywords: Commercial Motor Vehicle Safety, Dashboards, Performance Measures, Enforcement Initiatives, Maryland State Police

Summary: 
This study introduces CMV safety dashboards developed with input from CMV enforcement professionals. The dashboards enhance analysis of CMV safety performance measures based on probe vehicle speeds, inspection/citation data, and enforcement activities. Collaboration with Maryland State Police identified a section of I-81 for targeted CMV enforcement, with a post-enforcement evaluation revealing mixed results. The dashboards aim to facilitate efficient monitoring of CMV safety and enforcement initiatives, with a focus on improving highway safety. Further refinement of the dashboards and citation data is needed to enhance the effectiveness of targeted enforcement efforts.<br /><br /> <div>
arXiv:2507.06351v1 Announce Type: new 
Abstract: Commercial Motor Vehicle (CMV) safety is crucial in traffic management and public safety. CMVs account for numerous traffic incidents, so monitoring CMV safety and safety inspections is essential for ensuring safe and efficient highway movement. This paper presents the development and real-world application of CMV dashboards designed under the guidance of CMV safety enforcement professionals from the Maryland State Police (MSP), the Maryland Department of Transportation - State Highway Administration (MDOT - SHA), and the Federal Motor Carrier Safety Administration (FMCSA) to enable intuitive and efficient analysis of CMV safety performance measures. First, three CMV safety dashboards enable CMV safety professionals to identify sites with a history of safety performance issues. A supplemental dashboard automates the analysis of CMV enforcement initiatives using the same performance measures. These performance measures are based on CMV probe vehicle speeds, inspection/citation data from Truck Weigh and Inspection Stations (TWIS), patrolling enforcement, and Virtual Weigh Stations (VWS). The authors collaborated with MSP to identify a portion of I-81 in Maryland, susceptible to improvement from targeted CMV enforcement. The supplemental enforcement assessment dashboard was employed to evaluate the impact of enforcement, including the post-enforcement halo effect. The results of the post-enforcement evaluation were mixed, indicating a need for more fine-grained citation data.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eyes on the Road, Mind Beyond Vision: Context-Aware Multi-modal Enhanced Risk Anticipation</title>
<link>https://arxiv.org/abs/2507.06444</link>
<guid>https://arxiv.org/abs/2507.06444</guid>
<content:encoded><![CDATA[
<div> Keywords: accident anticipation, multi-modal framework, driver attention maps, adaptive mechanism, spatio-temporal dependencies <br />
Summary:<br />
The paper introduces CAMERA, a multi-modal framework for accident anticipation that integrates dashcam video, textual annotations, and driver attention maps. This model utilizes an adaptive mechanism based on scene complexity and gaze entropy to reduce false alarms while maintaining high recall in dynamic traffic scenarios. By employing a hierarchical fusion pipeline with Bi-GRU and a Geo-Context Vision-Language module, CAMERA captures spatio-temporal dependencies and translates spatial relationships into human-centric alerts. Evaluations on the DADA-2000 dataset and benchmarks show that CAMERA outperforms existing methods in accuracy and lead time, demonstrating the effectiveness of incorporating driver cognition and contextual information in accident anticipation models. <div>
arXiv:2507.06444v1 Announce Type: new 
Abstract: Accurate accident anticipation remains challenging when driver cognition and dynamic road conditions are underrepresented in predictive models. In this paper, we propose CAMERA (Context-Aware Multi-modal Enhanced Risk Anticipation), a multi-modal framework integrating dashcam video, textual annotations, and driver attention maps for robust accident anticipation. Unlike existing methods that rely on static or environment-centric thresholds, CAMERA employs an adaptive mechanism guided by scene complexity and gaze entropy, reducing false alarms while maintaining high recall in dynamic, multi-agent traffic scenarios. A hierarchical fusion pipeline with Bi-GRU (Bidirectional GRU) captures spatio-temporal dependencies, while a Geo-Context Vision-Language module translates 3D spatial relationships into interpretable, human-centric alerts. Evaluations on the DADA-2000 and benchmarks show that CAMERA achieves state-of-the-art performance, improving accuracy and lead time. These results demonstrate the effectiveness of modeling driver attention, contextual description, and adaptive risk thresholds to enable more reliable accident anticipation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forex Trading Robot Using Fuzzy Logic</title>
<link>https://arxiv.org/abs/2507.06383</link>
<guid>https://arxiv.org/abs/2507.06383</guid>
<content:encoded><![CDATA[
<div> Proposed fuzzy system, short-term transactions, forex market, fuzzy logic, improved accuracy<br />Summary:<br /> This study introduces a fuzzy system for short-term forex transactions, enhancing traditional strategies by utilizing fuzzy logic. Unlike conventional approaches using predefined ranges for technical indicators like RSI and CCI, this system employs fuzzy Mamdani systems for each indicator, with results combined via voting to create a trading robot. Compared to other methods, the proposed approach shows a significant increase in profitability factor, as demonstrated by calculations of net profit, gross profit, and maximum capital reduction. <div>
arXiv:2507.06383v1 Announce Type: cross 
Abstract: In this study, we propose a fuzzy system for conducting short-term transactions in the forex market. The system is designed to enhance common strategies in the forex market using fuzzy logic, thereby improving the accuracy of transactions. Traditionally, technical strategies based on oscillator indicators have relied on predefined ranges for indicators such as Relative Strength Index (RSI), Commodity Channel Indicator (CCI), and Stochastic to determine entry points for trades. However, the use of these classic indicators has yielded suboptimal results due to the changing nature of the market over time. In our proposed approach, instead of employing classical indicators, we introduce a fuzzy Mamdani system for each indicator. The results obtained from these systems are then combined through voting to design a trading robot. Our findings demonstrate a considerable increase in the profitability factor compared to three other methods. Additionally, net profit, gross profit, and maximum capital reduction are calculated and compared across all approaches.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls</title>
<link>https://arxiv.org/abs/2507.06423</link>
<guid>https://arxiv.org/abs/2507.06423</guid>
<content:encoded><![CDATA[
<div> Keywords: Rugsafe, cryptocurrency, protocol, vaults, anticoins

Summary: 
Rugsafe introduces a protocol to address rug pulls in the cryptocurrency ecosystem by using cryptographic security and economic incentives. The protocol includes specialized vaults where rugged tokens can be securely deposited, and anticoins are issued as receipts. These anticoins are pegged to the price of rugged tokens and can be used within the ecosystem or burned for additional rewards. The supply of Rugsafe tokens is adjusted based on activity, ensuring stability. Users deposit rugged tokens into vaults on multiple chains and burn anticoins to receive incentives on the RugSafe chain. The protocol's vaults work across different blockchains, offering a practical solution to cryptocurrency market challenges.<br /><br />Summary: <div>
arXiv:2507.06423v1 Announce Type: cross 
Abstract: Rugsafe introduces a comprehensive protocol aimed at mitigating the risks of rug pulls in the cryptocurrency ecosystem. By utilizing cryptographic security measures and economic incentives, the protocol provides a secure multichain system for recovering assets and transforming rugged tokens into opportunities and rewards. Foundational to Rugsafe are specialized vaults where rugged tokens can be securely deposited, and anticoin tokens are issued as receipts. These anticoins are designed to be inversely pegged to the price movement of the underlying rugged token. Users can utilize these anticoins within the ecosystem or choose to burn them, further securing the protocol and earning additional rewards. The supply of the native Rugsafe token is dynamically adjusted based on the volume, value, and activity of rugged tokens, ensuring stability and resilience. By depositing rugged tokens into a vault on several chains, and by burning anticoins, users receive incentives on the RugSafe chain. This protocol's vaults are designed to work in heterogenous blockchain ecosystems, offering a practical and effective solution to one of the most significant challenges in the cryptocurrency market.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams</title>
<link>https://arxiv.org/abs/2507.06803</link>
<guid>https://arxiv.org/abs/2507.06803</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamical systems, domain knowledge, expert knowledge, SysML diagrams, natural language processing

Summary:
This paper presents a strategy for automating the generation of computational models for engineering dynamical systems. The approach leverages domain and expert knowledge to extract information from relevant documents using System Modeling Language (SysML) diagrams. Natural Language Processing (NLP) techniques and Large Language Models (LLMs) are used to enhance the accuracy of the generated diagrams. The process involves several steps, including extracting key nouns and relationships, generating block attributes and relationships, and creating Block Definition Diagrams (BDDs). Case studies demonstrate the application of automated SysML diagram generation. The computational models are then derived from the SysML diagrams via code generation and computational model generation steps. NLP aids in summarization during code generation, while LLMs are used for validation. The proposed approach is flexible across systems, domains, and software, as shown through an example with a simple pendulum model. Improved performance is achieved compared to using LLMs alone.<br /><br />Summary: <div>
arXiv:2507.06803v1 Announce Type: cross 
Abstract: This paper contributes to speeding up the design and deployment of engineering dynamical systems by proposing a strategy for exploiting domain and expert knowledge for the automated generation of dynamical system computational model starting from a corpus of document relevant to the dynamical system of interest and an input document describing the specific system. This strategy is implemented in five steps and, crucially, it uses system modeling language diagrams (SysML) to extract accurate information about the dependencies, attributes, and operations of components. Natural Language Processing (NLP) strategies and Large Language Models (LLMs) are employed in specific tasks to improve intermediate outputs of the SySML diagrams automated generation, such as: list of key nouns; list of extracted relationships; list of key phrases and key relationships; block attribute values; block relationships; and BDD diagram generation. The applicability of automated SysML diagram generation is illustrated with different case studies. The computational models of complex dynamical systems from SysML diagrams are then obtained via code generation and computational model generation steps. In the code generation step, NLP strategies are used for summarization, while LLMs are used for validation only. The proposed approach is not limited to a specific system, domain, or computational software. The applicability of the proposed approach is shown via an end-to-end example from text to model of a simple pendulum, showing improved performance compared to results yielded by LLMs only.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models</title>
<link>https://arxiv.org/abs/2507.06853</link>
<guid>https://arxiv.org/abs/2507.06853</guid>
<content:encoded><![CDATA[
<div> diffusion models, structure elucidation, generative modeling, molecular spectra, machine learning

Summary: 
DiffSpectra is a novel generative framework for molecular structure elucidation from spectral data. It combines diffusion models with SE(3)-equivariant architectures to infer both 2D and 3D molecular structures. The model integrates topological and geometric information to accurately predict molecular structures. SpecFormer, a transformer-based spectral encoder, captures spectral dependencies for conditioning the generation process. DiffSpectra achieves high accuracy in structure elucidation, with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model's effectiveness is attributed to its 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. This work advances the field by unifying multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation. <div>
arXiv:2507.06853v1 Announce Type: cross 
Abstract: Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Load and Information Processing in Financial Markets: Theory and Evidence from Disclosure Complexity</title>
<link>https://arxiv.org/abs/2507.07037</link>
<guid>https://arxiv.org/abs/2507.07037</guid>
<content:encoded><![CDATA[
<div> complexity, cognitive load, financial markets, price discovery, information processing 

Summary: 
The article presents a theoretical framework for understanding how cognitive load affects information processing in financial markets. It distinguishes between attention allocation and cognitive processing capacity, demonstrating that complex information has varying effects on different types of investors. Using a dataset of corporate disclosures and regulatory changes, the study finds that cognitive load significantly impairs price discovery, particularly among less sophisticated investors. A one-standard-deviation increase in cognitive complexity leads to an 18% reduction in information incorporation speed and a 23% increase in mispricing duration. The research supports three theoretical mechanisms: selective attention, processing errors, and strategic complexity, indicating that cognitive constraints create inefficiencies in financial markets. These findings have implications for disclosure regulation and market design. 

Summary: <div>
arXiv:2507.07037v1 Announce Type: cross 
Abstract: We develop a theoretical framework for understanding how cognitive load affects information processing in financial markets and test it using exogenous variation in disclosure complexity. Our model distinguishes between attention allocation and cognitive processing capacity, showing that complex information creates differential effects across investor types. Using a comprehensive dataset of corporate disclosures and a novel identification strategy based on regulatory changes, we find that cognitive load significantly impairs price discovery, with effects concentrated among less sophisticated investors. A one-standard-deviation increase in cognitive complexity reduces information incorporation speed by 18\% and increases mispricing duration by 23\%. We provide evidence for three theoretical mechanisms: selective attention, processing errors, and strategic complexity. Our findings suggest that cognitive constraints create systematic inefficiencies in financial markets, with important implications for disclosure regulation and market design.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Bounded Rationality: Formal Verification of Simon's Satisficing Through Flexible Stochastic Dominance</title>
<link>https://arxiv.org/abs/2507.07052</link>
<guid>https://arxiv.org/abs/2507.07052</guid>
<content:encoded><![CDATA[
<div> formalization, bounded rationality, Lean 4, machine-verified proofs, decision-making

Summary: 
The paper introduces Flexible First-Order Stochastic Dominance (FFSD), a formal framework that bridges classical expected utility theory with Herbert Simon's theory of bounded rationality. Machine-verified proofs in Lean 4 demonstrate how FFSD incorporates parameterized tolerance thresholds to capture satisficing behavior. The paper identifies a critical threshold for unique reference points, establishes an equivalence theorem between FFSD and expected utility maximization for approximate indicator functions, and extends the framework to multi-dimensional decision settings. By encoding these concepts in Lean 4's dependent type theory, the paper presents the first machine-checked formalization of bounded rationality, enabling mechanized reasoning about economic decision-making under uncertainty and cognitive limitations. This work showcases the synergy between formal mathematics and economic theory, illustrating how interactive theorem proving can enhance the understanding of behavioral economics concepts traditionally expressed qualitatively.<br /><br />Summary: <div>
arXiv:2507.07052v1 Announce Type: cross 
Abstract: This paper introduces Flexible First-Order Stochastic Dominance (FFSD), a mathematically rigorous framework that formalizes Herbert Simon's concept of bounded rationality using the Lean 4 theorem prover. We develop machine-verified proofs demonstrating that FFSD bridges classical expected utility theory with Simon's satisficing behavior through parameterized tolerance thresholds. Our approach yields several key results: (1) a critical threshold $\varepsilon < 1/2$ that guarantees uniqueness of reference points, (2) an equivalence theorem linking FFSD to expected utility maximization for approximate indicator functions, and (3) extensions to multi-dimensional decision settings. By encoding these concepts in Lean 4's dependent type theory, we provide the first machine-checked formalization of Simon's bounded rationality, creating a foundation for mechanized reasoning about economic decision-making under uncertainty with cognitive limitations. This work contributes to the growing intersection between formal mathematics and economic theory, demonstrating how interactive theorem proving can advance our understanding of behavioral economics concepts that have traditionally been expressed only qualitatively.
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolFORM: Multi-modal Flow Matching for Structure-Based Drug Design</title>
<link>https://arxiv.org/abs/2507.05503</link>
<guid>https://arxiv.org/abs/2507.05503</guid>
<content:encoded><![CDATA[
<div> drug design, structure-based, generative models, molecular modalities, multi-modal flow

Summary:
This article introduces MolFORM, a novel generative framework for structure-based drug design that incorporates both discrete (atom types) and continuous (3D coordinates) molecular modalities using multi-flow matching. The framework also includes a preference-guided fine-tuning stage based on Direct Preference Optimization (DPO) using the Vina score as a reward signal. The proposed multi-modal flow DPO co-modeling strategy aligns discrete and continuous modalities, leading to consistent improvements in generation quality as measured by various evaluation metrics. This approach provides a promising alternative to diffusion-based generative models in structure-based drug design, offering a new direction for improving the effectiveness of drug discovery efforts. <div>
arXiv:2507.05503v1 Announce Type: new 
Abstract: Structure-based drug design (SBDD) seeks to generate molecules that bind effectively to protein targets by leveraging their 3D structural information. While diffusion-based generative models have become the predominant approach for SBDD, alternative non-autoregressive frameworks remain relatively underexplored. In this work, we introduce MolFORM, a novel generative framework that jointly models discrete (atom types) and continuous (3D coordinates) molecular modalities using multi-flow matching. To further enhance generation quality, we incorporate a preference-guided fine-tuning stage based on \textit{Direct Preference Optimization} (DPO), using Vina score as a reward signal. We propose a multi-modal flow DPO co-modeling strategy that simultaneously aligns discrete and continuous modalities, leading to consistent improvements across multiple evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCNP-GO: A python package for assembling MCNP input files with a systems engineering approach</title>
<link>https://arxiv.org/abs/2507.05659</link>
<guid>https://arxiv.org/abs/2507.05659</guid>
<content:encoded><![CDATA[
<div> MCNP-GO, Python package, MCNP input files, assembly, precise modeling<br />
<br />
Summary: 
MCNP-GO is a Python package designed to manipulate and assemble MCNP input files, making it easier for users to assemble multiple independent objects into a cohesive file. It addresses challenges in managing large databases of input files by providing various functionalities such as renumbering, extracting subsets, transforming, and assembling files while handling collisions and materials. The tool ensures reliability and traceability through configuration management systems, keeping track of operations performed on files for easy modification. It is especially useful for applications where precise modeling and positioning of equipment are crucial. The package is user-friendly and efficient, showcased through a practical example of assembling an MCNP input file for a tomographic experiment. MCNP-GO is suitable for users with minimal Python knowledge. <br /><br /> <div>
arXiv:2507.05659v1 Announce Type: new 
Abstract: This article introduces MCNP-GO (https://github.com/afriou/mcnpgo), a Python package designed to manipulate and assemble MCNP input files, allowing users to assemble a set of independent objects, each described by a valid MCNP file, into a single cohesive file. This tool is particularly useful for applications where precise modeling and positioning of equipment are crucial. The package addresses the challenges of managing large databases of MCNP input files, ensuring reliability and traceability through configuration management systems. MCNP-GO provides functionalities such as renumbering, extracting subsets of files, transforming files, and assembling files while managing collisions and materials. It also keeps track of the operations performed on files, enhancing traceability and ease of modification. The article demonstrates the package's capabilities through a practical example of assembling an MCNP input file for a tomographic experiment, highlighting its efficiency and user-friendliness. MCNP-GO is designed for users with minimal Python knowledge.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions</title>
<link>https://arxiv.org/abs/2507.06133</link>
<guid>https://arxiv.org/abs/2507.06133</guid>
<content:encoded><![CDATA[
<div> video diffusion, physics surrogate, partial differential equations, Sequential Deep Operator Network, conditional video diffusion

Summary:
- The article introduces a physics surrogate using conditional video diffusion models.
- A two-stage surrogate is proposed, consisting of an S-DeepONet for producing a physics-consistent prior and a conditional video diffusion model for learning the residual.
- By focusing on the residual space, the model can sharpen high-frequency structures while maintaining global coherence.
- The hybrid surrogate outperforms single-stage counterparts in vortex-dominated flow and plastic deformation benchmarks.
- The approach allows for faithful reconstruction of localized features, accelerates convergence, and transfers seamlessly between different types of nonlinear, time-dependent continua. 

<br /><br />Summary: <div>
arXiv:2507.06133v1 Announce Type: new 
Abstract: Video-diffusion models have recently set the standard in video generation, inpainting, and domain translation thanks to their training stability and high perceptual fidelity. Building on these strengths, we repurpose conditional video diffusion as a physics surrogate for spatio-temporal fields governed by partial differential equations (PDEs). Our two-stage surrogate first applies a Sequential Deep Operator Network (S-DeepONet) to produce a coarse, physics-consistent prior from the prescribed boundary or loading conditions. The prior is then passed to a conditional video diffusion model that learns only the residual: the point-wise difference between the ground truth and the S-DeepONet prediction. By shifting the learning burden from the full solution to its much smaller residual space, diffusion can focus on sharpening high-frequency structures without sacrificing global coherence. The framework is assessed on two disparate benchmarks: (i) vortex-dominated lid-driven cavity flow and (ii) tensile plastic deformation of dogbone specimens. Across these data sets the hybrid surrogate consistently outperforms its single-stage counterpart, cutting the mean relative L2 error from 4.57% to 0.83% for the flow problem and from 4.42% to 2.94% for plasticity, a relative improvements of 81.8% and 33.5% respectively. The hybrid approach not only lowers quantitative errors but also improves visual quality, visibly recovering fine spatial details. These results show that (i) conditioning diffusion on a physics-aware prior enables faithful reconstruction of localized features, (ii) residual learning reduces the problem, accelerating convergence and enhancing accuracy, and (iii) the same architecture transfers seamlessly from incompressible flow to nonlinear elasto-plasticity without problem-specific architectural modifications, highlighting its broad applicability to nonlinear, time-dependent continua.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A new engineering theory describing oblique free surface impact by flexible plates</title>
<link>https://arxiv.org/abs/2103.08012</link>
<guid>https://arxiv.org/abs/2103.08012</guid>
<content:encoded><![CDATA[
<div> fluid-structural interaction, slamming loads, water entry, flexible plate, impact force <br />
Summary: 
This paper addresses the challenge of incorporating slamming loads into the structural design of planning hulls by proposing a new engineering theory using a specialized fluid-structural interaction simulation approach. The researchers validated their simulation approach through water entry experiments with flexible plates. They then conducted numerical analyses to understand the impact force and plate deformations based on different parameters. Using their simulation as a "microscope," the study observed the evolution of fluid flows and plate deformations during slamming events. From these observations, a new engineering theory was proposed for flexible plates obliquely impacting the water surface, such as high-speed water craft reentry characterized by porpoising. The research contributes to advancing the understanding of slamming phenomena and offers insight into designing structures to withstand slamming loads.<br /><br />Summary: <div>
arXiv:2103.08012v3 Announce Type: cross 
Abstract: Consideration of slamming loads within the structural design of planning hulls is of critical importance in ensuring adequate structural performance in order to avoid potential catastrophic consequences. However, because of the intricacy in the interplay between complex fluid flows and nonlinear structural deformations that accompany the phenomenology of slamming, a general engineering theory in slamming has yet to be uncovered, and so design relies on specialized theories. In this paper, we propose one such theory for a design case that has, until now, eluded a proper description. In pursuit of this theory, we employ a specialized implicit, partitioned fluid-structural interaction (FSI) simulation approach, in order to study the underlying physical mechanisms accompanying the oblique impact of a flexible plate during water entry. In the present work, we first present validation results from flexible plate water entry experiments, to confirm the veracity of the developed FSI solver. Subsequent to validation, we carry out a series of numerical analyses, in an effort to characterize the regimes in impact force and plate out-of-plane deformations, as a function of impact velocities and plate flexural rigidity. Finally, we use our FSI solver, as a kind of "microscope", to study the mechanistic evolution of fluid flows and elastic plate deformations that occur during slamming. Based on these observations, we propose a novel, but simple engineering theory for flexible plates obliquely impacting the water free surface (e.g. high speed porpoising water craft reentry).
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying heterogeneous micromechanical properties of biological tissues via physics-informed neural networks</title>
<link>https://arxiv.org/abs/2402.10741</link>
<guid>https://arxiv.org/abs/2402.10741</guid>
<content:encoded><![CDATA[
<div> machine learning, elastic properties, soft materials, hyperelastic materials, physics-informed

Summary:
- The study focuses on identifying heterogeneous elastic properties in soft materials using a physics-informed machine learning approach.
- Traditional methods struggle with estimating local stress fields making it challenging to determine full-field mechanical responses.
- The proposed method utilizes physics-informed neural networks (PINNs) to infer elasticity maps in large deformation hyperelastic materials.
- The accuracy and computational efficiency of the approach were evaluated across various materials with structural complexity resembling real tissue patterns.
- The network architecture consistently produced highly accurate estimations of heterogeneous elasticity maps, even in the presence of up to 10% noise in the training data. <div>
arXiv:2402.10741v3 Announce Type: cross 
Abstract: The heterogeneous micromechanical properties of biological tissues have profound implications across diverse medical and engineering domains. However, identifying full-field heterogeneous elastic properties of soft materials using traditional engineering approaches is fundamentally challenging due to difficulties in estimating local stress fields. Recently, there has been a growing interest in using data-driven models to learn full-field mechanical responses such as displacement and strain from experimental or synthetic data. However, research studies on inferring full-field elastic properties of materials, a more challenging problem, are scarce, particularly for large deformation, hyperelastic materials. Here, we propose a physics-informed machine learning approach to identify the elasticity map in nonlinear, large deformation hyperelastic materials. We evaluate the prediction accuracies and computational efficiency of physics-informed neural networks (PINNs) by inferring the heterogeneous elasticity maps across three materials with structural complexity that closely resemble real tissue patterns, such as brain tissue and tricuspid valve tissue. We further applied our improved architecture to three additional examples of breast cancer tissue and extended our analysis to three hyperelastic constitutive models: Neo-Hookean, Mooney Rivlin, and Gent. Our selected network architecture consistently produced highly accurate estimations of heterogeneous elasticity maps, even when there was up to 10% noise present in the training data.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADEPT: A Noninvasive Method for Determining Elastic Parameters of Valve Tissue</title>
<link>https://arxiv.org/abs/2409.19081</link>
<guid>https://arxiv.org/abs/2409.19081</guid>
<content:encoded><![CDATA[
<div> Keywords: valve repair, computer simulation, noninvasive method, mechanical parameters, tricuspid valve

Summary: 
The study introduces a novel noninvasive method, ADEPT, for determining elastic parameters of valve tissue, focusing on the tricuspid valve in a child. By tracking valve displacements in 3D echocardiogram sequences and employing physics-informed neural networks, patient-specific mechanical properties were estimated and applied to a simulated model. The method significantly improved accuracy compared to generic parameters from literature, with the simulated model closely aligning with reference image segmentation. This approach enhances the feasibility of computer simulations for predicting optimal valve repair outcomes before intervention, addressing the current limitation of insufficient noninvasive methods to assess in vivo mechanical parameters of valves. The study demonstrates the potential of ADEPT in personalized medicine for valve interventions, paving the way for more precise and tailored treatment strategies in clinical practice.<br /><br />Summary: <div>
arXiv:2409.19081v2 Announce Type: cross 
Abstract: Computer simulation of "virtual interventions" may inform optimal valve repair for a given patient prior to intervention. However, the paucity of noninvasive methods to determine in vivo mechanical parameters of valves limits the accuracy of computer prediction and their clinical application. To address this, we propose ADEPT: A noninvasive method for Determining Elastic Parameters of valve Tissue. In this work, we demonstrated its application to the tricuspid valve of a child. We first tracked valve displacements from open to closed frames within a 3D echocardiogram time sequence using image registration. Physics-informed neural networks were subsequently applied to estimate the nonlinear mechanical properties from first principles and reference displacements. The simulated model using these patient-specific parameters closely aligned with the reference image segmentation, achieving a mean symmetric distance of less than 1 mm. Our approach doubled the accuracy of the simulated model compared to the generic parameters reported in the literature.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity</title>
<link>https://arxiv.org/abs/2507.05816</link>
<guid>https://arxiv.org/abs/2507.05816</guid>
<content:encoded><![CDATA[
<div> Keywords: ROP risk prediction, large language models, affective biases, Chinese benchmark dataset, Affective-ROPTester <br />
<br />
Summary: 
The study introduces a new Chinese benchmark dataset, CROP, for predicting retinopathy of prematurity (ROP) risk using large language models (LLMs). The Affective-ROPTester framework is proposed to evaluate LLMs' predictive capabilities and affective biases in ROP risk stratification. Results show that LLMs perform better with structured external inputs than with intrinsic knowledge alone in predicting ROP risk. Affective biases are observed in model outputs, with a tendency to overestimate medium- and high-risk cases. Positive emotional framing helps mitigate predictive bias compared to negative emotions. The study underscores the importance of affect-sensitive prompt engineering in enhancing diagnostic reliability and proposes Affective-ROPTester as a tool for evaluating and mitigating affective bias in clinical language modeling systems. <div>
arXiv:2507.05816v1 Announce Type: cross 
Abstract: Despite the remarkable progress of large language models (LLMs) across various domains, their capacity to predict retinopathy of prematurity (ROP) risk remains largely unexplored. To address this gap, we introduce a novel Chinese benchmark dataset, termed CROP, comprising 993 admission records annotated with low, medium, and high-risk labels. To systematically examine the predictive capabilities and affective biases of LLMs in ROP risk stratification, we propose Affective-ROPTester, an automated evaluation framework incorporating three prompting strategies: Instruction-based, Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and ICL schemes leverage external medical knowledge to enhance predictive accuracy. Crucially, we integrate emotional elements at the prompt level to investigate how different affective framings influence the model's ability to predict ROP and its bias patterns. Empirical results derived from the CROP dataset yield two principal observations. First, LLMs demonstrate limited efficacy in ROP risk prediction when operating solely on intrinsic knowledge, yet exhibit marked performance gains when augmented with structured external inputs. Second, affective biases are evident in the model outputs, with a consistent inclination toward overestimating medium- and high-risk cases. Third, compared to negative emotions, positive emotional framing contributes to mitigating predictive bias in model outputs. These findings highlight the critical role of affect-sensitive prompt engineering in enhancing diagnostic reliability and emphasize the utility of Affective-ROPTester as a framework for evaluating and mitigating affective bias in clinical language modeling systems.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topic Modeling and Link-Prediction for Material Property Discovery</title>
<link>https://arxiv.org/abs/2507.06139</link>
<guid>https://arxiv.org/abs/2507.06139</guid>
<content:encoded><![CDATA[
<div> Hierarchical Link Prediction, Matrix Factorization, Material Discovery, Topic Tree, Transition-Metal Dichalcogenides <br />
Summary:<br />
The study introduces a hierarchical link prediction framework utilizing matrix factorization to uncover hidden connections within scientific literature networks and knowledge graphs. By combining Hierarchical Nonnegative Matrix Factorization (HNMFk), Boolean matrix factorization (BNMFk), and Logistic matrix factorization (LMF), a three-level topic tree is constructed from a vast document corpus focusing on transition-metal dichalcogenides (TMDs). Through an ensemble approach of BNMFk + LMF, the method provides both interpretable clusters and probabilistic scoring, revealing coherent topics related to TMDs such as superconductivity and energy storage. Missing or weakly connected links between topics and materials are highlighted, sparking new hypotheses for cross-disciplinary exploration. Validation shows the model accurately predicts associations within TMD clusters, showcasing its ability to uncover hidden connections in a diverse scientific document corpus. Interactive tools like the Streamlit dashboard facilitate human-in-the-loop scientific discovery by presenting the inferred links and enabling further exploration. <br /> <div>
arXiv:2507.06139v1 Announce Type: cross 
Abstract: Link prediction infers missing or future relations between graph nodes, based on connection patterns. Scientific literature networks and knowledge graphs are typically large, sparse, and noisy, and often contain missing links between entities. We present an AI-driven hierarchical link prediction framework that integrates matrix factorization to infer hidden associations and steer discovery in complex material domains. Our method combines Hierarchical Nonnegative Matrix Factorization (HNMFk) and Boolean matrix factorization (BNMFk) with automatic model selection, as well as Logistic matrix factorization (LMF), we use to construct a three-level topic tree from a 46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs). These materials are studied in a variety of physics fields with many current and potential applications.
  An ensemble BNMFk + LMF approach fuses discrete interpretability with probabilistic scoring. The resulting HNMFk clusters map each material onto coherent topics like superconductivity, energy storage, and tribology. Also, missing or weakly connected links are highlight between topics and materials, suggesting novel hypotheses for cross-disciplinary exploration. We validate our method by removing publications about superconductivity in well-known superconductors, and show the model predicts associations with the superconducting TMD clusters. This shows the method finds hidden connections in a graph of material to latent topic associations built from scientific literature, especially useful when examining a diverse corpus of scientific documents covering the same class of phenomena or materials but originating from distinct communities and perspectives. The inferred links generating new hypotheses, produced by our method, are exposed through an interactive Streamlit dashboard, designed for human-in-the-loop scientific discovery.
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Membrane Degradation in PEM Electrolyzers with Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2507.02887</link>
<guid>https://arxiv.org/abs/2507.02887</guid>
<content:encoded><![CDATA[
<div> Keywords: Proton exchange membrane electrolyzers, degradation modeling, Physics-Informed Neural Networks, membrane thinning, predictive tools 

Summary: 
Proton exchange membrane (PEM) electrolyzers play a crucial role in sustainable hydrogen production but face challenges due to membrane degradation, impacting long-term performance. Traditional physics-based models require numerous parameters, while data-driven approaches like machine learning may lack physical consistency. This study introduces Physics-Informed Neural Networks (PINNs) to model membrane degradation in PEM electrolyzers accurately. By coupling two differential equations, one modeling membrane thinning and another governing cell voltage evolution, the PINN captures system degradation dynamics with limited noisy data. The hybrid approach offers a balance between interpretability and flexibility, providing a foundation for more robust predictive tools in electrochemical system diagnostics.<br /><br />Summary: <div>
arXiv:2507.02887v1 Announce Type: new 
Abstract: Proton exchange membrane (PEM) electrolyzers are pivotal for sustainable hydrogen production, yet their long-term performance is hindered by membrane degradation, which poses reliability and safety challenges. Therefore, accurate modeling of this degradation is essential for optimizing durability and performance. To address these concerns, traditional physics-based models have been developed, offering interpretability but requiring numerous parameters that are often difficult to measure and calibrate. Conversely, data-driven approaches, such as machine learning, offer flexibility but may lack physical consistency and generalizability. To address these limitations, this study presents the first application of Physics-Informed Neural Networks (PINNs) to model membrane degradation in PEM electrolyzers. The proposed PINN framework couples two ordinary differential equations, one modeling membrane thinning via a first-order degradation law and another governing the time evolution of the cell voltage under membrane degradation. Results demonstrate that the PINN accurately captures the long-term system's degradation dynamics while preserving physical interpretability with limited noisy data. Consequently, this work introduces a novel hybrid modeling approach for estimating and understanding membrane degradation mechanisms in PEM electrolyzers, offering a foundation for more robust predictive tools in electrochemical system diagnostics.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics Simulation with Implicit Neural Representations of Complex Geometries</title>
<link>https://arxiv.org/abs/2507.03087</link>
<guid>https://arxiv.org/abs/2507.03087</guid>
<content:encoded><![CDATA[
<div> Neural Implicit Representations, Shifted Boundary Method, Linear Elasticity Simulations, Computational Framework, Meshless Simulations <br />
Summary: <br />
This study introduces a novel computational framework that seamlessly integrates Implicit Neural Representations (INRs) with the Shifted Boundary Method (SBM) for linear elasticity simulations, eliminating the need for explicit geometry transformations and meshing. By directly accessing neural implicit geometry, the method acquires essential boundary information and distance vectors for SBM, enhancing efficiency and accuracy. The approach is tested on complex geometries (Stanford Bunny, Eiffel Tower, gyroids) sourced from triangle soups and point clouds, demonstrating significant computational advantages. The method's robustness and accuracy make it suitable for various applications, including biomedical, geophysical, and advanced manufacturing fields. <div>
arXiv:2507.03087v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs), characterized by neural network-encoded signed distance fields, provide a powerful means to represent complex geometries continuously and efficiently. While successful in computer vision and generative modeling, integrating INRs into computational analysis workflows, such as finite element simulations, remains underdeveloped. In this work, we propose a computational framework that seamlessly combines INRs with the Shifted Boundary Method (SBM) for high-fidelity linear elasticity simulations without explicit geometry transformations. By directly querying the neural implicit geometry, we obtain the surrogate boundaries and distance vectors essential for SBM, effectively eliminating the meshing step. We demonstrate the efficacy and robustness of our approach through elasticity simulations on complex geometries (Stanford Bunny, Eiffel Tower, gyroids) sourced from triangle soups and point clouds. Our method showcases significant computational advantages and accuracy, underscoring its potential in biomedical, geophysical, and advanced manufacturing applications.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding</title>
<link>https://arxiv.org/abs/2507.03197</link>
<guid>https://arxiv.org/abs/2507.03197</guid>
<content:encoded><![CDATA[
<div> Keywords: CD8+ killer T cells, CD4+ helper T cells, T cell receptors, transformer-based models, post-hoc explainable AI

Summary:
CD8+ "killer" T cells and CD4+ "helper" T cells are crucial in the immune system, recognizing antigens presented by pMHC via T cell receptors. Transformer models like TULIP have shown great performance in this area but lack interpretability. The new QCAI method aims to interpret cross-attention mechanisms in transformer decoders for TCR-pMHC modeling. A benchmark, TCR-XAI, with 274 TCR-pMHC structures is used to evaluate the method's performance. By computing physical distances between amino acid residues and assessing residue importance, QCAI demonstrates state-of-the-art interpretability and prediction accuracy. The study highlights the significance of understanding T cell response mechanisms and the potential of QCAI in enhancing comprehensibility in TCR-pMHC modeling. 

<br /><br />Summary: <div>
arXiv:2507.03197v1 Announce Type: new 
Abstract: CD8+ "killer" T cells and CD4+ "helper" T cells play a central role in the adaptive immune system by recognizing antigens presented by Major Histocompatibility Complex (pMHC) molecules via T Cell Receptors (TCRs). Modeling binding between T cells and the pMHC complex is fundamental to understanding basic mechanisms of human immune response as well as in developing therapies. While transformer-based models such as TULIP have achieved impressive performance in this domain, their black-box nature precludes interpretability and thus limits a deeper mechanistic understanding of T cell response. Most existing post-hoc explainable AI (XAI) methods are confined to encoder-only, co-attention, or model-specific architectures and cannot handle encoder-decoder transformers used in TCR-pMHC modeling. To address this gap, we propose Quantifying Cross-Attention Interaction (QCAI), a new post-hoc method designed to interpret the cross-attention mechanisms in transformer decoders. Quantitative evaluation is a challenge for XAI methods; we have compiled TCR-XAI, a benchmark consisting of 274 experimentally determined TCR-pMHC structures to serve as ground truth for binding. Using these structures we compute physical distances between relevant amino acid residues in the TCR-pMHC interaction region and evaluate how well our method and others estimate the importance of residues in this region across the dataset. We show that QCAI achieves state-of-the-art performance on both interpretability and prediction accuracy under the TCR-XAI benchmark.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElliottAgents: A Natural Language-Driven Multi-Agent System for Stock Market Analysis and Prediction</title>
<link>https://arxiv.org/abs/2507.03435</link>
<guid>https://arxiv.org/abs/2507.03435</guid>
<content:encoded><![CDATA[
<div> Keywords: ElliottAgents, multi-agent system, natural language processing, stock market data, AI-driven analysis

Summary:
ElliottAgents is a multi-agent system that utilizes natural language processing and large language models to analyze complex stock market data. The system integrates AI-driven analysis with the Elliott Wave Principle to create predictions and explanations that are easily understandable by humans. One of its key features is the natural language dialogue between agents, allowing for collaborative analysis refinement. The architecture, enhanced by large language models, enables advanced language understanding, reasoning, and autonomous decision-making. Through experiments, it has been demonstrated that ElliottAgents is effective in pattern recognition and generating natural language descriptions of market trends. This research contributes to the field of natural language processing applications in specialized domains, showcasing how AI-driven dialogue systems can enhance collaborative analysis in data-intensive fields. By bridging the gap between complex financial data and human comprehension, ElliottAgents addresses the need for interpretable and adaptive prediction systems in finance. 

<br /><br />Summary: <div>
arXiv:2507.03435v1 Announce Type: new 
Abstract: This paper presents ElliottAgents, a multi-agent system leveraging natural language processing (NLP) and large language models (LLMs) to analyze complex stock market data. The system combines AI-driven analysis with the Elliott Wave Principle to generate human-comprehensible predictions and explanations. A key feature is the natural language dialogue between agents, enabling collaborative analysis refinement. The LLM-enhanced architecture facilitates advanced language understanding, reasoning, and autonomous decision-making. Experiments demonstrate the system's effectiveness in pattern recognition and generating natural language descriptions of market trends. ElliottAgents contributes to NLP applications in specialized domains, showcasing how AI-driven dialogue systems can enhance collaborative analysis in data-intensive fields. This research bridges the gap between complex financial data and human understanding, addressing the need for interpretable and adaptive prediction systems in finance.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Concept for Autonomous Problem-Solving in Intralogistics Scenarios</title>
<link>https://arxiv.org/abs/2507.03534</link>
<guid>https://arxiv.org/abs/2507.03534</guid>
<content:encoded><![CDATA[
<div> autonomy, automation systems, enabling technologies, Large Language Models, problem-solving capabilities  
Summary:  
- The paper discusses the importance of achieving greater autonomy in automation systems to effectively handle unforeseen situations in complex real-world environments.  
- It outlines a structured concept consisting of context enrichment, situation analysis, and solution strategy generation as key steps towards increasing autonomy in automation systems.  
- The proposed approach aims to reduce the need for human intervention by enabling automation systems to make more independent decisions.  
- Possible realizations of the concept, including the use of Large Language Models, are discussed as ways to enhance autonomy in automation systems.  
- While some tasks may still require human assistance, the approach significantly improves the adaptive and intelligent problem-solving capabilities of automation systems.  
<br /><br />Summary: <div>
arXiv:2507.03534v1 Announce Type: new 
Abstract: Achieving greater autonomy in automation systems is crucial for handling unforeseen situations effectively. However, this remains challenging due to technological limitations and the complexity of real-world environments. This paper examines the need for increased autonomy, defines the problem, and outlines key enabling technologies. A structured concept is proposed, consisting of three main steps: context enrichment, situation analysis, and generation of solution strategies. By following this approach, automation systems can make more independent decisions, reducing the need for human intervention. Additionally, possible realizations of the concept are discussed, especially the use of Large Language Models. While certain tasks may still require human assistance, the proposed approach significantly enhances the autonomy of automation systems, enabling more adaptive and intelligent problem-solving capabilities.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Based Control for Power-to-X Platforms: Knowledge Integration for Digital Twins</title>
<link>https://arxiv.org/abs/2507.03553</link>
<guid>https://arxiv.org/abs/2507.03553</guid>
<content:encoded><![CDATA[
<div> Keywords: Offshore Power-to-X platforms, Digital Twins, adaptive process control, semantic technologies, Neo4j <br />
Summary: <br />
Offshore Power-to-X platforms face challenges in adaptive process control due to volatile operating conditions. To address this issue, utilizing Digital Twins in these platforms is seen as a promising solution. The integration of heterogeneous models and structured representation of model information is crucial for comprehensive knowledge integration in Digital Twins. The proposed approach utilizes a standardized description of behavior models, semantic technologies, and a graph-based model understanding for automatic adaptation and selection of suitable models. This approach is implemented using a graph-based knowledge representation with Neo4j, automatic data extraction from Asset Administration Shells, and port matching to ensure compatible model configurations. By combining these elements, the approach aims to enhance the flexibility and efficiency of Power-to-X platforms. <div>
arXiv:2507.03553v1 Announce Type: new 
Abstract: Offshore Power-to-X platforms enable flexible conversion of renewable energy, but place high demands on adaptive process control due to volatile operating conditions. To face this challenge, using Digital Twins in Power-to-X platforms is a promising approach. Comprehensive knowledge integration in Digital Twins requires the combination of heterogeneous models and a structured representation of model information. The proposed approach uses a standardized description of behavior models, semantic technologies and a graph-based model understanding to enable automatic adaption and selection of suitable models. It is implemented using a graph-based knowledge representation with Neo4j, automatic data extraction from Asset Administration Shells and port matching to ensure compatible model configurations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Operator-based machine learning framework for generalizable prediction of unsteady treatment dynamics in stormwater infrastructure</title>
<link>https://arxiv.org/abs/2507.04682</link>
<guid>https://arxiv.org/abs/2507.04682</guid>
<content:encoded><![CDATA[
<div> neural network, stormwater treatment, urban water management, computational fluid dynamics, pollutant transport

Summary: 
The study introduces a composite operator-based neural network (CPNN) framework for predicting hydraulics and pollutant dynamics in stormwater treatment systems. Traditional models lack accuracy due to oversimplified processes, while computational fluid dynamics is too computationally expensive. The CPNN framework achieves high accuracy in predicting hydraulic behavior and particulate matter concentration in stormwater treatment devices. Challenges are found in capturing dynamics under extreme low-flow conditions, as they contribute less to the training process. Sensitivity analyses using the CPNN highlight the impact of storm event loading on pollutant transport. The CPNN framework shows promise for continuous, long-term evaluation of stormwater infrastructure performance, aiding in climate-aware planning and implementation. <div>
arXiv:2507.04682v1 Announce Type: new 
Abstract: Stormwater infrastructures are decentralized urban water-management systems that face highly unsteady hydraulic and pollutant loadings from episodic rainfall-runoff events. Accurately evaluating their in-situ treatment performance is essential for cost-effective design and planning. Traditional lumped dynamic models (e.g., continuously stirred tank reactor, CSTR) are computationally efficient but oversimplify transport and reaction processes, limiting predictive accuracy and insight. Computational fluid dynamics (CFD) resolves detailed turbulent transport and pollutant fate physics but incurs prohibitive computational cost for unsteady and long-term simulations. To address these limitations, this study develops a composite operator-based neural network (CPNN) framework that leverages state-of-the-art operator learning to predict the spatial and temporal dynamics of hydraulics and particulate matter (PM) in stormwater treatment. The framework is demonstrated on a hydrodynamic separator (HS), a common urban treatment device. Results indicate that the CPNN achieves R2 > 0.8 for hydraulic predictions in 95.2% of test cases; for PM concentration predictions, R2 > 0.8 in 72.6% of cases and 0.4 < R2 < 0.8 in 22.6%. The analysis identifies challenges in capturing dynamics under extreme low-flow conditions, owing to their lower contribution to the training loss. Exploiting the automatic-differentiation capability of the CPNN, sensitivity analyses quantify the influence of storm event loading on PM transport. Finally, the potential of the CPNN framework for continuous, long-term evaluation of stormwater infrastructure performance is discussed, marking a step toward robust, climate-aware planning and implementation.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Gradient Low-Rank Projection Fine-Tuning for LLMs</title>
<link>https://arxiv.org/abs/2507.02503</link>
<guid>https://arxiv.org/abs/2507.02503</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Continual Learning, Low-Rank Adaptation, GORP, Gradient Projection

Summary:
Continual fine-tuning of Large Language Models (LLMs) often faces a trade-off between efficiency and expressiveness. The Low-Rank Adaptation (LoRA) method, while efficient, restricts the model's learning capabilities and knowledge transfer due to its low-rank nature and reliance on explicit parameter constraints. In response, GORP (Gradient LOw Rank Projection) for Continual Learning introduces a novel training strategy that combines full and low-rank parameters to update within a unified low-rank gradient subspace. This approach expands the optimization space while maintaining efficiency and reducing catastrophic forgetting. Through extensive experiments on continual learning benchmarks, GORP outperforms existing state-of-the-art methods. The code for GORP is publicly available on GitHub, providing a framework for implementing and testing this innovative training strategy.<br /><br />Summary: <div>
arXiv:2507.02503v1 Announce Type: cross 
Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable diffusion-based generation for multi-channel biological data</title>
<link>https://arxiv.org/abs/2507.02902</link>
<guid>https://arxiv.org/abs/2507.02902</guid>
<content:encoded><![CDATA[
<div> diffusion-based models; spatial profiling technologies; generative modeling; high-dimensional data; spatial alignment <br />
Summary: This article introduces a unified diffusion framework for generating structured and spatial biological data, such as imaging mass cytometry and spatial transcriptomics. The model incorporates a hierarchical feature injection mechanism for multi-resolution conditioning and a combination of latent-space and output-space channel-wise attention to capture inter-channel relationships. It is trained using a random masking strategy to support flexible conditioning and generalization to arbitrary subsets of observed channels. The model demonstrates superior performance in tasks such as protein imputation in IMC and gene-to-protein prediction in single-cell datasets, as well as strong generalization to unseen conditional configurations. <div>
arXiv:2507.02902v1 Announce Type: cross 
Abstract: Spatial profiling technologies in biology, such as imaging mass cytometry (IMC) and spatial transcriptomics (ST), generate high-dimensional, multi-channel data with strong spatial alignment and complex inter-channel relationships. Generative modeling of such data requires jointly capturing intra- and inter-channel structure, while also generalizing across arbitrary combinations of observed and missing channels for practical application. Existing diffusion-based models generally assume low-dimensional inputs (e.g., RGB images) and rely on simple conditioning mechanisms that break spatial correspondence and ignore inter-channel dependencies. This work proposes a unified diffusion framework for controllable generation over structured and spatial biological data. Our model contains two key innovations: (1) a hierarchical feature injection mechanism that enables multi-resolution conditioning on spatially aligned channels, and (2) a combination of latent-space and output-space channel-wise attention to capture inter-channel relationships. To support flexible conditioning and generalization to arbitrary subsets of observed channels, we train the model using a random masking strategy, enabling it to reconstruct missing channels from any combination of inputs. We demonstrate state-of-the-art performance across both spatial and non-spatial prediction tasks, including protein imputation in IMC and gene-to-protein prediction in single-cell datasets, and show strong generalization to unseen conditional configurations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolProphecy: Bridging Medicinal Chemists' Knowledge and Molecular Pre-Trained Models via a Multi-Modal Framework</title>
<link>https://arxiv.org/abs/2507.02932</link>
<guid>https://arxiv.org/abs/2507.02932</guid>
<content:encoded><![CDATA[
<div> framework, molecular property prediction, human-in-the-loop, ChatGPT, chemist knowledge <br />
Summary: 
The MolProphecy framework integrates chemists' domain knowledge into molecular property prediction models using a human-in-the-loop approach. By leveraging ChatGPT as a virtual chemist, it simulates expert reasoning and decision-making to enhance model accuracy. MolProphecy outperforms state-of-the-art models on benchmark datasets, showing improvements in RMSE and AUROC metrics. The framework combines chemist knowledge with structural features through a gated cross-attention mechanism, improving both accuracy and interpretability. It offers a collaborative approach to drug discovery, with the flexibility to incorporate real chemist input without requiring retraining. The implementation of MolProphecy is publicly available for use. <br /> <div>
arXiv:2507.02932v1 Announce Type: cross 
Abstract: MolProphecy is a human-in-the-loop (HITL) multi-modal framework designed to integrate chemists' domain knowledge into molecular property prediction models. While molecular pre-trained models have enabled significant gains in predictive accuracy, they often fail to capture the tacit, interpretive reasoning central to expert-driven molecular design. To address this, MolProphecy employs ChatGPT as a virtual chemist to simulate expert-level reasoning and decision-making. The generated chemist knowledge is embedded by the large language model (LLM) as a dedicated knowledge representation and then fused with graph-based molecular features through a gated cross-attention mechanism, enabling joint reasoning over human-derived and structural features. Evaluated on four benchmark datasets (FreeSolv, BACE, SIDER, and ClinTox), MolProphecy outperforms state-of-the-art (SOTA) models, achieving a 15.0 percent reduction in RMSE on FreeSolv and a 5.39 percent improvement in AUROC on BACE. Analysis reveals that chemist knowledge and structural features provide complementary contributions, improving both accuracy and interpretability. MolProphecy offers a practical and generalizable approach for collaborative drug discovery, with the flexibility to incorporate real chemist input in place of the current simulated proxy--without the need for model retraining. The implementation is publicly available at https://github.com/zhangruochi/MolProphecy.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-Through Tensors: A Unified Computational Graph Architecture for Multi-Layer Transportation Network Optimization</title>
<link>https://arxiv.org/abs/2507.02961</link>
<guid>https://arxiv.org/abs/2507.02961</guid>
<content:encoded><![CDATA[
<div> Keywords: Flow Through Tensors, transportation network modeling, tensor decomposition, gradient-based optimization, integrated mobility systems<br />
Summary:<br />
Flow Through Tensors (FTT) is introduced as a unified computational graph architecture for transportation network modeling. It connects origin destination flows, path probabilities, and link travel times as interconnected tensors, allowing for multidimensional analysis of traffic patterns. The framework enables gradient-based optimization across different modeling elements, supports efficient system efficiency quantification over time, space, and user groups, and implements tensor decomposition techniques for large-scale applications. These innovations enable real-time control strategies, coordination between transportation modes/operators, and enforcement of network constraints. FTT bridges the gap between theoretical models and practical deployment needs, providing a foundation for next-generation integrated mobility systems. <div>
arXiv:2507.02961v1 Announce Type: cross 
Abstract: Modern transportation network modeling increasingly involves the integration of diverse methodologies including sensor-based forecasting, reinforcement learning, classical flow optimization, and demand modeling that have traditionally been developed in isolation. This paper introduces Flow Through Tensors (FTT), a unified computational graph architecture that connects origin destination flows, path probabilities, and link travel times as interconnected tensors. Our framework makes three key contributions: first, it establishes a consistent mathematical structure that enables gradient-based optimization across previously separate modeling elements; second, it supports multidimensional analysis of traffic patterns over time, space, and user groups with precise quantification of system efficiency; third, it implements tensor decomposition techniques that maintain computational tractability for large scale applications. These innovations collectively enable real time control strategies, efficient coordination between multiple transportation modes and operators, and rigorous enforcement of physical network constraints. The FTT framework bridges the gap between theoretical transportation models and practical deployment needs, providing a foundation for next generation integrated mobility systems.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LANTERN: A Machine Learning Framework for Lipid Nanoparticle Transfection Efficiency Prediction</title>
<link>https://arxiv.org/abs/2507.03209</link>
<guid>https://arxiv.org/abs/2507.03209</guid>
<content:encoded><![CDATA[
<div> ionizable lipids, lipid nanoparticle, RNA delivery, machine learning, transfection efficiency

Summary:
LANTERN is a machine learning framework designed to predict transfection efficiency for lipid nanoparticles used in RNA delivery. By utilizing chemically informative features like Morgan fingerprints and Expert descriptors, LANTERN outperformed previous models, including AGILE, in predicting transfection efficiency. The model achieved a high performance with an R-squared value of 0.8161 and a correlation coefficient of 0.9053. LANTERN showed consistent strong performance across multiple evaluation metrics, making it a valuable tool for accelerating the design of lipid-based RNA delivery systems. The discovery of new ionizable lipids for efficient RNA delivery is crucial for the development of RNA-based therapeutics. The use of machine learning in predicting transfection efficiency from molecular structure has the potential to streamline the identification of lead compounds and advance the field of RNA-based therapeutics. <div>
arXiv:2507.03209v1 Announce Type: cross 
Abstract: The discovery of new ionizable lipids for efficient lipid nanoparticle (LNP)-mediated RNA delivery remains a critical bottleneck for RNA-based therapeutics development. Recent advances have highlighted the potential of machine learning (ML) to predict transfection efficiency from molecular structure, enabling high-throughput virtual screening and accelerating lead identification. However, existing approaches are hindered by inadequate data quality, ineffective feature representations, low predictive accuracy, and poor generalizability. Here, we present LANTERN (Lipid nANoparticle Transfection Efficiency pRedictioN), a robust ML framework for predicting transfection efficiency based on ionizable lipid representation. We benchmarked a diverse set of ML models against AGILE, a previously published model developed for transfection prediction. Our results show that combining simpler models with chemically informative features, particularly count-based Morgan fingerprints, outperforms more complex models that rely on internally learned embeddings, such as AGILE. We also show that a multi-layer perceptron trained on a combination of Morgan fingerprints and Expert descriptors achieved the highest performance ($\text{R}^2$ = 0.8161, r = 0.9053), significantly exceeding AGILE ($\text{R}^2$ = 0.2655, r = 0.5488). We show that the models in LANTERN consistently have strong performance across multiple evaluation metrics. Thus, LANTERN offers a robust benchmarking framework for LNP transfection prediction and serves as a valuable tool for accelerating lipid-based RNA delivery systems design.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time prediction of plasma instabilities with sparse-grid-accelerated optimized dynamic mode decomposition</title>
<link>https://arxiv.org/abs/2507.03245</link>
<guid>https://arxiv.org/abs/2507.03245</guid>
<content:encoded><![CDATA[
<div> sparse grid interpolation, parametric reduced-order models, gyrokinetic simulations, fusion experiments, dynamic mode decomposition <br />
Summary: 
This paper explores the efficient training of parametric reduced-order models (ROMs) using sparse grid interpolation with (L)-Leja points for scenarios with high-dimensional input spaces. By focusing on gyrokinetic simulations of plasma micro-instabilities in fusion experiments, the study constructs parametric ROMs for the full 5D gyrokinetic distribution function using optimized dynamic mode decomposition and sparse grids based on (L)-Leja points. The research assesses the ROM prediction capabilities in different scenarios, including the Cyclone Base Case benchmark and a real-world micro-instability simulation with six input parameters. The results demonstrate that accurate parametric ROMs can be achieved at a low cost of high-fidelity simulations using sparse grids, showing the potential of these models in enabling complex many-query tasks in fusion research. <br /><br />Summary: <div>
arXiv:2507.03245v1 Announce Type: cross 
Abstract: Parametric data-driven reduced-order models (ROMs) that embed dependencies in a large number of input parameters are crucial for enabling many-query tasks in large-scale problems. These tasks, including design optimization, control, and uncertainty quantification, are essential for developing digital twins in real-world applications. However, standard training data generation methods are computationally prohibitive due to the curse of dimensionality, as their cost scales exponentially with the number of inputs.This paper investigates efficient training of parametric data-driven ROMs using sparse grid interpolation with (L)-Leja points, specifically targeting scenarios with higher-dimensional input parameter spaces. (L)-Leja points are nested and exhibit slow growth, resulting in sparse grids with low cardinality in low-to-medium dimensional settings, making them ideal for large-scale, computationally expensive problems. Focusing on gyrokinetic simulations of plasma micro-instabilities in fusion experiments as a representative real-world application, we construct parametric ROMs for the full 5D gyrokinetic distribution function via optimized dynamic mode decomposition (optDMD) and sparse grids based on (L)-Leja points. We perform detailed experiments in two scenarios: First, the Cyclone Base Case benchmark assesses optDMD ROM prediction capabilities beyond training time horizons and across variations in the binormal wave number. Second, for a real-world electron temperature gradient driven micro-instability simulation featuring six input parameters, we demonstrate that an accurate parametric optDMD ROM can be constructed at a cost of only $28$ high-fidelity gyrokinetic simulations thanks to sparse grids. In the broader context of fusion research, these results demonstrate the potential of sparse grid-based parametric ROMs to enable otherwise intractable many-query tasks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-robust multi-fidelity surrogate modelling for parametric partial differential equations</title>
<link>https://arxiv.org/abs/2507.03691</link>
<guid>https://arxiv.org/abs/2507.03691</guid>
<content:encoded><![CDATA[
<div> surrogate models, multi-fidelity, stochastic collocation, numerical noise, PDEs <br />
Summary:
The article addresses the challenge of constructing noise-robust surrogate models for quantities of interest (QoIs) from parametric partial differential equations (PDEs) using multi-fidelity collocation techniques, specifically the Multi-Index Stochastic Collocation (MISC). In scenarios where PDE evaluations are corrupted by numerical noise, an improved version of MISC is proposed to automatically detect and ignore noisy fidelities, mitigating overfitting and degradation of surrogate quality. The approach monitors the spectral decay of the surrogate at each iteration to identify noise onset and selectively halt the use of noisy fidelities. Numerical validation on parabolic advection-diffusion PDE and parametric turbulent incompressible Navier-Stokes problem demonstrates the accuracy and robustness of the resulting multi-fidelity surrogate even on under-resolved meshes unsuitable for single-fidelity computations. This method enhances the utility of multi-fidelity surrogates for downstream tasks like uncertainty quantification, optimization, and control. <br /><br /> <div>
arXiv:2507.03691v1 Announce Type: cross 
Abstract: We address the challenge of constructing noise-robust surrogate models for quantities of interest (QoIs) arising from parametric partial differential equations (PDEs), using multi-fidelity collocation techniques; specifically, the Multi-Index Stochastic Collocation (MISC). In practical scenarios, the PDE evaluations used to build a response surface are often corrupted by numerical noise, especially for the low-fidelity models. This noise, which may originate from loose solver tolerances, coarse discretisations, or transient effects, can lead to overfitting in MISC, degrading surrogate quality through nonphysical oscillations and loss of convergence, thereby limiting its utility in downstream tasks like uncertainty quantification, optimisation, and control. To correct this behaviour, we propose an improved version of MISC that can automatically detect the presence of solver noise during the surrogate model construction and then ignore the exhausted fidelities. Our approach monitors the spectral decay of the surrogate at each iteration, identifying stagnation in the coefficient spectrum that signals the onset of noise. Once detected, the algorithm selectively halts the use of noisy fidelities, focusing computational resources on those fidelities that still provide meaningful information. The effectiveness of this approach is numerically validated on two challenging test cases: a parabolic advection--diffusion PDE with uncertain coefficients, and a parametric turbulent incompressible Navier--Stokes problem. The results showcase the accuracy and robustness of the resulting multi-fidelity surrogate and its capability to extract relevant information, even from under-resolved meshes not suitable for reliable single-fidelity computations.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Willchain: Decentralized, Privacy-Preserving, Self-Executing, Digital Wills</title>
<link>https://arxiv.org/abs/2507.03694</link>
<guid>https://arxiv.org/abs/2507.03694</guid>
<content:encoded><![CDATA[
<div> Decentralized protocol, digital estate planning, cryptography, distributed computing, blockchain technology <br />
<br />
Summary: This work introduces a decentralized protocol for digital estate planning that leverages advanced distributed computing and cryptography. The protocol is implemented as a layer-1 solution using modern interchain communication to bridge different types of blockchains. It incorporates modern cryptographic primitives to support various forms of claims for information validation, ensuring enhanced privacy for digital inheritance. The protocol features heterogeneous smart contracts on multiple chains, facilitating secure asset distribution according to the decedent's wishes without the need to transfer funds. A user interaction model with a check-in system and account abstraction process improves flexibility and security. By utilizing a permissionless blockchain secured by validators and interchain relayers, the protocol revolutionizes digital estate planning, demonstrating the potential of blockchain technology in legal and personal spheres. The incorporation of a cryptoeconomic network enables unique incentive-compatible economic mechanisms in inheritance planning. <br /><br /> <div>
arXiv:2507.03694v1 Announce Type: cross 
Abstract: This work presents a novel decentralized protocol for digital estate planning that integrates advances distributed computing, and cryptography. The original proof-of-concept was constructed using purely solidity contracts. Since then, we have enhanced the implementation into a layer-1 protocol that uses modern interchain communication to connect several heterogeneous chain types. A key contribution of this research is the implementation of several modern cryptographic primitives to support various forms of claims for information validation. These primitives introduce an unmatched level of privacy to the process of digital inheritance. We also demonstrate on a set of heterogeneous smart contracts, following the same spec, on each chain to serve as entry points, gateways, or bridge contracts that are invoked via a path from the will module on our protocol, to the contract. This ensures a fair and secure distribution of digital assets in accordance with the wishes of the decedent without the requirement of moving their funds. This research further extends its innovations with a user interaction model, featuring a check-in system and account abstraction process, which enhances flexibility and user-friendliness without compromising on security. By developing a dedicated permissionless blockchain that is secured by a network of validators, and interchain relayers, the proposed protocol signifies a transformation in the digital estate planning industry and illustrates the potential of blockchain technology in revolutionizing traditional legal and personal spheres. Implementing a cryptoeconomic network at the core of inheritance planning allows for unique incentive compatible economic mechanisms to be constructed.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM</title>
<link>https://arxiv.org/abs/2507.03868</link>
<guid>https://arxiv.org/abs/2507.03868</guid>
<content:encoded><![CDATA[
<div> Query-style prototypes, Prompt Bank, Mixture-of-Expert Low-Rank Adaptation, Uni-Retrieval, Uni-RAG <br />
Summary: 
The article introduces Uni-Retrieval, a multi-modal retrieval module that dynamically matches query-style prototypes with tokens from a Prompt Bank, enhancing its capability to handle diverse educational scenarios. By incorporating a language model, Uni-Retrieval becomes Uni-RAG, which retrieves educational materials and generates human-readable content aligned with learning objectives. Experimental results indicate Uni-RAG's superior performance in retrieval accuracy and generation quality compared to baseline systems, all while being computationally efficient. The framework offers a scalable and pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to provide personalized, explainable, and efficient learning assistance across STEM scenarios. <br /> <br />Summary: <div>
arXiv:2507.03868v1 Announce Type: cross 
Abstract: In AI-facilitated teaching, leveraging various query styles to interpret abstract educational content is crucial for delivering effective and accessible learning experiences. However, existing retrieval systems predominantly focus on natural text-image matching and lack the capacity to address the diversity and ambiguity inherent in real-world educational scenarios. To address this limitation, we develop a lightweight and efficient multi-modal retrieval module, named Uni-Retrieval, which extracts query-style prototypes and dynamically matches them with tokens from a continually updated Prompt Bank. This Prompt Bank encodes and stores domain-specific knowledge by leveraging a Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to enhance Uni-Retrieval's capability to accommodate unseen query types at test time. To enable natural language educational content generation, we integrate the original Uni-Retrieval with a compact instruction-tuned language model, forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given a style-conditioned query, Uni-RAG first retrieves relevant educational materials and then generates human-readable explanations, feedback, or instructional content aligned with the learning objective. Experimental results on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline retrieval and RAG systems in both retrieval accuracy and generation quality, while maintaining low computational cost. Our framework provides a scalable, pedagogically grounded solution for intelligent educational systems, bridging retrieval and generation to support personalized, explainable, and efficient learning assistance across diverse STEM scenarios.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency through Evolution, A Darwinian Approach to Agent-Based Economic Forecast Modeling</title>
<link>https://arxiv.org/abs/2507.04074</link>
<guid>https://arxiv.org/abs/2507.04074</guid>
<content:encoded><![CDATA[
<div> methodology, macroeconomic forecasting, agent-based modeling, evolutionary principles, computational efficiency

Summary:
- The paper introduces a novel Darwinian Agent-Based Modeling (ABM) methodology for macroeconomic forecasting.
- It leverages evolutionary principles to achieve computational efficiency and emergent realism.
- The approach uses simple "common sense" rules representative of small firms serving final consumers.
- The methodology treats households as primary drivers of economic dynamics, with firms adapting through market-based natural selection.
- The model, constrained by Input-Output table structures, generates realistic economic patterns without extensive parameter calibration.
- Using FIGARO Input-Output tables for 46 countries, the model reproduces empirical regularities for Austria with minimal country-specific parameter calibration.
- Key findings include realistic firm and employment distributions, reproduction of initial Social Accounting Matrix values, successful calibration with a few parameters, and computational efficiency on standard laptops.
- Evolutionary ABM approaches offer robust policy insights by capturing decentralized market adaptations while avoiding the complexity of traditional DSGE and comprehensive ABM models.

<br /><br />Summary: <div>
arXiv:2507.04074v1 Announce Type: cross 
Abstract: This paper presents a novel Darwinian Agent-Based Modeling (ABM) methodology formacroeconomic forecasting that leverages evolutionary principles to achieve remarkablecomputational efficiency and emergent realism. Unlike conventional DSGE and ABM approachesthat rely on complex behavioral rules derived from large firm analysis, our framework employssimple "common sense" rules representative of small firms directly serving final consumers. Themethodology treats households as the primary drivers of economic dynamics, with firms adaptingthrough market-based natural selection within limited interaction neighborhoods. We demonstrate that this approach, when constrained by Input-Output table structures,generates realistic economic patterns including wealth distributions, firm size distributions, andsectoral employment patterns without extensive parameter calibration. Using FIGARO Input-Output tables for 46 countries and focusing on Austria as a case study, we show that the modelreproduces empirical regularities while maintaining computational efficiency on standard laptopsrather than requiring supercomputing clusters. Key findings include: (1) emergence of realistic firm and employment distributions fromminimal behavioral assumptions, (2) accurate reproduction of the initial Social Accounting Matrixvalues through evolutionary dynamics, (3) successful calibration using only 5-6 country-specificparameters to complement the FIGARO data, and (4) computational performance enabling fullsimulations on consumer hardware. These results suggest that evolutionary ABM approaches canprovide robust policy insights by capturing decentralized market adaptations while avoiding thecomputational complexity of traditional DSGE and comprehensive ABM models.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems</title>
<link>https://arxiv.org/abs/2507.04996</link>
<guid>https://arxiv.org/abs/2507.04996</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomy, autonomous vehicles, agentic AI, human-machine interaction, mobility ecosystems

Summary: 
This paper introduces the concept of agentic vehicles (AgVs) as vehicles integrated with agentic AI to interact and reason in complex environments. It highlights the gap between technical autonomy and the cognitive and social capabilities required for future mobility systems. The paper presents a framework to characterize AgVs, focusing on their cognitive and communicative layers, differentiating them from conventional autonomous vehicles (AuVs). It discusses the integration of agentic AI, robotics, and human-machine interaction in AgVs, emphasizing their role as interactive agents within mobility ecosystems. Key challenges in developing and governing AgVs are identified, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.<br /><br />Summary: <div>
arXiv:2507.04996v1 Announce Type: cross 
Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are defined as systems capable of perceiving their environment and executing preprogrammed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 1 to 6), such as interaction with humans and machines, goal adaptation, contextual reasoning, external tool use, and long-term planning, particularly with the integration of large language models (LLMs) and agentic AI systems. These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this, we introduce the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and interact within complex environments. This paper presents a systems-level framework to characterize AgVs, focusing on their cognitive and communicative layers and differentiating them from conventional AuVs. It synthesizes relevant advances in agentic AI, robotics, multi-agent systems, and human-machine interaction, and highlights how agentic AI, through high-level reasoning and tool use, can function not merely as computational tools but as interactive agents embedded in mobility ecosystems. The paper concludes by identifying key challenges in the development and governance of AgVs, including safety, real-time control, public acceptance, ethical alignment, and regulatory frameworks.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Tweet Posting Behavior on Citizen Security: A Hawkes Point Process Analysis</title>
<link>https://arxiv.org/abs/2402.03378</link>
<guid>https://arxiv.org/abs/2402.03378</guid>
<content:encoded><![CDATA[
<div> Keywords: Perception of Security, social network data, predictive insights, external factors, proactive security planning

Summary: 
This article presents a novel approach to measuring the Perception of Security (PoS) using social network data to provide real-time monitoring and predictive insights. By analyzing social network content related to security perceptions, the model incorporates external factors that influence the publication and reposting of such content. The results show that the proposed model achieves competitive predictive performance and offers a high level of interpretability regarding the factors influencing security perceptions. The research highlights the importance of understanding temporal patterns and external factors in anticipating security perceptions, providing valuable insights for proactive security planning. <div>
arXiv:2402.03378v2 Announce Type: replace-cross 
Abstract: The Perception of Security (PoS) refers to people's opinions about security or insecurity in a place or situation. While surveys have traditionally been the primary means to capture such perceptions, they need to be improved in their ability to offer real-time monitoring or predictive insights into future security perceptions. Recent evidence suggests that social network content can provide complementary insights into quantifying these perceptions. However, the challenge of accurately predicting these perceptions, with the capacity to anticipate them, still needs to be explored. This article introduces an innovative approach to PoS within short time frames using social network data. Our model incorporates external factors that influence the publication and reposting of content related to security perceptions. Our results demonstrate that this proposed model achieves competitive predictive performance and maintains a high degree of interpretability regarding the factors influencing security perceptions. This research contributes to understanding how temporal patterns and external factors impact the anticipation of security perceptions, providing valuable insights for proactive security planning.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative and parametric insurance on the Ethereum blockchain</title>
<link>https://arxiv.org/abs/2412.05321</link>
<guid>https://arxiv.org/abs/2412.05321</guid>
<content:encoded><![CDATA[
<div> Blockchain, insurance, parametric, collaborative, smart contract
Summary:
This paper presents a novel blockchain-based insurance scheme that combines parametric and collaborative elements. It involves a group of investors, called surplus providers, who lock funds in a smart contract to allow blockchain users to underwrite parametric insurance contracts. These contracts automatically trigger compensation upon meeting specific conditions. The collaborative aspect is reflected in the generation of tokens distributed to surplus providers, representing their share of surplus and granting voting rights for management decisions. The smart contract is coded in Solidity for the Ethereum blockchain and deployed on the Sepolia testnet. Data processing and analysis are done using Python. Open-source code is provided for transparency, and key research challenges are outlined for potential improvements. Overall, this paper sets the foundation for future research and development in blockchain-based insurance mechanisms.'<br /><br />Summary: <div>
arXiv:2412.05321v2 Announce Type: replace-cross 
Abstract: This paper introduces a blockchain-based insurance scheme that integrates parametric and collaborative elements. A pool of investors, referred to as surplus providers, locks funds in a smart contract, enabling blockchain users to underwrite parametric insurance contracts. These contracts automatically trigger compensation when predefined conditions are met. The collaborative aspect is embodied in the generation of tokens, which are distributed to surplus providers. These tokens represent each participant's share of the surplus and grant voting rights for management decisions. The smart contract is developed in Solidity, a high-level programming language for the Ethereum blockchain, and deployed on the Sepolia testnet, with data processing and analysis conducted using Python. In addition, open-source code is provided and main research challenges are identified, so that further research can be carried out to overcome limitations of this first proof of concept.
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener</title>
<link>https://arxiv.org/abs/2507.02005</link>
<guid>https://arxiv.org/abs/2507.02005</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Explainable AI, Fatigue Strength, Welded Steel Structures, Feature Engineering<br />
Summary:<br />
This research presents a unified approach that combines Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. By incorporating expert-driven feature engineering with algorithmic feature creation, the accuracy and explainability of the models are enhanced. The study utilizes regression models such as gradient boosting, random forests, and neural networks trained using AutoML under three feature schemes. Ensemble methods, including CatBoost and LightGBM, demonstrated top performance. The domain-informed model achieved the best balance of test RMSE and $R^2 values. XAI methods identified key predictors such as stress ratio, stress range, yield strength, and post-weld treatment. Secondary geometric factors also significantly influenced fatigue life. The integration of AutoML with XAI leads to accurate, interpretable, and robust fatigue strength models for welded steel structures, facilitating AI-assisted design and assessment. Future research will focus on probabilistic fatigue life modeling and integration into digital twin environments. <br />Summary: <div>
arXiv:2507.02005v1 Announce Type: new 
Abstract: This research introduces a unified approach combining Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. It integrates expert-driven feature engineering with algorithmic feature creation to enhance accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient boosting, random forests, and neural networks - were trained using AutoML under three feature schemes: domain-informed, algorithmic, and combined. This allowed a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE $\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta \sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527% within the engineering-relevant 0 - 150 MPa domain. The denser-feature model ($\mathcal M_3$) showed minor gains during training but poorer generalization, while the simpler base-feature model ($\mathcal M_1$) performed comparably, confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG dressing vs. as-welded) as dominant predictors. Secondary geometric factors - plate width, throat thickness, stiffener height - also significantly affected fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate, interpretable, and robust fatigue strength models for welded steel structures. It bridges data-driven modeling with engineering validation, enabling AI-assisted design and assessment. Future work will explore probabilistic fatigue life modeling and integration into digital twin environments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Resolution Independent Operator Learning</title>
<link>https://arxiv.org/abs/2507.02524</link>
<guid>https://arxiv.org/abs/2507.02524</guid>
<content:encoded><![CDATA[
<div> Keywords: time-dependent partial differential equations, recurrent DeepONet, Neural Controlled Differential Equation, transient mechanics, operator learning

Summary:
Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data is a challenging task. Existing methods like recurrent DeepONet extensions and neural-ODE surrogates have limitations in handling discrete-time and new inputs after initialization. To address these issues, NCDE-DeepONet is introduced, which combines a Neural Controlled Differential Equation (NCDE) with explicit space-time coordinates to create a continuous-time operator network. The NCDE encodes the entire load history as a solution of a controlled ODE driven by a spline-interpolated input path, allowing for input-resolution-independent representation. The trunk of the network can probe this latent path at arbitrary spatial locations and times, enabling output-resolution independence. Benchmarks on various transient mechanics problems demonstrate the robustness and accuracy of the framework, showcasing almost instant solution prediction. The approach of using controlled dynamics proves to be a principled and efficient method for high-fidelity operator learning in transient mechanics. 

<br /><br />Summary: <div>
arXiv:2507.02524v1 Announce Type: new 
Abstract: Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data remains a challenging task. Recurrent DeepONet extensions inherit the discrete-time limitations of sequence-to-sequence (seq2seq) RNN architectures, while neural-ODE surrogates cannot incorporate new inputs after initialization. We introduce NCDE-DeepONet, a continuous-time operator network that embeds a Neural Controlled Differential Equation (NCDE) in the branch and augments the trunk with explicit space-time coordinates. The NCDE encodes an entire load history as the solution of a controlled ODE driven by a spline-interpolated input path, making the representation input-resolution-independent: it encodes different input signal discretizations of the observed samples. The trunk then probes this latent path at arbitrary spatial locations and times, rendering the overall map output-resolution independent: predictions can be queried on meshes and time steps unseen during training without retraining or interpolation. Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems confirm the robustness and accuracy of the framework, achieving almost instant solution prediction. These findings suggest that controlled dynamics provide a principled and efficient foundation for high-fidelity operator learning in transient mechanics.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation and Heterogeneity Shape the Resilience of Community Currency Networks</title>
<link>https://arxiv.org/abs/2507.02678</link>
<guid>https://arxiv.org/abs/2507.02678</guid>
<content:encoded><![CDATA[
<div> Keywords: community currency, mutual credit systems, graph theory, behavioral connectivity, network evolution

Summary:
This paper examines community currency networks, focusing on the case study of Sardex in Sardinia, Italy. The analysis is done through a graph theoretic framework, studying strongly connected components, condensed representations, and behavioral connectivity patterns. The evolution of the network over three years is analyzed, revealing temporal contraction, flow imbalances, and structural fragmentation based on user types. The findings show deviations from degree-based models, indicating behavioral imitation among users and a preference for more active peers. The impact of heterogeneous connections between user types is also evaluated, highlighting their role in strengthening the network topology and enhancing resilience.<br /><br />Summary: <div>
arXiv:2507.02678v1 Announce Type: new 
Abstract: Community currency networks are made up of individuals and or companies that share some physical or social characteristics and engage in economic transactions using a virtual currency. This paper investigates the structural and dynamic properties of such mutual credit systems through a case study of Sardex, a community currency initiated and mainly operating in Sardinia, Italy. The transaction network is modeled as a directed weighted graph and analyzed through a graph theoretic framework focused on the analysis of strongly connected components, condensed representations, and behavioral connectivity patterns. Emphasis is placed on understanding the evolution of the network's core and peripheral structures over a three year period, with attention to temporal contraction, flow asymmetries, and structural fragmentation depending on different user types. Our findings reveal persistent deviations from degree based null models and suggest the presence of behavioral imitation, specifically, a user preference for more active peers. We further assess the impact of heterogeneous connections between different type of users, which strengthen the network topology and enhance its resilience.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery</title>
<link>https://arxiv.org/abs/2507.02730</link>
<guid>https://arxiv.org/abs/2507.02730</guid>
<content:encoded><![CDATA[
<div> Physics-Informed Automated Discovery of Kinetics, catalytic processes, kinetic models, symbolic regression, Metropolis-Hastings algorithm <br />
<br />
Summary: The article introduces the Physics-Informed Automated Discovery of Kinetics (PI-ADoK) framework for industrial catalytic processes. Traditional mechanistic models for kinetics demand expertise, while data-driven approaches lack interpretability. PI-ADoK integrates physical constraints into symbolic regression to reduce the search space and experiments needed for model convergence. It includes a Metropolis-Hastings algorithm for uncertainty quantification, providing credible prediction intervals. Benchmarking against conventional methods in catalytic case studies shows PI-ADoK enhances model fidelity and decreases the experimental burden, offering efficient and reliable kinetic model discovery for chemical reaction engineering. <div>
arXiv:2507.02730v1 Announce Type: new 
Abstract: The industrialization of catalytic processes hinges on the availability of reliable kinetic models for design, optimization, and control. Traditional mechanistic models demand extensive domain expertise, while many data-driven approaches often lack interpretability and fail to enforce physical consistency. To overcome these limitations, we propose the Physics-Informed Automated Discovery of Kinetics (PI-ADoK) framework. By integrating physical constraints directly into a symbolic regression approach, PI-ADoK narrows the search space and substantially reduces the number of experiments required for model convergence. Additionally, the framework incorporates a robust uncertainty quantification strategy via the Metropolis-Hastings algorithm, which propagates parameter uncertainty to yield credible prediction intervals. Benchmarking our method against conventional approaches across several catalytic case studies demonstrates that PI-ADoK not only enhances model fidelity but also lowers the experimental burden, highlighting its potential for efficient and reliable kinetic model discovery in chemical reaction engineering.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification</title>
<link>https://arxiv.org/abs/2507.01971</link>
<guid>https://arxiv.org/abs/2507.01971</guid>
<content:encoded><![CDATA[
<div> Keywords: support and resistance levels, deep learning, financial analysis, market microstructure, attention-based architecture

Summary:
DeepSupp is a new deep learning approach designed to detect financial support levels using multi-head attention mechanisms and advanced feature engineering. It leverages dynamic correlation matrices to capture evolving market relationships and uses an attention-based autoencoder for robust representation learning. The final support levels are identified through unsupervised clustering with DBSCAN, resulting in state-of-the-art performance across six financial metrics on S&amp;P 500 tickers. DeepSupp outperforms six baseline methods in essential support accuracy and market regime sensitivity, demonstrating consistent results across diverse market conditions. This approach fills critical gaps in support and resistance level detection, providing a scalable and reliable solution for modern financial analysis. The use of attention-based architectures in DeepSupp uncovers nuanced market patterns and enhances technical trading strategies. <br /><br />Summary: <div>
arXiv:2507.01971v1 Announce Type: cross 
Abstract: Support and resistance (SR) levels are central to technical analysis, guiding traders in entry, exit, and risk management. Despite widespread use, traditional SR identification methods often fail to adapt to the complexities of modern, volatile markets. Recent research has introduced machine learning techniques to address the following challenges, yet most focus on price prediction rather than structural level identification. This paper presents DeepSupp, a new deep learning approach for detecting financial support levels using multi-head attention mechanisms to analyze spatial correlations and market microstructure relationships. DeepSupp integrates advanced feature engineering, constructing dynamic correlation matrices that capture evolving market relationships, and employs an attention-based autoencoder for robust representation learning. The final support levels are extracted through unsupervised clustering, leveraging DBSCAN to identify significant price thresholds. Comprehensive evaluations on S&amp;P 500 tickers demonstrate that DeepSupp outperforms six baseline methods, achieving state-of-the-art performance across six financial metrics, including essential support accuracy and market regime sensitivity. With consistent results across diverse market conditions, DeepSupp addresses critical gaps in SR level detection, offering a scalable and reliable solution for modern financial analysis. Our approach highlights the potential of attention-based architectures to uncover nuanced market patterns and improve technical trading strategies.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design of Corrugated Boards: A New FEM Modeling and Experimental Validation</title>
<link>https://arxiv.org/abs/2507.02189</link>
<guid>https://arxiv.org/abs/2507.02189</guid>
<content:encoded><![CDATA[
<div> FEM modeling, corrugated boards, homogenization method, Weibull distributions, packaging design<br />
Summary:<br />
The study presents a simplified Finite Element Method (FEM) modeling approach for large structures made of corrugated boards, focusing on customized packages. It utilizes a homogenization method to transform flute geometries into equivalent elastic models, reducing computational time. Correction factors are introduced for internal mechanisms, adjusting the effective elastic modulus and thickness in the presence of large deformations and contact. Two statistical Weibull distributions representing contact and buckling mechanisms in corrugated boards are derived experimentally and validated for computational efficiency. The statistical parameters obtained ($\beta_1 = 0.14$, $\beta_2 = 1.31) can be effectively used for simplistic representation. The research contributes to optimizing corrugated packaging design by simplifying FEM models for faster yet accurate simulations. <br /><br />Summary: <div>
arXiv:2507.02189v1 Announce Type: cross 
Abstract: This study presents a simplified FEM modeling approach suitable for large structures made of corrugated boards, such as customized packages, based on a homogenization method, which is combined with correction factors for internal mechanisms. The homogenization process reduces computational time by transforming flute geometries into equivalent elastic models. In large deformations and in the presence of contact for a given geometry, the effective elastic modulus in the thickness direction, as well as the effective thickness of the structure, are corrected by two statistical Weibull distributions representing the contact and buckling mechanisms in a corrugated board. The Weibull parameters are obtained via experimental analysis, and such a process is then validated. The results demonstrate that the statistical parameters ($\beta_1 = 0.14$, $\beta_2 = 1.31$) can be used for the simplistic representation of corrugated boards, being computationally efficient. This research contributes to the optimization of corrugated packaging design, specifically by simplifying FEM models for faster yet equally accurate simulations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Scale Finite Element Method for Investigating Fiber Remodeling in Hypertrophic Cardiomyopathy</title>
<link>https://arxiv.org/abs/2507.02193</link>
<guid>https://arxiv.org/abs/2507.02193</guid>
<content:encoded><![CDATA[
<div> fiber disarray, hypertrophic cardiomyopathy, cellular abnormalities, cardiac pumping function, myocardium mechanics <br />
Summary: <br />
- Fiber disarray is a significant hallmark of hypertrophic cardiomyopathy (HCM) and is associated with various cardiac events such as heart failure.
- Heterogeneous distributions of hypercontractility, hypocontractility, and fibrosis contribute to the development of fiber disarray in the myocardium.
- The pattern of fiber disarray varies depending on the specific perturbation, providing insights into the progression of HCM.
- Higher fiber disarray near the epicardium compared to the endocardium suggests the role of regional myocardial mechanics in HCM development.
- Remodeled left ventricles (LVs) with fibrosis and hypocontractility exhibit declined cardiac performance, highlighting the structural and functional consequences of HCM. <br /> <div>
arXiv:2507.02193v1 Announce Type: cross 
Abstract: A significant hallmark of hypertrophic cardiomyopathy (HCM) is fiber disarray, which is associated with various cardiac events such as heart failure. Quantifying fiber disarray remains critical for understanding the disease s complex pathophysiology. This study investigates the role of heterogeneous HCM-induced cellular abnormalities in the development of fiber disarray and their subsequent impact on cardiac pumping function. Fiber disarray is predicted using a stress-based law to reorient myofibers and collagen within a multiscale finite element cardiac modeling framework, MyoFE. Specifically, the model is used to quantify the distinct impacts of heterogeneous distributions of hypercontractility, hypocontractility, and fibrosis on fiber disarray development and examines their effect on functional characteristics of the heart. Our results show that heterogenous cell level abnormalities highly disrupt the normal mechanics of myocardium and lead to significant fiber disarray. The pattern of disarray varies depending on the specific perturbation, offering valuable insights into the progression of HCM. Despite the random distribution of perturbed regions within the cardiac muscle, significantly higher fiber disarray is observed near the epicardium compared to the endocardium across all perturbed left ventricle (LV) models. This regional difference in fiber disarray, irrespective of perturbation severity, aligns with previous DT-MRI studies, highlighting the role of regional myocardial mechanics in the development of fiber disarray. Furthermore, cardiac performance declined in the remodeled LVs, particularly in those with fibrosis and hypocontractility. These findings provide important insights into the structural and functional consequences of HCM and offer a framework for future investigations into therapeutic interventions targeting cardiac remodeling.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Effective Elastic Modulus and Thickness of Corrugated Boards Using Gaussian Process Regression and Expected Hypervolume Improvement</title>
<link>https://arxiv.org/abs/2507.02208</link>
<guid>https://arxiv.org/abs/2507.02208</guid>
<content:encoded><![CDATA[
<div> Keywords: hypersurface modeling, effective elastic modulus, thickness, corrugated boards, Gaussian Process Regression 

Summary: 
Latin Hypercube Sampling (LHS) combined with Gaussian Process Regression (GP) and Enhanced Expected Hypervolume Improvement (EHVI) were used to model the hypersurface of the effective elastic modulus and thickness in corrugated boards. Accurate modeling of these properties is crucial for optimizing the mechanical properties of corrugated materials. LHS efficiently samples the input space, while GP adapts to the complexity of response surfaces by incorporating prediction and uncertainty. Points are generated and evaluated based on the complexity of hypersurfaces, with emphasis on points with higher variance. The performance evaluation showed that GP with EHVI had improved accuracy, indicated by lower Mean Squared Error (MSE) values for the effective elastic modulus and thickness predictions. This approach demonstrates potential for future applications in structural optimization. 

Summary: <div>
arXiv:2507.02208v1 Announce Type: cross 
Abstract: This work aims to model the hypersurface of the effective elastic modulus, \( E_{z, \text{eff}} \), and thickness, \( th_{\text{eff}} \), in corrugated boards. A Latin Hypercube Sampling (LHS) is followed by Gaussian Process Regression (GP), enhanced by EHVI as a multi-objective acquisition function. Accurate modeling of \( E_{z, \text{eff}} \) and \( th_{\text{eff}} \) is critical for optimizing the mechanical properties of corrugated materials in engineering applications. LHS provides an efficient and straightforward approach for an initial sampling of the input space; GP is expected to be able to adapt to the complexity of the response surfaces by incorporating both prediction and uncertainty. Therefore, the next points being generated and evaluated are based on the complexity of the hypersurfaces, and some points, especially those with higher variance, are more exploited and carry more importance. The performance of GP with EHVI is measured by Mean Squared Error (MSE). Prediction of GP resulted in \( \text{MSE}(E_{z, \text{eff}}) = 5.24 \, \text{kPa}^2 \) and \( \text{MSE}(th_{\text{eff}}) = 1 \, \text{mm}^2 \). GP possesses then improved accuracy and adaptability for future applications in structural optimization.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Particle Flow Filters with Taylor Expansion Series</title>
<link>https://arxiv.org/abs/2505.01597</link>
<guid>https://arxiv.org/abs/2505.01597</guid>
<content:encoded><![CDATA[
<div> Particle Flow Filters, Measurement Update, Drift Term, Diffusion Term, High-Order Polynomial Expansions<br />
<br />
Summary:<br />
Particle Flow Filters update measurements by moving particles rather than adjusting weights based on likelihood. This study introduces a new derivation method using high-order polynomial expansions, improving upon linearization techniques. The technique utilizes differential algebra to derive high-order particle flows directly onto polynomial representations of distributions. Two new particle flow filters are proposed, differing in the selection of the expansion center for Taylor polynomial evaluations. Numerical experiments demonstrate enhanced performance, particularly compared to Gromov flow and "exact" flow. <div>
arXiv:2505.01597v2 Announce Type: replace 
Abstract: Particle Flow Filters perform the measurement update by moving particles to a different location rather than modifying the particles' weight based on the likelihood. Their movement (flow) is dictated by a drift term, which continuously pushes the particle toward the posterior distribution, and a diffusion term, which guarantees the spread of particles. This work presents a novel derivation of these terms based on high-order polynomial expansions, where the common techniques based on linearization reduce to a simpler version of the new methodology. Thanks to differential algebra, the high-order particle flow is derived directly onto the polynomials representation of the distribution, embedded with differentiation and evaluation. The resulting technique proposes two new particle flow filters, whose difference relies on the selection of the expansion center for the Taylor polynomial evaluation. Numerical applications show the improvement gained by the inclusion of high-order terms, especially when comparing performance with the Gromov flow and the "exact" flow.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPC-AI Coupling Methodology for Scientific Applications</title>
<link>https://arxiv.org/abs/2507.01025</link>
<guid>https://arxiv.org/abs/2507.01025</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, High-performance computing, Coupling, Materials science, Scientific discovery

Summary: 
This study explores the integration of high-performance computing (HPC) and artificial intelligence (AI) in scientific applications, focusing on three coupling patterns: surrogate, directive, and coordinate. Through case studies in materials science, the effectiveness of these patterns is demonstrated, highlighting technical challenges, performance improvements, and implementation details. The proposed coupling patterns offer valuable guidance for future HPC-AI ensembles in various scientific domains, not limited to materials science. The study emphasizes the transformation of numerical-based HPC applications with data-driven AI approaches to address computational intensity challenges. This research provides insight into promising perspectives for HPC-AI coupling and its potential impact on scientific discovery. <div>
arXiv:2507.01025v1 Announce Type: new 
Abstract: Artificial intelligence (AI) technologies have fundamentally transformed numerical-based high-performance computing (HPC) applications with data-driven approaches and endeavored to address existing challenges, e.g. high computational intensity, in various scientific domains. In this study, we explore the scenarios of coupling HPC and AI (HPC-AI) in the context of emerging scientific applications, presenting a novel methodology that incorporates three patterns of coupling: surrogate, directive, and coordinate. Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite, and typical HPC-AI ensembles. Through case studies in materials science, we demonstrate the application and effectiveness of these patterns. The study highlights technical challenges, performance improvements, and implementation details, providing insight into promising perspectives of HPC-AI coupling. The proposed coupling patterns are applicable not only to materials science but also to other scientific domains, offering valuable guidance for future HPC-AI ensembles in scientific discovery.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI in Product Management: A Co-Evolutionary Model</title>
<link>https://arxiv.org/abs/2507.01069</link>
<guid>https://arxiv.org/abs/2507.01069</guid>
<content:encoded><![CDATA[
<div> agentic AI, product management, co-evolutionary framework, systems theory, human-AI interaction theory

Summary: 
This study delves into the transformative impact of agentic AI on product management, proposing a conceptual framework for its integration throughout the product lifecycle. Agentic AI, known for its autonomy and goal-driven behavior, reshapes the role of product managers (PMs) as orchestrators within socio-technical ecosystems. The framework, drawing on systems theory, co-evolutionary theory, and human-AI interaction theory, outlines the capabilities of agentic AI in various stages of product development. Through an integrative review of 70+ sources, the study showcases the evolving roles of PMs in overseeing AI operations, aligning strategies, and ensuring effective integration. A key finding emphasizes the need for PMs to enhance their skills in AI literacy, governance, and systems thinking to facilitate mutual adaptation between PMs and AI. This study lays the groundwork for future research and practical implementation to promote responsible and efficient integration of agentic AI in software organizations. 

<br /><br />Summary: <div>
arXiv:2507.01069v1 Announce Type: new 
Abstract: This study explores agentic AI's transformative role in product management, proposing a conceptual co-evolutionary framework to guide its integration across the product lifecycle. Agentic AI, characterized by autonomy, goal-driven behavior, and multi-agent collaboration, redefines product managers (PMs) as orchestrators of socio-technical ecosystems. Using systems theory, co-evolutionary theory, and human-AI interaction theory, the framework maps agentic AI capabilities in discovery, scoping, business case development, development, testing, and launch. An integrative review of 70+ sources, including case studies from leading tech firms, highlights PMs' evolving roles in AI orchestration, supervision, and strategic alignment. Findings emphasize mutual adaptation between PMs and AI, requiring skills in AI literacy, governance, and systems thinking. Addressing gaps in traditional frameworks, this study provides a foundation for future research and practical implementation to ensure responsible, effective agentic AI integration in software organizations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatially Distributed Wettability Characterization in Porous Media</title>
<link>https://arxiv.org/abs/2507.01617</link>
<guid>https://arxiv.org/abs/2507.01617</guid>
<content:encoded><![CDATA[
<div> contact angle measurement, micro-CT images, wettability heterogeneity, spatially distributed, open-source tools<br />
Summary:
An enhanced geometric algorithm for automated contact angle measurement from micro-CT images is introduced, offering improved accuracy through robust interface extrapolation. The method generates contact angle maps revealing wettability heterogeneity in mixed-wet systems. Analysis shows that averaged metrics can mask significant variability within samples; a seemingly uniformly weakly water-wet sample may exhibit substantial intermediate-wetting regions. This variation impacts pore-filling mechanisms and interface structure. The study's open-source tools enable precise spatial wettability characterization, enhancing predictions of multiphase flow behavior in porous materials. Such insights are crucial for optimizing subsurface energy processes. <div>
arXiv:2507.01617v1 Announce Type: new 
Abstract: An enhanced geometric algorithm for automated pore-by-pore contact angle measurement from micro-CT images, is presented that achieves superior accuracy compared to existing methods through robust fluid-fluid and solid-fluid interface extrapolation. Using this high resolution data, we generate spatially distributed contact angle maps that reveal previously hidden wettability heterogeneity. Our analysis of mixed-wet systems demonstrates the severe limitations of averaged metrics: a sample with a mean contact angle of 64.7 degrees, conventionally classified as uniformly weakly water-wet, exhibits 40% of its pore space in the intermediate-wetting regime (70-110 degrees). This heterogeneity explains the presence of minimal surface interfaces and fundamentally different pore-filling mechanisms operating within the same sample. By providing open-source tools for spatially-resolved wettability characterization, this work enables more accurate predictions of multiphase flow behavior in heterogeneous porous materials, essential for optimizing subsurface energy storage and recovery processes.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified Levenberg-Marquardt method for estimating the elastic material parameters of polymer waveguides using residuals between autocorrelated frequency responses</title>
<link>https://arxiv.org/abs/2507.01706</link>
<guid>https://arxiv.org/abs/2507.01706</guid>
<content:encoded><![CDATA[
<div> optimization, ultrasound, polymers, elasticity, material parameters
Summary:
This article addresses the estimation of frequency-dependent elastic parameters of polymers in the ultrasound range as an inverse problem. The approach involves fitting simulation signals to measurement signals of displacement responses in hollow cylindrical waveguides for efficiency. Two novel methods are proposed to accelerate optimization: an adapted Levenberg-Marquardt method and an improved objective function based on autocorrelated envelopes of signals. Realistic material parameter ranges are considered for reproducibility. The study focuses on isotropic materials, showing that the proposed methods reduce the total number of model evaluations, speeding up parameter identification. <div>
arXiv:2507.01706v1 Announce Type: new 
Abstract: In this contribution, we address the estimation of the frequency-dependent elastic parameters of polymers in the ultrasound range, which is formulated as an inverse problem. This inverse problem is implemented as a nonlinear regression-type optimization problem, in which the simulation signals are fitted to the measurement signals. These signals consist of displacement responses in waveguides, focusing on hollow cylindrical geometries to enhance the simulation efficiency. To accelerate the optimization and reduce the number of model evaluations and wait times, we propose two novel methods. First, we introduce an adaptation of the Levenberg-Marquardt method derived from a geometrical interpretation of the least-squares optimization problem. Second, we introduce an improved objective function based on the autocorrelated envelopes of the measurement and simulation signals. Given that this study primarily relies on simulation data to quantify optimization convergence, we aggregate the expected ranges of realistic material parameters and derive their distributions to ensure the reproducibility of optimizations with proper measurements. We demonstrate the effectiveness of our objective function modification and step adaptation for various materials with isotropic material symmetry by comparing them with a state-of-the-art optimization method. In all cases, our method reduces the total number of model evaluations, thereby shortening the time to identify the material parameters.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relevance of the Basset history term for Lagrangian particle dynamics</title>
<link>https://arxiv.org/abs/2407.01041</link>
<guid>https://arxiv.org/abs/2407.01041</guid>
<content:encoded><![CDATA[
<div> Keywords: Maxey-Riley equation, fluid dynamics, Lagrangian dynamics, clustering patterns, turbulent flow <br />
Summary: <br />
The study focuses on the impact of the integral "history term" in the Maxey-Riley equation (MRE) on the movement of finite spherical particles in fluid dynamics. Numerical computations were carried out to compare trajectories with and without the history term for a large number of particles in various flow fields. The findings reveal that neglecting the history term significantly affects clustering patterns, especially for moderate to large Stokes numbers. Additionally, the computation of finite-time Lyapunov exponents demonstrates that even for small particles, disregarding the history term leads to notable differences in the resulting scalar field, particularly in turbulent flows. This highlights the importance of considering the history term in the MRE for accurately predicting the Lagrangian dynamics of particles in fluid systems. <br /> <div>
arXiv:2407.01041v3 Announce Type: replace-cross 
Abstract: The movement of small but finite spherical particles in a fluid can be described by the Maxey-Riley equation (MRE) if they are too large to be considered passive tracers. The MRE contains an integral "history term" modeling wake effects, which causes the force acting on a particle at some given time to depend on its full past trajectory. The history term causes complications in the numerical solution of the MRE and is therefore often neglected, despite both numerical and experimental evidence that its effects are generally not negligible. By numerically computing trajectories with and without the history term of a large number of particles in different flow fields, we investigate its impact on the large-scale Lagrangian dynamics of simulated particles. We show that for moderate to large Stokes numbers, ignoring the history term leads to significant differences in clustering patterns. Furthermore, we compute finite-time Lyapunov exponents and show that, even for small particles, the differences in the resulting scalar field from ignoring the BHT can be significant, in particular if the underlying flow is turbulent.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-design of magnetic soft robots with large deformation and contacts via material point method and topology optimization</title>
<link>https://arxiv.org/abs/2503.22767</link>
<guid>https://arxiv.org/abs/2503.22767</guid>
<content:encoded><![CDATA[
<div> soft robots, magnetic particles, topology optimization, magneto-elastic dynamics, autonomous design

Summary:
The article introduces a topology optimization framework for magnetic soft robots embedded with hard magnetic particles. This framework simultaneously designs structures, material magnetization, and time-varying magnetic stimuli to achieve target behaviors such as shape morphing and locomotion. It integrates generalized topology optimization with the magneto-elastic material point method, allowing for dynamic motion and solid contacts. The framework is computationally efficient, completing all design cases within minutes. It enables the autonomous co-design of active soft materials for various tasks, including metasurfaces, drug delivery, and minimally invasive procedures. <div>
arXiv:2503.22767v2 Announce Type: replace-cross 
Abstract: Magnetic soft robots embedded with hard magnetic particles enable untethered actuation via external magnetic fields, offering remote, rapid, and precise control, which is highly promising for biomedical applications. However, designing such systems is challenging due to the complex interplay of magneto-elastic dynamics, large deformation, solid contacts, time-varying stimuli, and posture-dependent loading. As a result, most existing research relies on heuristics and trial-and-error methods or focuses on the independent design of stimuli or structures under static conditions. We propose a topology optimization framework for magnetic soft robots that simultaneously designs structures, location-specific material magnetization and time-varying magnetic stimuli, accounting for large deformations, dynamic motion, and solid contacts. This is achieved by integrating generalized topology optimization with the magneto-elastic material point method, which supports GPU-accelerated parallel simulations and auto-differentiation for sensitivity analysis. We applied this framework to design magnetic robots for various tasks, including multi-task shape morphing and locomotion, in both 2D and 3D. The method autonomously generates optimized robotic systems to achieve target behaviors without requiring human intervention. Despite the nonlinear physics and large design space, it demonstrates high computational efficiency, completing all cases within minutes. The framework provides a computational foundation for the autonomous co-design of active soft materials in applications such as metasurfaces, drug delivery, and minimally invasive procedures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics</title>
<link>https://arxiv.org/abs/2507.00539</link>
<guid>https://arxiv.org/abs/2507.00539</guid>
<content:encoded><![CDATA[
<div> Ensemble Kalman Filter, Reduced-Order Model, Data Assimilation, Fluid Dynamics, Computational Efficiency <br />
<br />
Summary: 
This paper introduces an innovative Reduced-Order Model (ROM) that merges experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of fluid dynamics systems. The methodology incorporates the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework to improve prediction accuracy. To address the computational demands, the ROM employs low-resolution techniques, such as downsampling datasets and utilizing low-cost Singular Value Decomposition (lcSVD) for advanced reconstruction. Results show significant reductions in computation time and RAM usage without sacrificing accuracy. The EnKF is effective in estimating and predicting fluid flow systems based on limited observations and low-fidelity data. The proposed DA method shows promise in improving computational efficiency in CFD and related fields, making it suitable for large-scale and real-time applications like environmental monitoring and aerospace. <div>
arXiv:2507.00539v1 Announce Type: new 
Abstract: This paper presents an innovative Reduced-Order Model (ROM) for merging experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of a fluid dynamics system, leading to more accurate predictions. Our methodology introduces a novel approach implementing the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework, grounded in a robust theoretical foundation and applied to fluid dynamics. To address the substantial computational demands of DA, the proposed ROM employs low-resolution (LR) techniques to drastically reduce computational costs. This approach involves downsampling datasets for DA computations, followed by an advanced reconstruction technique based on low-cost Singular Value Decomposition (lcSVD). The lcSVD method, a key innovation in this paper, has never been applied to DA before and offers a highly efficient way to enhance resolution with minimal computational resources. Our results demonstrate significant reductions in both computation time and RAM usage through the LR techniques without compromising the accuracy of the estimations. For instance, in a turbulent test case, the LR approach with a compression rate of 15.9 can achieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a low Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the high-resolution (HR) reference. Furthermore, we highlight the effectiveness of the EnKF in estimating and predicting the state of fluid flow systems based on limited observations and low-fidelity numerical data. This paper highlights the potential of the proposed DA method in fluid dynamics applications, particularly for improving computational efficiency in CFD and related fields. Its ability to balance accuracy with low computational and memory costs makes it suitable for large-scale and real-time applications, such as environmental monitoring or aerospace.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark</title>
<link>https://arxiv.org/abs/2507.00034</link>
<guid>https://arxiv.org/abs/2507.00034</guid>
<content:encoded><![CDATA[
<div> neural network, critical heat flux, non-uniform heating, spatial power profiles, transfer-learning

Summary:
- The study compiles a comprehensive dataset on critical heat flux (CHF) under both uniform and non-uniform axial heating conditions to support Phase II of the OECD/NEA AI/ML CHF benchmark.
- Classical CHF correlations show errors under uniform heating and struggle with non-uniform profiles, while modern tabular methods offer improved but imperfect predictions.
- A neural network trained on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, emphasizing the need for models that consider axial power distributions.
- The curated datasets and modeling results provided in this study set the stage for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark. <br /><br />Summary: <div>
arXiv:2507.00034v1 Announce Type: cross 
Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. To support Phase II of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power profiles, this work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating and degrade markedly when applied to non-uniform profiles, while modern tabular methods offer improved but still imperfect predictions. A neural network trained solely on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, underscoring the need for models that explicitly incorporate axial power distributions. By providing these curated datasets and baseline modeling results, this study lays the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process</title>
<link>https://arxiv.org/abs/2507.00046</link>
<guid>https://arxiv.org/abs/2507.00046</guid>
<content:encoded><![CDATA[
<div> evolutionary computing, image segmentation, Additive Friction Stir Deposition, Particle Swarm Optimization, defect detection

Summary:
Evolutionary computing-based image segmentation using Particle Swarm Optimization (PSO) was proposed for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. The methodology integrates gradient magnitude analysis with distance transforms to create attention-weighted visualizations highlighting critical interface regions. Multiple visualization techniques, including self-attention maps and multi-channel visualization, were applied to five AFSD samples to detect material transitions and potential defects. PSO algorithm determined optimal threshold values for precise segmentation. The multi-channel visualization technique combined boundary information, spatial relationships, and material density data for cohesive representations. Attention-based analysis successfully identified incomplete bonding regions and inhomogeneities in AFSD joints, offering quantitative metrics for process optimization and quality assessment of additively manufactured components. <br /><br />Summary: <div>
arXiv:2507.00046v1 Announce Type: cross 
Abstract: This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A collaborative digital twin built on FAIR data and compute infrastructure</title>
<link>https://arxiv.org/abs/2507.00048</link>
<guid>https://arxiv.org/abs/2507.00048</guid>
<content:encoded><![CDATA[
<div> machine learning, automated experimentation, self-driving laboratories, FAIR data infrastructure, optimization

Summary:
The article discusses the integration of machine learning and automated experimentation in self-driving laboratories (SDL) to accelerate discovery and optimization tasks in science and engineering. By utilizing findable, accessible, interoperable, and reusable (FAIR) data infrastructure, geographically dispersed researchers can collaborate effectively within a distributed SDL implementation on nanoHUB services. The framework allows for the sharing of raw experimental data in a central database, enabling researchers to benefit from analysis tools and machine learning models that update automatically with new data. A separate workflow on nanoHUB facilitates sequential optimization through active learning, where researchers define the optimization objective and machine learning models guide future experiment selection. The article presents the application of these concepts in an optimization task involving food dyes, showing how researchers and students can conduct experiments, share data, and explore the combination of FAIR data, predictive ML models, and sequential optimization in a cost-effective manner. <div>
arXiv:2507.00048v1 Announce Type: cross 
Abstract: The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively. This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management. In this framework, geographically dispersed collaborators conducting independent optimization tasks contribute raw experimental data to a shared central database. These researchers can then benefit from analysis tools and machine learning models that automatically update as additional data become available. New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments. Inspired by the concept of ``frugal twin", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color. With easily accessible and inexpensive materials, researchers and students can set up their own experiments, share data with collaborators, and explore the combination of FAIR data, predictive ML models, and sequential optimization. The tools introduced are generally applicable and can easily be extended to other optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The gradual transformation of inland countries -- human plowing, horse plowing and equity incentives</title>
<link>https://arxiv.org/abs/2507.00067</link>
<guid>https://arxiv.org/abs/2507.00067</guid>
<content:encoded><![CDATA[
<div> civilization, history, governance, conflict, economic development
<br />Summary:
In the article, the author emphasizes the importance of learning from history to upgrade civilization and improve its strength and survival ability. By studying the long-term stability of countries in conflict, including economic benefits and means of suppression, and utilizing mathematical methods to find optimal solutions, it is suggested that civilizations can enhance their ability to handle conflicts and reduce internal strife. The transition from human plowing to horse plowing is mentioned as a means to suppress resistance and provide resistance ability to the people. The selection of rulers should consider various institutional aspects like exams, elections, and drawing lots. Economic development, following a lognormal distribution, can be adjusted using expected value and variance to address wealth inequality. <div>
arXiv:2507.00067v1 Announce Type: cross 
Abstract: Many modern countries have not learned their lessons and often hope for the wisdom of later generations, resulting in them only possessing modern technology and difficult to iterate ancient civilizations. At present, there is no way to tell how we should learn from history and promote the gradual upgrading of civilization. Therefore, we must tell the history of civilization's progress and the means of governance, learn from experience to improve the comprehensive strength and survival ability of civilization, and achieve an optimal solution for the tempering brought by conflicts and the reduction of internal conflicts. Firstly, we must follow the footsteps of history and explore the reasons for the long-term stability of each country in conflict, including providing economic benefits to the people and means of suppressing them; then, use mathematical methods to demonstrate how we can achieve the optimal solution at the current stage. After analysis, we can conclude that the civilization transformed from human plowing to horse plowing can easily suppress the resistance of the people and provide them with the ability to resist; The selection of rulers should consider multiple institutional aspects, such as exams, elections, and drawing lots; Economic development follows a lognormal distribution and can be adjusted by expected value and variance. Using a lognormal distribution with the maximum value to divide equity can adjust the wealth gap.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPGD: Steepest Perturbed Gradient Descent Optimization</title>
<link>https://arxiv.org/abs/2411.04946</link>
<guid>https://arxiv.org/abs/2411.04946</guid>
<content:encoded><![CDATA[
<div> algorithm, optimization, gradient descent, perturbation sampling, local minima<br />
<br />
Summary:
The paper introduces the Steepest Perturbed Gradient Descent (SPGD) algorithm, which combines gradient descent with periodic uniform perturbation sampling to overcome challenges like local minima, saddle points, and plateaus in optimization problems. SPGD generates candidate solutions and selects the one with the steepest loss difference, incorporating exploration to escape sub-optimal minima. It outperforms four established methods in solving the NP-hard 3D component packing problem and shows improvement in complex non-convex optimization problems. Comparative analyses with 2D benchmark functions demonstrate SPGD's superior performance in navigating intricate optimization landscapes. The algorithm's ability to effectively search for global optima across diverse problem spaces highlights its versatility in addressing various optimization challenges.<br /> 
<br />Summary: <div>
arXiv:2411.04946v2 Announce Type: replace-cross 
Abstract: Optimization algorithms are pivotal in advancing various scientific and industrial fields but often encounter obstacles such as trapping in local minima, saddle points, and plateaus (flat regions), which makes the convergence to reasonable or near-optimal solutions particularly challenging. This paper presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that innovatively combines the principles of the gradient descent method with periodic uniform perturbation sampling to effectively circumvent these impediments and lead to better solutions whenever possible. SPGD is distinctively designed to generate a set of candidate solutions and select the one exhibiting the steepest loss difference relative to the current solution. It enhances the traditional gradient descent approach by integrating a strategic exploration mechanism that significantly increases the likelihood of escaping sub-optimal local minima and navigating complex optimization landscapes effectively. Our approach not only retains the directed efficiency of gradient descent but also leverages the exploratory benefits of stochastic perturbations, thus enabling a more comprehensive search for global optima across diverse problem spaces. We demonstrate the efficacy of SPGD in solving the 3D component packing problem, an NP-hard challenge. Preliminary results show a substantial improvement over four established methods, particularly on response surfaces with complex topographies and in multidimensional non-convex continuous optimization problems. Comparative analyses with established 2D benchmark functions highlight SPGD's superior performance, showcasing its ability to navigate complex optimization landscapes. These results emphasize SPGD's potential as a versatile tool for a wide range of optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STONet: A neural operator for modeling solute transport in micro-cracked reservoirs</title>
<link>https://arxiv.org/abs/2412.05576</link>
<guid>https://arxiv.org/abs/2412.05576</guid>
<content:encoded><![CDATA[
<div> Neural Operator, Solute Transport Operator Network, Contaminant Transport, Micro-Cracked Porous Media, Deep Learning<br />
Summary: <br />
In this work, the Solute Transport Operator Network (STONet) is introduced as a novel neural operator to model contaminant transport in micro-cracked porous media efficiently. The model architecture of STONet combines a DeepONet structure with a transformer-based multi-head attention mechanism to enhance performance without additional computational overhead. Training data obtained from finite element simulations captures diverse scenarios, enabling accurate predictions of concentration field changes. STONet achieves high accuracy with relative errors below 1% compared to FEM simulations and reduces runtime significantly. This computational efficiency enables the development of digital twins for rapid assessment of subsurface contamination risks and optimization of environmental remediation strategies. Data and code for the paper will be available on GitHub for further research and application. <br /> <div>
arXiv:2412.05576v2 Announce Type: replace-cross 
Abstract: In this work, we introduce a novel neural operator, the Solute Transport Operator Network (STONet), to efficiently model contaminant transport in micro-cracked porous media. STONet's model architecture is specifically designed for this problem and uniquely integrates an enriched DeepONet structure with a transformer-based multi-head attention mechanism, enhancing performance without incurring additional computational overhead compared to existing neural operators. The model combines different networks to encode heterogeneous properties effectively and predict the rate of change of the concentration field to accurately model the transport process. The training data is obtained using finite element (FEM) simulations by random sampling of micro-fracture distributions and applied pressure boundary conditions, which capture diverse scenarios of fracture densities, orientations, apertures, lengths, and balance of pressure-driven to density-driven flow. Our numerical experiments demonstrate that, once trained, STONet achieves accurate predictions, with relative errors typically below 1% compared with FEM simulations while reducing runtime by approximately two orders of magnitude. This type of computational efficiency facilitates building digital twins for rapid assessment of subsurface contamination risks and optimization of environmental remediation strategies. The data and code for the paper will be published at https://github.com/ehsanhaghighat/STONet.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks</title>
<link>https://arxiv.org/abs/2503.08904</link>
<guid>https://arxiv.org/abs/2503.08904</guid>
<content:encoded><![CDATA[
<div> data-driven methods, state reconstruction, nuclear reactors, Shallow Recurrent Decoder, Generation-IV reactors

Summary:
The article discusses the application of data-driven methods for accurate state reconstruction in nuclear reactors, focusing on the challenging environment of Generation-IV reactors like the Molten Salt Fast Reactor (MSFR). By leveraging the Shallow Recurrent Decoder architecture, the study efficiently estimates the reactor's state vector, including neutron fluxes, precursors concentrations, and thermal parameters, using only three out-of-core time-series neutron flux measurements. The proposed approach extends the architecture to handle parametric time-series data, enabling robust state estimation under various operating conditions. Additionally, the methodology allows for quantifying uncertainty in state estimation with low training costs. The study's successful results highlight its potential for real-time monitoring, control, and the development of a reactor digital twin. <div>
arXiv:2503.08904v2 Announce Type: replace-cross 
Abstract: The recent developments in data-driven methods have paved the way to new methodologies to provide accurate state reconstruction of engineering systems; nuclear reactors represent particularly challenging applications for this task due to the complexity of the strongly coupled physics involved and the extremely harsh and hostile environments, especially for new technologies such as Generation-IV reactors. Data-driven techniques can combine different sources of information, including computational proxy models and local noisy measurements on the system, to robustly estimate the state. This work leverages the novel Shallow Recurrent Decoder architecture to infer the entire state vector (including neutron fluxes, precursors concentrations, temperature, pressure and velocity) of a reactor from three out-of-core time-series neutron flux measurements alone. In particular, this work extends the standard architecture to treat parametric time-series data, ensuring the possibility of investigating different accidental scenarios and showing the capabilities of this approach to provide an accurate state estimation in various operating conditions. This paper considers as a test case the Molten Salt Fast Reactor (MSFR), a Generation-IV reactor concept, characterised by strong coupling between the neutronics and the thermal hydraulics due to the liquid nature of the fuel. The promising results of this work are further strengthened by the possibility of quantifying the uncertainty associated with the state estimation, due to the considerably low training cost. The accurate reconstruction of every characteristic field in real-time makes this approach suitable for monitoring and control purposes in the framework of a reactor digital twin.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</title>
<link>https://arxiv.org/abs/2503.21248</link>
<guid>https://arxiv.org/abs/2503.21248</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, scientific research, benchmark, hypothesis generation, automated discovery

Summary:
Large language models (LLMs) are being evaluated for their potential in aiding scientific research through a new benchmark that focuses on inspiration retrieval, hypothesis composition, and ranking. An automated framework extracts key components from scientific papers across various disciplines with expert validation to ensure accuracy. By exclusively analyzing papers from 2024, the benchmark avoids data contamination. LLMs show promise in retrieving inspirations, indicating their ability to uncover novel knowledge associations. This positions LLMs as valuable tools for automated scientific discovery, capable of generating innovative hypotheses at scale with minimal human input. <div>
arXiv:2503.21248v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analogical Learning for Cross-Scenario Generalization: Framework and Application to Intelligent Localization</title>
<link>https://arxiv.org/abs/2504.08811</link>
<guid>https://arxiv.org/abs/2504.08811</guid>
<content:encoded><![CDATA[
<div> learning models, generalization, deep learning framework, analogical learning, intelligent wireless localization

Summary:
The article introduces an analogical learning (AL) framework, specifically a bipartite neural network called Mateformer, for multi-scenario learning in intelligent wireless localization. The AL approach leverages the understanding that data from different scenarios follow common underlying physical rules despite having distinct reference frames. The Mateformer network captures relativity within multiple latent feature spaces and uses this relativity to guide nonlinear analogy for accurate predictions. Experimental results show that AL outperforms existing models in accuracy in single-scenario benchmarks, exhibits stable transferability between scenarios without catastrophic forgetting, and robustly adapts to new, unseen scenarios such as dynamic weather and traffic conditions without any fine-tuning. The data and code for the framework are also made available for further research and implementation. <div>
arXiv:2504.08811v2 Announce Type: replace-cross 
Abstract: Existing learning models often exhibit poor generalization when deployed across diverse scenarios. It is primarily due to that the underlying reference frame of the data varies with the deployment environment and settings. However, despite that data of each scenario has a distinct reference frame, its generation generally follows common underlying physical rules. Based on this understanding, this article proposes a deep learning framework named analogical learning (AL), which implicitly retrieves the reference frame information associated with a scenario and then to make accurate prediction by relative analogy with other scenarios. Specifically, we design a bipartite neural network called Mateformer. Its first part captures the relativity within multiple latent feature spaces between the input data and a small amount of embedded data from the studied scenario, while its second part uses this relativity to guide the nonlinear analogy. We apply AL to the typical multi-scenario learning problem of intelligent wireless localization in cellular networks. Extensive experiments validate AL's superiority across three key dimensions. First, it achieves state-of-the-art accuracy in single-scenario benchmarks. Second, it demonstrates stable transferability between different scenarios, avoiding catastrophic forgetting. Finally, and most importantly, it robustly adapts to new, unseen scenarios--including dynamic weather and traffic conditions--without any tuning. All data and code are available at https://github.com/ziruichen-research/ALLoc.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Integrated Electrostatic Boundary Element Simulations with Non-Conforming Higher-Order Meshes</title>
<link>https://arxiv.org/abs/2506.22676</link>
<guid>https://arxiv.org/abs/2506.22676</guid>
<content:encoded><![CDATA[
<div> CAD plugin, design analysis, virtual prototyping, electric devices, boundary element method
Summary:
The article introduces a design through analysis workflow for virtual prototyping of electric devices. A CAD plugin facilitates the interaction between design and analysis, enabling the creation of analysis models and visualization of results within the design environment. Simulations employ a fast boundary element method (BEM) capable of handling non-conforming and higher-order meshes. Numerical experiments are conducted to assess the accuracy of the approach and its sensitivity to the initial CAD representation. The workflow facilitates a close link between design and analysis, with the non-conforming higher-order BEM technique producing precise results while simplifying the interaction between the two processes. <div>
arXiv:2506.22676v1 Announce Type: new 
Abstract: We present a design through analysis workflow that enables virtual prototyping of electric devices. A CAD plugin establishes the interaction between design and analysis, allowing the preparation of analysis models and the visualization of its results within the design environment. The simulations utilize a fast boundary element method (BEM) that allows for non-conforming and higher-order meshes. Our numerical experiments investigate the accuracy of the approach and its sensitivity to the initial CAD representation. Overall, the workflow enables a close link between design and analysis, where the non-conforming higher-order BEM approach provides accurate results and significantly simplifies the interaction.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved design of an active landing gear for a passenger aircraft using multi-objective optimization technique</title>
<link>https://arxiv.org/abs/2506.22870</link>
<guid>https://arxiv.org/abs/2506.22870</guid>
<content:encoded><![CDATA[
<div> Controller coefficients, hydraulic actuator parameters, vibration absorber, landing gear system, optimization algorithm<br />
<br />
Summary:<br />
This study focuses on optimizing the landing gear system of aircraft using a bee-inspired multi-objective algorithm. The research addresses the need for better performance under varying landing and runway conditions by optimizing controller coefficients, hydraulic actuator parameters, and vibration absorber simultaneously. Sensitivity analysis for three-point landings and robustness analysis for emergency wind conditions are conducted. By applying the active shock absorber system, optimized through bee-based algorithms, improvements are seen in reducing bounce and pitch displacements, suspension travel, and impact force in both time and frequency domains. The results show enhanced passenger comfort and potentially extended structural fatigue life, showcasing practical industrial application. <div>
arXiv:2506.22870v1 Announce Type: new 
Abstract: The landing gear system is a major aircraft subsystem that must withstand extreme forces during ground maneuvers and absorb vibrations. While traditional systems perform well under normal conditions, their efficiency drops under varying landing and runway scenarios. This study addresses this issue by simultaneously optimizing controller coefficients, parameters of a nonlinear hydraulic actuator integrated into the traditional shock absorber, and a vibration absorber using a bee-inspired multi-objective algorithm. To demonstrate adaptability, the paper includes sensitivity analysis for three-point landings affected by added payload and touchdown speed, and robustness analysis for one- and two-point landings under emergency wind conditions. The dynamic flight equations of an Airbus A320-200 during landing are derived and solved numerically. Results show that the active shock absorber system, optimized via two bee-based algorithms, outperforms the passive system in reducing bounce and pitch displacements and momenta, suspension travel, and impact force in both time and frequency domains. This leads to significantly improved passenger comfort and potentially longer structural fatigue life, demonstrating industrial applicability.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feasibility of spectral-element modeling of wave propagation through the anatomy of marine mammals</title>
<link>https://arxiv.org/abs/2506.22944</link>
<guid>https://arxiv.org/abs/2506.22944</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D spectral-element method, ultrasonic wave propagation, bottlenose dolphin, marine mammal bioacoustics, environmental challenges <br />
Summary: <br />
This study presents a novel 3D spectral-element method (SEM) simulation of ultrasonic wave propagation in a bottlenose dolphin's head, addressing limitations of traditional finite-element methods (FEM). The SEM approach offers exponential convergence and efficient parallel computation, enabling high-frequency simulations with complex anatomical features accurately represented in the mesh. Utilizing Computed Tomography (CT) scan data, detailed simulations of plane and spherical waves validate the efficacy of SEM for ultrasonic time-domain modeling in marine mammal bioacoustics research. This advancement in computational modeling opens up opportunities in studying dolphin echolocation, the effects of anthropogenic marine noise pollution, and the biophysics of hearing and click generation. By overcoming FEM's challenges, SEM emerges as a powerful tool to investigate hypotheses related to dolphin bioacoustics, with implications for conservation efforts and understanding marine mammal auditory systems in the face of increasing environmental challenges. <div>
arXiv:2506.22944v1 Announce Type: new 
Abstract: This study introduces the first 3D spectral-element method (SEM) simulation of ultrasonic wave propagation in a bottlenose dolphin (Tursiops truncatus) head. Unlike traditional finite-element methods (FEM), which struggle with high-frequency simulations due to costly linear-system inversions and slower convergence, SEM offers exponential convergence and efficient parallel computation. Using Computed Tomography (CT) scan data, we developed a detailed hexahedral mesh capturing complex anatomical features, such as acoustic fats and jaws. Our simulations of plane and spherical waves confirm SEM's effectiveness for ultrasonic time-domain modeling. This approach opens new avenues for marine biology, contributing to research in echolocation, the impacts of anthropogenic marine noise pollution and the biophysics of hearing and click generation in marine mammals. By overcoming FEM's limitations, SEM provides a powerful scalable tool to test hypotheses about dolphin bioacoustics, with significant implications for conservation and understanding marine mammal auditory systems under increasing environmental challenges.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparStencil: Retargeting Sparse Tensor Cores to Scientific Stencil Computations via Structured Sparsity Transformation</title>
<link>https://arxiv.org/abs/2506.22969</link>
<guid>https://arxiv.org/abs/2506.22969</guid>
<content:encoded><![CDATA[
<div> Sparse Tensor Core, scientific stencil computations, SparStencil, structured sparsity, adaptive layout morphing.<br />
<br />
Summary: <br />
Sparse Tensor Cores (TCUs) are efficient for AI workloads with 2:4 sparsity, but not utilized for scientific stencil computations. SparStencil transforms sparse TCUs for stencil computations by restructuring patterns into sparse matrices, ensuring compatibility with 2:4 sparsity constraints. It includes Adaptive Layout Morphing to align patterns, Structured Sparsity Conversion for graph matching, and Automatic Kernel Generation for optimized kernels. Evaluated on 79 kernels, SparStencil achieves up to 7.1x speedup, reduces complexity, and matches expert-tuned performance in compute throughput and memory efficiency. <div>
arXiv:2506.22969v1 Announce Type: new 
Abstract: Sparse Tensor Cores offer exceptional performance gains for AI workloads by exploiting structured 2:4 sparsity. However, their potential remains untapped for core scientific workloads such as stencil computations, which exhibit irregular sparsity patterns.This paper presents SparStencil, the first system to retarget sparse TCUs for scientific stencil computations through structured sparsity transformation. SparStencil introduces three key techniques: (1) Adaptive Layout Morphing, which restructures stencil patterns into staircase-aligned sparse matrices via a flatten-and-crush pipeline; (2) Structured Sparsity Conversion, which formulates transformation as a graph matching problem to ensure compatibility with 2:4 sparsity constraints; (3) Automatic Kernel Generation, which compiles transformed stencils into optimized sparse MMA kernels via layout search and table-driven memory mapping. Evaluated on 79 stencil kernels spanning diverse scientific domains, SparStencil achieves up to 7.1x speedup (3.1x on average) over state-of-the-art framework while reducing code complexity and matching or exceeding expert-tuned performance in both compute throughput and memory efficiency.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a better approach to the Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2506.23028</link>
<guid>https://arxiv.org/abs/2506.23028</guid>
<content:encoded><![CDATA[
<div> Keywords: Vehicle Routing Problem, Logistics, Combinatorial Optimization, Constraints, Extensions

Summary:<br /><br />
The Vehicle Routing Problem (VRP) is a critical issue in logistics management, impacting transportation efficiency, cost reduction, and service quality. As a combinatorial optimization problem, it is widely studied in the fields of transportation, logistics, and delivery systems due to its numerous formulations and extensions. This article provides a detailed overview of VRP, exploring its theoretical foundations, limitations of the classical model, and key extensions. By reviewing various constraints, objectives, and variants in recent literature, it aims to enhance understanding of VRP and its ongoing evolution in modern optimization and decision-making processes. <div>
arXiv:2506.23028v1 Announce Type: new 
Abstract: The Vehicle Routing Problem (VRP) is a fundamental challenge in logistics management research, given its substantial influence on transportation efficiency, cost minimization, and service quality. As a combinatorial optimization problem, VRP plays a crucial role in a wide range of real world applications, particularly in transportation, logistics, and delivery systems, due to its diverse formulations and numerous extensions. Over the years, researchers have introduced various VRP variants to address specific operational constraints, emerging industry requirements and optimize specific objectives, making it one of the most extensively studied problems in operations research. This article provides a comprehensive overview of VRP by exploring its theoretical foundations, discussing the limitations of its classical model, and introducing its key extensions. By systematically reviewing the diverse constraints, objectives, and variants examined in recent literature, this study aims to contribute to a deeper understanding of VRP while highlighting its ongoing evolution and relevance in modern optimization and decision making processes.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Multiscale Topology Optimization of Spinodoid Architected Materials with Controllable Anisotropy</title>
<link>https://arxiv.org/abs/2506.23420</link>
<guid>https://arxiv.org/abs/2506.23420</guid>
<content:encoded><![CDATA[
<div> neural networks, topology optimization, spinodoid materials, data-driven design, Gaussian Process

Summary:
The article introduces a new approach to design optimization for spinodoid architected materials, which have unique properties such as stochasticity, aperiodicity, and bi-continuity. Traditional design methods face challenges due to the complexity of spinodoid design parameters. The proposed framework utilizes neural networks to automate the computation of topological gradients, making it more efficient and scalable. Additionally, a Gaussian Process surrogate is integrated to enhance the accuracy of spinodoid constitutive models. This framework provides physical insights into material distribution, showing why anisotropic spinodoids with tailored orientations are preferred in certain regions. The interpretability of the framework bridges the gap between data-driven design and mechanistic understanding, offering a clearer understanding of material behavior. <div>
arXiv:2506.23420v1 Announce Type: new 
Abstract: Spinodoid architected materials have drawn significant attention due to their unique nature in stochasticity, aperiodicity, and bi-continuity. Compared to classic periodic truss-, beam- and plate-based lattice architectures, spinodoids are insensitive to manufacturing defects, scalable for high throughput production, functionally graded by tunable local properties, and material failure resistant due to low-curvature morphology. However, the design of spinodoids is often hindered by the curse of dimensionality with extremely large design space of spinodoid types, material density, orientation, continuity, and anisotropy. From a design optimization perspective, while genetic algorithms are often beyond the reach of computing capacity, gradient-based topology optimization is challenged by the intricate mathematical derivation of gradient fields with respect to various spinodoid parameters. To address such challenges, we propose a data-driven multiscale topology optimization framework. Our framework reformulates the design variables of spinodoid materials as the parameters of neural networks, enabling automated computation of topological gradients. Additionally, it incorporates a Gaussian Process surrogate for spinodoid constitutive models, eliminating the need for repeated computational homogenization and enhancing the scalability of multiscale topology optimization. Compared to 'black-box' deep learning approaches, the proposed framework provides clear physical insights into material distribution. It explicitly reveals why anisotropic spinodoids with tailored orientations are favored in certain regions, while isotropic spinodoids are more suitable elsewhere. This interpretability helps to bridge the gap between data-driven design with mechanistic understanding.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Multiscale Topology Optimization of Soft Functionally Graded Materials with Large Deformations</title>
<link>https://arxiv.org/abs/2506.23422</link>
<guid>https://arxiv.org/abs/2506.23422</guid>
<content:encoded><![CDATA[
<div> Keywords: Functionally Graded Materials, Topology Optimization, Multiscale Architecture, Nonlinear Material Behavior, Neural Network

Summary:
Functionally Graded Materials (FGMs), particularly soft FGMs, are increasingly important in various engineering applications. Designing these complex systems poses challenges due to their multiscale nature, multiple material phases, and nonlinear behaviors. This paper presents a novel topology optimization framework for soft FGMs under large deformations. Key innovations include a microstructure reconstruction algorithm, material homogenization approach, neural network-based optimization, and a nonlinear sensitivity analysis technique. The framework generates unique topological designs with spatially varying microstructures, not achievable using linear elasticity. To ensure efficient convergence, an energy interpolation scheme and a Newton-Raphson solver with adaptive steps are employed. Numerical experiments demonstrate the effectiveness of the proposed approach in automating the design innovation of soft FGMs. 

<br /><br />Summary: <div>
arXiv:2506.23422v1 Announce Type: new 
Abstract: Functionally Graded Materials (FGMs) made of soft constituents have emerged as promising material-structure systems in potential applications across many engineering disciplines, such as soft robots, actuators, energy harvesting, and tissue engineering. Designing such systems remains challenging due to their multiscale architectures, multiple material phases, and inherent material and geometric nonlinearities. The focus of this paper is to propose a general topology optimization framework that automates the design innovation of multiscale soft FGMs exhibiting nonlinear material behaviors under large deformations. Our proposed topology optimization framework integrates several key innovations: (1) a novel microstructure reconstruction algorithm that generates composite architecture materials from a reduced design space using physically interpretable parameters; (2) a new material homogenization approach that estimates effective properties by combining the stored energy functions of multiple soft constituents; (3) a neural network-based topology optimization that incorporates data-driven material surrogates to enable bottom-up, simultaneous optimization of material and structure; and (4) a generic nonlinear sensitivity analysis technique that computes design sensitivities numerically without requiring explicit gradient derivation. To enhance the convergence of the nonlinear equilibrium equations amid topology optimization, we introduce an energy interpolation scheme and employ a Newton-Raphson solver with adaptive step sizes and convergence criteria. Numerical experiments show that the proposed framework produces distinct topological designs, different from those obtained under linear elasticity, with spatially varying microstructures.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics</title>
<link>https://arxiv.org/abs/2506.22520</link>
<guid>https://arxiv.org/abs/2506.22520</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Curiosity, Engagement, Interactive Molecular Dynamics, Team Performance 

Summary: 
The study investigates the impact of an Artificial Intelligence tutor teammate on student curiosity-driven engagement and learning effectiveness in Interactive Molecular Dynamics tasks. Through a Wizard-of-Oz paradigm, the AI's behaviors are adjusted by a human experimenter to stimulate and sustain student curiosity. Results show that high-performing teams exhibit superior task completion, deeper understanding, and increased engagement. Advanced student questions are linked to AI curiosity-triggering, indicating heightened engagement and cognitive complexity. Cross Recurrence Quantification Analysis metrics demonstrate dynamic synchronization in student-AI interactions, promoting structured yet adaptive engagement to foster curiosity. The findings suggest that the AI's dual role as a teammate and educator can provide adaptive feedback, sustaining engagement and epistemic curiosity. <div>
arXiv:2506.22520v1 Announce Type: cross 
Abstract: This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research</title>
<link>https://arxiv.org/abs/2506.23545</link>
<guid>https://arxiv.org/abs/2506.23545</guid>
<content:encoded><![CDATA[
<div> training, diagnostics, psychological research, XR technologies, human performance <br />
<br />
In this panel discussion, the potential of Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies in various domains is highlighted. These immersive systems are being used to enhance human performance in clinical psychology, space exploration, and medical education. In psychological research and training, XR provides a controlled yet realistic environment for measuring cognitive and emotional processes. For space exploration, VR-based astronaut training and diagnostic systems allow for real-time health assessments. In the field of medical education and rehabilitation, immersive environments are utilized for procedural training and patient engagement. Whether through virtual surgical simulations or gamified rehabilitation exercises, XR technologies improve learning outcomes and encourage patient adherence to treatment plans. Overall, VR/AR/XR technologies are proving to be valuable tools in enhancing performance and advancing research in high-risk and regulated environments. <br /><br />Summary: <div>
arXiv:2506.23545v1 Announce Type: cross 
Abstract: Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are increasingly recognized for their applications in training, diagnostics, and psychological research, particularly in high-risk and highly regulated environments. In this panel we discuss how immersive systems enhance human performance across multiple domains, including clinical psychology, space exploration, and medical education. In psychological research and training, XR can offer a controlled yet ecologically valid setting for measuring cognitive and affective processes. In space exploration, we discuss the development of VR-based astronaut training and diagnostic systems, allowing astronauts to perform real-time health assessments. In medical education and rehabilitation, we cover procedural training and patient engagement. From virtual surgical simulations to gamified rehabilitation exercises, immersive environments enhance both learning outcomes and treatment adherence.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment</title>
<link>https://arxiv.org/abs/2506.23739</link>
<guid>https://arxiv.org/abs/2506.23739</guid>
<content:encoded><![CDATA[
<div> Vehicle-in-the-Loop test bench, vulnerable road users, cyber-physical testing, human pose estimation, urban environments <br />
<br />Summary: <br />
This paper discusses a test environment that combines a Vehicle-in-the-Loop test bench with a motion laboratory to test vehicle interactions with pedestrians and cyclists. The study validates a human pose estimation approach using real-world and virtual representations of vulnerable road users. Results show good alignment in human pose estimation between real-world and cyber-physical test conditions for stable motion patterns but inaccuracies persist under dynamic movements and occlusions, especially for complex cyclist postures. The research aims to enhance testing methodologies for evaluating AI-based vehicle perception and improving interaction models between automated vehicles and vulnerable road users in cyber-physical environments. <div>
arXiv:2506.23739v1 Announce Type: cross 
Abstract: Ensuring safe and realistic interactions between automated driving systems and vulnerable road users (VRUs) in urban environments requires advanced testing methodologies. This paper presents a test environment that combines a Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the feasibility of cyber-physical (CP) testing of vehicle-pedestrian and vehicle-cyclist interactions. Building upon previous work focused on pedestrian localization, we further validate a human pose estimation (HPE) approach through a comparative analysis of real-world (RW) and virtual representations of VRUs. The study examines the perception of full-body motion using a commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is generated in Unreal Engine 5, where VRUs are animated in real time and projected onto a screen to stimulate the camera. The proposed stimulation technique ensures the correct perspective, enabling realistic vehicle perception. To assess the accuracy and consistency of HPE across RW and CP domains, we analyze the reliability of detections as well as variations in movement trajectories and joint estimation stability. The validation includes dynamic test scenarios where human avatars, both walking and cycling, are monitored under controlled conditions. Our results show a strong alignment in HPE between RW and CP test conditions for stable motion patterns, while notable inaccuracies persist under dynamic movements and occlusions, particularly for complex cyclist postures. These findings contribute to refining CP testing approaches for evaluating next-generation AI-based vehicle perception and to enhancing interaction models of automated vehicles and VRUs in CP environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A topology optimisation framework to design test specimens for one-shot identification or discovery of material models</title>
<link>https://arxiv.org/abs/2501.12756</link>
<guid>https://arxiv.org/abs/2501.12756</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, Geometry Design, Material Model Calibration, Experimental Mechanics, Digital Image Correlation  
Summary:  
- The shift in material model calibration towards using complex geometry tests requires rich displacement data.  
- The paper proposes a density-based topology optimization approach to design specimen geometry for anisotropic material model calibration.  
- High-resolution specimen design aims to maximize the robustness of inverse problem solutions with noisy displacement measurements.  
- The study discusses cost function selection and topology optimization framework design for optimal specimen geometry.  
- Various optimized topologies are analyzed for identifying isotropic and anisotropic elastic responses.  

<br /><br />Summary: <div>
arXiv:2501.12756v2 Announce Type: replace 
Abstract: The increasing availability of full-field displacement data from imaging techniques in experimental mechanics is determining a gradual shift in the paradigm of material model calibration and discovery, from using several simple-geometry tests towards a few, or even one single test with complicated geometry. The feasibility of such a "one-shot" calibration or discovery heavily relies upon the richness of the measured displacement data, i.e., their ability to probe the space of the state variables and the stress space (whereby the stresses depend on the constitutive law being sought) to an extent sufficient for an accurate and robust calibration or discovery process. The richness of the displacement data is in turn directly governed by the specimen geometry. In this paper, we propose a density-based topology optimisation framework to optimally design the geometry of the target specimen for calibration of an anisotropic elastic material model. To this end, we perform automatic, high-resolution specimen design by maximising the robustness of the solution of the inverse problem, i.e., the identified material parameters, given noisy displacement measurements from digital image correlation. We discuss the choice of the cost function and the design of the topology optimisation framework, and we analyse a range of optimised topologies generated for the identification of isotropic and anisotropic elastic responses.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Evaluation Standards: A Unified Framework for Evaluating the Korean Capabilities of Language Models</title>
<link>https://arxiv.org/abs/2503.22968</link>
<guid>https://arxiv.org/abs/2503.22968</guid>
<content:encoded><![CDATA[
<div> framework, Korean large language models, evaluation, benchmarks, HRET  
Summary:  
- Recent advancements in Korean large language models have led to performance variations across institutions due to inconsistent evaluation protocols.  
- HRET (Haerae Evaluation Toolkit) is introduced as an open-source framework to unify Korean LLM assessment by integrating major benchmarks, inference backends, and diverse evaluation methods.  
- The modular registry design of HRET allows for rapid incorporation of new datasets, methods, and backends to adapt to evolving research needs.  
- HRET includes morphological-aware Type-Token Ratio and systematic keyword-omission detection for diagnostic insights into language-specific behaviors, aiding in identifying shortcomings and guiding improvements in Korean LLM development.  
- The framework promotes diverse experimental approaches and language consistency enforcement to ensure genuine Korean outputs.  
<br /><br />Summary: <div>
arXiv:2503.22968v3 Announce Type: replace 
Abstract: Recent advancements in Korean large language models (LLMs) have driven numerous benchmarks and evaluation methods, yet inconsistent protocols cause up to 10 p.p performance gaps across institutions. Overcoming these reproducibility gaps does not mean enforcing a one-size-fits-all evaluation. Rather, effective benchmarking requires diverse experimental approaches and a framework robust enough to support them. To this end, we introduce HRET (Haerae Evaluation Toolkit), an open-source, registry-based framework that unifies Korean LLM assessment. HRET integrates major Korean benchmarks, multiple inference backends, and multi-method evaluation, with language consistency enforcement to ensure genuine Korean outputs. Its modular registry design also enables rapid incorporation of new datasets, methods, and backends, ensuring the toolkit adapts to evolving research needs. Beyond standard accuracy metrics, HRET incorporates Korean-focused output analyses-morphology-aware Type-Token Ratio (TTR) for evaluating lexical diversity and systematic keyword-omission detection for identifying missing concepts-to provide diagnostic insights into language-specific behaviors. These targeted analyses help researchers pinpoint morphological and semantic shortcomings in model outputs, guiding focused improvements in Korean LLM development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calculation of Photocarrier Generation from Optical Absorption for Time-domain Simulation of Optoelectronic Devices</title>
<link>https://arxiv.org/abs/2102.06702</link>
<guid>https://arxiv.org/abs/2102.06702</guid>
<content:encoded><![CDATA[
<div> optoelectronic materials, photocarrier generation rate, Poynting vector, time-domain simulations, optical absorption <br />
Summary: 
The study addresses the inaccurate calculation of photocarrier generation rates in optoelectronic materials using the Poynting vector in time-domain simulations. It proposes an optical absorption-based model that considers material dispersion near the optical frequency corresponding to the bandgap energy. By calculating instantaneous optical absorption from the polarization current density associated with the dispersion model, the proposed approach offers more accurate results compared to the Poynting vector-based method. Numerical simulations demonstrate the efficacy of the new model, particularly when dealing with strong low-frequency fields that can lead to divergent carrier densities in the Poynting vector approach. The method is further validated through simulations of a photoconductive device, highlighting its improved accuracy in determining photocarrier generation rates. <br /> <div>
arXiv:2102.06702v3 Announce Type: replace-cross 
Abstract: Photocarrier generation rate in optoelectronic materials is often calculated using the Poynting vector in the frequency domain. However, this approach is not accurate in time-domain simulations of photoconductive devices because the instantaneous Poynting vector does not distinguish between power flux densities of optical and low-frequency electromagnetic fields. The latter is generated by photocurrents and is not supposed to contribute to the photocarrier generation since the corresponding photon energy is smaller than the bandgap energy of the optoelectronic material. This work proposes an optical absorption-based model to accurately calculate the generation rate in time-domain simulations. The proposed approach considers the material dispersion near the optical frequency corresponding to the bandgap energy of the optoelectronic material and calculates the instantaneous optical absorption from the polarization current density associated with this dispersion model. Numerical examples show that the proposed method is more accurate than the Poynting vector-based approach in calculating the instantaneous optical absorption. The method is further validated against experimental results via simulations of a photoconductive device, where the Poynting vector-based approach results in divergent carrier densities when the low-frequency fields are strong.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Self-Amplifying Hypergraph Structures through Mathematical Optimization</title>
<link>https://arxiv.org/abs/2412.15776</link>
<guid>https://arxiv.org/abs/2412.15776</guid>
<content:encoded><![CDATA[
<div> amplification factor, self-amplifying structures, hypergraphs, optimization, chemical reaction networks

Summary:
The paper introduces self-amplifying structures for hypergraphs, crucial for understanding propagation and internal reinforcement in complex systems. It defines the maximal amplification factor to quantify this phenomenon and develops an optimization-based methodology to compute it efficiently. The problem of identifying the subhypergraph maximizing the amplification factor is addressed as a mixed-integer nonlinear programming (MINLP) problem, solved with an exact iterative algorithm. Extensive computational experiments on synthetic instances demonstrate the relevance and effectiveness of the proposed approach. A case study on chemical reaction networks, including the Formose reaction and E. coli core metabolism, showcases the framework's ability to identify known and novel autocatalytic subnetworks, emphasizing its practical relevance in systems chemistry and biology.<br /><br />Summary: <div>
arXiv:2412.15776v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce the concept of self-amplifying structures for hypergraphs, positioning it as a key element for understanding propagation and internal reinforcement in complex systems. To quantify this phenomenon, we define the maximal amplification factor, a metric that captures how effectively a subhypergraph contributes to its own amplification. We then develop an optimization-based methodology to compute this measure. Building on this foundation, we tackle the problem of identifying the subhypergraph maximizing the amplification factor, formulating it as a mixed-integer nonlinear programming (MINLP) problem. To solve it efficiently, we propose an exact iterative algorithm with proven convergence guarantees. In addition, we report the results of extensive computational experiments on realistic synthetic instances, demonstrating both the relevance and effectiveness of the proposed approach. Finally, we present a case study on chemical reaction networks, including the Formose reaction and E. coli core metabolism, where our framework successfully identifies known and novel autocatalytic subnetworks, highlighting its practical relevance to systems chemistry and biology.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drivetrain simulation using variational autoencoders</title>
<link>https://arxiv.org/abs/2501.17653</link>
<guid>https://arxiv.org/abs/2501.17653</guid>
<content:encoded><![CDATA[
<div> Variational Autoencoders, Vehicle Jerk Prediction, Torque Demand, Drivetrain, Data Efficiency<br />
Summary:<br />
This work explores the use of variational autoencoders (VAEs) for predicting vehicle jerk signals based on torque demand in drivetrain applications with limited real-world datasets. The study involves training VAEs on experimental data from two electric SUV variants to synthesize jerk signals that capture characteristics of different drivetrain scenarios. Comparisons with physics-based and hybrid models demonstrate the effectiveness of VAEs in generating realistic jerk signals without extensive system parametrization. Unconditional VAEs successfully produce realistic signals without prior system knowledge, while conditional VAEs can tailor signals to specific torque inputs. By reducing the reliance on costly experiments and manual modeling, VAEs offer a data-efficient approach for exploring complex operational scenarios in drivetrain simulations. This integration of generative models like VAEs shows potential for enhancing validation procedures and accelerating vehicle development processes. <br /><br /> <div>
arXiv:2501.17653v2 Announce Type: replace-cross 
Abstract: This work proposes variational autoencoders (VAEs) to predict a vehicle's jerk signals from torque demand in the context of limited real-world drivetrain datasets. We implement both unconditional and conditional VAEs, trained on experimental data from two variants of a fully electric SUV with differing torque and drivetrain configurations. The VAEs synthesize jerk signals that capture characteristics from multiple drivetrain scenarios by leveraging the learned latent space. A performance comparison with baseline physics-based and hybrid models confirms the effectiveness of the VAEs, without requiring detailed system parametrization. Unconditional VAEs generate realistic jerk signals without prior system knowledge, while conditional VAEs enable the generation of signals tailored to specific torque inputs. This approach reduces the dependence on costly and time-intensive real-world experiments and extensive manual modeling. The results support the integration of generative models such as VAEs into drivetrain simulation pipelines, both for data augmentation and for efficient exploration of complex operational scenarios, with the potential to streamline validation and accelerate vehicle development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting</title>
<link>https://arxiv.org/abs/2506.21743</link>
<guid>https://arxiv.org/abs/2506.21743</guid>
<content:encoded><![CDATA[
<div> forecasting, storm surge, machine learning, ConvLSTM networks, coastal disaster

Summary:
This study introduces a novel approach to storm surge forecasting by utilizing ConvLSTM networks on structured RGB-encoded image representations of water elevation fields. The model incorporates ground-truth wind fields as dynamic conditioning signals and topo-bathymetry as a static input to capture surge evolution drivers. Evaluation on a dataset of synthetic storms in the Gulf of Mexico shows robust 48-hour forecasting performance across multiple regions along the Texas coast and spatial extensibility to other coastal areas. By combining structured representation, physically grounded forcings, and scalable deep learning, this approach advances storm surge forecasting in usability, adaptability, and interpretability. <div>
arXiv:2506.21743v1 Announce Type: new 
Abstract: Storm surge forecasting plays a crucial role in coastal disaster preparedness, yet existing machine learning approaches often suffer from limited spatial resolution, reliance on coastal station data, and poor generalization. Moreover, many prior models operate directly on unstructured spatial data, making them incompatible with modern deep learning architectures. In this work, we introduce a novel approach that projects unstructured water elevation fields onto structured Red Green Blue (RGB)-encoded image representations, enabling the application of Convolutional Long Short Term Memory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our model further integrates ground-truth wind fields as dynamic conditioning signals and topo-bathymetry as a static input, capturing physically meaningful drivers of surge evolution. Evaluated on a large-scale dataset of synthetic storms in the Gulf of Mexico, our method demonstrates robust 48-hour forecasting performance across multiple regions along the Texas coast and exhibits strong spatial extensibility to other coastal areas. By combining structured representation, physically grounded forcings, and scalable deep learning, this study advances the frontier of storm surge forecasting in usability, adaptability, and interpretability.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.21815</link>
<guid>https://arxiv.org/abs/2506.21815</guid>
<content:encoded><![CDATA[
<div> machine learning, laser powder bed fusion, microstructure, phase-field method, deep reinforcement learning 

Summary:
This research focuses on optimizing laser powder bed fusion (L-PBF) processes to control microstructure outcomes like equiaxed grains. By combining physics-guided modeling with machine learning, particularly a 3D U-Net convolutional neural network, the study aimed to accelerate the prediction of crystalline grain orientations. Three scanning strategies were explored, leading to a significant speedup in computational efficiency. Deep reinforcement learning (DRL) was then employed to generate optimized scan paths for target microstructures, showcasing its effectiveness in enhancing control over microstructure outcomes. The integration of the surrogate 3D U-Net model into the DRL environment further streamlined the training process. Ultimately, the study demonstrated the potential of machine learning methods to not only improve microstructure control but also enhance computational efficiency in optimizing L-PBF processes. <div>
arXiv:2506.21815v1 Announce Type: new 
Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing technology for producing intricate metal components with exceptional accuracy. A key challenge in L-PBF is the formation of complex microstructures affecting product quality. We propose a physics-guided, machine-learning approach to optimize scan paths for desired microstructure outcomes, such as equiaxed grains. We utilized a phase-field method (PFM) to model crystalline grain structure evolution. To reduce computational costs, we trained a surrogate machine learning model, a 3D U-Net convolutional neural network, using single-track phase-field simulations with various laser powers to predict crystalline grain orientations based on initial microstructure and thermal history. We investigated three scanning strategies across various hatch spacings within a square domain, achieving a two-orders-of-magnitude speedup using the surrogate model. To reduce trial and error in designing laser scan toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan paths for target microstructure. Results from three cases demonstrate the DRL approach's effectiveness. We integrated the surrogate 3D U-Net model into our DRL environment to accelerate the reinforcement learning training process. The reward function minimizes both aspect ratio and grain volume of the predicted microstructure from the agent's scan path. The reinforcement learning algorithm was benchmarked against conventional zigzag approach for smaller and larger domains, showing machine learning methods' potential to enhance microstructure control and computational efficiency in L-PBF optimization.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-free Forecasting of Rogue Waves using Reservoir Computing</title>
<link>https://arxiv.org/abs/2506.21918</link>
<guid>https://arxiv.org/abs/2506.21918</guid>
<content:encoded><![CDATA[
<div> Reservoir Computing, Hamiltonian systems, rogue waves, nonlinear Schrdinger equation, prediction horizon <br />
Summary: <br />
This paper explores the application of Reservoir Computing in modeling rogue wave dynamics from the nonlinear Schrdinger equation, a Hamiltonian system with modulation instability. The study demonstrates the effectiveness of Reservoir Computing in capturing these dynamics from breather simulations with unstable modes. The Echo State Network successfully predicts dynamics from two distinct testing datasets, including a higher-order breather, with remarkable agreement. The reservoir is able to forecast rogue wave propagation over a long prediction horizon, even when facing unseen dynamics. Additionally, a method is introduced to enhance the Reservoir Computing prediction in autonomous mode, improving its long-term forecasting ability. These findings contribute to advancing the use of Reservoir Computing in spatio-temporal Hamiltonian systems and underline the significance of phase space coverage in training data design. <br /> <div>
arXiv:2506.21918v1 Announce Type: new 
Abstract: Recent research has demonstrated Reservoir Computing's capability to model various chaotic dynamical systems, yet its application to Hamiltonian systems remains relatively unexplored. This paper investigates the effectiveness of Reservoir Computing in capturing rogue wave dynamics from the nonlinear Schr\"{o}dinger equation, a challenging Hamiltonian system with modulation instability. The model-free approach learns from breather simulations with five unstable modes. A properly tuned parallel Echo State Network can predict dynamics from two distinct testing datasets. The first set is a continuation of the training data, whereas the second set involves a higher-order breather. An investigation of the one-step prediction capability shows remarkable agreement between the testing data and the models. Furthermore, we show that the trained reservoir can predict the propagation of rogue waves over a relatively long prediction horizon, despite facing unseen dynamics. Finally, we introduce a method to significantly improve the Reservoir Computing prediction in autonomous mode, enhancing its long-term forecasting ability. These results advance the application of Reservoir Computing to spatio-temporal Hamiltonian systems and highlight the critical importance of phase space coverage in the design of training data.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Algorithm Based on CNN-LSTM Framework for Predicting Cancer Drug Sales Volume</title>
<link>https://arxiv.org/abs/2506.21927</link>
<guid>https://arxiv.org/abs/2506.21927</guid>
<content:encoded><![CDATA[
<div> deep learning, CNN-LSTM framework, cancer drug sales, forecasting, time series data

Summary:
This study explores the application potential of a deep learning model based on the CNN-LSTM framework in forecasting cancer drug sales. The research utilizes sales records of a specific cancer drug in Egypt from 2015 to 2024 to predict sales volume. A hybrid deep learning model combining CNN and LSTM networks is employed to improve prediction accuracy. The CNN component extracts local temporal features, while the LSTM component captures long-term dependencies. Model performance is evaluated using Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), showing effectiveness in handling nonlinear and volatile sales data. The results demonstrate the model's success on the test set, with an MSE of 1.150 and an RMSE of 1.072. This research provides valuable insights for data-driven decision-making in pharmaceutical marketing and healthcare resource planning. 

Summary: <div>
arXiv:2506.21927v1 Announce Type: new 
Abstract: This study explores the application potential of a deep learning model based on the CNN-LSTM framework in forecasting the sales volume of cancer drugs, with a focus on modeling complex time series data. As advancements in medical technology and cancer treatment continue, the demand for oncology medications is steadily increasing. Accurate forecasting of cancer drug sales plays a critical role in optimizing production planning, supply chain management, and healthcare policy formulation. The dataset used in this research comprises quarterly sales records of a specific cancer drug in Egypt from 2015 to 2024, including multidimensional information such as date, drug type, pharmaceutical company, price, sales volume, effectiveness, and drug classification. To improve prediction accuracy, a hybrid deep learning model combining Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks is employed. The CNN component is responsible for extracting local temporal features from the sales data, while the LSTM component captures long-term dependencies and trends. Model performance is evaluated using two widely adopted metrics: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). The results demonstrate that the CNN-LSTM model performs well on the test set, achieving an MSE of 1.150 and an RMSE of 1.072, indicating its effectiveness in handling nonlinear and volatile sales data. This research provides theoretical and technical support for data-driven decision-making in pharmaceutical marketing and healthcare resource planning.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</title>
<link>https://arxiv.org/abs/2501.06184</link>
<guid>https://arxiv.org/abs/2501.06184</guid>
<content:encoded><![CDATA[
<div> geologic map, MLLMs, GeoMap-Bench, GeoMap-Agent, AI applications <br />
Summary: 
Geologic maps play a crucial role in understanding Earth's subsurface and surface. Current Multimodal Large Language Models (MLLMs) often struggle with geologic map understanding due to cartographic generalization challenges. To address this gap, GeoMap-Bench, a benchmark for evaluating MLLMs in geologic map understanding, was established. The GeoMap-Agent, designed for geologic map understanding, consists of Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA) modules. The AI expert group acts as consultants, achieving a high overall score on GeoMap-Bench. This work, known as PEACE, empowers geologic map holistic understanding with MLLMs, paving the way for advanced AI applications in geology. <div>
arXiv:2501.06184v1 Announce Type: cross 
Abstract: Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth's subsurface and surface. These maps are indispensable in various fields, including disaster detection, resource exploration, and civil engineering. Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding. This gap is primarily due to the challenging nature of cartographic generalization, which involves handling high-resolution map, managing multiple associated components, and requiring domain-specific knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions. Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o. Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Simulations of Turbulent Flows using Lattice Boltzmann Methods on Heterogeneous High Performance Computers</title>
<link>https://arxiv.org/abs/2506.21804</link>
<guid>https://arxiv.org/abs/2506.21804</guid>
<content:encoded><![CDATA[
<div> Keywords: GPU-accelerated supercomputers, turbulent flows, Lattice Boltzmann Methods, wall-modeled LES, scalability<br />
Summary:<br />
This paper focuses on the potential of GPU-accelerated supercomputers for large-scale simulations of turbulent flows. It introduces a novel Lattice Boltzmann Method (LBM) scheme tailored for wall-modeled Large Eddy Simulations (LES) in complex geometries. The scheme is specifically designed for efficient implementation within the open source LBM framework OpenLB. The study provides detailed scalability results for different HoreKa partitions, utilizing up to 128 nodes and covering problem sizes up to 18 billion cells. The research highlights the compatibility of LBM with highly parallel execution on both CPUs and GPUs, making it a promising tool for simulating turbulent flows in various applications. <div>
arXiv:2506.21804v1 Announce Type: cross 
Abstract: Current GPU-accelerated supercomputers promise to enable large-scale simulations of turbulent flows. Lattice Boltzmann Methods (LBM) are particularly well-suited to fulfilling this promise due to their intrinsic compatibility with highly parallel execution on both SIMD CPUs and GPUs. A novel LBM scheme for wall-modeled LES in complex geometries is described with a special focus on the efficient implementation in the open source LBM framework OpenLB. Detailed scalability results are provided for all HoreKa partitions, utilizing up to 128 nodes and covering problem sizes up to 18 billion cells.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StructMG: A Fast and Scalable Structured Algebraic Multigrid</title>
<link>https://arxiv.org/abs/2506.21932</link>
<guid>https://arxiv.org/abs/2506.21932</guid>
<content:encoded><![CDATA[
<div> derive, multigrid, structured grid, algebraic multigrid, parallel efficiency
Summary:
StructMG is a novel algebraic multigrid preconditioner designed for solving large-scale sparse linear systems on structured grids. Three key principles derived from the classical 'multigrid seesaw' guide its efficient implementation. StructMG automatically constructs hierarchical grids, achieving low cost per iteration and good convergence in parallel. By using a stencil-based triple-matrix product for multi-dimensional Galerkin coarsening, grid complexity and implementation effort are reduced. A unified parallel framework for sparse triangular solvers ensures fast convergence and high parallel efficiency in smoothers. StructMG outperforms existing multigrid preconditioners like \textit{hypre}'s in radiation hydrodynamics, petroleum reservoir simulation, numerical weather prediction, and solid mechanics applications on ARM and X86 platforms, with average speedups of 15.5x, 5.5x, 6.7x, and 7.3x, respectively. Additionally, it improves strong and weak scaling efficiencies significantly.<br /><br />Summary: <div>
arXiv:2506.21932v1 Announce Type: cross 
Abstract: Parallel multigrid is widely used as preconditioners in solving large-scale sparse linear systems. However, the current multigrid library still needs more satisfactory performance for structured grid problems regarding speed and scalability. Based on the classical 'multigrid seesaw', we derive three necessary principles for an efficient structured multigrid, which instructs our design and implementation of StructMG, a fast and scalable algebraic multigrid that constructs hierarchical grids automatically. As a preconditioner, StructMG can achieve both low cost per iteration and good convergence when solving large-scale linear systems with iterative methods in parallel. A stencil-based triple-matrix product via symbolic derivation and code generation is proposed for multi-dimensional Galerkin coarsening to reduce grid complexity, operator complexity, and implementation effort. A unified parallel framework of sparse triangular solver is presented to achieve fast convergence and high parallel efficiency for smoothers, including dependence-preserving Gauss-Seidel and incomplete LU methods. Idealized and real-world problems from radiation hydrodynamics, petroleum reservoir simulation, numerical weather prediction, and solid mechanics, are evaluated on ARM and X86 platforms to show StructMG's effectiveness. In comparison to \textit{hypre}'s structured and general multigrid preconditioners, StructMG achieves the fastest time-to-solutions in all cases with average speedups of 15.5x, 5.5x, 6.7x, 7.3x over SMG, PFMG, SysPFMG, and BoomerAMG, respectively. StructMG also significantly improves strong and weak scaling efficiencies.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EUR/USD Exchange Rate Forecasting incorporating Text Mining Based on Pre-trained Language Models and Deep Learning Methods</title>
<link>https://arxiv.org/abs/2411.07560</link>
<guid>https://arxiv.org/abs/2411.07560</guid>
<content:encoded><![CDATA[
<div> Keywords: EUR/USD, exchange rate forecasting, deep learning, textual analysis, particle swarm optimization

Summary: 

This study presents a new approach for forecasting the EUR/USD exchange rate by combining deep learning, textual analysis, and particle swarm optimization. By utilizing online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior accuracy compared to traditional models. The research incorporates advanced text mining techniques such as sentiment analysis and topic modeling. Empirical results show that integrating textual data significantly improves forecasting performance, with the PSO-LSTM model outperforming benchmark models. Ablation experiments highlight the contribution of each textual data category to the overall accuracy. The study emphasizes the potential of artificial intelligence in finance and suggests future research avenues for real-time forecasting and alternative data integration. 

<br /><br />Summary: <div>
arXiv:2411.07560v2 Announce Type: replace 
Abstract: This study introduces a novel approach for EUR/USD exchange rate forecasting that integrates deep learning, textual analysis, and particle swarm optimization (PSO). By incorporating online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior performance compared to traditional econometric and machine learning models. The research employs advanced text mining techniques, including sentiment analysis using the RoBERTa-Large model and topic modeling with LDA. Empirical findings underscore the significant advantage of incorporating textual data, with the PSO-LSTM model outperforming benchmark models such as SVM, SVR, ARIMA, and GARCH. Ablation experiments reveal the contribution of each textual data category to the overall forecasting performance. The study highlights the transformative potential of artificial intelligence in finance and paves the way for future research in real-time forecasting and the integration of alternative data sources.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Data Quality for AI to AI for Data Quality: A Systematic Review of Tools for AI-Augmented Data Quality Management in Data Warehouses</title>
<link>https://arxiv.org/abs/2406.10940</link>
<guid>https://arxiv.org/abs/2406.10940</guid>
<content:encoded><![CDATA[
<div> tools, AI-augmented data quality management, data warehouses, automation capabilities, DQ rules<br />
Summary:<br />
The study evaluates the automation capabilities of 151 data quality tools for AI-augmented data quality management in data warehouse environments. Only 10 tools were found suitable, focusing more on data cleansing and preparation for AI rather than improving DQ using AI. Features like SQL-based rule specification, reconciliation logic, and explainability of AI-driven recommendations are lacking. The study provides guidance for tool selection and highlights design requirements for next-generation AI-driven DQ solutions. It advocates a shift from "data quality for AI" to "AI for data quality management".<br /> <div>
arXiv:2406.10940v3 Announce Type: replace-cross 
Abstract: While high data quality (DQ) is critical for analytics, compliance, and AI performance, data quality management (DQM) remains a complex, resource-intensive, and often manual process. This study investigates the extent to which existing tools support AI-augmented data quality management (DQM) in data warehouse environments. To this end, we conduct a systematic review of 151 DQ tools to evaluate their automation capabilities, particularly in detecting and recommending DQ rules in data warehouses -- a key component of modern data ecosystems. Using a multi-phase screening process based on functionality, trialability, regulatory compliance (e.g., GDPR), and architectural compatibility with data warehouses, only 10 tools met the criteria for AI-augmented DQM. The analysis reveals that most tools emphasize data cleansing and preparation for AI, rather than leveraging AI to improve DQ itself. Although metadata- and ML-based rule detection techniques are present, features such as SQL-based rule specification, reconciliation logic, and explainability of AI-driven recommendations remain scarce. This study offers practical guidance for tool selection and outlines critical design requirements for next-generation AI-driven DQ solutions -- advocating a paradigm shift from ``data quality for AI'' to ``AI for data quality management''.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EUR-USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods</title>
<link>https://arxiv.org/abs/2408.13214</link>
<guid>https://arxiv.org/abs/2408.13214</guid>
<content:encoded><![CDATA[
<div> Keywords: EUR/USD exchange rate, forecasting, textual data, structured data, Optuna-Bi-LSTM model 

Summary: 
The paper introduces the IUS framework, a novel approach for enhancing EUR/USD exchange rate prediction by integrating unstructured textual data with structured data. This framework utilizes large language models for sentiment analysis and exchange rate movement classification, combined with quantitative features in a Causality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then employed for forecasting. Experimental results show that the proposed method outperforms benchmark models, achieving a 10.69% reduction in MAE and a 9.56% decrease in RMSE. The fusion of unstructured and structured data proves beneficial, with the combined approach yielding higher accuracy than using structured data alone. Feature selection highlights the importance of the top 12 quantitative features combined with textual features. Overall, the IUS framework and Optuna-Bi-LSTM model offer a robust and effective method for EUR/USD exchange rate forecasting through the integration of multi-source data. 

<br /><br />Summary: <div>
arXiv:2408.13214v2 Announce Type: replace-cross 
Abstract: Accurate forecasting of the EUR/USD exchange rate is crucial for investors, businesses, and policymakers. This paper proposes a novel framework, IUS, that integrates unstructured textual data from news and analysis with structured data on exchange rates and financial indicators to enhance exchange rate prediction. The IUS framework employs large language models for sentiment polarity scoring and exchange rate movement classification of texts. These textual features are combined with quantitative features and input into a Causality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then used to forecast the EUR/USD exchange rate. Experiments demonstrate that the proposed method outperforms benchmark models, reducing MAE by 10.69% and RMSE by 9.56% compared to the best performing baseline. Results also show the benefits of data fusion, with the combination of unstructured and structured data yielding higher accuracy than structured data alone. Furthermore, feature selection using the top 12 important quantitative features combined with the textual features proves most effective. The proposed IUS framework and Optuna-Bi-LSTM model provide a powerful new approach for exchange rate forecasting through multi-source data integration.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion</title>
<link>https://arxiv.org/abs/2506.20763</link>
<guid>https://arxiv.org/abs/2506.20763</guid>
<content:encoded><![CDATA[
<div> formulation, phase field, multi-physics, finite element, engineering

Summary: 
This article introduces a new formulation to address coupled structural integrity problems by combining phase field and multi-physics modeling techniques. The approach leverages the flexibility of the heat transfer equation, making it suitable for integration into commercial finite element packages with only integration point-level implementation. The methodology is demonstrated through the implementation of coupled phenomena using user subroutines in the Abaqus finite element package. The theoretical and computational framework is applied to four engineering and scientific problems: thermo-mechanical fracture, hydraulic fracture, hydrogen-assisted cracking, and metallic corrosion, in both 2D and 3D scenarios. The results obtained show excellent agreement with experimental, numerical, and analytical solutions. The user subroutines developed for implementation are freely accessible online, enhancing the accessibility and usability of the proposed approach. <div>
arXiv:2506.20763v1 Announce Type: new 
Abstract: We present a novel, generalised formulation to treat coupled structural integrity problems by combining phase field and multi-physics modelling. The approach exploits the versatility of the heat transfer equation and is therefore well suited to be adopted in commercial finite element packages, requiring only integration point-level implementation. This aspect is demonstrated here by implementing coupled, multi-variable phenomena through simple \texttt{UMAT} and \texttt{UMATHT} subroutines in the finite element package \texttt{Abaqus}. The generalised theoretical and computational framework presented is particularised to four problems of engineering and scientific relevance: thermo-mechanical fracture, hydraulic fracture, hydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are considered. The results reveal a very good agreement with experimental data, and existing numerical and analytical solutions.The user subroutines developed are made freely available at https://mechmat.web.ox.ac.uk/codes.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers</title>
<link>https://arxiv.org/abs/2506.20773</link>
<guid>https://arxiv.org/abs/2506.20773</guid>
<content:encoded><![CDATA[
<div> Keywords: viscoelasticity, permanent set, polymers, transient network theory, numerical framework

Summary:
An efficient numerical framework for modeling viscoelasticity and permanent set of polymers is presented. The framework is based on the hereditary integral form of transient network theory, where polymer chains belong to distinct networks with different natural equilibrium states. Chains detach from previous networks and attach to new networks in a state of zero stress. The free energy of these networks is defined in terms of the deformation gradient relative to the network's birth configuration. A decomposition of the kernel for various free energies allows for a recurrence relationship without the need for full-time history integration. The technique applies to both highly compressible and nearly incompressible materials using various material models. The framework can handle rate-dependent response and residual strains under complex loading histories, as demonstrated through multiple examples. <div>
arXiv:2506.20773v1 Announce Type: new 
Abstract: An efficient numerical framework is presented for modeling viscoelasticity and permanent set of polymers. It is based on the hereditary integral form of transient network theory, in which polymer chains belong to distinct networks each with different natural equilibrium states. Chains continually detach from previously formed networks and reattach to new networks in a state of zero stress. The free energy of these networks is given in terms of the deformation gradient relative to the configuration at which the network was born. A decomposition of the kernel for various free energies allows for a recurrence relationship to be established, bypassing the need to integrate over all time history. The technique is established for both highly compressible and nearly incompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and Ogden-Hill material models. Multiple examples are presented showing the ability to handle rate-dependent response and residual strains under complex loading histories.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation</title>
<link>https://arxiv.org/abs/2506.21362</link>
<guid>https://arxiv.org/abs/2506.21362</guid>
<content:encoded><![CDATA[
<div> Counterfactual Voting Adjustment, online platforms, information quality, helpfulness voting, content ranking  
Summary:  
The article introduces the Counterfactual Voting Adjustment (CVA) framework for fairer assessment of information quality on online platforms. CVA addresses biases in aggregated votes by considering the context of individual votes, effectively modeling position and herding biases. Preliminary experiments demonstrate that CVA accurately recovers predefined content quality. In a real experiment, reranking content based on CVA-learned quality aligns better with user sentiment and quality evaluation by GPT-4o compared to rankings based on aggregated votes. The article also offers insights into the behavioral dynamics of expert user groups across 120 StackExchange communities using CVA embeddings. <div>
arXiv:2506.21362v1 Announce Type: new 
Abstract: Efficient access to high-quality information is vital for online platforms. To promote more useful information, users not only create new content but also evaluate existing content, often through helpfulness voting. Although aggregated votes help service providers rank their user content, these votes are often biased by disparate accessibility per position and the cascaded influence of prior votes. For a fairer assessment of information quality, we propose the Counterfactual Voting Adjustment (CVA), a causal framework that accounts for the context in which individual votes are cast. Through preliminary and semi-synthetic experiments, we show that CVA effectively models the position and herding biases, accurately recovering the predefined content quality. In a real experiment, we demonstrate that reranking content based on the learned quality by CVA exhibits stronger alignment with both user sentiment and quality evaluation assessed by GPT-4o, outperforming system rankings based on aggregated votes and model-based rerankings without causal inference. Beyond the individual quality inference, our embeddings offer comparative insights into the behavioral dynamics of expert user groups across 120 major StackExchange communities.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools</title>
<link>https://arxiv.org/abs/2506.20743</link>
<guid>https://arxiv.org/abs/2506.20743</guid>
<content:encoded><![CDATA[
<div> Foundation models, materials science, AI systems, scientific discovery, multimodal. 

Summary:
Foundation models are revolutionizing materials science by enabling scalable, versatile AI systems for scientific research. These models offer cross-domain generalization and emergent capabilities, making them ideal for the diverse challenges in materials science. The survey covers various application areas, including data extraction, atomistic simulation, property prediction, materials design, process planning, and multiscale modeling. It discusses advancements in unimodal and multimodal foundation models, as well as large language model agents. Standardized datasets, open-source tools, and autonomous experimental platforms are also reviewed. While foundation models have shown early success, challenges remain in generalizability, interpretability, data imbalance, safety, and multimodal fusion. Future research directions focus on scalable pretraining, continual learning, data governance, and trustworthiness.<br /><br />Summary: <div>
arXiv:2506.20743v1 Announce Type: cross 
Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\&amp;A atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pull-off strength of mushroom-shaped fibrils adhered to rigid substrates</title>
<link>https://arxiv.org/abs/2506.20745</link>
<guid>https://arxiv.org/abs/2506.20745</guid>
<content:encoded><![CDATA[
<div> Keywords: adhesion, fibrillar structures, mushroom-shaped fibrils, computational analysis, cohesive zone model

Summary: 
- The study focuses on analyzing the detachment behavior of mushroom-shaped fibrils adhered to a rigid substrate using a computational approach based on a Dugdale cohesive zone model.
- The results indicate that the separation process during detachment is inherently unstable under load control, regardless of whether detachment initiates at the edge or center of the fibril.
- Fibrils with wide, thin mushroom caps demonstrate enhanced adhesion by reducing stress concentrations and promoting central detachment, leading to improved adhesion strength.
- While central detachment is not observed in all geometries, edge detachment can occur under specific conditions in all cases.
- Adhesion defects at the fibril center can greatly reduce pull-off strength, especially at high values of the dimensionless parameter \c{hi}.

<br /><br />Summary: <div>
arXiv:2506.20745v1 Announce Type: cross 
Abstract: The exceptional adhesion properties of biological fibrillar structures -- such as those found in geckos -- have inspired the development of synthetic adhesive surfaces. Among these, mushroom-shaped fibrils have demonstrated superior pull-off strength compared to other geometries. In this study, we employ a computational approach based on a Dugdale cohesive zone model to analyze the detachment behavior of these fibrils when adhered to a rigid substrate. The results provide complete pull-off curves, revealing that the separation process is inherently unstable under load control, regardless of whether detachment initiates at the fibril edge or center. Our findings show that fibrils with a wide, thin mushroom cap effectively reduce stress concentrations and promote central detachment, leading to enhanced adhesion. However, detachment from the center is not observed in all geometries, whereas edge detachment can occur under certain conditions in all cases. Additionally, we investigate the impact of adhesion defects at the fibril center, showing that they can significantly reduce pull-off strength, particularly at high values of the dimensionless parameter \c{hi}. These insights contribute to the optimization of bio-inspired adhesives and microstructured surfaces for various engineering applications.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering</title>
<link>https://arxiv.org/abs/2506.20821</link>
<guid>https://arxiv.org/abs/2506.20821</guid>
<content:encoded><![CDATA[
<div> Keywords: financial documents, multimodal extraction, retrieval-augmented generation, cross-modal reasoning, MultiFinRAG

Summary: 
- Financial documents, such as 10-Ks and investor presentations, are complex and require joint reasoning across modalities.
- MultiFinRAG is a framework designed for financial question answering that integrates multimodal extraction and retrieval-augmented generation.
- The framework utilizes a lightweight multimodal LLM for extracting structured outputs and textual summaries from tables and figures.
- MultiFinRAG indexes these outputs with modality-aware similarity thresholds for precise retrieval and employs a tiered fallback strategy for cross-modal reasoning.
- Despite running on basic hardware, MultiFinRAG outperforms ChatGPT-4o in accuracy by 19 percentage points on financial QA tasks involving multiple modalities. 

<br /><br />Summary: <div>
arXiv:2506.20821v1 Announce Type: cross 
Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span hundreds of pages and combine diverse modalities, including dense narrative text, structured tables, and complex figures. Answering questions over such content often requires joint reasoning across modalities, which strains traditional large language models (LLMs) and retrieval-augmented generation (RAG) pipelines due to token limitations, layout loss, and fragmented cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation framework purpose-built for financial QA. MultiFinRAG first performs multimodal extraction by grouping table and figure images into batches and sending them to a lightweight, quantized open-source multimodal LLM, which produces both structured JSON outputs and concise textual summaries. These outputs, along with narrative text, are embedded and indexed with modality-aware similarity thresholds for precise retrieval. A tiered fallback strategy then dynamically escalates from text-only to text+table+image contexts when necessary, enabling cross-modal reasoning while reducing irrelevant context. Despite running on commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy than ChatGPT-4o (free-tier) on complex financial QA tasks involving text, tables, images, and combined multimodal reasoning.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multicontinuum Homogenization for Poroelasticity Model</title>
<link>https://arxiv.org/abs/2506.20890</link>
<guid>https://arxiv.org/abs/2506.20890</guid>
<content:encoded><![CDATA[
<div> homogenization, poroelasticity, multicontinuum, porous media, computational challenges  
Summary:
In this paper, the authors derive multicontinuum poroelasticity models using the multicontinuum homogenization method. Poroelasticity models are essential in various fields to describe coupled flow and mechanics in porous media. The high contrast properties of poroelastic media create computational challenges, which standard homogenization approaches struggle to address. By employing multicontinuum methods, multiple average states called continua are defined, allowing for accurate solutions in cases of high property contrast. The authors extend previous research by deriving a generalized multicontinuum poroelasticity model using a rigorous approach. They formulate coupled constraint cell problems in oversampled regions, obtain a multicontinuum expansion of fine-scale fields, and derive the multicontinuum model while assuming the smoothness of macroscopic variables. Numerical experiments confirm the accuracy of the proposed multicontinuum models for various heterogeneous media cases. <div>
arXiv:2506.20890v1 Announce Type: cross 
Abstract: In this paper, we derive multicontinuum poroelasticity models using the multicontinuum homogenization method. Poroelasticity models are widely used in many areas of science and engineering to describe coupled flow and mechanics processes in porous media. However, in many applications, the properties of poroelastic media possess high contrast, presenting serious computational challenges. It is well known that standard homogenization approaches often fail to give an accurate solution due to the lack of macroscopic parameters. Multicontinuum approaches allow us to consider such cases by defining several average states known as continua. In the field of poroelasticity, multiple-network models arising from the multiple porous media theory are representatives of these approaches. In this work, we extend previous findings by deriving the generalized multicontinuum poroelasticity model. We apply the recently developed multicontinuum homogenization method and provide a rigorous derivation of multicontinuum equations. For this purpose, we formulate coupled constraint cell problems in oversampled regions to consider different homogenized effects. Then, we obtain a multicontinuum expansion of the fine-scale fields and derive the multicontinuum model supposing the smoothness of macroscopic variables. We present the most general version of equations and the simplified ones based on our numerical experiments. Numerical results are presented for different heterogeneous media cases and demonstrate the high accuracy of our proposed multicontinuum models.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-guided Chemical Process Optimization with a Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2506.20921</link>
<guid>https://arxiv.org/abs/2506.20921</guid>
<content:encoded><![CDATA[
<div> framework, optimization, constraints, autonomous, process <br />
Summary: <br />
The article presents a novel approach to chemical process optimization using a multi-agent framework of large language model agents. This framework autonomously infers operating constraints from minimal process descriptions and collaboratively guides optimization. By employing OpenAI's o3 model, the AutoGen-based framework eliminates the need for predefined operational bounds, achieving better computational efficiency and competitive performance with conventional methods. Validated on the hydrodealkylation process, the framework demonstrated a 31-fold speedup over grid search, converging in under 20 minutes. The reasoning-guided search showcases sophisticated process understanding, correctly identifying utility trade-offs and applying domain-informed heuristics. This approach shows significant potential for optimization scenarios with poorly characterized or unavailable constraints, particularly in emerging processes and retrofit applications. <br /> <div>
arXiv:2506.20921v1 Announce Type: cross 
Abstract: Chemical process optimization is crucial to maximize production efficiency and economic performance. Traditional methods, including gradient-based solvers, evolutionary algorithms, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable, requiring engineers to rely on subjective heuristics to estimate feasible parameter ranges. To address this constraint definition bottleneck, we present a multi-agent framework of large language model (LLM) agents that autonomously infer operating constraints from minimal process descriptions, then collaboratively guide optimization using the inferred constraints. Our AutoGen-based agentic framework employs OpenAI's o3 model, with specialized agents for constraint generation, parameter validation, simulation execution, and optimization guidance. Through two phases - autonomous constraint generation using embedded domain knowledge, followed by iterative multi-agent optimization - the framework eliminates the need for predefined operational bounds. Validated on the hydrodealkylation process across cost, yield, and yield-to-cost ratio metrics, the framework demonstrated competitive performance with conventional optimization methods while achieving better computational efficiency, requiring fewer iterations to converge. Our approach converged in under 20 minutes, achieving a 31-fold speedup over grid search. Beyond computational efficiency, the framework's reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs, and applying domain-informed heuristics. This approach shows significant potential for optimization scenarios where operational constraints are poorly characterized or unavailable, particularly for emerging processes and retrofit applications.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty</title>
<link>https://arxiv.org/abs/2506.20992</link>
<guid>https://arxiv.org/abs/2506.20992</guid>
<content:encoded><![CDATA[
<div> blockchain, game theory, protocol mutability, institutional rules, cooperative mining

Summary:
Using a game-theoretic model, this paper examines the impact of protocol mutability on cooperative mining behavior in blockchain systems. It finds that uncertainty in institutional rules leads to higher discounting and strategic deviation among miners. Stable protocols support long-term investment and equilibrium strategies, while mutable protocols promote short-termism and collapse of cooperation. Simulation results highlight zones of instability where rational mining transitions to extractive practices. The study suggests that stable rules are crucial for productive action and sustainable cooperation in decentralized systems. Protocol design should be viewed as a fundamental economic constraint rather than a discretionary variable to foster sustainable cooperation. <div>
arXiv:2506.20992v1 Announce Type: cross 
Abstract: This paper develops a formal game-theoretic model to examine how protocol mutability disrupts cooperative mining behaviour in blockchain systems. Using a repeated game framework with stochastic rule shocks, we show that even minor uncertainty in institutional rules increases time preference and induces strategic deviation. Fixed-rule environments support long-term investment and stable equilibrium strategies; in contrast, mutable protocols lead to short-termism, higher discounting, and collapse of coordinated engagement. Simulation results identify instability zones in the parameter space where rational mining gives way to extractive or arbitrage conduct. These findings support an Austrian economic interpretation: calculability requires rule stability. Institutional noise undermines the informational basis for productive action. We conclude that protocol design must be treated as a constitutional economic constraint, not a discretionary variable, if sustainable cooperation is to emerge in decentralised systems.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Inverse Problem for Multi-armed Bandits via Convex Optimization</title>
<link>https://arxiv.org/abs/2501.18945</link>
<guid>https://arxiv.org/abs/2501.18945</guid>
<content:encoded><![CDATA[
<div> Keywords: IMAB, multi-armed bandits, convex optimization, heuristic method, neuroscience<br />
<br />
Summary: 
The article focuses on the inverse problem of multi-armed bandits (IMAB) and its application in neuroscience and psychology research. It demonstrates that the IMAB problem is not convex but can be transformed into a convex problem for easier solution. A two-step sequential heuristic method is proposed to approximately solve the IMAB problem efficiently. The method is shown to provide global solutions with a certificate under certain conditions and offers approximations to reduce computational time. Numerical experiments indicate that the proposed heuristic method outperforms traditional local optimization approaches and matches the performance of Monte Carlo methods in a shorter time frame. The method is implemented using CVXPY, making it accessible to users without expertise in convex optimization. <div>
arXiv:2501.18945v3 Announce Type: replace 
Abstract: We consider the inverse problem of multi-armed bandits (IMAB) that are widely used in neuroscience and psychology research for behavior modelling. We first show that the IMAB problem is not convex in general, but can be relaxed to a convex problem via variable transformation. Based on this result, we propose a two-step sequential heuristic for (approximately) solving the IMAB problem. We discuss a condition where our method provides global solution to the IMAB problem with certificate, as well as approximations to further save computing time. Numerical experiments indicate that our heuristic method is more robust than directly solving the IMAB problem via repeated local optimization, and can achieve the performance of Monte Carlo methods within a significantly decreased running time. We provide the implementation of our method based on CVXPY, which allows straightforward application by users not well versed in convex optimization.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Reinforcement Learning via Convex Optimization</title>
<link>https://arxiv.org/abs/2501.15957</link>
<guid>https://arxiv.org/abs/2501.15957</guid>
<content:encoded><![CDATA[
<div> Convex Optimization, Inverse Reinforcement Learning, Expert Demonstrations, Markov Decision Process, Auto-selection<br />
Summary: 
This article discusses the Inverse Reinforcement Learning (IRL) problem and introduces a convex formulation of the problem known as CIRL. The authors present a method to apply the domain-specific language CVXPY to solve the convex IRL problem, making it more accessible for users without a background in convex optimization. They also address scenarios where the expert policy is provided as state-action pairs rather than analytically, extending the CIRL problem to handle such cases. Theoretical analysis and practical implementation for hyperparameter auto-selection are discussed, allowing users to easily apply CIRL to their specific problems. This approach aims to overcome the challenges posed by traditional nonconvex IRL formulations, offering a more robust and reproducible solution for estimating unknown reward functions in Markov decision processes based on expert demonstrations. <div>
arXiv:2501.15957v2 Announce Type: replace-cross 
Abstract: We consider the inverse reinforcement learning (IRL) problem, where an unknown reward function of some Markov decision process is estimated based on observed expert demonstrations. In most existing approaches, IRL is formulated and solved as a nonconvex optimization problem, posing challenges in scenarios where robustness and reproducibility are critical. We discuss a convex formulation of the IRL problem (CIRL) initially proposed by Ng and Russel, and reformulate the problem such that the domain-specific language CVXPY can be applied directly to specify and solve the convex problem. We also extend the CIRL problem to scenarios where the expert policy is not given analytically but by trajectory as state-action pairs, which can be strongly inconsistent with optimality, by augmenting some of the constraints. Theoretical analysis and practical implementation for hyperparameter auto-selection are introduced. This note helps the users to easily apply CIRL for their problems, without background knowledge on convex optimization.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena Perspective</title>
<link>https://arxiv.org/abs/2503.18313</link>
<guid>https://arxiv.org/abs/2503.18313</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial decision-making, DeepFund, multi-agent framework, live environment

Summary:
DeepFund addresses the inadequately evaluated effectiveness of Large Language Models (LLMs) in financial decision-making by introducing a comprehensive arena platform for evaluating LLM-based trading strategies. The platform implements a multi-agent framework that simulates real-world investment decision processes, allowing LLMs to take on different key roles in managing assets and uncovering trading opportunities. DeepFund offers a web interface for visualizing LLMs' performance with fund investment metrics across various market conditions, enabling detailed comparative analysis. The platform aims to provide a more realistic and fair assessment of LLM capabilities in fund investment, addressing issues such as data leakage, navel-gazing, over-intervention, and maintenance-hardness identified in existing benchmarks. By offering diversified insights and potential applications in real-world financial markets, DeepFund contributes to advancing LLM research in financial decision-making. The code for DeepFund is publicly available on GitHub at https://github.com/HKUSTDial/DeepFund. 

<br /><br />Summary: <div>
arXiv:2503.18313v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision-making remains inadequately evaluated. Current benchmarks primarily assess LLMs' understanding on financial documents rather than the ability to manage assets or dig out trading opportunities in dynamic market conditions. Despite the release of new benchmarks for evaluating diversified tasks on the financial domain, we identified four major problems in these benchmarks, which are data leakage, navel-gazing, over-intervention, and maintenance-hard. To pave the research gap, we introduce DeepFund, a comprehensive arena platform for evaluating LLM-based trading strategies in a live environment. Our approach implements a multi-agent framework where they serve as multiple key roles that realize the real-world investment decision processes. Moreover, we provide a web interface that visualizes LLMs' performance with fund investment metrics across different market conditions, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more realistic and fair assessment on LLM's capabilities in fund investment, offering diversified insights and revealing their potential applications in real-world financial markets. Our code is publicly available at https://github.com/HKUSTDial/DeepFund.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-convex Programming for Discrete Latent Factor Models Prototyping</title>
<link>https://arxiv.org/abs/2504.01431</link>
<guid>https://arxiv.org/abs/2504.01431</guid>
<content:encoded><![CDATA[
<div> CVXPY, discrete latent factor models, fitting problem, regularization terms, constraints
Summary: 
Discrete latent factor models (DLFMs) are commonly used in various fields but require custom solvers for fitting, limiting their flexibility. This paper introduces a generic framework based on CVXPY that enables users to easily specify and solve the fitting problem of a range of DLFMs through a concise script. The framework supports both regression and classification models, integrates regularization terms, and allows for constraints on DLFM parameters and latent factors. This versatility allows users to prototype DLFM structures tailored to their dataset and application needs. The open-source Python implementation of the framework is demonstrated through several examples, showcasing its utility and effectiveness in practice. <div>
arXiv:2504.01431v2 Announce Type: replace-cross 
Abstract: Discrete latent factor models (DLFMs) are widely used in various domains such as machine learning, economics, neuroscience, psychology, etc. Currently, fitting a DLFM to some dataset relies on a customized solver for individual models, which requires lots of effort to implement and is limited to the targeted specific instance of DLFMs. In this paper, we propose a generic framework based on CVXPY, which allows users to specify and solve the fitting problem of a wide range of DLFMs, including both regression and classification models, within a very short script. Our framework is flexible and inherently supports the integration of regularization terms and constraints on the DLFM parameters and latent factors, such that the users can easily prototype the DLFM structure according to their dataset and application scenario. We introduce our open-source Python implementation and illustrate the framework in several examples.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiT-SGCR: Directed Temporal Structural Representation with Global-Cluster Awareness for Ethereum Malicious Account Detection</title>
<link>https://arxiv.org/abs/2506.20123</link>
<guid>https://arxiv.org/abs/2506.20123</guid>
<content:encoded><![CDATA[
<div> Keywords: Ethereum, DeFi, malicious account detection, temporal transaction evolution, unsupervised graph encoder

Summary: 
The article proposes DiT-SGCR, an unsupervised graph encoder designed to detect malicious accounts on the Ethereum platform, focusing on the dynamic transaction behaviors of users. By incorporating directional temporal aggregation, differentiable clustering, and graph Laplacian regularization, DiT-SGCR effectively captures money movement trajectories and organized collective behavior. This approach enhances the quality and dimensionality of account embeddings while maintaining scalability advantages. Through extensive experiments on three datasets, DiT-SGCR consistently outperforms existing methods in malicious account detection, showcasing significant improvements in F1-score accuracy ranging from 3.62% to 10.83%. The research highlights the importance of considering temporal dynamics, global topology, and cluster-specific behavioral patterns in identifying malicious actors within decentralized finance ecosystems. <br /><br />Summary: <div>
arXiv:2506.20123v1 Announce Type: new 
Abstract: The detection of malicious accounts on Ethereum - the preeminent DeFi platform - is critical for protecting digital assets and maintaining trust in decentralized finance. Recent advances highlight that temporal transaction evolution reveals more attack signatures than static graphs. However, current methods either fail to model continuous transaction dynamics or incur high computational costs that limit scalability to large-scale transaction networks. Furthermore, current methods fail to consider two higher-order behavioral fingerprints: (1) direction in temporal transaction flows, which encodes money movement trajectories, and (2) account clustering, which reveals coordinated behavior of organized malicious collectives. To address these challenges, we propose DiT-SGCR, an unsupervised graph encoder for malicious account detection. Specifically, DiT-SGCR employs directional temporal aggregation to capture dynamic account interactions, then coupled with differentiable clustering and graph Laplacian regularization to generate high-quality, low-dimensional embeddings. Our approach simultaneously encodes directional temporal dynamics, global topology, and cluster-specific behavioral patterns, thereby enhancing the discriminability and robustness of account representations. Furthermore, DiT-SGCR bypasses conventional graph propagation mechanisms, yielding significant scalability advantages. Extensive experiments on three datasets demonstrate that DiT-SGCR consistently outperforms state-of-the-art methods across all benchmarks, achieving F1-score improvements ranging from 3.62% to 10.83%.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing Artificial Mechanics Intuitions from Extremely Small Data</title>
<link>https://arxiv.org/abs/2506.20148</link>
<guid>https://arxiv.org/abs/2506.20148</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, mechanics intuitions, sample-switchable training, brachistochrone problem, nonlinear deformation problem

Summary:
The article discusses the development of artificial mechanics intuitions through a sample-switchable training method that allows learning from limited examples. The method enables artificial intelligence to master complex problems like the brachistochrone problem, catenary problem, and large nonlinear deformation problem of an elastic plate using only three samples. The model's intuitive prediction capacity improves non-linearly with the number of training samples, highlighting the possibility of achieving excellent mechanics intuitions with a finite set of examples. This approach offers a new perspective on educating AI systems to intuitively understand and predict material behavior, resembling scenarios often depicted in Science-Fiction movies.
<br /><br />Summary: The article explores the development of artificial mechanics intuitions using sample-switchable training, enabling AI to master complex problems with minimal data. The model's intuitive prediction improves with the number of training samples, demonstrating the potential to achieve exceptional mechanics intuition with a limited dataset. This work presents a novel approach to educating AI systems to intuitively predict material behavior, akin to scenarios in Science-Fiction films. <div>
arXiv:2506.20148v1 Announce Type: new 
Abstract: Humans can possess good mechanics intuitions by learning from a few examples, which leads to the question of how to develop artificial mechanics intuitions that can be learned from small data, as we are eagerly entering the era of artificial intelligence. We propose in this Letter the sample-switchable training method, which successfully develops highly-accurate artificial mechanics intuitions that can master brachistochrone problem, catenary problem, and large nonlinear deformation problem of elastic plate by learning from no more than three samples. The model's intuitive prediction ability increases nonlinearly with respect to the number of training samples, suggesting that superb mechanics intuitions can be in-principle achieved based on a finite number of samples, reflecting how human brains form good mechanics intuitions just by learning a few cases. Our current work presents an alternative perspective for educating artificial intelligence capable of intuitively understand and predict how materials deform and move, a scenario that has been frequently seen in Science-Fiction movies.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Dynamics with Highly Oscillating Opinions</title>
<link>https://arxiv.org/abs/2506.20472</link>
<guid>https://arxiv.org/abs/2506.20472</guid>
<content:encoded><![CDATA[
<div> Opinion Dynamics; Agent-Based Models; Evolutionary Algorithms; Immigration dataset; ATBCR <br />
Summary: <br />
Opinion Dynamics (OD) models focus on understanding how opinions evolve within a population through interactions between agents. This study addresses the limitations of existing OD models in analyzing scenarios of highly oscillating opinions, such as those found in real-world situations. The research formulates an optimization problem and applies Evolutionary Algorithms to evaluate the performance of various OD models in capturing highly oscillating dynamics. Using a real-world opinion dataset on immigration, the study finds that the ATBCR model, incorporating both rational and emotional opinion update mechanisms, is the most accurate for representing highly oscillating opinions. This research contributes to enhancing the understanding of opinion evolution in complex social contexts. <br /> <div>
arXiv:2506.20472v1 Announce Type: new 
Abstract: Opinion Dynamics (OD) models are a particular case of Agent-Based Models in which the evolution of opinions within a population is studied. In most OD models, opinions evolve as a consequence of interactions between agents, and the opinion fusion rule defines how those opinions are updated. In consequence, despite being simplistic, OD models provide an explainable and interpretable mechanism for understanding the underlying dynamics of opinion evolution. Unfortunately, existing OD models mainly focus on explaining the evolution of (usually synthetic) opinions towards consensus, fragmentation, or polarization, but they usually fail to analyze scenarios of (real-world) highly oscillating opinions. This work overcomes this limitation by studying the ability of several OD models to reproduce highly oscillating dynamics. To this end, we formulate an optimization problem which is further solved using Evolutionary Algorithms, providing both quantitative results on the performance of the optimization and qualitative interpretations on the obtained results. Our experiments on a real-world opinion dataset about immigration from the monthly barometer of the Spanish Sociological Research Center show that the ATBCR, based on both rational and emotional mechanisms of opinion update, is the most accurate OD model for capturing highly oscillating opinions.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ten simple rules for PIs to integrate Research Software Engineering into their research group</title>
<link>https://arxiv.org/abs/2506.20217</link>
<guid>https://arxiv.org/abs/2506.20217</guid>
<content:encoded><![CDATA[
<div> Research Software Engineering, high-quality research software, accessibility, PIs, reproducibility <br />
Summary:
Research Software Engineering (RSEng) plays a crucial role in producing high-quality research software, enhancing research outcomes. However, many principal investigators (PIs) and research group leaders may lack knowledge on RSEng and its benefits. The technical complexities of RSEng can also hinder accessibility for some researchers. To address these challenges, this paper presents ten simple rules to help PIs and leaders integrate RSEng into their research groups more effectively. By following these rules, researchers can improve the quality, reproducibility, and trustworthiness of their research software, ultimately leading to better, more reproducible, and trustworthy research outcomes. <div>
arXiv:2506.20217v1 Announce Type: cross 
Abstract: Research Software Engineering (RSEng) is a key success factor in producing high-quality research software, which in turn enables and improves research outcomes. However, as a principal investigator or leader of a research group you may not know what RSEng is, where to get started with it, or how to use it to maximize its benefit for your research. RSEng also often comes with technical complexity, and therefore reduced accessibility to some researchers. The ten simple rules presented in this paper aim to improve the accessibility of RSEng, and provide practical and actionable advice to PIs and leaders for integrating RSEng into their research group. By following these rules, readers can improve the quality, reproducibility, and trustworthiness of their research software, ultimately leading to better, more reproducible and more trustworthy research outcomes.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem</title>
<link>https://arxiv.org/abs/2506.20400</link>
<guid>https://arxiv.org/abs/2506.20400</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent-based simulations, electric vehicle home charging ecosystems, Python-based dashboard framework, emergent behavior, root-cause analysis

Summary:
This paper introduces a Python-based dashboard framework using Dash by Plotly for analyzing multi-agent-based simulations of electric vehicle home charging ecosystems. The framework consists of three coordinated views for exploring emergent behavior in the simulation outputs, including time-series plots, spatial heatmaps, and drill-down tools. A case study in a Danish residential network with full EV adoption and smart charging showcases the dashboard's ability to identify anomalies like transformer overloads and charging failures. The system enables quick detection and contextual explanation of system-level events that are difficult to detect through traditional post-processing methods. The dashboard provides actionable insights for researchers and distribution system operators, and its adaptable architecture can be used for analyzing other complex energy systems and distributed energy resources. 

<br /><br />Summary: <div>
arXiv:2506.20400v1 Announce Type: cross 
Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging ecosystems generate large, complex, and stochastic time-series datasets that capture interactions between households, grid infrastructure, and energy markets. These interactions can lead to unexpected system-level events, such as transformer overloads or consumer dissatisfaction, that are difficult to detect and explain through static post-processing. This paper presents a modular, Python-based dashboard framework, built using Dash by Plotly, that enables efficient, multi-level exploration and root-cause analysis of emergent behavior in MABS outputs. The system features three coordinated views (System Overview, System Analysis, and Consumer Analysis), each offering high-resolution visualizations such as time-series plots, spatial heatmaps, and agent-specific drill-down tools. A case study simulating full EV adoption with smart charging in a Danish residential network demonstrates how the dashboard supports rapid identification and contextual explanation of anomalies, including clustered transformer overloads and time-dependent charging failures. The framework facilitates actionable insight generation for researchers and distribution system operators, and its architecture is adaptable to other distributed energy resources and complex energy systems.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Company Adjustment Matter? Insights from Uplift Modeling on Financial Health</title>
<link>https://arxiv.org/abs/2506.19049</link>
<guid>https://arxiv.org/abs/2506.19049</guid>
<content:encoded><![CDATA[
<div> Keywords: uplift modeling, company adjustment, financial status, time-dependent actions, MTDnet

Summary:
Uplift modeling, using machine learning and deep learning, analyzes the impact of company adjustments on financial status by treating adjustments as interventions. Unlike simpler scenarios, company adjustments involve a series of time-dependent actions, necessitating the consideration of both individual treatment traits and temporal order. Real-world data from Luxembourg is used for experiments, comparing two meta-learners and three existing uplift models with a new framework called MTDnet. Results highlight the importance of timing in estimating the effects of company adjustments, demonstrating the need for models that can account for the time-dependent nature of these interventions.<br /><br />Summary: <div>
arXiv:2506.19049v1 Announce Type: new 
Abstract: Uplift modeling has achieved significant success in various fields, particularly in online marketing. It is a method that primarily utilizes machine learning and deep learning to estimate individual treatment effects. This paper we apply uplift modeling to analyze the effect of company adjustment on their financial status, and we treat these adjustment as treatments or interventions in this study. Although there have been extensive studies and application regarding binary treatments, multiple treatments, and continuous treatments, company adjustment are often more complex than these scenarios, as they constitute a series of multiple time-dependent actions. The effect estimation of company adjustment needs to take into account not only individual treatment traits but also the temporal order of this series of treatments. This study collects a real-world data set about company financial statements and reported behavior in Luxembourg for the experiments. First, we use two meta-learners and three other well-known uplift models to analyze different company adjustment by simplifying the adjustment as binary treatments. Furthermore, we propose a new uplift modeling framework (MTDnet) to address the time-dependent nature of these adjustment, and the experimental result shows the necessity of considering the timing of these adjustment.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LKA: Large Kernel Adapter for Enhanced Medical Image Classification</title>
<link>https://arxiv.org/abs/2506.19118</link>
<guid>https://arxiv.org/abs/2506.19118</guid>
<content:encoded><![CDATA[
<div> Keyword: Parameter-Efficient Fine-Tuning, Medical Image Analysis, Large Kernel Adapter, Receptive Field, State-of-the-art <br />
Summary:
A new approach called the Large Kernel Adapter (LKA) is introduced to improve Parameter-Efficient Fine-Tuning (PEFT) methods for medical image analysis. The challenge in medical datasets lies in the anatomical variation and low contrast, necessitating larger receptive fields. The LKA comprises down-projection, channel-wise large kernel convolution, and up-projection to enhance adaptation. It outperforms 11 existing PEFT methods and achieves a 3.5% higher top-1 accuracy across five medical datasets, surpassing the state-of-the-art. The key factors tackled include the need for larger receptive fields in medical images and the lack of explicit enhancement in existing PEFT methods. The incorporation of a larger kernel size proves pivotal in improving pre-trained model adaptation for medical image analysis. The proposed LKA demonstrates significant advancements in parameter efficiency and performance metrics for medical image datasets. <br /><br />Summary: <div>
arXiv:2506.19118v1 Announce Type: new 
Abstract: Despite the notable success of current Parameter-Efficient Fine-Tuning (PEFT) methods across various domains, their effectiveness on medical datasets falls short of expectations. This limitation arises from two key factors: (1) medical images exhibit extensive anatomical variation and low contrast, necessitating a large receptive field to capture critical features, and (2) existing PEFT methods do not explicitly address the enhancement of receptive fields. To overcome these challenges, we propose the Large Kernel Adapter (LKA), designed to expand the receptive field while maintaining parameter efficiency. The proposed LKA consists of three key components: down-projection, channel-wise large kernel convolution, and up-projection. Through extensive experiments on various datasets and pre-trained models, we demonstrate that the incorporation of a larger kernel size is pivotal in enhancing the adaptation of pre-trained models for medical image analysis. Our proposed LKA outperforms 11 commonly used PEFT methods, surpassing the state-of-the-art by 3.5% in top-1 accuracy across five medical datasets.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Industrial Gas Turbines: Recent Trends, Advancements and Challenges</title>
<link>https://arxiv.org/abs/2506.19503</link>
<guid>https://arxiv.org/abs/2506.19503</guid>
<content:encoded><![CDATA[
<div> PINNs, Industrial Gas Turbines, aerodynamics, aeromechanical, deep learning<br />
<br />
Summary: Physics-Informed Neural Networks (PINNs) are a computational framework combining deep learning with physical constraints to solve differential equations. This survey specifically focuses on their application in Industrial Gas Turbines (IGTs). PINNs have shown promise in analyzing aerodynamic and aeromechanical phenomena in gas turbines, as well as in tasks such as flow field reconstruction, fatigue evaluation, and flutter prediction. Recent advancements in accuracy, computational efficiency, and hybrid modelling strategies have been reviewed. The survey also discusses challenges in implementation and future research directions aimed at enhancing the robustness and scalability of PINNs in the context of IGTs. <div>
arXiv:2506.19503v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising computational framework for solving differential equations by integrating deep learning with physical constraints. However, their application in gas turbines is still in its early stages, requiring further refinement and standardization for wider adoption. This survey provides a comprehensive review of PINNs in Industrial Gas Turbines (IGTs) research, highlighting their contributions to the analysis of aerodynamic and aeromechanical phenomena, as well as their applications in flow field reconstruction, fatigue evaluation, and flutter prediction, and reviews recent advancements in accuracy, computational efficiency, and hybrid modelling strategies. In addition, it explores key research efforts, implementation challenges, and future directions aimed at improving the robustness and scalability of PINNs.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spline-Based Stress Function Approach for the Principle of Minimum Complementary Energy</title>
<link>https://arxiv.org/abs/2506.19534</link>
<guid>https://arxiv.org/abs/2506.19534</guid>
<content:encoded><![CDATA[
<div> spline-based stress function, computational engineering, finite element methods, stress prediction, structural analysis <br />
Summary:<br />
This article introduces a novel numerical approach for accurate stress prediction in computational engineering applications. The current methods have limitations in predicting stress accurately and efficiently for complex geometries and boundary conditions due to the high number of unknowns required. The proposed method is based on a spline-based stress function formulation for the principle of minimum complementary energy, applied to plane, linear elastostatics. It is validated against analytical solutions and tested on challenging test cases showing promising results. The method offers improved flexibility, accuracy, and efficiency in stress prediction for various geometries and boundary conditions. It achieves comparable stress accuracy to displacement-based finite element methods while requiring fewer degrees of freedom, making it a valuable tool for structural analysis and numerical design.<br /> <div>
arXiv:2506.19534v1 Announce Type: new 
Abstract: In computational engineering, ensuring the integrity and safety of structures in fields such as aerospace and civil engineering relies on accurate stress prediction. However, analytical methods are limited to simple test cases, and displacement-based finite element methods (FEMs), while commonly used, require a large number of unknowns to achieve high accuracy; stress-based numerical methods have so far failed to provide a simple and effective alternative. This work aims to develop a novel numerical approach that overcomes these limitations by enabling accurate stress prediction with improved flexibility for complex geometries and boundary conditions and fewer degrees of freedom (DOFs). The proposed method is based on a spline-based stress function formulation for the principle of minimum complementary energy, which we apply to plane, linear elastostatics. The method is first validated against an analytical power series solution and then tested on two test cases challenging for current state-of-the-art numerical schemes, a bi-layer cantilever with anisotropic material behavior, and a cantilever with a non-prismatic, parabolic-shaped beam geometry. Results demonstrate that our approach, unlike analytical methods, can be easily applied to general geometries and boundary conditions, and achieves stress accuracy comparable to that reported in the literature for displacement-based FEMs, while requiring significantly fewer DOFs. This novel spline-based stress function approach thus provides an efficient and flexible tool for accurate stress prediction, with promising applications in structural analysis and numerical design.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis</title>
<link>https://arxiv.org/abs/2506.19610</link>
<guid>https://arxiv.org/abs/2506.19610</guid>
<content:encoded><![CDATA[
<div> localization, multimodal techniques, medical visual question answering, vision to text chain-of-thought, reasoning pathways

Summary:
From Vision to Text Chain-of-Thought (V2T-CoT) is a new method for Medical Visual Question Answering (Med-VQA) that focuses on localizing disease-specific regions in biomedical images for more accurate diagnosis. V2T-CoT automates this localization process and integrates it into the reasoning pathway, balancing answer accuracy and clinical decision-making. By fine-tuning on the R-Med 39K dataset, V2T-CoT provides precise medical reasoning paths, combining visual grounding with textual rationale generation for explainable diagnostic results. Experimental results across four Med-VQA benchmarks show that V2T-CoT achieves state-of-the-art performance, improving both performance and interpretability significantly. <div>
arXiv:2506.19610v1 Announce Type: new 
Abstract: Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), a novel approach that automates the localization of preference areas within biomedical images and incorporates this localization into region-level pixel attention as knowledge for Vision CoT. By fine-tuning the vision language model on constructed R-Med 39K dataset, V2T-CoT provides definitive medical reasoning paths. V2T-CoT integrates visual grounding with textual rationale generation to establish precise and explainable diagnostic results. Experimental results across four Med-VQA benchmarks demonstrate state-of-the-art performance, achieving substantial improvements in both performance and interpretability.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReLink: Computational Circular Design of Planar Linkage Mechanisms Using Available Standard Parts</title>
<link>https://arxiv.org/abs/2506.19657</link>
<guid>https://arxiv.org/abs/2506.19657</guid>
<content:encoded><![CDATA[
<div> Circular Economy, sustainability, planar linkage mechanisms, generative design algorithm, CO2 footprint

Summary:
ReLink is a computational framework focusing on circular design for planar linkage mechanisms, promoting sustainability by reusing standardized parts. It consists of design generation and inverse design components, aiming to minimize the need for new parts and prioritize reuse. Trade-offs between kinematic performance and CO2 footprint are analyzed when incorporating new parts. The framework addresses challenges like the combinatorial nature of the design problem and ensuring valid solutions. By combining sustainability principles with kinematic synthesis, ReLink lays the foundation for further research in computational circular design, supporting the integration of reused components into mechanical products.<br /><br />Summary: <div>
arXiv:2506.19657v1 Announce Type: new 
Abstract: The Circular Economy framework emphasizes sustainability by reducing resource consumption and waste through the reuse of components and materials. This paper presents ReLink, a computational framework for the circular design of planar linkage mechanisms using available standard parts. Unlike most mechanism design methods, which assume the ability to create custom parts and infinite part availability, ReLink prioritizes the reuse of discrete, standardized components, thus minimizing the need for new parts. The framework consists of two main components: design generation, where a generative design algorithm generates mechanisms from an inventory of available parts, and inverse design, which uses optimization methods to identify designs that match a user-defined trajectory curve. The paper also examines the trade-offs between kinematic performance and CO2 footprint when incorporating new parts. Challenges such as the combinatorial nature of the design problem and the enforcement of valid solutions are addressed. By combining sustainability principles with kinematic synthesis, ReLink lays the groundwork for further research into computational circular design to support the development of systems that integrate reused components into mechanical products.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modular and extensible library for parameterized terrain generation</title>
<link>https://arxiv.org/abs/2506.19751</link>
<guid>https://arxiv.org/abs/2506.19751</guid>
<content:encoded><![CDATA[
<div> terrain generation, procedural, Python, simulation-driven, reproducibility <br />
<br />
The article introduces a modular Python library for procedural terrain generation, focusing on controllable and well-defined artificial terrains for simulation-driven development of intelligent machines. The library allows users to create complex, parameterized terrains by combining simple modules, supporting both structured and noise-based terrain elements. It integrates with Blender for rendering and object placement, making it suitable for applications like training machine learning models or perception tasks. The system prioritizes reproducibility, variation, and easy integration with automated pipelines by offering fine-grained control over terrain features like slope, roughness, and object placement. Overall, the library's minimal yet extensible set of modules enables users to achieve high flexibility in terrain generation while maintaining simplicity in configuration and expansion.<br /><br />Summary: <div>
arXiv:2506.19751v1 Announce Type: new 
Abstract: Simulation-driven development of intelligent machines benefits from artificial terrains with controllable, well-defined characteristics. However, most existing tools for terrain generation focus on artist-driven workflows and visual realism, with limited support for parameterization, reproducibility, or scripting. We present a modular, Python-based library for procedural terrain generation that enables users to construct complex, parameterized terrains by chaining together simple modules. The system supports both structured and noise-based terrain elements, and integrates with Blender for rendering and object placement. The framework is designed to support applications such as generating synthetic terrains for training machine learning models or producing ground truth for perception tasks. By using a minimal but extensible set of modules, the system achieves high flexibility while remaining easy to configure and expand. We demonstrate that this enables fine-grained control over features such as slope, roughness, and the number of rocks, as well as extension to additional measures. This makes it well suited for workflows that demand reproducibility, variation, and integration with automated pipelines.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate identification of communication between multiple interacting neural populations</title>
<link>https://arxiv.org/abs/2506.19094</link>
<guid>https://arxiv.org/abs/2506.19094</guid>
<content:encoded><![CDATA[
<div> variational autoencoder, neural recording, brain regions, inter-regional communication, dynamical systems
Summary:
Variational autoencoder MR-LFADS is introduced for disentangling communication between brain regions, unobserved inputs, and local neural dynamics. It outperforms existing models in identifying inter-regional communication in simulations and predicts brain-wide effects of circuit perturbations in electrophysiology data. MR-LFADS shows promise in uncovering brain-wide information processing principles. <div>
arXiv:2506.19094v1 Announce Type: cross 
Abstract: Neural recording technologies now enable simultaneous recording of population activity across many brain regions, motivating the development of data-driven models of communication between brain regions. However, existing models can struggle to disentangle the sources that influence recorded neural populations, leading to inaccurate portraits of inter-regional communication. Here, we introduce Multi-Region Latent Factor Analysis via Dynamical Systems (MR-LFADS), a sequential variational autoencoder designed to disentangle inter-regional communication, inputs from unobserved regions, and local neural population dynamics. We show that MR-LFADS outperforms existing approaches at identifying communication across dozens of simulations of task-trained multi-region networks. When applied to large-scale electrophysiology, MR-LFADS predicts brain-wide effects of circuit perturbations that were held out during model fitting. These validations on synthetic and real neural data position MR-LFADS as a promising tool for discovering principles of brain-wide information processing.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research</title>
<link>https://arxiv.org/abs/2412.02065</link>
<guid>https://arxiv.org/abs/2412.02065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data access, democratization, data collection, corporate disclosures

Summary: 
Large Language Models (LLMs) have the potential to democratize access to costly datasets essential for research by automating data collection from corporate disclosures. A novel methodology using GPT-4o-mini in a Retrieval-Augmented Generation (RAG) framework successfully collected CEO pay ratios from 10,000 proxy statements and Critical Audit Matters (CAMs) from over 12,000 10-K filings with human-level accuracy. The LLM processing times were notably efficient at 9 and 40 minutes respectively, with minimal costs under $10. This approach contrasts significantly with manual collection methods requiring hundreds of hours or expensive commercial database subscriptions. By sharing their methodology and resulting datasets, the researchers aim to empower those with limited resources to engage in research and foster a more inclusive research community. 

<br /><br />Summary: <div>
arXiv:2412.02065v2 Announce Type: replace-cross 
Abstract: Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear optimals and their role in sustaining turbulence in channel flow</title>
<link>https://arxiv.org/abs/2503.08283</link>
<guid>https://arxiv.org/abs/2503.08283</guid>
<content:encoded><![CDATA[
<div> investigation, energy transfer, channel flow, optimal disturbances, turbulence

Summary:
The study focuses on the energy transfer in channel flow by analyzing nonlinear optimal disturbances that maximize energy growth over a fixed time period. Results indicate that these disturbances exhibit streak spacing and amplitude consistent with Direct Numerical Simulation (DNS) data at Re_tau = 180, suggesting they capture key mechanisms sustaining turbulence. Additionally, the time horizon for nonlinear disturbances to surpass linear optimal disturbances aligns with estimates based on eddy turnover time from previous DNS studies. This finding sheds light on the determination of turbulent time scales and highlights the potential of nonlinear disturbances in understanding turbulence dynamics. Overall, the research provides valuable insights into the energy dynamics and mechanisms underlying turbulence in channel flow. 

<br /><br />Summary: <div>
arXiv:2503.08283v2 Announce Type: replace-cross 
Abstract: We investigate the energy transfer from the mean profile to velocity fluctuations in channel flow by calculating nonlinear optimal disturbances,i.e. the initial condition of a given finite energy that achieves the highest possible energy growth during a given fixed time horizon. It is found that for a large range of time horizons and initial disturbance energies, the nonlinear optimal exhibits streak spacing and amplitude consistent with DNS at least at Re_tau = 180, which suggests that they isolate the relevant physical mechanisms that sustain turbulence. Moreover, the time horizon necessary for a nonlinear disturbance to outperform a linear optimal is consistent with previous DNS-based estimates using eddy turnover time, which offers a new perspective on how some turbulent time scales are determined.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Equilibrium and Kinetics Prediction with a Data-Weighted Neural Network Model of Methane Steam Reforming</title>
<link>https://arxiv.org/abs/2506.17224</link>
<guid>https://arxiv.org/abs/2506.17224</guid>
<content:encoded><![CDATA[
<div> neural network, methane steam reforming, process optimization, surrogate model, energy carrier  
Summary:  
- A surrogate model was developed using an artificial neural network to simulate methane steam reforming for hydrogen production.  
- The model successfully unified kinetic and equilibrium regimes by training on a comprehensive dataset including experimental, interpolated, and theoretical data.  
- Data augmentation and weighted training improved the model's accuracy in predicting the composition of post-reaction mixtures.  
- The surrogate model demonstrated a mean squared error of 0.000498 and strong Pearson correlation coefficients of 0.927, indicating high predictive accuracy.  
- The model's capability to provide continuous derivatives of its predictions makes it useful for process modeling and optimization.  
- Overall, the surrogate model proves robust for simulating methane steam reforming, offering a valuable tool for design and process optimization.  

<br /><br />Summary: <div>
arXiv:2506.17224v1 Announce Type: new 
Abstract: Hydrogen's role is growing as an energy carrier, increasing the need for efficient production, with methane steam reforming being the most widely used technique. This process is crucial for applications like fuel cells, where hydrogen is converted into electricity, pushing for reactor miniaturization and optimized process control through numerical simulations. Existing models typically address either kinetic or equilibrium regimes, limiting their applicability. Here we show a surrogate model capable of unifying both regimes. An artificial neural network trained on a comprehensive dataset that includes experimental data from kinetic and equilibrium experiments, interpolated data, and theoretical data derived from theoretical models for each regime. Data augmentation and assigning appropriate weights to each data type enhanced training. After evaluating Bayesian Optimization and Random Sampling, the optimal model demonstrated high predictive accuracy for the composition of the post-reaction mixture under varying operating parameters, indicated by a mean squared error of 0.000498 and strong Pearson correlation coefficients of 0.927. The network's ability to provide continuous derivatives of its predictions makes it particularly useful for process modeling and optimization. The results confirm the surrogate model's robustness for simulating methane steam reforming in both kinetic and equilibrium regimes, making it a valuable tool for design and process optimization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Quantum Latent Encoding for Topology Optimization</title>
<link>https://arxiv.org/abs/2506.17487</link>
<guid>https://arxiv.org/abs/2506.17487</guid>
<content:encoded><![CDATA[
<div> neural decoding, quantum encoding, classical encoding, variational framework, topology optimization

Summary:<br />
The article introduces a variational framework for structural topology optimization that combines quantum and classical latent encoding strategies within a coordinate-based neural decoding architecture. A low-dimensional latent vector is generated through either a variational quantum circuit or Gaussian distribution and mapped to a higher-dimensional latent space via a learnable projection layer. The enriched representation is decoded into a high-resolution material distribution using a neural network that takes both the latent vector and spatial coordinates as input. Optimization is performed on the latent parameters guided by physics-based objectives such as compliance minimization and volume constraints evaluated through finite element analysis. Quantum latent vectors constructed from measured Pauli observables provide an entangled encoding. The variational formulation enables the generation of diverse and valid topologies by exploring the latent space through sampling or perturbation. Numerical experiments show that both quantum and classical encodings produce high-quality structural designs, with quantum encoding showing advantages in compliance and design diversity. This suggests the potential of quantum circuits in physics-constrained topology optimization using near-term quantum hardware.<br /> <div>
arXiv:2506.17487v1 Announce Type: new 
Abstract: A variational framework for structural topology optimization is developed, integrating quantum and classical latent encoding strategies within a coordinate-based neural decoding architecture. In this approach, a low-dimensional latent vector, generated either by a variational quantum circuit or sampled from a Gaussian distribution, is mapped to a higher-dimensional latent space via a learnable projection layer. This enriched representation is then decoded into a high-resolution material distribution using a neural network that takes both the latent vector and Fourier-mapped spatial coordinates as input. The optimization is performed directly on the latent parameters, guided solely by physics-based objectives such as compliance minimization and volume constraints evaluated through finite element analysis, without requiring any precomputed datasets or supervised training. Quantum latent vectors are constructed from the expectation values of Pauli observables measured on parameterized quantum circuits, providing a structured and entangled encoding of information. The classical baseline uses Gaussian-sampled latent vectors projected in the same manner. The proposed variational formulation enables the generation of diverse and physically valid topologies by exploring the latent space through sampling or perturbation, in contrast to traditional optimization methods that yield a single deterministic solution. Numerical experiments show that both classical and quantum encodings produce high-quality structural designs. However, quantum encodings demonstrate advantages in several benchmark cases in terms of compliance and design diversity. These results highlight the potential of quantum circuits as an effective and scalable tool for physics-constrained topology optimization and suggest promising directions for applying near-term quantum hardware in structural design.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A predictor-corrector scheme for approximating signed distances using finite element methods</title>
<link>https://arxiv.org/abs/2506.17830</link>
<guid>https://arxiv.org/abs/2506.17830</guid>
<content:encoded><![CDATA[
<div> prediction-correction approach, signed distance functions, finite element method, Eikonal equation, interfaces

Summary:
The article introduces a finite element method for computing approximate signed distance functions to arbitrary boundaries in 2D and 3D. The method utilizes a prediction-correction approach involving a linear diffusion-based prediction step and a nonlinear minimization-based correction step related to the Eikonal equation. This approach efficiently generates an initial guess for complex level set functions and facilitates convergence. The method can handle complex interfaces and level set functions with steep or flat regions, overcoming challenges faced by existing techniques. Through various examples, including classical geometries, star domains, and 3D tori, the method demonstrates accuracy, efficiency, and robustness in reinitializing diverse level set functions. <div>
arXiv:2506.17830v1 Announce Type: new 
Abstract: In this article, we introduce a finite element method designed for the robust computation of approximate signed distance functions to arbitrary boundaries in two and three dimensions. Our method employs a novel prediction-correction approach, involving first the solution of a linear diffusion-based prediction problem, followed by a nonlinear minimization-based correction problem associated with the Eikonal equation. The prediction step efficiently generates a suitable initial guess, significantly facilitating convergence of the nonlinear correction step. A key strength of our approach is its ability to handle complex interfaces and initial level set functions with arbitrary steep or flat regions, a notable challenge for existing techniques. Through several representative examples, including classical geometries and more complex shapes such as star domains and three-dimensional tori, we demonstrate the accuracy, efficiency, and robustness of the method, validating its broad applicability for reinitializing diverse level set functions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from the Storm: A Multivariate Machine Learning Approach to Predicting Hurricane-Induced Economic Losses</title>
<link>https://arxiv.org/abs/2506.17964</link>
<guid>https://arxiv.org/abs/2506.17964</guid>
<content:encoded><![CDATA[
<div> Keywords: hurricanes, economic loss, modeling framework, machine learning, insurance claims 

Summary:
Hurricanes in Florida result in substantial economic losses, prompting the need for a comprehensive framework to assess contributing factors. This study introduces a modeling framework categorizing factors into hurricane characteristics, water-related environmental factors, and socioeconomic factors at the ZIP Code Tabulation Area level. By utilizing machine learning models and insurance claims as indicators, the approach accurately predicts economic loss and evaluates the importance of each component. The findings offer valuable insights for disaster mitigation, risk assessment, and urban strategies in storm-exposed areas. The code for this research is now publicly available to facilitate further study and application in disaster management.  <br /><br />Summary: <div>
arXiv:2506.17964v1 Announce Type: new 
Abstract: Florida is particularly vulnerable to hurricanes, which frequently cause substantial economic losses. While prior studies have explored specific contributors to hurricane-induced damage, few have developed a unified framework capable of integrating a broader range of influencing factors to comprehensively assess the sources of economic loss. In this study, we propose a comprehensive modeling framework that categorizes contributing factors into three key components: (1) hurricane characteristics, (2) water-related environmental factors, and (3) socioeconomic factors of affected areas. By integrating multi-source data and aggregating all variables at the finer spatial granularity of the ZIP Code Tabulation Area (ZCTA) level, we employ machine learning models to predict economic loss, using insurance claims as indicators of incurred damage. Beyond accurate loss prediction, our approach facilitates a systematic assessment of the relative importance of each component, providing practical guidance for disaster mitigation, risk assessment, and the development of adaptive urban strategies in coastal and storm-exposed areas. Our code is now available at: https://github.com/LabRAI/Hurricane-Induced-Economic-Loss-Prediction
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A phase field model for hydraulic fracture: Drucker-Prager driving force and a hybrid coupling strategy</title>
<link>https://arxiv.org/abs/2506.18161</link>
<guid>https://arxiv.org/abs/2506.18161</guid>
<content:encoded><![CDATA[
<div> phase field approach, hydraulic fracture, coupling, Drucker-Prager, simulation

Summary: 
This study introduces a novel phase field framework for simulating hydraulic fracture, with a focus on improving the coupling between fluid flow and the phase field and incorporating a universal fracture driving force. Two key innovations are presented: a hybrid coupling approach for enhanced accuracy and flexibility in handling the fracture-fluid flow interplay, and a Drucker-Prager-based strain energy decomposition to model materials with asymmetric tension-compression fracture behavior. The framework is applied to four case studies to demonstrate its capabilities in addressing permeability coupling, cracking behavior, and multiaxial conditions in hydraulic fracturing simulations. The developed codes are freely available for download, offering valuable resources to the community for further research in this field. <div>
arXiv:2506.18161v1 Announce Type: new 
Abstract: Recent years have seen a significant interest in using phase field approaches to model hydraulic fracture, so as to optimise a process that is key to industries such as petroleum engineering, mining and geothermal energy extraction. Here, we present a novel theoretical and computational phase field framework to simulate hydraulic fracture. The framework is general and versatile, in that it allows for improved treatments of the coupling between fluid flow and the phase field, and encompasses a universal description of the fracture driving force. Among others, this allows us to bring two innovations to the phase field hydraulic fracture community: (i) a new hybrid coupling approach to handle the fracture-fluid flow interplay, offering enhanced accuracy and flexibility; and (ii) a Drucker-Prager-based strain energy decomposition, extending the simulation of hydraulic fracture to materials exhibiting asymmetric tension-compression fracture behaviour (such as shale rocks) and enabling the prediction of geomechanical phenomena such as fault reactivation and stick-slip behaviour. Four case studies are addressed to illustrate these additional modelling capabilities and bring insight into permeability coupling, cracking behaviour, and multiaxial conditions in hydraulic fracturing simulations. The codes developed are made freely available to the community and can be downloaded from {https://mechmat.web.ox.ac.uk/
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Fractal Dimension using Discrete Global Grid Systems</title>
<link>https://arxiv.org/abs/2506.18175</link>
<guid>https://arxiv.org/abs/2506.18175</guid>
<content:encoded><![CDATA[
<div> fractal dimension, Discrete Global Grid System, Minkowski-Bouligand dimension, geospatial vector data, satellite images <br />
Summary: This study explores the relationship between fractal dimension and Discrete Global Grid Systems (DGGS), used for geospatial vector data analysis. By applying the method to synthetic data and opaque cloud fields from satellite images, the results closely match theoretical fractal dimensions. The use of DGGSs addresses issues such as arbitrary grid placement, orientation, and cell size progression in geospatial data analysis. DGGSs also account for the curvature of the Earth when calculating intersections with large geographic extents. The paper validates the use of DGGSs as covering sets and highlights desirable properties for effective application in geospatial analysis. <br /> <div>
arXiv:2506.18175v1 Announce Type: new 
Abstract: This study builds a bridge between two well-studied but distant topics: fractal dimension and Discrete Global Grid System (DGGS). DGGSs are used as covering sets for geospatial vector data to calculate the Minkowski-Bouligand dimension. Using the method on synthetic data yields results within 1% of their theoretical fractal dimensions. A case study on opaque cloud fields obtained from satellite images gives fractal dimension in agreement with that available in the literature. The proposed method alleviates the problems of arbitrary grid placement and orientation, as well as the progression of cell sizes of the covering sets for geospatial data. Using DGGSs further ensure that intersections of the covering sets with the geospatial vector having large geographic extents are calculated by taking the curvature of the earth into account. This paper establishes the validity of DGGSs as covering sets theoretically and discusses desirable properties of DGGSs suitable for this purpose.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conservative data-driven finite element formulation</title>
<link>https://arxiv.org/abs/2506.18206</link>
<guid>https://arxiv.org/abs/2506.18206</guid>
<content:encoded><![CDATA[
<div> finite element framework, mixed finite element formulation, data-driven approach, conservation law, numerical simulations<br />
<br />
Summary:<br />
This paper introduces a data-driven finite element framework using a mixed finite element formulation for diffusion problems. Unlike traditional methods that rely on fitting experimental data to material models, this approach directly utilizes experimental data in the simulations without the need for parameter fitting. By satisfying conservation laws through the finite element method and enforcing continuity of flux components, a mixed formulation is introduced to handle uncertainties in datasets and predict solution non-uniqueness. Error indicators tailored for data-driven approaches allow for adaptive refinement. An example of nonlinear heat transfer in nuclear graphite demonstrates the formulation's capabilities using synthetically generated datasets to showcase its effectiveness in predicting uncertainties and providing accurate results. <div>
arXiv:2506.18206v1 Announce Type: new 
Abstract: This paper presents a new data-driven finite element framework derived with mixed finite element formulation. The standard approach to diffusion problems requires the solution of the mathematical equations that describe both the conservation law and the constitutive relations, where the latter is traditionally obtained after fitting experimental data to simplified material models. To exploit all available information and avoid bias in the material model, we follow a data-driven approach. While the conservation laws and boundary conditions are satisfied by means of the finite element method, the experimental data is used directly in the numerical simulations, avoiding the need of fitting material model parameters. In order to satisfy the conservation law a priori in the strong sense, we introduce a mixed finite element formulation. This relaxes the regularity requirements on approximation spaces while enforcing continuity of the normal flux component across all of the inner boundaries. This weaker mixed formulation provides a posteriori error indicators tailored for this data-driven approach, enabling adaptive hp-refinement. The relaxed regularity of the approximation spaces makes it easier to observe how the variation in the datasets results in the non-uniqueness of the solution, which can be quantified to predict the uncertainty of the results. The capabilities of the formulation are demonstrated in an example of the nonlinear heat transfer in nuclear graphite using synthetically generated material datasets.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Conditional Score-Guided Generative Modeling for Amortized Inference in Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2506.18227</link>
<guid>https://arxiv.org/abs/2506.18227</guid>
<content:encoded><![CDATA[
<div> conditional inference, amortized, diffusion models, generative model, neural network

Summary:
The article introduces a framework for efficient conditional inference by utilizing exact conditional score-guided diffusion models to train a non-reversible neural network as a generative model. Traditional normalizing flow methods typically require reversible architectures, limiting their expressiveness and efficiency. Leveraging a two-stage method, the authors first create a training-free conditional diffusion model with an exact score function derived under a Gaussian mixture prior. This allows for the generation of noise-labeled data efficiently. This data is then used to train a feedforward neural network that directly maps noise and observations to posterior samples, eliminating the need for reversibility or iterative sampling during inference. The resulting model offers fast, accurate, and scalable conditional sampling for high-dimensional and multi-modal posterior distributions, making it suitable for uncertainty quantification tasks such as parameter estimation for complex physical systems. The effectiveness of the approach is demonstrated through numerical experiments. 

<br /><br />Summary: <div>
arXiv:2506.18227v1 Announce Type: new 
Abstract: We propose an efficient framework for amortized conditional inference by leveraging exact conditional score-guided diffusion models to train a non-reversible neural network as a conditional generative model. Traditional normalizing flow methods require reversible architectures, which can limit their expressiveness and efficiency. Although diffusion models offer greater flexibility, they often suffer from high computational costs during inference. To combine the strengths of both approaches, we introduce a two-stage method. First, we construct a training-free conditional diffusion model by analytically deriving an exact score function under a Gaussian mixture prior formed from samples of the underlying joint distribution. This exact conditional score model allows us to efficiently generate noise-labeled data, consisting of initial diffusion Gaussian noise and posterior samples conditioned on various observation values, by solving a reverse-time ordinary differential equation. Second, we use this noise-labeled data to train a feedforward neural network that maps noise and observations directly to posterior samples, eliminating the need for reversibility or iterative sampling at inference time. The resulting model provides fast, accurate, and scalable conditional sampling for high-dimensional and multi-modal posterior distributions, making it well-suited for uncertainty quantification tasks, e.g., parameter estimation of complex physical systems. We demonstrate the effectiveness of our approach through a series of numerical experiments.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural-operator element method: Efficient and scalable finite element method enabled by reusable neural operators</title>
<link>https://arxiv.org/abs/2506.18427</link>
<guid>https://arxiv.org/abs/2506.18427</guid>
<content:encoded><![CDATA[
<div> neural-operator element method, finite element method, machine learning, PDEs, multiscale simulations 
Summary: 
The neural-operator element method (NOEM) combines the finite element method (FEM) with neural operators to efficiently solve partial differential equations (PDEs) without the need for dense meshing. By using neural operators to simulate subdomains where FEM would require many elements, NOEM reduces computational costs. Each subdomain is represented by a neural-operator element (NOE), which is integrated with standard finite elements to achieve accurate and scalable solutions. This approach addresses the challenges of high training costs and low model reusability associated with machine learning-based methods for PDEs. Extensive numerical experiments demonstrate the accuracy, efficiency, and scalability of NOEM across various scenarios, including nonlinear PDEs, multiscale problems, complex geometries, and discontinuous coefficient fields. <br /><br />Summary: <div>
arXiv:2506.18427v1 Announce Type: new 
Abstract: The finite element method (FEM) is a well-established numerical method for solving partial differential equations (PDEs). However, its mesh-based nature gives rise to substantial computational costs, especially for complex multiscale simulations. Emerging machine learning-based methods (e.g., neural operators) provide data-driven solutions to PDEs, yet they present challenges, including high training cost and low model reusability. Here, we propose the neural-operator element method (NOEM) by synergistically combining FEM with operator learning to address these challenges. NOEM leverages neural operators (NOs) to simulate subdomains where a large number of finite elements would be required if FEM was used. In each subdomain, an NO is used to build a single element, namely a neural-operator element (NOE). NOEs are then integrated with standard finite elements to represent the entire solution through the variational framework. Thereby, NOEM does not necessitate dense meshing and offers efficient simulations. We demonstrate the accuracy, efficiency, and scalability of NOEM by performing extensive and systematic numerical experiments, including nonlinear PDEs, multiscale problems, PDEs on complex geometries, and discontinuous coefficient fields.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual failure assessment diagrams for hydrogen transmission pipelines</title>
<link>https://arxiv.org/abs/2506.18554</link>
<guid>https://arxiv.org/abs/2506.18554</guid>
<content:encoded><![CDATA[
<div> Thermo-metallurgical welding, diffusion-elastic-plastic phase field fracture, hydrogen transport pipelines, residual stress, Virtual Failure Assessment Diagrams <br />
Summary: <br />
The study combines advanced welding process modeling with fracture simulations to predict failure states in hydrogen transport pipelines. By considering factors such as residual stress, material properties, and hydrogen purity, failure pressures can be efficiently quantified. Virtual Failure Assessment Diagrams are created to facilitate the assessment of asset fitness under various conditions. The model's predictions align well with industry standards but highlight the need for a more mechanistic approach to account for the heterogeneous weld microstructure. Safety factors are established to address residual stresses and brittle weld regions, ensuring a more accurate assessment of pipeline integrity and safety. <div>
arXiv:2506.18554v1 Announce Type: new 
Abstract: We combine state-of-the-art thermo-metallurgical welding process modelling with coupled diffusion-elastic-plastic phase field fracture simulations to predict the failure states of hydrogen transport pipelines. This enables quantitatively resolving residual stress states and the role of brittle, hard regions of the weld such as the heat affected zone (HAZ). Failure pressures can be efficiently quantified as a function of asset state (existing defects), materials and weld procedures adopted, and hydrogen purity. Importantly, simulations spanning numerous relevant conditions (defect size and orientations) are used to build \emph{Virtual} Failure Assessment Diagrams (FADs), enabling a straightforward uptake of this mechanistic approach in fitness-for-service assessment. Model predictions are in very good agreement with FAD approaches from the standards but show that the latter are not conservative when resolving the heterogeneous nature of the weld microstructure. Appropriate, \emph{mechanistic} FAD safety factors are established that account for the role of residual stresses and hard, brittle weld regions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Informed Neural Network Framework for Simulating Creep Buckling in Growing Viscoelastic Biological Tissues</title>
<link>https://arxiv.org/abs/2506.18565</link>
<guid>https://arxiv.org/abs/2506.18565</guid>
<content:encoded><![CDATA[
<div> framework, viscoelastic behavior, physics-informed neural network, stress relaxation, morphogenesis <br />
Summary: <br />
This study presents a novel energy-based physics-informed neural network (PINN) framework for modeling viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. The framework eliminates the need for explicit meshing or custom programs, simplifying computational complexity. By training neural networks to minimize potential energy functional, the framework ensures physics consistency, capturing creep buckling and predicting tissue growth with high accuracy. Results show the framework can predict viscoelastic instabilities, post-buckling evolution, and tissue morphological changes effectively. This versatile tool offers a promising alternative to traditional methods and can find applications in structural engineering, soft materials, and tissue development. <div>
arXiv:2506.18565v1 Announce Type: new 
Abstract: Modeling viscoelastic behavior is crucial in engineering and biomechanics, where materials undergo time-dependent deformations, including stress relaxation, creep buckling and biological tissue development. Traditional numerical methods, like the finite element method, often require explicit meshing, artificial perturbations or embedding customised programs to capture these phenomena, adding computational complexity. In this study, we develop an energy-based physics-informed neural network (PINN) framework using an incremental approach to model viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. Physics consistency is ensured by training neural networks to minimize the systems potential energy functional, implicitly satisfying equilibrium and constitutive laws. We demonstrate that this framework can naturally capture creep buckling without pre-imposed imperfections, leveraging inherent training dynamics to trigger instabilities. Furthermore, we extend our framework to biological tissue growth and morphogenesis, predicting both uniform expansion and differential growth-induced buckling in cylindrical structures. Results show that the energy-based PINN effectively predicts viscoelastic instabilities, post-buckling evolution and tissue morphological evolution, offering a promising alternative to traditional methods. This study demonstrates that PINN can be a flexible robust tool for modeling complex, time-dependent material behavior, opening possible applications in structural engineering, soft materials, and tissue development.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication Architecture for Autonomous Power-to-X Platforms: Enhancing Inspection and Operation With Legged Robots and 5G</title>
<link>https://arxiv.org/abs/2506.18572</link>
<guid>https://arxiv.org/abs/2506.18572</guid>
<content:encoded><![CDATA[
<div> Keywords: Power to X platforms, communication architecture, robotic system, inspection, 5G network

Summary: 
Power to X platforms are classified, and a communication architecture is proposed for monitoring, control, and teleoperation. A robotic system using a quadruped robot is integrated to autonomously perform inspection and maintenance tasks, reducing the need for human labor. The implementation considers a 5G standalone network for remote monitoring, control, and teleoperation of the robot. Evaluation includes recording and comparison of aspects such as availability and latency within this network. <div>
arXiv:2506.18572v1 Announce Type: new 
Abstract: Inspection and maintenance of offshore platforms are associated with high costs, primarily due to the significant personnel requirements and challenging operational conditions. This paper first presents a classification of Power to X platforms. Building upon this foundation, a communication architecture is proposed to enable monitoring, control, and teleoperation for a Power to X platform. To reduce the demand for human labor, a robotic system is integrated to autonomously perform inspection and maintenance tasks. The implementation utilizes a quadruped robot. Remote monitoring, control, and teleoperation of the robot are analyzed within the context of a 5G standalone network. As part of the evaluation, aspects such as availability and latency are recorded, compared, and critically assessed.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Dynamic Stock Relationship Modeling and S&amp;P500 Price Forecasting Based on Differential Graph Transformer</title>
<link>https://arxiv.org/abs/2506.18717</link>
<guid>https://arxiv.org/abs/2506.18717</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock price prediction, Differential Graph Transformer, Dynamic relationship modeling, Temporal attention, Correlation metrics

Summary:
The study introduces the Differential Graph Transformer (DGT) framework for dynamic stock price prediction by capturing evolving stock relationships. DGT utilizes differential graph mechanisms and causal temporal attention to model global and local dependencies in price sequences. The framework incorporates correlation metrics (Pearson, Mutual Information, Spearman, Kendall's Tau) across different scopes as spatial-attention priors, improving prediction accuracy over traditional models. Using 10 years of S&amp;P 500 closing prices, DGT with spatial priors outperformed GRU baselines, with optimal results obtained using Kendall's Tau global matrices. Clustering analysis identified distinct stock clusters such as "high-volatility growth" and "defensive blue-chip," with each exhibiting varying prediction errors based on correlation stability. The study highlights the importance of dynamic modeling and optimal correlation metrics in enhancing financial time-series prediction and quantitative investment strategies.

Summary: <br />Keywords: Stock price prediction, Differential Graph Transformer, Dynamic relationship modeling, Temporal attention, Correlation metrics <div>
arXiv:2506.18717v1 Announce Type: new 
Abstract: Stock price prediction is vital for investment decisions and risk management, yet remains challenging due to markets' nonlinear dynamics and time-varying inter-stock correlations. Traditional static-correlation models fail to capture evolving stock relationships. To address this, we propose a Differential Graph Transformer (DGT) framework for dynamic relationship modeling and price prediction. Our DGT integrates sequential graph structure changes into multi-head self-attention via a differential graph mechanism, adaptively preserving high-value connections while suppressing noise. Causal temporal attention captures global/local dependencies in price sequences. We further evaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's Tau) across global/local/dual scopes as spatial-attention priors. Using 10 years of S&amp;P 500 closing prices (z-score normalized; 64-day sliding windows), DGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87). Kendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means clustering revealed "high-volatility growth" and "defensive blue-chip" stocks, with the latter showing lower errors (RMSE: 0.13) due to stable correlations. Kendall's Tau and Mutual Information excelled in volatile sectors. This study innovatively combines differential graph structures with Transformers, validating dynamic relationship modeling and identifying optimal correlation metrics/scopes. Clustering analysis supports tailored quantitative strategies. Our framework advances financial time-series prediction through dynamic modeling and cross-asset interaction analysis.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Real-time Structural Dynamics Simulation with Graph-based Digital Twin Modelling</title>
<link>https://arxiv.org/abs/2506.18724</link>
<guid>https://arxiv.org/abs/2506.18724</guid>
<content:encoded><![CDATA[
<div> Graph-Based Digital Twin Modelling, Structural Dynamics, Computational Efficiency, Health Monitoring, Spatial Topologies
<br />
Summary:
<br />
This study introduces a graph-based digital twin modelling (GDTM) framework for simulating structural dynamic behavior. The framework utilizes adjacency matrices to capture spatial relationships between structural elements, enhancing physical interpretability. Results from numerical and experimental validation show high accuracy in simulating dynamics across various topologies, with low Normalized Mean-Squared Error values. The framework significantly enhances computational efficiency, outperforming traditional finite element methods (FEM) by over 80-fold. By addressing challenges in data-driven methods, this research paves the way for practical applications in structural performance evaluation and health monitoring. <div>
arXiv:2506.18724v1 Announce Type: new 
Abstract: Precise and timely simulation of a structure's dynamic behavior is crucial for evaluating its performance and assessing its health status. Traditional numerical methods are often limited by high computational costs and low efficiency, while deep learning approaches offer a promising alternative. However, these data-driven methods still face challenges, such as limited physical interpretability and difficulty in adapting to diverse structural configurations. To address these issues, this study proposes a graph-based digital twin modelling (GDTM) framework to simulate structural dynamic responses across various spatial topologies. In this framework, the adjacency matrix explicitly represents the spatial relationships between structural vertices, enhancing the model's physical interpretability. The effectiveness of the proposed framework was validated through comprehensive numerical and experimental studies. The results demonstrate that the framework accurately simulated structural dynamics across different topological configurations, with Normalized Mean-Squared Error (NMSE) values consistently below 0.005 in numerical simulations and 0.0015 in experimental validations. Furthermore, the framework achieved over 80-fold improvements in computational efficiency compared to traditional finite element methods (FEM). This research promotes the practical application of graph-based structural dynamics modelling, which has the potential to significantly advance structural performance evaluation and health monitoring.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeletal Reaction Models for Gasoline Surrogate Combustion</title>
<link>https://arxiv.org/abs/2506.18853</link>
<guid>https://arxiv.org/abs/2506.18853</guid>
<content:encoded><![CDATA[
<div> Gasoline surrogate model, skeletal reaction models, sensitivity analysis, reduced-order modeling, CUR matrix decomposition<br />
<br />
Summary: <br />
Skeletal reaction models for a four-component gasoline surrogate model were developed using an implicit time-dependent basis CUR methodology. The sensitivities of species mass fractions and temperature to reaction rates were estimated using reduced-order modeling. These sensitivities were then used to create skeletal reaction models automatically. The developed models, with 679 and 494 species, accurately reproduced detailed kinetics model results, with errors of less than 1% and less than 10%, respectively. This automated approach provides a valuable tool for reducing complex detailed models while maintaining high predictive accuracy. <div>
arXiv:2506.18853v1 Announce Type: new 
Abstract: Skeletal reaction models are derived for a four-component gasoline surrogate model via an instantaneous local sensitivity analysis technique. The sensitivities of the species mass fractions and the temperature with respect to the reaction rates are estimated by a reduced-order modeling (ROM) methodology. Termed "implicit time-dependent basis CUR (implicit TDB-CUR)," this methodology is based on the CUR matrix decomposition and incorporates implicit time integration for evolving the bases. The estimated sensitivities are subsequently analyzed to develop skeletal reaction models with a fully automated procedure. The 1389-species gasoline surrogate model developed at Lawrence Livermore National Laboratory (LLNL) is selected as the detailed kinetics model. The skeletal reduction procedure is applied to this model in a zero-dimensional constant-pressure reactor over a wide range of initial conditions. The performances of the resulting skeletal models are appraised by comparison against the results via the LLNL detailed model, and also predictions via other skeletal models. Two new skeletal models are developed consisting of 679 and 494 species, respectively. The first is an alternative to an existing model with the same number of species. The predictions with this model reproduces the detailed models vital flame results with less than 1% errors. The errors via the second model are less than 10%.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems</title>
<link>https://arxiv.org/abs/2506.17236</link>
<guid>https://arxiv.org/abs/2506.17236</guid>
<content:encoded><![CDATA[
<div> blockchain, shared resources, fairness, non-commercial networks, Max-min Fair algorithms

Summary:
The dissertation focuses on fair distribution of shared resources in non-commercial blockchain networks. Blockchain networks order and timestamp records in a secure and consensual manner. In non-commercial networks, monetary solutions are not feasible, leading to challenges in fairness. The current faucet mechanism, offering fixed amounts of free tokens, is susceptible to attacks and lacks fairness. The study proposes 6 Max-min Fair algorithms as efficient blockchain faucets, addressing fairness issues. These algorithms are resistant to denial of service attacks, cost-effective in terms of computational economics, and allow for different user weighting policies. By adapting the faucet mechanism for fair distribution, the study aims to improve the efficiency and fairness of resource distribution in non-commercial blockchain networks. 

<br /><br />Summary: <div>
arXiv:2506.17236v1 Announce Type: cross 
Abstract: The present dissertation addresses the problem of fairly distributing shared resources in non-commercial blockchain networks. Blockchains are distributed systems that order and timestamp records of a given network of users, in a public, cryptographically secure, and consensual way. The records, which may in kind be events, transaction orders, sets of rules for structured transactions etc. are placed within well-defined datastructures called blocks, and they are linked to each other by the virtue of cryptographic pointers, in a total ordering which represents their temporal relations of succession. The ability to operate on the blockchain, and/or to contribute a record to the content of a block are shared resources of the blockchain systems. In commercial networks, these resources are exchanged in return for fiat money, and consequently, fairness is not a relevant problem in terms of computer engineering. In non-commercial networks, however, monetary solutions are not available, by definition. The present non-commercial blockchain networks employ trivial distribution mechanisms called faucets, which offer fixed amounts of free tokens (called cryptocurrencies) specific to the given network. This mechanism, although simple and efficient, is prone to denial of service (DoS) attacks and cannot address the fairness problem. In the present dissertation, the faucet mechanism is adapted for fair distribution, in line with Max-min Fairness scheme. In total, we contributed 6 distinct Max-min Fair algorithms as efficient blockchain faucets. The algorithms we contribute are resistant to DoS attacks, low-cost in terms of blockchain computation economics, and they also allow for different user weighting policies.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Beyond Order: A Chaos-Markov-Gaussian Framework for Short-Term Sentiment Forecasting of Any Financial OHLC timeseries Data</title>
<link>https://arxiv.org/abs/2506.17244</link>
<guid>https://arxiv.org/abs/2506.17244</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment forecasting, financial markets, CMG framework, chaos theory, deep learning

Summary:
The paper presents a novel CMG (Chaos-Markov-Gaussian) framework for short-term sentiment forecasting in financial markets. The framework integrates chaos theory, Markov property, and Gaussian processes to enhance prediction accuracy. By incorporating transformer-based deep learning models, temporal patterns can be efficiently captured. The CMG Framework is designed for fast, resource-efficient, and accurate forecasting of any financial instrument's OHLC time series. Unlike traditional models, CMG reduces overhead and generalizes well, making it valuable for analysts and financial institutions. Evaluations on market indices show that CMG consistently outperforms statistical, machine learning, and deep learning baselines in terms of accuracy and efficiency. This framework provides a promising approach for improving sentiment forecasting in financial markets. 

<br /><br />Summary: <div>
arXiv:2506.17244v1 Announce Type: cross 
Abstract: Short-term sentiment forecasting in financial markets (e.g., stocks, indices) is challenging due to volatility, non-linearity, and noise in OHLC (Open, High, Low, Close) data. This paper introduces a novel CMG (Chaos-Markov-Gaussian) framework that integrates chaos theory, Markov property, and Gaussian processes to improve prediction accuracy. Chaos theory captures nonlinear dynamics; the Markov chain models regime shifts; Gaussian processes add probabilistic reasoning. We enhance the framework with transformer-based deep learning models to capture temporal patterns efficiently. The CMG Framework is designed for fast, resource-efficient, and accurate forecasting of any financial instrument's OHLC time series. Unlike traditional models that require heavy infrastructure and instrument-specific tuning, CMG reduces overhead and generalizes well. We evaluate the framework on market indices, forecasting sentiment for the next trading day's first quarter. A comparative study against statistical, ML, and DL baselines trained on the same dataset with no feature engineering shows CMG consistently outperforms in accuracy and efficiency, making it valuable for analysts and financial institutions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation</title>
<link>https://arxiv.org/abs/2506.17747</link>
<guid>https://arxiv.org/abs/2506.17747</guid>
<content:encoded><![CDATA[
<div> Keywords: geological modeling, reservoir characterization, Pix2Geomodel, conditional generative adversarial network, Groningen gas field

Summary:
Accurate geological modeling is essential for understanding reservoir properties, but traditional methods face challenges with complex subsurface heterogeneity and conditioning to observed data. The Pix2Geomodel, a conditional generative adversarial network (cGAN) based on Pix2Pix, was developed to predict reservoir properties in the Groningen gas field. Utilizing a large dataset, the framework achieved high accuracy in predicting facies and water saturation and moderate success with porosity and permeability. The model's performance was validated through evaluation metrics and visualizations, showcasing its ability to capture spatial variability and geological realism. Despite limitations like microstructural variability and 2D constraints, future iterations (Pix2Geomodel v2.0) could potentially integrate multi-modal data for 3D modeling. This study signifies a significant advancement in using generative AI in geoscience, which can enhance reservoir management and support open science initiatives. 

<br /><br />Summary: <div>
arXiv:2506.17747v1 Announce Type: cross 
Abstract: Accurate geological modeling is critical for reservoir characterization, yet traditional methods struggle with complex subsurface heterogeneity, and they have problems with conditioning to observed data. This study introduces Pix2Geomodel, a novel conditional generative adversarial network (cGAN) framework based on Pix2Pix, designed to predict reservoir properties (facies, porosity, permeability, and water saturation) from the Rotliegend reservoir of the Groningen gas field. Utilizing a 7.6 million-cell dataset from the Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology included data preprocessing, augmentation to generate 2,350 images per property, and training with a U-Net generator and PatchGAN discriminator over 19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection over union (mIoU), frequency weighted intersection over union (FWIoU), and visualizations assessed performance in masked property prediction and property-to-property translation tasks. Results demonstrated high accuracy for facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74, FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA 0.98, FWIoU 0.97). The framework captured spatial variability and geological realism, as validated by variogram analysis, and calculated the training loss curves for the generator and discriminator for each property. Compared to traditional methods, Pix2Geomodel offers enhanced fidelity in direct property mapping. Limitations include challenges with microstructural variability and 2D constraints, suggesting future integration of multi-modal data and 3D modeling (Pix2Geomodel v2.0). This study advances the application of generative AI in geoscience, supporting improved reservoir management and open science initiatives.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks</title>
<link>https://arxiv.org/abs/2506.18154</link>
<guid>https://arxiv.org/abs/2506.18154</guid>
<content:encoded><![CDATA[
<div> Taylor's Power Law, insect populations, universality, ecology, statistics<br />
<br />
Summary: Taylor's Power Law (TPL) correlates mean population abundances and variances across insect populations using a power function. TPL has been studied for six decades, with distinct periods and themes identified, including ecological mechanisms, skewed distributions, and mathematical/statistical explanations. Future research directions include fostering interactions between abstract and physical worlds, measuring heterogeneity, and exploring evolutionary contexts. TPL's significance lies in its practical applications in various fields such as agriculture and epidemiology, as well as its theoretical implications related to phase transitions and scale invariance. <div>
arXiv:2506.18154v1 Announce Type: cross 
Abstract: First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL) correlates the mean (M) population abundances and the corresponding variances (V) across a set of insect populations using a power function (V=aM^b). TPL has demonstrated its 'universality' across numerous fields of sciences, social sciences, and humanities. This universality has inspired two main prongs of exploration: one from mathematicians and statisticians, who might instinctively respond with a convergence theorem similar to the central limit theorem of the Gaussian distribution, and another from biologists, ecologists, physicists, etc., who are more interested in potential underlying ecological or organizational mechanisms. Over the past six decades, TPL studies have produced a punctuated landscape with three relatively distinct periods (1960s-1980s; 1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical worlds. Eight themes have been identified and reviewed on this landscape, including population spatial aggregation and ecological mechanisms, TPL and skewed statistical distributions, mathematical/statistical mechanisms of TPL, sample vs. population TPL, population stability, synchrony, and early warning signals for tipping points, TPL on complex networks, TPL in macrobiomes, and in microbiomes. Three future research directions including fostering reciprocal interactions between the two prongs, heterogeneity measuring, and exploration in the context of evolution. The significance of TPL research includes practically, population fluctuations captured by TPL are relevant for agriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor heterogeneity, earthquakes, social inequality, stock illiquidity, financial stability, tipping point events, etc.; theoretically, TPL is one form of power laws, which are related to phase transitions, universality, scale-invariance, etc.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airalogy: AI-empowered universal data digitization for research automation</title>
<link>https://arxiv.org/abs/2506.18586</link>
<guid>https://arxiv.org/abs/2506.18586</guid>
<content:encoded><![CDATA[
<div> AI, research data, platform, standardization, multidisciplinary <br />
Summary: <br />
Research data are crucial for AI-driven science, but current applications are limited due to fragmented and inefficient data collection processes. Existing platforms lack the balance between universality and standardization needed to support diverse disciplines. Airalogy aims to bridge this gap by providing a platform that integrates scientific domain knowledge with advanced computing skills. It offers customizable, standardized data records and advanced AI tools for research assistance and automation. Already deployed in laboratories at Westlake University, Airalogy has the potential to accelerate scientific innovation across academia, industry, and the global research community. By addressing the challenges of data standardization and leveraging AI-driven capabilities, Airalogy aims to benefit humanity through enhanced research efficiency and automation. <br /> <div>
arXiv:2506.18586v1 Announce Type: cross 
Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven science, yet current AI applications remain limited to a few fields with readily available, well-structured, digitized datasets. Achieving comprehensive AI empowerment across multiple disciplines is still out of reach. Present-day research data collection is often fragmented, lacking unified standards, inefficiently managed, and difficult to share. Creating a single platform for standardized data digitization needs to overcome the inherent challenge of balancing between universality (supporting the diverse, ever-evolving needs of various disciplines) and standardization (enforcing consistent formats to fully enable AI). No existing platform accommodates both facets. Building a truly multidisciplinary platform requires integrating scientific domain knowledge with sophisticated computing skills. Researchers often lack the computational expertise to design customized and standardized data recording methods, whereas platform developers rarely grasp the intricate needs of multiple scientific domains. These gaps impede research data standardization and hamper AI-driven progress. In this study, we address these challenges by developing Airalogy (https://airalogy.com), the world's first AI- and community-driven platform that balances universality and standardization for digitizing research data across multiple disciplines. Airalogy represents entire research workflows using customizable, standardized data records and offers an advanced AI research copilot for intelligent Q&amp;A, automated data entry, analysis, and research automation. Already deployed in laboratories across all four schools of Westlake University, Airalogy has the potential to accelerate and automate scientific innovation in universities, industry, and the global research community-ultimately benefiting humanity as a whole.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-consistent integration of mechanical systems based on Livens principle</title>
<link>https://arxiv.org/abs/2312.02825</link>
<guid>https://arxiv.org/abs/2312.02825</guid>
<content:encoded><![CDATA[
<div> Hamilton-Pontryagin principle, Livens principle, structure-preserving integrator, mechanical systems, singular mass matrices  
Summary:  
Livens principle, also known as the Hamilton-Pontryagin principle, is utilized to develop a new integrator that preserves the structure of mechanical systems. Unlike traditional Hamiltonian equations, the Euler-Lagrange equations derived from Livens principle avoid the need to invert mass matrices, particularly beneficial for systems with singular mass matrices. This approach unifies Lagrangian and Hamiltonian perspectives, eliminating the requirement to define the system's Hamiltonian. The integrator conserves energy and preserves momentum maps related to system symmetries. An extension is proposed for systems with holonomic constraints, and its performance is evaluated through examples. Overall, the novel integrator based on Livens principle offers a comprehensive and efficient method for simulating mechanical systems while overcoming challenges associated with singular mass matrices. 

<br /><br />Summary: <div>
arXiv:2312.02825v3 Announce Type: replace 
Abstract: In this work we make use of Livens principle (sometimes also referred to as Hamilton-Pontryagin principle) in order to obtain a novel structure-preserving integrator for mechanical systems. In contrast to the canonical Hamiltonian equations of motion, the Euler-Lagrange equations pertaining to Livens principle circumvent the need to invert the mass matrix. This is an essential advantage with respect to singular mass matrices, which can yield severe difficulties for the modelling and simulation of multibody systems. Moreover, Livens principle unifies both Lagrangian and Hamiltonian viewpoints on mechanics. Additionally, the present framework avoids the need to set up the system's Hamiltonian. The novel scheme algorithmically conserves a general energy function and aims at the preservation of momentum maps corresponding to symmetries of the system. We present an extension to mechanical systems subject to holonomic constraints. The performance of the newly devised method is studied in representative examples.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedded Model Form Uncertainty Quantification with Measurement Noise for Bayesian Model Calibration</title>
<link>https://arxiv.org/abs/2410.12037</link>
<guid>https://arxiv.org/abs/2410.12037</guid>
<content:encoded><![CDATA[
<div> parameter calibration, Bayesian methods, model inadequacy, uncertainty quantification, heat flux estimation
Summary: 
This paper addresses the challenge of accurately calibrating simulation parameters in computer models of physical systems. It introduces a novel framework for embedding model inadequacy in Bayesian inference, allowing for more reliable propagation of uncertainties to non-observed quantities of interest (QoIs). By adapting existing likelihood models and proposing new formulations to account for noise and outliers in measurements, the approach improves predictions' representation of observed data points. The study evaluates the method's performance in the presence of discrepancies between measurements and predictions and demonstrates how uncertainty in the model inadequacy term influences QoIs. Through an application to heat flux estimation from transient thermal simulations, the proposed approach enables a more comprehensive statistical analysis of prediction reliability. <div>
arXiv:2410.12037v2 Announce Type: replace 
Abstract: A key factor in ensuring the accuracy of computer simulations that model physical systems is the proper calibration of their parameters based on real-world observations or experimental data. Inevitably, uncertainties arise, and Bayesian methods provide a robust framework for quantifying and propagating these uncertainties to model predictions. Nevertheless, Bayesian methods paired with inexact models usually produce predictions unable to represent the observed datapoints. Additionally, the quantified uncertainties of these overconfident models cannot be propagated to other Quantities of Interest (QoIs) reliably. A promising solution involves embedding a model inadequacy term in the inference parameters, allowing the quantified model form uncertainty to influence non-observed QoIs. This paper introduces a more interpretable framework for embedding the model inadequacy compared to existing methods. To overcome the limitations of current approaches, we adapt the existing likelihood models to properly account for noise in the measurements and propose two new formulations designed to address their shortcomings. Moreover, we evaluate the performance of this inadequacy-embedding approach in the presence of discrepancies between measurements and model predictions, including noise and outliers. Particular attention is given to how the uncertainty associated with the model inadequacy term propagates to the QoIs, enabling a more comprehensive statistical analysis of prediction's reliability. Finally, the proposed approach is applied to estimate the uncertainty in the predicted heat flux from a transient thermal simulation using temperature observations.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilience-based post disaster recovery optimization for infrastructure system via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.18577</link>
<guid>https://arxiv.org/abs/2410.18577</guid>
<content:encoded><![CDATA[
<div> Keywords: infrastructure systems, post-disaster recovery, deep reinforcement learning, resilience metric, optimization

Summary:
Infrastructure systems face significant challenges during post-disaster recovery, requiring efficient repair-scheduling approaches under resource constraints. Existing methods have limitations in such contexts, prompting the proposal of a novel approach using Deep Reinforcement Learning (DRL) and a specialized resilience metric. The system's recovery process is modeled as a sequential decision-making problem on a graph-based structure, with Deep Q-learning algorithms optimizing recovery strategies. Testing on post-earthquake recovery for an electrical substation system showed Double DQN (DDQN) to be superior. Comparative analysis demonstrated the proposed method's effectiveness in optimizing resilience and rapid recovery while minimizing computational costs. This approach presents an attractive solution for enhancing infrastructure system resilience and response efficiency. 

<br /><br />Summary: <div>
arXiv:2410.18577v2 Announce Type: replace 
Abstract: Infrastructure systems are critical in modern communities but are highly susceptible to various natural and man-made disasters. Efficient post-disaster recovery requires repair-scheduling approaches under the limitation of capped resources that need to be shared across the system. Existing approaches, including component ranking methods, greedy evolutionary algorithms, and data-driven machine learning models, face various limitations when tested within such a context. To tackle these issues, we propose a novel approach to optimize post-disaster recovery of infrastructure systems by leveraging Deep Reinforcement Learning (DRL) methods and incorporating a specialized resilience metric to lead the optimization. The system topology is represented adopting a graph-based structure, where the system's recovery process is formulated as a sequential decision-making problem. Deep Q-learning algorithms are employed to learn optimal recovery strategies by mapping system states to specific actions, as for instance which component ought to be repaired next, with the goal of maximizing long-term recovery from a resilience-oriented perspective. To demonstrate the efficacy of our proposed approach, we implement this scheme on the example of post-earthquake recovery optimization for an electrical substation system. We assess different deep Q-learning algorithms to this end, namely vanilla Deep Q-Networks (DQN), Double DQN(DDQN), Duel DQN, and duel DDQN, demonstrating superiority of the DDQN for the considered problem. A further comparative analysis against baseline methods during testing reveals the superior performance of the proposed method in terms of both optimization effect and computational cost, rendering this an attractive approach in the context of resilience enhancement and rapid response and recovery.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Cancer Gene Identification through Graph Anomaly Analysis</title>
<link>https://arxiv.org/abs/2412.17240</link>
<guid>https://arxiv.org/abs/2412.17240</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, protein-protein interaction networks, cancer genes, weight heterogeneity, HIPGNN

Summary: 
This study explores the use of Graph Neural Networks (GNNs) in analyzing protein-protein interaction (PPI) networks to identify cancer genes. It identifies a unique graph anomaly in cancer genes known as weight heterogeneity, characterized by significantly higher variance in edge weights of cancer gene nodes within the graph. The study also highlights how weight heterogeneity can impact the spectral energy distribution, leading to a concentration towards the extremes of the spectrum. To address these insights, the study proposes the HIerarchical-Perspective Graph Neural Network (HIPGNN), which considers variations in spectral energy distribution and protein interaction context. Experimental results on reprocessed datasets demonstrate the superiority of HIPGNN in identifying cancer genes within PPI networks. <div>
arXiv:2412.17240v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the "flattening out" of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics Informatics: A paradigm for efficiently learning constitutive models</title>
<link>https://arxiv.org/abs/2501.08314</link>
<guid>https://arxiv.org/abs/2501.08314</guid>
<content:encoded><![CDATA[
<div> stress state entropy, constitutive model learning, parameter identification, information content, mechanics informatics

Summary:<br />
- Mechanics informatics is introduced as a paradigm for efficient and accurate learning of constitutive laws, focusing on the interplay between experimental data and model parameters.
- The stress state entropy is proposed as a metric for quantifying the information content of experimental data, enabling the analysis of specimen geometries with varying information content for model learning.
- Specimen designs are optimized using stress state entropy in a Bayesian optimization scheme, leading to the creation of cruciform specimens with maximized entropy for accurate parameter identification.
- Tailoring specimen designs for specific experimental goals is demonstrated through minimizing entropy in Peirs shear specimens to achieve a uniform shear stress state.
- The framework addresses experimental uncertainties, explores transfer learning for replacing challenging testing protocols with simpler alternatives, and shows potential for extension to different material laws.

<br /><br />Summary: <div>
arXiv:2501.08314v2 Announce Type: replace 
Abstract: Efficient and accurate learning of constitutive laws is crucial for accurately predicting the mechanical behavior of materials under complex loading conditions. Accurate model calibration hinges on a delicate interplay between the information embedded in experimental data and the parameters that define our constitutive models.The information encoded in the parameters of the constitutive model must be complemented by the information in the data used for calibration. This interplay raises fundamental questions: How can we quantify the information content of test data? How much information does a single test convey? Also, how much information is required to accurately learn a constitutive model? To address these questions, we introduce mechanics informatics, a paradigm for efficient and accurate constitutive model learning. At its core is the stress state entropy, a metric for quantifying the information content of experimental data. Using this framework, we analyzed specimen geometries with varying information content for learning an anisotropic inelastic law. Specimens with limited information enabled accurate identification of a few parameters sensitive to the information in the data. Furthermore, we optimized specimen design by incorporating stress state entropy into a Bayesian optimization scheme. This led to the design of cruciform specimens with maximized entropy for accurate parameter identification. Conversely, minimizing entropy in Peirs shear specimens yielded a uniform shear stress state, showcasing the framework's flexibility in tailoring designs for specific experimental goals. Finally, we addressed experimental uncertainties, demonstrated the potential of transfer learning for replacing challenging testing protocols with simpler alternatives, and extension of the framework to different material laws.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks in Supply Chain Analytics and Optimization: Concepts, Perspectives, Dataset and Benchmarks</title>
<link>https://arxiv.org/abs/2411.08550</link>
<guid>https://arxiv.org/abs/2411.08550</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, supply chain management, benchmark dataset, optimization, real-world applications

Summary: 
Graph Neural Networks (GNNs) have shown promise in various fields but have yet to be extensively studied in supply chain management. This study bridges the gap by providing a conceptual foundation for applying GNNs in supply chain optimization. By connecting supply chains with graph structures, the authors explain formulations, examples, and task guidelines for effective GNN application. They also introduce a benchmark dataset from a leading FMCG company in Bangladesh, focused on supply chain planning. Through various supply chain tasks using GNNs, the study shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models in regression, classification, detection, and anomaly detection tasks. This work lays the groundwork for leveraging GNNs in solving complex supply chain problems with methodological insights and a comprehensive dataset.
<br /><br />Summary: <div>
arXiv:2411.08550v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have recently gained traction in transportation, bioinformatics, language and image processing, but research on their application to supply chain management remains limited. Supply chains are inherently graph-like, making them ideal for GNN methodologies, which can optimize and solve complex problems. The barriers include a lack of proper conceptual foundations, familiarity with graph applications in SCM, and real-world benchmark datasets for GNN-based supply chain research. To address this, we discuss and connect supply chains with graph structures for effective GNN application, providing detailed formulations, examples, mathematical definitions, and task guidelines. Additionally, we present a multi-perspective real-world benchmark dataset from a leading FMCG company in Bangladesh, focusing on supply chain planning. We discuss various supply chain tasks using GNNs and benchmark several state-of-the-art models on homogeneous and heterogeneous graphs across six supply chain analytics tasks. Our analysis shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models by around 10-30% in regression, 10-30% in classification and detection tasks, and 15-40% in anomaly detection tasks on designated metrics. With this work, we lay the groundwork for solving supply chain problems using GNNs, supported by conceptual discussions, methodological insights, and a comprehensive dataset.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Optimization of Physics-Informed Neural Networks: Evo-PINN Frontiers and Opportunities</title>
<link>https://arxiv.org/abs/2501.06572</link>
<guid>https://arxiv.org/abs/2501.06572</guid>
<content:encoded><![CDATA[
<div> Keywords: PINNs, physics-informed neural networks, optimization, generalization, evolutionary algorithms

Summary:
PINNs, or physics-informed neural networks, integrate mathematical laws of nature into their training loss function, providing advantages over data-driven models in limited-data scenarios. However, optimizing and generalizing PINNs present challenges, including training speed, precision, and generalizability. Evolutionary algorithms (EAs) are proposed as a solution to optimize complex loss landscapes in PINNs, potentially improving training efficiency and accuracy. Synergizing gradient descent with EAs can lead to tailored neural architectures and better balancing of physics-informed learning objectives. Additionally, using evolutionary algorithms as meta-learners for generalizable PINN models shows promise. Recent literature demonstrates the early success of these approaches in tackling optimization and generalization issues in PINNs. <div>
arXiv:2501.06572v3 Announce Type: replace-cross 
Abstract: Deep learning models trained on finite data lack a complete understanding of the physical world. On the other hand, physics-informed neural networks (PINNs) are infused with such knowledge through the incorporation of mathematically expressible laws of nature into their training loss function. By complying with physical laws, PINNs provide advantages over purely data-driven models in limited-data regimes and present as a promising route towards Physical AI. This feature has propelled them to the forefront of scientific machine learning, a domain characterized by scarce and costly data. However, the vision of accurate physics-informed learning comes with significant challenges. This work examines PINNs for the first time in terms of model optimization and generalization, shedding light on the need for new algorithmic advances to overcome issues pertaining to the training speed, precision, and generalizability of today's PINN models. Of particular interest are gradient-free evolutionary algorithms (EAs) for optimizing the uniquely complex loss landscapes arising in PINN training. Methods synergizing gradient descent and EAs for discovering bespoke neural architectures and balancing multiple terms in physics-informed learning objectives are positioned as important avenues for future research. Another exciting track is to cast evolutionary as a meta-learner of generalizable PINN models. To substantiate these proposed avenues, we further highlight results from recent literature to showcase the early success of such approaches in addressing the aforementioned challenges in PINN optimization and generalization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast Iterative Robust Principal Component Analysis Method</title>
<link>https://arxiv.org/abs/2506.16013</link>
<guid>https://arxiv.org/abs/2506.16013</guid>
<content:encoded><![CDATA[
<div> Keywords: Principal Component Analysis, Robust PCA, Outliers, Fast Iterative Robust PCA, Incremental PCA

Summary: 
The article introduces a new method called Fast Iterative Robust (FIR) PCA to improve the robustness of Principal Component Analysis (PCA) in the presence of outliers. Traditional PCA methods are sensitive to outliers, affecting the accuracy of the results. The FIR PCA method efficiently estimates the center location and covariance of inliers, using Incremental PCA (IPCA) to iteratively construct a subset of data points that improve location and covariance estimation while mitigating the impact of outliers. The approach demonstrates competitive accuracy and performance compared to existing robust methods, offering enhanced robustness to outlier contamination. Simulated and real-world datasets are used to evaluate the effectiveness of the method in identifying and preserving underlying data structures in the presence of outliers. <div>
arXiv:2506.16013v1 Announce Type: new 
Abstract: Principal Component Analysis (PCA) is widely used for dimensionality reduction and data analysis. However, PCA results are adversely affected by outliers often observed in real-world data. Existing robust PCA methods are often computationally expensive or exhibit limited robustness. In this work, we introduce a Fast Iterative Robust (FIR) PCA method by efficiently estimating the inliers center location and covariance. Our approach leverages Incremental PCA (IPCA) to iteratively construct a subset of data points that ensures improved location and covariance estimation that effectively mitigates the influence of outliers on PCA projection. We demonstrate that our method achieves competitive accuracy and performance compared to existing robust location and covariance methods while offering improved robustness to outlier contamination. We utilize simulated and real-world datasets to evaluate and demonstrate the efficacy of our approach in identifying and preserving underlying data structures in the presence of contamination.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Converging Single Trace Quasi-local PMCHWT Equation for the Modelling of Composite Systems</title>
<link>https://arxiv.org/abs/2506.16376</link>
<guid>https://arxiv.org/abs/2506.16376</guid>
<content:encoded><![CDATA[
<div> PMCHWT integral equation, scattering, time-harmonic fields, piecewise homogeneous, composite systems<br />
Summary:<br />
The article introduces a single trace quasi-local PMCHWT equation for modeling scattering of time-harmonic fields by composite systems with junctions. Traditional methods for solving such systems rely on Krylov iterative methods, with the number of iterations needed influenced by the system matrix's eigenvalue distribution. While Caldern preconditioning is effective for systems without junction lines, it is insufficient for those with junctions. The new approach, utilizing the global multi-trace method, offers a solution with slower iteration growth as mesh size decreases. The method maintains continuity conditions at domain interfaces and is free from interior resonances. Extensive numerical experiments confirm the method's accuracy, convergence behavior, and efficiency. This advancement represents a significant step forward in the modeling and simulation of complex systems with junctions. <br /> <div>
arXiv:2506.16376v1 Announce Type: new 
Abstract: The PMCHWT integral equation enables the modelling of scattering of time-harmonic fields by penetrable, piecewise homogeneous, systems. They have been generalised to include the modelling of composite systems that may contain junctions, i.e. lines along which three or more materials meet. Linear systems resulting upon discretisation of the PMCHWT are, because of their large dimension, typically solved by Krylov iterative methods. The number of iterations required for this solution critically depends on the eigenvalue distribution of the system matrix. For systems that do not contain junction lines, Calder\'on preconditioning, which was first applied to the electric field integral equation, has been generalised to the PMCHWT equation. When junctions are present, this approach cannot be applied. Alternative approaches, such as the global multi-trace method, conceptually remove the junction lines and as a result are amenable to Calder\'on preconditioning. This approach entails a doubling of the degrees of freedom, and the solution that is produced only approximately fulfils the continuity conditions at interfaces separating domains. In this contribution, a single trace quasi-local PMCHWT equation is introduced that requires a number of iterations for its solution that only slowly increases as the mesh size tends to zero. The method is constructed as a generalisation of the classic PMCHWT, and its discretisation is thoroughly discussed. A comprehensive suite of numerical experiments demonstrates the correctness, convergence behaviour, and efficiency of the method. The integral equation is demonstrated to be free from interior resonances.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aethorix v1.0: AI-Driven Inverse Design of Inorganic Materials for Scalable Industrial Innovation</title>
<link>https://arxiv.org/abs/2506.16609</link>
<guid>https://arxiv.org/abs/2506.16609</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, industrial manufacturing, materials development, Aethorix v1.0, R&amp;D pipelines 

Summary: 
Artificial intelligence for Science (AI4S) is revolutionizing industrial manufacturing by accelerating the discovery and optimization of advanced materials. Aethorix v1.0 is introduced as a platform that combines large language models, generative models for crystal design, and machine-learned potentials for property prediction. This platform aims to streamline the materials development cycle, from design to deployment, while adhering to manufacturing standards. The industrial value of Aethorix v1.0 is validated through a real use case, demonstrating its seamless integration into scalable R&amp;D pipelines. Its capabilities in objective mining, inorganic crystal design, and property prediction enable rapid advancement in materials science, offering a promising solution for the industry's challenges. Aethorix v1.0 showcases the potential of AI in transforming the field of industrial manufacturing and driving innovation in high-performance materials. 

<br /><br />Summary: <div>
arXiv:2506.16609v1 Announce Type: new 
Abstract: Artificial intelligence for Science (AI4S) is poised to transform industrial manufacturing by enabling the accelerated discovery and optimization of advanced (bio)materials, dramatically reducing development cycles, and unlocking novel high-performance solutions. We introduce Aethorix v1.0, a platform that integrates large language models for objective mining, diffusion-based generative models for zero-shot inorganic crystal design, and machine-learned interatomic potentials for rapid property prediction at ab initio accuracy. The platform is developed to enhance the full materials development cycle, ranging from design to deployment in use cases, while incorporating critical operational constraints to meet rigorous manufacturing standards. We validated its industrial value through a real use case, showcasing how the framework can be seamlessly embedded into scalable materials R&amp;D pipelines.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training Time Series Models with Stock Data Customization</title>
<link>https://arxiv.org/abs/2506.16746</link>
<guid>https://arxiv.org/abs/2506.16746</guid>
<content:encoded><![CDATA[
<div> stock selection, pre-training, transformer architecture, financial data, experimental results <br />
Summary:<br />
The paper introduces novel pre-training tasks tailored to stock data characteristics, including stock code classification, stock sector classification, and moving average prediction. The Stock Specialized Pre-trained Transformer (SSPT) is developed based on a two-layer transformer architecture. Experimental results demonstrate the effectiveness of the proposed pre-training methods, outperforming existing methods in terms of cumulative investment return ratio and Sharpe ratio across various stock datasets and time periods. The research also includes evaluations on simulated data to provide insights into understanding price series. The code for the proposed methods is publicly available. <div>
arXiv:2506.16746v1 Announce Type: new 
Abstract: Stock selection, which aims to predict stock prices and identify the most profitable ones, is a crucial task in finance. While existing methods primarily focus on developing model structures and building graphs for improved selection, pre-training strategies remain underexplored in this domain. Current stock series pre-training follows methods from other areas without adapting to the unique characteristics of financial data, particularly overlooking stock-specific contextual information and the non-stationary nature of stock prices. Consequently, the latent statistical features inherent in stock data are underutilized. In this paper, we propose three novel pre-training tasks tailored to stock data characteristics: stock code classification, stock sector classification, and moving average prediction. We develop the Stock Specialized Pre-trained Transformer (SSPT) based on a two-layer transformer architecture. Extensive experimental results validate the effectiveness of our pre-training methods and provide detailed guidance on their application. Evaluations on five stock datasets, including four markets and two time periods, demonstrate that SSPT consistently outperforms the market and existing methods in terms of both cumulative investment return ratio and Sharpe ratio. Additionally, our experiments on simulated data investigate the underlying mechanisms of our methods, providing insights into understanding price series. Our code is publicly available at: https://github.com/astudentuser/Pre-training-Time-Series-Models-with-Stock-Data-Customization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Traditional Technical Analysis with AI: A Multi-Agent LLM-Based Approach to Stock Market Forecasting</title>
<link>https://arxiv.org/abs/2506.16813</link>
<guid>https://arxiv.org/abs/2506.16813</guid>
<content:encoded><![CDATA[
<div> ElliottAgents, multi-agent system, Elliott Wave Principle, AI, stock market forecasting<br />
Summary:<br />
Traditional technical analysis methods struggle to predict trends in complex financial markets. ElliottAgents integrates the Elliott Wave Principle with AI to address these challenges. The system utilizes LLMs to enhance natural language understanding and decision-making in a multi-agent framework. By employing technologies like RAG and DRL, ElliottAgents continuously analyzes market data to identify wave patterns and predict future price movements. Experimental results on historical data from U.S. companies validate its effectiveness in pattern recognition and trend forecasting across different time frames. This research demonstrates the successful combination of traditional technical analysis with modern AI approaches, providing traders with reliable and interpretable market prediction systems. <div>
arXiv:2506.16813v1 Announce Type: new 
Abstract: Traditional technical analysis methods face limitations in accurately predicting trends in today's complex financial markets. This paper introduces ElliottAgents, an multi-agent system that integrates the Elliott Wave Principle with AI for stock market forecasting. The inherent complexity of financial markets, characterized by non-linear dynamics, noise, and susceptibility to unpredictable external factors, poses significant challenges for accurate prediction. To address these challenges, the system employs LLMs to enhance natural language understanding and decision-making capabilities within a multi-agent framework. By leveraging technologies such as Retrieval-Augmented Generation (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs continuous, multi-faceted analysis of market data to identify wave patterns and predict future price movements. The research explores the system's ability to process historical stock data, recognize Elliott wave patterns, and generate actionable insights for traders. Experimental results, conducted on historical data from major U.S. companies, validate the system's effectiveness in pattern recognition and trend forecasting across various time frames. This paper contributes to the field of AI-driven financial analysis by demonstrating how traditional technical analysis methods can be effectively combined with modern AI approaches to create more reliable and interpretable market prediction systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Deprivation Cost Functions for Power Outages During Disasters: A Discrete Choice Modeling Approach</title>
<link>https://arxiv.org/abs/2506.16993</link>
<guid>https://arxiv.org/abs/2506.16993</guid>
<content:encoded><![CDATA[
<div> deprivation costs, electricity outages, stated preference survey, discrete choice model, Harris County<br />
<br />
Summary: 
This study addresses the lack of systematic measurement of deprivation costs related to electricity outages by developing a methodology to estimate deprivation cost functions using stated preference survey data from Harris County, Texas. The analysis compares different model architectures and utility transformations to capture the increasing and convex nature of deprivation cost functions over time. The study also identifies heterogeneity in deprivation valuation, particularly among different income groups. The results show that deprivation cost functions are strictly increasing with time and highlight the importance of flexible modeling approaches to account for systematic and random taste variation. By providing a methodological framework and empirical evidence, this research enables policymakers to better quantify service disruption costs and develop more equitable resilience strategies in infrastructure risk assessments and humanitarian logistics. <br /><br /> <div>
arXiv:2506.16993v1 Announce Type: new 
Abstract: Systems for the generation and distribution of electrical power represents critical infrastructure and, when extreme weather events disrupt such systems, this imposes substantial costs on consumers. These costs can be conceptualized as deprivation costs, an increasing function of time without service, quantifiable through individuals' willingness to pay for power restoration. Despite widespread recognition of outage impacts, a gap in the research literature exists regarding the systematic measurement of deprivation costs. This study addresses this deficiency by developing and implementing a methodology to estimate deprivation cost functions for electricity outages, using stated preference survey data collected from Harris County, Texas. This study compares multiple discrete choice model architectures, including multinomial logit and mixed logit specifications, as well as models incorporating BoxCox and exponential utility transformations for the deprivation time attribute. The analysis examines heterogeneity in deprivation valuation through sociodemographic interactions, particularly across income groups. Results confirm that power outage deprivation cost functions are convex and strictly increasing with time. Additionally, the study reveals both systematic and random taste variation in how individuals value power loss, highlighting the need for flexible modeling approaches. By providing both methodological and empirical foundations for incorporating deprivation costs into infrastructure risk assessments and humanitarian logistics, this research enables policymakers to better quantify service disruption costs and develop more equitable resilience strategies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance Language Model Evaluation (FLaME)</title>
<link>https://arxiv.org/abs/2506.15846</link>
<guid>https://arxiv.org/abs/2506.15846</guid>
<content:encoded><![CDATA[
<div> Language Models, Finance NLP, Evaluation Frameworks, Benchmarking, Reasoning-reinforced LMs <br />
Summary: <br />
This paper introduces a benchmarking suite, FLaME, for evaluating Language Models (LMs) on Finance NLP tasks. It addresses gaps in existing evaluation methodologies and challenges the belief in LMs' lower performance bounds in finance tasks. The study compares 23 LMs across 20 core NLP tasks in finance, including 'reasoning-reinforced' LMs. The research aims to demonstrate the potential of LMs in specialized finance tasks and provides open-source access to the FLaME framework, data, and results. <div>
arXiv:2506.15846v1 Announce Type: cross 
Abstract: Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials</title>
<link>https://arxiv.org/abs/2506.16918</link>
<guid>https://arxiv.org/abs/2506.16918</guid>
<content:encoded><![CDATA[
<div> deep learning, multiscale modeling, FE^2 approach, computational homogenization, viscoelastic material

Summary:
This study focuses on developing a hybrid model that combines data-driven deep learning techniques with physics-based approaches for multiscale simulations in time-dependent solid mechanics problems. By incorporating neural operators to predict microscale physics, the model efficiently predicts homogenized stresses with minimal error (less than 6%) while being approximately 100 times faster computationally. The approach integrates constitutive relations of the microscale into the model architecture and computes internal variables based on established physical principles. This hybrid model allows for physics-guided learning and flexibility for different materials and spatial discretizations, making it a promising method for accurately predicting the global response of materials influenced by microstructure across various time and length scales. <br /><br />Summary: <div>
arXiv:2506.16918v1 Announce Type: cross 
Abstract: The behavior of materials is influenced by a wide range of phenomena occurring across various time and length scales. To better understand the impact of microstructure on macroscopic response, multiscale modeling strategies are essential. Numerical methods, such as the $\text{FE}^2$ approach, account for micro-macro interactions to predict the global response in a concurrent manner. However, these methods are computationally intensive due to the repeated evaluations of the microscale. This challenge has led to the integration of deep learning techniques into computational homogenization frameworks to accelerate multiscale simulations. In this work, we employ neural operators to predict the microscale physics, resulting in a hybrid model that combines data-driven and physics-based approaches. This allows for physics-guided learning and provides flexibility for different materials and spatial discretizations. We apply this method to time-dependent solid mechanics problems involving viscoelastic material behavior, where the state is represented by internal variables only at the microscale. The constitutive relations of the microscale are incorporated into the model architecture and the internal variables are computed based on established physical principles. The results for homogenized stresses ($<6\%$ error) show that the approach is computationally efficient ($\sim 100 \times$ faster).
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete nonlinear elastodynamics in a port-Hamiltonian framework</title>
<link>https://arxiv.org/abs/2306.17740</link>
<guid>https://arxiv.org/abs/2306.17740</guid>
<content:encoded><![CDATA[
<div> Keywords: port-Hamiltonian, discrete elastodynamical systems, nonlinear strains, hyperelastic material behavior, midpoint discrete gradient <br />
<br />
Summary: 
The article presents a fully nonlinear port-Hamiltonian formulation for discrete elastodynamical systems. The governing equations are derived variationaly, resulting in index-1 differential algebraic equations. By performing index reduction, a port-Hamiltonian state space model is obtained, featuring nonlinear strains as independent states. The model captures hyperelastic material behavior through a nonlinear stored energy function and exhibits passivity, losslessness, and symmetry conserving angular momentum. Temporal discretization is achieved using the midpoint discrete gradient, preserving the model's beneficial properties in a discrete sense. Numerical validation in a representative example confirms the effectiveness of the proposed approach. <div>
arXiv:2306.17740v2 Announce Type: replace-cross 
Abstract: We provide a fully nonlinear port-Hamiltonian formulation for discrete elastodynamical systems as well as a structure-preserving time discretization. The governing equations are obtained in a variational manner and represent index-1 differential algebraic equations. Performing an index reduction one obtains the port-Hamiltonian state space model, which features the nonlinear strains as an independent state next to position and velocity. Moreover, hyperelastic material behavior is captured in terms of a nonlinear stored energy function. The model exhibits passivity and losslessness and has an underlying symmetry yielding the conservation of angular momentum. We perform temporal discretization using the midpoint discrete gradient, such that the beneficial properties are inherited by the developed time stepping scheme in a discrete sense. The numerical results obtained in a representative example are demonstrated to validate the findings.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid high-order methods for elasto-acoustic wave propagation in the time domain</title>
<link>https://arxiv.org/abs/2502.10870</link>
<guid>https://arxiv.org/abs/2502.10870</guid>
<content:encoded><![CDATA[
<div> Keywords: Hybrid High-Order method, Acoustic and Elastic wave equations, Time domain, Spectral analysis, Elasto-acoustic wave propagation<br />
<br />
Summary: 
The study introduces a Hybrid High-Order (HHO) method for coupling acoustic and elastic wave equations in the time domain. The method, utilizing first-order time formulation, can employ equal-order and mixed-order settings with polynomial degree k>=0 for face unknowns, along with O(1)- and O(1/h)-stabilizations. An energy-error estimate is established for the time-continuous scenario, and a numerical spectral analysis reveals the necessity of O(1)-stabilization to avoid excessive CFL limitations with explicit time discretizations. Optimal convergence rates of order (k+1) are demonstrated for general mesh analytical solutions in both equal- and mixed-order settings with O(1)-stabilization, while mixed-order setting with O(1/h)-stabilization achieves order (k+2). Test cases using a Ricker wavelet as an initial condition illustrate the method's effectiveness in simulating elasto-acoustic wave propagation in media with differing material properties. <div>
arXiv:2502.10870v2 Announce Type: replace-cross 
Abstract: We devise a Hybrid High-Order (HHO) method for the coupling between the acoustic and elastic wave equations in the time domain. A first-order formulation in time is considered. The HHO method can use equal-order and mixed-order settings with polynomial degree k>=0 for the face unknowns, together with O(1)- and O(1/h)-stabilizations. An energy-error estimate is established in the time-continuous case. A numerical spectral analysis is performed, showing that O(1)-stabilization is required to avoid excessive CFL limitations for explicit time discretizations. Moreover, the spectral radius of the stiffness matrix is fairly independent of the geometry of the mesh cells. For analytical solutions on general meshes, optimal convergence rates of order (k+1) are shown in both equal- and mixed-order settings using O(1)-stabilization, whereas order (k+2) is achieved in the mixed-order setting using O(1/h)-stabilization. Test cases with a Ricker wavelet as an initial condition showcase the relevance of the proposed method for the simulation of elasto-acoustic wave propagation across media with contrasted material properties.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.14933</link>
<guid>https://arxiv.org/abs/2506.14933</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized finance, DeFi, cryptocurrency, financial crime, automated detection tools

Summary:
The article discusses the rapid growth of the decentralized finance (DeFi) community driven by cryptocurrency enthusiasts and the resulting increase in financial crime. The rise of cryptocurrency has created opportunities for criminal activity, and the complex and decentralized nature of the technology makes it challenging to catch and prosecute offenders. To address this issue, the implementation of automated detection tools and policies is essential. These tools can help detect and prevent financial crimes in the cryptocurrency realm, ensuring a safer environment for users and investors. By utilizing technology to monitor and enforce regulations, the DeFi community can mitigate the risks associated with criminal activity and maintain the integrity of the market. <div>
arXiv:2506.14933v1 Announce Type: new 
Abstract: The decentralized finance (DeFi) community has grown rapidly in recent years, pushed forward by cryptocurrency enthusiasts interested in the vast untapped potential of new markets. The surge in popularity of cryptocurrency has ushered in a new era of financial crime. Unfortunately, the novelty of the technology makes the task of catching and prosecuting offenders particularly challenging. Thus, it is necessary to implement automated detection tools related to policies to address the growing criminality in the cryptocurrency realm.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing Structural Vibrations via Guided Flow Matching Design Optimization</title>
<link>https://arxiv.org/abs/2506.15263</link>
<guid>https://arxiv.org/abs/2506.15263</guid>
<content:encoded><![CDATA[
<div> Flow matching, design optimization, structural vibrations, plate-like structures, generative model

Summary: 
This article introduces a novel design optimization approach for reducing structural vibrations in plate-like structures by incorporating guided flow matching. The method combines a generative flow matching model with a surrogate model trained to predict structural vibrations, aiming to create manufacturable designs with low vibrations. By leveraging the flow matching model and its training data, the approach explores a wide range of potential solutions without the need for manually-defined design parameters. Various optimization objectives, including direct optimization of specific eigenfrequencies, are considered to improve passenger comfort in engineering systems like cars, trains, and airplanes. Results show that the method outperforms random search, criterion-based design heuristics, and genetic optimization in generating diverse and effective plate designs with reduced vibrations. The code and data for this approach are publicly available for further research and application. 

<br /><br />Summary: <div>
arXiv:2506.15263v1 Announce Type: new 
Abstract: Structural vibrations are a source of unwanted noise in engineering systems like cars, trains or airplanes. Minimizing these vibrations is crucial for improving passenger comfort. This work presents a novel design optimization approach based on guided flow matching for reducing vibrations by placing beadings (indentations) in plate-like structures. Our method integrates a generative flow matching model and a surrogate model trained to predict structural vibrations. During the generation process, the flow matching model pushes towards manufacturability while the surrogate model pushes to low-vibration solutions. The flow matching model and its training data implicitly define the design space, enabling a broader exploration of potential solutions as no optimization of manually-defined design parameters is required. We apply our method to a range of differentiable optimization objectives, including direct optimization of specific eigenfrequencies through careful construction of the objective function. Results demonstrate that our method generates diverse and manufacturable plate designs with reduced structural vibrations compared to designs from random search, a criterion-based design heuristic and genetic optimization. The code and data are available from https://github.com/ecker-lab/Optimizing_Vibrating_Plates.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation of parametrized cardiac electrophysiology in three dimensions using physics-informed neural networks</title>
<link>https://arxiv.org/abs/2506.15405</link>
<guid>https://arxiv.org/abs/2506.15405</guid>
<content:encoded><![CDATA[
<div> PINNs, cardiac electrophysiology, neural networks, Aliev-Panfilov model, 3D prediction<br />
<br />
Summary:
Physics-informed neural networks (PINNs) are used in cardiac electrophysiology to predict myocardium activity in 3D based on the Aliev-Panfilov model. An FCNN architecture is employed with scaled input normalization and the strong form of partial differential equations. Training data is generated using the finite element method. Variations in spatial dimensions and parameters are studied with comparison to FE simulations. Losses in the network are weighted and controlled to optimize training and prediction accuracy. By investigating optimal hyperparameters, this study aims to improve the accuracy of predicting action potential and recovery variable fields in the myocardium. <div>
arXiv:2506.15405v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) are extensively used to represent various physical systems across multiple scientific domains. The same can be said for cardiac electrophysiology, wherein fully-connected neural networks (FCNNs) have been employed to predict the evolution of an action potential in a 2D space following the two-parameter phenomenological Aliev-Panfilov (AP) model. In this paper, the training behaviour of PINNs is investigated to determine optimal hyperparameters to predict the electrophysiological activity of the myocardium in 3D according to the AP model, with the inclusion of boundary and material parameters. An FCNN architecture is employed with the governing partial differential equations in their strong form, which are scaled consistently with normalization of network inputs. The finite element (FE) method is used to generate training data for the network. Numerical examples with varying spatial dimensions and parameterizations are generated using the trained models. The network predicted fields for both the action potential and the recovery variable are compared with the respective FE simulations. Network losses are weighed with individual scalar values. Their effect on training and prediction is studied to arrive at a method of controlling losses during training.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An improved point-to-surface contact algorithm with penalty method for peridynamics</title>
<link>https://arxiv.org/abs/2408.06556</link>
<guid>https://arxiv.org/abs/2408.06556</guid>
<content:encoded><![CDATA[
<div> contact forces, peridynamics, point-to-surface, accuracy, surface particles identification

Summary:
The article proposes an improved point-to-surface contact model for accurate contact force determination in peridynamics simulations, especially for complex geometries. The outer surface is identified using an eigenvalue method, and a Verlet list efficiently identifies potential contact particle pairs. A point-to-surface contact search algorithm, coupled with a penalty function method, is used to calculate precise contact locations and forces. Validation through various contact examples confirms high accuracy, aligning well with Hertz contact theory solutions. The model automatically recognizes external surface particles and accurately computes contact forces, providing valuable insights for multi-body and complex contact scenarios. <br /><br />Summary: <div>
arXiv:2408.06556v2 Announce Type: replace 
Abstract: It is significantly challenging to obtain accurate contact forces in peridynamics (PD) simulations due to the difficulty of surface particles identification, particularly for complex geometries. Here, an improved point-to-surface contact model is proposed for PD with high accuracy. First, the outer surface is identified using the eigenvalue method and then we construct a Verlet list to identify potential contact particle pairs efficiently. Subsequently, a point-to-surface contact search algorithm is utilized to determine precise contact locations with the penalty function method calculating the contact force. Finally, the accuracy of this point-to-surface contact model is validated through several representative contact examples. The results demonstrate that the point-to-surface contact model model can predict contact forces and deformations with high accuracy, aligning well with the classical Hertz contact theory solutions. This work presents a contact model for PD that automatically recognizes external surface particles and accurately calculates the contact force, which provides guidance for the study of multi-body contact as well as complex contact situations.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing oncology with federated learning: transcending boundaries in breast, lung, and prostate cancer. A systematic review</title>
<link>https://arxiv.org/abs/2408.05249</link>
<guid>https://arxiv.org/abs/2408.05249</guid>
<content:encoded><![CDATA[
<div> FL, oncology, federated learning, cancer, data privacy
<br />
Summary:
This systematic review explores the state-of-the-art of Federated Learning (FL) in oncology, focusing on breast, lung, and prostate cancer. The review highlights FL as a promising solution to overcome privacy concerns and utilize diverse, multi-center data in clinical settings. FL showed superior performance compared to centralised machine learning in 15 out of 25 studies, demonstrating its potential to enhance ML generalisability and data privacy. Despite challenges in reproducibility and methodology standardisation, FL proved effective in integrating multi-modal information for precision medicine. The review calls for future research to address these limitations and explore advanced FL methods to fully leverage data diversity in advancing cancer research. Ultimately, FL presents significant potential for transforming cancer care through its ability to harness real-world data and address clinical needs. 
<br /><br /> <div>
arXiv:2408.05249v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has emerged as a promising solution to address the limitations of centralised machine learning (ML) in oncology, particularly in overcoming privacy concerns and harnessing the power of diverse, multi-center data. This systematic review synthesises current knowledge on the state-of-the-art FL in oncology, focusing on breast, lung, and prostate cancer. Distinct from previous surveys, our comprehensive review critically evaluates the real-world implementation and impact of FL on cancer care, demonstrating its effectiveness in enhancing ML generalisability, performance and data privacy in clinical settings and data. We evaluated state-of-the-art advances in FL, demonstrating its growing adoption amid tightening data privacy regulations. FL outperformed centralised ML in 15 out of the 25 studies reviewed, spanning diverse ML models and clinical applications, and facilitating integration of multi-modal information for precision medicine. Despite the current challenges identified in reproducibility, standardisation and methodology across studies, the demonstrable benefits of FL in harnessing real-world data and addressing clinical needs highlight its significant potential for advancing cancer research. We propose that future research should focus on addressing these limitations and investigating further advanced FL methods, to fully harness data diversity and realise the transformative power of cutting-edge FL in cancer care.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy conservative and entropy stable solid wall boundary conditions for the resistive magnetohydrodynamic equations</title>
<link>https://arxiv.org/abs/2412.11132</link>
<guid>https://arxiv.org/abs/2412.11132</guid>
<content:encoded><![CDATA[
<div> diagonal-norm, summation-by-parts, entropy conservative, resistive magnetohydrodynamic equations, boundary conditions <br />
<br />
Summary: <br /> 
This article presents a novel technique for imposing non-linear entropy conservative and entropy stable wall boundary conditions for the resistive magnetohydrodynamic equations in various scenarios. The method relies on diagonal-norm, summation-by-parts, and simultaneous-approximation-term operators for ensuring entropy stability. By using a penalty flux approach and simultaneous-approximation-term technique, boundary data at the wall are weakly imposed. The proposed technique demonstrates accuracy, robustness, and efficacy in enforcing boundary conditions on high-order unstructured grids. It is shown to be compatible with various spatial operators, including finite element and finite volume schemes. Numerical simulations confirm the stability of the method in three-dimensional flows, making it a valuable tool for accurately modeling complex MHD systems. <div>
arXiv:2412.11132v2 Announce Type: replace-cross 
Abstract: We present a novel technique for imposing non-linear entropy conservative and entropy stable wall boundary conditions for the resistive magnetohydrodynamic equations in the presence of an adiabatic wall or a wall with a prescribed heat entropy flow, addressing three scenarios: electrically insulating walls, thin walls with finite conductivity, and perfectly conducting walls. The procedure relies on the formalism and mimetic properties of diagonal-norm, summation-by-parts, and simultaneous-approximation-term operators. Using the method of lines, a semi-discrete entropy estimate for the entire domain is obtained when the proposed numerical imposition of boundary conditions is coupled with an entropy-conservative or entropy-stable discrete interior operator. The resulting estimate mimics the global entropy estimate obtained at the continuous level. The boundary data at the wall are weakly imposed using a penalty flux approach and a simultaneous-approximation-term technique for both the conservative variables and the gradient of the entropy variables. Discontinuous spectral collocation operators (mass lumped nodal discontinuous Galerkin operators) on high-order unstructured grids are used to demonstrate the new procedure's accuracy, robustness, and efficacy for weakly enforcing boundary conditions. Numerical simulations confirm the non-linear stability of the proposed technique, with applications to three-dimensional flows. The procedure described is compatible with any diagonal-norm summation-by-parts spatial operator, including finite element, finite difference, finite volume, nodal and modal discontinuous Galerkin, and flux reconstruction schemes.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Chain Arbitrage: The Next Frontier of MEV in Decentralized Finance</title>
<link>https://arxiv.org/abs/2501.17335</link>
<guid>https://arxiv.org/abs/2501.17335</guid>
<content:encoded><![CDATA[
<div> arbitrage, DeFi, decentralized exchanges, cross-chain, inventory-based execution 

Summary: 
The study focuses on cross-chain arbitrage in decentralized finance (DeFi) markets, where prices are aligned through arbitrage between Layer-1 (L1) and Layer-2 (L2) blockchains. Centralized exchanges (CEXes) currently play a key role in price discovery, but as trading volume shifts on-chain, cross-chain arbitrage between decentralized exchanges (DEXes) is becoming more significant. The research includes a profit-cost model and a year-long measurement of arbitrage activity across nine blockchains. Findings show a high concentration of market activity on Ethereum-centric pairs, with a surge in volume after a blockchain upgrade. Most trades use pre-positioned inventory and settle quickly, while bridge-based arbitrages face latency issues. The study highlights the centralized nature of cross-chain arbitrage, leading to vertical integration and concentration of economic power, posing risks of censorship and centralization in the DeFi space. Decentralizing block building and reducing entry barriers are suggested as critical measures to address these challenges.<br /><br />Summary: <div>
arXiv:2501.17335v2 Announce Type: replace-cross 
Abstract: Decentralized finance (DeFi) markets spread across Layer-1 (L1) and Layer-2 (L2) blockchains rely on arbitrage to keep prices aligned. Today most price gaps are closed against centralized exchanges (CEXes), whose deep liquidity and fast execution make them the primary venue for price discovery. As trading volume migrates on-chain, cross-chain arbitrage between decentralized exchanges (DEXes) will become the canonical mechanism for price alignment. Yet, despite its importance to DeFi-and the on-chain transparency making real activity tractable in a way CEX-to-DEX arbitrage is not-existing research remains confined to conceptual overviews and hypothetical opportunity analyses.
  We study cross-chain arbitrage with a profit-cost model and a year-long measurement. The model shows that opportunity frequency, bridging time, and token depreciation determine whether inventory- or bridge-based execution is more profitable. Empirically, we analyze one year of transactions (September 2023 - August 2024) across nine blockchains and identify 242,535 executed arbitrages totaling 868.64 million USD volume. Activity clusters on Ethereum-centric L1-L2 pairs, grows 5.5x over the study period, and surges-higher volume, more trades, lower fees-after the Dencun upgrade (March 13, 2024). Most trades use pre-positioned inventory (66.96%) and settle in 9s, whereas bridge-based arbitrages take 242s, underscoring the latency cost of today's bridges. Market concentration is high: the five largest addresses execute more than half of all trades, and one alone captures almost 40% of daily volume post-Dencun. We conclude that cross-chain arbitrage fosters vertical integration, centralizing sequencing infrastructure and economic power and thereby exacerbating censorship, liveness, and finality risks; decentralizing block building and lowering entry barriers are critical to countering these threats.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Oder Splitting Schemes for Fluids with Variable Viscosity</title>
<link>https://arxiv.org/abs/2506.14424</link>
<guid>https://arxiv.org/abs/2506.14424</guid>
<content:encoded><![CDATA[
<div> Keywords: matrix-free, discontinuous Galerkin, Navier-Stokes equations, variable viscosity, nonlinear solver<br />
Summary: <br />
This article explores matrix-free higher-order discontinuous Galerkin (DG) discretizations of the Navier-Stokes equations for incompressible flows with variable viscosity. The study involves comparing linearized variants of saddle point block systems and projection-based splitting time integration schemes in terms of computational performance. The research extends the dual splitting method for non-constant viscosity, presents a higher-order DG method for incompressible flows with variable viscosity, introduces accelerated nonlinear solver variants, and evaluates monolithic and projection-based solvers in terms of their solver performance. The schemes are tested in numerical examples to verify spatial and temporal accuracy, preconditioner performance under increased viscosity contrasts, and efficiency in the backward-facing step benchmark. The study investigates conditions under which fully implicit schemes with improved temporal stability and expensive nonlinear solves outperform stable linearized variants and splitting schemes. <div>
arXiv:2506.14424v1 Announce Type: new 
Abstract: This article investigates matrix-free higher-order discontinuous Galerkin (DG) discretizations of the Navier-Stokes equations for incompressible flows with variable viscosity. The viscosity field may be prescribed analytically or governed by a rheological law, as often found in biomedical or industrial applications. We compare several linearized variants of saddle point block systems and projection-based splitting time integration schemes in terms of their computational performance. Compared to the velocity-pressure block-system for the former, the splitting scheme allows solving a sequence of simple problems such as mass, convection-diffusion and Poisson equations. We investigate under which conditions the improved temporal stability of fully implicit schemes and resulting expensive nonlinear solves outperform the splitting schemes and linearized variants that are stable under hyperbolic time step restrictions.
  The key aspects of this work are i) the extension of the dual splitting method originally proposed by G.E. Karniadakis et al. (J. Comput. Phys. 97, 414-443, 1991) towards non-constant viscosity, ii) a higher-order DG method for incompressible flows with variable viscosity, iii) accelerated nonlinear solver variants and suitable linearizations adopting a matrix-free $hp$-multigrid solver, and iv) a detailed comparison of the monolithic and projection-based solvers in terms of their (non-)linear solver performance.
  The presented schemes are evaluated in a series of numerical examples verifying their spatial and temporal accuracy, and the preconditioner performance under increasing viscosity contrasts, while their efficiency is showcased in the backward-facing step benchmark.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Digital Twins via Active Inference</title>
<link>https://arxiv.org/abs/2506.14453</link>
<guid>https://arxiv.org/abs/2506.14453</guid>
<content:encoded><![CDATA[
<div> Keywords: digital twin, active inference, Bayesian framework, predictive maintenance, railway bridge

Summary:
The paper introduces the concept of active digital twins, which leverage active inference, a neuroscience-inspired Bayesian framework, to enhance adaptability in uncertain and dynamic environments. By formulating the evolution of the active digital twin as a partially observable Markov decision process, the agent continuously refines its generative model through Bayesian updates. Decision-making is based on balancing exploitation and exploration, with actions planned to minimize expected free energy. This approach offers superior exploration capabilities, increasing autonomy and resilience in digital twins. The framework is applied to the health monitoring and predictive maintenance of a railway bridge, demonstrating its effectiveness in real-world applications. <div>
arXiv:2506.14453v1 Announce Type: new 
Abstract: Digital twins are transforming engineering and applied sciences by enabling real-time monitoring, simulation, and predictive analysis of physical systems and processes. However, conventional digital twins rely primarily on passive data assimilation, which limits their adaptability in uncertain and dynamic environments. This paper introduces the active digital twin paradigm, based on active inference. Active inference is a neuroscience-inspired, Bayesian framework for probabilistic reasoning and predictive modeling that unifies inference, decision-making, and learning under a unique, free energy minimization objective. By formulating the evolution of the active digital twin as a partially observable Markov decision process, the active inference agent continuously refines its generative model through Bayesian updates and forecasts future states and observations. Decision-making emerges from an optimization process that balances pragmatic exploitation (maximizing goal-directed utility) and epistemic exploration or information gain (actively resolving uncertainty). Actions are dynamically planned to minimize expected free energy, which quantifies both the divergence between predicted and preferred future observations, and the epistemic value of expected information gain about hidden states. This approach enables a new level of autonomy and resilience in digital twins, offering superior spontaneous exploration capabilities. The proposed framework is assessed on the health monitoring and predictive maintenance of a railway bridge.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Framework for Climate-Resilient Insurance and Real Estate Decisions</title>
<link>https://arxiv.org/abs/2506.14638</link>
<guid>https://arxiv.org/abs/2506.14638</guid>
<content:encoded><![CDATA[
<div> Keywords: Extreme weather events, Insurance viability, Building protection, Climate risks, Sustainability

Summary: 
The SSC-Insurance Model, combining SMOTE, SVM, and C-D-C algorithms, evaluates weather impacts on policies and investments, achieving high accuracy in Zhejiang and Ireland. It determines a critical threshold (43% weather increase) for insurance viability. The TOA-Preservation Model, utilizing TOPSIS-ORM and AHP, prioritizes building protection, with cultural value deemed most significant. Case studies on Nanxun Ancient Town reveal a 65.32% insurability probability and a protection score of 0.512. This research equips insurers, developers, and policymakers with practical tools to manage climate risks sustainably.<br /><br />Summary: <div>
arXiv:2506.14638v1 Announce Type: new 
Abstract: Extreme weather events increasingly threaten the insurance and real estate industries, creating conflicts between profitability and homeowner burdens. To address this, we propose the SSC-Insurance Model, which integrates SMOTE, SVM, and C-D-C algorithms to evaluate weather impacts on policies and investments. Our model achieves 88.3% accuracy in Zhejiang and 79.6% in Ireland, identifying a critical threshold (43% weather increase) for insurance viability. Additionally, we develop the TOA-Preservation Model using TOPSIS-ORM and AHP to prioritize building protection, with cultural value scoring highest (weight: 0.3383). Case studies on Nanxun Ancient Town show a 65.32% insurability probability and a protection score of 0.512. This work provides actionable tools for insurers, developers, and policymakers to manage climate risks sustainably.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimistic MEV in Ethereum Layer 2s: Why Blockspace Is Always in Demand</title>
<link>https://arxiv.org/abs/2506.14768</link>
<guid>https://arxiv.org/abs/2506.14768</guid>
<content:encoded><![CDATA[
<div> optimistic MEV, Layer 2 rollups, DeFi, on-chain arbitrage, Ethereum gas fees
<br />
Summary:
Layer 2 rollups in DeFi have absorbed over $40 billion and nearly half of Ethereum's DEX volume by Q1 2025, but their MEV dynamics are understudied. This study defines and quantifies optimistic MEV, a speculative on-chain arbitrage method prevalent on Arbitrum, Base, and Optimism. In Q1 2025, optimistic MEV accounted for over 50% of on-chain gas on Base and Optimism and 7% on Arbitrum, mainly driven by on-chain computations for arbitrage detection. Despite high gas consumption, optimistic MEV transactions pay only a fraction of total gas fees. Different success rates, code reuse patterns, and sensitivities to sequencing and block production times across networks were observed. OLS regressions linked optimistic MEV trades to ETH volatility, retail trading, and DEX aggregator usage, highlighting how Layer 2 protocols uniquely incentivize speculative MEV. 
<br /> <div>
arXiv:2506.14768v1 Announce Type: new 
Abstract: Layer 2 rollups are rapidly absorbing DeFi activity, securing over $40 billion and accounting for nearly half of Ethereum's DEX volume by Q1 2025, yet their MEV dynamics remain understudied. We address this gap by defining and quantifying optimistic MEV, a form of speculative, on-chain cyclic arbitrage whose detection and execution logic reside largely on-chain in smart contracts. As a result of their speculative nature and lack of off-chain opportunity verification, optimistic MEV transactions frequently fail to execute a profitable arbitrage.
  Applying our multi-stage identification pipeline to Arbitrum, Base, and Optimism, we find that in Q1 2025, optimistic MEV accounts for over 50% of on-chain gas on Base and Optimism and 7% on Arbitrum, driven mainly by "interaction" probes (on-chain computations searching for arbitrage). This speculative probing keeps blocks on Base and Optimism persistently full. Despite consuming over half of on-chain gas, optimistic MEV transactions pay less than one quarter of total gas fees. Cross-network comparison reveals divergent success rates, differing patterns of code reuse, and sensitivity to varying sequencer ordering and block production times. Finally, OLS regressions link optimistic MEV trade count to ETH volatility, retail trading activity, and DEX aggregator usage, showing how Layer 2 protocol parameters uniquely encourage speculative MEV.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Charging Scheduling via Balanced Bounding Box Methods</title>
<link>https://arxiv.org/abs/2506.14461</link>
<guid>https://arxiv.org/abs/2506.14461</guid>
<content:encoded><![CDATA[
<div> Keywords: Electric mobility, charging infrastructure, sustainable urban logistics, collaborative scheduling, bi-objective optimization

Summary:
The paper examines the challenges facing electric mobility and proposes shared charging as a solution. It focuses on sustainable urban logistics by facilitating collaboration between fleet operators. A bi-objective nonlinear integer programming model is formulated to address the scheduling problem for shared charging stations, balancing the cost minimization goals of each company. The Balanced Bounding Box Methods (B3Ms) are introduced to efficiently derive the efficient frontier, reducing computational time while maintaining solution diversity. Cooperative bargaining methods are applied to determine the final solution and promote balanced collaboration. Numerical case studies demonstrate the effectiveness and scalability of the developed methods, showcasing their potential for solving various bi-objective optimization problems beyond the collaborative scheduling issue presented in the study.<br /><br />Summary: <div>
arXiv:2506.14461v1 Announce Type: cross 
Abstract: Electric mobility faces several challenges, most notably the high cost of infrastructure development and the underutilization of charging stations. The concept of shared charging offers a promising solution. The paper explores sustainable urban logistics through horizontal collaboration between two fleet operators and addresses a scheduling problem for the shared use of charging stations. To tackle this, the study formulates a collaborative scheduling problem as a bi-objective nonlinear integer programming model, in which each company aims to minimize its own costs, creating inherent conflicts that require trade-offs. The Balanced Bounding Box Methods (B3Ms) are introduced in order to efficiently derive the efficient frontier, identifying a reduced set of representative solutions. These methods enhance computational efficiency by selectively disregarding closely positioned and competing solutions, preserving the diversity and representativeness of the solutions over the efficient frontier. To determine the final solution and ensure balanced collaboration, cooperative bargaining methods are applied. Numerical case studies demonstrate the viability and scalability of the developed methods, showing that the B3Ms can significantly reduce computational time while maintaining the integrity of the frontier. These methods, along with cooperative bargaining, provide an effective framework for solving various bi-objective optimization problems, extending beyond the collaborative scheduling problem presented here.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and scalable exchange-correlation with deep learning</title>
<link>https://arxiv.org/abs/2506.14665</link>
<guid>https://arxiv.org/abs/2506.14665</guid>
<content:encoded><![CDATA[
<div> machine learning, density functional theory, electronic structure, predictive modeling, atomization energies

Summary:
Skala is a new deep learning-based exchange-correlation (XC) functional designed to improve the accuracy and generality of Density Functional Theory (DFT) for predicting molecular and material properties. Unlike traditional XC functionals, Skala learns representations directly from data without relying on hand-crafted features. By training on a large dataset of high-accuracy reference data, Skala achieves chemical accuracy for atomization energies of small molecules while maintaining computational efficiency. It improves systematically with additional diverse chemistry training data, competing with hybrid functionals in general main group chemistry at the cost of semi-local DFT. Skala's performance is expected to further enhance the predictive power of first-principles simulations as the training dataset expands. 

<br /><br />Summary: <div>
arXiv:2506.14665v1 Announce Type: cross 
Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\"odinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative analysis for different finite element types in strain-gradient elasticity simulations performed on Firedrake and FEniCS</title>
<link>https://arxiv.org/abs/2411.12043</link>
<guid>https://arxiv.org/abs/2411.12043</guid>
<content:encoded><![CDATA[
<div> Keywords: additive manufacturing, metamaterials, finite element methods, strain-gradient elasticity, open-source packages<br />
<br />
Summary: 
This study focuses on the use of finite element methods to solve problems in strain-gradient elasticity in architectured materials such as metallic foams. The research compares different finite element formulations, including Lagrange, Argyris, Hermite elements, a mixed formulation, and isogeometric analysis with NURBS. Open-source packages from Firedrake and FEniCS are utilized for the study. The numerical investigation includes one- and two-dimensional problems commonly found in strain-gradient modeling literature. The developed codes are openly accessible to promote research in FEM-based computation of generalized continua. <div>
arXiv:2411.12043v2 Announce Type: replace 
Abstract: The layer-upon-layer approach in additive manufacturing, open or closed cells in polymeric or metallic foams involve an intrinsic microstructure tailored to the underlying applications. Homogenization of such architectured materials creates metamaterials modeled by higher-gradient models, specifically when the microstructure's characteristic length is comparable to the length scale of the structure. In this study, we conduct a comparative analysis of various finite elements methods for solving problems in strain-gradient elasticity. We employ open-source packages from Firedrake and FEniCS. Different finite element formulations are tested: we implement Lagrange, Argyris, Hermite elements, a Hu--Washizu type (mixed) formulation, as well as isogeometric analysis with Non-Uniform Rational B-Splines (NURBS). For the numerical study, we investigate one- and two-dimensional problems discussed in the literature of strain-gradient modeling. All developed codes are open-access to encourage research in Finite Element Method (FEM) based computation of generalized continua.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Selection Format on LLM Performance</title>
<link>https://arxiv.org/abs/2503.06926</link>
<guid>https://arxiv.org/abs/2503.06926</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, classification task, prompts, formatting, performance <br />
Summary: <br />
This paper delves into the impact of different formatting options, specifically bullet points versus plain English, on the performance of large language models (LLMs) in classification tasks. The extensive experimental study conducted by the researchers revealed that presenting classification options via bullet points generally led to better model performance, although there were some exceptions to this trend. The study emphasizes the significance of carefully considering the formatting of options in prompts to enhance the overall performance of LLMs. The research further indicates the need for ongoing exploration and experimentation with various formatting techniques to continue driving improvements in model performance. The findings suggest that the presentation of classification task options can significantly influence the efficiency and effectiveness of LLMs, underscoring the importance of this aspect in the development and optimization of language models. <br /><br />Summary: <div>
arXiv:2503.06926v2 Announce Type: replace-cross 
Abstract: This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks</title>
<link>https://arxiv.org/abs/2503.16974</link>
<guid>https://arxiv.org/abs/2503.16974</guid>
<content:encoded><![CDATA[
<div> consistency, reproducibility, Large Language Model, finance, accounting

Summary:
This study evaluates the consistency and reproducibility of Large Language Models (LLMs) in finance and accounting research. Through extensive experimentation with three OpenAI models and 50 independent runs across five tasks, the study found task-dependent consistency with binary classification and sentiment analysis achieving near-perfect reproducibility. While more advanced models did not consistently demonstrate better consistency, task-specific patterns emerged. LLMs outperformed human annotators in consistency and agreement, even in cases where experts disagreed. Simple aggregation strategies across 3-5 runs improved consistency and accuracy for sentiment analysis. Despite some inconsistency in LLM outputs, downstream statistical inferences remained robust. The study addressed concerns of "G-hacking" by showing that risks of selective reporting from multiple LLM runs are relatively low for finance and accounting tasks. <div>
arXiv:2503.16974v3 Announce Type: replace-cross 
Abstract: This study provides the first comprehensive assessment of consistency and reproducibility in Large Language Model (LLM) outputs in finance and accounting research. We evaluate how consistently LLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs across five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models (GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse financial source texts and data, covering MD&amp;As, FOMC statements, finance news articles, earnings call transcripts, and financial statements. Our findings reveal substantial but task-dependent consistency, with binary classification and sentiment analysis achieving near-perfect reproducibility, while complex tasks show greater variability. More advanced models do not consistently demonstrate better consistency and reproducibility, with task-specific patterns emerging. LLMs significantly outperform expert human annotators in consistency and maintain high agreement even where human experts significantly disagree. We further find that simple aggregation strategies across 3-5 runs dramatically improve consistency. We also find that aggregation may come with an additional benefit of improved accuracy for sentiment analysis when using newer models. Simulation analysis reveals that despite measurable inconsistency in LLM outputs, downstream statistical inferences remain remarkably robust. These findings address concerns about what we term "G-hacking," the selective reporting of favorable outcomes from multiple Generative AI runs, by demonstrating that such risks are relatively low for finance and accounting tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Characterization of Aggregate Flexibility via Generalized Polymatroids</title>
<link>https://arxiv.org/abs/2503.23458</link>
<guid>https://arxiv.org/abs/2503.23458</guid>
<content:encoded><![CDATA[
<div> flexibility, distributed energy resources, aggregation, Minkowski sum, optimization <br />
Summary: 
The article discusses the importance of leveraging the flexibility of distributed energy resources (DERs) to address issues related to renewable generation variability. It highlights the challenge of accurately computing the aggregate flexibility of a population, proposing the use of generalized polymatroids to efficiently represent flexibility sets. The study shows that individual flexibility sets can be categorized under this family, facilitating the exact computation of their Minkowski sum. For homogeneous DER populations, simplified representations of aggregate flexibility are derived. An optimization framework is developed to efficiently allocate aggregate flexibility among individual DERs, with a vertex-based disaggregation method proposed. The approach's optimality and computational efficiency are validated through comparisons with existing methods. <div>
arXiv:2503.23458v2 Announce Type: replace-cross 
Abstract: It is well established that the aggregate flexibility inherent in populations of distributed energy resources (DERs) can be leveraged to mitigate the intermittency and uncertainty associated with renewable generation, while also providing ancillary grid services. To enable this, aggregators must effectively represent the flexibility in the populations they control to the market or system operator. A key challenge is accurately computing the aggregate flexibility of a population, which can be formally expressed as the Minkowski sum of a collection of polytopes, a problem that is generally computationally intractable. However, the flexibility polytopes of many DERs exhibit structural symmetries that can be exploited for computational efficiency. To this end, we introduce generalized polymatroids, a family of polytopes, into the flexibility aggregation literature. We demonstrate that individual flexibility sets belong to this family, enabling efficient computation of their exact Minkowski sum. For homogeneous populations of DERs we further derive simplifications that yield more succinct representations of aggregate flexibility. Additionally, we develop an efficient optimization framework over these sets and propose a vertex-based disaggregation method, to allocate aggregate flexibility among individual DERs. Finally, we validate the optimality and computational efficiency of our approach through comparisons with existing methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Classification of Levantine Ceramic Thin Sections via Neural Networks</title>
<link>https://arxiv.org/abs/2506.12250</link>
<guid>https://arxiv.org/abs/2506.12250</guid>
<content:encoded><![CDATA[
<div> deep learning, Convolutional Neural Networks, Vision Transformers, petrographic analysis, Levantine ceramics 

Summary:
Classification of ceramic thin sections is essential for understanding ancient pottery production techniques and trade networks. This study explores the use of deep learning models like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to classify Levantine ceramics based on their petrographic fabrics. A dataset of 1,424 thin section images from archaeological sites in the Levantine area was used to train these models, with transfer learning significantly improving classification accuracy. The ResNet18 model achieved 92.11% accuracy, while the ViT reached 88.34%. Explainability techniques such as Guided Grad-CAM and attention maps were used to interpret the models' decisions, showing that both CNNs and ViTs focus on key mineralogical features for classification. This study demonstrates the potential of explainable AI in archaeometric studies, providing an efficient and transparent methodology for ceramic analysis. 

<br /><br />Summary: <div>
arXiv:2506.12250v1 Announce Type: new 
Abstract: Classification of ceramic thin sections is fundamental for understanding ancient pottery production techniques, provenance, and trade networks. Although effective, traditional petrographic analysis is time-consuming. This study explores the application of deep learning models, specifically Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), as complementary tools to support the classification of Levantine ceramics based on their petrographic fabrics. A dataset of 1,424 thin section images from 178 ceramic samples belonging to several archaeological sites across the Levantine area, mostly from the Bronze Age, with few samples dating to the Iron Age, was used to train and evaluate these models. The results demonstrate that transfer learning significantly improves classification performance, with a ResNet18 model achieving 92.11% accuracy and a ViT reaching 88.34%. Explainability techniques, including Guided Grad-CAM and attention maps, were applied to interpret and visualize the models' decisions, revealing that both CNNs and ViTs successfully focus on key mineralogical features for the classification of the samples into their respective petrographic fabrics. These findings highlight the potential of explainable AI in archaeometric studies, providing a reproducible and efficient methodology for ceramic analysis while maintaining transparency in model decision-making.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified Newmark/Newton-Raphson method with automatic differentiation for general nonlinear dynamics analysis</title>
<link>https://arxiv.org/abs/2506.13226</link>
<guid>https://arxiv.org/abs/2506.13226</guid>
<content:encoded><![CDATA[
<div> automatic differentiation, Newmark/Newton-Raphson method, nonlinear dynamic systems, Jacobian matrix, complex nonlinear systems 

Summary: 
The study introduces the NNR-AD method, which integrates automatic differentiation into the Newmark/Newton-Raphson method to address limitations in solving complex nonlinear dynamic systems. By utilizing automatic differentiation, the NNR-AD method improves the NNR method's capability to handle complex nonlinear characteristics, simplifies the computation of Jacobian matrices, and enhances modularity for effective solutions. The NNR-AD method has been demonstrated to directly solve dynamic systems with complex nonlinear features, with rigorous validation of its accuracy and generality. This integration of automatic differentiation offers a significant advancement in solving complex nonlinear dynamic systems, providing a more efficient and effective approach compared to traditional methods. <br /><br /> <div>
arXiv:2506.13226v1 Announce Type: new 
Abstract: The Newmark/Newton-Raphson (NNR) method is widely employed for solving nonlinear dynamic systems. However, the current NNR method exhibits limited applicability in complex nonlinear dynamic systems, as the acquisition of the Jacobian matrix required for Newton iterations incurs substantial computational costs and may even prove intractable in certain cases. To address these limitations, we integrate automatic differentiation (AD) into the NNR method, proposing a modified NNR method with AD (NNR-AD) to significantly improve its capability for effectively handling complex nonlinear systems. We have demonstrated that the NNR-AD method can directly solve dynamic systems with complex nonlinear characteristics, and its accuracy and generality have been rigorously validated. Furthermore, automatic differentiation significantly simplifies the computation of Jacobian matrices for such complex nonlinear dynamic systems. This improvement endows the NNR method with enhanced modularity, thereby enabling convenient and effective solutions for complex nonlinear dynamic systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Entropy-Stable/Double-Flux scheme for the multi-component compressible Navier-Stokes equations</title>
<link>https://arxiv.org/abs/2506.13231</link>
<guid>https://arxiv.org/abs/2506.13231</guid>
<content:encoded><![CDATA[
<div> Entropy-Stable Formulation, Double-Flux Scheme, Multi-Component Flows, Specific Heat Ratios, Compressible Flow<br />
Summary:<br />
The article introduces a novel numerical approach for improving multi-component compressible flow simulations. It combines an Entropy-Stable formulation and a Double-Flux scheme designed for multi-component flows. This approach ensures low-dissipation, oscillation-free solutions with enhanced stability compared to traditional methods. A hybrid dissipation strategy further improves robustness by blending the new approach with conventional dissipation mechanisms while maintaining consistency with the second law of thermodynamics. The method also utilizes an explicit Runge-Kutta scheme and adaptive mesh refinement for efficient time integration and capturing local flow features dynamically. Implemented in an existing compressible Navier-Stokes solver based on OpenFOAM, benchmark cases demonstrate the effectiveness of the framework in handling multi-dimensional interface and shock-interface interactions. The results confirm its favorable stability and robustness, making it a promising advancement for high-fidelity simulations of supersonic flows.<br /><br /> <div>
arXiv:2506.13231v1 Announce Type: new 
Abstract: We present a novel combination of numerical techniques to improve the efficiency, accuracy, and robustness of multi-component compressible flow simulations. At the core of our approach is an Entropy-Stable formulation that preserves kinetic energy and integrates a Double-Flux scheme tailored for multi-component flows with variable specific heat ratios. This formulation yields low-dissipation, oscillation-free solutions and enhances stability compared to standard fully conservative methods. To further improve robustness, we introduce a new hybrid dissipation strategy that blends the Entropy-Stable/Double-Flux approach with conventional dissipation mechanisms. We provide a rigorous proof that the resulting numerical flux satisfies a semi-discrete entropy inequality, ensuring consistency with the second law of thermodynamics. For time integration, we employ an explicit Runge-Kutta scheme in combination with adaptive mesh refinement to capture local flow features dynamically. The method is implemented within an existing compressible Navier-Stokes solver based on OpenFOAM. Benchmark cases, including multi-dimensional interface and shock-interface interactions, demonstrate the effectiveness of the proposed framework. The results confirm its favorable stability and robustness, validating the approach as a promising advancement for high-fidelity simulations of supersonic flows.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constitutive Manifold Neural Networks</title>
<link>https://arxiv.org/abs/2506.13648</link>
<guid>https://arxiv.org/abs/2506.13648</guid>
<content:encoded><![CDATA[
<div> neural networks, manifold learning, thermal conductivity, stochastic tensors, engineering applications  
Summary:  
Constitutive Manifold Neural Networks (CMNN) are introduced to address the challenge of accurately modeling stochastic tensor properties, such as thermal conductivity, in engineering applications. These properties, represented as symmetric positive definite tensors on a curved Riemannian manifold, require preservation of their geometric properties during neural network processing. By preprocessing the tensors to a flat vector space, CMNN ensures that the input to the neural network preserves the tensor's symmetry and spatial symmetries. A case study on stochastic anisotropic conductivity in a heat conduction problem demonstrates that CMNN outperforms traditional multi-layer perceptron architectures by preserving the geometric properties of the tensors. This highlights the importance of using manifold-aware techniques in engineering applications involving tensor-valued data. <div>
arXiv:2506.13648v1 Announce Type: new 
Abstract: Important material properties like thermal conductivity are often represented as symmetric positive definite (SPD) tensors, which exhibit variability due to inherent material heterogeneity and manufacturing uncertainties. These tensors reside on a curved Riemannian manifold, and accurately modeling their stochastic nature requires preserving both their symmetric positive definite properties and spatial symmetries. To achieve this, uncertainties are parametrized into scaling (magnitude) and rotation (orientation) components, modeled as independent random variables on a manifold structure derived from the maximum entropy principle. The propagation of such stochastic tensors through physics-based simulations necessitates computationally efficient surrogate models. However, traditional multi-layer perceptron (MLP) architectures are not well-suited for SPD tensors, as directly inputting their components fails to preserve their geometric properties, often leading to suboptimal results. To address this, we introduce Constitutive Manifold Neural Networks (CMNN). This approach introduces a preprocessing layer by mapping the SPD tensor from the curved manifold to the local tangent, a flat vector space, creating an information preserving map for input to the hidden layers of the neural networks. A case study on a steady-state heat conduction problem with stochastic anisotropic conductivity demonstrates that geometry-preserving preprocessing, such as logarithmic maps for scaling data, significantly improves learning performance over conventional MLPs. These findings underscore the importance of manifold-aware techniques when working with tensor-valued data in engineering applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Network for Gene Regulatory Network Inference</title>
<link>https://arxiv.org/abs/2506.13740</link>
<guid>https://arxiv.org/abs/2506.13740</guid>
<content:encoded><![CDATA[
<div> scKAN, gene regulatory networks, single-cell RNA sequencing, explainable AI, cellular dynamics<br />
Summary: <br />
The paper introduces scKAN, a novel model utilizing a Kolmogorov-Arnold network (KAN) with explainable AI to infer gene regulatory networks (GRNs) from single-cell RNA sequencing data. scKAN models gene expression as differentiable functions to accurately detect activation and inhibition regulations through explainable AI and geometric tools. It surpasses existing signed GRN inference models in AUROC and AUPRC metrics on the BEELINE benchmark, showcasing its potential in capturing biological processes in gene regulation without prior knowledge of the graph structure. The model addresses the challenges of high dimensionality and complexity in GRN inference from scRNA-seq data, offering improved scalability, explainability, and precision in detecting regulation types and capturing continuous cellular dynamics. <div>
arXiv:2506.13740v1 Announce Type: new 
Abstract: Gene regulation is central to understanding cellular processes and development, potentially leading to the discovery of new treatments for diseases and personalized medicine. Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data presents significant challenges due to its high dimensionality and complexity. Existing tree-based models, such as GENIE3 and GRNBOOST2, demonstrated scalability and explainability in GRN inference, but they cannot distinguish regulation types nor effectively capture continuous cellular dynamics. In this paper, we introduce scKAN, a novel model that employs a Kolmogorov-Arnold network (KAN) with explainable AI to infer GRNs from scRNA-seq data. By modeling gene expression as differentiable functions matching the smooth nature of cellular dynamics, scKAN can accurately and precisely detect activation and inhibition regulations through explainable AI and geometric tools. We conducted extensive experiments on the BEELINE benchmark, and scKAN surpasses and improves the leading signed GRN inference models ranging from 5.40\% to 28.37\% in AUROC and from 1.97\% to 40.45\% in AUPRC. These results highlight the potential of scKAN in capturing the underlying biological processes in gene regulation without prior knowledge of the graph structure.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Trust at Scale: Physics-Aware Neural Watermarking for Secure and Verifiable Data Pipelines</title>
<link>https://arxiv.org/abs/2506.12032</link>
<guid>https://arxiv.org/abs/2506.12032</guid>
<content:encoded><![CDATA[
<div> framework, neural watermarking, scientific data integrity, high-dimensional fields, climate modeling

Summary:
The article presents a robust neural watermarking framework designed specifically for scientific data integrity, focusing on high-dimensional fields typically found in climate modeling and fluid simulations. Utilizing a convolutional autoencoder, binary messages can be invisibly embedded into structured data like temperature, vorticity, and geopotential. The method ensures that the watermark persists even under lossy transformations such as noise injection, cropping, and compression while maintaining high fidelity with sub-1% Mean Squared Error (MSE). Compared to traditional singular value decomposition (SVD)-based watermarking, this approach achieves over 98% bit accuracy and provides visually indistinguishable reconstructions for datasets like ERA5 and Navier-Stokes. The system serves as a scalable and model-compatible tool for ensuring data provenance, auditability, and traceability in high-performance scientific workflows. It also contributes to the larger goal of securing AI systems through verifiable watermarking that is informed by physics. The evaluation was conducted on scientifically grounded datasets, demonstrating the frameworks adaptability to other structured domains like satellite imagery and autonomous vehicle perception streams. <br /><br />Summary: <div>
arXiv:2506.12032v1 Announce Type: cross 
Abstract: We present a robust neural watermarking framework for scientific data integrity, targeting high-dimensional fields common in climate modeling and fluid simulations. Using a convolutional autoencoder, binary messages are invisibly embedded into structured data such as temperature, vorticity, and geopotential. Our method ensures watermark persistence under lossy transformations - including noise injection, cropping, and compression - while maintaining near-original fidelity (sub-1\% MSE). Compared to classical singular value decomposition (SVD)-based watermarking, our approach achieves $>$98\% bit accuracy and visually indistinguishable reconstructions across ERA5 and Navier-Stokes datasets. This system offers a scalable, model-compatible tool for data provenance, auditability, and traceability in high-performance scientific workflows, and contributes to the broader goal of securing AI systems through verifiable, physics-aware watermarking. We evaluate on physically grounded scientific datasets as a representative stress-test; the framework extends naturally to other structured domains such as satellite imagery and autonomous-vehicle perception streams.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUST: Quantifying Free-Form Geometric Uncertainty of Metamaterials Using Small Data</title>
<link>https://arxiv.org/abs/2506.12051</link>
<guid>https://arxiv.org/abs/2506.12051</guid>
<content:encoded><![CDATA[
<div> Generative Uncertainty Learning, Self-supervised pretraining, Transfer learning, Geometric Uncertainties, Metamaterials <br />
Summary:
GUST framework uses deep generative models to quantify geometric uncertainties in metamaterial manufacturing. It leverages self-supervised pretraining on synthetic data to capture structure variability and a conditional distribution of fabricated geometries. Transfer learning fine-tunes the model on real-world data to adapt to specific processes. The approach requires only 960 manufactured unit cells for effective uncertainty quantification. Directly training on limited real-world data proves insufficient compared to GUST. This cost-effective method reduces data requirements while accurately modeling complex geometric uncertainties. The scalable approach benefits high-precision industries like aerospace and biomedical engineering by enabling the understanding and mitigation of manufacturing uncertainties. <br /> <div>
arXiv:2506.12051v1 Announce Type: cross 
Abstract: This paper introduces GUST (Generative Uncertainty learning via Self-supervised pretraining and Transfer learning), a framework for quantifying free-form geometric uncertainties inherent in the manufacturing of metamaterials. GUST leverages the representational power of deep generative models to learn a high-dimensional conditional distribution of as-fabricated unit cell geometries given nominal designs, thereby enabling uncertainty quantification. To address the scarcity of real-world manufacturing data, GUST employs a two-stage learning process. First, it leverages self-supervised pretraining on a large-scale synthetic dataset to capture the structure variability inherent in metamaterial geometries and an approximated distribution of as-fabricated geometries given nominal designs. Subsequently, GUST employs transfer learning by fine-tuning the pretrained model on limited real-world manufacturing data, allowing it to adapt to specific manufacturing processes and nominal designs. With only 960 unit cells additively manufactured in only two passes, GUST can capture the variability in geometry and effective material properties. In contrast, directly training a generative model on the same amount of real-world data proves insufficient, as demonstrated through both qualitative and quantitative comparisons. This scalable and cost-effective approach significantly reduces data requirements while maintaining the effectiveness in learning complex, real-world geometric uncertainties, offering an affordable method for free-form geometric uncertainty quantification in the manufacturing of metamaterials. The capabilities of GUST hold significant promise for high-precision industries such as aerospace and biomedical engineering, where understanding and mitigating manufacturing uncertainties are critical.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Green AI Architectures for Circular Economies Through Multi-Layered Sustainable Resource Optimization Framework</title>
<link>https://arxiv.org/abs/2506.12262</link>
<guid>https://arxiv.org/abs/2506.12262</guid>
<content:encoded><![CDATA[
<div> energy-efficient, Green AI, circular economies, sustainable resource consumption, optimization techniques

Summary:<br /><br />In this research paper, a new energy-efficient Green AI architecture is proposed to support circular economies and address sustainable resource consumption. The architecture integrates machine learning algorithms, energy-conscious models, and optimization techniques to facilitate decision-making for resource reuse and waste reduction. Real-world tests on lithium-ion battery recycling and urban waste management show a 25 percent reduction in energy consumption and an 18 percent improvement in resource recovery efficiency. Mathematical models and AI algorithms improve classification accuracy and reduce transportation emissions. Graphical analyses demonstrate the framework's impact on energy efficiency and sustainability, aligning with UN Sustainability Goals. This scalable solution can contribute to sustainable management strategies and technological progress, potentially safeguarding natural capital while advancing AI technologies. <br /> <div>
arXiv:2506.12262v1 Announce Type: cross 
Abstract: In this research paper, we propose a new type of energy-efficient Green AI architecture to support circular economies and address the contemporary challenge of sustainable resource consumption in modern systems. We introduce a multi-layered framework and meta-architecture that integrates state-of-the-art machine learning algorithms, energy-conscious computational models, and optimization techniques to facilitate decision-making for resource reuse, waste reduction, and sustainable production.We tested the framework on real-world datasets from lithium-ion battery recycling and urban waste management systems, demonstrating its practical applicability. Notably, the key findings of this study indicate a 25 percent reduction in energy consumption during workflows compared to traditional methods and an 18 percent improvement in resource recovery efficiency. Quantitative optimization was based on mathematical models such as mixed-integer linear programming and lifecycle assessments. Moreover, AI algorithms improved classification accuracy on urban waste by 20 percent, while optimized logistics reduced transportation emissions by 30 percent. We present graphical analyses and visualizations of the developed framework, illustrating its impact on energy efficiency and sustainability as reflected in the simulation results. This paper combines the principles of Green AI with practical insights into how such architectural models contribute to circular economies, presenting a fully scalable and scientifically rooted solution aligned with applicable UN Sustainability Goals worldwide. These results open avenues for incorporating newly developed AI technologies into sustainable management strategies, potentially safeguarding local natural capital while advancing technological progress.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Software Landscape for the Density Matrix Renormalization Group</title>
<link>https://arxiv.org/abs/2506.12629</link>
<guid>https://arxiv.org/abs/2506.12629</guid>
<content:encoded><![CDATA[
<div> Keywords: density matrix renormalization group, software landscape, modularization, standardization, collaboration

Summary: 
The article discusses the landscape of density matrix renormalization group (DMRG) software, highlighting 35 existing packages and their features. There is significant overlap in features among these packages, particularly in areas such as parallelism strategies and symmetry-adapted formulations. The lack of standard interfaces and modularity suggests a need for more collaboration, standardization, and modularization. The authors advocate for more cohesion and modularity in DMRG software to reduce duplication of efforts and improve interoperability. They believe that the challenges in the software landscape are more social than technical and aim to raise awareness among researchers and developers to encourage collaboration and optimization. Ultimately, greater cohesion and modularity in DMRG software would enhance its capabilities in tackling complex problems. 

<br /><br />Summary: <div>
arXiv:2506.12629v1 Announce Type: cross 
Abstract: The density matrix renormalization group (DMRG) algorithm is a cornerstone computational method for studying quantum many-body systems, renowned for its accuracy and adaptability. Despite DMRG's broad applicability across fields such as materials science, quantum chemistry, and quantum computing, numerous independent implementations have been developed. This survey maps the rapidly expanding DMRG software landscape, providing a comprehensive comparison of features among 35 existing packages. We found significant overlap in features among the packages when comparing key aspects, such as parallelism strategies for high-performance computing and symmetry-adapted formulations that enhance efficiency. This overlap suggests opportunities for modularization of common operations, including tensor operations, symmetry representations, and eigensolvers, as the packages are mostly independent and share few third-party library dependencies where functionality is factored out. More widespread modularization and standardization would result in reduced duplication of efforts and improved interoperability. We believe that the proliferation of packages and the current lack of standard interfaces and modularity are more social than technical. We aim to raise awareness of existing packages, guide researchers in finding a suitable package for their needs, and help developers identify opportunities for collaboration, modularity standardization, and optimization. Ultimately, this work emphasizes the value of greater cohesion and modularity, which would benefit DMRG software, allowing these powerful algorithms to tackle more complex and ambitious problems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimLOB: Learning Representations of Limited Order Book for Financial Market Simulation</title>
<link>https://arxiv.org/abs/2406.19396</link>
<guid>https://arxiv.org/abs/2406.19396</guid>
<content:encoded><![CDATA[
<div> Transformer-based autoencoder, Limit Order Book, financial market simulation, calibration, representation learning <br />
<br />
Summary: <br />
Financial market simulation (FMS) is essential for understanding market anomalies and trading behaviors. Traditional calibration methods focused on mid-price data, resulting in biased models. This study introduces a Transformer-based autoencoder to learn vectorized representations of Limit Order Book (LOB) data, capturing market micro-structure. The learned latent representation preserves temporal auto-correlation and precedence between price levels in LOB. Experiment results demonstrate the effectiveness of the learned representation for downstream calibration tasks. This work advances FMS on LOB data, addressing the challenge of transforming LOB's tabular structure to a vectorized format for calibration purposes. <div>
arXiv:2406.19396v4 Announce Type: replace 
Abstract: Financial market simulation (FMS) serves as a promising tool for understanding market anomalies and the underlying trading behaviors. To ensure high-fidelity simulations, it is crucial to calibrate the FMS model for generating data closely resembling the observed market data. Previous efforts primarily focused on calibrating the mid-price data, leading to essential information loss of the market activities and thus biasing the calibrated model. The Limit Order Book (LOB) data is the fundamental data fully capturing the market micro-structure and is adopted by worldwide exchanges. However, LOB is not applicable to existing calibration objective functions due to its tabular structure not suitable for the vectorized input requirement. This paper proposes to explicitly learn the vectorized representations of LOB with a Transformer-based autoencoder. Then the latent vector, which captures the major information of LOB, can be applied for calibration. Extensive experiments show that the learned latent representation not only preserves the non-linear auto-correlation in the temporal axis, but the precedence between successive price levels of LOB. Besides, it is verified that the performance of the representation learning stage is consistent with the downstream calibration tasks. Thus, this work also progresses the FMS on LOB data, for the first time.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Interpretable Climate Emulators for Economics</title>
<link>https://arxiv.org/abs/2411.10768</link>
<guid>https://arxiv.org/abs/2411.10768</guid>
<content:encoded><![CDATA[
<div> framework, climate emulators, economic models, carbon-cycle, land-use change<br />
<br />
Summary: 
This paper introduces a framework for efficient and interpretable climate emulators for economic models of climate change. It proposes a generalized linear multi-reservoir model for constructing carbon-cycle emulators, customizable for specific applications. The paper evaluates three versions of the model within a representative agent economic model, finding that incorporating land-use change impacts significantly alters atmospheric carbon stocks, temperature trajectories, and optimal mitigation paths. Additionally, the study investigates pattern-scaling techniques to transform global-mean temperature projections into spatially heterogeneous warming fields and examines how regional baseline climates, non-uniform warming, and associated uncertainties influence economic damages. The findings highlight the importance of considering land-use change effects and spatial variations in climate projections for more accurate economic assessments of climate change impacts. <br /><br />Summary: <div>
arXiv:2411.10768v2 Announce Type: replace-cross 
Abstract: We introduce a framework for developing efficient and interpretable climate emulators (CEs) for economic models of climate change. The paper makes two main contributions. First, we propose a general framework for constructing carbon-cycle emulators (CCEs) for macroeconomic models. The framework is implemented as a generalized linear multi-reservoir (box) model that conserves key physical quantities and can be customized for specific applications. We consider three versions of the CCE, which we evaluate within a simple representative agent economic model: (i) a three-box setting comparable to DICE-2016, (ii) a four-box extension, and (iii) a four-box version that explicitly captures land-use change. While the three-box model reproduces benchmark results well and the fourth reservoir adds little, incorporating the impact of land-use change on the carbon storage capacity of the terrestrial biosphere substantially alters atmospheric carbon stocks, temperature trajectories, and the optimal mitigation path. Second, we investigate pattern-scaling techniques that transform global-mean temperature projections from CEs into spatially heterogeneous warming fields. We show how regional baseline climates, non-uniform warming, and the associated uncertainties propagate into economic damages.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOB-Bench: Benchmarking Generative AI for Finance -- an Application to Limit Order Book Data</title>
<link>https://arxiv.org/abs/2502.09172</link>
<guid>https://arxiv.org/abs/2502.09172</guid>
<content:encoded><![CDATA[
<div> benchmark, financial data, generative models, limit order books, evaluation

Summary:
The article introduces LOB-Bench, a benchmarking tool developed in Python, to assess the quality of generative data for limit order books (LOB) in the finance sector. The tool aims to address the challenges posed by noisy and complex financial data by providing a quantitative evaluation framework. LOB-Bench compares generated and real LOB data using various statistical measures, including conditional and unconditional statistics, order book volumes, and order imbalance. The framework also incorporates scores from a discriminator network and market impact metrics, such as cross-correlations and price response functions. The study evaluates different generative models, including generative autoregressive state-space models and (C)GANs, and concludes that the autoregressive GenAI approach outperforms traditional model classes. This research contributes to advancing the field of financial sequence modeling by offering a standardized assessment tool for evaluating generative models in the context of limit order books. 

<br /><br />Summary: <div>
arXiv:2502.09172v2 Announce Type: replace-cross 
Abstract: While financial data presents one of the most challenging and interesting sequence modelling tasks due to high noise, heavy tails, and strategic interactions, progress in this area has been hindered by the lack of consensus on quantitative evaluation paradigms. To address this, we present LOB-Bench, a benchmark, implemented in python, designed to evaluate the quality and realism of generative message-by-order data for limit order books (LOB) in the LOBSTER format. Our framework measures distributional differences in conditional and unconditional statistics between generated and real LOB data, supporting flexible multivariate statistical evaluation. The benchmark also includes features commonly used LOB statistics such as spread, order book volumes, order imbalance, and message inter-arrival times, along with scores from a trained discriminator network. Lastly, LOB-Bench contains "market impact metrics", i.e. the cross-correlations and price response functions for specific events in the data. We benchmark generative autoregressive state-space models, a (C)GAN, as well as a parametric LOB model and find that the autoregressive GenAI approach beats traditional model classes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of physics-informed neural networks modeling time-harmonic wave fields</title>
<link>https://arxiv.org/abs/2506.11395</link>
<guid>https://arxiv.org/abs/2506.11395</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, Partial Differential Equations, Acoustic Wave Field, Room Acoustics <br />
Summary: <br />
- The study focuses on using physics-informed neural networks (PINNs) to model partial differential equations for solving the acoustic wave field.
- Initial results show promise for simple geometries in two-dimensional domains but face challenges in achieving convergence towards the accurate solution.
- Various factors including 3D dimensionality, realistic source modeling, Neumann boundary conditions, and complex solution quantities play a role in the optimization process.
- The research examines 3D room acoustic scenarios at low frequencies, testing different source definitions, boundary condition sets, and incorporating a complex speed of sound model.
- Convergence studies indicate the need for at least six training points per wavelength to achieve accurate training and predictions with the PINN architecture. This work contributes to the larger goal of modeling low-frequency room acoustics, including absorbers. <br /> <div>
arXiv:2506.11395v1 Announce Type: new 
Abstract: Studying physics-informed neural networks (PINNs) for modeling partial differential equations to solve the acoustic wave field has produced promising results for simple geometries in two-dimensional domains. One option is to compute the time-harmonic wave field using the Helmholtz equation. Compared to existing numerical models, the physics-informed neural networks forward problem has to overcome several topics related to the convergence of the optimization toward the "true" solution. The topics reach from considering the physical dimensionality (from 2D to 3D), the modeling of realistic sources (from a self-similar source to a realistic confined point source), the modeling of sound-hard (Neumann) boundary conditions, and the modeling of the full wave field by considering the complex solution quantities. Within this contribution, we study 3D room acoustic cases at low frequency, varying the source definition and the number of boundary condition sets and using a complex speed of sound model to account for some degree of absorption. We assess the convergence behavior by looking at the loss landscape of the PINN architecture, the $L^2$ error compared to a finite element reference simulation for each network architecture and configuration. The convergence studies showed that at least six training points per wavelength are necessary for accurate training and subsequent predictions of the PINN. The developments are part of an initiative aiming to model the low-frequency behavior of room acoustics, including absorbers.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAN-MI: A Scalable and Efficient Pipeline for Constructing High-Quality Neurodata in Motor Imagery Paradigm</title>
<link>https://arxiv.org/abs/2506.11830</link>
<guid>https://arxiv.org/abs/2506.11830</guid>
<content:encoded><![CDATA[
<div> Keyword: EEG signals, motor imagery, brain-computer interfaces, data construction, neurodata<br />
Summary:<br />
The article introduces CLEAN-MI, a data construction pipeline for motor imagery-based brain-computer interfaces. It addresses challenges such as low signal-to-noise ratio and inter-subject variability in EEG signals. CLEAN-MI combines frequency band filtering, channel template selection, subject screening, and marginal distribution alignment to enhance data quality and standardize multi-source EEG datasets. The pipeline was tested on various public MI datasets, showing consistent improvements in data quality and classification performance. This systematic approach aims to develop large-scale, efficient, and accurate neurodata for building robust and generalizable foundation models in BCI systems. <div>
arXiv:2506.11830v1 Announce Type: new 
Abstract: The construction of large-scale, high-quality datasets is a fundamental prerequisite for developing robust and generalizable foundation models in motor imagery (MI)-based brain-computer interfaces (BCIs). However, EEG signals collected from different subjects and devices are often plagued by low signal-to-noise ratio, heterogeneity in electrode configurations, and substantial inter-subject variability, posing significant challenges for effective model training. In this paper, we propose CLEAN-MI, a scalable and systematic data construction pipeline for constructing large-scale, efficient, and accurate neurodata in the MI paradigm. CLEAN-MI integrates frequency band filtering, channel template selection, subject screening, and marginal distribution alignment to systematically filter out irrelevant or low-quality data and standardize multi-source EEG datasets. We demonstrate the effectiveness of CLEAN-MI on multiple public MI datasets, achieving consistent improvements in data quality and classification performance.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances</title>
<link>https://arxiv.org/abs/2506.10990</link>
<guid>https://arxiv.org/abs/2506.10990</guid>
<content:encoded><![CDATA[
<div> carbon footprint, Follow-the-Sun, Artificial Intelligence, anomaly detection, experiment 

Summary:
- The 'Follow-the-Sun' (FtS) model aims to minimize the carbon footprint of computer workloads by dynamically relocating them to regions with cleaner energy sources.
- A study was conducted to assess the effectiveness of FtS in reducing the carbon emissions of AI training models, comparing it with two other strategies.
- Results from benchmarking four AI algorithms showed that the FtS strategy led to an average reduction of up to 14.6% in carbon emissions, with peaks of 16.3%.
- Additionally, FtS was found to help preserve the training time needed for AI models.
- Utilizing historical carbon intensity data from seven European cities in 2021, the experiment provided scientific evidence supporting the advantages of the FtS strategy in mitigating the carbon footprint of AI workloads.<br /><br />Summary: <div>
arXiv:2506.10990v1 Announce Type: cross 
Abstract: 'Follow-the-Sun' (FtS) is a theoretical computational model aimed at minimizing the carbon footprint of computer workloads. It involves dynamically moving workloads to regions with cleaner energy sources as demand increases and energy production relies more on fossil fuels. With the significant power consumption of Artificial Intelligence (AI) being a subject of extensive debate, FtS is proposed as a strategy to mitigate the carbon footprint of training AI models. However, the literature lacks scientific evidence on the advantages of FtS to mitigate the carbon footprint of AI workloads. In this paper, we present the results of an experiment conducted in a partial synthetic scenario to address this research gap. We benchmarked four AI algorithms in the anomaly detection domain and measured the differences in carbon emissions in four cases: no strategy, FtS, and two strategies previously introduced in the state of the art, namely Flexible Start and Pause and Resume. To conduct our experiment, we utilized historical carbon intensity data from the year 2021 for seven European cities. Our results demonstrate that the FtS strategy not only achieves average reductions of up to 14.6% in carbon emissions (with peaks of 16.3%) but also helps in preserving the time needed for training.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Attention-based Spatio-Temporal Neural Operator for Evolving Physics</title>
<link>https://arxiv.org/abs/2506.11328</link>
<guid>https://arxiv.org/abs/2506.11328</guid>
<content:encoded><![CDATA[
<div> machine learning, SciML, spatio-temporal, interpretability, ASNO <br />
Summary:<br /> 
The article introduces the Attention-based Spatio-Temporal Neural Operator (ASNO) to address challenges in scientific machine learning (SciML). ASNO combines separate attention mechanisms for spatial and temporal interactions to adapt to unknown physical parameters. Inspired by the backward differentiation formula, ASNO learns a transformer for temporal prediction and extrapolation and an attention-based neural operator for handling varying external loads. This enhances interpretability by isolating historical state contributions and external forces, enabling the discovery of underlying physical laws and generalizability to unseen environments. Empirical results on SciML benchmarks show that ASNO outperforms existing models, demonstrating its potential for engineering applications, physics discovery, and interpretable machine learning. <br /> <div>
arXiv:2506.11328v1 Announce Type: cross 
Abstract: In scientific machine learning (SciML), a key challenge is learning unknown, evolving physical processes and making predictions across spatio-temporal scales. For example, in real-world manufacturing problems like additive manufacturing, users adjust known machine settings while unknown environmental parameters simultaneously fluctuate. To make reliable predictions, it is desired for a model to not only capture long-range spatio-temporal interactions from data but also adapt to new and unknown environments; traditional machine learning models excel at the first task but often lack physical interpretability and struggle to generalize under varying environmental conditions. To tackle these challenges, we propose the Attention-based Spatio-Temporal Neural Operator (ASNO), a novel architecture that combines separable attention mechanisms for spatial and temporal interactions and adapts to unseen physical parameters. Inspired by the backward differentiation formula (BDF), ASNO learns a transformer for temporal prediction and extrapolation and an attention-based neural operator for handling varying external loads, enhancing interpretability by isolating historical state contributions and external forces, enabling the discovery of underlying physical laws and generalizability to unseen physical environments. Empirical results on SciML benchmarks demonstrate that ASNO outperforms over existing models, establishing its potential for engineering applications, physics discovery, and interpretable machine learning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design</title>
<link>https://arxiv.org/abs/2506.11420</link>
<guid>https://arxiv.org/abs/2506.11420</guid>
<content:encoded><![CDATA[
<div> protein-binding proteins, affinity, biomedical research, biotechnology, PPDiff<br />
<br />
Summary: 
Designing high-affinity protein-binding proteins is crucial in biomedical research and biotechnology. The PPDiff model, based on SSINC, is introduced to design binders for any protein target in a non-autoregressive manner. PPDiff integrates self-attention layers, graph layers, and causal attention layers to capture global amino acid correlations, local interactions, and simplify interdependencies within the protein sequence. PPDiff is evaluated on the PPBench dataset and achieves success rates of 50.00% for pretraining and 23.16% for target-protein mini-binder complex design. For antigen-antibody complex design, PPDiff achieves a success rate of 16.89%, outperforming baseline methods. This demonstrates the effectiveness of PPDiff in designing high-affinity binders for arbitrary protein targets without extensive wet-lab testing. <div>
arXiv:2506.11420v1 Announce Type: cross 
Abstract: Designing protein-binding proteins with high affinity is critical in biomedical research and biotechnology. Despite recent advancements targeting specific proteins, the ability to create high-affinity binders for arbitrary protein targets on demand, without extensive rounds of wet-lab testing, remains a significant challenge. Here, we introduce PPDiff, a diffusion model to jointly design the sequence and structure of binders for arbitrary protein targets in a non-autoregressive manner. PPDiffbuilds upon our developed Sequence Structure Interleaving Network with Causal attention layers (SSINC), which integrates interleaved self-attention layers to capture global amino acid correlations, k-nearest neighbor (kNN) equivariant graph layers to model local interactions in three-dimensional (3D) space, and causal attention layers to simplify the intricate interdependencies within the protein sequence. To assess PPDiff, we curate PPBench, a general protein-protein complex dataset comprising 706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on PPBenchand finetuned on two real-world applications: target-protein mini-binder complex design and antigen-antibody complex design. PPDiffconsistently surpasses baseline methods, achieving success rates of 50.00%, 23.16%, and 16.89% for the pretraining task and the two downstream applications, respectively.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the performance of multi-fidelity and reduced-dimensional neural emulators for inference of physiologic boundary conditions</title>
<link>https://arxiv.org/abs/2506.11683</link>
<guid>https://arxiv.org/abs/2506.11683</guid>
<content:encoded><![CDATA[
<div> Bayesian parameter estimation, cardiovascular modeling, surrogate modeling, high-fidelity simulations, computational cost <br />
Summary: 
This work focuses on solving inverse problems in cardiovascular modeling through Bayesian parameter estimation. It explores various methods to reduce computational costs by using low-fidelity approximations. Different approaches include constructing surrogates for high-fidelity simulations, building surrogates for the discrepancy between high and low-fidelity models, and treating the discrepancy as random noise to estimate its distribution. Five variations of these methods are validated on analytical test cases, comparing them to high-fidelity posterior distributions in terms of accuracy and computational cost. The approaches are then demonstrated on two cardiovascular examples: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy. The study showcases the effectiveness of leveraging low-fidelity approximations in reducing computational costs while maintaining accuracy in Bayesian parameter estimation for cardiovascular modeling. <br /> <div>
arXiv:2506.11683v1 Announce Type: cross 
Abstract: Solving inverse problems in cardiovascular modeling is particularly challenging due to the high computational cost of running high-fidelity simulations. In this work, we focus on Bayesian parameter estimation and explore different methods to reduce the computational cost of sampling from the posterior distribution by leveraging low-fidelity approximations. A common approach is to construct a surrogate model for the high-fidelity simulation itself. Another is to build a surrogate for the discrepancy between high- and low-fidelity models. This discrepancy, which is often easier to approximate, is modeled with either a fully connected neural network or a nonlinear dimensionality reduction technique that enables surrogate construction in a lower-dimensional space. A third possible approach is to treat the discrepancy between the high-fidelity and surrogate models as random noise and estimate its distribution using normalizing flows. This allows us to incorporate the approximation error into the Bayesian inverse problem by modifying the likelihood function. We validate five different methods which are variations of the above on analytical test cases by comparing them to posterior distributions derived solely from high-fidelity models, assessing both accuracy and computational cost. Finally, we demonstrate our approaches on two cardiovascular examples of increasing complexity: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Level set-based inverse homogenisation of three-dimensional piezoelectric materials</title>
<link>https://arxiv.org/abs/2410.03148</link>
<guid>https://arxiv.org/abs/2410.03148</guid>
<content:encoded><![CDATA[
<div> piezoelectric materials, topology optimisation, iterative solvers, metamaterials, level-set implementation <br />
Summary: 
This paper utilizes memory-distributed level set-based topology optimization to enhance the properties of three-dimensional periodic piezoelectric materials. Various iterative solvers are evaluated for their weak scalability, with the approximate Schur complement preconditioned generalized minimal residual method showing the best performance for solving piezoelectric homogenization equations. Through computational design, high-resolution piezoelectric metamaterials with improved stiffness and piezoelectric properties are created for sensor, hydrophone, and actuator applications. Two robust structures without fine-scale features are proposed, exhibiting significantly enhanced piezoelectric properties compared to the base material. The level-set approach proves effective in piezoelectricity problems by eliminating large regions of intermediate density material. An open-source memory-distributed level-set implementation is provided for the community practitioners to utilize. <br /><br />Summary: <div>
arXiv:2410.03148v3 Announce Type: replace 
Abstract: In this paper we use memory-distributed level set-based topology optimisation to design three-dimensional periodic piezoelectric materials with enhanced properties. We compare and assess several existing iterative solvers with respect to their weak scalability and find that an approximate Schur complement preconditioned generalized minimal residual method method demonstrates the best performance and scalability for solving the piezoelectric homogenisation equations. We use the developed techniques to computationally design high-resolution piezoelectric metamaterials with enhanced stiffness and piezoelectric properties that yield new insights into material design for sensor, hydrophone, and actuator applications. We suggest two robust structures with no fine-scale features that exhibit enhanced piezoelectric properties several times larger than those of the base material. We find that level set-based topology optimisation is well suited to problems involving piezoelectricity and has the advantage of avoiding large regions of intermediate density material. Our memory-distributed level-set implementation is open source and provided for practitioners in the community.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form</title>
<link>https://arxiv.org/abs/2412.15801</link>
<guid>https://arxiv.org/abs/2412.15801</guid>
<content:encoded><![CDATA[
<div> metrics, urban form, performance evaluation, morphology, sustainable urban design
Summary:
- The article discusses the importance of connecting morphology metrics with complex urban forms to enhance performance-driven computational urban design.
- It highlights the need for bi-directional mapping between metrics and urban forms to improve urban performance.
- An approach is presented to formulate metrics that can characterize urban forms and retrieve similar 3D forms.
- The methodology is demonstrated using 3D urban models of New York City, analyzing 14,248 blocks.
- Neural networks and information retrieval techniques are utilized for morphology metric encoding, urban form clustering, and evaluation.
<br /><br />Summary: <div>
arXiv:2412.15801v2 Announce Type: replace 
Abstract: Urban morphology, examining city spatial configurations, links urban design to sustainability. Morphology metrics play a fundamental role in performance-driven computational urban design (CUD) which integrates urban form generation, performance evaluation and optimization. However, a critical gap remains between performance evaluation and complex urban form generation, caused by the disconnection between morphology metrics and urban form, particularly in metric-to-form workflows. It prevents the application of optimized metrics to generate improved urban form with enhanced urban performance. Formulating morphology metrics that not only effectively characterize complex urban forms but also enable the reconstruction of diverse forms is of significant importance. This paper highlights the importance of establishing a bi-directional mapping between morphology metrics and complex urban form to enable the integration of urban form generation with performance evaluation. We present an approach that can 1) formulate morphology metrics to both characterize urban forms and in reverse, retrieve diverse similar 3D urban forms, and 2) evaluate the effectiveness of morphology metrics in representing 3D urban form characteristics of blocks by comparison. We demonstrate the methodology with 3D urban models of New York City, covering 14,248 blocks. We use neural networks and information retrieval for morphology metric encoding, urban form clustering and morphology metric evaluation. We identified an effective set of morphology metrics for characterizing block-scale urban forms through comparison. The proposed methodology tightly couples complex urban forms with morphology metrics, hence it can enable a seamless and bidirectional relationship between urban form generation and optimization in performance-driven urban design towards sustainable urban design and planning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring EEG Responses during Observation of Actions Performed by Human Actor and Humanoid Robot</title>
<link>https://arxiv.org/abs/2506.10170</link>
<guid>https://arxiv.org/abs/2506.10170</guid>
<content:encoded><![CDATA[
<div> action observation therapy humanoid robots EEG sensorimotor brain activity<br />
<br />
Summary: <br />
This pilot study investigated the potential of humanoid robots in supporting action observation therapy for motor and language function in neurological rehabilitation. Three healthy participants were monitored using EEG while observing actions performed by a human actor and a robot. Analysis of their brain activity revealed variability in ERSP patterns, including power suppression in sensorimotor mu and beta rhythms. One participant showed a stronger response to robot conditions compared to human conditions. Positive correlations in ERSP across all conditions implied common cognitive processes or neural networks in the mirror neuron system during action observation. Overall, the results support the feasibility of using EEG to explore differences in neural responses to observing robot- and human-induced actions. <div>
arXiv:2506.10170v1 Announce Type: new 
Abstract: Action observation (AO) therapy is a promising rehabilitative treatment for motor and language function in individuals recovering from neurological conditions, such as stroke. This pilot study aimed to investigate the potential of humanoid robots to support AO therapy in rehabilitation settings. The brain activity of three healthy right-handed participants was monitored with electroencephalography (EEG) while they observed eight different actions performed by two agents, a human actor and a robot, using their left and right arms. Their event-related spectral perturbations (ERSPs, changes in the spectral power of neural oscillations in response to an event or stimulus, compared to baseline) in sensorimotor regions were analyzed. The single-subject analysis showed variability in ERSP patterns among all participants, including power suppression in sensorimotor mu and beta rhythms. One participant showed stronger responses to "robot" AO conditions than to "human" conditions. Strong and positive correlations in ERSP across all conditions were observed for almost all participants and channels, implying common cognitive processes or neural networks at play in the mirror neuron system during AO. The results support the feasibility of using EEG to explore differences in neural responses to observation of robot- and human-induced actions.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable and flexible non-intrusive reduced-order models using reproducing kernel Hilbert spaces</title>
<link>https://arxiv.org/abs/2506.10224</link>
<guid>https://arxiv.org/abs/2506.10224</guid>
<content:encoded><![CDATA[
<div> Regularized kernel interpolation, reduced-order modeling, interpretable models, reproducing kernel Hilbert space, a posteriori error bound <br />
<br />
Summary: <br />
This paper introduces a novel non-intrusive reduced-order modeling technique using regularized kernel interpolation. Unlike traditional methods that rely on least-squares regression, this approach utilizes a reproducing kernel Hilbert space to capture the dynamics of a reduced-order model (ROM) in an interpretable manner. By incorporating carefully selected feature maps into the kernel, the resulting ROMs closely mimic the structure of the full-order model. The flexibility of the approach allows for the inclusion of both structured features and general nonlinear terms in the kernel, providing a comprehensive representation of the system dynamics. Additionally, the authors derive a computable a posteriori error bound that combines error estimates from traditional projection-based ROMs and kernel interpolants. Numerical experiments showcase the effectiveness of the approach compared to operator inference techniques using proper orthogonal decomposition and quadratic manifold dimension reduction. <div>
arXiv:2506.10224v1 Announce Type: new 
Abstract: This paper develops an interpretable, non-intrusive reduced-order modeling technique using regularized kernel interpolation. Existing non-intrusive approaches approximate the dynamics of a reduced-order model (ROM) by solving a data-driven least-squares regression problem for low-dimensional matrix operators. Our approach instead leverages regularized kernel interpolation, which yields an optimal approximation of the ROM dynamics from a user-defined reproducing kernel Hilbert space. We show that our kernel-based approach can produce interpretable ROMs whose structure mirrors full-order model structure by embedding judiciously chosen feature maps into the kernel. The approach is flexible and allows a combination of informed structure through feature maps and closure terms via more general nonlinear terms in the kernel. We also derive a computable a posteriori error bound that combines standard error estimates for intrusive projection-based ROMs and kernel interpolants. The approach is demonstrated in several numerical experiments that include comparisons to operator inference using both proper orthogonal decomposition and quadratic manifold dimension reduction.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment</title>
<link>https://arxiv.org/abs/2506.10711</link>
<guid>https://arxiv.org/abs/2506.10711</guid>
<content:encoded><![CDATA[
<div> refiner models, diffusion models, PDEs, spectral space, Kuramoto-Sivashinsky equation <br />
Summary: 
The article discusses the challenge of generating accurate and stable long rollouts for time-dependent Partial Differential Equations (PDEs). A refiner model called PDERefiner has been developed, utilizing diffusion models to refine outputs for each time step, with a focus on high-frequency accuracy. While this approach works well for some PDEs like the 1-D Kuramoto-Sivashinsky equation by degrading the amplitude of the high-frequency part, it may not be as effective for more complex cases like the Navier-Stokes equation. To address this, adjustments were made in the spectral space, leading to the development of PDE-SpectralRefiner with a v-prediction technique for Blurring diffusion models. This enhancement improved the accuracy of the outputs for both one-step Mean Squared Error (MSE) loss and rollout loss across different model backbones, such as U-Net and neural operators. <div>
arXiv:2506.10711v1 Announce Type: new 
Abstract: Generating accurate and stable long rollouts is a notorious challenge for time-dependent PDEs (Partial Differential Equations). Recently, motivated by the importance of high-frequency accuracy, a refiner model called PDERefiner utilizes diffusion models to refine outputs for every time step, since the denoising process could increase the correctness of modeling high frequency part. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the amplitude of high frequency part better than not doing refinement process. However, for some other cases, the spectrum might be more complicated. For example, for a harder PDE like Navior-Stokes equation, diffusion models could over-degrade the higher frequency part. This motivates us to release the constraint that each frequency weighs the same. We enhance our refiner model with doing adjustments on spectral space, which recovers Blurring diffusion models. We developed a new v-prediction technique for Blurring diffusion models, recovering the MSE training objective on the first refinement step. We show that in this case, for different model backbones, such as U-Net and neural operators, the outputs of PDE-SpectralRefiner are more accurate for both one-step MSE loss and rollout loss.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Analysis of Discretized Boundary Integral Operators in 3D: a High-Frequency Perspective</title>
<link>https://arxiv.org/abs/2506.10880</link>
<guid>https://arxiv.org/abs/2506.10880</guid>
<content:encoded><![CDATA[
<div> wavelength, integral equations, boundary element method, scattering phenomena, discretization<br />
Summary:<br />
The study explores the common practice of using integral equations discretized by the boundary element method to model propagation and scattering phenomena. It focuses on approximating the scatterer's boundary with mesh elements of size around a fraction of the incident wave's wavelength. By analyzing operator matrix spectra, the research reveals a discrepancy compared to continuous operators that increases as the simulation frequency rises. This challenges the conventional assumption that discretizing boundaries at a fraction of the wavelength, like /10, ensures constant solution accuracy at higher frequencies. <div>
arXiv:2506.10880v1 Announce Type: new 
Abstract: When modeling propagation and scattering phenomena using integral equations discretized by the boundary element method, it is common practice to approximate the boundary of the scatterer with a mesh comprising elements of size approximately equal to a fraction of the wavelength $\lambda$ of the incident wave, e.g., $\lambda/10$. In this work, by analyzing the spectra of the operator matrices, we show a discrepancy with respect to the continuous operators which grows with the simulation frequency, challenging the common belief that the aforementioned widely used discretization approach is sufficient to maintain the accuracy of the solution constant when increasing the frequency.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Categorization with Relational Deep Learning in QuickBooks</title>
<link>https://arxiv.org/abs/2506.09234</link>
<guid>https://arxiv.org/abs/2506.09234</guid>
<content:encoded><![CDATA[
<div> Graph-based model, Rel-Cat, transaction categorization, relational database, link prediction <br />
<br />
Summary: 
The paper introduces a novel graph-based model, Rel-Cat, for automatic transaction categorization in QuickBooks. This model addresses challenges such as unique transaction descriptions and a wide variety of categories by formulating categorization as a link prediction task within a graph structure. By integrating natural language processing and graph machine learning techniques, Rel-Cat outperforms the existing QuickBooks model and scales effectively to a growing customer base. The model is designed to handle the cold start problem by adapting to minimal data, providing accurate categorization without compromising on accuracy. This innovative approach simplifies the architecture and enhances the customer experience by providing more accurate accounting and bookkeeping in QuickBooks. <div>
arXiv:2506.09234v1 Announce Type: new 
Abstract: Automatic transaction categorization is crucial for enhancing the customer experience in QuickBooks by providing accurate accounting and bookkeeping. The distinct challenges in this domain stem from the unique formatting of transaction descriptions, the wide variety of transaction categories, and the vast scale of the data involved. Furthermore, organizing transaction data in a relational database creates difficulties in developing a unified model that covers the entire database. In this work, we develop a novel graph-based model, named Rel-Cat, which is built directly over the relational database. We introduce a new formulation of transaction categorization as a link prediction task within this graph structure. By integrating techniques from natural language processing and graph machine learning, our model not only outperforms the existing production model in QuickBooks but also scales effectively to a growing customer base with a simpler, more effective architecture without compromising on accuracy. This design also helps tackle a key challenge of the cold start problem by adapting to minimal data.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Design Structure Matrix Optimization</title>
<link>https://arxiv.org/abs/2506.09749</link>
<guid>https://arxiv.org/abs/2506.09749</guid>
<content:encoded><![CDATA[
<div> machine learning, optimization, engineering design, design structure matrix, Large Language Models 

Summary:
Large Language Models (LLMs) show promise in solving combinatorial optimization (CO) problems in complex engineering systems by leveraging advanced reasoning and contextual understanding. This study proposes an LLM-based framework that combines network topology with domain knowledge to optimize Design Structure Matrix (DSM) element sequencing. Experiments demonstrate that this method achieves faster convergence and better solutions compared to traditional optimization methods. Incorporating contextual domain knowledge enhances optimization performance regardless of the specific LLM backbone used. The findings suggest that LLMs can effectively solve complex CO problems in engineering design by combining semantic and mathematical reasoning, paving the way for a new paradigm in LLM-based engineering design optimization. 

<br /><br />Summary: <div>
arXiv:2506.09749v1 Announce Type: new 
Abstract: In complex engineering systems, the interdependencies among components or development activities are often modeled and analyzed using Design Structure Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and enhance modularity or process efficiency constitutes a challenging combinatorial optimization (CO) problem in engineering design and operations. As problem sizes increase and dependency networks become more intricate, traditional optimization methods that solely use mathematical heuristics often fail to capture the contextual nuances and struggle to deliver effective solutions. In this study, we explore the potential of Large Language Models (LLMs) for helping solve such CO problems by leveraging their capabilities for advanced reasoning and contextual understanding. We propose a novel LLM-based framework that integrates network topology with contextual domain knowledge for iterative optimization of DSM element sequencing - a common CO problem. Experiments on various DSM cases show that our method consistently achieves faster convergence and superior solution quality compared to both stochastic and deterministic baselines. Notably, we find that incorporating contextual domain knowledge significantly enhances optimization performance regardless of the chosen LLM backbone. These findings highlight the potential of LLMs to solve complex engineering CO problems by combining semantic and mathematical reasoning. This approach paves the way towards a new paradigm in LLM-based engineering design optimization.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era</title>
<link>https://arxiv.org/abs/2506.09755</link>
<guid>https://arxiv.org/abs/2506.09755</guid>
<content:encoded><![CDATA[
<div> Keywords: Intelligent Design, Engineering innovation, Foundation Models, Multi-agent collaboration, Autonomous systems

Summary:<br />
The paper introduces Intelligent Design 4.0 (ID 4.0) as a new paradigm in engineering design, utilizing agentic AI systems. It discusses the historical evolution of Intelligent Design through different stages, leading to the emergence of multi-agent collaboration. ID 4.0 aims to automate engineering design processes through coordinated, autonomous multi-agent-based systems. The potential of ID 4.0 lies in supporting end-to-end automation, enhancing adaptivity, autonomy, and effectiveness in tackling complex design challenges. Future perspectives include addressing more complex design scenarios, practical design implementations, novel agent coordination mechanisms, and autonomous design goal-setting with improved human value alignment. The insights presented in the paper lay the groundwork for advancing Intelligent Design towards greater efficiency and innovation in engineering design.<br /><br />Summary: <div>
arXiv:2506.09755v1 Announce Type: new 
Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced engineering innovation, efficiency, quality, and productivity over recent decades, fundamentally reshaping how engineering designers think, behave, and interact with design processes. The recent emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), has demonstrated general knowledge-based reasoning capabilities, and open new paths and avenues for further transformation in engineering design. In this context, this paper introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by agentic AI systems. We review the historical evolution of ID across four distinct stages: rule-based expert systems, task-specific machine learning models, large-scale foundation AI models, and the recent emerging paradigm of multi-agent collaboration. We propose a conceptual framework for ID 4.0 and discuss its potential to support end-to-end automation of engineering design processes through coordinated, autonomous multi-agent-based systems. Furthermore, we discuss future perspectives to enhance and fully realize ID 4.0's potential, including more complex design scenarios, more practical design implementations, novel agent coordination mechanisms, and autonomous design goal-setting with better human value alignment. In sum, these insights lay a foundation for advancing Intelligent Design toward greater adaptivity, autonomy, and effectiveness in addressing increasingly complex design challenges.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superstudent intelligence in thermodynamics</title>
<link>https://arxiv.org/abs/2506.09822</link>
<guid>https://arxiv.org/abs/2506.09822</guid>
<content:encoded><![CDATA[
<div> OpenAI, o3, thermodynamics, exam, artificial intelligence <br />
<br />
Summary: OpenAI's language model o3 has surpassed university students in a thermodynamics exam, showcasing its ability to creatively apply principles. The exam is known for high failure rates and rare A-grades, requiring deep knowledge of thermodynamics. In a zero-shot mode, o3 outperformed students, achieving top scores in comparison to thousands of exams since 1985. This achievement signifies a shift in machine capabilities in complex tasks previously seen as exclusive to human intellect. The implications of this feat raise questions about the role of machines in engineering tasks and the future education of engineers. <div>
arXiv:2506.09822v1 Announce Type: new 
Abstract: In this short note, we report and analyze a striking event: OpenAI's large language model o3 has outwitted all students in a university exam on thermodynamics. The thermodynamics exam is a difficult hurdle for most students, where they must show that they have mastered the fundamentals of this important topic. Consequently, the failure rates are very high, A-grades are rare - and they are considered proof of the students' exceptional intellectual abilities. This is because pattern learning does not help in the exam. The problems can only be solved by knowledgeably and creatively combining principles of thermodynamics. We have given our latest thermodynamics exam not only to the students but also to OpenAI's most powerful reasoning model, o3, and have assessed the answers of o3 exactly the same way as those of the students. In zero-shot mode, the model o3 solved all problems correctly, better than all students who took the exam; its overall score was in the range of the best scores we have seen in more than 10,000 similar exams since 1985. This is a turning point: machines now excel in complex tasks, usually taken as proof of human intellectual capabilities. We discuss the consequences this has for the work of engineers and the education of future engineers.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Guided Ligand-Binding Protein Design</title>
<link>https://arxiv.org/abs/2506.09332</link>
<guid>https://arxiv.org/abs/2506.09332</guid>
<content:encoded><![CDATA[
<div> Keywords: AI protein models, protein design, ligand binding, natural language instructions, machine learning

Summary: In this paper, the authors introduce InstructPro, a family of AI protein generative models trained to design proteins that bind to specific ligands using human language instructions. The models, InstructPro-1B and InstructPro-3B, outperform existing baselines in generating ligand-binding proteins. InstructPro-1B achieves an impressive docking success rate of 81.52% and an average RMSD of 4.026 compared to ground truth structures. InstructPro-3B further improves the average RMSD to 2.527, demonstrating the models' capability to accurately design proteins based on textual descriptions and ligand formulas. The dataset InstructProBench, containing over 9 million triples of function descriptions, ligand formulas, and protein sequences, supports the training and evaluation of the models. These results suggest that AI protein models can effectively follow natural language instructions to design proteins with desired functions, offering a promising approach for protein engineering in various fields. 

<br /><br />Summary: <div>
arXiv:2506.09332v1 Announce Type: cross 
Abstract: Can AI protein models follow human language instructions and design proteins with desired functions (e.g. binding to a ligand)? Designing proteins that bind to a given ligand is crucial in a wide range of applications in biology and chemistry. Most prior AI models are trained on protein-ligand complex data, which is scarce due to the high cost and time requirements of laboratory experiments. In contrast, there is a substantial body of human-curated text descriptions about protein-ligand interactions and ligand formula. In this paper, we propose InstructPro, a family of protein generative models that follow natural language instructions to design ligand-binding proteins. Given a textual description of the desired function and a ligand formula in SMILES, InstructPro generates protein sequences that are functionally consistent with the specified instructions. We develop the model architecture, training strategy, and a large-scale dataset, InstructProBench, to support both training and evaluation. InstructProBench consists of 9,592,829 triples of (function description, ligand formula, protein sequence). We train two model variants: InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion parameters). Both variants consistently outperform strong baselines, including ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking success rate (81.52% at moderate confidence) and the lowest average root mean square deviation (RMSD) compared to ground truth structures (4.026{\AA}). InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating InstructPro's ability to generate ligand-binding proteins that align with the functional specifications.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Climate Emulation with Bayesian Filtering</title>
<link>https://arxiv.org/abs/2506.09891</link>
<guid>https://arxiv.org/abs/2506.09891</guid>
<content:encoded><![CDATA[
<div> climate change, machine learning, emulator, physics-informed, causal relationships

Summary: 
This study introduces a novel approach to climate modeling using machine learning techniques. Traditional climate models are computationally expensive due to their complex systems of equations. The proposed emulator leverages causal representation learning to incorporate physics-informed causal relationships, leading to more accurate climate dynamics predictions. A Bayesian filter ensures stable long-term autoregressive emulation. The emulator is shown to accurately capture climate dynamics on both synthetic and real-world climate model data. This innovative approach opens up new possibilities for quicker and more efficient simulations of climate change dynamics, allowing for improved predictions and analyses of its causes and impacts. <div>
arXiv:2506.09891v1 Announce Type: cross 
Abstract: Traditional models of climate change use complex systems of coupled equations to simulate physical processes across the Earth system. These simulations are highly computationally expensive, limiting our predictions of climate change and analyses of its causes and effects. Machine learning has the potential to quickly emulate data from climate models, but current approaches are not able to incorporate physics-informed causal relationships. Here, we develop an interpretable climate model emulator based on causal representation learning. We derive a physics-informed approach including a Bayesian filter for stable long-term autoregressive emulation. We demonstrate that our emulator learns accurate climate dynamics, and we show the importance of each one of its components on a realistic synthetic dataset and data from two widely deployed climate models.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Note on the Reliability of Goal-Oriented Error Estimates for Galerkin Finite Element Methods with Nonlinear Functionals</title>
<link>https://arxiv.org/abs/2506.09913</link>
<guid>https://arxiv.org/abs/2506.09913</guid>
<content:encoded><![CDATA[
<div> variational problem; Galerkin finite element method; discretization error; nonlinear functionals; error estimates <br />
<br />Summary: 
The study focuses on estimating discretization errors in nonlinear functionals within a variational problem solved using the Galerkin finite element method. It examines error estimates in the form of $J(u) - J(u_h) \approx \eta = L(z) - B(u_h, z)$. The research reveals instances where these error estimates are unreliable, even with an exact adjoint solution. Reliability is defined by the existence of a constant $C$ such that $|J(u) - J(u_h)| \leq C|\eta|$, and multiple examples are provided where this criterion is not met. The analysis highlights the challenges in ensuring the accuracy of error estimates in nonlinear functionals, shedding light on the complexities involved in such estimations. <div>
arXiv:2506.09913v1 Announce Type: cross 
Abstract: We consider estimating the discretization error in a nonlinear functional $J(u)$ in the setting of an abstract variational problem: find $u \in \mathcal{V}$ such that $B(u,\varphi) = L(\varphi) \; \forall \varphi \in \mathcal{V}$, as approximated by a Galerkin finite element method. Here, $\mathcal{V}$ is a Hilbert space, $B(\cdot,\cdot)$ is a bilinear form, and $L(\cdot)$ is a linear functional. We consider well-known error estimates $\eta$ of the form $J(u) - J(u_h) \approx \eta = L(z) - B(u_h, z)$, where $u_h$ denotes a finite element approximation to $u$, and $z$ denotes the solution to an auxiliary adjoint variational problem. We show that there exist nonlinear functionals for which error estimates of this form are not reliable, even in the presence of an exact adjoint solution solution $z$. An estimate $\eta$ is said to be reliable if there exists a constant $C \in \mathbb{R}_{>0}$ independent of $u_h$ such that $|J(u) - J(u_h)| \leq C|\eta|$. We present several example pairs of bilinear forms and nonlinear functionals where reliability of $\eta$ is not achieved.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks</title>
<link>https://arxiv.org/abs/2506.08563</link>
<guid>https://arxiv.org/abs/2506.08563</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Physics Informed Neural Networks, Differential Equations, Kernel Packet, RKHS norm

Summary:<br /> 
1. Differential equations play a crucial role in engineering modeling, and the Physics Informed Neural Networks (PINNs) framework has been utilized to solve complex equations efficiently.
2. The default L2 loss function in PINNs may lead to incorrect and unstable solutions for some complex equations.
3. A new framework called Kernel Packet accelerated PINNs (KP-PINNs) is introduced, which uses the reproducing kernel Hilbert space (RKHS) norm and the Kernel Packet (KP) method to enhance computation speed.
4. Theoretical analysis confirms that KP-PINNs are stable across various differential equations.
5. Numerical experiments demonstrate the effectiveness and efficiency of KP-PINNs in solving differential equations, offering a promising advancement for enhancing the stability and accuracy of PINNs-based solvers in scientific computing.<br /><br /> <div>
arXiv:2506.08563v1 Announce Type: new 
Abstract: Differential equations are involved in modeling many engineering problems. Many efforts have been devoted to solving differential equations. Due to the flexibility of neural networks, Physics Informed Neural Networks (PINNs) have recently been proposed to solve complex differential equations and have demonstrated superior performance in many applications. While the L2 loss function is usually a default choice in PINNs, it has been shown that the corresponding numerical solution is incorrect and unstable for some complex equations. In this work, we propose a new PINNs framework named Kernel Packet accelerated PINNs (KP-PINNs), which gives a new expression of the loss function using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel Packet (KP) method to accelerate the computation. Theoretical results show that KP-PINNs can be stable across various differential equations. Numerical experiments illustrate that KP-PINNs can solve differential equations effectively and efficiently. This framework provides a promising direction for improving the stability and accuracy of PINNs-based solvers in scientific computing.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid cardiac activation prediction for cardiac resynchronization therapy planning using geometric deep learning</title>
<link>https://arxiv.org/abs/2506.08987</link>
<guid>https://arxiv.org/abs/2506.08987</guid>
<content:encoded><![CDATA[
<div> deep learning, cardiac resynchronization therapy, prediction, optimization, in-silico modeling

Summary: 
- The study focuses on predicting cardiac activation time map for cardiac resynchronization therapy (CRT) using geometric deep learning models.
- Two models, GNN and GINO, were developed and trained on a synthetic dataset to predict activation time maps in real-time.
- The GINO model outperformed the GNN model, showing lower prediction errors and better robustness.
- A workflow for optimizing the pacing site in CRT using the GINO model was developed, resulting in a significant reduction in maximum activation time compared to random selection.
- An interactive web-based GUI was also developed to utilize the GINO model as a clinical decision-support tool for personalized CRT optimization. 

<br /><br />Summary: <div>
arXiv:2506.08987v1 Announce Type: new 
Abstract: Cardiac resynchronization therapy (CRT) is a common intervention for patients with dyssynchronous heart failure, yet approximately one-third of recipients fail to respond due to suboptimal lead placement. Identifying optimal pacing sites remains challenging, largely due to patient-specific anatomical variability and the limitations of current individualized planning strategies. In a step towards constructing an in-silico approach to help address this issue, we develop two geometric deep learning (DL) models, based on graph neural network (GNN) and geometry-informed neural operator (GINO), to predict cardiac activation time map in real-time for CRT planning and optimization. Both models are trained on a large synthetic dataset generated from finite-element (FE) simulations over a wide range of left ventricular (LV) geometries, pacing site configurations, and tissue conductivities. The GINO model significantly outperforms the GNN model, with lower prediction errors (1.14% vs 3.14%) and superior robustness to noise and various mesh discretization. Using the GINO model, we also develop a workflow for optimizing the pacing site in CRT from given activation time map and LV geometry. Compared to randomly selecting a pacing site, the CRT optimization workflow produces a larger reduction in maximum activation time (20% vs. 8%). In conjunction with an interactive web-based graphical user interface (GUI) available at https://dcsim.egr.msu.edu/, the GINO model shows promising potential as a clinical decision-support tool for personalized pre-procedural CRT optimization.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Proteins and Language: A Foundation Model for Protein Retrieval</title>
<link>https://arxiv.org/abs/2506.08023</link>
<guid>https://arxiv.org/abs/2506.08023</guid>
<content:encoded><![CDATA[
<div> **Keywords:** protein structures, functional interpretation, cryo-Electron Microscopy, vision-language models, contrastive learning

Summary:
This paper introduces a framework inspired by CLIP-style models to align 3D protein structures with functional annotations using contrastive learning. A dataset of 200,000 protein-caption pairs with detailed functional descriptors is created for model training. The model is evaluated on both in-domain and cross-database retrieval tasks using Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) datasets. Promising zero-shot retrieval performance is demonstrated, showcasing the potential of multimodal foundation models for enhancing our understanding of protein structure-function relationships in biology.<br /><br />Summary: <div>
arXiv:2506.08023v1 Announce Type: cross 
Abstract: This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset, facilitating the functional interpretation of protein structures derived by structural determination methods like cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of vision-language models (VLMs), we propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning. For model training, we propose a large-scale dataset of approximately 200,000 protein-caption pairs with rich functional descriptors. We evaluate our model in both in-domain and more challenging cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In both cases, our approach demonstrates promising zero-shot retrieval performance, highlighting the potential of multimodal foundation models for structure-function understanding in protein biology.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts</title>
<link>https://arxiv.org/abs/2506.08205</link>
<guid>https://arxiv.org/abs/2506.08205</guid>
<content:encoded><![CDATA[
<div> simulation, residual stress, machine learning, U-Net architecture, experimental data

Summary:
- Residual stresses can affect component performance, and accurately determining their distributions is crucial.
- The proposed Residual Stress Generator (RSG) uses machine learning to infer full-field stresses from limited measurements.
- An extensive dataset was created through process simulations with varied parameters.
- The RSG, based on U-Net architecture, showed excellent predictive accuracy and generalization in generating simulated stresses.
- The RSG successfully learned the latent structure of residual stress distribution and reduced the need for extensive experimental efforts.
- Testing on actual characterization data validated the RSG's effectiveness in predicting experimentally measured residual stresses. 

<br /><br />Summary: <div>
arXiv:2506.08205v1 Announce Type: cross 
Abstract: Residual stresses, which remain within a component after processing, can deteriorate performance. Accurately determining their full-field distributions is essential for optimizing the structural integrity and longevity. However, the experimental effort required for full-field characterization is impractical. Given these challenges, this work proposes a machine learning (ML) based Residual Stress Generator (RSG) to infer full-field stresses from limited measurements. An extensive dataset was initially constructed by performing numerous process simulations with a diverse parameter set. A ML model based on U-Net architecture was then trained to learn the underlying structure through systematic hyperparameter tuning. Then, the model's ability to generate simulated stresses was evaluated, and it was ultimately tested on actual characterization data to validate its effectiveness. The model's prediction of simulated stresses shows that it achieved excellent predictive accuracy and exhibited a significant degree of generalization, indicating that it successfully learnt the latent structure of residual stress distribution. The RSG's performance in predicting experimentally characterized data highlights the feasibility of the proposed approach in providing a comprehensive understanding of residual stress distributions from limited measurements, thereby significantly reducing experimental efforts.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems</title>
<link>https://arxiv.org/abs/2506.08475</link>
<guid>https://arxiv.org/abs/2506.08475</guid>
<content:encoded><![CDATA[
<div> autoencoders, dimensionality reduction, parametric neural networks, reduced-order modeling, thermodynamics

Summary:
The article introduces a novel framework, tLaSDI, for efficient identification of latent space dynamics in parametric nonlinear dynamical systems. By combining autoencoders for dimensionality reduction with parametric neural networks informed by thermodynamic principles, the framework can accurately learn parametric latent dynamics while maintaining important thermodynamic laws. The inclusion of a physics-informed active learning strategy further enhances model performance by adaptively sampling informative training data. Numerical experiments on various equations demonstrate the framework's superior speed and accuracy compared to traditional methods, with significant reductions in training and inference costs. The learned latent space dynamics not only provide accurate representations of physical-space dynamics but also offer valuable insights into the underlying thermodynamic behavior of the system.<br /><br />Summary: <div>
arXiv:2506.08475v1 Announce Type: cross 
Abstract: We propose an efficient thermodynamics-informed latent space dynamics identification (tLaSDI) framework for the reduced-order modeling of parametric nonlinear dynamical systems. This framework integrates autoencoders for dimensionality reduction with newly developed parametric GENERIC formalism-informed neural networks (pGFINNs), which enable efficient learning of parametric latent dynamics while preserving key thermodynamic principles such as free energy conservation and entropy generation across the parameter space. To further enhance model performance, a physics-informed active learning strategy is incorporated, leveraging a greedy, residual-based error indicator to adaptively sample informative training data, outperforming uniform sampling at equivalent computational cost. Numerical experiments on the Burgers' equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed method achieves up to 3,528x speed-up with 1-3% relative errors, and significant reduction in training (50-90%) and inference (57-61%) cost. Moreover, the learned latent space dynamics reveal the underlying thermodynamic behavior of the system, offering valuable insights into the physical-space dynamics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation</title>
<link>https://arxiv.org/abs/2506.08604</link>
<guid>https://arxiv.org/abs/2506.08604</guid>
<content:encoded><![CDATA[
<div> Keywords: generative machine learning, physics-based flow matching, PDE problems, surrogate modeling, uncertainty quantification

Summary: 
Physics-Based Flow Matching (PBFM) is a novel generative framework that incorporates physical constraints, such as PDE residuals and algebraic relations, into the flow matching objective. This method explicitly embeds physics knowledge into the learning process, resulting in improved accuracy in noise-free sample prediction. The inclusion of temporal unrolling during training further enhances prediction accuracy. PBFM minimizes both flow matching loss and physics-based residual loss simultaneously without the need for hyperparameter tuning. By analyzing the role of minimum noise level and implementing a stochastic sampling strategy, physical residuals can be reduced. Benchmarking on three PDE problems demonstrates that PBFM outperforms existing algorithms by up to 8 times in physical residual accuracy and distributional accuracy. This approach offers a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.<br /><br />Summary: <div>
arXiv:2506.08604v1 Announce Type: cross 
Abstract: Generative machine learning methods, such as diffusion models and flow matching, have shown great potential in modeling complex system behaviors and building efficient surrogate models. However, these methods typically learn the underlying physics implicitly from data. We propose Physics-Based Flow Matching (PBFM), a novel generative framework that explicitly embeds physical constraints, both PDE residuals and algebraic relations, into the flow matching objective. We also introduce temporal unrolling at training time that improves the accuracy of the final, noise-free sample prediction. Our method jointly minimizes the flow matching loss and the physics-based residual loss without requiring hyperparameter tuning of their relative weights. Additionally, we analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of physical constraints and evaluate a stochastic sampling strategy that helps to reduce physical residuals. Through extensive benchmarks on three representative PDE problems, we show that our approach yields up to an $8\times$ more accurate physical residuals compared to FM, while clearly outperforming existing algorithms in terms of distributional accuracy. PBFM thus provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements</title>
<link>https://arxiv.org/abs/2506.08762</link>
<guid>https://arxiv.org/abs/2506.08762</guid>
<content:encoded><![CDATA[
<div> Benchmark, Japanese financial data, large language model, accounting fraud detection, earnings forecasting

Summary:
The article introduces EDINET-Bench, an open-source Japanese financial benchmark aiming to evaluate the performance of large language models (LLMs) in challenging financial tasks such as accounting fraud detection and earnings forecasting using data from Japan's EDINET. The experiments show that even state-of-the-art LLMs struggle in binary classification for fraud detection and earnings forecasting, only performing slightly better than logistic regression. This indicates significant challenges in applying LLMs to real-world financial tasks and highlights the need for domain-specific adaptation. The dataset, benchmark construction code, and evaluation code are publicly available to support further research in utilizing LLMs for financial analysis.<br /><br />Summary: <div>
arXiv:2506.08762v1 Announce Type: cross 
Abstract: Financial analysis presents complex challenges that could leverage large language model (LLM) capabilities. However, the scarcity of challenging financial datasets, particularly for Japanese financial data, impedes academic innovation in financial analytics. As LLMs advance, this lack of accessible research resources increasingly hinders their development and evaluation in this specialized domain. To address this gap, we introduce EDINET-Bench, an open-source Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction. EDINET-Bench is constructed by downloading annual reports from the past 10 years from Japan's Electronic Disclosure for Investors' NETwork (EDINET) and automatically assigning labels corresponding to each evaluation task. Our experiments reveal that even state-of-the-art LLMs struggle, performing only slightly better than logistic regression in binary classification for fraud detection and earnings forecasting. These results highlight significant challenges in applying LLMs to real-world financial applications and underscore the need for domain-specific adaptation. Our dataset, benchmark construction code, and evaluation code is publicly available to facilitate future research in finance with LLMs.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)</title>
<link>https://arxiv.org/abs/2506.08844</link>
<guid>https://arxiv.org/abs/2506.08844</guid>
<content:encoded><![CDATA[
<div> dataset, missing data imputation, socioeconomic research, benchmark, IMAGIC-500

Summary:
- The study focuses on missing data imputation in socioeconomic datasets, which are often restricted due to privacy concerns.
- The authors introduce the IMAGIC-500 dataset derived from the World Bank's synthetic dataset, allowing for broad access.
- A comprehensive missing data imputation benchmark is conducted on IMAGIC-500 with various missing mechanisms and ratios.
- Evaluation criteria include imputation accuracy, computational efficiency, and impact on predictive tasks like estimating educational attainment.
- Results assess statistical, traditional machine learning, deep learning, and diffusion-based imputation methods, aiming to improve algorithm development and reproducibility in social science research. 

<br /><br />Summary: <div>
arXiv:2506.08844v1 Announce Type: cross 
Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in data science and machine learning, particularly within socioeconomic research. However, real-world socioeconomic datasets are typically subject to strict data protection protocols, which often prohibit public sharing, even for synthetic derivatives. This severely limits the reproducibility and accessibility of benchmark studies in such settings. Further, there are very few publicly available synthetic datasets. Thus, there is limited availability of benchmarks for systematic evaluation of imputation methods on socioeconomic datasets, whether real or synthetic. In this study, we utilize the World Bank's publicly available synthetic dataset, Synthetic Data for an Imaginary Country, which closely mimics a real World Bank household survey while being fully public, enabling broad access for methodological research. With this as a starting point, we derived the IMAGIC-500 dataset: we select a subset of 500k individuals across approximately 100k households with 19 socioeconomic features, designed to reflect the hierarchical structure of real-world household surveys. This paper introduces a comprehensive missing data imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR, MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation considers the imputation accuracy for continuous and categorical variables, computational efficiency, and impact on downstream predictive tasks, such as estimating educational attainment at the individual level. The results highlight the strengths and weaknesses of statistical, traditional machine learning, and deep learning imputation techniques, including recent diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate the development of robust imputation algorithms and foster reproducible social science research.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market</title>
<link>https://arxiv.org/abs/2506.06356</link>
<guid>https://arxiv.org/abs/2506.06356</guid>
<content:encoded><![CDATA[
<div> deep learning, quantitative trading, Chinese A-share market, backtesting, risk management <br />
Summary:
This paper introduces a multi-day turnover quantitative trading algorithm for the Chinese A-share market that incorporates advanced deep learning techniques for stock prediction. The algorithm consists of five interconnected modules, including initial stock selection, opening signal analysis, position sizing, profit-taking, stop-loss mechanisms, and market timing models. Trained on a decade of A-share data and backtested rigorously, the algorithm achieves impressive annualized returns of 15.2% with controlled maximum drawdown and a high Sharpe ratio. It balances capital efficiency and risk management through adaptive holding periods and optimized entry/exit timing. The strategy maintains a large number of daily positions with a short maximum holding period, utilizing dynamic profit-taking and stop-loss mechanisms to enhance turnover efficiency while preserving risk-adjusted returns. The approach shows robust performance across different market conditions and is suitable for institutional deployment due to its high capital capacity. <br /> <div>
arXiv:2506.06356v1 Announce Type: new 
Abstract: This paper presents a sophisticated multi-day turnover quantitative trading algorithm that integrates advanced deep learning techniques with comprehensive cross-sectional stock prediction for the Chinese A-share market. Our framework combines five interconnected modules: initial stock selection through deep cross-sectional prediction networks, opening signal distribution analysis using mixture models for arbitrage identification, market capitalization and liquidity-based dynamic position sizing, grid-search optimized profit-taking and stop-loss mechanisms, and multi-granularity volatility-based market timing models. The algorithm employs a novel approach to balance capital efficiency with risk management through adaptive holding periods and sophisticated entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and rigorously backtested on 2021-2024 data, our method achieves remarkable performance with 15.2\% annualized returns, maximum drawdown constrained below 5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional scalability by maintaining 50-100 daily positions with a 9-day maximum holding period, incorporating dynamic profit-taking and stop-loss mechanisms that enhance capital turnover efficiency while preserving risk-adjusted returns. Our approach exhibits robust performance across various market regimes while maintaining high capital capacity suitable for institutional deployment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality</title>
<link>https://arxiv.org/abs/2506.06622</link>
<guid>https://arxiv.org/abs/2506.06622</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial analysis, QuantMCP, data APIs, decision-making

Summary:
QuantMCP introduces a framework to ground Large Language Models (LLMs) in financial reality by enabling them to access real-time financial data through standardized and secure tools like the Model Context Protocol (MCP). This allows LLMs to overcome issues such as data hallucination and lack of access to verified information. By leveraging Python-accessible financial data APIs, users can interact with LLMs via natural language to retrieve up-to-date and structured financial data. This enables LLMs to enhance their analytical capabilities, generate insights, and support informed financial decision-making processes. QuantMCP serves as a reliable and secure bridge between conversational AI and complex financial data, aiming to improve the reliability and analytical depth of LLM applications in finance. 

<br /><br />Summary: <div>
arXiv:2506.06622v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing financial analysis and decision-making, yet their direct application is often hampered by issues of data hallucination and lack of access to real-time, verifiable financial information. This paper introduces QuantMCP, a novel framework designed to rigorously ground LLMs in financial reality. By leveraging the Model Context Protocol (MCP) for standardized and secure tool invocation, QuantMCP enables LLMs to accurately interface with a diverse array of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can interact via natural language to precisely retrieve up-to-date financial data, thereby overcoming LLM's inherent limitations in factual data recall. More critically, once furnished with this verified, structured data, the LLM's analytical capabilities are unlocked, empowering it to perform sophisticated data interpretation, generate insights, and ultimately support more informed financial decision-making processes. QuantMCP provides a robust, extensible, and secure bridge between conversational AI and the complex world of financial data, aiming to enhance both the reliability and the analytical depth of LLM applications in finance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hype Index: an NLP-driven Measure of Market News Attention</title>
<link>https://arxiv.org/abs/2506.06329</link>
<guid>https://arxiv.org/abs/2506.06329</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, Hype Index, Media Attention, Financial News, Stock Volatility

Summary:
The paper introduces the Hype Index as a metric to quantify media attention towards large-cap equities using Natural Language Processing (NLP) techniques. Two versions of the Hype Index are constructed - News Count-Based and Capitalization Adjusted - to measure media exposure relative to stock or sector market capitalization. The Hype Index is evaluated based on its classification into hype groups, associations with returns and volatility, signaling power for market movements, and empirical properties such as correlations and trends. The findings suggest that the Hype Index family is a valuable tool for analyzing stock volatility, market signaling, and extending NLP applications in finance. <div>
arXiv:2506.06329v1 Announce Type: cross 
Abstract: This paper introduces the Hype Index as a novel metric to quantify media attention toward large-cap equities, leveraging advances in Natural Language Processing (NLP) for extracting predictive signals from financial news. Using the S&amp;P 100 as the focus universe, we first construct a News Count-Based Hype Index, which measures relative media exposure by computing the share of news articles referencing each stock or sector. We then extend it to the Capitalization Adjusted Hype Index, adjusts for economic size by taking the ratio of a stock's or sector's media weight to its market capitalization weight within its industry or sector. We compute both versions of the Hype Index at the stock and sector levels, and evaluate them through multiple lenses: (1) their classification into different hype groups, (2) their associations with returns, volatility, and VIX index at various lags, (3) their signaling power for short-term market movements, and (4) their empirical properties including correlations, samplings, and trends. Our findings suggest that the Hype Index family provides a valuable set of tools for stock volatility analysis, market signaling, and NLP extensions in Finance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models</title>
<link>https://arxiv.org/abs/2506.06335</link>
<guid>https://arxiv.org/abs/2506.06335</guid>
<content:encoded><![CDATA[
<div> Keywords: FinBERT2, financial-specific, bidirectional encoder, LLMs, fine-tuned models

Summary:
- FinBERT2 is introduced as a specialized bidirectional encoder pretrained on a financial-specific corpus of 32b tokens, representing the largest Chinese financial pretraining corpus for models of this parameter size.
- FinBERT2 addresses the limitations of LLMs in the financial sector by outperforming other variants on discriminatory tasks by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks.
- FinBERT2 also excels in contrastive fine-tuned models, outperforming open-source and proprietary embedders across five financial retrieval tasks.
- The construction of Fin-TopicModel based on FinBERT2 variants enables superior clustering and topic representation for financial titles. 
<br /><br />Summary: FinBERT2, a specialized bidirectional encoder pretrained on a financial-specific corpus, fills the gap in financial-specific deployment of LLMs by outperforming other variants and LLMs on classification and retrieval tasks. Additionally, the Fin-TopicModel offers enhanced clustering and topic representation for financial titles, showcasing the practical applications and benefits of utilizing FinBERT in the LLMs era. <div>
arXiv:2506.06335v1 Announce Type: cross 
Abstract: In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs' practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment</title>
<link>https://arxiv.org/abs/2506.06355</link>
<guid>https://arxiv.org/abs/2506.06355</guid>
<content:encoded><![CDATA[
<div> Efficient simulation, proactive preparedness, sudden-onset disasters, large language models, world models<br />
<br />
Efficient simulation is crucial for preparing for sudden-onset disasters like earthquakes. This study explores the use of large language models (LLMs) to predict earthquake impacts. By analyzing various datasets, including geospatial and socioeconomic data, the framework generates Modified Mercalli Intensity (MMI) predictions at different scales. Evaluation on past earthquakes shows a high correlation and low error rate compared to real reports. Techniques like RAG and ICL can enhance simulation performance, with visual inputs improving accuracy. These findings highlight the potential of LLMs in simulating disaster impacts, aiding in pre-event planning and response strategies.<br /><br />Summary: <div>
arXiv:2506.06355v1 Announce Type: cross 
Abstract: Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes. Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios. This study examines multiple LLMs to proactively estimate perceived earthquake impacts. Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales. Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real reports at the zip code level. Techniques such as RAG and ICL can improve simulation performance, while visual inputs notably enhance accuracy compared to structured numerical data alone. These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events</title>
<link>https://arxiv.org/abs/2506.06380</link>
<guid>https://arxiv.org/abs/2506.06380</guid>
<content:encoded><![CDATA[
<div> Keywords: extreme events, synthetic data generation, generative modeling techniques, evaluation framework, underexplored areas<br />
Summary:<br />
This article discusses the challenges in predicting extreme events due to the scarcity of data and introduces synthetic data generation as a solution. It reviews generative modeling techniques and large language models tailored for capturing heavy-tailed distributions in extreme event data. The authors propose an evaluation framework encompassing various metrics to assess the performance of models in extreme settings. The article categorizes application domains and highlights underexplored areas like behavioral finance and infectious outbreaks. It also offers insights into statistical theory enhancements and specialized training mechanisms for generating synthetic data. The survey outlines open challenges in synthetic rare-event research, providing a structured foundation for future advancements in this domain. <div>
arXiv:2506.06380v1 Announce Type: cross 
Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Probabilistic Framework for Learning with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.07003</link>
<guid>https://arxiv.org/abs/2506.07003</guid>
<content:encoded><![CDATA[
<div> probabilistic forecasting, operational constraints, uncertainty quantification, neural networks, probabilistic projection layer

Summary: 
ProbHardE2E is a probabilistic forecasting framework designed to incorporate operational or physical constraints as hard requirements. It enforces these constraints by utilizing variance information in a unique way, allowing for uncertainty quantification within the model. The framework utilizes a differentiable probabilistic projection layer (DPPL) that can be combined with various neural network architectures to learn the system in an end-to-end manner. ProbHardE2E can optimize a proper scoring rule without assuming a specific target distribution, enabling robust distributional estimates. It can also handle a range of non-linear constraints, increasing modeling power and flexibility. The framework is applied to learning partial differential equations with uncertainty estimates and probabilistic time-series forecasting, demonstrating its broad applicability across diverse domains. <div>
arXiv:2506.07003v1 Announce Type: cross 
Abstract: We present a general purpose probabilistic forecasting framework, ProbHardE2E, to learn systems that can incorporate operational/physical constraints as hard requirements. ProbHardE2E enforces hard constraints by exploiting variance information in a novel way; and thus it is also capable of performing uncertainty quantification (UQ) on the model. Our methodology uses a novel differentiable probabilistic projection layer (DPPL) that can be combined with a wide range of neural network architectures. This DPPL allows the model to learn the system in an end-to-end manner, compared to other approaches where the constraints are satisfied either through a post-processing step or at inference. In addition, ProbHardE2E can optimize a strictly proper scoring rule, without making any distributional assumptions on the target, which enables it to obtain robust distributional estimates (in contrast to existing approaches that generally optimize likelihood-based objectives, which are heavily biased by their distributional assumptions and model choices); and it can incorporate a range of non-linear constraints (increasing the power of modeling and flexibility). We apply ProbHardE2E to problems in learning partial differential equations with uncertainty estimates and to probabilistic time-series forecasting, showcasing it as a broadly applicable general setup that connects these seemingly disparate domains.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning</title>
<link>https://arxiv.org/abs/2506.07551</link>
<guid>https://arxiv.org/abs/2506.07551</guid>
<content:encoded><![CDATA[
<div> Chemistry, Large language models, Chemical tools, Dataset curation, Hierarchical Evolutionary Monte Carlo Tree Search

Summary: 
This article introduces a novel approach to enhance the performance of Large Language Models (LLMs) in chemistry tasks by integrating external chemical tools and dataset curation. The proposed Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework allows for independent optimization of tool planning and execution, leading to improved performance in Chemistry QA and discovery tasks. By generating the ChemToolBench dataset and leveraging self-generated data, the approach supports step-level fine-tuning of the policy model and training task-adaptive PRM and ORM. Experimental evaluations show that this approach surpasses existing models such as GPT-4o, offering a robust solution for integrating specialized tools with LLMs in advanced chemical applications. The datasets and code for this study are available on GitHub at https://github.com/AI4Chem/ChemistryAgent.

<br /><br />Summary: <div>
arXiv:2506.07551v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated promising capabilities in chemistry tasks while still facing challenges due to outdated pretraining knowledge and the difficulty of incorporating specialized chemical expertise. To address these issues, we propose an LLM-based agent that synergistically integrates 137 external chemical tools created ranging from basic information retrieval to complex reaction predictions, and a dataset curation pipeline to generate the dataset ChemToolBench that facilitates both effective tool selection and precise parameter filling during fine-tuning and evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework, enabling independent optimization of tool planning and execution. By leveraging self-generated data, our approach supports step-level fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM that surpass GPT-4o. Experimental evaluations demonstrate that our approach significantly improves performance in Chemistry QA and discovery tasks, offering a robust solution to integrate specialized tools with LLMs for advanced chemical applications. All datasets and code are available at https://github.com/AI4Chem/ChemistryAgent .
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</title>
<link>https://arxiv.org/abs/2506.07865</link>
<guid>https://arxiv.org/abs/2506.07865</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scene geometry, appearance, physics, multi-view videos, dynamic scenes

Summary:
FreeGave is a novel approach for modeling complex dynamic 3D scenes solely from multi-view videos, without requiring object priors. The method introduces a physics code and a divergence-free module to estimate per-Gaussian velocity fields, without relying on inefficient PINN losses. Extensive experiments on various datasets demonstrate the superior performance of FreeGave in future frame extrapolation and motion segmentation tasks. The approach is able to learn meaningful 3D physical motion patterns without the need for human labels in training, showcasing its ability to capture complex physical motions at boundaries. By effectively incorporating physics simulation into neural networks, FreeGave shows promising results in learning scene geometry, appearance, and underlying physics, making it a valuable tool for understanding and analyzing dynamic scenes. 

<br /><br />Summary: <div>
arXiv:2506.07865v1 Announce Type: cross 
Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the underlying physics purely from multi-view videos. By applying various governing PDEs as PINN losses or incorporating physics simulation into neural networks, existing works often fail to learn complex physical motions at boundaries or require object priors such as masks or types. In this paper, we propose FreeGave to learn the physics of complex dynamic 3D scenes without needing any object priors. The key to our approach is to introduce a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on the inefficient PINN losses. Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of our method for future frame extrapolation and motion segmentation. Most notably, our investigation into the learned physics codes reveals that they truly learn meaningful 3D physical motion patterns in the absence of any human labels in training.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>pyCFS-data: Data Processing Framework in Python for openCFS</title>
<link>https://arxiv.org/abs/2405.03437</link>
<guid>https://arxiv.org/abs/2405.03437</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical simulation, multi-field problems, aeroacoustics, open-source framework, data processing

Summary:
openCFS is an open-source framework developed for simulating multi-field problems, focusing on aeroacoustics. It allows for the implementation of partial differential equations using the finite element method. The software, which has been continuously developed since 2000, is now known as openCFS (previously CFS++ Coupled Field Simulations written in C++). In this paper, pyCFS-data, a data processing framework written in Python, is introduced to provide a flexible and user-friendly toolbox for accessing, manipulating, pre- and postprocessing data related to openCFS simulations. This tool aims to make the handling of simulation data more efficient and convenient for users. <div>
arXiv:2405.03437v2 Announce Type: replace 
Abstract: Many numerical simulation tools have been developed and are on the market, but there is still a strong need for appropriate tools capable of simulating multi-field problems, especially in aeroacoustics. Therefore, openCFS provides an open-source framework for implementing partial differential equations using the finite element method. Since 2000, the software has been developed continuously. The result is openCFS (before 2020, known as CFS++ Coupled Field Simulations written in C++). In this paper, we present pyCFS-data, a data processing framework written in Python to provide a flexible and easy-to-use toolbox to access and manipulate, pre- and postprocess data generated by or for usage with openCFS.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting the Canonicalization for Fast and Accurate Crystal Tensor Property Prediction</title>
<link>https://arxiv.org/abs/2410.02372</link>
<guid>https://arxiv.org/abs/2410.02372</guid>
<content:encoded><![CDATA[
<div> canonicalization, crystal tensor property prediction, O(3)-equivariant framework, polar decomposition, computational efficiency

Summary: 
In the field of materials science, predicting the tensor properties of crystalline materials is crucial but computationally expensive due to the need for O(3) group tensor equivariance. The proposed framework, GoeCTP, uses polar decomposition as a form of canonicalization to ensure efficient and accurate crystal tensor property prediction. By incorporating canonicalization, GoeCTP achieves high prediction accuracy and runs significantly faster than existing methods. This novel approach eliminates the need for complex architecture designs to maintain equivariance constraints, making it a more efficient and effective solution for crystal tensor property prediction. <div>
arXiv:2410.02372v3 Announce Type: replace 
Abstract: Predicting the tensor properties of crystalline materials is a fundamental task in materials science. Unlike single-value property prediction, which is inherently invariant, tensor property prediction requires maintaining O(3) group tensor equivariance. Such equivariance constraint often requires specialized architecture designs to achieve effective predictions, inevitably introducing tremendous computational costs. Canonicalization, a classical technique for geometry, has recently been explored for efficient learning with symmetry. In this work, we revisit the problem of crystal tensor property prediction through the lens of canonicalization. Specifically, we demonstrate how polar decomposition, a simple yet efficient algebraic method, can serve as a form of canonicalization and be leveraged to ensure equivariant tensor property prediction. Building upon this insight, we propose a general O(3)-equivariant framework for fast and accurate crystal tensor property prediction, referred to as GoeCTP. By utilizing canonicalization, GoeCTP achieves high efficiency without requiring the explicit incorporation of equivariance constraints into the network architecture. Experimental results indicate that GoeCTP achieves the best prediction accuracy and runs up to 13 times faster compared to existing state-of-the-art methods in benchmarking datasets, underscoring its effectiveness and efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-PINN: A Physics-Informed Neural Network with Finite Geometric Encoding for Solid Mechanics</title>
<link>https://arxiv.org/abs/2412.09453</link>
<guid>https://arxiv.org/abs/2412.09453</guid>
<content:encoded><![CDATA[
<div> finite element method, solid mechanics, PINN, neural network, hybrid space <br />
Summary: <br />
This study addresses challenges in using Physics-Informed Neural Networks (PINN) for solid mechanics problems compared to traditional methods like the finite element method (FEM). The main challenges are the infinite domain generated by PINN conflicting with finite boundaries in solid structures and the Euclidean solution space being insufficient for complex solid geometries. The Finite-PINN model proposed in this work overcomes these challenges by incorporating finite geometric encoding into neural network inputs, creating a hybrid Euclidean-topological solution space. This model is trained using both strong-form and weak-form loss formulations, allowing it to efficiently solve forward problems with preprocessed structural geometric information and reconstruct full-field solutions from sparse observations in inverse problems by embedding physical laws and geometric information. The Finite-PINN model shows promise for addressing a wide range of solid mechanics problems effectively. <div>
arXiv:2412.09453v2 Announce Type: replace 
Abstract: PINN models have demonstrated capabilities in addressing fluid PDE problems, and their potential in solid mechanics is beginning to emerge. This study identifies two key challenges when using PINN to solve general solid mechanics problems. These challenges become evident when comparing the limitations of PINN with the well-established numerical methods commonly used in solid mechanics, such as the finite element method (FEM). Specifically: a) PINN models generate solutions over an infinite domain, which conflicts with the finite boundaries typical of most solid structures; and b) the solution space utilised by PINN is Euclidean, which is inadequate for addressing the complex geometries often present in solid structures.
  This work presents a PINN architecture for general solid mechanics problems, referred to as the Finite-PINN model. The model is designed to effectively tackle two key challenges, while retaining as much of the original PINN framework as possible. To this end, the Finite-PINN incorporates finite geometric encoding into the neural network inputs, thereby transforming the solution space from a conventional Euclidean space into a hybrid Euclidean-topological space. The model is comprehensively trained using both strong-form and weak-form loss formulations, enabling its application to a wide range of forward and inverse problems in solid mechanics. For forward problems, the Finite-PINN model efficiently approximates solutions to solid mechanics problems when the geometric information of a given structure has been preprocessed. For inverse problems, it effectively reconstructs full-field solutions from very sparse observations by embedding both physical laws and geometric information within its architecture.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Battery Energy Storage in the Continuous Intraday Market: Forecast vs. Perfect Foresight Strategies</title>
<link>https://arxiv.org/abs/2501.07121</link>
<guid>https://arxiv.org/abs/2501.07121</guid>
<content:encoded><![CDATA[
<div> modeling, grid-scale battery, energy storage system, intraday market, price forecast

Summary:
Grid-scale battery energy storage systems can optimize trading in the European continuous intraday market by using a forecast-driven model. This model utilizes price forecasts to optimize trading schedules, resulting in significant earning potential. The approach outperforms key market indices and demonstrates the profitability of participating in the CID market despite its complexity. By comparing profits across various spot markets, the forecast-driven model proves to be effective in capturing market dynamics and maximizing revenue. Using real data, a 1 MW/1 MWh system earns EUR 146,237, showcasing the model's success in estimating earnings potential and optimizing trading strategies. The method surpasses key market indices by a significant margin, confirming its reliability and effectiveness in trading energy in the CID market. <div>
arXiv:2501.07121v2 Announce Type: replace 
Abstract: Grid-scale battery energy storage systems (BESSs) can provide flexibility to the power system and capture shortterm price volatility by shifting energy in time through controlled charging and discharging. The highly volatile European continuous intraday (CID) market allows trading until just a few minutes before physical delivery, offering significant earning potential. However, its high trading frequency poses substantial modeling challenges. Accurate modeling of BESSs trading in the CID market is essential to estimate revenue potential and optimize trading strategies. Additionally, comparing CID profits with other spot markets helps determine whether participating in the CID is worthwhile despite its complexity. We propose a forecast-driven model to optimize BESS trading in the CID market. Our strategy employs a rolling window modeling framework to capture market dynamics. Price forecasts for impending CID products are generated at the beginning of each window and used to optimize trading schedules for subsequent execution. We also benchmark our approach across various spot markets, offering a broad cross-market profit comparison. We evaluate our forecast-driven model across different BESS power-to-capacity ratios, comparing it to a perfect-foresight scenario and key CID market indices, such as ID1 and ID3. Using real 2023 German CID data, a 1 MW/1 MWh system adopting our method earns EUR 146 237, only 11% below perfect foresight, surpassing all other markets and indices. Our approach surpasses ID1 and ID3 by over 4% and 32%, respectively, confirming ID1 as a reliable lower-bound estimate for earnings potential in the CID market.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Deep Learning Surrogate Models for Uncertainty Propagation in Microstructure-Properties of Ceramic Aerogels</title>
<link>https://arxiv.org/abs/2501.13255</link>
<guid>https://arxiv.org/abs/2501.13255</guid>
<content:encoded><![CDATA[
<div> surrogate models, deep learning, ceramic aerogel, microstructure, mechanical response
Summary:
This study introduces a computational framework that combines physics-based simulations with deep learning surrogate models to predict the microstructural morphology and mechanical behavior of ceramic aerogel porous materials. Lattice Boltzmann simulations model microstructure formation during material synthesis, while a finite element model calculates mechanical properties. To address the computational demands of analyzing microstructural randomness, Convolutional Neural Networks (CNNs) are used to develop surrogate models for microstructure generation and mapping. CNN training is treated as a Bayesian inference problem to enable uncertainty quantification in predictions. The surrogate models produce microstructural images consistent with training data and accurately predict strain energy for in-distribution microstructures. The study investigates the generalization capability of the surrogate models and utilizes them for efficient uncertainty propagation to quantify the influence of microstructural variability on macroscopic mechanical properties.<br /><br />Summary: <div>
arXiv:2501.13255v3 Announce Type: replace 
Abstract: This study presents an integrated computational framework that, given synthesis parameters, predicts the resulting microstructural morphology and mechanical response of ceramic aerogel porous materials by combining physics-based simulations with deep learning surrogate models. Lattice Boltzmann simulations are employed to model microstructure formation during material synthesis process, while a finite element model is used to compute the corresponding mechanical properties. To overcome the prohibitive computational demands of repeated physics-based simulations required for characterizing the impact of microstructure randomness on mechanical properties, surrogate models are developed using Convolutional Neural Networks (CNNs) for both microstructure generation and microstructure-property mapping. CNN training is formulated as a Bayesian inference problem to enable uncertainty quantification and provide confidence estimates in surrogate model predictions, under limited training data furnished by physics-based simulations. Numerical results demonstrate that the microstructure surrogate model effectively generates microstructural images consistent with the morphology of training data across larger domains. The Bayesian CNN surrogate accurately predicts strain energy for in-distribution microstructures and its generalization capability to interpolated morphologies are further investigated. Finally, the surrogate models are employed for efficient uncertainty propagation, quantifying the influence of microstructural variability on macroscopic mechanical property.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay</title>
<link>https://arxiv.org/abs/2502.16789</link>
<guid>https://arxiv.org/abs/2502.16789</guid>
<content:encoded><![CDATA[
<div> alpha decay, alpha mining, Large Language Models, genetic programming, factor overfitting
<br />
Summary: 
The article introduces AlphaAgent, a framework designed to address the challenge of alpha decay in quantitative investment. By integrating Large Language Models with ad hoc regularizations, AlphaAgent aims to generate decay-resistant alpha factors. It enforces originality by comparing generated factors with existing ones, aligns hypotheses with factors for market consistency, and controls complexity to prevent overfitting. Through extensive evaluations in Chinese CSI 500 and US S&amp;P 500 markets, AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay. It demonstrates significant resistance to alpha decay and consistently delivers substantial alpha over the past four years, offering promising potential for powerful factors. <div>
arXiv:2502.16789v2 Announce Type: replace 
Abstract: Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay, where factors lose their predictive power over time, poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and US S&amp;P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications</title>
<link>https://arxiv.org/abs/2408.11878</link>
<guid>https://arxiv.org/abs/2408.11878</guid>
<content:encoded><![CDATA[
<div> Financial LLMs, Open-FinLLMs, multimodal capabilities, zero-shot, few-shot, fine-tuning.

Summary:
Open-FinLLMs are introduced as the first open-source multimodal financial Language Models (LLMs) to handle various financial tasks using text, tabular, time-series, and chart data. The suite includes FinLLaMA pre-trained on a large corpus, FinLLaMA-Instruct fine-tuned with financial instructions, and FinLLaVA enhanced with multimodal tuning pairs for cross-modal reasoning. Evaluation across 14 financial tasks and 4 multimodal tasks shows that Open-FinLLMs outperform other financial and general LLMs such as GPT-4 in tasks like financial NLP and decision-making, indicating their potential to address real-world challenges. Codes and models are released under OSI-approved licenses to encourage collaboration between academia and industry. <br /><br />Summary: Financial LLMs Open-FinLLMs introduced as open-source multimodal models, excel in diverse tasks, outperforming GPT-4, released with codes and models for collaboration. <div>
arXiv:2408.11878v3 Announce Type: replace-cross 
Abstract: Financial LLMs hold promise for advancing financial tasks and domain-specific applications. However, they are limited by scarce corpora, weak multimodal capabilities, and narrow evaluations, making them less suited for real-world application. To address this, we introduce \textit{Open-FinLLMs}, the first open-source multimodal financial LLMs designed to handle diverse tasks across text, tabular, time-series, and chart data, excelling in zero-shot, few-shot, and fine-tuning settings. The suite includes FinLLaMA, pre-trained on a comprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K financial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning pairs for strong cross-modal reasoning. We comprehensively evaluate Open-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in zero-shot, few-shot, and supervised fine-tuning settings, introducing two new multimodal evaluation datasets. Our results show that Open-FinLLMs outperforms afvanced financial and general LLMs such as GPT-4, across financial NLP, decision-making, and multi-modal tasks, highlighting their potential to tackle real-world challenges. To foster innovation and collaboration across academia and industry, we release all codes (https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under OSI-approved licenses.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Deep Learning Model for Line-integral Diagnostics Across Fusion Devices</title>
<link>https://arxiv.org/abs/2412.00087</link>
<guid>https://arxiv.org/abs/2412.00087</guid>
<content:encoded><![CDATA[
<div> physics-informed model, deep learning, plasma profiles, line-integral measurements, nuclear fusion <br />
Summary: <br />
The paper introduces a physics-informed model architecture called Onion for rapid 2D plasma profile reconstruction from line-integral measurements in nuclear fusion research. By incorporating physical information through a multiplication process and applying a physics-informed loss function based on the principle of line integration, the model shows improvements in performance. Results indicate a reduction in average relative error in reconstruction profiles compared to target profiles on both synthetic and experimental datasets. The use of Softplus activation function in the final two fully connected layers further enhances model performance. The physics-informed loss function corrects prediction errors, bringing back-projections closer to actual inputs and reducing inversion algorithm errors. Synthetic data models for generating customized diagnostic datasets and collection of soft x-ray diagnostic datasets from EAST and HL-2A contribute to the study's success in reducing reconstruction errors and accelerating the development of surrogate models in fusion research. <br /> <div>
arXiv:2412.00087v3 Announce Type: replace-cross 
Abstract: Rapid reconstruction of 2D plasma profiles from line-integral measurements is important in nuclear fusion. This paper introduces a physics-informed model architecture called Onion, that can enhance the performance of models and be adapted to various backbone networks. The model under Onion incorporates physical information by a multiplication process and applies the physics-informed loss function according to the principle of line integration. Prediction results demonstrate that the additional input of physical information improves the deep learning model's ability, leading to a reduction in the average relative error E_1 between the reconstruction profiles and the target profiles by approximately 0.84x10^(-2) on synthetic datasets and about 0.06x10^(-2) on experimental datasets. Furthermore, the implementation of the Softplus activation function in the final two fully connected layers improves model performance. This enhancement results in a reduction in the E_1 by approximately 1.06x10^(-2) on synthetic datasets and about 0.11x10^(-2) on experimental datasets. The incorporation of the physics-informed loss function has been shown to correct the model's predictions, bringing the back-projections closer to the actual inputs and reducing the errors associated with inversion algorithms. Besides, we have developed a synthetic data model to generate customized line-integral diagnostic datasets and have also collected soft x-ray diagnostic datasets from EAST and HL-2A. This study achieves reductions in reconstruction errors, and accelerates the development of surrogate models in fusion research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven inventory management for new products: An adjusted Dyna-$Q$ approach with transfer learning</title>
<link>https://arxiv.org/abs/2501.08109</link>
<guid>https://arxiv.org/abs/2501.08109</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, inventory management, transfer learning, model-based approach, Dyna-Q

Summary:
The paper presents a novel reinforcement learning algorithm for inventory management of new products without historical demand data. It combines model-free and model-based approaches in a Dyna-Q structure, enhancing training efficiency and reducing model discrepancy. By incorporating transfer learning from similar existing products' demand data, the algorithm stabilizes early-stage training and improves policy estimation accuracy. Validation through a bakery inventory case study demonstrates up to a 23.7% cost reduction compared to Q-learning and up to a 77.5% faster training time than traditional Dyna-Q. Utilizing transfer learning, the adjusted Dyna-Q outperforms benchmark algorithms in terms of total cost, cost variance, and shortage percentages during a 30-day testing period. <div>
arXiv:2501.08109v4 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel reinforcement learning algorithm for inventory management of newly launched products with no historical demand information. The algorithm follows the classic Dyna-$Q$ structure, balancing the model-free and model-based approaches, while accelerating the training process of Dyna-$Q$ and mitigating the model discrepancy generated by the model-based feedback. Based on the idea of transfer learning, warm-start information from the demand data of existing similar products can be incorporated into the algorithm to further stabilize the early-stage training and reduce the variance of the estimated optimal policy. Our approach is validated through a case study of bakery inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\% reduction in average daily cost compared with $Q$-learning, and up to a 77.5\% reduction in training time within the same horizon compared with classic Dyna-$Q$. By using transfer learning, it can be found that the adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and relatively low shortage percentages among all the benchmarking algorithms under a 30-day testing.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Aspects of Strategic Trading</title>
<link>https://arxiv.org/abs/2502.07606</link>
<guid>https://arxiv.org/abs/2502.07606</guid>
<content:encoded><![CDATA[
<div> Algorithmic trading, position building, equilibrium strategies, temporary market impact, permanent market impact <br />
<br />Summary: 
This study focuses on algorithmic trading in financial markets, particularly in the context of position building with temporary and permanent market impact. The research presents an efficient algorithm for computing best responses in trading strategies, highlighting the challenges of convergence in the general setting. While the temporary impact-only scenario forms a potential game, convergence is not guaranteed in the broader context. The concept of Coarse Correlated Equilibria (CCE) is introduced as an alternative solution, computable via Follow the Perturbed Leader (FTPL) implementation. An experimental investigation demonstrates the behavior of FTPL in varying conditions of temporary and permanent market impact weighting, shedding light on strategic behaviors in complex trading environments. <div>
arXiv:2502.07606v2 Announce Type: replace-cross 
Abstract: Algorithmic trading in modern financial markets is widely acknowledged to exhibit strategic, game-theoretic behaviors whose complexity can be difficult to model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress in the setting of trading for position building. Here parties wish to buy or sell a fixed number of shares in a fixed time period in the presence of both temporary and permanent market impact, resulting in exponentially large strategy spaces. While these papers primarily consider the existence and structural properties of equilibrium strategies, in this work we focus on the algorithmic aspects of the proposed model. We give an efficient algorithm for computing best responses, and show that while the temporary impact only setting yields a potential game, best response dynamics do not generally converge for the general setting, for which no fast algorithm for (Nash) equilibrium computation is known. This leads us to consider the broader notion of Coarse Correlated Equilibria (CCE), which we show can be computed efficiently via an implementation of Follow the Perturbed Leader (FTPL). We illustrate the model and our results with an experimental investigation, where FTPL exhibits interesting behavior in different regimes of the relative weighting between temporary and permanent market impact.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation</title>
<link>https://arxiv.org/abs/2503.09409</link>
<guid>https://arxiv.org/abs/2503.09409</guid>
<content:encoded><![CDATA[
<div> AI-based framework, automation, cable connector mating, force control, deep visuotactile learning<br />
<br />
Summary: 
An AI-based framework has been developed to automate the manual process of cable connector mating in automotive assembly. The system integrates force control with deep visuotactile learning to optimize search-and-insertion strategies using multimodal transformer architecture. A novel automated data collection and optimization pipeline reduces the need for machine learning expertise. The framework optimizes robot programs that can run on standard industrial controllers, allowing for human auditing and certification. Experimental validations on a center console assembly task show improved cycle times and robustness compared to traditional robot programming methods. Videos demonstrating the system's capabilities are available for further reference. <div>
arXiv:2503.09409v2 Announce Type: replace-cross 
Abstract: Despite the widespread adoption of industrial robots in automotive assembly, wire harness installation remains a largely manual process, as it requires precise and flexible manipulation. To address this challenge, we design a novel AI-based framework that automates cable connector mating by integrating force control with deep visuotactile learning. Our system optimizes search-and-insertion strategies using first-order optimization over a multimodal transformer architecture trained on visual, tactile, and proprioceptive data. Additionally, we design a novel automated data collection and optimization pipeline that minimizes the need for machine learning expertise. The framework optimizes robot programs that run natively on standard industrial controllers, permitting human experts to audit and certify them. Experimental validations on a center console assembly task demonstrate significant improvements in cycle times and robustness compared to conventional robot programming approaches. Videos are available under https://claudius-kienle.github.io/AppMuTT.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Informer for Option Pricing: A Transformer-Based Approach</title>
<link>https://arxiv.org/abs/2506.05565</link>
<guid>https://arxiv.org/abs/2506.05565</guid>
<content:encoded><![CDATA[
<div> neural network, option pricing, financial forecasting, long-term dependencies, market fluctuations

Summary:
This paper explores the use of the Informer neural network for accurate option pricing in financial markets. Traditional models like Black-Scholes are limited in capturing market volatility, but Informer's efficient architecture allows for better prediction accuracy by incorporating long-term dependencies and dynamically adapting to market fluctuations. The study demonstrates that Informer surpasses traditional approaches in option pricing, highlighting its potential to enhance data-driven financial forecasting in this field. The research contributes to advancing the capabilities of option pricing by providing a more adaptable and resilient framework for effective trading and risk management in financial markets.<br /><br />Summary: <div>
arXiv:2506.05565v1 Announce Type: new 
Abstract: Accurate option pricing is essential for effective trading and risk management in financial markets, yet it remains challenging due to market volatility and the limitations of traditional models like Black-Scholes. In this paper, we investigate the application of the Informer neural network for option pricing, leveraging its ability to capture long-term dependencies and dynamically adjust to market fluctuations. This research contributes to the field of financial forecasting by introducing Informer's efficient architecture to enhance prediction accuracy and provide a more adaptable and resilient framework compared to existing methods. Our results demonstrate that Informer outperforms traditional approaches in option pricing, advancing the capabilities of data-driven financial forecasting in this domain.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction</title>
<link>https://arxiv.org/abs/2506.05427</link>
<guid>https://arxiv.org/abs/2506.05427</guid>
<content:encoded><![CDATA[
<div> prediction, drug discovery, material design, computational methods, interaction details 

Summary:
The article presents the Multi-Grained Target Perception network (MTPNet) for activity cliff prediction in drug discovery and material design. MTPNet integrates Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance to optimize molecular representations based on protein interactions. This approach, utilizing receptor proteins as guiding information, outperforms previous methods by achieving an average RMSE improvement of 18.95% on various datasets. By incorporating interaction patterns through conditional deep learning, MTPNet provides accurate predictions of activity cliffs, aiding in compound optimization and design. The availability of codes for MTPNet on GitHub enhances its accessibility and applicability in research and industry. <div>
arXiv:2506.05427v1 Announce Type: cross 
Abstract: Activity cliff prediction is a critical task in drug discovery and material design. Existing computational methods are limited to handling single binding targets, which restricts the applicability of these prediction models. In this paper, we present the Multi-Grained Target Perception network (MTPNet) to incorporate the prior knowledge of interactions between the molecules and their target proteins. Specifically, MTPNet is a unified framework for activity cliff prediction, which consists of two components: Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet dynamically optimizes molecular representations through multi-grained protein semantic conditions. To our knowledge, it is the first time to employ the receptor proteins as guiding information to effectively capture critical interaction details. Extensive experiments on 30 representative activity cliff datasets demonstrate that MTPNet significantly outperforms previous approaches, achieving an average RMSE improvement of 18.95% on top of several mainstream GNN architectures. Overall, MTPNet internalizes interaction patterns through conditional deep learning to achieve unified predictions of activity cliffs, helping to accelerate compound optimization and design. Codes are available at: https://github.com/ZishanShu/MTPNet.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Network Model of Spatial and Feature-Based Attention</title>
<link>https://arxiv.org/abs/2506.05487</link>
<guid>https://arxiv.org/abs/2506.05487</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual attention, neural network model, human cognition, spatial attention, feature-based attention

Summary: 
This article introduces a neural network model inspired by human visual attention mechanisms. The model consists of two networks, one performing a basic task and the other guiding attention based on contextual information for more complex tasks. The trained model's attention patterns closely resemble spatial and feature-based attention observed in human vision. This similarity suggests that neural network models can effectively mimic human cognition, offering valuable insights into the workings of visual attention. The study highlights the potential of using neural network models to explore and understand human cognitive processes, particularly in the realm of visual attention. <div>
arXiv:2506.05487v1 Announce Type: cross 
Abstract: Visual attention is a mechanism closely intertwined with vision and memory. Top-down information influences visual processing through attention. We designed a neural network model inspired by aspects of human visual attention. This model consists of two networks: one serves as a basic processor performing a simple task, while the other processes contextual information and guides the first network through attention to adapt to more complex tasks. After training the model and visualizing the learned attention response, we discovered that the model's emergent attention patterns corresponded to spatial and feature-based attention. This similarity between human visual attention and attention in computer vision suggests a promising direction for studying human cognition using neural network models.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Stabilization Protocol for Cross-Chain Digital Assets Using Adaptor Signatures and AI-Driven Arbitrage</title>
<link>https://arxiv.org/abs/2506.05708</link>
<guid>https://arxiv.org/abs/2506.05708</guid>
<content:encoded><![CDATA[
<div> decentralization, stability, regulatory compliance, stablecoins, hybrid stabilization protocol

Summary:
The article discusses the challenges faced by stablecoins in balancing decentralization, stability, and regulatory compliance. A hybrid stabilization protocol is proposed, combining crypto-collateralized reserves, algorithmic futures contracts, and cross-chain liquidity pools. Stabilization futures contracts (SFCs) are introduced as non-collateralized derivatives that incentivize third-party arbitrageurs to maintain price adherence. Autonomous AI agents optimize delta hedging on decentralized exchanges, while zkSNARKs ensure compliance with anti-money laundering regulations without revealing user identities. The cryptographic design reduces cross-chain liquidity concentration and ensures atomicity in transactions. The protocol's layered architecture, including SFCs, AI market making, and zero-knowledge regulatory proofs, serves as a blueprint for advanced decentralized financial infrastructure. <div>
arXiv:2506.05708v1 Announce Type: cross 
Abstract: Stablecoins face an unresolved trilemma of balancing decentralization, stability, and regulatory compliance. We present a hybrid stabilization protocol that combines crypto-collateralized reserves, algorithmic futures contracts, and cross-chain liquidity pools to achieve robust price adherence while preserving user privacy. At its core, the protocol introduces stabilization futures contracts (SFCs), non-collateralized derivatives that programmatically incentivize third-party arbitrageurs to counteract price deviations via adaptor signature atomic swaps. Autonomous AI agents optimize delta hedging across decentralized exchanges (DEXs), while zkSNARKs prove compliance with anti-money laundering (AML) regulations without exposing identities or transaction details. Our cryptographic design reduces cross-chain liquidity concentration (Herfindahl-Hirschman Index: 2,400 vs. 4,900 in single-chain systems) and ensures atomicity under standard cryptographic assumptions. The protocol's layered architecture encompassing incentive-compatible SFCs, AI-driven market making, and zero-knowledge regulatory proofs. It provides a blueprint for next-generation decentralized financial infrastructure.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts</title>
<link>https://arxiv.org/abs/2506.05755</link>
<guid>https://arxiv.org/abs/2506.05755</guid>
<content:encoded><![CDATA[
<div> flow matching models, optimal execution, financial markets, imitation learning framework, market impact costs

Summary:
flowOE is introduced as a novel imitation learning framework for optimal execution in financial markets. It learns from expert traditional strategies and selects the most suitable behavior based on market conditions, incorporating a refining loss function to improve upon learned actions. This approach outperforms both expert models and traditional benchmarks in empirical evaluations, achieving higher profits with reduced risk. FlowOE demonstrates practical applicability and potential to enhance adaptive optimal execution in dynamic financial markets. <br /><br />Summary: <div>
arXiv:2506.05755v1 Announce Type: cross 
Abstract: Optimal execution in financial markets refers to the process of strategically transacting a large volume of assets over a period to achieve the best possible outcome by balancing the trade-off between market impact costs and timing or volatility risks. Traditional optimal execution strategies, such as static Almgren-Chriss models, often prove suboptimal in dynamic financial markets. This paper propose flowOE, a novel imitation learning framework based on flow matching models, to address these limitations. FlowOE learns from a diverse set of expert traditional strategies and adaptively selects the most suitable expert behavior for prevailing market conditions. A key innovation is the incorporation of a refining loss function during the imitation process, enabling flowOE not only to mimic but also to improve upon the learned expert actions. To the best of our knowledge, this work is the first to apply flow matching models in a stochastic optimal execution problem. Empirical evaluations across various market conditions demonstrate that flowOE significantly outperforms both the specifically calibrated expert models and other traditional benchmarks, achieving higher profits with reduced risk. These results underscore the practical applicability and potential of flowOE to enhance adaptive optimal execution.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator</title>
<link>https://arxiv.org/abs/2506.05797</link>
<guid>https://arxiv.org/abs/2506.05797</guid>
<content:encoded><![CDATA[
<div> equivariant neural fields simulator, deformable objects, collisions, Graph Neural Network, scalability
Summary:
EqCollide is a novel end-to-end neural fields simulator designed for simulating collisions of deformable objects. The model incorporates an equivariant encoder to map object geometry and velocity into latent control points, followed by a Graph Neural Network-based Neural Ordinary Differential Equation to model interactions among control points via collision-aware message passing. The approach allows for accurate and stable simulations across diverse object configurations, with a significant reduction in MSE compared to baseline models. Furthermore, EqCollide demonstrates scalability, generalization to more colliding objects and extended temporal horizons, and robustness to input transformations with group action. The model also enables continuous and resolution-independent motion predictions, showcasing its potential for a wide range of applications in simulating complex interactions among deformable objects. 
<br /><br />Summary: <div>
arXiv:2506.05797v1 Announce Type: cross 
Abstract: Simulating collisions of deformable objects is a fundamental yet challenging task due to the complexity of modeling solid mechanics and multi-body interactions. Existing data-driven methods often suffer from lack of equivariance to physical symmetries, inadequate handling of collisions, and limited scalability. Here we introduce EqCollide, the first end-to-end equivariant neural fields simulator for deformable objects and their collisions. We propose an equivariant encoder to map object geometry and velocity into latent control points. A subsequent equivariant Graph Neural Network-based Neural Ordinary Differential Equation models the interactions among control points via collision-aware message passing. To reconstruct velocity fields, we query a neural field conditioned on control point features, enabling continuous and resolution-independent motion predictions. Experimental results show that EqCollide achieves accurate, stable, and scalable simulations across diverse object configurations, and our model achieves 24.34% to 35.82% lower rollout MSE even compared with the best-performing baseline model. Furthermore, our model could generalize to more colliding objects and extended temporal horizons, and stay robust to input transformed with group action.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging</title>
<link>https://arxiv.org/abs/2506.05828</link>
<guid>https://arxiv.org/abs/2506.05828</guid>
<content:encoded><![CDATA[
<div> Keywords: FinanceReasoning, benchmark, large reasoning models, financial concepts, numerical precision

Summary: 
FinanceReasoning introduces a new benchmark to evaluate the reasoning abilities of large reasoning models (LRMs) in financial numerical tasks. The benchmark includes updated questions with detailed Python solutions, covering 67.8% of financial concepts and formulas. LRMs benefit from 3,133 Python-formatted functions, enhancing their financial reasoning capabilities. The benchmark also presents 238 challenging problems requiring precise numerical reasoning. The best-performing model achieves 89.1% accuracy, highlighting the ongoing challenges faced by LRMs in numerical precision. The study demonstrates that combining Reasoner and Programmer models can improve LRMs' performance. This advancement in evaluating LRMs in specific reasoning tasks opens avenues for future research in complex domain-specific reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2506.05828v1 Announce Type: cross 
Abstract: We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs' financial reasoning capabilities through refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs' performance (e.g., 83.2% $\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Tasks and Their Complexity for the European Train Control System with Hybrid Train Detection</title>
<link>https://arxiv.org/abs/2308.02572</link>
<guid>https://arxiv.org/abs/2308.02572</guid>
<content:encoded><![CDATA[
<div> Keywords: railway networks, train control system, ETCS L2 HTD, design tasks, computational complexity

Summary:
Railway networks play a crucial role in transportation, especially for freight and public transit. Maximizing rail capacity is essential, but existing constraints must be considered. The European Train Control System (ETCS L2 HTD) introduces virtual subsections to improve train following times and increase track capacity. Design tasks for ETCS L2 HTD present new challenges that can benefit from automated methods. This paper provides a formal description of these design tasks and proves their computational complexity to be NP-complete or NP-hard. This research forms the foundation for developing methods to address these tasks and will be integrated into the Munich Train Control Toolkit for the railway industry. <div>
arXiv:2308.02572v4 Announce Type: replace-cross 
Abstract: Railway networks have become increasingly important in recent times, especially in moving freight and public transportation from road traffic and planes to more environmentally friendly trains. Since expanding the global railway network is time- and resource-consuming, maximizing the rail capacity of the existing infrastructure is desirable. However, simply running more trains is infeasible as certain constraints enforced by the train control system must be satisfied. The capacity of a network depends (amongst others) on the distance between trains allowed by this safety system. While most signaling systems rely on fixed blocks defined by costly hardware, new specifications provided by Level 2 with Hybrid Train Detection of the European Train Control System (ETCS L2 HTD), formerly known as ETCS Hybrid Level 3, allow the usage of virtual subsections. This additional degree of freedom allows for shorter train following times and, thus, more trains on existing railway tracks. On the other hand, new design tasks arise on which automated methods might be helpful for designers of modern railway networks. However, although first approaches exist that solve design problems arising within ETCS L2 HTD, neither formal descriptions nor results on the computational complexity of the corresponding design tasks exist. In this paper, we fill this gap by providing a formal description of design tasks for ETCS L2 HTD and proof that these tasks are NP-complete or NP-hard, respectively. By that, we are providing a solid basis for the future development of methods to solve those tasks, which will be integrated into the Munich Train Control Toolkit available open-source on GitHub at https://github.com/cda-tum/mtct.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and efficient predictions of keyhole dynamics in laser materials processing using machine learning-aided simulations</title>
<link>https://arxiv.org/abs/2402.16190</link>
<guid>https://arxiv.org/abs/2402.16190</guid>
<content:encoded><![CDATA[
<div> machine learning, simulation, laser materials processing, keyhole phenomenon, defects

Summary:
- The study focuses on the keyhole phenomenon in laser materials processing, which leads to the formation of pores and affects product performance.
- Pores are associated with the dynamic behavior of the keyhole, making accurate characterization and prediction challenging.
- In situ characterization using synchrotron X-ray technique is informative but complex and costly.
- The developed machine learning-aided simulation method accurately predicts keyhole dynamics, particularly keyhole depth fluctuations, for a wide range of processing parameters.
- The method achieved a mean absolute percentage error of 10%, surpassing ray-tracing simulations with a 30% error margin and reducing computational time.
- This cost-effective and efficient model can serve as an alternative to synchrotron experiments, offering potential for defect elimination or reduction in various laser materials processing techniques. 

<br /><br />Summary: <div>
arXiv:2402.16190v2 Announce Type: replace-cross 
Abstract: The keyhole phenomenon has been widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing. Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies. The formation of these pores is typically associated with the dynamic behavior of the keyhole. So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time, has been a challenging task. In situ characterization of keyhole dynamic behavior using the synchrotron X-ray technique is informative but complicated and expensive. Current simulations are generally hindered by their poor accuracy and generalization abilities in predicting keyhole depths due to the lack of accurate laser absorptance data. In this study, we develop a machine learning-aided simulation method that accurately predicts keyhole dynamics, especially in keyhole depth fluctuations, over a wide range of processing parameters. In two case studies involving titanium and aluminum alloys, we achieve keyhole depth prediction with a mean absolute percentage error of 10%, surpassing those simulated using the ray-tracing method with an error margin of 30%, while also reducing computational time. This exceptional fidelity and efficiency empower our model to serve as a cost-effective alternative to synchrotron experiments. Our machine learning-aided simulation method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Structure of Financial Equity Research Reports -- Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4</title>
<link>https://arxiv.org/abs/2407.18327</link>
<guid>https://arxiv.org/abs/2407.18327</guid>
<content:encoded><![CDATA[
<div> Keywords: financial equity research reports, automation, language models, information extraction, question archetypes

Summary:
- The research aims to categorize the content of financial equity research reports (ERRs) by analyzing 72 reports sentence-by-sentence.
- 4940 sentences from ERRs were classified into 169 unique question archetypes without predefined questions, providing an unbiased view of their content.
- 78.7% of the questions in ERRs were found to be automatable, with 48.2% as text-extractable and 30.5% as database-extractable.
- Only 21.3% of questions required human judgment, suggesting a potential for automation in the ERR writing process.
- Empirical validation using advanced language models like Llama-3-70B and GPT-4-turbo-2024-04-09 confirmed the feasibility of automating approximately 80% of ERR content.
- The study highlights the potential benefits of introducing large language models in the ERR writing process to enhance quality and efficiency. 

<br /><br />Summary: This research analyzes financial equity research reports to identify question archetypes and determine the potential for automation in answering these questions. The study reveals that a significant percentage of questions in ERRs can be automated, with text-extractable and database-extractable questions being prominent. By leveraging advanced language models, such as Llama-3-70B and GPT-4-turbo-2024-04-09, the automation of ERR writing process is feasible, providing opportunities to improve quality and efficiency. <div>
arXiv:2407.18327v2 Announce Type: replace-cross 
Abstract: This research dissects financial equity research reports (ERRs) by mapping their content into categories. There is insufficient empirical analysis of the questions answered in ERRs. In particular, it is not understood how frequently certain information appears, what information is considered essential, and what information requires human judgment to distill into an ERR. The study analyzes 72 ERRs sentence-by-sentence, classifying their 4940 sentences into 169 unique question archetypes. We did not predefine the questions but derived them solely from the statements in the ERRs. This approach provides an unbiased view of the content of the observed ERRs. Subsequently, we used public corporate reports to classify the questions' potential for automation. Answers were labeled "text-extractable" if the answers to the question were accessible in corporate reports. 78.7% of the questions in ERRs can be automated. Those automatable question consist of 48.2% text-extractable (suited to processing by large language models, LLMs) and 30.5% database-extractable questions. Only 21.3% of questions require human judgment to answer. We empirically validate using Llama-3-70B and GPT-4-turbo-2024-04-09 that recent advances in language generation and information extraction enable the automation of approximately 80% of the statements in ERRs. Surprisingly, the models complement each other's strengths and weaknesses well. The research confirms that the current writing process of ERRs can likely benefit from additional automation, improving quality and efficiency. The research thus allows us to quantify the potential impacts of introducing large language models in the ERR writing process. The full question list, including the archetypes and their frequency, will be made available online after peer review.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemReservoir -- An Open-Source Framework for Chemically-Inspired Reservoir Computing</title>
<link>https://arxiv.org/abs/2506.04249</link>
<guid>https://arxiv.org/abs/2506.04249</guid>
<content:encoded><![CDATA[
<div> Keywords: reservoir computing, cheminformatics, open-source framework, chemically-inspired reservoirs, memory capacity tasks

Summary:
ChemReservoir is introduced as an open-source framework for chemically-inspired reservoir computing, addressing the limitations of previous studies focused on DNA chemistry. Unlike previous tools, ChemReservoir is a general framework for constructing and analyzing chemically-inspired reservoirs, ensuring enhanced testing, evaluation, and reproducibility. The tool was evaluated using various cycle-based reservoir topologies and showed stable performance across different configurations in memory capacity tasks. This framework allows for the development of reservoir models not limited to DNA chemistry, making it a versatile tool for cheminformatics research. By providing a user-friendly and accessible platform, ChemReservoir contributes to the advancement of reservoir computing in the field of cheminformatics. <br /><br />Summary: <div>
arXiv:2506.04249v1 Announce Type: new 
Abstract: Reservoir computing is a type of a recurrent neural network, mapping the inputs into higher dimensional space using fixed and nonlinear dynamical systems, called reservoirs. In the literature, there are various types of reservoirs ranging from in-silico to in-vitro. In cheminformatics, previous studies contributed to the field by developing simulation-based chemically inspired in-silico reservoir models. Yahiro used a DNA-based chemical reaction network as its reservoir and Nguyen developed a DNA chemistry-inspired tool based on Gillespie algorithm. However, these software tools were designed mainly with the focus on DNA chemistry and their maintenance status has limited their current usability. Due to these limitations, there was a need for a proper open-source tool. This study introduces ChemReservoir, an open-source framework for chemically-inspired reservoir computing. In contrast to the former studies focused on DNA-chemistry, ChemReservoir is a general framework for the construction and analysis of chemically-inspired reservoirs, which also addresses the limitations in these previous studies by ensuring enhanced testing, evaluation, and reproducibility. The tool was evaluated using various cycle-based reservoir topologies and demonstrated stable performance across a range of configurations in memory capacity tasks.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive recycled plastic architecture: Vacuum-Sealed Chainmail Structures Through Computational Design</title>
<link>https://arxiv.org/abs/2506.04660</link>
<guid>https://arxiv.org/abs/2506.04660</guid>
<content:encoded><![CDATA[
<div> Keywords: recycled plastics, construction industry, modular chainmail systems, sustainability, innovative applications<br />
<br />
Summary: This paper explores the use of recycled plastics as a primary construction material in modular chainmail systems. The research demonstrates the optimization of design, testing, and fabrication of vacuum-sealed chainmail structures made of recycled plastic filaments. The study identifies the rectangular chainmail configuration as the most efficient for architectural use, with superior deformation capacity, material efficiency, and load-bearing performance. Optimization strategies for temporary structures are also proposed to balance material savings, usable area, and water drainage efficiency. The findings suggest innovative applications for extreme conditions such as disaster-prone areas, high-altitude environments, underwater platforms, and extraterrestrial habitats, leveraging the properties of recycled plastics and modular chainmail systems. This research bridging waste management and high-performance design offers solutions for challenges in harsh and resource-constrained environments. <br /><br /> <div>
arXiv:2506.04660v1 Announce Type: new 
Abstract: The construction industry is a major consumer of raw materials, accounting for nearly half of global material usage annually, while generating significant waste that poses sustainability challenges. This paper explores the untapped potential of recycled plastics as a primary construction material, leveraging their lightweight, flexible, and customizable properties for advanced applications in modular chainmail systems. Through a computational workflow, the study optimizes the design, testing, and fabrication of vacuum-sealed chainmail structures composed of recycled plastic filaments, demonstrating their adaptability and structural performance for architectural use.
  Key contributions include a novel methodology for integrating recycled plastic filaments into chainmail geometries, validated through 2D sectional testing, 3D shell structure generation, and physical modeling under vacuum constraints. The research identifies the rectangular chainmail configuration as the most efficient and adaptable, achieving superior deformation capacity, material efficiency, and load-bearing performance. Optimization strategies for temporary structures highlight practical deployment potential, balancing material savings, usable area, and water drainage efficiency.
  The findings offer a foundation for innovative applications in extreme conditions, including disaster-prone areas, high-altitude environments, underwater platforms, and extraterrestrial habitats. These applications leverage the lightweight, adaptable, and durable properties of recycled plastics and modular chainmail systems, bridging the gap between waste management and high-performance design while addressing unique challenges in harsh and resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear elastodynamic material identification of heterogeneous isogeometric Bernoulli-Euler beams</title>
<link>https://arxiv.org/abs/2506.04960</link>
<guid>https://arxiv.org/abs/2506.04960</guid>
<content:encoded><![CDATA[
<div> Finite Element Model Updating, Material Identification, Isogeometric Formulation, Bernoulli-Euler Beams, Elastic Properties<br />
<br />
Summary: This paper presents a Finite Element Model Updating framework for identifying heterogeneous material distributions in planar Bernoulli-Euler beams. The process involves identifying elastic properties from quasi-static displacements and determining density from modal data. Three independent discretizations are used, including isogeometric finite element mesh, high-resolution experimental measurement grid, and material mesh with low-order Lagrange elements. The method minimizes errors between experiments and numerical model using local optimization with trust-region method. Results from numerical examples show effectiveness in handling large displacements and noise in experimental data. B2M1 discretization is used to alleviate membrane locking. Regularization ensures stable solutions for dense material meshes. The proposed framework can be extended to shells and 3D continua. <div>
arXiv:2506.04960v1 Announce Type: new 
Abstract: This paper presents a Finite Element Model Updating framework for identifying heterogeneous material distributions in planar Bernoulli-Euler beams based on a rotation-free isogeometric formulation. The procedure follows two steps: First, the elastic properties are identified from quasi-static displacements; then, the density is determined from modal data (low frequencies and mode shapes), given the previously obtained elastic properties. The identification relies on three independent discretizations: the isogeometric finite element mesh, a high-resolution grid of experimental measurements, and a material mesh composed of low-order Lagrange elements. The material mesh approximates the unknown material distributions, with its nodal values serving as design variables. The error between experiments and numerical model is expressed in a least squares manner. The objective is minimized using local optimization with the trust-region method, providing analytical derivatives to accelerate computations. Several numerical examples exhibiting large displacements are provided to test the proposed approach. To alleviate membrane locking, the B2M1 discretization is employed when necessary. Quasi-experimental data is generated using refined finite element models with random noise applied up to 4%. The method yields satisfactory results as long as a sufficient amount of experimental data is available, even for high measurement noise. Regularization is used to ensure a stable solution for dense material meshes. The density can be accurately reconstructed based on the previously identified elastic properties. The proposed framework can be straightforwardly extended to shells and 3D continua.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series Analysis</title>
<link>https://arxiv.org/abs/2506.05019</link>
<guid>https://arxiv.org/abs/2506.05019</guid>
<content:encoded><![CDATA[
<div> Dataset, financial news, structured tables, K-line charts, stock prices  
Summary:  
- The paper introduces FinMultiTime, a large-scale multimodal financial time series dataset that includes financial news, structured financial tables, K-line technical charts, and stock price time series.
- The dataset encompasses 5,105 stocks from the S&amp;P 500 and HS 300 universes, covering the period from 2009 to 2025 in the U.S. and China.
- It provides data at minute-level, daily, and quarterly resolutions, enhancing the capturing of short, medium, and long-term market signals with high fidelity.
- Experiments show that the dataset's scale and data quality significantly improve prediction accuracy.
- Multimodal fusion in Transformer models results in moderate performance gains.
<br /><br />Summary: <div>
arXiv:2506.05019v1 Announce Type: new 
Abstract: Pure time series forecasting tasks typically focus exclusively on numerical features; however, real-world financial decision-making demands the comparison and analysis of heterogeneous sources of information. Recent advances in deep learning and large scale language models (LLMs) have made significant strides in capturing sentiment and other qualitative signals, thereby enhancing the accuracy of financial time series predictions. Despite these advances, most existing datasets consist solely of price series and news text, are confined to a single market, and remain limited in scale. In this paper, we introduce FinMultiTime, the first large scale, multimodal financial time series dataset. FinMultiTime temporally aligns four distinct modalities financial news, structured financial tables, K-line technical charts, and stock price time series across both the S&amp;P 500 and HS 300 universes. Covering 5,105 stocks from 2009 to 2025 in the United States and China, the dataset totals 112.6 GB and provides minute-level, daily, and quarterly resolutions, thus capturing short, medium, and long term market signals with high fidelity. Our experiments demonstrate that (1) scale and data quality markedly boost prediction accuracy; (2) multimodal fusion yields moderate gains in Transformer models; and (3) a fully reproducible pipeline enables seamless dataset updates.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmark for Antibody Binding Affinity Maturation and Design</title>
<link>https://arxiv.org/abs/2506.04235</link>
<guid>https://arxiv.org/abs/2506.04235</guid>
<content:encoded><![CDATA[
<div> Benchmarking, Antibody binding affinity maturation, Design, AbBiBench, Protein models <br />
Summary: <br />
The article introduces AbBiBench, a benchmarking framework for evaluating antibody binding affinity maturation and design. Unlike current methods that focus on the antibody alone, AbBiBench considers the antibody-antigen complex as a functional unit. The framework includes 9 datasets with 9 antigens and 155,853 mutated antibodies, allowing for systematic comparison of 14 protein models. The correlation between model likelihood and experimental affinity values is used to assess model performance. In a case study involving increasing binding affinity of antibody F045-092 to influenza H1N1, structure-conditioned inverse folding models prove to be the most effective. AbBiBench offers a biologically grounded evaluation framework to enhance the development of antibody design models that are more effective and function-aware. <br /> <div>
arXiv:2506.04235v1 Announce Type: cross 
Abstract: We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking framework for antibody binding affinity maturation and design. Unlike existing antibody evaluation strategies that rely on antibody alone and its similarity to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench considers an antibody-antigen (Ab-Ag) complex as a functional unit and evaluates the potential of an antibody design binding to given antigen by measuring protein model's likelihood on the Ab-Ag complex. We first curate, standardize, and share 9 datasets containing 9 antigens (involving influenza, anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain mutated antibodies. Using these datasets, we systematically compare 14 protein models including masked language models, autoregressive language models, inverse folding models, diffusion-based generative models, and geometric graph models. The correlation between model likelihood and experimental affinity values is used to evaluate model performance. Additionally, in a case study to increase binding affinity of antibody F045-092 to antigen influenza H1N1, we evaluate the generative power of the top-performing models by sampling a set of new antibodies binding to the antigen and ranking them based on structural integrity and biophysical properties of the Ab-Ag complex. As a result, structure-conditioned inverse folding models outperform others in both affinity correlation and generation tasks. Overall, AbBiBench provides a unified, biologically grounded evaluation framework to facilitate the development of more effective, function-aware antibody design models.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding</title>
<link>https://arxiv.org/abs/2506.04353</link>
<guid>https://arxiv.org/abs/2506.04353</guid>
<content:encoded><![CDATA[
<div> Keywords: ReXVQA, visual question answering, chest radiology, multimodal large language models, AI performance

Summary: 
ReXVQA is a new benchmark for visual question answering in chest radiology, encompassing a vast dataset of questions paired with X-ray studies. It introduces diverse and authentic tasks reflecting various radiological reasoning skills. State-of-the-art multimodal large language models were evaluated, with MedGemma achieving the highest accuracy. A human reader study revealed that MedGemma outperformed radiology residents in chest X-ray interpretation, indicating a milestone where AI surpassed expert human evaluation. The study also highlighted distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists. ReXVQA establishes a standard for evaluating radiological AI systems, offering public leaderboards and detailed evaluation metrics. The dataset will be open-sourced, laying the groundwork for AI systems capable of expert-level clinical reasoning. 

<br /><br />Summary: <div>
arXiv:2506.04353v1 Announce Type: cross 
Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual question answering (VQA) in chest radiology, comprising approximately 696,000 questions paired with 160,000 chest X-rays studies across training, validation, and test sets. Unlike prior efforts that rely heavily on template based queries, ReXVQA introduces a diverse and clinically authentic task suite reflecting five core radiological reasoning skills: presence assessment, location analysis, negation detection, differential diagnosis, and geometric reasoning. We evaluate eight state-of-the-art multimodal large language models, including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge the gap between AI performance and clinical expertise, we conducted a comprehensive human reader study involving 3 radiology residents on 200 randomly sampled cases. Our evaluation demonstrates that MedGemma achieved superior performance (83.84% accuracy) compared to human readers (best radiology resident: 77.27%), representing a significant milestone where AI performance exceeds expert human evaluation on chest X-ray interpretation. The reader study reveals distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists while showing more variable agreement patterns between human readers and AI models. ReXVQA establishes a new standard for evaluating generalist radiological AI systems, offering public leaderboards, fine-grained evaluation splits, structured explanations, and category-level breakdowns. This benchmark lays the foundation for next-generation AI systems capable of mimicking expert-level clinical reasoning beyond narrow pathology classification. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor-based multivariate function approximation: methods benchmarking and comparison</title>
<link>https://arxiv.org/abs/2506.04791</link>
<guid>https://arxiv.org/abs/2506.04791</guid>
<content:encoded><![CDATA[
<div> evaluation, tensor-based multivariate function construction, approximation, machine learning, benchmark 

Summary: 
This note evaluates methods for tensor-based multivariate function construction and approximation, using a collection of functions with varying complexity. The performance, features, and user experience of each method are assessed based on accuracy, computational time, and parameter tuning impact. The goal is to provide a fair comparison of available strategies to guide users in understanding the process, advantages, and limitations of each tool. The note introduces a benchmark collection of tools for tensor approximation by surrogate models and gives explicit attention to the multivariate Loewner Framework approach. The detailed comparison allows readers to grasp the capabilities and limitations of each method, with examples provided for clarity. <div>
arXiv:2506.04791v1 Announce Type: cross 
Abstract: In this note, we evaluate the performances, the features and the user-experience of some methods (and their implementations) designed for tensor- (or data-) based multivariate function construction and approximation. To this aim, a collection of multivariate functions extracted from contributive works coming from different communities, is suggested. First, these functions with varying complexity (e.g. number and degree of the variables) and nature (e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to construct tensors, each of different dimension and size on the disk. Second, grounded on this tensor, we inspect performances of each considered method (e.g. the accuracy, the computational time, the parameters tuning impact, etc.). Finally, considering the "best" parameter tuning set, we compare each method using multiple evaluation criteria. The purpose of this note is not to rank the methods but rather to evaluate as fairly as possible the different available strategies, with the idea in mind to guide users to understand the process, the possibilities, the advantages and the limits brought by each tools. The contribution claimed is to suggest a complete benchmark collection of some available tools for tensor approximation by surrogate models (e.g. rational functions, networks, etc.). In addition, as contributors of the multivariate Loewner Framework (mLF) approach (and its side implementation in MDSPACK), attention and details of the latter are more explicitly given, in order to provide readers a digest of this contributive work and some details with simple examples.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Private Smart Wallet with Probabilistic Compliance</title>
<link>https://arxiv.org/abs/2506.04853</link>
<guid>https://arxiv.org/abs/2506.04853</guid>
<content:encoded><![CDATA[
<div> smart wallet, privacy-preserving, private onboarding mechanism, compliance checks, digital payments

Summary: 
The article presents a privacy-preserving smart wallet that incorporates a novel invitation-based private onboarding mechanism. It combines proof of innocence and ancestral commitment tracking systems for compliance with an authority party. Performance analysis reveals efficient private transfers with compliance checks completing in seconds on a standard laptop, and low proof generation. On-chain costs remain minimal for affordability on a Base layer 2 network. The smart wallet enables encrypted contact list management and transaction unlinkability for enhanced privacy. The evaluation confirms the effectiveness of the approach for compliance-aware digital payments, with reduced computational and financial burdens. <div>
arXiv:2506.04853v1 Announce Type: cross 
Abstract: We propose a privacy-preserving smart wallet with a novel invitation-based private onboarding mechanism. The solution integrates two levels of compliance in concert with an authority party: a proof of innocence mechanism and an ancestral commitment tracking system using bloom filters for probabilistic UTXO chain states. Performance analysis demonstrates practical efficiency: private transfers with compliance checks complete within seconds on a consumer-grade laptop, and overall with proof generation remaining low. On-chain costs stay minimal, ensuring affordability for all operations on Base layer 2 network. The wallet facilitates private contact list management through encrypted data blobs while maintaining transaction unlinkability. Our evaluation validates the approach's viability for privacy-preserving, compliance-aware digital payments with minimized computational and financial overhead.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRC20 Pinning Attack</title>
<link>https://arxiv.org/abs/2410.11295</link>
<guid>https://arxiv.org/abs/2410.11295</guid>
<content:encoded><![CDATA[
<div> tokens, Bitcoin network, security, attack vector, Binance

Summary:
BRC20 tokens are unique assets on the Bitcoin network that allow users to embed custom content within satoshis. Despite their growing market size, the transfer mechanism of BRC20 tokens has not been thoroughly examined for security vulnerabilities. A new attack vector, known as the BRC20 pinning attack, exploits the fee levels of bundled transactions to disrupt liquidity and withdrawal requests. This attack was successfully validated in collaboration with Binance researchers, resulting in a temporary suspension of withdrawals. The impact of the attack extends to a significant portion of inscription-based tokens in the Bitcoin ecosystem, highlighting the potential risks associated with BRC20 tokens and similar assets. Security measures and further analysis are necessary to mitigate the threat posed by such attacks. 

<br /><br />Summary: <div>
arXiv:2410.11295v3 Announce Type: replace-cross 
Abstract: BRC20 tokens are a type of non-fungible asset on the Bitcoin network. They allow users to embed customised content within Bitcoin's satoshis. The token frenzy reached a market size of US\$2.811\,b (2023Q3--2025Q1). However, this intuitive design has not undergone serious security scrutiny.
  We present the first analysis of BRC20's \emph{transfer} mechanism and identify a new attack vector. A typical BRC20 transfer involves two "bundled" on-chain transactions with different fee levels: the first (i.e., \textbf{Tx1}) with a lower fee inscribes the \textsf{transfer} request, while the second (i.e., \textbf{Tx2}) with a higher fee finalizes the actual transfer. An adversary can send a manipulated fee transaction (falling between the two fee levels), which causes \textbf{Tx1} to be processed while \textbf{Tx2} is pinned in the mempool. This locks BRC20 liquidity and disrupts normal withdrawal requests from users. We term this the \emph{BRC20 pinning attack}.
  We validated the attack in real-world settings in collaboration with Binance researchers. With their knowledge and permission, we conducted a controlled test against Binance's ORDI hot wallet, resulting in a temporary suspension of ORDI withdrawals for 3.5 hours. Recovery was performed shortly after. Further analysis confirms that the attack can be applied to over \textbf{90\%} of inscription-based tokens within the Bitcoin ecosystem.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.08435</link>
<guid>https://arxiv.org/abs/2412.08435</guid>
<content:encoded><![CDATA[
<div> Proactive Adaptation, Concept Drift Estimation, Forecast Model, Online Learning, Time Series<br />
<br />
Summary: Proceed is a proactive model adaptation framework designed for online time series forecasting. It addresses the challenge of concept drift by estimating and adapting to changes between training and test samples. By utilizing an adaptation generator, Proceed translates estimated drift into parameter adjustments, enhancing model performance. Trained on diverse synthetic concept drifts, Proceed demonstrates improved resilience against concept drift compared to existing online learning methods. Extensive experiments on real-world datasets confirm the effectiveness of Proceed in enhancing forecast model generalization capability and performance. The code for Proceed is available on GitHub for further exploration and application in time series forecasting tasks. <br /><br /> <div>
arXiv:2412.08435v4 Announce Type: replace-cross 
Abstract: Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Functional Materials Design with Optimal Initial Data in Surrogate-Based Active Learning</title>
<link>https://arxiv.org/abs/2506.03329</link>
<guid>https://arxiv.org/abs/2506.03329</guid>
<content:encoded><![CDATA[
<div> Keywords: functional materials, optimization, data-driven algorithms, surrogate-based active learning, quantum computing

Summary: 
This study focuses on optimizing functional materials through data-driven algorithms, which efficiently explore complex design spaces by learning relationships between material structures and performance metrics. Surrogate-based active learning, coupled with quantum computing, is highlighted as a cost-effective approach for material optimization. The use of a special surrogate model called quadratic unconstrained binary optimization is emphasized. The research investigates the impact of initial data sizes on optimization efficiency, showing that adequate initial data is crucial for achieving fast convergence and reducing computational costs. Averaged piecewise linear regression is used to identify the optimal initiation points for convergence, emphasizing the importance of proper initial data in efficient optimization of functional materials. This work contributes to improving optimization processes for functional materials by ensuring faster convergence and reduced computational expenses in surrogate-based active learning. 

<br /><br />Summary: <div>
arXiv:2506.03329v1 Announce Type: new 
Abstract: The optimization of functional materials is important to enhance their properties, but their complex geometries pose great challenges to optimization. Data-driven algorithms efficiently navigate such complex design spaces by learning relationships between material structures and performance metrics to discover high-performance functional materials. Surrogate-based active learning, continually improving its surrogate model by iteratively including high-quality data points, has emerged as a cost-effective data-driven approach. Furthermore, it can be coupled with quantum computing to enhance optimization processes, especially when paired with a special form of surrogate model ($i.e.$, quadratic unconstrained binary optimization), formulated by factorization machine. However, current practices often overlook the variability in design space sizes when determining the initial data size for optimization. In this work, we investigate the optimal initial data sizes required for efficient convergence across various design space sizes. By employing averaged piecewise linear regression, we identify initiation points where convergence begins, highlighting the crucial role of employing adequate initial data in achieving efficient optimization. These results contribute to the efficient optimization of functional materials by ensuring faster convergence and reducing computational costs in surrogate-based active learning.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the robustness of Dirichlet-Neumann coupling schemes for fluid-structure-interaction problems with nearly-closed fluid domains</title>
<link>https://arxiv.org/abs/2506.04027</link>
<guid>https://arxiv.org/abs/2506.04027</guid>
<content:encoded><![CDATA[
<div> Dirichlet-Neumann split, incompressible fluid, added-mass effect, flow resistance, nearly-closed fluid-domain<br />
<br />
Summary:<br />
Partitioned methods for fluid-structure interaction (FSI) typically use a Dirichlet-Neumann (DN) split for interface conditions. However, for nearly-closed fluid domains with incompressible fluids and Robin conditions, the DN scheme can become unstable due to increasing flow resistance. Convergence deteriorates as resistance increases, leading to instability at high resistances. This instability is linked to an added-damping effect, affecting the convergence rate of the partitioned method. Understanding this effect can improve the robustness and efficiency of FSI simulations, especially for applications like valves. The analysis also sheds light on the incompressibility dilemma for FSI problems with nearly closed fluid domains. Numerical experiments confirm these findings in more complex scenarios, highlighting the challenges and potential solutions for such FSI problems. <div>
arXiv:2506.04027v1 Announce Type: new 
Abstract: Partitioned methods for fluid-structure interaction (FSI) involve solving the structural and flow problems sequentially. These methods allow for separate settings for the fluid and solid subsystems and thus modularity, enabling reuse of advanced commercial and open-source software. Most partitioned FSI schemes apply a Dirichlet-Neumann (DN) split of the interface conditions. The DN scheme is adequate in a wide range of applications, but it is sensitive to the added-mass effect, and it is susceptible to the incompressibility dilemma, i.e. it completely fails for FSI problems with an incompressible fluid furnished with Dirichlet boundary conditions on the part of its boundary complementary to the interface. In this paper, we show that if the fluid is incompressible and the fluid domain is nearly-closed, i.e. it carries Dirichlet conditions except for a permeable part of the boundary carrying a Robin condition, then the DN partitioned approach is sensitive to the flow resistance at the permeable part, and convergence of the partitioned approach deteriorates as the flow resistance increases. The DN scheme then becomes unstable in the limit as the flow resistance passes to infinity. Based on a simple model problem, we show that in the nearly-closed case, the convergence rate of the DN partitioned method depends on a so-called added-damping effect. The analysis gives insights that can aid to improve robustness and efficiency of partitioned method for FSI problems with contact, e.g. valve applications. In addition, the results elucidate the incompressibility dilemma as a limit of the added-damping effect passing to infinity, and the corresponding challenges related to FSI problems with nearly closed fluid-domain configurations. Via numerical experiments, we consider the generalization of the results of the simple model problem to more complex nearly-closed FSI problems.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk and Reward of Transitioning from a National to a Zonal Electricity Market in Great Britain</title>
<link>https://arxiv.org/abs/2506.04107</link>
<guid>https://arxiv.org/abs/2506.04107</guid>
<content:encoded><![CDATA[
<div> consumer savings, producer surplus impacts, socioeconomic benefits, zonal market, electricity market  

Summary:  
The study evaluates the potential benefits of transitioning from a single-price national wholesale market to a zonal market design in Great Britain. Using an open-source electricity market model, the analysis shows that a six-zone market could result in significant consumer savings of around 9.4/MWh annually, totaling over 2.3 billion per year. However, generators in northern regions may experience revenue reductions of 30-40%. Policy interventions could mitigate these negative impacts, allowing for up to 97% restoration of national market revenues for affected units while still preserving around 3.1/MWh in consumer savings. The current system could achieve an annual welfare gain of 380-770 million through operational efficiency improvements alone during 2022-2024, with potential annual benefits exceeding 1-2 billion beyond 2029. These benefits outweigh potential downsides associated with increased capital costs. <div>
arXiv:2506.04107v1 Announce Type: cross 
Abstract: More spatially granular electricity wholesale markets promise more efficient operation and better asset siting in highly renewable power systems. Great Britain is considering moving from its current single-price national wholesale market to a zonal design. Existing studies reach varying and difficult-to-reconcile conclusions about the desirability of a zonal market in GB, partly because they rely on models that vary in their transparency and assumptions about future power systems. Using a novel open-source electricity market model, calibrated to match observed network behaviour, this article quantifies consumer savings, unit-level producer surplus impacts, and broader socioeconomic benefits that would have arisen had a six-zone market operated in Great Britain during 2022-2024. In the absence of mitigating policies, it is estimated that during those three years GB consumers would save approximately {\pounds}9.4/MWh (equalling an average of more than {\pounds}2.3B per year), but generators in northern regions would experience revenue reductions of 30-40\%. Policy interventions can restore these units' national market revenues to up to 97\% while still preserving around {\pounds}3.1/MWh in consumer savings (about {\pounds}750M per year). It is further estimated that the current system could achieve approximately {\pounds}380-{\pounds}770 million in annual welfare gain during 2022-2024 through improved operational efficiency alone. The drivers behind these benefits, notably wind curtailment volumes, are expected to become more pronounced towards 2030, suggesting that purely operationally achieved annual benefits of around {\pounds}1-2 billion beyond 2029 are likely. It is found that the scale of these benefits would outweigh the potential downsides related to increases in the cost of capital that have been estimated elsewhere.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.04171</link>
<guid>https://arxiv.org/abs/2506.04171</guid>
<content:encoded><![CDATA[
<div> Flow-Based Generative Models, Physics-Constrained Inference, Partial Differential Equations, Constraint Satisfaction, Zero-Shot Inference<br />
Summary:<br />
The article introduces Physics-Constrained Flow Matching (PCFM), a method for enforcing nonlinear constraints in pretrained flow-based generative models. Existing methods struggle to enforce physical constraints effectively, but PCFM addresses this by guiding the sampling process with physics-based corrections while maintaining alignment with learned flow. The framework outperforms both unconstrained and constrained baselines on various PDEs, including those with shocks and sharp features. PCFM ensures exact satisfaction of constraints at the final solution. This approach presents a general framework for enforcing hard constraints in scientific and general-purpose generative models, particularly valuable in applications where constraint satisfaction is critical. <br /><br />Summary: <div>
arXiv:2506.04171v1 Announce Type: cross 
Abstract: Deep generative models have recently been applied to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty-aware inference. However, enforcing physical constraints, such as conservation laws (linear and nonlinear) and physical consistencies, remains challenging. Existing methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints. In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints. Empirically, PCFM outperforms both unconstrained and constrained baselines on a range of PDEs, including those with shocks, discontinuities, and sharp features, while ensuring exact constraint satisfaction at the final solution. Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models, especially in applications where constraint satisfaction is essential.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Financial Foundation Models (MFFMs): Progress, Prospects, and Challenges</title>
<link>https://arxiv.org/abs/2506.01973</link>
<guid>https://arxiv.org/abs/2506.01973</guid>
<content:encoded><![CDATA[
<div> Keywords: FinLLMs, MFFMs, financial data, multimodal, research

Summary:
Financial Large Language Models (FinLLMs) and Multimodal Financial Foundation Models (MFFMs) are revolutionizing the analysis of financial data. While FinLLMs focus on language-centric approaches, MFFMs can process a wide range of multimodal financial data, offering a more comprehensive understanding of complex financial tasks. The progress and potential of MFFMs were discussed in a position paper presented at the MFFM Workshop at the ACM International Conference on AI in Finance 2024. Ongoing research on FinAgents at the SecureFinAI Lab at Columbia University aims to further explore the capabilities of MFFMs. By leveraging diverse data sources such as fundamental data, market data, and alternative data, MFFMs have the potential to streamline financial operations and investment processes. The Github repository for MFFMs provides a platform for collaboration and development in this emerging field. <div>
arXiv:2506.01973v1 Announce Type: new 
Abstract: Financial Large Language Models (FinLLMs), such as open FinGPT and proprietary BloombergGPT, have demonstrated great potential in select areas of financial services. Beyond this earlier language-centric approach, Multimodal Financial Foundation Models (MFFMs) can digest interleaved multimodal financial data, including fundamental data, market data, data analytics, macroeconomic, and alternative data (e.g., natural language, audio, images, and video). In this position paper, presented at the MFFM Workshop joined with ACM International Conference on AI in Finance (ICAIF) 2024, we describe the progress, prospects, and challenges of MFFMs. This paper also highlights ongoing research on FinAgents in the \textbf{SecureFinAI Lab}\footnote{\https://openfin.engineering.columbia.edu/} at Columbia University. We believe that MFFMs will enable a deeper understanding of the underlying complexity associated with numerous financial tasks and data, streamlining the operation of financial services and investment processes. Github Repo https://github.com/Open-Finance-Lab/Awesome-MFFMs/.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Polymer Property Predictions</title>
<link>https://arxiv.org/abs/2506.02129</link>
<guid>https://arxiv.org/abs/2506.02129</guid>
<content:encoded><![CDATA[
<div> Machine learning, polymer informatics, large language models, thermal properties, molecular embeddings
Summary: 
- The study explores the use of large language models (LLMs) in predicting key thermal properties in polymer science, comparing them to traditional fingerprinting-based methods.
- LLaMA-3 outperforms GPT-3.5 in predictive accuracy, likely due to its open-source architecture.
- Single-task learning proves more effective than multi-task learning, as LLMs struggle with capturing cross-property correlations.
- Analysis of molecular embeddings shows limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features.
- The findings provide guidance on selecting LLMs for polymer informatics, highlighting the interplay between molecular embeddings and natural language processing. 

<br /><br />Summary: <div>
arXiv:2506.02129v1 Announce Type: new 
Abstract: Machine learning has revolutionized polymer science by enabling rapid property prediction and generative design. Large language models (LLMs) offer further opportunities in polymer informatics by simplifying workflows that traditionally rely on large labeled datasets, handcrafted representations, and complex feature engineering. LLMs leverage natural language inputs through transfer learning, eliminating the need for explicit fingerprinting and streamlining training. In this study, we finetune general purpose LLMs -- open-source LLaMA-3-8B and commercial GPT-3.5 -- on a curated dataset of 11,740 entries to predict key thermal properties: glass transition, melting, and decomposition temperatures. Using parameter-efficient fine-tuning and hyperparameter optimization, we benchmark these models against traditional fingerprinting-based approaches -- Polymer Genome, polyGNN, and polyBERT -- under single-task (ST) and multi-task (MT) learning. We find that while LLM-based methods approach traditional models in performance, they generally underperform in predictive accuracy and efficiency. LLaMA-3 consistently outperforms GPT-3.5, likely due to its tunable open-source architecture. Additionally, ST learning proves more effective than MT, as LLMs struggle to capture cross-property correlations, a key strength of traditional methods. Analysis of molecular embeddings reveals limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features and domain-specific embeddings. These findings provide insight into the interplay between molecular embeddings and natural language processing, guiding LLM selection for polymer informatics.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singularity Blockchain Key Management via non-custodial key management</title>
<link>https://arxiv.org/abs/2506.02282</link>
<guid>https://arxiv.org/abs/2506.02282</guid>
<content:encoded><![CDATA[
<div> web3 wallets, user identity, blockchain, key management, non-custodial<br />
<br />
Summary: <br />
Web3 wallets play a crucial role in managing user identity on the blockchain by storing and providing access to private keys. Key management schemes can be either custodial or non-custodial, with the latter placing the burden of key storage and recovery on the user. Existing non-custodial schemes often require users to remember seed phrases, leading to onboarding challenges and the risk of asset loss if the key is forgotten. This paper introduces a novel non-custodial key management approach that allows users to back up and recover their private key using third-party sign-in methods such as google-oAuth. By enabling users to securely backup their keys through independent authentication methods, this technique aims to enhance user experience and reduce the likelihood of key loss. <div>
arXiv:2506.02282v1 Announce Type: new 
Abstract: web3 wallets are key to managing user identity on blockchain. The main purpose of a web3 wallet application is to manage the private key for the user and provide an interface to interact with the blockchain. The key management scheme ( KMS ) used by the wallet to store and recover the private key can be either custodial, where the keys are permissioned and in custody of the wallet provider or noncustodial where the keys are in custody of the user. The existing non-custodial key management schemes tend to offset the burden of storing and recovering the key entirely on the user by asking them to remember seed-phrases. This creates onboarding hassles for the user and introduces the risk that the user may lose their assets if they forget or lose their seedphrase/private key. In this paper, we propose a novel method of backing up user keys using a non-custodial key management technique that allows users to save and recover a backup of their private key using any independent sign-in method such as google-oAuth or other 3P oAuth.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the fracture mechanics validity of small scale tests</title>
<link>https://arxiv.org/abs/2506.02538</link>
<guid>https://arxiv.org/abs/2506.02538</guid>
<content:encoded><![CDATA[
<div> Keywords: fracture behaviour, micro-scale mechanical tests, crack growth, material properties, hydrogen embrittlement

Summary: 
This study focuses on conducting small-scale tests to understand the fracture behavior of materials. Using numerical and semi-analytical approaches, the researchers determine the conditions necessary for a valid and quantitative fracture experiment, considering factors such as sample geometry, material properties, and crack lengths. They establish the maximum value of the J-integral, known as Jmax, where fracture must occur for accurate results. Maps are generated to show the maximum valid J value as a function of yield strength, strain hardening, and sample size, providing guidance for conducting experiments. The analysis is extended to metals embrittled by hydrogen exposure, overlaying the response of these materials on the established maps to determine the conditions required for obtaining quantitative insight into such materials. <div>
arXiv:2506.02538v1 Announce Type: new 
Abstract: There is growing interest in conducting small-scale tests to gain additional insight into the fracture behaviour of components across a wide range of materials. For example, micro-scale mechanical tests inside of a microscope (\emph{in situ}) enable direct, high-resolution observation of the interplay between crack growth and microstructural phenomena (e.g., dislocation behaviour or the fracture resistance of a particular interface), and sub-size samples are increasingly used when only a limited amount of material is available. However, to obtain quantitative insight and extract relevant fracture parameters, the sample must be sufficiently large for a $J$- (HRR) or a $K$-field to exist. We conduct numerical and semi-analytical studies to map the conditions (sample geometry, material) that result in a valid, quantitative fracture experiment. Specifically, for a wide range of material properties, crack lengths and sample dimensions, we establish the maximum value of the $J$-integral where an HRR field ceases to exist (i.e., the maximum $J$ value at which fracture must occur for the test to be valid, $J_\mathrm{max}$). Maps are generated to establish the maximum valid $J$ value ($J_\mathrm{max}$) as a function of yield strength, strain hardening and minimum sample size. These maps are then used to discuss the existing experimental literature and provide guidance on how to conduct quantitative experiments. Finally, our study is particularised to the analysis of metals that have been embrittled due to hydrogen exposure. The response of relevant materials under hydrogen-containing environments are superimposed on the aforementioned maps, determining the conditions that will enable quantitative insight.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching Location Representation with Detailed Semantic Information</title>
<link>https://arxiv.org/abs/2506.02744</link>
<guid>https://arxiv.org/abs/2506.02744</guid>
<content:encoded><![CDATA[
<div> Embeddings, Urban Modeling, Contrastive Learning, Point-of-Interest, Multimodal<br />
<br />
Summary: 
The study introduces CaLLiPer+, an urban modeling approach that integrates Point-of-Interest names with categorical labels in a contrastive learning framework. The model shows improved performance in land use classification and socioeconomic status mapping compared to baseline methods, with gains of 4% to 11%. By incorporating POI names, the model enhances location retrieval and captures complex urban concepts accurately. Ablation studies demonstrate the complementary role of POI names and the benefits of using pretrained text encoders for spatial representations. The research underscores the significance of integrating fine-grained semantic attributes and multimodal learning techniques for advancing urban foundation models. <div>
arXiv:2506.02744v1 Announce Type: new 
Abstract: Spatial representations that capture both structural and semantic characteristics of urban environments are essential for urban modeling. Traditional spatial embeddings often prioritize spatial proximity while underutilizing fine-grained contextual information from places. To address this limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that systematically integrates Point-of-Interest (POI) names alongside categorical labels within a multimodal contrastive learning framework. We evaluate its effectiveness on two downstream tasks, land use classification and socioeconomic status distribution mapping, demonstrating consistent performance gains of 4% to 11% over baseline methods. Additionally, we show that incorporating POI names enhances location retrieval, enabling models to capture complex urban concepts with greater precision. Ablation studies further reveal the complementary role of POI names and the advantages of leveraging pretrained text encoders for spatial representations. Overall, our findings highlight the potential of integrating fine-grained semantic attributes and multimodal learning techniques to advance the development of urban foundation models.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Aware Density Estimation in Multiple Dimensions</title>
<link>https://arxiv.org/abs/2506.02323</link>
<guid>https://arxiv.org/abs/2506.02323</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization, probability densities, multidimensional problems, splines, PET rebinning  
Summary:  
- The article presents an optimization problem to estimate probability densities in multidimensional problems with uneven sampling probability.  
- Detector sensitivity is considered as an heterogeneous density, utilizing splines on a grid for computational speed and flexible boundary conditions.  
- The method uses nuclear norm regularization on the spline's Hessian to promote sparsity, making it spatially adaptive and stable against the choice of regularization parameter.  
- The computational pipeline is tested on standard densities, with provided software for implementation.  
- A new approach to PET rebinning is showcased as an application of the framework.<br /><br />Summary: <div>
arXiv:2506.02323v1 Announce Type: cross 
Abstract: We formulate an optimization problem to estimate probability densities in the context of multidimensional problems that are sampled with uneven probability. It considers detector sensitivity as an heterogeneous density and takes advantage of the computational speed and flexible boundary conditions offered by splines on a grid. We choose to regularize the Hessian of the spline via the nuclear norm to promote sparsity. As a result, the method is spatially adaptive and stable against the choice of the regularization parameter, which plays the role of the bandwidth. We test our computational pipeline on standard densities and provide software. We also present a new approach to PET rebinning as an application of our framework.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning</title>
<link>https://arxiv.org/abs/2506.02485</link>
<guid>https://arxiv.org/abs/2506.02485</guid>
<content:encoded><![CDATA[
<div> Generative AI, Wildfire prediction, Multimodal approaches, 2D fire spread forecasting, 3D simulations 

Summary:
Generative AI models like GANs and VAEs show promise in improving wildfire prediction by integrating multimodal data and generating diverse scenarios. These models can enhance 2D fire spread forecasting and enable more realistic 3D simulations. A human-AI collaboration framework using large language models aids in automated knowledge extraction and literature synthesis. Five key visions for integrating generative AI into wildfire management include multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. The challenges of implementing these visions are also addressed, with proposed solutions to overcome them.<br /><br />Summary: <div>
arXiv:2506.02485v1 Announce Type: cross 
Abstract: Wildfires continue to inflict devastating human, environmental, and economic losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and the urgent demand for more effective response strategies. While physics-based and deep learning models have advanced wildfire simulation, they face critical limitations in predicting and visualizing multimodal fire spread in real time, particularly in both 2D and 3D spatial domains using dynamically updated GIS data. These limitations hinder timely emergency response, infrastructure protection, and community safety. Generative AI has recently emerged as a transformative approach across research and industry. Models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and diffusion-based architectures offer distinct advantages over traditional methods, including the integration of multimodal data, generation of diverse scenarios under uncertainty, and improved modeling of wildfire dynamics across spatial and temporal scales. This position paper advocates for the adoption of generative AI as a foundational framework for wildfire prediction. We explore how such models can enhance 2D fire spread forecasting and enable more realistic, scalable 3D simulations. Additionally, we employ a novel human-AI collaboration framework using large language models (LLMs) for automated knowledge extraction, literature synthesis, and bibliometric mapping. Looking ahead, we identify five key visions for integrating generative AI into wildfire management: multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. We also address three major challenges accompanying these opportunities and propose potential solutions to support their implementation.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression</title>
<link>https://arxiv.org/abs/2506.02678</link>
<guid>https://arxiv.org/abs/2506.02678</guid>
<content:encoded><![CDATA[
<div> Dynamic ratio-based training, Large Language Models, efficient language reasoning, inference, System-1, System-2 <br />
<br />
Dynamic ratio-based training is proposed in this work to improve the efficiency of language reasoning in large language models. The method continuously balances the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. The approach is validated on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B models across a variety of benchmarks with varying difficulty levels. Results show a reduction of nearly 40% in the number of output tokens while maintaining reasoning accuracy. The research presents an innovative solution to the challenge of performing efficient language reasoning, particularly during inference with long outputs, without the need for sophisticated data annotations or interpolation between models. Code and data for the research will be made available soon. <br /><br />Summary: <div>
arXiv:2506.02678v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mesoscale phase-field model of intergranular liquid lithium corrosion of ferritic/martensitic steels</title>
<link>https://arxiv.org/abs/2506.02776</link>
<guid>https://arxiv.org/abs/2506.02776</guid>
<content:encoded><![CDATA[
<div> phase-field model, intergranular corrosion, ferritic/martensitic steels, liquid lithium, chromium concentration

Summary:
The article presents a phase-field model for simulating intergranular corrosion in ferritic/martensitic steels exposed to liquid lithium. By tracking the chromium concentration in the material, mass transport within the metal and liquid phases is analyzed. The model effectively captures intergranular corrosion by enhancing chromium diffusion along grain boundaries without the need for specific treatment. Results from simulations align closely with experimental measurements of weight loss and corrosion depth in a 9 wt% Cr steel at 600C. A sensitivity analysis reveals the influence of microstructural factors such as near-surface grain density and grain size on the corrosion process. The study also evaluates the impact of saturation on corrosion behavior. Overall, near-surface grain density is identified as a critical factor, while grain size is found to influence susceptibility to intergranular corrosion. <div>
arXiv:2506.02776v1 Announce Type: cross 
Abstract: A phase-field model is developed to simulate intergranular corrosion of ferritic/martensitic steels exposed to liquid lithium. The chromium concentration of the material is used to track the mass transport within the metal and liquid (corrosive) phase. The framework naturally captures intergranular corrosion by enhancing the diffusion of chromium along grain boundaries relative to the grain bulk with no special treatment for the corrosion front evolution. The formulation applies to arbitrary 2D and 3D polycrystalline geometries. The framework reproduces experimental measurements of weight loss and corrosion depth for a 9 wt\% Cr ferritic/martensitic steel exposed to static lithium at 600 $^\circ$C. A sensitivity analysis, varying near-surface grain density, grain size, and chromium depletion thickness, highlights the microstructural influence in the corrosion process. Moreover, the significance of saturation is considered and evaluated. Simulation results show that near-surface grain density is a deciding factor, whereas grain size dictates the susceptibility to intergranular corrosion.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.02911</link>
<guid>https://arxiv.org/abs/2506.02911</guid>
<content:encoded><![CDATA[
<div> cell type annotation, single-cell RNA sequencing data, CellPuzzles, large language models, batch-level accuracy

Summary:
Cell type annotation plays a critical role in analyzing single-cell RNA sequencing data heterogeneity. Current foundation models lack the ability to consider batch-level cellular context and provide explanatory reasoning in cell type annotation tasks. To address this, the CellPuzzles task was introduced to mimic expert annotation workflows, requiring unique cell type assignment across batches of cells. Existing large language models struggle with this task, with limited accuracy. In response, Cell-o1, a 7B large language model, was developed through supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 surpasses previous models, demonstrating superior performance and generalization across different contexts. The training dynamics and reasoning behaviors of Cell-o1 provide insights into improved batch-level annotation performance and expert-like reasoning strategies. <div>
arXiv:2506.02911v1 Announce Type: cross 
Abstract: Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to optimize convex risk measures: The cases of utility-based shortfall risk and optimized certainty equivalent risk</title>
<link>https://arxiv.org/abs/2506.01101</link>
<guid>https://arxiv.org/abs/2506.01101</guid>
<content:encoded><![CDATA[
<div> risk measures, estimation, optimization, gradient estimators, stochastic gradient algorithm

Summary: 
The article introduces the estimation and optimization of convex risk measures, specifically utility-based shortfall risk (UBSR) and Optimized Certainty Equivalent (OCE) risk, covering unbounded random variables. It extends various risk measures like entropic risk and Value-at-Risk. Non-asymptotic bounds are derived for mean absolute error and mean-squared error in estimation using sample average approximation (SAA) estimators. Expressions for UBSR and OCE gradients under smooth parameterization are provided, with gradient estimators proposed using the SAA estimator of UBSR and non-asymptotic bounds on error. A stochastic gradient algorithm is developed for optimization using these gradient estimators. Non-asymptotic convergence rate bounds are derived for the optimization of UBSR and OCE risk measures. <div>
arXiv:2506.01101v1 Announce Type: new 
Abstract: We consider the problems of estimation and optimization of two popular convex risk mea- sures: utility-based shortfall risk (UBSR) and Optimized Certainty Equivalent (OCE) risk. We extend these risk measures to cover possibly unbounded random variables. We cover prominent risk measures like the entropic risk, expectile risk, monotone mean-variance risk, Value-at-Risk, and Conditional Value-at-Risk as few special cases of either the UBSR or the OCE risk. In the context of estimation, we derive non-asymptotic bounds on the mean absolute error (MAE) and mean-squared error (MSE) of the classical sample average approximation (SAA) estimators of both, the UBSR and the OCE. Next, in the context of optimization, we derive expressions for the UBSR gradient and the OCE gradient under a smooth parameterization. Utilizing these expres- sions, we propose gradient estimators for both, the UBSR and the OCE. We use the SAA estimator of UBSR in both these gradient estimators, and derive non-asymptotic bounds on MAE and MSE for the proposed gradient estimation schemes. We incorporate the aforementioned gradient estima- tors into a stochastic gradient (SG) algorithm for optimization. Finally, we derive non-asymptotic bounds that quantify the rate of convergence of our SG algorithm for the optimization of the UBSR and the OCE risk measure
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast numerical generation of Lie closure</title>
<link>https://arxiv.org/abs/2506.01120</link>
<guid>https://arxiv.org/abs/2506.01120</guid>
<content:encoded><![CDATA[
<div> Keywords: Lie algebra, matrix, numerical construction, quantum computing, linear independence<br />
Summary:<br />
The article discusses the importance of finding the Lie-algebraic closure of matrices in quantum computing and quantum control. Analytically determining the closure is challenging for most cases, leading to a need for numerical construction. The standard algorithm for this construction relies on a subroutine to check linear independence, which can be computationally intensive. The authors present efficient methods for linear independence checks that reduce computational complexity and memory usage. One of these methods is implemented and validated against known results. These new algorithms allow for the exploration of Lie closure in larger system sizes that were previously unattainable, opening up possibilities for numerical studies in quantum computing and control applications. <div>
arXiv:2506.01120v1 Announce Type: new 
Abstract: Finding the Lie-algebraic closure of a handful of matrices has important applications in quantum computing and quantum control. For most realistic cases, the closure cannot be determined analytically, necessitating an explicit numerical construction. The standard construction algorithm makes repeated calls to a subroutine that determines whether a matrix is linearly independent from a potentially large set of matrices. Because the common implementation of this subroutine has a high complexity, the construction of Lie closure is practically limited to trivially small matrix sizes. We present efficient alternative methods of linear independence check that simultaneously reduce the computational complexity and memory footprint. An implementation of one of the methods is validated against known results. Our new algorithms enable numerical studies of Lie closure in larger system sizes than was previously possible.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging ML-AI Techniques for Analog and RF EDA</title>
<link>https://arxiv.org/abs/2506.00007</link>
<guid>https://arxiv.org/abs/2506.00007</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, EDA workflows, analog design, RF circuits, optimization techniques

Summary: 
This survey delves into the integration of machine learning (ML) into electronic design automation (EDA) workflows specifically tailored for analog and RF circuits. The challenges unique to analog design, such as complex constraints, nonlinear design spaces, and high computational costs, are addressed. The review encompasses state-of-the-art ML and optimization techniques for various circuit tasks, including constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey emphasizes how ML can enhance automation, elevate design quality, and reduce time-to-market while meeting desired circuit specifications. In addition, emerging trends and cross-cutting challenges, like robustness to variations and considerations of interconnect parasitics, are explored. Overall, the survey underscores the potential of ML to revolutionize analog and RF circuit design by optimizing workflow efficiency and achieving superior design outcomes. 

<br /><br />Summary: <div>
arXiv:2506.00007v1 Announce Type: cross 
Abstract: This survey explores the integration of machine learning (ML) into EDA workflows for analog and RF circuits, addressing challenges unique to analog design, which include complex constraints, nonlinear design spaces, and high computational costs. State-of-the-art learning and optimization techniques are reviewed for circuit tasks such as constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey highlights the capability of ML to enhance automation, improve design quality, and reduce time-to-market while meeting the target specifications of an analog or RF circuit. Emerging trends and cross-cutting challenges, including robustness to variations and considerations of interconnect parasitics, are also discussed.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Vessel Behavior using AIS Trajectory Data and Markovian Models in the Gulf of St. Lawrence</title>
<link>https://arxiv.org/abs/2506.00025</link>
<guid>https://arxiv.org/abs/2506.00025</guid>
<content:encoded><![CDATA[
<div> Keywords: maritime mobility, spatio-temporal analysis, vessel movement patterns, discrete-time Markov chains, COVID-19 pandemic <br />
Summary: <br />
This article presents a spatio-temporal analytical framework using discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, focusing on changes during the COVID-19 pandemic. The ocean space is divided into hexagonal cells, and mobility signatures for different vessel types are constructed based on cell transitions and dwell time. Origin-destination matrices and spatial transition probability models are developed to understand vessel dynamics at different time scales. The study reveals consistent mobility signatures for specific vessel types across different regions, suggesting underlying behavioral patterns. During the pandemic, passenger and fishing vessels show significant temporal deviations, reflecting the impact of social isolation measures and operational restrictions on non-essential maritime activities in the region. These findings contribute to a better understanding of maritime mobility patterns and highlight the influence of external factors on vessel movements. <br /> <div>
arXiv:2506.00025v1 Announce Type: cross 
Abstract: Maritime Mobility is at the center of the global economy, and analyzing and understanding such data at scale is critical for ocean conservation and governance. Accordingly, this work introduces a spatio-temporal analytical framework based on discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, emphasizing changes induced during the COVID-19 pandemic. We discretize the ocean space into hexagonal cells and construct mobility signatures for individual vessel types using the frequency of cell transitions and the dwell time within each cell. These features are used to build origin-destination matrices and spatial transition probability models that characterize vessel dynamics at different temporal resolutions. Under multiple vessel types, we contribute with a temporal evolution analysis of mobility patterns during pandemic times, highlighting significant but transient changes to recurring transportation behaviors. Our findings indicate vessel-specific mobility signatures consistent across spatially disjoint regions, suggesting that those are latent behavioral invariants. Besides, we observe significant temporal deviations among passenger and fishing vessels during the pandemic, indicating a strong influence of social isolation policies and operational limitations imposed on non-essential maritime activity in this region.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risks of AI-driven product development and strategies for their mitigation</title>
<link>https://arxiv.org/abs/2506.00047</link>
<guid>https://arxiv.org/abs/2506.00047</guid>
<content:encoded><![CDATA[
<div> progressing, automated product development, risks, mitigation strategies, AI-driven product development
<br />
Summary:
Humanity is moving towards automated product development to accelerate technological progress, but this trend poses risks that must be addressed. To mitigate these risks, principles for safer AI-driven product development are outlined, emphasizing human oversight, accountability, and explainable design. The risk assessment includes technical risks affecting product quality and safety, as well as sociotechnical risks impacting society. While AI-driven product development is still evolving, this discussion aims to balance opportunities and risks without hindering progress in understanding, norm-setting, and regulation. <div>
arXiv:2506.00047v1 Announce Type: cross 
Abstract: Humanity is progressing towards automated product development, a trend that promises faster creation of better products and thus the acceleration of technological progress. However, increasing reliance on non-human agents for this process introduces many risks. This perspective aims to initiate a discussion on these risks and appropriate mitigation strategies. To this end, we outline a set of principles for safer AI-driven product development which emphasize human oversight, accountability, and explainable design, among others. The risk assessment covers both technical risks which affect product quality and safety, and sociotechnical risks which affect society. While AI-driven product development is still in its early stages, this discussion will help balance its opportunities and risks without delaying essential progress in understanding, norm-setting, and regulation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance</title>
<link>https://arxiv.org/abs/2506.00494</link>
<guid>https://arxiv.org/abs/2506.00494</guid>
<content:encoded><![CDATA[
<div> Keywords: Soft Fin-Ray grippers, multi-objective optimization, finite element method, multilayer perception, non-dominated sorting genetic algorithm

Summary:<br />
Soft Fin-Ray grippers are effective for delicate manipulation but face challenges in modeling grasp force and deformation for design purposes. The study uses finite element method (FEM) to estimate deflections and contact forces of the gripper when grasping cylindrical objects, creating a dataset for predicting contact force and tip displacement using a multilayer perception (MLP). The dataset includes design variables related to beam thickness and spacing, with target features of maximum contact forces and tip displacements. A multi-objective optimization problem is addressed, balancing the trade-off between force and delicate manipulation. The non-dominated sorting genetic algorithm (NSGA-II) is used to find optimized design solutions. The methodologies presented in the study can enhance the design and gripping performance of soft robotic grippers, aiding in choosing designs suitable for both delicate grasping and high-force applications.<br />Summary: <div>
arXiv:2506.00494v1 Announce Type: cross 
Abstract: Soft Fin-Ray grippers can perform delicate and careful manipulation, which has caused notable attention in different fields. These grippers can handle objects of various forms and sizes safely. The internal structure of the Fin-Ray finger plays a significant role in its adaptability and grasping performance. However, modeling the non-linear grasp force and deformation behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger becomes more rigid and capable of exerting higher forces, it becomes less delicate in handling objects. The contrast between these two objectives gives rise to a multi-objective optimization problem. In this study, we employ finite element method (FEM) to estimate the deflections and contact forces of the Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a multilayer perception (MLP) for prediction of the contact force and the tip displacement. The FEM dataset consists of three input and four target features. The three input features of the MLP and optimization design variables are the thickness of the front and supporting beams, the thickness of the cross beams, and the equal spacing between the cross beams. In addition, the target features are the maximum contact forces and maximum tip displacements in x- and y-directions. The magnitude of maximum contact force and magnitude of maximum tip displacement are the two objectives, showing the trade-off between force and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized set of solutions are found using multi-objective optimal techniques. We use non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our findings demonstrate that our methodologies can be used to improve the design and gripping performance of soft robotic grippers, helping us to choose a design not only for delicate grasping but also for high-force applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling the Spread of Epidemics on Networks with Differential Privacy</title>
<link>https://arxiv.org/abs/2506.00745</link>
<guid>https://arxiv.org/abs/2506.00745</guid>
<content:encoded><![CDATA[
<div> strategies, epidemic spread, vaccination, contact network, differential privacy<br />
<br />
Summary: Designing effective vaccination strategies for controlling epidemic spread on heterogeneous contact networks is crucial, especially when sensitive information is involved and privacy guarantees are needed. This study introduces $(\varepsilon,\delta)$-differentially private algorithms for reducing the maximum degree and spectral radius to design optimal vaccination strategies. A private algorithm for the multi-set multi-cover problem is developed to control network properties while preserving privacy. The tradeoff between privacy and utility of these algorithms is evaluated on various synthetic and real-world networks, demonstrating their effectiveness in controlling epidemic spread while maintaining privacy. <div>
arXiv:2506.00745v1 Announce Type: cross 
Abstract: Designing effective strategies for controlling epidemic spread by vaccination is an important question in epidemiology, especially in the early stages when vaccines are limited. This is a challenging question when the contact network is very heterogeneous, and strategies based on controlling network properties, such as the degree and spectral radius, have been shown to be effective. Implementation of such strategies requires detailed information on the contact structure, which might be sensitive in many applications. Our focus here is on choosing effective vaccination strategies when the edges are sensitive and differential privacy guarantees are needed. Our main contributions are $(\varepsilon,\delta)$-differentially private algorithms for designing vaccination strategies by reducing the maximum degree and spectral radius. Our key technique is a private algorithm for the multi-set multi-cover problem, which we use for controlling network properties. We evaluate privacy-utility tradeoffs of our algorithms on multiple synthetic and real-world networks, and show their effectiveness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking</title>
<link>https://arxiv.org/abs/2506.01093</link>
<guid>https://arxiv.org/abs/2506.01093</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time transaction monitoring, graph-based modeling, narrative field embedding, generative explanation, financial compliance <br />
Summary: 
The paper introduces a real-time transaction monitoring framework that combines graph-based modeling, narrative field embedding, and generative explanation to automate financial compliance. The system creates dynamic transaction graphs, extracts features, and detects suspicious behavior using a graph neural network. It also generates natural language explanations aligned with regulatory clauses for flagged transactions. Experimental results on simulated financial data demonstrate high performance metrics with a 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation confirms the quality and interpretability of generated justifications. The study showcases the potential of integrating graph intelligence and generative models for explainable compliance in high-risk financial settings. <br /><br />Summary: <div>
arXiv:2506.01093v1 Announce Type: cross 
Abstract: This paper presents a real-time transaction monitoring framework that integrates graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. The system constructs dynamic transaction graphs, extracts structural and contextual features, and classifies suspicious behavior using a graph neural network. A retrieval-augmented generation module generates natural language explanations aligned with regulatory clauses for each flagged transaction. Experiments conducted on a simulated stream of financial data show that the proposed method achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation further confirms the quality and interpretability of generated justifications. The findings demonstrate the potential of combining graph intelligence and generative models to support explainable, audit-ready compliance in high-risk financial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents</title>
<link>https://arxiv.org/abs/2506.01900</link>
<guid>https://arxiv.org/abs/2506.01900</guid>
<content:encoded><![CDATA[
<div> framework, autonomous LLM agents, cost optimization, task outsourcing, agent economies

Summary:
COALESCE is a framework designed to optimize resource utilization in autonomous Large Language Model (LLM) agents by enabling them to outsource specific subtasks to cost-effective third-party agents. The framework incorporates hybrid skill representation, dynamic skill discovery, task decomposition, cost comparison models, decision-making algorithms, and a communication protocol. The theoretical simulations show a 41.8% cost reduction potential, while empirical validation confirms a 20.3% cost reduction with epsilon-greedy exploration. The framework aims to leverage open standards like Google's Agent2Agent protocol to foster efficient agent interactions, reduce operational costs, enhance scalability, and create specialized agent economies. By facilitating a dynamic market for agent capabilities, COALESCE enables complex LLM agent functionalities to become more accessible and economically viable. 

<br /><br />Summary: <div>
arXiv:2506.01900v1 Announce Type: cross 
Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Guided Diffusion Model for Accelerating Computational Fluid Dynamics</title>
<link>https://arxiv.org/abs/2504.04375</link>
<guid>https://arxiv.org/abs/2504.04375</guid>
<content:encoded><![CDATA[
<div> diffusion models, fluid dynamics computation, machine learning, numerical solvers, turbulent flow<br />
Summary:<br />
Machine learning methods, such as diffusion models, aim to accelerate high-fidelity fluid dynamics computation by utilizing low-fidelity data produced by numerical solvers. However, existing approaches struggle to reconstruct fine-scale details when using solver-generated low-fidelity inputs. To address this issue, SG-Diff, a novel diffusion model, is proposed. It incorporates an Importance Weight strategy during training to focus on intricate fluid details and a Predictor-Corrector-Advancer SDE solver to embed physical guidance into the diffusion sampling process. Experimental results on turbulent flow datasets demonstrate SG-Diff's effectiveness in achieving more accurate reconstructions compared to state-of-the-art baselines. <br />Summary: <div>
arXiv:2504.04375v2 Announce Type: replace 
Abstract: Machine learning methods, such as diffusion models, are widely explored as a promising way to accelerate high-fidelity fluid dynamics computation via a super-resolution process from faster-to-compute low-fidelity input. However, existing approaches usually make impractical assumptions that the low-fidelity data is down-sampled from high-fidelity data. In reality, low-fidelity data is produced by numerical solvers that use a coarser resolution. Solver-generated low-fidelity data usually sacrifices fine-grained details, such as small-scale vortices compared to high-fidelity ones. Our findings show that SOTA diffusion models struggle to reconstruct fine-scale details when faced with solver-generated low-fidelity inputs. To bridge this gap, we propose SG-Diff, a novel diffusion model for reconstruction, where both low-fidelity inputs and high-fidelity targets are generated from numerical solvers. We propose an \textit{Importance Weight} strategy during training that serves as a form of self-guidance, focusing on intricate fluid details, and a \textit{Predictor-Corrector-Advancer} SDE solver that embeds physical guidance into the diffusion sampling process. Together, these techniques steer the diffusion model toward more accurate reconstructions. Experimental results on four 2D turbulent flow datasets demonstrate the efficacy of \model~against state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Business Analytics: A Clash of Expectations and Reality</title>
<link>https://arxiv.org/abs/2205.09337</link>
<guid>https://arxiv.org/abs/2205.09337</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, machine learning, structured datasets, gradient boosting, business analytics

Summary:
Deep learning, a popular tool in the field of artificial intelligence and machine learning, faces challenges in widespread adoption within business analytics. These challenges include computational complexity, lack of big data architecture, black-box transparency, skill shortages, and leadership commitment. Despite its benefits, deep learning may not outperform traditional machine learning models for structured datasets with fixed-length feature vectors. The study suggests that gradient boosting models are more suitable for making predictions on structured datasets in business analytics. This finding highlights the importance of viewing deep learning as a complement to existing machine learning models rather than a universal solution. The paper provides insights from empirical studies in three industry use cases, discussing practical implications and outlining future research directions. 

<br /><br />Summary: <div>
arXiv:2205.09337v2 Announce Type: replace-cross 
Abstract: Our fast-paced digital economy shaped by global competition requires increased data-driven decision-making based on artificial intelligence (AI) and machine learning (ML). The benefits of deep learning (DL) are manifold, but it comes with limitations that have, so far, interfered with widespread industry adoption. This paper explains why DL, despite its popularity, has difficulties speeding up its adoption within business analytics. It is shown that the adoption of deep learning is not only affected by computational complexity, lacking big data architecture, lack of transparency (black-box), skill shortage, and leadership commitment, but also by the fact that DL does not outperform traditional ML models in the case of structured datasets with fixed-length feature vectors. Deep learning should be regarded as a powerful addition to the existing body of ML models instead of a one size fits all solution. The results strongly suggest that gradient boosting can be seen as the go-to model for predictions on structured datasets within business analytics. In addition to the empirical study based on three industry use cases, the paper offers a comprehensive discussion of those results, practical implications, and a roadmap for future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated machine learning: AI-driven decision making in business analytics</title>
<link>https://arxiv.org/abs/2205.10538</link>
<guid>https://arxiv.org/abs/2205.10538</guid>
<content:encoded><![CDATA[
<div> AutoML, industrial machine learning, H2O AutoML framework, business analytics, automated decision-making <br />
Summary: <br />
The rise of AI-driven decision-making in the business world has led to a surge in interest in industrial machine learning applications. The shortage of analytics experts can be addressed by enhancing the user-friendliness of ML frameworks. Automated machine learning (AutoML) offers a solution by providing automated off-the-shelf solutions for model selection and hyperparameter tuning. In a study comparing the H2O AutoML framework with a manually tuned stacked ML model, the manual model outperformed in all three case studies, but the H2O AutoML package showed promising results. It is fast, easy to use, and delivers reliable results close to a professionally tuned model. This tool can aid in fast prototyping, shorten development cycles, and bridge the gap between demand and supply for ML experts. AutoML has the potential to empower individuals in an increasingly automated and digital world. <br /> <div>
arXiv:2205.10538v2 Announce Type: replace-cross 
Abstract: The realization that AI-driven decision-making is indispensable in today's fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards automated decisions in business analytics. Finally, AutoML has the potential to foster human empowerment in a world that is rapidly becoming more automated and digital.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETS-C: Leveraging Text Embedding for Time Series Classification</title>
<link>https://arxiv.org/abs/2407.06533</link>
<guid>https://arxiv.org/abs/2407.06533</guid>
<content:encoded><![CDATA[
<div> Keywords: language modeling, time series data, text embedding model, convolutional neural networks, lightweight model architecture <br />
Summary: 
This study introduces a novel approach for time series classification using a text embedding model in conjunction with a lightweight classification head. While previous methods focused on fine-tuning large language models for time series data, this new approach, called LETS-C, outperforms the state-of-the-art models in classification accuracy. The LETS-C model combines text embeddings with a simple classification head composed of convolutional neural networks and multilayer perceptron. By utilizing text embeddings to encode time series data, the LETS-C model achieves high performance while keeping a lightweight architecture. Through extensive experiments on a well-established time series classification benchmark, it was found that LETS-C requires only 14.5% of the trainable parameters compared to the current SOTA model. This suggests that leveraging text embedding models for time series classification tasks presents a promising direction for achieving high performance with a simpler model architecture. <br /><br />Summary: <div>
arXiv:2407.06533v2 Announce Type: replace-cross 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much is Enough? The Diminishing Returns of Tokenization Training Data</title>
<link>https://arxiv.org/abs/2502.20273</link>
<guid>https://arxiv.org/abs/2502.20273</guid>
<content:encoded><![CDATA[
<div> tokenization, hyperparameter, training data size, BPE, UnigramLM, WordPiece
<br />
Summary: 
This study examines the impact of tokenizer training data size on tokenization quality in natural language processing. By training BPE, UnigramLM, and WordPiece tokenizers on varying English training data sizes from 1GB to 900GB, it is found that improvements in tokenization quality diminish beyond 150GB due to constraints introduced by the pre-tokenization stage. The saturation effect is observed in both English and Russian data, indicating a practical limit to tokenization quality improvements achievable through increasing data size. This insight can guide the optimization of tokenization processes, reducing the computational resources needed for training on large corpora. Further research directions in tokenization algorithms are suggested based on these findings. 
<br /> <div>
arXiv:2502.20273v2 Announce Type: replace-cross 
Abstract: Tokenization, a crucial initial step in natural language processing, is governed by several key parameters, such as the tokenization algorithm, vocabulary size, pre-tokenization strategy, inference strategy, and training data corpus. This paper investigates the impact of an often-overlooked hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and WordPiece tokenizers across various vocabulary sizes using English training data ranging from 1GB to 900GB. Our findings reveal diminishing returns as training data size increases beyond roughly 150GB, suggesting a practical limit to the improvements in tokenization quality achievable through additional data. We analyze this phenomenon and attribute the saturation effect to constraints introduced by the pre-tokenization stage. We then demonstrate the extent to which these findings can generalize by experimenting on data in Russian, a language typologically distant from English. While the limit appears to materialize at a later phase of pre-training, around 200GB, it is in fact observed. These results provide valuable insights for optimizing the tokenization process by reducing the compute required for training on large corpora and suggest promising directions for future research in tokenization algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singularity Protocol for Cross Chain AMM without Intermediate Tokens or Bridges</title>
<link>https://arxiv.org/abs/2505.24337</link>
<guid>https://arxiv.org/abs/2505.24337</guid>
<content:encoded><![CDATA[
<div> AMMs, decentralized exchange, cross-chain swaps, liquidity, blockchain<br />
Summary:<br />
Automated Market Makers (AMMs) have revolutionized decentralized exchanges but face scalability challenges with cross-chain swaps. The current double-sided AMMs are inefficient and introduce risks like volatility and blockchain issues. This paper proposes a new class of AMMs that eliminate the need for intermediate tokens or bridging, enabling efficient cross-chain swaps with lower gas requirements. The new technology is based on an invariant that does not rely on bi-state dependency between assets being swapped. This innovation supports cross-chain swaps across various blockchain layers and offers a more streamlined and secure method for value transfer swaps. The proposed solution addresses the limitations of existing AMMs and provides a promising approach for improving liquidity and efficiency in cross-chain transactions. <br /><br /> <div>
arXiv:2505.24337v1 Announce Type: new 
Abstract: Automated Market Makers (AMMs) are decentralized exchange protocols that provide continuous access to token liquidity without the need for order books or traditional market makers. However, this innovation has failed to scale when it comes to cross-chain swaps. Modern cross-chain swaps employ double-sided AMMs, which are not only inefficient due to liquidity fragmentation but also require an intermediate token. This introduces inherent volatility risk as well as blockchain and bridging risk, especially in the case of wrapped tokens. This paper describes the inefficiencies of existing AMM invariants, particularly their mixed polynomial nature, and derives a new class of AMMs that do not have bi-state dependency between the assets being swapped. We propose a novel method of value transfer swaps using the described invariant that mitigates the need for bi-state dependency and eliminates the need for intermediate tokens or bridging. Furthermore, we show how this mechanism enables efficient cross-chain swaps with lower gas requirements and no bridging risks. The proposed technology is designed to support cross-chain swaps across any permutation of L1, L2, and L3 blockchains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Black Box: Interpretability of LLMs in Finance</title>
<link>https://arxiv.org/abs/2505.24650</link>
<guid>https://arxiv.org/abs/2505.24650</guid>
<content:encoded><![CDATA[
<div> interpretability, financial services, large language models, transparency, regulatory compliance

Summary: 
This paper introduces the concept of mechanistic interpretability in the context of Large Language Models (LLMs) in the financial services sector. LLMs have shown great potential in various financial tasks but their complexity and lack of transparency raise concerns in the regulated financial industry. Mechanistic interpretability offers a transparent way to understand LLM behavior by reverse-engineering their internal workings, providing insights into how specific features influence predictions and allowing for the modification of model behavior. The paper explores the theoretical aspects of mechanistic interpretability and demonstrates its practical relevance through financial use cases such as trading strategies, sentiment analysis, bias detection, and hallucination detection. The adoption of advanced interpretability tools is expected to be crucial as LLM usage increases, ensuring that AI systems in finance are ethical, transparent, and compliant with evolving regulations. The paper emphasizes how these techniques can address interpretability requirements for regulatory and compliance purposes in the financial sector. 

<br /><br />Summary: <div>
arXiv:2505.24650v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a spectrum of tasks in financial services, including report generation, chatbots, sentiment analysis, regulatory compliance, investment advisory, financial knowledge retrieval, and summarization. However, their intrinsic complexity and lack of transparency pose significant challenges, especially in the highly regulated financial sector, where interpretability, fairness, and accountability are critical. As far as we are aware, this paper presents the first application in the finance domain of understanding and utilizing the inner workings of LLMs through mechanistic interpretability, addressing the pressing need for transparency and control in AI systems. Mechanistic interpretability is the most intuitive and transparent way to understand LLM behavior by reverse-engineering their internal workings. By dissecting the activations and circuits within these models, it provides insights into how specific features or components influence predictions - making it possible not only to observe but also to modify model behavior. In this paper, we explore the theoretical aspects of mechanistic interpretability and demonstrate its practical relevance through a range of financial use cases and experiments, including applications in trading strategies, sentiment analysis, bias, and hallucination detection. While not yet widely adopted, mechanistic interpretability is expected to become increasingly vital as adoption of LLMs increases. Advanced interpretability tools can ensure AI systems remain ethical, transparent, and aligned with evolving financial regulations. In this paper, we have put special emphasis on how these techniques can help unlock interpretability requirements for regulatory and compliance purposes - addressing both current needs and anticipating future expectations from financial regulators globally.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Bayesian multi-fidelity inverse analysis for expensive and non-differentiable physics-based simulations in high stochastic dimensions</title>
<link>https://arxiv.org/abs/2505.24708</link>
<guid>https://arxiv.org/abs/2505.24708</guid>
<content:encoded><![CDATA[
<div> Bayesian, multi-fidelity, inverse analysis, high-dimensional, computational<br />
Summary:<br />
This article introduces a novel approach called Bayesian multi-fidelity inverse analysis (BMFIA) to address the challenges of high-dimensional Bayesian inverse analysis for computationally demanding, nonlinear physics-based high-fidelity models. The method leverages simpler lower-fidelity models designed to provide model derivatives, learning a probabilistic dependence between the lower and higher-fidelity models. This allows for statistically correcting the inaccurate lower-fidelity responses in an altered likelihood formulation. BMFIA is fully differentiable and can be applied to a wide range of scenarios, including finely-resolved spatial reconstruction problems for nonlinear and transient coupled poro-elastic media physics. The approach is demonstrated to solve Bayesian inverse problems efficiently, even with a small amount of data, making it a valuable tool for addressing complex multi-physics problems. <div>
arXiv:2505.24708v1 Announce Type: new 
Abstract: High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible for computationally demanding, nonlinear physics-based high-fidelity (HF) models. Usually, the use of more efficient gradient-based inference schemes is impeded if the multi-physics models are provided by complex legacy codes. Adjoint-based derivatives are either exceedingly cumbersome to derive or non-existent for practically relevant large-scale nonlinear and coupled multi-physics problems. Similarly, holistic automated differentiation w.r.t. primary variables of multi-physics codes is usually not yet an option and requires extensive code restructuring if not considered from the outset in the software design. This absence of differentiability further exacerbates the already present computational challenges. To overcome the existing limitations, we propose a novel inference approach called Bayesian multi-fidelity inverse analysis (BMFIA), which leverages simpler and computationally cheaper lower-fidelity (LF) models that are designed to provide model derivatives. BMFIA learns a simple, probabilistic dependence of the LF and HF models, which is then employed in an altered likelihood formulation to statistically correct the inaccurate LF response. From a Bayesian viewpoint, this dependence represents a multi-fidelity conditional density (discriminative model). We demonstrate how this multi-fidelity conditional density can be learned robustly in the small data regime from only a few HF and LF simulations (50 to 300), which would not be sufficient for naive surrogate approaches. The formulation is fully differentiable and allows the flexible design of a wide range of LF models. We demonstrate that BMFIA solves Bayesian inverse problems for scenarios that used to be prohibitive, such as finely-resolved spatial reconstruction problems for nonlinear and transient coupled poro-elastic media physics.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Soupability of Documents in State Space Models</title>
<link>https://arxiv.org/abs/2505.24033</link>
<guid>https://arxiv.org/abs/2505.24033</guid>
<content:encoded><![CDATA[
<div> Keywords: Structured State Space Models, document souping, Mamba2 models, multi-hop QA, long-document reasoning

Summary:
Structured State Space Models (SSMs) are investigated to determine if their hidden states can be merged post-hoc to support downstream reasoning. A strategy called document souping is proposed, where documents are encoded independently and their representations are pooled into a single context state. This approach allows for modular encoding and reuse without the need to reprocess the full input for each query. Mamba2 models are modified to produce soupable representations, enabling support for multi-hop QA, sparse retrieval, and long-document reasoning with high accuracy. In experiments on HotpotQA, souping ten independently encoded documents achieves performance near that of a cross-encoder trained on the same inputs. Overall, this method demonstrates the effectiveness of combining independently encoded document representations for improved downstream reasoning tasks. 

Summary: <div>
arXiv:2505.24033v1 Announce Type: cross 
Abstract: We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Proximity: A Graph-Based Approach to Blockchain Fraud Prevention</title>
<link>https://arxiv.org/abs/2505.24284</link>
<guid>https://arxiv.org/abs/2505.24284</guid>
<content:encoded><![CDATA[
<div> Keywords: fraud-deterrent, public blockchains, transaction proximity, Easily Attainable Identities (EAIs), directed graph analysis

Summary: 
This paper presents a fraud-deterrent access validation system for public blockchains using Transaction Proximity and Easily Attainable Identities (EAIs) concepts. The system analyzes transaction patterns to identify wallets closely connected to centralized exchanges, aiming to prevent fraudulent activities. The analysis of the Ethereum blockchain reveals a high percentage of large USDC wallets being EAI or within one transaction hop of an EAI. Moreover, a significant number of past exploits were found to not involve EAIs, highlighting the need for such a validation system. Three implementation approaches are proposed, balancing gas cost and privacy considerations. This approach allows for programmatic compliance without restricting access or sharing personal information, maintaining blockchain openness and enabling protocols to implement customized validation systems.<br /><br />Summary: <div>
arXiv:2505.24284v1 Announce Type: cross 
Abstract: This paper introduces a fraud-deterrent access validation system for public blockchains, leveraging two complementary concepts: "Transaction Proximity", which measures the distance between wallets in the transaction graph, and "Easily Attainable Identities (EAIs)", wallets with direct transaction connections to centralized exchanges. Recognizing the limitations of traditional approaches like blocklisting (reactive, slow) and strict allow listing (privacy-invasive, adoption barriers), we propose a system that analyzes transaction patterns to identify wallets with close connections to centralized exchanges.
  Our directed graph analysis of the Ethereum blockchain reveals that 56% of large USDC wallets (with a lifetime maximum balance greater than \$10,000) are EAI and 88% are within one transaction hop of an EAI. For transactions exceeding \$2,000, 91% involve at least one EAI. Crucially, an analysis of past exploits shows that 83% of the known exploiter addresses are not EAIs, with 21% being more than five hops away from any regulated exchange. We present three implementation approaches with varying gas cost and privacy tradeoffs, demonstrating that EAI-based access control can potentially prevent most of these incidents while preserving blockchain openness. Importantly, our approach does not restrict access or share personally identifiable information, but it provides information for protocols to implement their own validation or risk scoring systems based on specific needs. This middle-ground solution enables programmatic compliance while maintaining the core values of open blockchain.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking for Attention: Randomized Attention Test Design for Validator Monitoring in Optimistic Rollups</title>
<link>https://arxiv.org/abs/2505.24393</link>
<guid>https://arxiv.org/abs/2505.24393</guid>
<content:encoded><![CDATA[
<div> scalability, blockchain, Optimistic Rollups, Randomized Attention Test, game-theoretic analysis

Summary:
Optimistic Rollups (ORUs) enhance blockchain scalability but face the verifier's dilemma due to a lack of mechanisms ensuring validator attentiveness. The Randomized Attention Test (RAT) protocol is introduced to challenge validators in ORUs, verifying their liveness and readiness. Game-theoretic analysis shows that an Ideal Security Equilibrium can be achieved with RAT, where validators are attentive and proposers honest. This equilibrium is attainable with low penalties for non-responsive validators and a low attention test frequency. RAT serves as a practical mechanism to enforce validator diligence, increasing the security and integrity of ORU systems without significant additional costs. <div>
arXiv:2505.24393v1 Announce Type: cross 
Abstract: Optimistic Rollups (ORUs) significantly enhance blockchain scalability but inherently suffer from the verifier's dilemma, particularly concerning validator attentiveness. Current systems lack mechanisms to proactively ensure validators are diligently monitoring L2 state transitions, creating a vulnerability where fraudulent states could be finalized. This paper introduces the Randomized Attention Test (RAT), a novel L1-based protocol designed to probabilistically challenge validators in ORUs, thereby verifying their liveness and computational readiness. Our game-theoretic analysis demonstrates that an Ideal Security Equilibrium, where all validators are attentive and proposers are honest, can be achieved with RAT. Notably, this equilibrium is attainable and stable with relatively low economic penalties (e.g., under $1000) for non-responsive validators and a low attention test frequency (e.g., under 1% per epoch). RAT thus provides a crucial, practical mechanism to enforce validator diligence, fortifying the overall security and integrity of ORU systems with minimizing additional costs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV</title>
<link>https://arxiv.org/abs/2505.24781</link>
<guid>https://arxiv.org/abs/2505.24781</guid>
<content:encoded><![CDATA[
<div> Regularized Tyler's M-estimator, shrinkage coefficient, leave-one-out cross-validation, computational efficiency, high-dimensional data<br />
<br />
Summary: <br />
The study focuses on estimating a shrinkage coefficient for Regularized Tyler's M-estimator using leave-one-out cross-validation (LOOCV) log-likelihood loss. The proposed approach aims to find an optimal shrinkage coefficient by solving a selected objective function, enhancing computational efficiency by approximating the LOOCV log-likelihood loss. This approximation significantly reduces the running time complexity for the LOOCV procedure by O(n), offering a faster computation of the LOOCV estimate. The efficiency and accuracy of the method were demonstrated through synthetic high-dimensional data and real datasets for object recognition, face recognition, and handwritten digit recognition. Results indicate the proposed approach is both efficient and more precise compared to existing methods for shrinkage coefficient estimation. <div>
arXiv:2505.24781v1 Announce Type: cross 
Abstract: We consider the problem of estimating a regularization parameter, or a shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator (RTME). In particular, we propose to estimate an optimal shrinkage coefficient by setting $\alpha$ as the solution to a suitably chosen objective function; namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since LOOCV is computationally prohibitive even for moderate sample size $n$, we propose a computationally efficient approximation for the LOOCV log-likelihood loss that eliminates the need for invoking the RTME procedure $n$ times for each sample left out during the LOOCV procedure. This approximation yields an $O(n)$ reduction in the running time complexity for the LOOCV procedure, which results in a significant speedup for computing the LOOCV estimate. We demonstrate the efficiency and accuracy of the proposed approach on synthetic high-dimensional data sampled from heavy-tailed elliptical distributions, as well as on real high-dimensional datasets for object recognition, face recognition, and handwritten digit's recognition. Our experiments show that the proposed approach is efficient and consistently more accurate than other methods in the literature for shrinkage coefficient estimation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpolating Neural Network-Tensor Decomposition (INN-TD): a scalable and interpretable approach for large-scale physics-based problems</title>
<link>https://arxiv.org/abs/2503.02041</link>
<guid>https://arxiv.org/abs/2503.02041</guid>
<content:encoded><![CDATA[
<div> Interpolating Neural Network-Tensor Decomposition, scalable, interpretable, machine learning, finite element methods, large-scale physical systems
Summary:
Interpolating Neural Network-Tensor Decomposition (INN-TD) is introduced as a framework that combines machine learning and finite element methods to model large-scale physical systems accurately and efficiently. By incorporating locally supported interpolation functions from finite element methods into the network architecture, INN-TD achieves a sparse learning structure, leading to enhanced accuracy, faster training/solving speed, and reduced memory usage. This framework is well-suited for addressing large-scale high-dimensional parametric partial differential equations in physical problems that require high precision in tasks such as training, solving, and inverse optimization. Its effectiveness lies in its ability to provide interpretable solutions for industrial problems while maintaining computational efficiency and accuracy in modeling complex physics-based systems. <br /><br />Summary: <div>
arXiv:2503.02041v3 Announce Type: replace 
Abstract: Deep learning has been extensively employed as a powerful function approximator for modeling physics-based problems described by partial differential equations (PDEs). Despite their popularity, standard deep learning models often demand prohibitively large computational resources and yield limited accuracy when scaling to large-scale, high-dimensional physical problems. Their black-box nature further hinders the application in industrial problems where interpretability and high precision are critical. To overcome these challenges, this paper introduces Interpolating Neural Network-Tensor Decomposition (INN-TD), a scalable and interpretable framework that has the merits of both machine learning and finite element methods for modeling large-scale physical systems. By integrating locally supported interpolation functions from finite element into the network architecture, INN-TD achieves a sparse learning structure with enhanced accuracy, faster training/solving speed, and reduced memory footprint. This makes it particularly effective for tackling large-scale high-dimensional parametric PDEs in training, solving, and inverse optimization tasks in physical problems where high precision is required.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data</title>
<link>https://arxiv.org/abs/2505.22670</link>
<guid>https://arxiv.org/abs/2505.22670</guid>
<content:encoded><![CDATA[
<div> semantic-spatial-topological, BIM models, network-based representation, IFC, design patterns

Summary:
This study introduces a unified network-based representation method for BIM models to capture complex spatial and topological relationships between components. By extending the IFC standard, the method incorporates local spatial relationships and topological connections, enriching the network structure. This approach enhances understanding of component interactions, dependencies, and design patterns in BIM models. The proposed representation method effectively captures semantic, topological, and spatial relationships, offering significant potential for learning design patterns in construction industry. <div>
arXiv:2505.22670v1 Announce Type: new 
Abstract: Building Information Modeling (BIM) has revolutionized the construction industry by providing a comprehensive digital representation of building structures throughout their lifecycle. However, existing research lacks effective methods for capturing the complex spatial and topological relationships between components in BIM models, which are essential for understanding design patterns and enhancing decision-making. This study proposes a unified network-based representation method that integrates the "semantic-spatial-topological" multi-dimensional design features of BIM models. By extending the IFC (Industry Foundation Classes) standard, we introduce local spatial relationships and topological connections between components to enrich the network structure. This representation method enables a more detailed understanding of component interactions, dependencies, and implicit design patterns, effectively capturing the semantic, topological, and spatial relationships in BIM, and holds significant potential for the representation and learning of design patterns.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive analysis of PINNs: Variants, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2505.22761</link>
<guid>https://arxiv.org/abs/2505.22761</guid>
<content:encoded><![CDATA[
<div> Physics Informed Neural Networks (PINNs), differential equations, architecture, variants, application, challenges

Summary:
Physics Informed Neural Networks (PINNs) are a powerful computational tool for solving differential equations. This survey provides a comprehensive overview of PINNs, covering architecture, variants, applications, real-world use cases, and challenges. Existing surveys lack detail, but this one offers a thorough analysis of PINNs, including recent advancements and research. Three main contributions are discussed: an in-depth look at PINNs architecture and variants, performance analysis on various equations and applications, and a discussion of current issues and future research directions. This survey aims to standardize and popularize the use of PINNs by addressing key areas and providing valuable insights for the field moving forward.<br /><br />Summary: <div>
arXiv:2505.22761v1 Announce Type: new 
Abstract: Physics Informed Neural Networks (PINNs) have been emerging as a powerful computational tool for solving differential equations. However, the applicability of these models is still in its initial stages and requires more standardization to gain wider popularity. Through this survey, we present a comprehensive overview of PINNs approaches exploring various aspects related to their architecture, variants, areas of application, real-world use cases, challenges, and so on. Even though existing surveys can be identified, they fail to provide a comprehensive view as they primarily focus on either different application scenarios or limit their study to a superficial level. This survey attempts to bridge the gap in the existing literature by presenting a detailed analysis of all these factors combined with recent advancements and state-of-the-art research in PINNs. Additionally, we discuss prevalent challenges in PINNs implementation and present some of the future research directions as well. The overall contributions of the survey can be summarised into three sections: A detailed overview of PINNs architecture and variants, a performance analysis of PINNs on different equations and application domains highlighting their features. Finally, we present a detailed discussion of current issues and future research directions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution analysis of software quality metrics in an open-source java project: A case study on TestNG</title>
<link>https://arxiv.org/abs/2505.22884</link>
<guid>https://arxiv.org/abs/2505.22884</guid>
<content:encoded><![CDATA[
<div> Keywords: software quality, TestNG, Java, object-oriented metrics, static analysis<br />
Summary:<br />
This study examines the evolution of software quality metrics in five versions of the TestNG Java testing framework. Using Understand, key object-oriented metrics were analyzed, showing trends such as increased stability and maintainability. The results suggest ongoing development, refactoring, and architectural improvements have contributed to TestNG's maturity over time. This study offers insights on design evolution and recommendations for maintaining code quality in similar projects. <div>
arXiv:2505.22884v1 Announce Type: cross 
Abstract: Software quality is critical in modern software engineering, especially in large and evolving codebases. This study analyzes the evolution of software quality metrics in five successive versions of the open-source Java testing framework TestNG. Using the static analysis tool Understand, eleven key object-oriented metrics, including cyclomatic complexity, class coupling, and lines of code, were extracted for each version. Statistical and visual analyses reveal structural trends over time. The results indicate that TestNG has matured into a more stable and maintainable framework, reflecting ongoing development, refactoring, and architectural improvements. This study provides insights into design evolution and offers recommendations for maintaining code quality in similar projects.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Be.FM: Open Foundation Models for Human Behavior</title>
<link>https://arxiv.org/abs/2505.23058</link>
<guid>https://arxiv.org/abs/2505.23058</guid>
<content:encoded><![CDATA[
<div> modeling, human behavior, decision-making, benchmark tasks, behavioral science

Summary:
Be.FM is introduced as an open foundation model for human behavior modeling, using large language models and fine-tuning on diverse behavioral data. It has the potential to predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge. Through comprehensive benchmark tasks, Be.FM demonstrates its capabilities in understanding and predicting human decision-making. This opens up possibilities for leveraging foundation models in various fields to gain insights into human behavior. <div>
arXiv:2505.23058v1 Announce Type: cross 
Abstract: Despite their success in numerous fields, the potential of foundation models for modeling and understanding human behavior remains largely unexplored. We introduce Be.FM, one of the first open foundation models designed for human behavior modeling. Built upon open-source large language models and fine-tuned on a diverse range of behavioral data, Be.FM can be used to understand and predict human decision-making. We construct a comprehensive set of benchmark tasks for testing the capabilities of behavioral foundation models. Our results demonstrate that Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid subgradient and simulated annealing method for hemivariational inequalities</title>
<link>https://arxiv.org/abs/2505.23676</link>
<guid>https://arxiv.org/abs/2505.23676</guid>
<content:encoded><![CDATA[
<div> global aggregate subgradient method, hemivariational inequality problems, contact mechanics, local minimization algorithm, performance comparison

Summary:
The paper introduces a global aggregate subgradient method for solving hemivariational inequality problems in contact mechanics. It combines global search capabilities to determine starting points for a local minimization algorithm. The algorithm incorporates null steps, using aggregate and current iteration subgradients to find the search direction, as well as serious steps. The method's performance is evaluated against other solvers using a representative contact mechanics problem. The study showcases the effectiveness of the proposed approach in efficiently solving complex contact mechanics problems compared to existing methods. <div>
arXiv:2505.23676v1 Announce Type: cross 
Abstract: In this paper, we employ a global aggregate subgradient method for the numerical solution of hemivariational inequality problems arising in contact mechanics. The method integrates a global search procedure to identify starting points for a local minimization algorithm. The algorithm consists of two types of steps: null steps and serious steps. In each null step, only two subgradients are utilized: the aggregate subgradient and the subgradient computed at the current iteration point, which together determine the search direction. Furthermore, we compare the performance of the proposed method with selected solvers using a representative contact mechanics problem as a case study.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computerized Modeling of Electrophysiology and Pathoelectrophysiology of the Atria -- How Much Detail is Needed?</title>
<link>https://arxiv.org/abs/2505.23717</link>
<guid>https://arxiv.org/abs/2505.23717</guid>
<content:encoded><![CDATA[
<div> Keywords: computerized modeling, atrial electrophysiology, arrhythmias, fibrotic tissue, atrial ablation<br />
Summary:<br />
This review focuses on computerized modeling of human atrial electrophysiology, specifically addressing common arrhythmias like atrial flutter and atrial fibrillation. The key question is identifying the necessary components for accurate simulation of arrhythmogenic tissue modifications, including remodeling, cardiomyopathy, and fibrosis. It examines the balance between model complexity and computational efficiency, emphasizing the risks of oversimplification and excessive detail. Various aspects of atrial modeling, from cellular to whole atria levels, are covered, considering factors like atrial geometry, fiber direction, anisotropy, and wall thickness. The impact of different modeling approaches and the latest advances in modeling fibrotic tissue are discussed, along with verification and validation methods. The use of these models in planning atrial ablation strategies, both personalized and cohort-based, is highlighted, stressing the importance of integrating experimental data and clinical validation for improved patient outcomes.<br /> <div>
arXiv:2505.23717v1 Announce Type: cross 
Abstract: This review focuses on the computerized modeling of the electrophysiology of the human atria, emphasizing the simulation of common arrhythmias such as atrial flutter (AFlut) and atrial fibrillation (AFib). Which components of the model are necessary to accurately model arrhythmogenic tissue modifications, including remodeling, cardiomyopathy, and fibrosis, to ensure reliable simulations? The central question explored is the level of detail required for trustworthy simulations for a specific context of use. The review discusses the balance between model complexity and computational efficiency, highlighting the risks of oversimplification and excessive detail. It covers various aspects of atrial modeling, from cellular to whole atria levels, including the influence of atrial geometry, fiber direction, anisotropy, and wall thickness on simulation outcomes. The article also examines the impact of different modeling approaches, such as volumetric 3D models, bilayer models, and single surface models, on the realism of simulations. In addition, it reviews the latest advances in the modeling of fibrotic tissue and the verification and validation of atrial models. The intended use of these models in planning and optimization of atrial ablation strategies is discussed, with a focus on personalized modeling for individual patients and cohort-based approaches for broader applications. The review concludes by emphasizing the importance of integrating experimental data and clinical validation to enhance the utility of computerized atrial models to improve patient outcomes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A blockchain-based intelligent recommender system framework for enhancing supply chain resilience</title>
<link>https://arxiv.org/abs/2404.00306</link>
<guid>https://arxiv.org/abs/2404.00306</guid>
<content:encoded><![CDATA[
<div> Intelligent recommender system technology, blockchain technology, smart contract, supply chain disruption, system dynamics simulation<br />
<br />
Summary: 
This research proposes a data-driven supply chain disruption response framework utilizing intelligent recommender system (IRS) technology integrated with blockchain (BLC) technology. A smart contract prototype demonstrates information exchange within a BLC network. An industrial case study implementation validates the BLC-IRS framework's effectiveness in responding to disruptions. A system dynamics simulation model confirms the framework's ability to mitigate disruptions in the supply chain response phase. This approach provides a practical digital solution for supply chain resilience, allowing participants to react swiftly and effectively to disruptions. By utilizing synthetic technologies, the BLC-IRS framework enhances the SCRes community's ability to access supplementary resource information in a secure and real-time manner following disruptions. <div>
arXiv:2404.00306v3 Announce Type: replace 
Abstract: This research proposed a data-driven supply chain disruption response baseline framework based on intelligent recommender system technology as an initial SCRes reactive solution. To improve the data quality and reliability of the proposed IRS as a stable, secure, and resilient decision support system, blockchain technology is integrated into the baseline architecture. The smart contract is prototyped to demonstrate the information exchange mechanism under a BLC network environment. The BLC-IRS framework is then implemented with an industrial case to demonstrate its executable function. A system dynamics (SD) simulation model is adopted to validate the BLC-IRS framework as an effective digital SCRes enhancement measure. The simulation results indicated that the proposed BLC-IRS framework can be effectively implemented as a SC disruption mitigation measure in the SCRes response phase as reactive measure, enabling SC participants to react better to SC disruptions at the physical level. Compared to previous studies that limited at the conceptual level as the proactive SCRes measure with a standalone fashion, the developed BLC-IRS contributes an executable SCRes digital solution with synthetic technologies as a reactive SCRes measure for the SCRes community, by identifying the internal and external supplementary resource information in an agile, safe, and real-time manner after SC disruption.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Perishable and Non-Perishable Product Assignment to Packaging Lines in a Sustainable Manufacturing System: An AUGMECON2VIKOR Algorithm</title>
<link>https://arxiv.org/abs/2410.21844</link>
<guid>https://arxiv.org/abs/2410.21844</guid>
<content:encoded><![CDATA[
<div> manufacturing systems, food industry, mathematical model, optimization, perishable products<br />
<br />
Summary: 
This study introduces a new mathematical model and assignment approach to optimize manufacturing systems for perishable, non-perishable, and hybrid products in the food industry. The model considers three objective functions: minimizing production costs, maximizing product quality, and reducing CO2 emissions. Comparing the proposed AUGMECON2VIKOR model to AUGMECON2, the former outperforms in generating superior Pareto solutions across all objectives. Additionally, a sensitivity analysis demonstrates the positive environmental impact, influencing cost and quality factors. By leveraging knowledge discovery and a tailored assignment model, this study offers a comprehensive approach to address the unique constraints of perishable products and enhance operational efficiency in manufacturing systems. <div>
arXiv:2410.21844v2 Announce Type: replace-cross 
Abstract: Identifying appropriate manufacturing systems for products can be considered a pivotal manufacturing task contributing to the optimization of operational and planning activities. It has gained importance in the food industry due to the distinct constraints and considerations posed by perishable and non-perishable items in this problem. Hence, this study proposes a new mathematical model according to knowledge discovery as well as an assignment model to optimize manufacturing systems for perishable, non-perishable, and hybrid products tailored to meet their unique characteristics. In the presented model, three objective functions are taken into account: (1) minimizing production costs by assigning the products to the right set of manufacturing systems, (2) maximizing the product quality by assigning the products to the systems, and (3) minimizing total CO2 emissions of the machines. A numerical example is utilized to evaluate the performance of AUGMECON2VIKOR compared to AUGMECON2. The results show that AUGMECON2VIKOR obtains superior Pareto solutions across all objective functions. Furthermore, the sensitivity analysis explores the positive green impacts, influencing both cost and quality.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Representation Learning for fMRI-based Neurological Disorder Identification</title>
<link>https://arxiv.org/abs/2412.16197</link>
<guid>https://arxiv.org/abs/2412.16197</guid>
<content:encoded><![CDATA[
<div> representation learning, meta-learning, self-supervised learning, functional Magnetic Resonance Imaging, neurological disorders<br />
Summary:<br />
The study addresses challenges in identifying neurological disorders due to data heterogeneity and scarcity. It introduces a novel representation learning approach that combines meta-learning and self-supervised learning to enhance generalization from normal to clinical features. By leveraging self-supervised learning on control data and incorporating meta-learning, the model can generalize to clinical tasks with limited training data. The approach is applied to four different clinical datasets for neurological disorder classification, demonstrating its effectiveness for diverse tasks. The public availability of the code allows for further exploration and application of the representation learning strategy. <div>
arXiv:2412.16197v2 Announce Type: replace-cross 
Abstract: Despite the impressive advances achieved using deep learning for functional brain activity analysis, the heterogeneity of functional patterns and the scarcity of imaging data still pose challenges in tasks such as identifying neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while data may be abundantly available from healthy controls, clinical data is often scarce, especially for rare diseases, limiting the ability of models to identify clinically-relevant features. We overcome this limitation by introducing a novel representation learning strategy integrating meta-learning with self-supervised learning to improve the generalization from normal to clinical features. This approach enables generalization to challenging clinical tasks featuring scarce training data. We achieve this by leveraging self-supervised learning on the control dataset to focus on inherent features that are not limited to a particular supervised task and incorporating meta-learning to improve the generalization across domains. To explore the generalizability of the learned representations to unseen clinical applications, we apply the model to four distinct clinical datasets featuring scarce and heterogeneous data for neurological disorder classification. Results demonstrate the superiority of our representation learning strategy on diverse clinically-relevant tasks. Code is publicly available at https://github.com/wenhui0206/MeTSK/tree/main
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling extreme events and intermittency in turbulent diffusion with a mean gradient</title>
<link>https://arxiv.org/abs/2505.21688</link>
<guid>https://arxiv.org/abs/2505.21688</guid>
<content:encoded><![CDATA[
<div> Keywords: passive tracer transport, turbulent flows, intermittency, extreme events, stochastic dynamics<br />
<br />
Summary: 
This study examines the statistical properties of passive tracer transport in turbulent flows with a mean gradient, focusing on tracer intermittency and extreme events. An analytically tractable model is developed, combining zonal and shear velocity components with linear and nonlinear stochastic dynamics. By formulating the model in Fourier space, the researchers derive a straightforward explicit solution for the tracer invariant statistics. They identify the resonance condition responsible for non-Gaussian behavior and bursts in the tracer, pinpointing the occurrence of peak tracer variance when the zonal flow and shear flow phase speeds are equal. Through numerical experiments across various regimes, the study validates these findings and showcases how the velocity field and stochasticity influence tracer extremes. These results offer valuable insights into the mechanisms governing turbulent tracer transport, which can significantly impact uncertainty quantification and data assimilation in geophysical and environmental applications.<br /><br />Summary: <div>
arXiv:2505.21688v1 Announce Type: new 
Abstract: We study the statistical properties of passive tracer transport in turbulent flows with a mean gradient, emphasizing tracer intermittency and extreme events. An analytically tractable model is developed, coupling zonal and shear velocity components with both linear and nonlinear stochastic dynamics. Formulating the model in Fourier space, a simple explicit solution for the tracer invariant statistics is derived. Through this model we identify the resonance condition responsible for non-Gaussian behavior and bursts in the tracer. Resonant conditions, that lead to a peak in the tracer variance, occur when the zonal flow and the shear flow phase speeds are equivalent. Numerical experiments across a range of regimes, including different energy spectra and zonal flow models, are performed to validate these findings and demonstrate how the velocity field and stochasticity determines tracer extremes. These results provide additional insight into the mechanisms underlying turbulent tracer transport, with implications for uncertainty quantification and data assimilation in geophysical and environmental applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero</title>
<link>https://arxiv.org/abs/2505.21536</link>
<guid>https://arxiv.org/abs/2505.21536</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, circular economy, materials, thermodynamics, Python library
<br />
Summary: 
CiRL is a deep reinforcement learning library that focuses on the circularity of solid and fluid materials to address the challenges of achieving a net zero target. It integrates DRL into material circularity design using thermodynamical material networks. The library features CE-oriented environments in the state-space form, based on the Stable-Baselines3 Python library, and developed in Google Colaboratory for accessibility to researchers. By leveraging DRL algorithms in the context of circular economy, CiRL aims to support the transition towards a more sustainable and efficient use of finite raw materials in modern society. <div>
arXiv:2505.21536v1 Announce Type: cross 
Abstract: The demand of finite raw materials will keep increasing as they fuel modern society. Simultaneously, solutions for stopping carbon emissions in the short term are not available, thus making the net zero target extremely challenging to achieve at scale. The circular economy (CE) paradigm is gaining attention as a solution to address climate change and the uncertainties of supplies of critical materials. Hence, in this paper, we introduce CiRL, a deep reinforcement learning (DRL) library of environments focused on the circularity of both solid and fluid materials. The integration of DRL into the design of material circularity is possible thanks to the formalism of thermodynamical material networks, which is underpinned by compartmental dynamical thermodynamics. Along with the focus on circularity, this library has three more features: the new CE-oriented environments are in the state-space form, which is typically used in dynamical systems analysis and control designs; it is based on a state-of-the-art Python library of DRL algorithms, namely, Stable-Baselines3; and it is developed in Google Colaboratory to be accessible to researchers from different disciplines and backgrounds as is often the case for circular economy researchers and engineers. CiRL is publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-Capping Metric Evaluation for Improving Energy Efficiency</title>
<link>https://arxiv.org/abs/2505.21758</link>
<guid>https://arxiv.org/abs/2505.21758</guid>
<content:encoded><![CDATA[
<div> power-scaling management, resource utilization, energy-performance metrics, GPU power-capping, exascale applications

Summary:
This paper delves into the optimization of power-scaling management and resource utilization in high-performance computing systems running at exascale. By leveraging integrated CPU-GPU power management on architectures like the NVIDIA GH200 superchip, the study evaluates energy-performance metrics considering simultaneous CPU and GPU power-capping effects. Focusing on the Locally Self-Consistent Multiple Scattering (LSMS) application, the research identifies potential opportunities for energy savings in exascale applications. The results demonstrate that GPU task-specific dynamic power-cap adjustments, combined with integrated CPU-GPU power steering, can enhance energy utilization for certain GPU tasks. These findings lay the foundation for future adaptive optimization strategies in exascale computing, emphasizing the significance of even small reductions in energy consumption for overall system efficiency. <div>
arXiv:2505.21758v1 Announce Type: cross 
Abstract: With high-performance computing systems now running at exascale, optimizing power-scaling management and resource utilization has become more critical than ever. This paper explores runtime power-capping optimizations that leverage integrated CPU-GPU power management on architectures like the NVIDIA GH200 superchip. We evaluate energy-performance metrics that account for simultaneous CPU and GPU power-capping effects by using two complementary approaches: speedup-energy-delay and a Euclidean distance-based multi-objective optimization method. By targeting a mostly compute-bound exascale science application, the Locally Self-Consistent Multiple Scattering (LSMS), we explore challenging scenarios to identify potential opportunities for energy savings in exascale applications, and we recognize that even modest reductions in energy consumption can have significant overall impacts. Our results highlight how GPU task-specific dynamic power-cap adjustments combined with integrated CPU-GPU power steering can improve the energy utilization of certain GPU tasks, thereby laying the groundwork for future adaptive optimization strategies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2505.21887</link>
<guid>https://arxiv.org/abs/2505.21887</guid>
<content:encoded><![CDATA[
<div> Keywords: Robust routing, uncertainty, logistics, benchmark, stochastic dynamics 

Summary:
SVRPBench introduces an open benchmark for robust routing under uncertainty in logistics, specifically focusing on high-fidelity stochastic dynamics in vehicle routing at an urban scale. This benchmark comprises over 500 instances with realistic delivery conditions, including time-dependent congestion, delays, accidents, and time windows for customers. The dataset simulates constraint-rich scenarios such as multi-depot and multi-vehicle setups. Benchmarking results demonstrate that classical and metaheuristic methods outperform state-of-the-art RL solvers like POMO and AM under distributional shift. The findings highlight the importance of designing solvers that can generalize beyond synthetic assumptions and adapt to real-world uncertainty. The release of the dataset and evaluation suite enables reproducible research in the field of robust routing. <br /><br />Summary: SVRPBench provides an open benchmark for robust routing under uncertainty in real-world logistics, showcasing the challenges faced by state-of-the-art RL solvers and the need for adaptive solutions to dynamic stochastic conditions. <div>
arXiv:2505.21887v1 Announce Type: cross 
Abstract: Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</title>
<link>https://arxiv.org/abs/2505.21923</link>
<guid>https://arxiv.org/abs/2505.21923</guid>
<content:encoded><![CDATA[
<div> machine learning, analog circuits, topology selection, parameter inference, layout feasibility

Summary:<br />
The article introduces FALCON, a machine learning framework for automated analog circuit synthesis. FALCON guides the circuit design process by first selecting an appropriate circuit topology based on performance specifications. It then uses a graph neural network to infer circuit parameters and predict performance. The design process is constrained by layout considerations and design rules to ensure feasibility. FALCON was trained and evaluated on a large dataset of analog mm-wave circuits, demonstrating high accuracy in topology inference and performance prediction. The automated design process is efficient, completing in under 1 second per instance. Overall, FALCON shows promise as a practical and scalable tool for end-to-end analog circuit design automation.<br /> <div>
arXiv:2505.21923v1 Announce Type: cross 
Abstract: Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99\% accuracy in topology inference, <10\% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data</title>
<link>https://arxiv.org/abs/2505.22252</link>
<guid>https://arxiv.org/abs/2505.22252</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, cheminformatics, drug discovery, Explainable AI, Graph Neural Networks

Summary:
The study focuses on the importance of understanding the rationale behind predictions made by deep learning models in cheminformatics and drug discovery. Existing evaluation frameworks for Explainable AI (XAI) in this field lack real-world data and fail to capture the complexity of actual scenarios. To address this gap, the researchers introduce a new benchmark called B-XAIC, which uses real-world molecular data and diverse tasks with known ground-truth rationales. By evaluating XAI methods for Graph Neural Networks (GNNs) using B-XAIC, the study reveals the limitations of current methods in the molecular domain. This benchmark serves as a valuable resource for enhancing the faithfulness of explanations in XAI and improving the interpretability of models used in cheminformatics and drug discovery. <div>
arXiv:2505.22252v1 Announce Type: cross 
Abstract: Understanding the reasoning behind deep learning model predictions is crucial in cheminformatics and drug discovery, where molecular design determines their properties. However, current evaluation frameworks for Explainable AI (XAI) in this domain often rely on artificial datasets or simplified tasks, employing data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness. To address this, we introduce B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through a comprehensive evaluation using B-XAIC, we reveal limitations of existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain. This benchmark provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation</title>
<link>https://arxiv.org/abs/2505.22391</link>
<guid>https://arxiv.org/abs/2505.22391</guid>
<content:encoded><![CDATA[
<div> Keywords: generative modeling, diffusion models, PDE constraints, Physics-Informed Distillation, inverse problem solving<br />
Summary: 
Physics-based generative modeling offers advantages in handling partial observations and addressing both forward and inverse problems. Diffusion models have been increasingly used for modeling physical systems governed by partial differential equations (PDEs). However, a trade-off exists when enforcing PDE constraints on clean samples, leading to reduced generative accuracy. To address this, a post-hoc distillation approach called Physics-Informed Distillation of Diffusion Models (PIDDM) is proposed. This method enforces PDE constraints after the diffusion process, improving PDE satisfaction and supporting both forward and inverse problem solving. Experimental results demonstrate that PIDDM outperforms recent baselines like PIDM and DiffusionPDE, with lower computation overhead. The approach provides insights into more efficient ways of integrating physical constraints into diffusion models.<br /><br />Summary: <div>
arXiv:2505.22391v1 Announce Type: cross 
Abstract: Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2L Translation Operators for Kernel Independent Fast Multipole Methods on Modern Architectures</title>
<link>https://arxiv.org/abs/2408.07436</link>
<guid>https://arxiv.org/abs/2408.07436</guid>
<content:encoded><![CDATA[
<div> Multipole-to-Local Translation Operators, Kernel-independent Fast Multipole Method, Benchmarking, BLAS-based M2L, Randomized Low-rank Compression<br />
<br />
Summary:<br />
Hardware trends prioritize data reuse in algorithms. This study focuses on high-performance Multipole-to-Local (M2L) translation operators for the kernel-independent Fast Multipole Method (kiFMM). The traditional M2L approach is bandwidth-limited, presenting a bottleneck in the FMM. While FFT-based M2L implementations are efficient, they lack operational intensity and require specific optimizations. In contrast, BLAS-based M2L with randomized low-rank compression offers competitive performance, portability, and a simpler implementation leveraging existing BLAS infrastructure. A Rust-based implementation allows seamless strategy switching for fair benchmarking. CPU results show that FFT-based M2L excels in low-accuracy or dynamic simulations, whereas BLAS-based M2L is more effective in high-accuracy settings for static distributions, despite higher setup costs that are offset in many practical FMM applications. <br /> <div>
arXiv:2408.07436v4 Announce Type: replace 
Abstract: Hardware trends favor algorithm designs that maximize data reuse per FLOP. We develop and benchmark high-performance Multipole-to-Local (M2L) translation operators for the kernel-independent Fast Multipole Method (kiFMM), a widely adopted FMM variant that supports a broad class of kernels and has been favored by recent implementations for its simple specification. Naively implemented, M2L is bandwidth-limited and therefore a key bottleneck in the FMM. State-of-the-art FFT-based M2L implementations, though elegant and with a fast setup time, suffer from low operational intensity and require architecture-specific optimizations. We demonstrate that a BLAS-based M2L, combined with randomized low-rank compression, achieves competitive performance with greater portability and a simpler implementation leveraging existing BLAS infrastructure, at the cost of higher setup times-especially for high-accuracy settings in double precision. Our Rust-based implementation enables seamless switching between strategies for fair benchmarking. Results on CPUs show that FFT-based M2L is favorable in low-accuracy settings or dynamic particle simulations, while BLAS-based M2L is favored for high-accuracy settings for static particle distributions, where its higher setup costs are amortized in many practical applications of the FMM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving precision of A/B experiments using trigger intensity</title>
<link>https://arxiv.org/abs/2411.03530</link>
<guid>https://arxiv.org/abs/2411.03530</guid>
<content:encoded><![CDATA[
<div> randomized controlled experiment, A/B experiment, signal-to-noise ratio, sampling, bias

Summary: 
- Online randomized controlled experiments are widely used in industry to measure causal changes but often lack statistical significance due to low signal-to-noise ratios.
- Traditional methods focus on trigger observations where treatment and control models differ, leading to a costly process.
- A proposed sampling-based evaluation method reduces costs by introducing bias inversely proportional to the number of observations sampled.
- Simulation results show that bias effectively reduces to zero with a limited number of observations sampled.
- Empirical data demonstrates a 36.48% reduction in standard error with partial knowledge evaluation. 
<br /><br /> <div>
arXiv:2411.03530v2 Announce Type: replace-cross 
Abstract: In industry, online randomized controlled experiment (a.k.a. A/B experiment) is a standard approach to measure the impact of a causal change. These experiments have small treatment effect to reduce the potential blast radius. As a result, these experiments often lack statistical significance due to low signal-to-noise ratio. A standard approach for improving the precision (or reducing the standard error) focuses only on the trigger observations, where the output of the treatment and the control model are different. Although evaluation with full information about trigger observations (full knowledge) improves the precision, detecting all such trigger observations is a costly affair. In this paper, we propose a sampling based evaluation method (partial knowledge) to reduce this cost. The randomness of sampling introduces bias in the estimated outcome. We theoretically analyze this bias and show that the bias is inversely proportional to the number of observations used for sampling. We also compare the proposed evaluation methods using simulation and empirical data. In simulation, bias in evaluation with partial knowledge effectively reduces to zero when a limited number of observations (<= 0.1%) are sampled for trigger estimation. In empirical setup, evaluation with partial knowledge reduces the standard error by 36.48%.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIT-BO: High-Dimensional Bayesian Optimization with Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2505.20685</link>
<guid>https://arxiv.org/abs/2505.20685</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, high-dimensional spaces, Gradient-Informed Bayesian Optimization, Tabular Foundation Models, pre-trained model<br />
<br />
Summary: 
The paper introduces Gradient-Informed Bayesian Optimization using Tabular Foundation Models (GIT-BO) to address challenges in high-dimensional Bayesian optimization. GIT-BO leverages a pre-trained tabular foundation model to identify low-dimensional subspaces for optimization using gradient information. By creating a gradient-informed diagnostic matrix, the most sensitive directions of the model's predictions are identified for adaptive optimization without repeated model retraining. Extensive evaluation across 23 benchmarks shows GIT-BO outperforms existing Gaussian process-based methods in scalability and optimization performance, especially in high dimensions up to 500. This work showcases the effectiveness of foundation models with gradient-informed adaptive subspace identification as competitive alternatives for high-dimensional Bayesian optimization tasks. <br /> <div>
arXiv:2505.20685v1 Announce Type: new 
Abstract: Bayesian optimization (BO) effectively optimizes expensive black-box functions but faces significant challenges in high-dimensional spaces (dimensions exceeding 100) due to the curse of dimensionality. Existing high-dimensional BO methods typically leverage low-dimensional embeddings or structural assumptions to mitigate this challenge, yet these approaches frequently incur considerable computational overhead and rigidity due to iterative surrogate retraining and fixed assumptions. To address these limitations, we propose Gradient-Informed Bayesian Optimization using Tabular Foundation Models (GIT-BO), an approach that utilizes a pre-trained tabular foundation model (TFM) as a surrogate, leveraging its gradient information to adaptively identify low-dimensional subspaces for optimization. We propose a way to exploit internal gradient computations from the TFM's forward pass by creating a gradient-informed diagnostic matrix that reveals the most sensitive directions of the TFM's predictions, enabling optimization in a continuously re-estimated active subspace without the need for repeated model retraining. Extensive empirical evaluation across 23 synthetic and real-world benchmarks demonstrates that GIT-BO consistently outperforms four state-of-the-art Gaussian process-based high-dimensional BO methods, showing superior scalability and optimization performances, especially as dimensionality increases up to 500 dimensions. This work establishes foundation models, augmented with gradient-informed adaptive subspace identification, as highly competitive alternatives to traditional Gaussian process-based approaches for high-dimensional Bayesian optimization tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced and mixed precision turbulent flow simulations using explicit finite difference schemes</title>
<link>https://arxiv.org/abs/2505.20911</link>
<guid>https://arxiv.org/abs/2505.20911</guid>
<content:encoded><![CDATA[
<div> Keywords: reduced precision computing, mixed precision arithmetic, compressible turbulent flow simulations, explicit finite difference schemes, performance gains

Summary:
Reduced and mixed precision computing is being increasingly utilized in high-performance computing (HPC) for enhanced computational efficiency, especially on modern hardware like GPUs. The study focuses on applying mixed precision arithmetic in compressible turbulent flow simulations using explicit finite difference schemes. They modify the OPS and OpenSBLI frameworks to allow for customizable precision levels, enabling precise control over precision allocation for various tasks. Through numerical experiments on the Taylor-Green vortex benchmark, the researchers showcase significant performance improvements with mixed precision strategies like half-single and single-double combinations, maintaining numerical accuracy. Pure half-precision computations, however, exhibit unacceptable accuracy degradation, emphasizing the importance of careful precision selection. The study demonstrates that mixed precision configurations can decrease memory usage and communication overhead, resulting in noticeable speedups, particularly on multi-CPU and multi-GPU systems. 

<br /><br />Summary: <div>
arXiv:2505.20911v1 Announce Type: new 
Abstract: The use of reduced and mixed precision computing has gained increasing attention in high-performance computing (HPC) as a means to improve computational efficiency, particularly on modern hardware architectures like GPUs. In this work, we explore the application of mixed precision arithmetic in compressible turbulent flow simulations using explicit finite difference schemes. We extend the OPS and OpenSBLI frameworks to support customizable precision levels, enabling fine-grained control over precision allocation for different computational tasks. Through a series of numerical experiments on the Taylor-Green vortex benchmark, we demonstrate that mixed precision strategies, such as half-single and single-double combinations, can offer significant performance gains without compromising numerical accuracy. However, pure half-precision computations result in unacceptable accuracy loss, underscoring the need for careful precision selection. Our results show that mixed precision configurations can reduce memory usage and communication overhead, leading to notable speedups, particularly on multi-CPU and multi-GPU systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limitations of Nyquist Criteria in the Discretization of 2D Electromagnetic Integral Equations at High Frequency: Spectral Insights into Pollution Effects</title>
<link>https://arxiv.org/abs/2505.20942</link>
<guid>https://arxiv.org/abs/2505.20942</guid>
<content:encoded><![CDATA[
<div> Boundary Integral Equations, Boundary Element Methods, Spectral Analysis, Solution Accuracy, Electromagnetic Scattering<br />
<br />
Summary: The article discusses the use of boundary integral equations in modeling wave phenomena in various fields like elastic, acoustic, or electromagnetic. The focus is on analyzing the impact of Boundary Element Methods (BEMs) discretization on solution accuracy, particularly in electromagnetic scattering from a conducting cylinder. The study examines both ill-conditioned and well-conditioned equations, identifying a form of pollution affecting accuracy in different ways. The research proposes a solution strategy to mitigate this pollution problem. Through rigorous spectral analysis, the article provides deep insight into the root causes of numerical pollution in BEMs, highlighting the importance of understanding how discretization affects solution accuracy in boundary value problems. <div>
arXiv:2505.20942v1 Announce Type: new 
Abstract: The use of boundary integral equations in modeling boundary value problems-such as elastic, acoustic, or electromagnetic ones-is well established in the literature and widespread in practical applications. These equations are typically solved numerically using boundary element methods (BEMs), which generally provide accurate and reliable solutions. When the frequency of the wave phenomenon under study increases, the discretization of the problem is typically chosen to maintain a fixed number of unknowns per wavelength. Under these conditions, the BEM over finite-dimensional subspaces of piecewise polynomial basis functions is commonly believed to provide a bounded solution accuracy. If proven, this would constitute a significant advantage of the BEM with respect to finite element and finite difference time domain methods, which, in contrast, are affected by numerical pollution. In this work, we conduct a rigorous spectral analysis of some of the most commonly used boundary integral operators and examine the impact of the BEM discretization on the solution accuracy of widely used integral equations modeling two-dimensional electromagnetic scattering from a perfectly electrically conducting cylinder. We consider both ill-conditioned and well-conditioned equations, the latter being characterized by solution operators bounded independently of frequency. Our analysis, which is capable of tracking the effects of BEM discretization on compositions and sums of different operators, reveals a form of pollution that affects, in different measures, equations of both kinds. After elucidating the mechanism by which the BEM discretization impacts accuracy, we propose a solution strategy that can cure the pollution problem thus evidenced. The defining strength of the proposed theoretical model lies in its capacity to deliver deep insight into the root causes of the phenomenon.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of the Past: An AI-Enabled Pipeline for Traffic Simulation from Noisy, Multimodal Detector Data and Stakeholder Feedback</title>
<link>https://arxiv.org/abs/2505.21349</link>
<guid>https://arxiv.org/abs/2505.21349</guid>
<content:encoded><![CDATA[
<div> computer vision, combinatorial optimization, large language models, traffic simulation, data-driven

Summary:
The article introduces a new approach to designing traffic simulations that accurately reflect real-world traffic conditions. The proposed pipeline involves three steps: using computer vision for vehicle counting from camera footage, applying combinatorial optimization for vehicle route generation from multimodal data, and utilizing large language models for iterative simulation refinement based on natural language feedback. Through testing on a road network in Strongsville, Ohio, the framework successfully captures the city's traffic patterns in a detailed simulation. The pipeline's flexibility allows for generalization to other municipalities with varying levels of data and infrastructure availability. <div>
arXiv:2505.21349v1 Announce Type: new 
Abstract: How can a traffic simulation be designed to faithfully reflect real-world traffic conditions? Past data-driven approaches to traffic simulation in the literature have relied on unrealistic or suboptimal heuristics. They also fail to adequately account for the effects of uncertainty and multimodality in the data on simulation outcomes. In this work, we integrate advances in AI to construct a three-step, end-to-end pipeline for generating a traffic simulation from detector data: computer vision for vehicle counting from camera footage, combinatorial optimization for vehicle route generation from multimodal data, and large language models for iterative simulation refinement from natural language feedback. Using a road network from Strongsville, Ohio as a testbed, we demonstrate that our pipeline can accurately capture the city's traffic patterns in a granular simulation. Beyond Strongsville, our traffic simulation framework can be generalized to other municipalities with different levels of data and infrastructure availability.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information</title>
<link>https://arxiv.org/abs/2505.20650</link>
<guid>https://arxiv.org/abs/2505.20650</guid>
<content:encoded><![CDATA[
<div> FinTagging, XBRL benchmark, large language models, structured information extraction, semantic alignment <br />
Summary: <br />
FinTagging is introduced as a comprehensive XBRL benchmark for evaluating language models' capabilities in financial reporting. It consists of two subtasks, FinNI for entity extraction and FinCL for concept alignment, emphasizing the extraction and alignment of facts within the US-GAAP taxonomy. Large language models exhibit strong extraction abilities but struggle with fine-grained concept alignment, especially in distinguishing closely related taxonomy entries. The study underscores the importance of improved semantic reasoning and schema-aware modeling for accurate financial disclosure. The code is available on GitHub, and the data can be accessed through the Hugging Face repository. <div>
arXiv:2505.20650v1 Announce Type: cross 
Abstract: We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction</title>
<link>https://arxiv.org/abs/2505.21109</link>
<guid>https://arxiv.org/abs/2505.21109</guid>
<content:encoded><![CDATA[
<div> adaptation, language models, hallucination issues, Small Language Graph, lightweight

Summary:<br />
- The study addresses challenges in adapting large language models, focusing on reducing computational resources and minimizing hallucination issues.
- The Small Language Graph (SLG) approach, based on a graph structure with lightweight expert nodes, outperformed traditional fine-tuning methods by 3 times on the Exact Match metric.
- SLG also demonstrated a 1.7 times faster fine-tuning process compared to stand-alone models.
- The findings suggest that SLG could enable small to medium-sized engineering companies to leverage generative AI technologies without the need for expensive computational resources.
- The graph architecture and small expert nodes offer potential for distributed AI systems, potentially reducing the reliance on costly centralized compute clusters. 

<br /><br /> <div>
arXiv:2505.21109v1 Announce Type: cross 
Abstract: Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap Between Data-Driven And Theory-Driven Modelling - Leveraging Causal Machine Learning for Integrative Modelling of Dynamical Systems</title>
<link>https://arxiv.org/abs/2410.09516</link>
<guid>https://arxiv.org/abs/2410.09516</guid>
<content:encoded><![CDATA[
<div> causal feature selection, domain knowledge, time-series data, machine learning, predictive robustness<br />
<br />
Summary: 
This study explores the use of causal feature selection with domain knowledge in improving machine learning applications, specifically in the context of a data center system. Traditional machine learning techniques often face challenges of overfitting and unreliable predictions in novel conditions. By incorporating causality into the modeling process, predictive robustness can be enhanced. The study compares causal feature selection with traditional feature selection methods using simulated time-series data. Results show that predictions based on causal features are more robust, highlighting the potential benefits of combining causal discovery algorithms with human expertise. This approach can help address the time-consuming process of manually constructing causal graphs, particularly in complex time series with numerous variables. <div>
arXiv:2410.09516v3 Announce Type: replace 
Abstract: Classical machine learning techniques often struggle with overfitting and unreliable predictions when exposed to novel conditions. Introducing causality into the modelling process offers a promising way to mitigate these challenges by enhancing predictive robustness. However, constructing an initial causal graph manually using domain knowledge is time-consuming, particularly in complex time series with numerous variables. To address this, causal discovery algorithms can provide a preliminary causal structure that domain experts can refine. This study investigates causal feature selection with domain knowledge using a data center system as an example. We use simulated time-series data to compare different causal feature selection with traditional machine-learning feature selection methods. Our results show that predictions based on causal features are more robust compared to those derived from traditional methods. These findings underscore the potential of combining causal discovery algorithms with human expertise to improve machine learning applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fusion with Relational Learning for Molecular Property Prediction</title>
<link>https://arxiv.org/abs/2410.12128</link>
<guid>https://arxiv.org/abs/2410.12128</guid>
<content:encoded><![CDATA[
<div> Graph-based molecular representation learning, multimodal fusion, relational learning, drug discovery, materials science
<br />
Summary: 
MMFRL (Multimodal Fusion with Relational Learning for Molecular Property Prediction) is introduced as a novel framework to enhance graph-based molecular representation learning. It addresses challenges in molecular property prediction by incorporating multimodal fusion at different stages such as early, intermediate, and late. The method improves embedding initialization through multimodal pretraining using relational learning. Extensive experiments on MoleculeNet benchmarks show that MMFRL outperforms existing methods significantly, allowing for task-specific optimizations. The explainability of MMFRL provides valuable chemical insights, enhancing its potential for real-world drug discovery applications. <div>
arXiv:2410.12128v2 Announce Type: replace 
Abstract: Graph based molecular representation learning is essential for accurately predicting molecular properties in drug discovery and materials science; however, it faces significant challenges due to the intricate relationships among molecules and the limited chemical knowledge utilized during training. While contrastive learning is often employed to handle molecular relationships, its reliance on binary metrics is insufficient for capturing the complexity of these interactions. Multimodal fusion has gained attention for property reasoning, but previous work has explored only a limited range of modalities, and the optimal stages for fusing different modalities in molecular property tasks remain underexplored. In this paper, we introduce MMFRL (Multimodal Fusion with Relational Learning for Molecular Property Prediction), a novel framework designed to overcome these limitations. Our method enhances embedding initialization through multimodal pretraining using relational learning. We also conduct a systematic investigation into the impact of modality fusion at different stages such as early, intermediate, and late, highlighting their advantages and shortcomings. Extensive experiments on MoleculeNet benchmarks demonstrate that MMFRL significantly outperforms existing methods. Furthermore, MMFRL enables task-specific optimizations. Additionally, the explainability of MMFRL provides valuable chemical insights, emphasizing its potential to enhance real-world drug discovery applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A batch production scheduling problem in a reconfigurable hybrid manufacturing-remanufacturing system</title>
<link>https://arxiv.org/abs/2504.00605</link>
<guid>https://arxiv.org/abs/2504.00605</guid>
<content:encoded><![CDATA[
<div> sustainability, remanufacturing, Hybrid Manufacturing-Remanufacturing System, Reconfigurable Manufacturing System, production scheduling

Summary:
The study focuses on production scheduling in a Hybrid Manufacturing-Remanufacturing System (HMRS) with non-identical parallel reconfigurable machines and batch orders. Models using Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) are developed, with a computationally efficient Logic-based Benders Decomposition (LBBD) method for solution. The LBBD approach outperforms MILP, CP, and warm-started MILP models, achieving an average gap of about 2%. The study highlights the benefits of utilizing Reconfigurable Manufacturing System (RMS) technologies in HMRSs and provides actionable managerial insights for scheduling in such systems. It addresses the complexity of production management in HMRSs and showcases the importance of customized production capabilities for increased flexibility and efficiency in processing both new products and End-of-Life (EOL) products in a shared facility. 

<br /><br />Summary: <div>
arXiv:2504.00605v2 Announce Type: replace 
Abstract: In recent years, remanufacturing of End-of-Life (EOL) products has been adopted by manufacturing sectors as a competent practice to enhance their sustainability and market share. Due to the mass customization of products and high volatility of market, processing of new products and remanufacturing of EOLs in the same shared facility, namely Hybrid Manufacturing-Remanufacturing System (HMRS), is a mean to keep such production efficient. Accordingly, customized production capabilities are required to increase flexibility, which can be effectively provided under the Reconfigurable Manufacturing System (RMS) paradigm. Despite the advantages of utilizing RMS technologies in HMRSs, production management of such systems suffers excessive complexity. Hence, this study concentrates on the production scheduling of an HMRS consisting of non-identical parallel reconfigurable machines where the orders can be grouped into batches. In this regard, Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) models are devised to formulate the problem. Furthermore, a computationally efficient solution method is developed based on a Logic-based Benders Decomposition (LBBD) approach. The warm start technique is also implemented by providing a decent initial solution to the MILP model. Computational experiments attest to the LBBD method's superiority over the MILP, CP, and warm-started MILP models by obtaining an average gap of about 2%, besides it yields actionable managerial insights for scheduling in HMRSs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-Enhanced Monte Carlo: A Machine Learning View on Control Variate</title>
<link>https://arxiv.org/abs/2412.11257</link>
<guid>https://arxiv.org/abs/2412.11257</guid>
<content:encoded><![CDATA[
<div> ML, Monte Carlo, Prediction-Enhanced, Variance Reduction, Simulation

Summary:
Prediction-Enhanced Monte Carlo (PEMC) is introduced as a framework that uses machine learning models as predictors to reduce variance and runtime in complex simulation tasks. It overcomes the computational challenges of traditional Monte Carlo methods by leveraging modern ML surrogates for unbiased evaluation. PEMC acts as a modernized control variate approach, considering overall computation-cost-aware variance reduction. It demonstrates efficacy in various scenarios, including equity derivatives, interest rate derivatives, and healthcare decision-making processes. In equity derivatives, it reduces variance for variance swaps under stochastic local volatility models. In interest rate derivatives, it enhances swaption pricing under the HJM interest-rate model. In healthcare decision-making, it assists in ambulance dispatch and hospital load balancing by providing accurate mortality rate estimates. PEMC consistently improves performance by reducing variance while maintaining unbiasedness, highlighting its potential as a valuable enhancement to standard Monte Carlo methods. 

<br /><br />Summary: 
Keywords: ML, Monte Carlo, Prediction-Enhanced, Variance Reduction, Simulation <div>
arXiv:2412.11257v2 Announce Type: replace-cross 
Abstract: For many complex simulation tasks spanning areas such as healthcare, engineering, and finance, Monte Carlo (MC) methods are invaluable due to their unbiased estimates and precise error quantification. Nevertheless, Monte Carlo simulations often become computationally prohibitive, especially for nested, multi-level, or path-dependent evaluations lacking effective variance reduction techniques. While machine learning (ML) surrogates appear as natural alternatives, naive replacements typically introduce unquantifiable biases. We address this challenge by introducing Prediction-Enhanced Monte Carlo (PEMC), a framework that leverages modern ML models as learned predictors, using cheap and parallelizable simulation as features, to output unbiased evaluation with reduced variance and runtime. PEMC can also be viewed as a "modernized" view of control variates, where we consider the overall computation-cost-aware variance reduction instead of per-replication reduction, while bypassing the closed-form mean function requirement and maintaining the advantageous unbiasedness and uncertainty quantifiability of Monte Carlo.
  We illustrate PEMC's broader efficacy and versatility through three examples: first, equity derivatives such as variance swaps under stochastic local volatility models; second, interest rate derivatives such as swaption pricing under the Heath-Jarrow-Morton (HJM) interest-rate model. Finally, we showcase PEMC in a socially significant context - ambulance dispatch and hospital load balancing - where accurate mortality rate estimates are key for ethically sensitive decision-making. Across these diverse scenarios, PEMC consistently reduces variance while preserving unbiasedness, highlighting its potential as a powerful enhancement to standard Monte Carlo baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Beyond Words: MatVQA for Challenging Visual-Scientific Reasoning in Materials Science</title>
<link>https://arxiv.org/abs/2505.18319</link>
<guid>https://arxiv.org/abs/2505.18319</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Materials Science, MatVQA, Scientific Reasoning, Benchmarking

Summary:
MatVQA is a new benchmark designed to address the limitations of current materials science evaluation datasets by integrating visual and language modalities for scientific reasoning. The dataset features 1325 questions across four structure-property-performance reasoning tasks, encouraging MLLMs to perform fine-grained visual analysis of material imagery. By benchmarking 17 MLLMs on MatVQA, significant gaps in multimodal reasoning capabilities were revealed. The benchmark data and evaluation code are publicly available to facilitate further research in applying MLLMs to complex materials science problems.<br /><br />Summary: <div>
arXiv:2505.18319v1 Announce Type: new 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) that integrate vision and language modalities has unlocked new potentials for scientific reasoning, outperforming prior benchmarks in both natural language and coding domains. Current materials science evaluation datasets such as MaScQA and SciQA remain largely text-based and fail to capture the visual and research-level analytic complexity required in materials discovery and design. We introduce MatVQA, a scalable benchmark specifically designed to address this gap. Generated via an automated pipeline, MArxivAgent, from recent materials literature, MatVQA features 1325 questions across four critical structure-property-performance (SPP) reasoning tasks. Uniquely, MatVQA employs an iterative process to eliminate textual shortcuts, compelling MLLMs to perform fine-grained, low-level visual analysis of material imagery (e.g., microscopy, diffraction patterns) integrated with multi-step scientific reasoning. Benchmarking 17 open- and closed-source MLLMs on MatVQA reveals substantial gaps in current multimodal reasoning capabilities. MatVQA benchmark data, along with evaluation code, is publicly available in \href{https://anonymous.4open.science/r/matvqa-1E01}{https://anonymous.4open.science/r/matvqa-1E01/README.md} to catalyze further research in applying MLLMs to complex materials science problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AERO: An autonomous platform for continuous research</title>
<link>https://arxiv.org/abs/2505.18408</link>
<guid>https://arxiv.org/abs/2505.18408</guid>
<content:encoded><![CDATA[
<div> automated research, data infrastructure, collaboration, epidemiology, public health

Summary: 
The article introduces AERO, an automated research and data sharing platform developed to support cross-sector investigations during the COVID-19 pandemic. AERO allows for the automatic ingestion, validation, and transformation of monitored data for analysis, as well as the execution of analyses and data sharing among different entities. Leveraging capabilities from the Globus platform and GitHub, AERO facilitates automation, distributed execution, data sharing, and authentication. The implementation of AERO has been demonstrated with two public health surveillance applications and benchmarking with a synthetic application. Users can access these applications for testing purposes. AERO addresses the need for new data infrastructure in rapidly evolving situations, such as public health emergencies, to enable continuous, distributed, and multi-disciplinary collaboration. <div>
arXiv:2505.18408v1 Announce Type: new 
Abstract: The COVID-19 pandemic highlighted the need for new data infrastructure, as epidemiologists and public health workers raced to harness rapidly evolving data, analytics, and infrastructure in support of cross-sector investigations. To meet this need, we developed AERO, an automated research and data sharing platform for continuous, distributed, and multi-disciplinary collaboration. In this paper, we describe the AERO design and how it supports the automatic ingestion, validation, and transformation of monitored data into a form suitable for analysis; the automated execution of analyses on this data; and the sharing of data among different entities. We also describe how our AERO implementation leverages capabilities provided by the Globus platform and GitHub for automation, distributed execution, data sharing, and authentication. We present results obtained with an instance of AERO running two public health surveillance applications and demonstrate benchmarking results with a synthetic application, all of which are publicly available for testing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Equilibrium: Non-Equilibrium Foundations Should Underpin Generative Processes in Complex Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.18621</link>
<guid>https://arxiv.org/abs/2505.18621</guid>
<content:encoded><![CDATA[
<div> Generative models, non-equilibrium physics, complex dynamical systems, empirical experiments, scientific modeling <br />
<br />
Generative models inspired by non-equilibrium physics are proposed as essential for better modeling complex dynamical systems due to limitations of classical equilibrium-based models. These non-equilibrium frameworks naturally capture evolving distributions and non-stationary behavior. Empirical experiments on a dynamic system validate the effectiveness of non-equilibrium generative models in tracking temporal evolution and adapting to changing landscapes. Future directions include integrating non-equilibrium principles with generative AI to simulate rare events, infer underlying mechanisms, and represent multi-scale dynamics across scientific domains. Embracing non-equilibrium physics is deemed necessary for generative AI to serve as a scientific modeling tool, offering new capabilities for simulating, understanding, and controlling complex systems.<br /><br />Summary: <div>
arXiv:2505.18621v1 Announce Type: new 
Abstract: This position paper argues that next-generation non-equilibrium-inspired generative models will provide the essential foundation for better modeling real-world complex dynamical systems. While many classical generative algorithms draw inspiration from equilibrium physics, they are fundamentally limited in representing systems with transient, irreversible, or far-from-equilibrium behavior. We show that non-equilibrium frameworks naturally capture non-equilibrium processes and evolving distributions. Through empirical experiments on a dynamic Printz potential system, we demonstrate that non-equilibrium generative models better track temporal evolution and adapt to non-stationary landscapes. We further highlight future directions such as integrating non-equilibrium principles with generative AI to simulate rare events, inferring underlying mechanisms, and representing multi-scale dynamics across scientific domains. Our position is that embracing non-equilibrium physics is not merely beneficial--but necessary--for generative AI to serve as a scientific modeling tool, offering new capabilities for simulating, understanding, and controlling complex systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A high-order matrix-free adaptive solver for the shallow water equations with irregular bathymetry</title>
<link>https://arxiv.org/abs/2505.18743</link>
<guid>https://arxiv.org/abs/2505.18743</guid>
<content:encoded><![CDATA[
<div> AMR, coastal engineering, Discontinuous Galerkin method, deal.II library, parallelization<br />
<br />
Summary: <br />
The article introduces a new Adaptive Mesh Refinement (AMR) solver for coastal engineering applications. It is based on the Discontinuous Galerkin (DG) method and implemented in the deal.II library, offering efficient parallelization and handling of non-conforming meshes. The method is well-balanced, adaptable to realistic bathymetry data without regularity assumptions, and conservatively discretizes transported chemical species. Idealized benchmarks validate the approach, demonstrating its potential for accurate and efficient adaptive simulations of coastal flows. The solver's capabilities are further evidenced through experiments on realistic bathymetries and complex domains. <div>
arXiv:2505.18743v1 Announce Type: new 
Abstract: We present the first step in the development of an Adaptive Mesh Refinement (AMR) solver for coastal engineering applications, based on a high-order Discontinuous Galerkin (DG) method as implemented in the deal.II library. This environment provides efficient and native parallelization techniques and automatically handles non-conforming meshes to implement both static and dynamic AMR approaches. The proposed method is automatically well-balanced, allows the use of realistic bathymetry data without any regularity assumption, and includes a consistent conservative discretization for transported chemical species. Numerical experiments on idealized benchmarks validate the proposed approach, while results obtained on realistic bathymetries and complex domains show its potential for accurate and efficient adaptive simulations of coastal flows.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoofNet: A Global Multimodal Dataset for Roof Material Classification</title>
<link>https://arxiv.org/abs/2505.19358</link>
<guid>https://arxiv.org/abs/2505.19358</guid>
<content:encoded><![CDATA[
<div> Dataset, RoofNet, Earth Observation imagery, roofing types, global exposure datasets
<br />
Summary: 
RoofNet is a new dataset that combines Earth Observation imagery with curated text annotations to classify roofing materials in buildings around the world. With over 51,500 samples from 184 sites, RoofNet labels 14 key roofing types, such as asphalt shingles and clay tiles, to enhance global exposure datasets. By fine-tuning a vision-language model on a subset of annotated images, RoofNet provides accurate predictions on roofing materials across different regions. The dataset includes rich metadata like roof shape, footprint area, solar panel presence, and mixed materials indicators. RoofNet's AI-driven risk assessment capabilities make it valuable for insurance underwriting, disaster preparedness, and infrastructure policy planning. It serves as a benchmark for evaluating model generalization and offers insights for decision-making in various sectors. 
<br /> <div>
arXiv:2505.19358v1 Announce Type: new 
Abstract: Natural disasters are increasing in frequency and severity, causing hundreds of billions of dollars in damage annually and posing growing threats to infrastructure and human livelihoods. Accurate data on roofing materials is critical for modeling building vulnerability to natural hazards such as earthquakes, floods, wildfires, and hurricanes, yet such data remain unavailable. To address this gap, we introduce RoofNet, the largest and most geographically diverse novel multimodal dataset to date, comprising over 51,500 samples from 184 geographically diverse sites pairing high-resolution Earth Observation (EO) imagery with curated text annotations for global roof material classification. RoofNet includes geographically diverse satellite imagery labeled with 14 key roofing types -- such as asphalt shingles, clay tiles, and metal sheets -- and is designed to enhance the fidelity of global exposure datasets through vision-language modeling (VLM). We sample EO tiles from climatically and architecturally distinct regions to construct a representative dataset. A subset of 6,000 images was annotated in collaboration with domain experts to fine-tune a VLM. We used geographic- and material-aware prompt tuning to enhance class separability. The fine-tuned model was then applied to the remaining EO tiles, with predictions refined through rule-based and human-in-the-loop verification. In addition to material labels, RoofNet provides rich metadata including roof shape, footprint area, solar panel presence, and indicators of mixed roofing materials (e.g., HVAC systems). RoofNet supports scalable, AI-driven risk assessment and serves as a downstream benchmark for evaluating model generalization across regions -- offering actionable insights for insurance underwriting, disaster preparedness, and infrastructure policy planning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Finite Element Neural Network (IFENN) for Phase-Field Fracture with Minimal Input and Generalized Geometry-Load Handling</title>
<link>https://arxiv.org/abs/2505.19566</link>
<guid>https://arxiv.org/abs/2505.19566</guid>
<content:encoded><![CDATA[
<div> novel formulation, phase-field fracture propagation, Integrated Finite Element Neural Network (IFENN), physics-informed convolutional networks (PICNNs), unsupervised training<br />
Summary:<br />
The article presents a new approach for modeling phase-field fracture propagation using the Integrated Finite Element Neural Network (IFENN) framework. This hybrid solver scheme combines neural networks as PDE solvers within FEM, improving accuracy and speed of predictions. The novel formulation involves using physics-informed convolutional networks (PICNNs) to calculate the phase-field variable while solving equilibrium equations with FEM. By eliminating temporal features and focusing on spatial coupling between strain energy density and phase-field variable, the training process is significantly reduced to just 5 minutes. The trained PICNN embedded within IFENN can simulate crack propagation in various scenarios with high accuracy, including rectangular domains, multiple interacting cracks, and different mesh densities. This breakthrough in hybrid modeling offers a physics-consistent solution for fracture and coupled problems. <br /><br />Summary: <div>
arXiv:2505.19566v1 Announce Type: new 
Abstract: We present a novel formulation for modeling phase-field fracture propagation based on the Integrated Finite Element Neural Network (IFENN) framework. IFENN is a hybrid solver scheme that utilizes neural networks as PDE solvers within FEM, preserving accuracy via residual minimization while achieving speed-up via swift network predictions and reduction of the size of system of equations in coupled problems. In this work, we introduce a radically new formulation of IFENN in which the phase-field variable is calculated using physics-informed convolutional networks (PICNNs), while the equilibrium equation is still solved using FEM to maintain the solver robustness. Unlike conventional approaches, which rely on sequence or time-dependent models, we eliminate the need to include temporal features in the training setup and inference stage. Instead, we show that it is sufficient to learn only the spatial coupling between the strain energy density and the phase-field variable in the vicinity of the fracture process zone, and utilize this information along the advancing crack simulation. We train a single CNN in a purely physics-based, unsupervised manner on just two load increments from a single-notch tension problem, with a total training time of only 5 minutes. Following this exceptionally minimal and fast training, we show that the same PICNN can (when embedded within IFENN) model crack propagation in a very wide range of unseen scenarios, including arbitrarily rectangular domains, single and multiple interacting cracks, varying mesh densities, and arbitrary loading paths. The proposed formulation delivers breakthroughs that address many of the limitations in the existing literature of hybrid modeling, introducing a new paradigm for the development of generalizable, physics-consistent hybrid models that are applicable to fracture and other coupled problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets</title>
<link>https://arxiv.org/abs/2505.19819</link>
<guid>https://arxiv.org/abs/2505.19819</guid>
<content:encoded><![CDATA[
<div> datasets, LoRA methods, financial tasks, performance gains, democratize financial intelligence <br />
<br />
Keywords: datasets, LoRA methods, financial tasks, performance gains, democratize financial intelligence <br />
<br />
Summary: 
The paper introduces the FinLoRA project, which explores the efficacy of low-rank adaptation (LoRA) methods in high-stakes financial domains. The project curated 19 datasets covering various financial applications, including four novel XBRL analysis datasets based on SEC filings. Five LoRA methods and five base Large Language Models (LLMs) were evaluated, with LoRA methods showing substantial performance gains averaging 36% over base models. Extensive experimental results in terms of accuracy, F1, and BERTScore were provided, along with information on computational costs during fine-tuning and inference stages. The FinLoRA project aims to democratize financial intelligence by providing an affordable and scalable approach for the general public. The datasets, LoRA adapters, code, and documentation are available on GitHub at https://github.com/Open-Finance-Lab/FinLoRA. <br /> <div>
arXiv:2505.19819v1 Announce Type: new 
Abstract: Low-rank adaptation (LoRA) methods show great potential for scaling pre-trained general-purpose Large Language Models (LLMs) to hundreds or thousands of use scenarios. However, their efficacy in high-stakes domains like finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings. In this paper, we present the open-source FinLoRA project that benchmarks LoRA methods on both general and highly professional financial tasks. First, we curated 19 datasets covering diverse financial applications; in particular, we created four novel XBRL analysis datasets based on 150 SEC filings. Second, we evaluated five LoRA methods and five base LLMs. Finally, we provide extensive experimental results in terms of accuracy, F1, and BERTScore and report computational cost in terms of time and GPU memory during fine-tuning and inference stages. We find that LoRA methods achieved substantial performance gains of 36\% on average over base models. Our FinLoRA project provides an affordable and scalable approach to democratize financial intelligence to the general public. Datasets, LoRA adapters, code, and documentation are available at https://github.com/Open-Finance-Lab/FinLoRA
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision</title>
<link>https://arxiv.org/abs/2505.18181</link>
<guid>https://arxiv.org/abs/2505.18181</guid>
<content:encoded><![CDATA[
<div> dataset, machine learning, NMR spectroscopy, molecular representation, annotated

Summary:
The article introduces 2DNMRGym, a dataset for machine learning-based molecular representation learning in two-dimensional Nuclear Magnetic Resonance (NMR) spectroscopy. The dataset contains over 22,000 Heteronuclear Single Quantum Coherence (HSQC) spectra along with corresponding molecular graphs and SMILES strings. Utilizing a surrogate supervision setup, models are trained on algorithm-generated annotations and evaluated on human-annotated gold-standard labels, allowing for rigorous assessment of model generalization. Benchmark results using 2D and 3D Graph Neural Network (GNN) and GNN transformer models demonstrate promising performance. This dataset supports scalable model training and provides a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. The data and code are open-source and available on Huggingface and Github. 

<br /><br />Summary: <div>
arXiv:2505.18181v1 Announce Type: cross 
Abstract: Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy, particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays a critical role in elucidating molecular structures, interactions, and electronic properties. However, accurately interpreting 2D NMR data remains labor-intensive and error-prone, requiring highly trained domain experts, especially for complex molecules. Machine Learning (ML) holds significant potential in 2D NMR analysis by learning molecular representations and recognizing complex patterns from data. However, progress has been limited by the lack of large-scale and high-quality annotated datasets. In this work, we introduce 2DNMRGym, the first annotated experimental dataset designed for ML-based molecular representation learning in 2D NMR. It includes over 22,000 HSQC spectra, along with the corresponding molecular graphs and SMILES strings. Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained using algorithm-generated annotations derived from a previously validated method and evaluated on a held-out set of human-annotated gold-standard labels. This enables rigorous assessment of a model's ability to generalize from imperfect supervision to expert-level interpretation. We provide benchmark results using a series of 2D and 3D GNN and GNN transformer models, establishing a strong foundation for future work. 2DNMRGym supports scalable model training and introduces a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. Our data and code is open-source and available on Huggingface and Github.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Optimization Algorithms for Energy Management Systems in Microgrids: A Control Strategy Based on a PHIL System</title>
<link>https://arxiv.org/abs/2505.18210</link>
<guid>https://arxiv.org/abs/2505.18210</guid>
<content:encoded><![CDATA[
<div> optimization, microgrid, power hardware, renewable energy sources, energy management

Summary:<br />
- An adaptive multi-objective optimization approach was implemented in a real-time power hardware-in-loop configuration for a microgrid with various energy resources.
- The approach effectively balanced factors such as fuel consumption, load mismatch, power quality, battery degradation, and the use of renewable energy sources.
- Real-time experimental data was used for dynamic system state updates, with adaptive preference-based selection adjusted based on battery charging thresholds.
- The technique integrated six technical objectives and complex constraints, aiding in practical microgrid decision making and dynamic energy system optimization.
- The energy management process successfully maximized photovoltaic production, minimized power mismatch, and stabilized battery state of charge in different conditions, outperforming a baseline system without optimization techniques.<br /><br /> <div>
arXiv:2505.18210v1 Announce Type: cross 
Abstract: In this research a real time power hardware in loop configuration has been implemented for an microgrid with the combination of distribution energy resources such as photovoltaic, grid tied inverter, battery, utility grid, and a diesel generator. This paper introduces an unique adaptive multi-objective optimization approach that employs weighted optimization techniques for real-time microgrid systems. The aim is to effectively balance various factors including fuel consumption, load mismatch, power quality, battery degradation, and the utilization of renewable energy sources. A real time experimental data from power hardware in loop system has been used for dynamically updating system states. The adaptive preference-based selection method are adjusted based on state of battery charging thresholds. The technique has been integrated with six technical objectives and complex constraints. This approach helps to practical microgrid decision making and optimization of dynamic energy systems. The energy management process were also able to maximize photovoltaic production where minimizing power mismatch, stabilizing battery state of charge under different condition. The research results were also compared with the baseline system without optimization techniques, and a reliable outcome was found.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
<div> PINNs, immersed boundary method, fluid-structure interaction, neural network architectures, adaptive activation functions <br />
Summary:
Neural network architectures combining physics-informed neural networks (PINNs) with the immersed boundary method (IBM) were proposed for fluid-structure interaction (FSI) problems. Two architectures, Single-FSI and Eulerian-Lagrangian, were compared using standard Tanh and adaptive B-spline activation functions. The Eulerian-Lagrangian architecture showed superior performance, with the adaptive B-spline activation improving accuracy near boundaries. While velocity field prediction was successful, pressure recovery proved challenging without explicit force-coupling constraints. The study emphasized the importance of domain-specific architectural design and adaptive activation functions in modeling FSI within the PINN framework. <br /> <div>
arXiv:2505.18565v1 Announce Type: cross 
Abstract: We introduce neural network architectures that combine physics-informed neural networks (PINNs) with the immersed boundary method (IBM) to solve fluid-structure interaction (FSI) problems. Our approach features two distinct architectures: a Single-FSI network with a unified parameter space, and an innovative Eulerian-Lagrangian network that maintains separate parameter spaces for fluid and structure domains. We study each architecture using standard Tanh and adaptive B-spline activation functions. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that the Eulerian-Lagrangian architecture performs significantly better. The adaptive B-spline activation further enhances accuracy by providing locality-aware representation near boundaries. While our methodology shows promising results in predicting the velocity field, pressure recovery remains challenging due to the absence of explicit force-coupling constraints in the current formulation. Our findings underscore the importance of domain-specific architectural design and adaptive activation functions for modeling FSI problems within the PINN framework.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete gradient methods for port-Hamiltonian differential-algebraic equations</title>
<link>https://arxiv.org/abs/2505.18810</link>
<guid>https://arxiv.org/abs/2505.18810</guid>
<content:encoded><![CDATA[
<div> discrete gradient methods, nonlinear port-Hamiltonian differential-algebraic equations, numerical scheme, Dirac-dissipative structures, multibody system dynamics 

Summary: 
Discrete gradient methods are effective for time discretization of dynamical systems, preserving structure regardless of the total energy form. This study focuses on applying these methods to nonlinear port-Hamiltonian differential-algebraic equations, commonly used in modeling physical systems. A novel numerical scheme is introduced for semi-explicit differential-algebraic equations, with the utilization of discrete gradient pairs and Dirac-dissipative structures in general settings. The behavior under system transformations is analyzed, showing that these equations can be represented as a parametrized port-Hamiltonian semi-explicit system and an unstructured equation under specific conditions. The application to multibody system dynamics is demonstrated through numerical results, showcasing the efficiency of the approach. <div>
arXiv:2505.18810v1 Announce Type: cross 
Abstract: Discrete gradient methods are a powerful tool for the time discretization of dynamical systems, since they are structure-preserving regardless of the form of the total energy. In this work, we discuss the application of discrete gradient methods to the system class of nonlinear port-Hamiltonian differential-algebraic equations - as they emerge from the port- and energy-based modeling of physical systems in various domains. We introduce a novel numerical scheme tailored for semi-explicit differential-algebraic equations and further address more general settings using the concepts of discrete gradient pairs and Dirac-dissipative structures. Additionally, the behavior under system transformations is investigated and we demonstrate that under suitable assumptions port-Hamiltonian differential-algebraic equations admit a representation which consists of a parametrized port-Hamiltonian semi-explicit system and an unstructured equation. Finally, we present the application to multibody system dynamics and discuss numerical results to demonstrate the capabilities of our approach.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search</title>
<link>https://arxiv.org/abs/2505.19209</link>
<guid>https://arxiv.org/abs/2505.19209</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, scientific hypothesis discovery, combinatorial optimization, hierarchical search method, chemistry literature

Summary:
1. The study introduces the task of fine-grained scientific hypothesis discovery, aiming to generate detailed and experimentally actionable hypotheses from initial research directions.
2. The research explores how to utilize an LLM's internal heuristics to formulate the most promising hypothesis based on its own scoring system.
3. The alignment between LLM-generated hypotheses and ground-truth hypotheses is investigated to evaluate the quality of the generated hypotheses.
4. Comparisons are made between using an ensemble of diverse LLMs and repeated instances of the strongest LLM to shape the reward landscape for hypothesis generation.
5. The study also analyzes the effectiveness of an ensemble of identical LLMs in providing a reliable reward landscape for hypothesis discovery.
6. The proposed hierarchical search method incrementally adds details to hypotheses, leading to a smoother reward landscape and more effective optimization.
7. Empirical evaluations on a chemistry literature benchmark demonstrate that the hierarchical search method consistently outperforms strong baselines. 

<br /><br />Summary: <div>
arXiv:2505.19209v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the novel task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent chemistry literature show that our method consistently outperforms strong baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid Development of Efficient Participant-Specific Computational Models of the Wrist</title>
<link>https://arxiv.org/abs/2505.19282</link>
<guid>https://arxiv.org/abs/2505.19282</guid>
<content:encoded><![CDATA[
<div> Keywords: computational modeling, hand and wrist injuries, finite element models, participant-specific, ligament injury

Summary:
Computational modeling plays a crucial role in developing treatment options for hand and wrist injuries. However, the current models are limited, often relying on average material properties from literature. A novel automated workflow has been developed to create participant-specific finite element models using non-linear morphing techniques and algorithmic approaches. By utilizing four-dimensional computed tomography (4DCT) data, three participant-specific models were created within 2 hours, allowing for individual simulations to be performed in just 45 seconds. The models were used to investigate clinical questions such as optimizing ligament properties to participant-specific kinematics and conducting Monte Carlo analysis on the effects of ligament injury on joint contact pressure. This work paves the way for future patient-specific modeling of hand and wrist injuries, offering a more personalized approach to treatment and understanding the impacts of injuries on joint health. 

<br /><br />Summary: <div>
arXiv:2505.19282v1 Announce Type: cross 
Abstract: While computational modeling may help to develop new treatment options for hand and wrist injuries, at present, few models exist. The time and expertise required to develop and use these models is considerable. Moreover, most do not allow for variation of material properties, instead relying on literature reported averages. We have developed a novel automated workflow combining non-linear morphing techniques with various algorithmic techniques to create participant-specific finite element models. Using this workflow, three participant-specific models were created from our existing four-dimensional computed tomography (4DCT) data. These were then used to perform two analyses to demonstrate the usefulness of the models to investigate clinical questions, namely optimization of ligament properties to participant-specific kinematics, and Monte Carlo (MC) analysis of the impacts of ligament injury on joint contact pressure, as an analogue for joint injury that may lead to osteoarthritis. Participant-specific models can be created in 2 hours and individual simulations performed in 45 seconds. This work lays the groundwork for future patient-specific modeling of the hand and wrist.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs</title>
<link>https://arxiv.org/abs/2505.19457</link>
<guid>https://arxiv.org/abs/2505.19457</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, finance, evaluation, reasoning <br /> 
<br />
Summary: 
The article introduces BizFinBench, a benchmark designed to evaluate Large Language Models (LLMs) in financial applications. It includes 6,781 annotated queries in Chinese across various dimensions such as numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering. The benchmark utilizes both objective and subjective metrics and introduces IteraJudge, a method to reduce bias when LLMs serve as evaluators. The evaluation of 25 models shows no single model excels in all tasks, revealing distinct capability patterns. In Numerical Calculation, Claude-3.5-Sonnet and DeepSeek-R1 lead, while in Reasoning, proprietary models outperform open-source models. Information Extraction shows the largest performance spread, while Prediction Recognition performance variance is minimal. The study highlights that while current LLMs handle routine finance queries well, they struggle with complex scenarios that require cross-concept reasoning. BizFinBench aims to provide a rigorous benchmark for future research in the financial domain. <div>
arXiv:2505.19457v1 Announce Type: cross 
Abstract: Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients</title>
<link>https://arxiv.org/abs/2505.19538</link>
<guid>https://arxiv.org/abs/2505.19538</guid>
<content:encoded><![CDATA[
<div> Knowledge bases, experiential knowledge, DoctorRAG, retrieval precision, Med-TextGrad <br />
Summary: <br />
Existing medical reasoning systems often focus on knowledge bases, neglecting the importance of experiential knowledge from patient cases. DoctorRAG is introduced as a framework that combines clinical knowledge and case-based experience to mimic doctor-like reasoning. It enhances retrieval precision by tagging concepts and employing a hybrid retrieval mechanism from knowledge sources and patient data. The integration of the Med-TextGrad module ensures the output aligns with retrieved knowledge and patient queries. Experimental results on diverse datasets show DoctorRAG outperforms traditional RAG models, with iterative refinements further improving performance. The approach generates more accurate, relevant, and comprehensive responses, highlighting progress towards developing medical reasoning systems that more closely mimic human clinical reasoning. <div>
arXiv:2505.19538v1 Announce Type: cross 
Abstract: Existing medical RAG systems mainly leverage knowledge from medical knowledge bases, neglecting the crucial role of experiential knowledge derived from similar patient cases -- a key component of human clinical reasoning. To bridge this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like reasoning by integrating both explicit clinical knowledge and implicit case-based experience. DoctorRAG enhances retrieval precision by first allocating conceptual tags for queries and knowledge sources, together with a hybrid retrieval mechanism from both relevant knowledge and patient. In addition, a Med-TextGrad module using multi-agent textual gradients is integrated to ensure that the final output adheres to the retrieved knowledge and patient query. Comprehensive experiments on multilingual, multitask datasets demonstrate that DoctorRAG significantly outperforms strong baseline RAG models and gains improvements from iterative refinements. Our approach generates more accurate, relevant, and comprehensive responses, taking a step towards more doctor-like medical reasoning systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based Visual Pathway Delineation</title>
<link>https://arxiv.org/abs/2505.19733</link>
<guid>https://arxiv.org/abs/2505.19733</guid>
<content:encoded><![CDATA[
<div> Framework, visual pathway, MRI, feature decomposition, semi-supervised<br />
<br />
Summary: 
Accurately delineating the visual pathway (VP) using multi-parametric MR imaging data is crucial for understanding the human visual system and diagnosing related disorders. Existing methods struggle with complex cross-sequence relationships and the need for large labeled datasets. To address these challenges, a novel semi-supervised framework is proposed. It includes a correlation-constrained feature decomposition (CFD) to capture unique MRI sequence characteristics and aid in information fusion. Additionally, a consistency-based sample enhancement (CSE) module generates edge information from unlabeled data to mitigate the limited labeled data issue. The framework outperforms seven state-of-the-art approaches in VP delineation on public and in-house datasets, showcasing its effectiveness in leveraging multi-parametric MRI data for improved visual pathway delineation. <br /> <div>
arXiv:2505.19733v1 Announce Type: cross 
Abstract: Accurately delineating the visual pathway (VP) is crucial for understanding the human visual system and diagnosing related disorders. Exploring multi-parametric MR imaging data has been identified as an important way to delineate VP. However, due to the complex cross-sequence relationships, existing methods cannot effectively model the complementary information from different MRI sequences. In addition, these existing methods heavily rely on large training data with labels, which is labor-intensive and time-consuming to obtain. In this work, we propose a novel semi-supervised multi-parametric feature decomposition framework for VP delineation. Specifically, a correlation-constrained feature decomposition (CFD) is designed to handle the complex cross-sequence relationships by capturing the unique characteristics of each MRI sequence and easing the multi-parametric information fusion process. Furthermore, a consistency-based sample enhancement (CSE) module is developed to address the limited labeled data issue, by generating and promoting meaningful edge information from unlabeled data. We validate our framework using two public datasets, and one in-house Multi-Shell Diffusion MRI (MDM) dataset. Experimental results demonstrate the superiority of our approach in terms of delineation performance when compared to seven state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Approach to Credit Prediction with Learnable Prompts for Multi-scale Temporal Representation Learning</title>
<link>https://arxiv.org/abs/2404.13004</link>
<guid>https://arxiv.org/abs/2404.13004</guid>
<content:encoded><![CDATA[
<div> Keywords: credit scoring, deep learning, FinLangNet, multi-scale distributions, time series classification

Summary:
FinLangNet is a novel approach for credit scoring that utilizes deep learning to generate multi-scale distributions of a user's future behavior by transforming tabular data into sequential representations. By incorporating prompt-based training inspired by Large Language Models, FinLangNet introduces prompts at different levels to capture user behavior effectively. Experimental results show that FinLangNet outperforms traditional methods like XGBoost, achieving significant improvements in performance metrics such as KS metric and relative bad debt rate. Moreover, FinLangNet demonstrates superior performance in time series classification tasks on public UEA archives, highlighting its scalability and adaptability in financial scenarios.<br /><br />Summary: <div>
arXiv:2404.13004v4 Announce Type: replace 
Abstract: Recent industrial credit scoring models remain heavily reliant on manually tuned statistical learning methods. While deep learning offers promising solutions, its effectiveness is often limited by the complexity of financial data, particularly in long-horizon scenarios. In this work, we propose FinLangNet, which addresses credit scoring by reframing it as the task of generating multi-scale distributions of a user's future behavior. Within this framework, tabular data is transformed into sequential representations, enabling the generation of user embeddings across multiple temporal scales. Inspired by the recent success of prompt-based training in Large Language Models (LLMs), FinLangNet also introduces two types of prompts to model and capture user behavior at both the feature-granularity and user-granularity levels. Experimental results demonstrate that FinLangNet outperforms the online XGBoost benchmark, achieving a 7.2\% improvement in KS metric performance and a 9.9\% reduction in the relative bad debt rate. Furthermore, FinLangNet exhibits superior performance on public UEA archives, underscoring its scalability and adaptability in time series classification tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Pathways in Reaction Networks guided by Energy Barriers using Integer Linear Programming</title>
<link>https://arxiv.org/abs/2504.10609</link>
<guid>https://arxiv.org/abs/2504.10609</guid>
<content:encoded><![CDATA[
<div> methodology, pathways, reaction networks, kinetic information, energy barriers 
Summary: 
This study introduces a computational methodology for exploring synthesis pathways in chemical reaction networks. The approach incorporates integer linear programming and directed hypergraphs to model reaction networks. Multiple pathways meeting search criteria can be ranked based on an objective function maximizing pathway probability. An automated pipeline estimates energy barriers for reactions in the network. This methodology allows for flexible and kinetically informed pathway exploration on large reaction networks, even when lacking kinetic annotations. It can be applied to networks generated via molecular space expansion approaches. <div>
arXiv:2504.10609v2 Announce Type: replace 
Abstract: Analyzing synthesis pathways for target molecules in a chemical reaction network annotated with information on the kinetics of individual reactions is an area of active study. This work presents a computational methodology for searching for pathways in reaction networks which is based on integer linear programming and the modeling of reaction networks by directed hypergraphs. Often multiple pathways fit the given search criteria. To rank them, we develop an objective function based on physical arguments maximizing the probability of the pathway. We furthermore develop an automated pipeline to estimate the energy barriers of individual reactions in reaction networks. Combined, the methodology facilitates flexible and kinetically informed pathway investigations on large reaction networks by computational means, even for networks coming without kinetic annotation, such as those created via generative approaches for expanding molecular spaces.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models</title>
<link>https://arxiv.org/abs/2411.02083</link>
<guid>https://arxiv.org/abs/2411.02083</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, quantitative reasoning, Number Token Loss, regression-like loss, token level<br />
Summary:<br />
Language models excel at text generation but struggle with quantitative reasoning due to a lack of inductive bias for numbers. The Cross Entropy loss, designed for nominal scale data, hinders number token proximity understanding. To address this, a Number Token Loss (NTL) is proposed, minimizing Lp norm or Wasserstein distance between real and predicted number tokens. NTL enhances math-related task performance without increasing runtime, even matching regression head performance in a direct comparison. Scaling to 3B parameter models shows improved performance, showcasing potential for seamless integration into Large Language Models (LLMs). This work aims to inspire LLM developers to enhance pretraining objectives. <div>
arXiv:2411.02083v2 Announce Type: replace-cross 
Abstract: While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the Cross Entropy loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the Lp norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the Cross Entropy objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope that this work can inspire LLM developers to improve their pretraining objectives. The code is available via: https://tum-ai.github.io/number-token-loss/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemToolAgent: The Impact of Tools on Language Agents for Chemistry Problem Solving</title>
<link>https://arxiv.org/abs/2411.07228</link>
<guid>https://arxiv.org/abs/2411.07228</guid>
<content:encoded><![CDATA[
<div> ChemToolAgent, Chemistry tasks, Large language models, Evaluation, Specialized tools<br />
Summary:<br />
ChemToolAgent, an enhanced chemistry agent over ChemCrow, was developed to improve large language models (LLMs) for chemistry problem solving. Comprehensive evaluation showed that while ChemToolAgent did not consistently outperform LLMs without tools, it excelled in specialized chemistry tasks like synthesis prediction when augmented with specialized tools. However, for general chemistry questions like those in exams, agents' ability to reason correctly with chemistry knowledge proved to be more critical, and tool augmentation did not always provide added benefits. Error analysis with a chemistry expert supported these findings, highlighting the importance of considering task specificity and the role of tools in enhancing LLMs for diverse chemistry tasks. <div>
arXiv:2411.07228v3 Announce Type: replace-cross 
Abstract: To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemToolAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemToolAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents' ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Diffusion Autoencoder for Test-time Adapting Prediction of Complex Systems</title>
<link>https://arxiv.org/abs/2505.17459</link>
<guid>https://arxiv.org/abs/2505.17459</guid>
<content:encoded><![CDATA[
<div> SparseDiff, test-time adaptation, sparse encoder, graph neural ordinary differential equation, spatial interactions <br />
Summary: SparseDiff is a novel approach for predicting the behavior of complex systems by dynamically updating the encoding scheme to capture emergent spatiotemporal structures. It utilizes a codebook-based sparse encoder to create a sparse graph topology of the spatial domain, coupled with a graph neural ordinary differential equation to model dynamics and a diffusion decoder for reconstruction. This method enables autoregressive prediction of spatiotemporal evolution while adapting the sparse topological structure to accommodate emerging spatial patterns through adaptive re-encoding. Extensive evaluations show that SparseDiff significantly reduces prediction errors compared to baselines, requiring only a fraction of the spatial resolution. <div>
arXiv:2505.17459v1 Announce Type: new 
Abstract: Predicting the behavior of complex systems is critical in many scientific and engineering domains, and hinges on the model's ability to capture their underlying dynamics. Existing methods encode the intrinsic dynamics of high-dimensional observations through latent representations and predict autoregressively. However, these latent representations lose the inherent spatial structure of spatiotemporal dynamics, leading to the predictor's inability to effectively model spatial interactions and neglect emerging dynamics during long-term prediction. In this work, we propose SparseDiff, introducing a test-time adaptation strategy to dynamically update the encoding scheme to accommodate emergent spatiotemporal structures during the long-term evolution of the system. Specifically, we first design a codebook-based sparse encoder, which coarsens the continuous spatial domain into a sparse graph topology. Then, we employ a graph neural ordinary differential equation to model the dynamics and guide a diffusion decoder for reconstruction. SparseDiff autoregressively predicts the spatiotemporal evolution and adjust the sparse topological structure to adapt to emergent spatiotemporal patterns by adaptive re-encoding. Extensive evaluations on representative systems demonstrate that SparseDiff achieves an average prediction error reduction of 49.99\% compared to baselines, requiring only 1\% of the spatial resolution.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Imputation before Prediction: A New Computational Paradigm for De Novo Peptide Sequencing</title>
<link>https://arxiv.org/abs/2505.17524</link>
<guid>https://arxiv.org/abs/2505.17524</guid>
<content:encoded><![CDATA[
<div> Keywords: de novo peptide sequencing, missing fragmentation, latent imputation, set-prediction, performance improvement

Summary:
De novo peptide sequencing is a critical technique for determining peptide sequences directly from mass spectrometry data. The new computational approach, LIPNovo, addresses the challenge of missing fragmentation in spectra by imputing missing information in the latent space using theoretical peak profiles of target peptides. By framing the imputation as a set-prediction problem and utilizing learnable peak queries, LIPNovo effectively supplements missing data during inference, leading to improved performance. Experimental results on benchmark datasets show that LIPNovo surpasses existing methods significantly. The code for LIPNovo is available on GitHub for further exploration and application. 

<br /><br />Summary: De novo peptide sequencing is vital for peptide identification from mass spectrometry data. LIPNovo introduces a novel approach to handle missing fragmentation by imputing information in the latent space, enhancing performance through set-prediction techniques. Experimentally, LIPNovo outperforms state-of-the-art methods, underscoring its effectiveness in peptide sequencing tasks. The availability of the code on GitHub enables wider adoption and exploration of LIPNovo's capabilities. <div>
arXiv:2505.17524v1 Announce Type: new 
Abstract: De novo peptide sequencing is a fundamental computational technique for ascertaining amino acid sequences of peptides directly from tandem mass spectrometry data, eliminating the need for reference databases. Cutting-edge models usually encode the observed mass spectra into latent representations from which peptides are predicted autoregressively. However, the issue of missing fragmentation, attributable to factors such as suboptimal fragmentation efficiency and instrumental constraints, presents a formidable challenge in practical applications. To tackle this obstacle, we propose a novel computational paradigm called \underline{\textbf{L}}atent \underline{\textbf{I}}mputation before \underline{\textbf{P}}rediction (LIPNovo). LIPNovo is devised to compensate for missing fragmentation information within observed spectra before executing the final peptide prediction. Rather than generating raw missing data, LIPNovo performs imputation in the latent space, guided by the theoretical peak profile of the target peptide sequence. The imputation process is conceptualized as a set-prediction problem, utilizing a set of learnable peak queries to reason about the relationships among observed peaks and directly generate the latent representations of theoretical peaks through optimal bipartite matching. In this way, LIPNovo manages to supplement missing information during inference and thus boosts performance. Despite its simplicity, experiments on three benchmark datasets demonstrate that LIPNovo outperforms state-of-the-art methods by large margins. Code is available at \href{https://github.com/usr922/LIPNovo}{https://github.com/usr922/LIPNovo}.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4D-CTA Image and geometry dataset for kinematic analysis of abdominal aortic aneurysms</title>
<link>https://arxiv.org/abs/2505.17647</link>
<guid>https://arxiv.org/abs/2505.17647</guid>
<content:encoded><![CDATA[
<div> Dataset, Abdominal Aortic Aneurysm, 4D-CTA, Biomechanics, Image Registration

Summary:<br /><br />This article presents a dataset of 4D-CTA images and patient-specific AAA geometries from ten patients for kinematic analysis of abdominal aortic aneurysms (AAA). The dataset includes images captured throughout the cardiac cycle, allowing for the study of AAA wall displacement and strain. Synthetic ground truth data from Patient 1's image is also included for method verification. The dataset facilitates non-invasive analysis of AAA kinematics using an image registration-based approach. The use of open-source file formats enhances the applicability and reusability of the dataset in AAA biomechanics studies. The research was conducted at the ISML-UWA, using images acquired at Fiona Stanley Hospital in Western Australia. This dataset provides valuable information for understanding the biomechanics of AAA and could aid in the development of improved diagnostic and treatment strategies.<br />Summary: <div>
arXiv:2505.17647v1 Announce Type: new 
Abstract: This article presents a dataset used in the article "Kinematics of Abdominal Aortic Aneurysms" [arXiv:2405.13377], published in the Journal of Biomechanics. The dataset is publicly available for download from the Zenodo data repository (https://doi.org/10.5281/zenodo.15477710). The dataset includes time-resolved 3D computed tomography angiography (4D-CTA) images of abdominal aortic aneurysm (AAA) captured throughout the cardiac cycle from ten patients diagnosed with AAA, along with ten patient-specific AAA geometries extracted from these images. Typically, the 4D-CTA dataset for each patient contains ten electrocardiogram (ECG)-gated 3D-CTA image frames acquired over a cardiac cycle, capturing both the systolic and diastolic phases of the AAA configuration. For method verification, the dataset also includes synthetic ground truth data generated from Patient 1's 3D-CTA AAA image in the diastolic phase. The ground truth data includes the patient-specific finite element (FE) biomechanical model and a synthetic systolic 3D-CTA image. The synthetic systolic image was generated by warping Patient 1's diastolic 3D-CTA image using the realistic displacement field obtained from the AAA biomechanical FE model. The images were acquired at Fiona Stanley Hospital in Western Australia and provided to the researchers at the Intelligent Systems for Medicine Laboratory at The University of Western Australia (ISML-UWA), where image-based AAA kinematic analysis was performed. Our dataset enabled the analysis of AAA wall displacement and strain throughout the cardiac cycle using a non-invasive, in vivo, image registration-based approach. The use of widely adopted, open-source file formats (NRRD for images and STL for geometries) facilitates broad applicability and reusability in AAA biomechanics studies that require patient-specific geometry and information about AAA kinematics during cardiac cycle.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A brief review of the Deep BSDE method for solving high-dimensional partial differential equations</title>
<link>https://arxiv.org/abs/2505.17032</link>
<guid>https://arxiv.org/abs/2505.17032</guid>
<content:encoded><![CDATA[
<div> Deep BSDE method, high-dimensional PDEs, numerical computation, deep learning techniques, neural networks <br />
<br />
Summary: 
The article discusses the challenges posed by high-dimensional partial differential equations (PDEs) in traditional numerical computation methods. It introduces the Deep BSDE method, which utilizes deep learning techniques to effectively solve nonlinear PDEs in high dimensions. Since its inception in 2017, the Deep BSDE method has generated widespread interest in using neural networks for tackling high-dimensional PDEs. The article briefly outlines the method, its subsequent developments, and highlights the active research being conducted in this area. The Deep BSDE method has opened up new possibilities for solving complex PDEs in very high dimensions, offering a promising approach for addressing the curse of dimensionality in numerical computations. Future directions for research in this field are also discussed, suggesting potential advancements and applications of deep learning techniques in solving high-dimensional PDEs. <div>
arXiv:2505.17032v1 Announce Type: cross 
Abstract: High-dimensional partial differential equations (PDEs) pose significant challenges for numerical computation due to the curse of dimensionality, which limits the applicability of traditional mesh-based methods. Since 2017, the Deep BSDE method has introduced deep learning techniques that enable the effective solution of nonlinear PDEs in very high dimensions. This innovation has sparked considerable interest in using neural networks for high-dimensional PDEs, making it an active area of research. In this short review, we briefly sketch the Deep BSDE method, its subsequent developments, and future directions for the field.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</title>
<link>https://arxiv.org/abs/2505.17050</link>
<guid>https://arxiv.org/abs/2505.17050</guid>
<content:encoded><![CDATA[
<div> Keywords: Project-Based Learning, Multimodal Large Language Models, PBLBench, Analytic Hierarchy Process, Education

Summary:
PBLBench is a new benchmark designed to evaluate complex reasoning tasks in Project-Based Learning using multimodal large language models. The benchmark challenges models with tasks that mirror those handled by human experts, assessing their performance using structured and weighted evaluation criteria derived from the Analytic Hierarchy Process. The study tested 15 leading models and found that even the most advanced ones achieved only 59% rank accuracy, highlighting the difficulty of the benchmark. This indicates the challenges presented by real-world educational tasks and the need for more capable AI agents to assist teachers effectively. PBLBench aims to improve teacher workload and enhance educational productivity by providing a rigorous evaluation platform for multimodal large language models in educational settings.<br /><br />Summary: <div>
arXiv:2505.17050v1 Announce Type: cross 
Abstract: Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback</title>
<link>https://arxiv.org/abs/2505.17873</link>
<guid>https://arxiv.org/abs/2505.17873</guid>
<content:encoded><![CDATA[
<div> simulator, experiment-guided ranking, hypothesis, chemistry, automated scientific discovery

Summary:
The paper introduces the concept of experiment-guided ranking in the context of hypothesis prioritization in natural sciences. Existing approaches in automated scientific discovery focus on pre-experiment ranking without considering empirical outcomes. To address this gap, the authors propose a simulator that models hypothesis performance based on similarity to a known ground truth hypothesis, incorporating noise. They validate the simulator using a dataset of chemistry hypotheses with experimentally reported outcomes. The proposed method clusters hypotheses based on functional characteristics and uses insights from simulated experimental feedback to prioritize candidates. Experimental results demonstrate that the method outperforms pre-experiment baselines and strong ablations in hypothesis ranking. <div>
arXiv:2505.17873v1 Announce Type: cross 
Abstract: Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on large language model's internal reasoning without incorporating empirical outcomes from experiments. We introduce the task of experiment-guided ranking, which aims to prioritize candidate hypotheses based on the results of previously tested ones. However, developing such strategies is challenging due to the impracticality of repeatedly conducting real experiments in natural science domains. To address this, we propose a simulator grounded in three domain-informed assumptions, modeling hypothesis performance as a function of similarity to a known ground truth hypothesis, perturbed by noise. We curate a dataset of 124 chemistry hypotheses with experimentally reported outcomes to validate the simulator. Building on this simulator, we develop a pseudo experiment-guided ranking method that clusters hypotheses by shared functional characteristics and prioritizes candidates based on insights derived from simulated experimental feedback. Experiments show that our method outperforms pre-experiment baselines and strong ablations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SINDyG: Sparse Identification of Nonlinear Dynamical Systems from Graph-Structured Data</title>
<link>https://arxiv.org/abs/2409.04463</link>
<guid>https://arxiv.org/abs/2409.04463</guid>
<content:encoded><![CDATA[
<div> Sparse Identification of Nonlinear Dynamical Systems, Machine Learning, Sparsity-promoting techniques, Network structure, Neuronal dynamics

Summary: 
The article introduces a new method called Sparse Identification of Nonlinear Dynamical Systems from Graph-structured data (SINDyG), which incorporates network structure into sparse regression for identifying model parameters that explain underlying network dynamics. Unlike existing methods, SINDyG considers interactions between subsystems, allowing for capturing small changes in emergent system behavior. The method is applied to neuronal dynamics, modeling macroscopic oscillations of a neuron population with the extended Stuart-Landau equation. Computational experiments demonstrate improved accuracy and simplicity in identifying network dynamics compared to the original SINDy approach. This innovative approach of combining machine learning with sparsity-promoting techniques opens up new possibilities for extracting governing equations from data in various scientific fields. <br /><br />Summary: <div>
arXiv:2409.04463v3 Announce Type: replace-cross 
Abstract: The combination of machine learning (ML) and sparsity-promoting techniques is enabling direct extraction of governing equations from data, revolutionizing computational modeling in diverse fields of science and engineering. The discovered dynamical models could be used to address challenges in climate science, neuroscience, ecology, finance, epidemiology, and beyond. However, most existing sparse identification methods for discovering dynamical systems treat the whole system as one without considering the interactions between subsystems. As a result, such models are not able to capture small changes in the emergent system behavior. To address this issue, we developed a new method called Sparse Identification of Nonlinear Dynamical Systems from Graph-structured data (SINDyG), which incorporates the network structure into sparse regression to identify model parameters that explain the underlying network dynamics. We showcase the application of our proposed method using several case studies of neuronal dynamics, where we model the macroscopic oscillation of a population of neurons using the extended Stuart-Landau (SL) equation and utilize the SINDyG method to identify the underlying nonlinear dynamics. Our extensive computational experiments validate the improved accuracy and simplicity of discovered network dynamics when compared to the original SINDy approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A finite element solver for a thermodynamically consistent electrolyte model</title>
<link>https://arxiv.org/abs/2505.16296</link>
<guid>https://arxiv.org/abs/2505.16296</guid>
<content:encoded><![CDATA[
<div> finite element solver, electrolyte model, multicomponent ionic transport, non-equilibrium thermodynamics, FEniCSx platform
Summary:<br /><br />In this study, a finite element solver for a thermodynamically consistent electrolyte model is presented. The model accurately captures multicomponent ionic transport, incorporating steric effects, solvation, and pressure coupling. Rooted in non-equilibrium thermodynamics, the model enforces mass conservation, charge neutrality, and entropy production. It surpasses classical frameworks by using modified partial mass balances, the electrostatic Poisson equation, and a momentum balance with electrostatic potential, atomic fractions, and pressure. The solver, implemented in FEniCSx, handles one- and two-dimensional problems efficiently with various boundary conditions. It demonstrates excellent convergence behavior and robustness, validated against benchmark problems. Simulations showcase critical electrolyte phenomena such as electric double layer formation, rectification behavior, and the impacts of solvation number, Debye length, and compressibility. The modular variational formulation allows extension to complex electrochemical systems with multiple ionic species of asymmetric valences.<br /><br /> <div>
arXiv:2505.16296v1 Announce Type: new 
Abstract: In this study, we present a finite element solver for a thermodynamically consistent electrolyte model that accurately captures multicomponent ionic transport by incorporating key physical phenomena such as steric effects, solvation, and pressure coupling. The model is rooted in the principles of non-equilibrium thermodynamics and strictly enforces mass conservation, charge neutrality, and entropy production. It extends beyond classical frameworks like the Nernst-Planck system by employing modified partial mass balances, the electrostatic Poisson equation, and a momentum balance expressed in terms of electrostatic potential, atomic fractions, and pressure, thereby enhancing numerical stability and physical consistency. Implemented using the FEniCSx platform, the solver efficiently handles one- and two-dimensional problems with varied boundary conditions and demonstrates excellent convergence behavior and robustness. Validation against benchmark problems confirms its improved physical fidelity, particularly in regimes characterized by high ionic concentrations and strong electrochemical gradients. Simulation results reveal critical electrolyte phenomena, including electric double layer formation, rectification behavior, and the effects of solvation number, Debye length, and compressibility. The solver's modular variational formulation facilitates its extension to complex electrochemical systems involving multiple ionic species with asymmetric valences.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling</title>
<link>https://arxiv.org/abs/2505.16573</link>
<guid>https://arxiv.org/abs/2505.16573</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Stock Price Prediction, Cross-Stock Trend Integration, Collaborative Learning, Data Privacy<br />
<br />
Summary: 
The article introduces a novel approach, Cross-Stock Trend Integration (CSTI), for stock price prediction by merging local stock patterns into a global model. Inspired by Federated Learning (FL), this method allows collaborative learning across distributed datasets without sharing raw data, maintaining data privacy. Individual stock models are trained separately and then merged to create a unified global model, which is fine-tuned on specific stock data for local relevance. CSTI enables parallel training, optimizing computational resources and reducing training time. Extensive experiments show that CSTI outperforms benchmark models and enhances predictive capabilities of existing approaches, providing a robust alternative to single-stock learning methodologies. <div>
arXiv:2505.16573v1 Announce Type: new 
Abstract: Stock price prediction is a critical area of financial forecasting, traditionally approached by training models using the historical price data of individual stocks. While these models effectively capture single-stock patterns, they fail to leverage potential correlations among stock trends, which could improve predictive performance. Current single-stock learning methods are thus limited in their ability to provide a broader understanding of price dynamics across multiple stocks. To address this, we propose a novel method that merges local patterns into a global understanding through cross-stock pattern integration. Our strategy is inspired by Federated Learning (FL), a paradigm designed for decentralized model training. FL enables collaborative learning across distributed datasets without sharing raw data, facilitating the aggregation of global insights while preserving data privacy. In our adaptation, we train models on individual stock data and iteratively merge them to create a unified global model. This global model is subsequently fine-tuned on specific stock data to retain local relevance. The proposed strategy enables parallel training of individual stock models, facilitating efficient utilization of computational resources and reducing overall training time. We conducted extensive experiments to evaluate the proposed method, demonstrating that it outperforms benchmark models and enhances the predictive capabilities of state-of-the-art approaches. Our results highlight the efficacy of Cross-Stock Trend Integration (CSTI) in advancing stock price prediction, offering a robust alternative to traditional single-stock learning methodologies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Integration Strategies for ESD Protection and Termination in High-Speed LVDS Systems</title>
<link>https://arxiv.org/abs/2505.16200</link>
<guid>https://arxiv.org/abs/2505.16200</guid>
<content:encoded><![CDATA[
<div> Keywords: Electrostatic Discharge (ESD), protection diodes, termination resistors, Low Voltage Differential Signaling (LVDS), signal integrity maintenance<br />
Summary:<br />
This technical article delves into the integration strategies for ESD protection diodes and termination resistors in LVDS designs. It covers critical aspects such as protection mechanisms, design considerations, impedance matching, and placement optimization techniques. The article emphasizes the significance of maintaining signal integrity and ensuring protection effectiveness in LVDS systems. It provides detailed analyses of layout considerations and advanced design strategies to address common integration challenges. The importance of balancing protection requirements with signal integrity demands is highlighted, with practical guidelines offered for implementing robust high-speed digital systems. The article discusses various methodologies for optimizing performance and validating designs, offering designers a comprehensive framework for creating reliable LVDS systems. <div>
arXiv:2505.16200v1 Announce Type: cross 
Abstract: This technical article explores comprehensive strategies for integrating Electrostatic Discharge (ESD) protection diodes and termination resistors in LowVoltage Differential Signaling (LVDS) designs. The article examines critical aspects of protection mechanisms, design considerations, impedance matching, and placement optimization techniques. Through detailed analysis of layout considerations and advanced design strategies, the article presents solutions for common integration challenges. It emphasizes the importance of signal integrity maintenance and protection effectiveness while providing practical guidelines for implementing robust LVDS systems. Various methodologies for performance optimization and validation are discussed, offering designers a thorough framework for creating reliable high-speed digital systems that balance protection requirements with signal integrity demands.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGLDBench: A Benchmark Suite for Stress-Guided Lightweight 3D Designs</title>
<link>https://arxiv.org/abs/2501.03068</link>
<guid>https://arxiv.org/abs/2501.03068</guid>
<content:encoded><![CDATA[
<div> benchmark, material layout strategies, stiff, lightweight designs, 3D domains, stress-guided<br />
<br />
Summary: 
The Stress-Guided Lightweight Design Benchmark (SGLDBench) is introduced as a benchmark suite for assessing material layout strategies in 3D domains to create stiff, lightweight designs. It includes six reference strategies and a multigrid elasticity solver for efficient execution and stiffness validation. The benchmark enables systematic comparison of design strategies based on mechanical properties, supports diverse load conditions, and offers high-resolution designs and stiffness analysis. Visual analysis is emphasized to understand the relationship between design geometry and stress distribution, providing insights into design strategy properties and behaviors. The benchmark's features are showcased through experiments comparing reference strategy results in terms of geometric and mechanical properties. <div>
arXiv:2501.03068v2 Announce Type: replace 
Abstract: We introduce the Stress-Guided Lightweight Design Benchmark (SGLDBench), a comprehensive benchmark suite for applying and evaluating material layout strategies to generate stiff, lightweight designs in 3D domains. SGLDBench provides a seamlessly integrated simulation and analysis framework, including six reference strategies and a scalable multigrid elasticity solver to efficiently execute these strategies and validate the stiffness of their results. This facilitates the systematic analysis and comparison of design strategies based on the mechanical properties they achieve. SGLDBench enables the evaluation of diverse load conditions and, through the tight integration of the solver, supports high-resolution designs and stiffness analysis. Additionally, SGLDBench emphasizes visual analysis to explore the relationship between the geometric structure of a design and the distribution of stresses, offering insights into the specific properties and behaviors of different design strategies. SGLDBench's specific features are highlighted through several experiments, comparing the results of reference strategies with respect to geometric and mechanical properties.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stochastic Dynamic Network Model of the Space Environment</title>
<link>https://arxiv.org/abs/2411.03173</link>
<guid>https://arxiv.org/abs/2411.03173</guid>
<content:encoded><![CDATA[
<div> network model, space environment, stochastic dynamics, species, collision avoidance

Summary:
The article proposes a network model to study the space environment as a dynamic system with different species of objects represented as nodes connected by stochastic links. Stochastic dynamic equations are derived to describe the evolution of the network, replicating existing results on the space environment's evolution. The analysis of the network structure identifies critical species and orbit regimes that impact the environment the most. The concept of carrying capacity in space is introduced based on the stability of network equilibria. Using current object populations and launch traffic forecasts, the model demonstrates how different policies can affect collision avoidance and post-mission disposal maneuvers. The proposed network model provides a framework for understanding and managing the space environment effectively. 

<br /><br />Summary: <div>
arXiv:2411.03173v2 Announce Type: replace-cross 
Abstract: This work proposes to model the space environment as a stochastic dynamic network where each node is a group of objects of a given class, or species, and their relationship is represented by stochastic links. A set of stochastic dynamic equations, governing the evolution of the network, are derived from the network structure and topology. It will be shown that the proposed system of stochastic dynamic equations well reproduces existing results on the evolution of the space environment. The analysis of the structure of the network and relationships among node can help to understand which species of objects and orbit regimes are more critical and affect the most the future evolution of the space environment. In analogy with ecological networks, we develop a theory of the carrying capacity of space based on the stability of equilibria of the network dynamics. Some examples are presented starting from the current population of resident objects and different launch traffic forecast models. It will be shown how the proposed network model can be used to study the effect of the adoption of different policies on the execution of collision avoidance and post mission disposal manoeuvres.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deployment of Traditional and Hybrid Machine Learning for Critical Heat Flux Prediction in the CTF Thermal Hydraulics Code</title>
<link>https://arxiv.org/abs/2505.14701</link>
<guid>https://arxiv.org/abs/2505.14701</guid>
<content:encoded><![CDATA[
<div> machine learning, critical heat flux, hybrid models, subchannel code, nuclear reactors

Summary:
- The study focuses on predicting critical heat flux (CHF) using machine learning (ML) models, crucial for safety in nuclear reactors.
- Traditional empirical correlations for CHF prediction often show discrepancies, prompting the need for more reliable methods.
- Hybrid models, combining data-driven ML with physics-based models, show improved accuracy over conventional methods.
- Integration of ML-based CHF models into subchannel codes proves effective in enhancing prediction performance.
- The study's results demonstrate that ML-based models, integrated with hybrid approaches, can reduce CHF overprediction and enhance overall accuracy, highlighting their potential in improving operational efficiency and safety in nuclear reactor systems.<br /><br />Summary: <div>
arXiv:2505.14701v1 Announce Type: new 
Abstract: Critical heat flux (CHF) marks the transition from nucleate to film boiling, where heat transfer to the working fluid can rapidly deteriorate. Accurate CHF prediction is essential for efficiency, safety, and preventing equipment damage, particularly in nuclear reactors. Although widely used, empirical correlations frequently exhibit discrepancies in comparison with experimental data, limiting their reliability in diverse operational conditions. Traditional machine learning (ML) approaches have demonstrated the potential for CHF prediction but have often suffered from limited interpretability, data scarcity, and insufficient knowledge of physical principles. Hybrid model approaches, which combine data-driven ML with physics-based models, mitigate these concerns by incorporating prior knowledge of the domain. This study integrated a purely data-driven ML model and two hybrid models (using the Biasi and Bowring CHF correlations) within the CTF subchannel code via a custom Fortran framework. Performance was evaluated using two validation cases: a subset of the Nuclear Regulatory Commission CHF database and the Bennett dryout experiments. In both cases, the hybrid models exhibited significantly lower error metrics in comparison with conventional empirical correlations. The pure ML model remained competitive with the hybrid models. Trend analysis of error parity indicates that ML-based models reduce the tendency for CHF overprediction, improving overall accuracy. These results demonstrate that ML-based CHF models can be effectively integrated into subchannel codes and can potentially increase performance in comparison with conventional methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local-Global Associative Frames for Symmetry-Preserving Crystal Structure Modeling</title>
<link>https://arxiv.org/abs/2505.15315</link>
<guid>https://arxiv.org/abs/2505.15315</guid>
<content:encoded><![CDATA[
<div> Keywords: crystal structures, invariance, symmetry, frames, crystal property prediction

Summary:
In the study of crystal structures, maintaining invariance to rotational transformations is essential for accurate property prediction. Traditional approaches using global or local frames have limitations in capturing both local structure heterogeneity and preserving crystal symmetry. To address this, the proposed Symmetry-Preserving Frames (SPFrame) method constructs invariant local frames while integrating global structural information to enforce invariance to SO(3) rotations. The SPFrame approach outperforms existing techniques and baselines in crystal property prediction tasks. By combining local and global frame information, SPFrame achieves superior performance in capturing the complexity of crystal structures while maintaining symmetry, demonstrating its efficacy in improving predictive accuracy for various crystal properties. 

<br /><br />Summary: <div>
arXiv:2505.15315v1 Announce Type: new 
Abstract: Crystal structures are defined by the periodic arrangement of atoms in 3D space, inherently making them equivariant to SO(3) group. A fundamental requirement for crystal property prediction is that the model's output should remain invariant to arbitrary rotational transformations of the input structure. One promising strategy to achieve this invariance is to align the given crystal structure into a canonical orientation with appropriately computed rotations, or called frames. However, existing work either only considers a global frame or solely relies on more advanced local frames based on atoms' local structure. A global frame is too coarse to capture the local structure heterogeneity of the crystal, while local frames may inadvertently disrupt crystal symmetry, limiting their expressivity. In this work, we revisit the frame design problem for crystalline materials and propose a novel approach to construct expressive Symmetry-Preserving Frames, dubbed as SPFrame, for modeling crystal structures. Specifically, this local-global associative frame constructs invariant local frames rather than equivariant ones, thereby preserving the symmetry of the crystal. In parallel, it integrates global structural information to construct an equivariant global frame to enforce SO(3) invariance. Extensive experimental results demonstrate that SPFrame consistently outperforms traditional frame construction techniques and existing crystal property prediction baselines across multiple benchmark tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Open Earth Science as Fast and Accessible as Natural Language</title>
<link>https://arxiv.org/abs/2505.15690</link>
<guid>https://arxiv.org/abs/2505.15690</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural language processing, Earth observation, Large Language Models, Data analysis, Software framework

Summary: 
This study explores the feasibility of using Large Language Models (LLMs) for natural-language-driven earth observation data analysis. The research focuses on achieving high accuracy, interactive latencies, low costs, and open source software. Techniques such as model scaling, prompt optimization, and inference-time scaling optimization are employed to achieve near 100% accuracy across multiple metrics. The analysis also considers cost, latency, and maintainability of the techniques. Opportunities for further research, framework development, and ongoing work towards a comprehensive solution are identified. Collaboration and contributions are encouraged for the advancement of this field.

<br /><br />Summary: <div>
arXiv:2505.15690v1 Announce Type: new 
Abstract: Is natural-language-driven earth observation data analysis now feasible with the assistance of Large Language Models (LLMs)? For open science in service of public interest, feasibility requires reliably high accuracy, interactive latencies, low (sustainable) costs, open LLMs, and openly maintainable software -- hence, the challenge. What are the techniques and programming system requirements necessary for satisfying these constraints, and what is the corresponding development and maintenance burden in practice? This study lays the groundwork for exploring these questions, introducing an impactful earth science use-case, and providing a software framework with evaluation data and metrics, along with initial results from employing model scaling, prompt-optimization, and inference-time scaling optimization techniques. While we attain high accuracy (near 100%) across 10 of 11 metrics, the analysis further considers cost (token-spend), latency, and maintainability across this space of techniques. Finally, we enumerate opportunities for further research, general programming and evaluation framework development, and ongoing work for a comprehensive, deployable solution. This is a call for collaboration and contribution.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
<link>https://arxiv.org/abs/2505.15155</link>
<guid>https://arxiv.org/abs/2505.15155</guid>
<content:encoded><![CDATA[
<div> Framework, Quantitative Finance, Automation, Factor-model, Co-optimization
Summary: 
RD-Agent(Q) is a data-centric multi-agent framework designed to automate the research and development of quantitative strategies in financial markets. It breaks down the quant process into two stages: Research and Development, connected through a feedback loop with a multi-armed bandit scheduler for direction selection. The framework sets goal-aligned prompts, formulates hypotheses, and maps them to tasks in the Research stage, while using a code-generation agent, Co-STEER, to implement task-specific code in the Development stage. Empirical results show RD-Agent(Q) achieving higher annualized returns than classical factor libraries with fewer factors, and outperforming deep time-series models on real markets. The joint factor-model optimization provides a balance between predictive accuracy and strategy robustness. The code for RD-Agent(Q) is available on GitHub at: https://github.com/microsoft/RD-Agent.<br /><br />Summary: <div>
arXiv:2505.15155v1 Announce Type: cross 
Abstract: Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility. Despite advances in large language models and multi-agent systems, current quantitative research pipelines suffer from limited automation, weak interpretability, and fragmented coordination across key components such as factor mining and model innovation. In this paper, we propose R&amp;D-Agent for Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization. RD-Agent(Q) decomposes the quant process into two iterative stages: a Research stage that dynamically sets goal-aligned prompts, formulates hypotheses based on domain priors, and maps them to concrete tasks, and a Development stage that employs a code-generation agent, Co-STEER, to implement task-specific code, which is then executed in real-market backtests. The two stages are connected through a feedback stage that thoroughly evaluates experimental outcomes and informs subsequent iterations, with a multi-armed bandit scheduler for adaptive direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher annualized returns than classical factor libraries using 70% fewer factors, and outperforms state-of-the-art deep time-series models on real markets. Its joint factor-model optimization delivers a strong balance between predictive accuracy and strategy robustness. Our code is available at: https://github.com/microsoft/RD-Agent.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2505.15228</link>
<guid>https://arxiv.org/abs/2505.15228</guid>
<content:encoded><![CDATA[
<div> Keywords: CP-KAN, neural architecture, Chebyshev polynomial, QUBO, regression tasks <br />
Summary: <br />
- This article introduces the cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a neural architecture that combines Chebyshev polynomial basis functions and quadratic unconstrained binary optimization (QUBO).
- The primary contribution is a reformulation of the degree selection problem as a QUBO task, which significantly reduces complexity and allows for efficient degree selection across neurons.
- CP-KAN performs well in regression tasks with limited data, showing robustness to input scales and natural regularization properties from its polynomial basis.
- The architecture is theoretically linked to properties of financial time series, demonstrating its potential for analyzing financial data.
- Empirical validation across different domains shows that CP-KAN performs competitively compared to traditional architectures, particularly in scenarios where data efficiency and numerical stability are crucial. <div>
arXiv:2505.15228v1 Announce Type: cross 
Abstract: We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a neural architecture combining Chebyshev polynomial basis functions and quadratic unconstrained binary optimization (QUBO). Our primary contribution involves reformulating the degree selection problem as a QUBO task, reducing the complexity from $O(D^N)$ to a single optimization step per layer. This approach enables efficient degree selection across neurons while maintaining computational tractability. The architecture performs well in regression tasks with limited data, showing good robustness to input scales and natural regularization properties from its polynomial basis. Additionally, theoretical analysis establishes connections between CP-KAN's performance and properties of financial time series. Our empirical validation across multiple domains demonstrates competitive performance compared to several traditional architectures tested, especially in scenarios where data efficiency and numerical stability are important. Our implementation, including strategies for managing computational overhead in larger networks is available in Ref.~\citep{cpkan_implementation}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization of Probability Distributions via Divide-and-Conquer: Convergence and Error Propagation under Distributional Arithmetic Operations</title>
<link>https://arxiv.org/abs/2505.15283</link>
<guid>https://arxiv.org/abs/2505.15283</guid>
<content:encoded><![CDATA[
<div> algorithm, distribution, approximation, stability, convergence
<br />
Summary: 
This article examines a divide-and-conquer algorithm for approximating continuous one-dimensional probability distributions with finite mean. It conducts a numerical comparison with existing schemes, emphasizing the stability of discrete approximations during arithmetic operations. The study establishes an upper bound for the approximation error based on the Wasserstein-1 distance, applicable to all continuous distributions with finite mean. The algorithm often achieves optimal convergence rates, and numerical tests demonstrate its superior stability compared to other methods when subjected to arithmetic operations. <div>
arXiv:2505.15283v1 Announce Type: cross 
Abstract: This article studies a general divide-and-conquer algorithm for approximating continuous one-dimensional probability distributions with finite mean. The article presents a numerical study that compares pre-existing approximation schemes with a special focus on the stability of the discrete approximations when they undergo arithmetic operations. The main results are a simple upper bound of the approximation error in terms of the Wasserstein-1 distance that is valid for all continuous distributions with finite mean. In many use-cases, the studied method achieve optimal rate of convergence, and numerical experiments show that the algorithm is more stable than pre-existing approximation schemes in the context of arithmetic operations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elasto-acoustic wave propagation in geophysical media using hybrid high-order methods on general meshes</title>
<link>https://arxiv.org/abs/2505.15771</link>
<guid>https://arxiv.org/abs/2505.15771</guid>
<content:encoded><![CDATA[
<div> Keywords: HHO methods, elasto-acoustic waves, explicit schemes, implicit schemes, numerical simulations
<br />
Summary:
Hybrid high-order (HHO) methods are investigated for the space semi-discretization of coupled elasto-acoustic waves using explicit and implicit Runge-Kutta schemes for time discretization. The implementation of these schemes requires static condensation of face and cell unknowns with block-diagonal matrices. CFL stability limits of explicit schemes are estimated, and a comparison between explicit and implicit schemes shows the competitiveness of implicit schemes in various scenarios. Simulations in a 2D geophysical model demonstrate the geometrical flexibility of the HHO method in handling hybrid and nonconforming meshes, yielding results comparable to spectral element software. Overall, the study highlights the effectiveness of HHO methods in accurately simulating coupled elasto-acoustic waves with efficient time discretization strategies. 
<br /> <div>
arXiv:2505.15771v1 Announce Type: cross 
Abstract: Hybrid high-order (HHO) methods are numerical methods characterized by several interesting properties such as local conservativity, geometric flexibility and high-order accuracy. Here, HHO schemes are studied for the space semi-discretization of coupled elasto-acoustic waves in the time domain using a first-order formulation. Explicit and singly diagonal implicit Runge--Kutta (ERK & SDIRK) schemes are used for the time discretization. We show that an efficient implementation of explicit (resp. implicit) time schemes calls for a static condensation of the face (resp. cell) unknowns. Crucially, both static condensation procedures only involve block-diagonal matrices. Then, we provide numerical estimates for the CFL stability limit of ERK schemes and present a comparative study on the efficiency of explicit versus implicit schemes. Our findings indicate that implicit time schemes remain competitive in many situations. Finally, simulations in a 2D realistic geophysical configuration are performed, illustrating the geometrical flexibility of the HHO method: both hybrid (triangular and quadrangular) and nonconforming (with hanging nodes) meshes are easily handled, delivering results of comparable accuracy to a reference spectral element software based on tensorized elements.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Prediction-Assisted Safe Reinforcement Learning for Electric Vehicle Charging Station Recommendation in Dynamically Coupled Transportation-Power Systems</title>
<link>https://arxiv.org/abs/2407.20679</link>
<guid>https://arxiv.org/abs/2407.20679</guid>
<content:encoded><![CDATA[
<div> Recommendation, Electric Vehicles, Transportation-Power Systems, Reinforcement Learning, Constrained Markov Decision Process

Summary:
- The study tackles the en-route charging station recommendation problem for electric vehicles in dynamically coupled transportation-power systems.
- The objective is to maximize overall traffic efficiency while ensuring power grid safety, a novel approach in existing literature.
- The problem is formulated as a constrained Markov decision process (CMDP) and addressed using an online prediction-assisted safe reinforcement learning (OP-SRL) method.
- Challenges of constrained optimization and uncertain delays are overcome through innovative approaches such as Lagrangian method and online sequence-to-sequence predictor.
- Comprehensive experimental studies demonstrate the superior performance of the proposed method in road network efficiency, power grid safety, and EV user satisfaction.
<br /><br />Summary: <div>
arXiv:2407.20679v2 Announce Type: replace 
Abstract: With the proliferation of electric vehicles (EVs), the transportation network and power grid become increasingly interdependent and coupled via charging stations. The concomitant growth in charging demand has posed challenges for both networks, highlighting the importance of charging coordination. Existing literature largely overlooks the interactions between power grid security and traffic efficiency. In view of this, we study the en-route charging station (CS) recommendation problem for EVs in dynamically coupled transportation-power systems. The system-level objective is to maximize the overall traffic efficiency while ensuring the safety of the power grid. This problem is for the first time formulated as a constrained Markov decision process (CMDP), and an online prediction-assisted safe reinforcement learning (OP-SRL) method is proposed to learn the optimal and secure policy by extending the PPO method. To be specific, we mainly address two challenges. First, the constrained optimization problem is converted into an equivalent unconstrained optimization problem by applying the Lagrangian method. Second, to account for the uncertain long-time delay between performing CS recommendation and commencing charging, we put forward an online sequence-to-sequence (Seq2Seq) predictor for state augmentation to guide the agent in making forward-thinking decisions. Finally, we conduct comprehensive experimental studies based on the Nguyen-Dupuis network and a large-scale real-world road network, coupled with IEEE 33-bus and IEEE 69-bus distribution systems, respectively. Results demonstrate that the proposed method outperforms baselines in terms of road network efficiency, power grid safety, and EV user satisfaction. The case study on the real-world network also illustrates the applicability in the practical context.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Method for Satellite Pattern-of-Life Identification</title>
<link>https://arxiv.org/abs/2412.10814</link>
<guid>https://arxiv.org/abs/2412.10814</guid>
<content:encoded><![CDATA[
<div> machine learning, satellite behavior, pattern-of-life identification, diffusion model, data sampling rates

Summary:
The article introduces a novel diffusion-based method for satellite pattern-of-life (PoL) identification, which overcomes limitations of existing approaches by eliminating the need for manual refinement or domain-specific knowledge. The method combines a multivariate time-series encoder with a diffusion model to capture hidden representations of satellite positional data and generate PoL labels. It achieves high identification quality and robustness even with varying data sampling rates, demonstrating potential for practical satellite behavior pattern identification and mission deployment. This innovative approach outperforms traditional methods and addresses challenges in analyzing satellite behaviors, offering a promising solution for space safety and satellite monitoring. The Expert-ML method, previously developed, is also discussed as a domain expertise-informed machine learning approach for PoL identification. <div>
arXiv:2412.10814v2 Announce Type: replace-cross 
Abstract: Satellite pattern-of-life (PoL) identification is crucial for space safety and satellite monitoring, involving the analysis of typical satellite behaviors such as station-keeping, drift, etc. However, existing PoL identification methods remain underdeveloped due to the complexity of aerospace systems, variability in satellite behaviors, and fluctuating observation sampling rates. In a first attempt, we developed a domain expertise-informed machine learning method (Expert-ML) to combine satellite orbital movement knowledge and machine learning models. The Expert-ML method achieved high accuracy results in simulation data and real-world data with normal sampling rate. However, this approach lacks of generality as it requires domain expertise and its performance degraded significantly when data sampling rate varied. To achieve generality, we propose a novel diffusion-based PoL identification method. Distinct from prior approaches, the proposed method leverages a diffusion model to achieve end-to-end identification without manual refinement or domain-specific knowledge. Specifically, we employ a multivariate time-series encoder to capture hidden representations of satellite positional data. The encoded features are subsequently incorporated as conditional information in the denoising process to generate PoL labels. Through experimentation across real-world satellite settings, our proposed diffusion-based method demonstrates its high identification quality and provides a robust solution even with reduced data sampling rates, indicating its great potential in practical satellite behavior pattern identification, tracking and related mission deployment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Dynamical Systems across Environments via Diffusive Model Weight Generation</title>
<link>https://arxiv.org/abs/2505.13919</link>
<guid>https://arxiv.org/abs/2505.13919</guid>
<content:encoded><![CDATA[
<div> Data-driven methods, dynamic behaviors, cross-environment prediction, model weight generation, \texttt{EnvAd-Diff}<br />
<br />
Summary: 
The study focuses on using data-driven methods to predict physical dynamics in different environments. It addresses the issue of prediction functions failing when transferred to unseen environments by proposing a model weight generation method called \texttt{EnvAd-Diff}. The method operates in the weight space of the dynamic function to generate suitable weights based on environmental conditions for zero-shot prediction. By training expert prediction functions on dynamic trajectories from visible environments, a model zoo is created to construct sample pairs of prediction function weights and their corresponding environments. A latent space diffusion model conditioned on the environment is then trained to model the joint distribution of weights and environments. The study also introduces a physics-informed surrogate label to distinguish different environments. Generalization experiments across multiple systems show that the 1M parameter prediction function generated by \texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model. <div>
arXiv:2505.13919v1 Announce Type: new 
Abstract: Data-driven methods offer an effective equation-free solution for predicting physical dynamics. However, the same physical system can exhibit significantly different dynamic behaviors in various environments. This causes prediction functions trained for specific environments to fail when transferred to unseen environments. Therefore, cross-environment prediction requires modeling the dynamic functions of different environments. In this work, we propose a model weight generation method, \texttt{EnvAd-Diff}. \texttt{EnvAd-Diff} operates in the weight space of the dynamic function, generating suitable weights from scratch based on environmental condition for zero-shot prediction. Specifically, we first train expert prediction functions on dynamic trajectories from a limited set of visible environments to create a model zoo, thereby constructing sample pairs of prediction function weights and their corresponding environments. Subsequently, we train a latent space diffusion model conditioned on the environment to model the joint distribution of weights and environments. Considering the lack of environmental prior knowledge in real-world scenarios, we propose a physics-informed surrogate label to distinguish different environments. Generalization experiments across multiple systems demonstrate that a 1M parameter prediction function generated by \texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLUTUS Open Source -- Breaking Barriers in Algorithmic Trading</title>
<link>https://arxiv.org/abs/2505.14050</link>
<guid>https://arxiv.org/abs/2505.14050</guid>
<content:encoded><![CDATA[
<div> Keywords: Algorithmic trading, reproducibility, standardization, collaboration, PLUTUS

Summary: 
Algorithmic trading has traditionally been a secretive and fragmented domain, lacking transparency, standardization, and collaboration. The PLUTUS Open Source initiative, sponsored by ALGOTRADE, aims to reshape this landscape by promoting openness, structure, and collaboration within the algorithmic trading ecosystem. PLUTUS introduces a reproducibility standard, a modular development framework, and a suite of community-built reference strategies, providing a systematic approach to designing, testing, and documenting trading algorithms for users regardless of their technical or financial background. The initiative invites contributions from the research and trading communities to build a transparent and inclusive future for algorithmic trading. The foundational structure of PLUTUS is outlined, along with working examples that adhere to the PLUTUS standard. By promoting reproducibility, standardization, and collaboration, PLUTUS aims to drive a positive transformation in the algorithmic trading industry. 

Summary: <br /><br />PLUTUS Open Source aims to reshape the algorithmic trading ecosystem by promoting openness, structure, and collaboration. The initiative introduces a reproducibility standard, a modular development framework, and a suite of community-built reference strategies. PLUTUS provides a systematic approach to designing, testing, and documenting trading algorithms, accessible to users with varying technical and financial backgrounds. By inviting contributions from the research and trading communities, PLUTUS aims to build a transparent and inclusive future for algorithmic trading. The initiative's foundational structure and working examples adhering to the PLUTUS standard are presented as a foundation for driving positive change in the industry. <div>
arXiv:2505.14050v1 Announce Type: new 
Abstract: Algorithmic trading has long been an opaque, fragmented domain, guarded by secrecy and built around proprietary systems. In contrast to the open, collaborative evolution in fields like machine learning or software engineering, the algorithmic trading ecosystem has been slow to adopt reproducibility, standardization, and shared infrastructure. This paper introduces PLUTUS Open Source, an initiative sponsored by ALGOTRADE to reshape this landscape through openness, structure, and collaboration. PLUTUS combines a reproducibility standard, a modular development framework, and a growing suite of community-built reference strategies. The project provides a systematic approach to designing, testing, and documenting trading algorithms, regardless of the user's technical or financial background. We outline the motivation behind the initiative, present its foundational structure, and showcase working examples that adhere to the PLUTUS standard. We also invite the broader research and trading communities to contribute, iterate, and help build a transparent and inclusive future for algorithmic trading.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order, mixed-hybrid finite elements for Kirchhoff-Love shells</title>
<link>https://arxiv.org/abs/2505.14115</link>
<guid>https://arxiv.org/abs/2505.14115</guid>
<content:encoded><![CDATA[
<div> mixed-hybrid method, Kirchhoff-Love shells, Lagrange elements, hybridization, numerical analyses <br />
Summary: 
A novel mixed-hybrid method for Kirchhoff-Love shells is introduced, allowing the use of classical Lagrange elements in numerical analyses. Unlike displacement-based formulations, this method features displacements and moments as primary unknowns, reducing continuity requirements and enabling equal-order interpolations. Hybridization simplifies static condensation while introducing rotational degrees of freedom as Lagrange multipliers. The formulation, based on Tangential Differential Calculus, is applicable for explicit and implicit shell geometries. Mechanical boundary conditions are considered, and numerical results show optimal convergence rates for smooth solutions. New benchmark test cases are proposed to assess the method's effectiveness. <div>
arXiv:2505.14115v1 Announce Type: new 
Abstract: A novel mixed-hybrid method for Kirchhoff-Love shells is proposed that enables the use of classical, possibly higher-order Lagrange elements in numerical analyses. In contrast to purely displacement-based formulations that require higher continuity of shape functions as in IGA, the mixed formulation features displacements and moments as primary unknowns. Thereby the continuity requirements are reduced, allowing equal-order interpolations of the displacements and moments. Hybridization enables an element-wise static condensation of the degrees of freedom related to the moments, at the price of introducing (significantly less) rotational degrees of freedom acting as Lagrange multipliers to weakly enforce the continuity of tangential moments along element edges. The mixed model is formulated coordinate-free based on the Tangential Differential Calculus, making it applicable for explicitly and implicitly defined shell geometries. All mechanically relevant boundary conditions are considered. Numerical results confirm optimal higher-order convergence rates whenever the mechanical setup allows for sufficiently smooth solutions; new benchmark test cases of this type are proposed.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Maxwell Tomography Using the Volume-Surface Integral Equation for Improved Estimation of Electrical Properties</title>
<link>https://arxiv.org/abs/2505.14546</link>
<guid>https://arxiv.org/abs/2505.14546</guid>
<content:encoded><![CDATA[
<div> Tomography, Maxwell, Electrical properties, MRI, VSIE<br />
<br />
Summary:
Global Maxwell Tomography (GMT) is a method for estimating electrical properties (EP) from magnetic resonance measurements. A novel version of GMT using the volume-surface integral equation (VSIE) recalculates coil currents based on updated EP estimates, yielding more accurate reconstructions. Simulation and experimental results showed that VSIE-based GMT outperformed the traditional VIE-based method, with improvements of at least 12% in simulations and relative differences of 13-26% in experiments compared to probe-measured values. By accounting for the effect of EP on coil currents, VSIE-based GMT does not rely on an initial EP estimate, making it more suitable for experimental reconstructions. The study highlights the significance of using VSIE for enhancing GMT performance in noninvasive EP estimation. <br /><br /> <div>
arXiv:2505.14546v1 Announce Type: new 
Abstract: Objective: Global Maxwell Tomography (GMT) is a noninvasive inverse optimization method for the estimation of electrical properties (EP) from magnetic resonance (MR) measurements. GMT uses the volume integral equation (VIE) in the forward problem and assumes that the sample has negligible effect on the coil currents. Consequently, GMT calculates the coil's incident fields with an initial EP distribution and keeps them constant for all optimization iterations. This can lead to erroneous reconstructions. This work introduces a novel version of GMT that replaces VIE with the volume-surface integral equation (VSIE), which recalculates the coil currents at every iteration based on updated EP estimates before computing the associated fields. Methods: We simulated an 8-channel transceiver coil array for 7 T brain imaging and reconstructed the EP of a realistic head model using VSIE-based GMT. We built the coil, collected experimental MR measurements, and reconstructed EP of a two-compartment phantom. Results: In simulations, VSIE-based GMT outperformed VIE-based GMT by at least 12% for both EP. In experiments, the relative difference with respect to probe-measured EP values in the inner (outer) compartment was 13% (26%) and 17% (33%) for the permittivity and conductivity, respectively. Conclusion: The use of VSIE over VIE enhances GMT's performance by accounting for the effect of the EP on the coil currents. Significance: VSIE-based GMT does not rely on an initial EP estimate, rendering it more suitable for experimental reconstructions compared to the VIE-based GMT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact</title>
<link>https://arxiv.org/abs/2505.13469</link>
<guid>https://arxiv.org/abs/2505.13469</guid>
<content:encoded><![CDATA[
<div> fair lending, machine learning, algorithmic fairness, profit margins, default rates 
Summary:
- The study investigates the tradeoff between enforcing fairness in lending algorithms and maximizing profitability in financial institutions by using machine learning models.
- Simulations on synthetic data reflecting real-world lending patterns reveal that equal opportunity constraints generally have a lower impact on profit margins compared to demographic parity.
- Surprisingly, removing protected attributes from the model through fairness through unawareness results in improved fairness and profitability metrics.
- Fair lending can be profitable under specific economic conditions, and the study also examines the feature-specific drivers of unfairness in lending algorithms.
- The findings provide practical guidance for designing lending algorithms that balance ethical concerns with business objectives. 
<br /><br />Summary: <div>
arXiv:2505.13469v1 Announce Type: cross 
Abstract: As financial institutions increasingly rely on machine learning models to automate lending decisions, concerns about algorithmic fairness have risen. This paper explores the tradeoff between enforcing fairness constraints (such as demographic parity or equal opportunity) and maximizing lender profitability. Through simulations on synthetic data that reflects real-world lending patterns, we quantify how different fairness interventions impact profit margins and default rates. Our results demonstrate that equal opportunity constraints typically impose lower profit costs than demographic parity, but surprisingly, removing protected attributes from the model (fairness through unawareness) outperforms explicit fairness interventions in both fairness and profitability metrics. We further identify the specific economic conditions under which fair lending becomes profitable and analyze the feature-specific drivers of unfairness. These findings offer practical guidance for designing lending algorithms that balance ethical considerations with business objectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Methods for Model Pruning and Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.14052</link>
<guid>https://arxiv.org/abs/2505.14052</guid>
<content:encoded><![CDATA[
<div> Pruning, Language models, Model optimization, Computational complexity, Knowledge distillation
<br />
Summary:
MAMA Pruning is introduced as an enhanced method for model pruning in large language models like R1 or o3-mini. The technique aims to remove neurons and connections that are unlikely to contribute significantly during human-computer interaction. By analyzing movement and magnitude of weights and biases in the pre-training phase, as well as GRPO rewards in the post-training phase, MAMA Pruning effectively reduces model size and computational complexity. The method maintains performance comparable to unpruned models even at extreme pruning levels. Experimental results demonstrate the superiority of MAMA Pruning over existing methods across various pruning levels and computational linguistics tasks.
<br /><br />Summary: <div>
arXiv:2505.14052v1 Announce Type: cross 
Abstract: Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Evaluation of a Microservices Cloud Framework for Online Travel Platforms</title>
<link>https://arxiv.org/abs/2505.14508</link>
<guid>https://arxiv.org/abs/2505.14508</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Travel Agents, Microservices architecture, Cloud computing, Fault Tolerance, Cost-effective analysis<br />
Summary: <br />
The paper discusses the use of Microservices architecture in handling online travel agents globally, highlighting the benefits of flexibility and efficiency in managing large amounts of data. The Microservices Cloud Framework for Online Travel Platforms (MCF-OTP) is designed to improve performance, flexibility, and maintenance by utilizing cloud computing and microservice technologies. The framework aims to enhance fault tolerance and response time, surpassing traditional monolithic designs in scalability and uptime. Cost-effective analysis shows a net gain from startup fees and operational costs. The cloud-based environment helps reduce costs and optimize resource allocation, ultimately increasing efficiency in managing online travel platforms. <div>
arXiv:2505.14508v1 Announce Type: cross 
Abstract: Handling online travel agents globally requires efficient and flexible software solution architectures. When it needs to handle thousands of agents and billions of clients data globally. Microservices architecture is used to break down a large program into numerous, smaller services which can run individually and perform individual tasks. This paper analyses and integrates a unique Microservices Cloud Framework designed to support Online Travel Platforms (MCF-OTP). MCF-OTPs main goal is to increase the performance, flexibility, and maintenance of online travel platforms via cloud computing and microservice technologies. Large-scale travel apps, including managing numerous data sources, dealing with traffic peaks, and providing fault tolerance, can be addressed by the suggested framework. The framework increases good interpretation between flawless data synchronization, microservices, and dynamic scaling based on demand technology. An organization framework that optimizes service borders and minimizes inter-service dependencies is recommended. Thus, this can result in elevated development adaptability. In this research, the principal goal is to evaluate MCF-OTPs efficiency using the indicators of fault tolerance and response time. It is indicated by the findings that the MCF-OTP structure excels traditional monolithic designs in terms of dependability and scalability, managing traffic spikes seamlessly and decreasing downtime. The cost-effective analysis helps ascertain the net gain attained by the startup fees and the ongoing operational costs. The cloud-based environment is used to reduce the fracture cost which also helps to increase the efficiency of resource allocation, according to the research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions</title>
<link>https://arxiv.org/abs/2410.03779</link>
<guid>https://arxiv.org/abs/2410.03779</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, mesh-based physical simulation, EvoMesh, anisotropic message passing, adaptively guided, physical inputs<br />
<br />
Summary:<br />
EvoMesh is introduced as a fully differentiable framework for mesh-based physical simulation that learns graph hierarchies and physical dynamics together. It utilizes direction-specific aggregation of dynamic features and node selection probabilities for hierarchical levels based on physical context, enabling flexible message shortcuts and long-range dependency capture. Experimental results on various datasets demonstrate EvoMesh's superior performance over fixed-hierarchy message passing networks. The code for EvoMesh is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2410.03779v2 Announce Type: replace-cross 
Abstract: Graph neural networks have been a powerful tool for mesh-based physical simulation. To efficiently model large-scale systems, existing methods mainly employ hierarchical graph structures to capture multi-scale node relations. However, these graph hierarchies are typically manually designed and fixed, limiting their ability to adapt to the evolving dynamics of complex physical systems. We propose EvoMesh, a fully differentiable framework that jointly learns graph hierarchies and physical dynamics, adaptively guided by physical inputs. EvoMesh introduces anisotropic message passing, which enables direction-specific aggregation of dynamic features between nodes within each hierarchy, while simultaneously learning node selection probabilities for the next hierarchical level based on physical context. This design creates more flexible message shortcuts and enhances the model's capacity to capture long-range dependencies. Extensive experiments on five benchmark physical simulation datasets show that EvoMesh outperforms recent fixed-hierarchy message passing networks by large margins. Code is available at https://github.com/hbell99/EvoMesh.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries</title>
<link>https://arxiv.org/abs/2410.19236</link>
<guid>https://arxiv.org/abs/2410.19236</guid>
<content:encoded><![CDATA[
<div> algorithm, SHAP zero, Shapley values, biological sequences, interpretability

Summary:
SHAP zero is a novel algorithm that addresses the need for interpretable predictions in machine learning models for biological sequences. By amortizing the computational cost of Shapley value computation across large-scale datasets, SHAP zero enables near-zero marginal cost for future queries. It leverages the sparse Fourier transform of the model to uncover high-order feature interactions and efficiently explain predictions in models of guide RNA efficacy, DNA repair outcomes, and protein fitness. The algorithm significantly accelerates the process of extracting global biological insights and reveals rich combinatorial interactions that were previously inaccessible at scale. This work paves the way for scalable and efficient interpretability of black-box sequence models in biology.<br /><br />Summary: <div>
arXiv:2410.19236v3 Announce Type: replace-cross 
Abstract: The growing adoption of machine learning models for biological sequences has intensified the need for interpretable predictions, with Shapley values emerging as a theoretically grounded standard for model explanation. While effective for local explanations of individual input sequences, scaling Shapley-based interpretability to extract global biological insights requires evaluating thousands of sequences--incurring exponential computational cost per query. We introduce SHAP zero, a novel algorithm that amortizes the cost of Shapley value computation across large-scale biological datasets. After a one-time model sketching step, SHAP zero enables near-zero marginal cost for future queries by uncovering an underexplored connection between Shapley values, high-order feature interactions, and the sparse Fourier transform of the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and protein fitness, SHAP zero explains predictions orders of magnitude faster than existing methods, recovering rich combinatorial interactions previously inaccessible at scale. This work opens the door to principled, efficient, and scalable interpretability for black-box sequence models in biology.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced Feature Engineering for Multi-Factor Electricity Price Predictions</title>
<link>https://arxiv.org/abs/2505.11890</link>
<guid>https://arxiv.org/abs/2505.11890</guid>
<content:encoded><![CDATA[
<div> forecasting, electricity price, volatility, New South Wales, machine learning

Summary:<br /><br />
- Traditional forecasting models struggle in capturing the complex dynamics of electricity markets, leading to unreliable predictions in volatile markets like New South Wales.
- FAEP, a Feature-Augmented Electricity Price Prediction framework, addresses these challenges by leveraging Large Language Models (LLMs) and advanced feature engineering.
- By incorporating external features such as weather data and price volatility jumps, FAEP enhances prediction accuracy.
- Utilizing Retrieval-Augmented Generation (RAG) for feature extraction and a hybrid XGBoost-LSTM model, FAEP outperforms other models in the NSW electricity market.
- Experimental results demonstrate the state-of-the-art performance of FAEP in electricity price prediction, showcasing the efficiency of LLM-enhanced feature engineering and hybrid machine learning architectures. 

<br /><br />Summary: Hello, please summarize this article in en language, first extract 5 keywords, output in the same line, then line break, write a summary containing all the points in 200 words in en, output in order by points, and output in the following format '<br /><br />Summary:' , <br /> is the line break of HTML, 2 must be retained when output, and must be before the word 'Summary:' <div>
arXiv:2505.11890v1 Announce Type: new 
Abstract: Accurately forecasting electricity price volatility is crucial for effective risk management and decision-making. Traditional forecasting models often fall short in capturing the complex, non-linear dynamics of electricity markets, particularly when external factors like weather conditions and market volatility are involved. These limitations hinder their ability to provide reliable predictions in markets with high volatility, such as the New South Wales (NSW) electricity market. To address these challenges, we introduce FAEP, a Feature-Augmented Electricity Price Prediction framework. FAEP leverages Large Language Models (LLMs) combined with advanced feature engineering to enhance prediction accuracy. By incorporating external features such as weather data and price volatility jumps, and utilizing Retrieval-Augmented Generation (RAG) for effective feature extraction, FAEP overcomes the shortcomings of traditional approaches. A hybrid XGBoost-LSTM model in FAEP further refines these augmented features, resulting in a more robust prediction framework. Experimental results demonstrate that FAEP achieves state-of-art (SOTA) performance compared to other electricity price prediction models in the Australian New South Wale electricity market, showcasing the efficiency of LLM-enhanced feature engineering and hybrid machine learning architectures.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-based Discovery of Intrinsic Coordinates and Governing Equations from High-Dimensional Data</title>
<link>https://arxiv.org/abs/2505.11940</link>
<guid>https://arxiv.org/abs/2505.11940</guid>
<content:encoded><![CDATA[
<div> Keywords: governing equations, high-dimensional data, multimodal large language models, zero-shot method, spatial perception <br />
<br />
Summary: 
Discovering governing equations from high-dimensional scientific data is a challenging task due to the exponential expansion of the equation space. This paper introduces a novel zero-shot approach using multimodal large language models (MLLM) to automatically identify physical coordinates and equations from data. By enhancing MLLM's spatial perception with visual prompts and leveraging its domain knowledge, the proposed method effectively navigates the equation space. Evaluation on simulated and real data shows improved accuracy in discovering physical coordinates and equations. Long-term extrapolation accuracy is enhanced by approximately 26.96% compared to the baseline, showcasing the efficiency of the approach in understanding the evolution of systems. <div>
arXiv:2505.11940v1 Announce Type: new 
Abstract: Discovering governing equations from scientific data is crucial for understanding the evolution of systems, and is typically framed as a search problem within a candidate equation space. However, the high-dimensional nature of dynamical systems leads to an exponentially expanding equation space, making the search process extremely challenging. The visual perception and pre-trained scientific knowledge of multimodal large language models (MLLM) hold promise for providing effective navigation in high-dimensional equation spaces. In this paper, we propose a zero-shot method based on MLLM for automatically discovering physical coordinates and governing equations from high-dimensional data. Specifically, we design a series of enhanced visual prompts for MLLM to enhance its spatial perception. In addition, MLLM's domain knowledge is employed to navigate the search process within the equation space. Quantitative and qualitative evaluations on two representative types of systems demonstrate that the proposed method effectively discovers the physical coordinates and equations from both simulated and real experimental data, with long-term extrapolation accuracy improved by approximately 26.96% compared to the baseline.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Gas Well Performance with Decline Curve Analysis: A Case Study on Semutang Gas Field</title>
<link>https://arxiv.org/abs/2505.12333</link>
<guid>https://arxiv.org/abs/2505.12333</guid>
<content:encoded><![CDATA[
<div> DCA, production forecasting, gas reservoir, Semutang gas field, decline curve parameters <br />
<br />
Summary: <br />
Decline-curve analysis (DCA) is an important method for forecasting production and estimating reserves in gas reservoirs. This study applied DCA to predict future production performance and estimate ultimate recovery for a well in the Semutang gas field in Bangladesh. Different decline curve models were used, with the hyperbolic model providing the most accurate forecast. It is crucial to select the right decline model for precise production forecasting and reserve estimation, essential for effective reservoir management and resource optimization. The study emphasizes the significance of accurate decline curve analysis in the oil and gas industry. <div>
arXiv:2505.12333v1 Announce Type: new 
Abstract: Decline-curve analysis (DCA) is a widely utilized method for production forecasting and estimating remaining reserves in gas reservoir. Based on the assumptions that past production trend can be mathematically characterized and used to predict future performance. It relies on historical production data and assumes that production methods remain unchanged throughout the analysis. This method is particularly valuable due to its accuracy in forecasting and its broad acceptance within the industry. Wells in the same geographical area and producing from similar geological formations often exhibit similar decline curve parameters. This study applies DCA to forecast the future production performance and estimate the ultimate recovery for the Semutang gas field's well 5 in Bangladesh. Using historical production data, decline curves were generated based on exponential, hyperbolic, and harmonic model equations. The cumulative production estimations were 11,139.34 MMSCF for the exponential model, 11,620.26 MMSCF for the hyperbolic model, and 14,021.92 MMSCF for the harmonic model. In terms of the well's productive life, the estimates were 335.13 days, 1,152 days, and 22,611 days, respectively. Among these models, the hyperbolic decline provided the most realistic forecast, closely aligning with observed production trend. The study highlights the importance of selecting an appropriate decline model for accurate production forecasting and reserve estimation, which is essential for effective reservoir management and resource optimization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seismic analysis based on a new interval method with incomplete information</title>
<link>https://arxiv.org/abs/2505.12607</link>
<guid>https://arxiv.org/abs/2505.12607</guid>
<content:encoded><![CDATA[
<div> Keywords: seismic analysis, interval uncertainty, Monte Carlo simulation, covariance matrix adaptation evolution strategy, computational efficiency <br />
Summary: 
This study focuses on describing time-variant uncertain seismic accelerations using the minimum interval radius-based interval process (MRIP) based on the convex model for seismic analysis in engineering structures. To enhance computational efficiency in uncertainty analysis, the paper introduces the improved covariance matrix adaptation evolution strategy (CMA-ES) called DES-ES, which exhibits higher efficiency. Additionally, a computational framework named DES-ES-SS is proposed to leverage response dependency and further enhance computational efficiency while maintaining accuracy in interval uncertainty analysis of seismic structures under stationary or non-stationary seismic acceleration conditions. Numerical experiments validate the effectiveness of DES-ES-SS in improving computational efficiency without compromising accuracy. <br /><br /> <div>
arXiv:2505.12607v1 Announce Type: new 
Abstract: For seismic analysis in engineering structures, it is essential to consider the dynamic responses under seismic excitation, necessitating the description of seismic accelerations. Limit seismics samples lead to incomplete uncertainty information, which is described by the non-probabilistic method reasonable. This study employs the minimum interval radius-based interval process (MRIP) based on the convex model to describe the time-variant uncertain seismic acceleration, subsequently conducting uncertainty analysis for seismic structures. However, the Monte Carlo simulation for uncertainty analysis requires extensive deterministic computations to ensure accuracy, exhibiting poor computational efficiency. To address this issue, this paper first improves the covariance matrix adaptation evolution strategy (CMA-ES) through the dynamic evolution sequence, proposing DES-ES, whose efficiency is validated to be higher than that of CMA-ES. Furthermore, leveraging the dependency of the responses, a computational framework named DES-ES-SS is proposed. Numerical experiments demonstrate that DES-ES-SS improves computational efficiency while maintaining the accuracy of the interval uncertainty analysis of the seismic structures whether the seismic acceleration is stationary or non-stationary.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit differentiation with second-order derivatives and benchmarks in finite-element-based differentiable physics</title>
<link>https://arxiv.org/abs/2505.12646</link>
<guid>https://arxiv.org/abs/2505.12646</guid>
<content:encoded><![CDATA[
<div> differentiable programming, automatic differentiation, second-order derivatives, finite-element-based, PDE-constrained optimization

Summary:
This paper introduces a framework for computing second-order derivatives (Hessians) for implicit functions in finite-element-based differentiable physics. By utilizing primitive automatic differentiation tools, the authors develop an algorithm for efficiently calculating Hessian-vector products. Validation against finite difference approximations shows accurate results. Benchmark tests across various problem types demonstrate the advantages of using second-order information in optimization. The Newton-CG method with exact Hessians proves beneficial for nonlinear inverse problems like traction force identification and shape optimization, while the L-BFGS-B method is suitable for linear cases. These findings establish a solid foundation for incorporating second-order implicit differentiation into differentiable physics engines, enhancing optimization speed and reliability.<br /><br />Summary: <div>
arXiv:2505.12646v1 Announce Type: new 
Abstract: Differentiable programming is revolutionizing computational science by enabling automatic differentiation (AD) of numerical simulations. While first-order gradients are well-established, second-order derivatives (Hessians) for implicit functions in finite-element-based differentiable physics remain underexplored. This work bridges this gap by deriving and implementing a framework for implicit Hessian computation in PDE-constrained optimization problems. We leverage primitive AD tools (Jacobian-vector product/vector-Jacobian product) to build an algorithm for Hessian-vector products and validate the accuracy against finite difference approximations. Four benchmarks spanning linear/nonlinear, 2D/3D, and single/coupled-variable problems demonstrate the utility of second-order information. Results show that the Newton-CG method with exact Hessians accelerates convergence for nonlinear inverse problems (e.g., traction force identification, shape optimization), while the L-BFGS-B method suffices for linear cases. Our work provides a robust foundation for integrating second-order implicit differentiation into differentiable physics engines, enabling faster and more reliable optimization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid Topology Estimation using an Information Theoretic Approach</title>
<link>https://arxiv.org/abs/2505.11517</link>
<guid>https://arxiv.org/abs/2505.11517</guid>
<content:encoded><![CDATA[
<div> mutual information, power grid, graph modeling, maximum spanning tree, IEEE networks

Summary:
The article introduces an information-theoretic approach to estimate the topology of a power grid by modeling it as a graph and analyzing voltage magnitude data of individual nodes. The mutual information between pairs of nodes is computed using different approximation methods, leading to the estimation of the power grid topology through a maximum spanning tree based on mutual information, generated with the Chow-Liu algorithm. The approach is successfully optimized and validated on IEEE networks created with MATPOWER and GridLAB-D data, as well as on networks from the European Union Joint Research Council. The experiments and results demonstrate the effectiveness of this method in accurately estimating power grid topologies. <br /><br />Summary: <div>
arXiv:2505.11517v1 Announce Type: cross 
Abstract: The topology of a power grid is estimated using an information theoretic approach. By modeling the grid as a graph and using voltage magnitude data of individual nodes in the grid, the mutual information between pairs of nodes is computed using different approximation methods. Using the well-known Chow-Liu algorithm, a maximum spanning tree based on mutual information is computed to estimate the power grid topology. Experiments and results are presented to optimize this approach with success shown for IEEE networks generated with MATPOWER and data generated using GridLAB-D. The algorithm is then cross-validated on IEEE networks generated by the European Union Joint Research Council.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASL-HJX: A Comprehensive Guide to Solving Deterministic and Stochastic Hamilton-Jacobi Equations</title>
<link>https://arxiv.org/abs/2505.11527</link>
<guid>https://arxiv.org/abs/2505.11527</guid>
<content:encoded><![CDATA[
<div> framework, Hamilton-Jacobi equations, numerical methods, operator splitting techniques, neuroscience<br />
Summary:<br />
CASL-HJX is a computational framework designed for solving deterministic and stochastic Hamilton-Jacobi equations in two spatial dimensions. It integrates numerical methods for hyperbolic PDEs with operator splitting techniques and implements implicit methods for second-order derivative terms to ensure convergence to viscosity solutions. The high-performance C++ core efficiently handles mixed-order derivative systems with time-varying dynamics, making it suitable for real-world applications. The solver's versatility is demonstrated through tutorial examples and applications in neuroscience, enabling the design of energy-efficient controllers for regulating neural populations. CASL-HJX's modular architecture allows researchers to define computational domains, configure problems, and execute simulations with high numerical accuracy. It serves as a robust tool for managing uncertainty in complex dynamical systems, bridging the gap between deterministic control methods and stochastic models. <div>
arXiv:2505.11527v1 Announce Type: cross 
Abstract: CASL-HJX is a computational framework designed for solving deterministic and stochastic Hamilton-Jacobi equations in two spatial dimensions. It provides a flexible and efficient approach to modeling front propagation problems, optimal control problems, and stochastic Hamilton-Jacobi Bellman equations. The framework integrates numerical methods for hyperbolic PDEs with operator splitting techniques and implements implicit methods for second-order derivative terms, ensuring convergence to viscosity solutions while achieving global rather than local optimization. Built with a high-performance C++ core, CASL-HJX efficiently handles mixed-order derivative systems with time-varying dynamics, making it suitable for real-world applications across multiple domains. We demonstrate the solver's versatility through tutorial examples covering various PDEs and through applications in neuroscience, where it enables the design of energy-efficient controllers for regulating neural populations to mitigate pathological synchrony. While our examples focus on these applications, the mathematical foundation of the solver makes it applicable to problems in finance, engineering, and machine learning. The modular architecture allows researchers to define computational domains, configure problems, and execute simulations with high numerical accuracy. CASL-HJX bridges the gap between deterministic control methods and stochastic models, providing a robust tool for managing uncertainty in complex dynamical systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median</title>
<link>https://arxiv.org/abs/2505.11725</link>
<guid>https://arxiv.org/abs/2505.11725</guid>
<content:encoded><![CDATA[
<div> bootstrap, quantiles, central limit theorem, moment condition, asymptotic distributions

Summary:
This paper investigates the m-out-of-n bootstrap method for estimating sample quantiles, providing parameter-free guarantees for its soundness. A central limit theorem is established for a data-driven estimator under a mild moment condition, without the need for unknown nuisance parameters. The paper presents a counter-example demonstrating the tightness of the moment assumption and derives an Edgeworth expansion for exact convergence rates, along with a Berry Esseen bound for bootstrap approximation errors. The results are illustrated through applications to practical statistics like quantiles for random walk Metropolis-Hastings and rewards of ergodic Markov decision processes, showcasing the theory's utility in modern estimation and learning tasks.<br /><br />Summary: <div>
arXiv:2505.11725v1 Announce Type: cross 
Abstract: The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet (1992), approximates the distribution of a statistic by repeatedly drawing m subsamples (with m much smaller than n) without replacement from an original sample of size n. It is now routinely used for robust inference with heavy-tailed data, bandwidth selection, and other large-sample applications. Despite its broad applicability across econometrics, biostatistics, and machine learning, rigorous parameter-free guarantees for the soundness of the m-out-of-n bootstrap when estimating sample quantiles have remained elusive.
  This paper establishes such guarantees by analyzing the estimator of sample quantiles obtained from m-out-of-n resampling of a dataset of size n. We first prove a central limit theorem for a fully data-driven version of the estimator that holds under a mild moment condition and involves no unknown nuisance parameters. We then show that the moment assumption is essentially tight by constructing a counter-example in which the CLT fails. Strengthening the assumptions slightly, we derive an Edgeworth expansion that provides exact convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap approximation error. Finally, we illustrate the scope of our results by deriving parameter-free asymptotic distributions for practical statistics, including the quantiles for random walk Metropolis-Hastings and the rewards of ergodic Markov decision processes, thereby demonstrating the usefulness of our theory in modern estimation and learning tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
<link>https://arxiv.org/abs/2505.12638</link>
<guid>https://arxiv.org/abs/2505.12638</guid>
<content:encoded><![CDATA[
<div> ChromFound, scATAC-seq, single-cell, regulatory mechanisms, cell identification<br />
Summary:<br />
ChromFound is a foundation model designed for scATAC-seq data analysis, addressing challenges such as high dimensionality and lack of standardized OCR representation. It utilizes a hybrid architecture and genome-aware tokenization to capture genome-wide regulatory signals, trained on a large dataset for diverse tasks. The model demonstrates robust zero-shot performance, universal cell representation generation, and transferability in cell type annotation and cross-omics prediction. By identifying enhancer-gene links missed by other methods, ChromFound provides insights into disease risk variants in the noncoding genome. The model offers a promising framework for deciphering regulatory mechanisms and understanding complex chromatin landscapes in single-cell studies. <br /><br />Summary: <div>
arXiv:2505.12638v1 Announce Type: cross 
Abstract: The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present \textbf{ChromFound}, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Modeling of Random Fields from Limited Data via Constrained Latent Flow Matching</title>
<link>https://arxiv.org/abs/2505.13007</link>
<guid>https://arxiv.org/abs/2505.13007</guid>
<content:encoded><![CDATA[
<div> framework, generative modeling, random fields, latent flow matching, variational autoencoder <br />
Summary: 
- The article discusses a novel framework for generative modeling of random fields that incorporates domain knowledge to supplement limited data.
- The approach uses latent flow matching to operate on compressed function representations in the latent space of a pre-trained VAE.
- It includes a function decoder within the VAE and integrates physical/statistical constraints into the training process.
- The framework learns a latent function representation that generates continuous random field samples satisfying domain-specific constraints, even with sparse data.
- Two applications, wind velocity field reconstruction and material property inference, demonstrate the effectiveness of the framework in achieving improved reconstruction accuracy and inference with small training datasets. 
<br /> <div>
arXiv:2505.13007v1 Announce Type: cross 
Abstract: Deep generative models are promising tools for science and engineering, but their reliance on abundant, high-quality data limits applicability. We present a novel framework for generative modeling of random fields (probability distributions over continuous functions) that incorporates domain knowledge to supplement limited, sparse, and indirect data. The foundation of the approach is latent flow matching, where generative modeling occurs on compressed function representations in the latent space of a pre-trained variational autoencoder (VAE). Innovations include the adoption of a function decoder within the VAE and integration of physical/statistical constraints into the VAE training process. In this way, a latent function representation is learned that yields continuous random field samples satisfying domain-specific constraints when decoded, even in data-limited regimes. Efficacy is demonstrated on two challenging applications: wind velocity field reconstruction from sparse sensors and material property inference from a limited number of indirect measurements. Results show that the proposed framework achieves significant improvements in reconstruction accuracy compared to unconstrained methods and enables effective inference with relatively small training datasets that is intractable without constraints.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System reduction-based approximate reanalysis method for statically indeterminate structures with high-rank modification</title>
<link>https://arxiv.org/abs/2302.02171</link>
<guid>https://arxiv.org/abs/2302.02171</guid>
<content:encoded><![CDATA[
<div> spectral decomposition, structural reanalysis, high-rank modification, approximate static reanalysis, iterative solution <br />
Summary: <br />
The article proposes a novel approximate static reanalysis method for statically indeterminate structures with high-rank modification. The method involves dividing the structure into the basis system and additional components, rewriting equilibrium equations, and establishing a reduced equation system using spectral decomposition. The reduced system is solved using a pre-conditioned iterative solution algorithm to obtain approximate solutions for modified structures. The method shows excellent computational performance for structures with homogeneous material and functionally graded beams. The combination of system reduction and iterative solution technology proves to be an effective approach for developing high-performance reanalysis methods. <div>
arXiv:2302.02171v2 Announce Type: replace 
Abstract: Efficient structural reanalysis for high-rank modification plays an important role in engineering computations which require repeated evaluations of structural responses, such as structural optimization and probabilistic analysis. To improve the efficiency of engineering computations, a novel approximate static reanalysis method based on system reduction and iterative solution is proposed for statically indeterminate structures with high-rank modification. In this approach, a statically indeterminate structure is divided into the basis system and the additional components. Subsequently, the structural equilibrium equations are rewritten as the equation system with the stiffness matrix of the basis system and the pseudo forces derived from the additional elements. With the introduction of spectral decomposition, a reduced equation system with the element forces of the additional elements as the unknowns is established. Then, the approximate solutions of the modified structure can be obtained by solving the reduced equation system through a pre-conditioned iterative solution algorithm. The computational costs of the proposed method and the other two reanalysis methods are compared and numerical examples including static reanalysis and static nonlinear analysis are presented. The results demonstrate that the proposed method has excellent computational performance for both the structures with homogeneous material and structures composed of functionally graded beams. Meanwhile, the superiority of the proposed method indicates that the combination of system reduction and pre-conditioned iterative solution technology is an effective way to develop high-performance reanalysis methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSP-free adaptive Kriging surrogate model method for reliability analysis with small failure probability</title>
<link>https://arxiv.org/abs/2304.07010</link>
<guid>https://arxiv.org/abs/2304.07010</guid>
<content:encoded><![CDATA[
<div> Active learning reliability method, Kriging, Monte Carlo Simulation, Candidate Sample Pool, Particle Swarm Optimization

Summary:
The paper introduces a new method, CSP-free AK-MCS, to enhance reliability analysis in engineering. The method eliminates the reliance on Candidate Sample Pool (CSP) size, especially for systems with low failure probabilities. It consists of two stages: constructing a surrogate model and using Monte Carlo simulation. Surrogate model refinement is done iteratively with representative samples selected through Particle Swarm Optimization (PSO) algorithm. Adjustments like penalty intensity control and density control optimize the objective function, balancing accuracy and efficiency. Numerical examples prove the effectiveness of CSP-free AK-MCS in handling small failure probabilities, showing improved performance compared to conventional methods. This innovative approach overcomes limitations and provides exceptional results for reliability analysis. 

<br /><br />Summary: <div>
arXiv:2304.07010v5 Announce Type: replace 
Abstract: In the field of reliability engineering, the Active learning reliability method combining Kriging and Monte Carlo Simulation (AK-MCS) has been developed and demonstrated to be effective in reliability analysis. However, the performance of AK-MCS is sensitive to the size of Candidate Sample Pool (CSP), particularly for systems with small failure probabilities. To address the limitations of conventional AK-MCS that relies on CSP, this paper proposes a CSP-free AK-MCS. The proposed methodology consists of two stages: surrogate model construction and Monte Carlo simulation for estimating the failure probability. In the stage of surrogate model construction, the surrogate model is iteratively refined based on the representative samples selected by solving the optimization problem facilitated by Particle Swarm Optimization (PSO) algorithm. To achieve an optimal balance between solution accuracy and efficiency, the penalty intensity control and the density control for the experimental design points are introduced to modify the objective function in optimization. The performance of the proposed methodology is evaluated using numerical examples, and results indicate that by leveraging an optimization algorithm to select representative samples, the proposed CSP-free AK-MCS overcomes the limitations of conventional CSP-based AK-MCS and exhibits exceptional performance in addressing small failure probabilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A cable finite element formulation based on exact tension field for static nonlinear analysis of cable structures</title>
<link>https://arxiv.org/abs/2401.05609</link>
<guid>https://arxiv.org/abs/2401.05609</guid>
<content:encoded><![CDATA[
<div> beam theory, cable structures, finite element model, nonlinear analysis, numerical precision <br />
Summary: 
This paper introduces a numerically exact finite element model for analyzing the static nonlinear behavior of cable structures. The model accurately calculates the tension field by incorporating the geometrically exact beam theory and fundamental mechanical properties of cables. Unlike previous approaches, this model prioritizes numerical precision and universal applicability, deriving linearized equations with implicit expressions that include integrals. It also considers the variation in cross-sectional stiffness along the cable's length. The formulation ensures equilibrium and compatibility within cable elements, enabling accurate computation of internal forces, deformation states, and the unstrained length of the cable. The study discusses the implementation of solutions using the complete tangent matrix and internal iterations within elements. Numerical examples validate the effectiveness of the proposed cable element. <div>
arXiv:2401.05609v5 Announce Type: replace 
Abstract: This paper presents a numerically exact cable finite element model for static nonlinear analysis of cable structures. The model derives the exact expression of the tension field using the geometrically exact beam theory coupled with the fundamental mechanical characteristics of cables. The equations for the cable element are formulated by addressing the equilibrium conditions at the element boundaries and ensuring compatibility within the element. Unlike previous studies that typically provide explicit expressions for cable models, this study develops a formulation that emphasizes numerical precision and broad applicability. It achieves this by deriving linearized equations with implicit expressions incorporating integrals. The proposed model accurately computes internal forces and deformation states, and determines the unstrained length of the cable. Additionally, it accounts for the variability in cross-sectional stiffness along the cable's length. The paper discusses solution implementations using the complete tangent matrix and element internal iterations. The effectiveness of the proposed cable element is demonstrated through numerical examples.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enforced Interface Constraints for Domain Decomposition Method of Discrete Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.10925</link>
<guid>https://arxiv.org/abs/2505.10925</guid>
<content:encoded><![CDATA[
<div> Discrete Physics-Informed Neural Network, Domain Decomposition Method, Gaussian quadrature, Interface constraints, Computational efficiency<br />
<br />
Summary:
The study introduces a discrete physics-informed neural network (dPINN) framework with enforced interface constraints (EIC) for modeling physical systems using the domain decomposition method (DDM). The dPINN accurately calculates system energy through element-wise integration using Gaussian quadrature. The EIC enforces interfacial displacement constraints to ensure physical field continuity without auxiliary sampling or loss penalties, supporting independent meshing in each subdomain. By eliminating weak spatial constraints (WSC), the EIC-dPINN provides more stable and physically consistent predictions. Extensive numerical experiments in two and three dimensions confirm the framework's accuracy and showcase computational efficiency gains through parallel training. The results illustrate the framework's scalability, robustness, and potential for solving large-scale, geometrically complex problems.<br /><br />Summary: <div>
arXiv:2505.10925v1 Announce Type: new 
Abstract: This study presents a discrete physics-informed neural network (dPINN) framework, enhanced with enforced interface constraints (EIC), for modeling physical systems using the domain decomposition method (DDM). Built upon finite element-style mesh discretization, the dPINN accurately evaluates system energy through Gaussian quadrature-based element-wise integration. To ensure physical field continuity across subdomain interfaces, the EIC mechanism enforces interfacial displacement constraints without requiring auxiliary sampling or loss penalties.This formulation supports independent meshing in each subdomain, simplifying preprocessing and improving computational flexibility. Additionally, by eliminating the influence of weak spatial constraints (WSC) commonly observed in traditional PINNs, the EIC-dPINN delivers more stable and physically consistent predictions.Extensive two- and three-dimensional numerical experiments validate the proposed framework's accuracy and demonstrate the computational efficiency gains achieved through parallel training. The results highlight the framework's scalability, robustness, and potential for solving large-scale, geometrically complex problems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking</title>
<link>https://arxiv.org/abs/2505.11065</link>
<guid>https://arxiv.org/abs/2505.11065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, DeepFund, real-time market data, fund investment, evaluation

Summary:<br />
Large Language Models (LLMs) have shown promise in financial tasks but their effectiveness in fund investment remains unexplored. To address limitations in evaluating LLM-driven trading strategies, a new benchmark tool called DeepFund is introduced. DeepFund directly connects with real-time stock market data to prevent information leakage and provide fair evaluations. Test results on nine top LLMs reveal challenges in various investment dimensions, showcasing practical limitations in active fund management. Even advanced models like DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses in the real-time evaluation environment of DeepFund. This highlights the current constraints of LLMs in active fund management.<br /> <div>
arXiv:2505.11065v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated notable capabilities across financial tasks, including financial report summarization, earnings call transcript analysis, and asset classification. However, their real-world effectiveness in managing complex fund investment remains inadequately assessed. A fundamental limitation of existing benchmarks for evaluating LLM-driven trading strategies is their reliance on historical back-testing, inadvertently enabling LLMs to "time travel"-leveraging future information embedded in their training corpora, thus resulting in possible information leakage and overly optimistic performance estimates. To address this issue, we introduce DeepFund, a live fund benchmark tool designed to rigorously evaluate LLM in real-time market conditions. Utilizing a multi-agent architecture, DeepFund connects directly with real-time stock market data-specifically data published after each model pretraining cutoff-to ensure fair and leakage-free evaluations. Empirical tests on nine flagship LLMs from leading global institutions across multiple investment dimensions-including ticker-level analysis, investment decision-making, portfolio management, and risk control-reveal significant practical challenges. Notably, even cutting-edge models such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses within DeepFund real-time evaluation environment, underscoring the present limitations of LLMs for active fund management. Our code is available at https://github.com/HKUSTDial/DeepFund.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment</title>
<link>https://arxiv.org/abs/2505.11194</link>
<guid>https://arxiv.org/abs/2505.11194</guid>
<content:encoded><![CDATA[
<div> sequence-to-text model, protein function prediction, multimodal, contrastive alignment learning, fine-tuning

Summary:
Prot2Text-V2 is a novel multimodal sequence-to-text model that generates natural language descriptions of protein function directly from amino acid sequences. This model combines a protein language encoder with a decoder-only language model through a modality projector, improving cross-modal learning through Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE). The model is trained on curated entries from SwissProt and performs well under low-homology conditions. Prot2Text-V2 outperforms traditional and LLM-based baselines across various metrics. The combination of contrasting mean- and std-pooled protein embeddings with text representations, along with instruction-based fine-tuning using LoRA, results in accurate protein function description generation conditioned on the protein sequence.<br /><br />Summary: <div>
arXiv:2505.11194v1 Announce Type: new 
Abstract: Predicting protein function from sequence is a central challenge in computational biology. While existing methods rely heavily on structured ontologies or similarity-based techniques, they often lack the flexibility to express structure-free functional descriptions and novel biological functions. In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text model that generates free-form natural language descriptions of protein function directly from amino acid sequences. Our method combines a protein language model as a sequence encoder (ESM-3B) and a decoder-only language model (LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A key innovation is our Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE), which improves cross-modal learning by matching mean- and std-pooled protein embeddings with text representations via contrastive loss. After the alignment phase, we apply instruction-based fine-tuning using LoRA on the decoder to teach the model how to generate accurate protein function descriptions conditioned on the protein sequence. We train Prot2Text-V2 on about 250K curated entries from SwissProt and evaluate it under low-homology conditions, where test sequences have low similarity with training samples. Prot2Text-V2 consistently outperforms traditional and LLM-based baselines across various metrics.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11208</link>
<guid>https://arxiv.org/abs/2505.11208</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, analog circuit design, variation-aware design, robustness, PVT variations

Summary:
GLOVA is introduced as an analog circuit sizing framework to address challenges in variation-aware design. It utilizes risk-sensitive reinforcement learning to improve robustness against PVT variations, with an ensemble-based critic for efficient learning. A $\mu$-$\sigma$ evaluation and simulation reordering method is proposed for design verification, reducing simulation costs. GLOVA supports industrial-level PVT variation evaluation methods, including corner simulation and global/local Monte Carlo simulations. Compared to existing frameworks, GLOVA demonstrates significant improvements in sample efficiency and time, achieving up to 80.5$\times$ improvement in sample efficiency and 76.0$\times$ reduction in time. <div>
arXiv:2505.11208v1 Announce Type: cross 
Abstract: Analog/mixed-signal circuit design encounters significant challenges due to performance degradation from process, voltage, and temperature (PVT) variations. To achieve commercial-grade reliability, iterative manual design revisions and extensive statistical simulations are required. While several studies have aimed to automate variation aware analog design to reduce time-to-market, the substantial mismatches in real-world wafers have not been thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing framework that effectively manages the impact of diverse random mismatches to improve robustness against PVT variations. In the proposed approach, risk-sensitive reinforcement learning is leveraged to account for the reliability bound affected by PVT variations, and ensemble-based critic is introduced to achieve sample-efficient learning. For design verification, we also propose $\mu$-$\sigma$ evaluation and simulation reordering method to reduce simulation costs of identifying failed designs. GLOVA supports verification through industrial-level PVT variation evaluation methods, including corner simulation as well as global and local Monte Carlo (MC) simulations. Compared to previous state-of-the-art variation-aware analog sizing frameworks, GLOVA achieves up to 80.5$\times$ improvement in sample efficiency and 76.0$\times$ reduction in time.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets</title>
<link>https://arxiv.org/abs/2502.01506</link>
<guid>https://arxiv.org/abs/2502.01506</guid>
<content:encoded><![CDATA[
<div> Keywords: social emergence, large language model agents, multi-agent framework, socio-economic systems, emergent phenomena

Summary: 
 This study introduces TwinMarket, a new framework that uses large language model agents to simulate socio-economic systems. Traditional modeling approaches struggle to capture the complexity of human behavior, but TwinMarket's use of LLMs allows for more realistic simulations. By focusing on individual behaviors and their interactions, the framework demonstrates how collective dynamics and emergent phenomena can arise. The experiments conducted in a simulated stock market environment show how individual actions can lead to group behaviors, resulting in outcomes such as financial bubbles and recessions. This approach provides valuable insights into the intricate relationship between individual decision-making and collective socio-economic patterns. Overall, TwinMarket offers a promising avenue for studying social emergence and understanding the complexities of human behavior in socio-economic systems. 

<br /><br /> <div>
arXiv:2502.01506v3 Announce Type: replace 
Abstract: The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Network Structure Search with Program Synthesis</title>
<link>https://arxiv.org/abs/2502.02711</link>
<guid>https://arxiv.org/abs/2502.02711</guid>
<content:encoded><![CDATA[
<div> tensor networks, data compression, program synthesis, constraint-based assessment, search efficiency 

Summary: 
Tensor networks are effective for compressing multi-dimensional data, but finding the optimal structure is challenging. This study proposes a novel approach that views tensor network structure search as a program synthesis problem. By establishing a link between transformation programs and network structures, the method reduces the search space using output-directed splits. A synthesis algorithm identifies promising network candidates through constraint solving, minimizing the need for costly tensor decomposition. Experimental results demonstrate a significant improvement in search speed, achieving compression ratios surpassing state-of-the-art methods by 1.5 to 3 times. The approach also scales effectively to larger tensors and generalizes well to similar data, outperforming generic structures with compression ratios up to 2.4 times better. This enhanced efficiency and effectiveness make the proposed method a valuable tool for data compression tasks. 

<br /><br />Summary: <div>
arXiv:2502.02711v3 Announce Type: replace 
Abstract: Tensor networks provide a powerful framework for compressing multi-dimensional data. The optimal tensor network structure for a given data tensor depends on both data characteristics and specific optimality criteria, making tensor network structure search a difficult problem. Existing solutions typically rely on sampling and compressing numerous candidate structures; these procedures are computationally expensive and therefore limiting for practical applications. We address this challenge by viewing tensor network structure search as a program synthesis problem and introducing an efficient constraint-based assessment method that avoids costly tensor decomposition. Specifically, we establish a correspondence between transformation programs and network structures. We also design a novel operation named output-directed splits to reduce the search space without hindering expressiveness. We then propose a synthesis algorithm to identify promising network candidates through constraint solving, and avoid tensor decomposition for all but the most promising candidates. Experimental results show that our approach improves search speed by up to $10\times$ and achieves compression ratios $1.5\times$ to $3\times$ better than state-of-the-art. Notably, our approach scales to larger tensors that are unattainable by prior work. Furthermore, the discovered topologies generalize well to similar data, yielding compression ratios up to $ 2.4\times$ better than a generic structure while the runtime remains around $110$ seconds.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Market Environments for FinRL Contests</title>
<link>https://arxiv.org/abs/2504.02281</link>
<guid>https://arxiv.org/abs/2504.02281</guid>
<content:encoded><![CDATA[
<div> Contests, FinRL, financial tasks, datasets, evaluation protocols<br />
<br />
Summary: Financial reinforcement learning (FinRL) has shown promise in sequential decision-making in finance, but faces challenges in real-world trading. To address this, three FinRL Contests were organized from 2023 to 2025, covering various financial tasks with standardized definitions, datasets, environments, and baselines. Over 200 participants from 100 institutions and 22 countries joined, with open-source starter kits provided for reproducibility. The contests focused on stock trading, order execution, cryptocurrency trading, and the use of large language model-generated signals. The efforts included task formulations, data curation pipelines, environment implementations, evaluation protocols, participant performance analysis, and organizational insights. These benchmarking initiatives have enhanced the understanding and development of FinRL methods for financial applications. <br /><br /> <div>
arXiv:2504.02281v3 Announce Type: replace 
Abstract: Financial reinforcement learning (FinRL) has emerged as a promising paradigm for sequential decision-making in financial engineering. However, applying RL in real-world trading tasks remains challenging due to the non-stationarity of financial data, low signal-to-noise ratios, and various market frictions. Although numerous FinRL methods have been developed for tasks such as trading and portfolio management, the lack of standardized task definitions, datasets, environments, and baselines has hindered consistent evaluation and reproducibility. To bridge this gap, we organized three FinRL Contests from 2023 to 2025, covering a diverse range of financial tasks such as stock trading, order execution, cryptocurrency trading, and the use of large language model (LLM)-generated signals. These contests attracted 200 participants from over 100 institutions across 22 countries. To promote reproduction, we provided open-source starter kits featuring GPU-optimized parallel market environments and comprehensive documentation. In this paper, we summarize these benchmarking efforts, detailing task formulations, data curation pipelines, environment implementations, evaluation protocols, participant performance, and key organizational insights.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promise of Data-Driven Modeling and Decision Support for Precision Oncology and Theranostics</title>
<link>https://arxiv.org/abs/2505.09899</link>
<guid>https://arxiv.org/abs/2505.09899</guid>
<content:encoded><![CDATA[
<div> Keywords: Cancer, Theranostics, Precision oncology, Reinforcement learning, Data-driven decision support 

Summary: 
This paper discusses the potential of theranostics in precision oncology by combining molecular imaging with targeted therapy for personalized cancer treatment. It explores current data-driven decision support applications with a focus on reinforcement learning to optimize patient-specific care plans for cancer treatment. The study reviews the training environments, state-space representation, performance evaluation criteria, and the measurement of risk and reward in precision oncology applications. It identifies key challenges in the field and proposes a framework that integrates data-driven modeling with reinforcement learning-based decision support to optimize radiopharmaceutical therapy dosing. The framework utilizes advanced technologies such as Neural Ordinary Differential Equations and Physics-Informed Neural Networks to enhance Physiologically Based Pharmacokinetic models. By employing reinforcement learning algorithms, it iteratively refines treatment policies based on individual patient data, aiming to improve outcomes in cancer therapy. 

<br /><br />Summary: <div>
arXiv:2505.09899v1 Announce Type: new 
Abstract: Cancer remains a leading cause of death worldwide, necessitating personalized treatment approaches to improve outcomes. Theranostics, combining molecular-level imaging with targeted therapy, offers potential for precision oncology but requires optimized, patient-specific care plans. This paper investigates state-of-the-art data-driven decision support applications with a reinforcement learning focus in precision oncology. We review current applications, training environments, state-space representation, performance evaluation criteria, and measurement of risk and reward, highlighting key challenges. We propose a framework integrating data-driven modeling with reinforcement learning-based decision support to optimize radiopharmaceutical therapy dosing, addressing identified challenges and setting directions for future research. The framework leverages Neural Ordinary Differential Equations and Physics-Informed Neural Networks to enhance Physiologically Based Pharmacokinetic models while applying reinforcement learning algorithms to iteratively refine treatment policies based on patient-specific data.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical regularized Hierarchical Generative Model for Metallic Glass Structural Generation and Energy Prediction</title>
<link>https://arxiv.org/abs/2505.09977</link>
<guid>https://arxiv.org/abs/2505.09977</guid>
<content:encoded><![CDATA[
<div> variational autoencoder, disordered materials, glass, generative AI models, physics-informed regularizers <br />
Summary: 
The article introduces GlassVAE, a hierarchical graph variational autoencoder designed to handle the complexity of disordered materials like glasses. GlassVAE uses graph representations to create rotation, translation, and permutation invariant embeddings of atomic configurations, enabling efficient generation of realistic structures and exploration of the glass energy landscape. To ensure structural realism and physical fidelity, GlassVAE is augmented with two physics-informed regularizers: a radial distribution function (RDF) loss and an energy regression loss. The regularizers play a crucial role in enhancing the accuracy of the model. By encoding high-dimensional atomistic data into a compact latent vector and decoding it to generate structures with accurate energy predictions, GlassVAE provides a rapid, physics-aware approach for modeling and designing disordered materials. <br /><br /> <div>
arXiv:2505.09977v1 Announce Type: new 
Abstract: Disordered materials such as glasses, unlike crystals, lack long range atomic order and have no periodic unit cells, yielding a high dimensional configuration space with widely varying properties. The complexity not only increases computational costs for atomistic simulations but also makes it difficult for generative AI models to deliver accurate property predictions and realistic structure generation. In this work, we introduce GlassVAE, a hierarchical graph variational autoencoder that uses graph representations to learn compact, rotation, translation, and permutation invariant embeddings of atomic configurations. The resulting structured latent space not only enables efficient generation of novel, physically plausible structures but also supports exploration of the glass energy landscape. To enforce structural realism and physical fidelity, we augment GlassVAE with two physics informed regularizers, a radial distribution function (RDF) loss that captures characteristic short and medium range ordering and an energy regression loss that reflects the broad configurational energetics. Both theoretical analysis and experimental results highlight the critical impact of these regularizers. By encoding high dimensional atomistic data into a compact latent vector and decoding it into structures with accurate energy predictions, GlassVAE provides a fast, physics aware path for modeling and designing disordered materials.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Based Aerospace Engineering -- A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2505.10142</link>
<guid>https://arxiv.org/abs/2505.10142</guid>
<content:encoded><![CDATA[
<div> Knowledge-Based Engineering, aerospace industry, knowledge management, SWARM-SLR, knowledge graph<br />
<br />
Summary: 
The study explores state-of-the-art knowledge management practices in aerospace engineering. It involves an extensive review of over 1,000 articles using SWARM-SLR methodology, qualitative analysis of 164 selected articles, and input from aerospace engineering domain experts. The research results include the creation of a knowledge graph comprising over 700 aerospace engineering processes, software, and data, formalized in the Web Ontology Language (OWL) and mapped to Wikidata entries. The knowledge graph is available on the Open Research Knowledge Graph (ORKG) and an aerospace Wikibase for further collaboration and knowledge exchange. The study highlights the importance of structured, semantic-based approaches in managing aerospace engineering knowledge, aiming to improve design processes, foster collaboration, and promote sustainable aviation practices. The intermediate and final artifacts of the study are documented in a Zenodo dataset for wider dissemination and reuse. <div>
arXiv:2505.10142v1 Announce Type: new 
Abstract: The aerospace industry operates at the frontier of technological innovation while maintaining high standards regarding safety and reliability. In this environment, with an enormous potential for re-use and adaptation of existing solutions and methods, Knowledge-Based Engineering (KBE) has been applied for decades. The objective of this study is to identify and examine state-of-the-art knowledge management practices in the field of aerospace engineering. Our contributions include: 1) A SWARM-SLR of over 1,000 articles with qualitative analysis of 164 selected articles, supported by two aerospace engineering domain expert surveys. 2) A knowledge graph of over 700 knowledge-based aerospace engineering processes, software, and data, formalized in the interoperable Web Ontology Language (OWL) and mapped to Wikidata entries where possible. The knowledge graph is represented on the Open Research Knowledge Graph (ORKG), and an aerospace Wikibase, for reuse and continuation of structuring aerospace engineering knowledge exchange. 3) Our resulting intermediate and final artifacts of the knowledge synthesis, available as a Zenodo dataset. This review sets a precedent for structured, semantic-based approaches to managing aerospace engineering knowledge. By advancing these principles, research, and industry can achieve more efficient design processes, enhanced collaboration, and a stronger commitment to sustainable aviation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Space-Time Multigrid Methods Suitable for Topology Optimisation of Transient Heat Conduction</title>
<link>https://arxiv.org/abs/2505.10168</link>
<guid>https://arxiv.org/abs/2505.10168</guid>
<content:encoded><![CDATA[
<div> topology optimization, transient heat conduction, Space-Time MultiGrid methods, diffusivity, adjoint problem <br />
Summary: <br />
This paper introduces Space-Time MultiGrid (STMG) methods for topology optimization in transient heat conduction problems. The methods employ pointwise smoothers and uniform Cartesian space-time meshes, with a coarsening strategy based on the geometric mean of minimum and maximum diffusivity for high contrast problems. Different discretization methods for coarse levels were tested, with averaging thermal resistivities on finer levels showing best results for one-dimensional cases. A proposed coarsening strategy ensuring spatial resolution on coarse grids yielded mixed results. The STMG methods successfully served as a solver for one-dimensional topology optimization, including solving the adjoint problem. The methods were robust and converged reliably during optimization cycles. They proved effective for the adjoint problem even when the prolongation operator only forwards information in time, despite the adjoint problem moving backwards in time. <div>
arXiv:2505.10168v1 Announce Type: new 
Abstract: This paper presents Space-Time MultiGrid (STMG) methods which are suitable for performing topology optimisation of transient heat conduction problems. The proposed methods use a pointwise smoother and uniform Cartesian space-time meshes. For problems with high contrast in the diffusivity, it was found that it is beneficial to define a coarsening strategy based on the geometric mean of the minimum and maximum diffusivity. However, other coarsening strategies may be better for other smoothers. Several methods of discretising the coarse levels were tested. Of these, it was best to use a method which averages the thermal resistivities on the finer levels. However, this was likely a consequence of the fact that only one spatial dimension was considered for the test problems. A second coarsening strategy was proposed which ensures spatial resolution on the coarse grids. Mixed results were found for this strategy. The proposed STMG methods were used as a solver for a one-dimensional topology optimisation problem. In this context, the adjoint problem was also solved using the STMG methods. The STMG methods were sufficiently robust for this application, since they converged during every optimisation cycle. It was found that the STMG methods also work for the adjoint problem when the prolongation operator only sends information forwards in time, even although the direction of time for the adjoint problem is backwards.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture</title>
<link>https://arxiv.org/abs/2505.09907</link>
<guid>https://arxiv.org/abs/2505.09907</guid>
<content:encoded><![CDATA[
<div> Keywords: Hass avocados, price forecasting, deep learning, TCN-MLP-Attention Architecture, agricultural markets <br />
Summary:  
- The study addresses the importance of price forecasting for high-value crops like Hass avocados due to the demand for healthy foods.
- The proposed hybrid deep learning model, TCN-MLP-Attention Architecture, combines TCN, MLP, and an Attention mechanism to handle complex price fluctuations influenced by various factors.
- The dataset consists of over 50,000 records of Hass avocado sales in the U.S. from 2015 to 2018, including variables like sales volume, average price, region, weather, and variety type.
- After systematic preprocessing, the model demonstrated excellent predictive performance, outperforming traditional methods with an RMSE of 1.23 and an MSE of 1.51.
- The research offers a scalable and effective approach for time series forecasting in agricultural markets, providing valuable insights for intelligent supply chain management and price strategy optimization. 

<br /><br />Summary: <div>
arXiv:2505.09907v1 Announce Type: cross 
Abstract: With the growing demand for healthy foods, agricultural product price forecasting has become increasingly important. Hass avocados, as a high-value crop, exhibit complex price fluctuations influenced by factors such as seasonality, region, and weather. Traditional prediction models often struggle with highly nonlinear and dynamic data. To address this, we propose a hybrid deep learning model, TCN-MLP-Attention Architecture, combining Temporal Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for dynamic feature weighting. The dataset used covers over 50,000 records of Hass avocado sales across the U.S. from 2015 to 2018, including variables such as sales volume, average price, time, region, weather, and variety type, collected from point-of-sale systems and the Hass Avocado Board. After systematic preprocessing, including missing value imputation and feature normalization, the proposed model was trained and evaluated. Experimental results demonstrate that the TCN-MLP-Attention model achieves excellent predictive performance, with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods. This research provides a scalable and effective approach for time series forecasting in agricultural markets and offers valuable insights for intelligent supply chain management and price strategy optimization.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Foundation Model for Chemical Reactor Modeling: Meta-Learning with Physics-Informed Adaptation</title>
<link>https://arxiv.org/abs/2405.11752</link>
<guid>https://arxiv.org/abs/2405.11752</guid>
<content:encoded><![CDATA[
<div> neural network framework, chemical reactors, meta-learning, physics-informed, generalizable

Summary:
This work introduces a neural network framework for chemical reactor modeling that can generalize across different reactor types and quickly adapt to new chemical processes. By leveraging meta-learning, the model is pretrained on a wide range of reactor dynamics, allowing for efficient adaptation to unseen reactions with minimal data. Additionally, physics-informed fine-tuning is incorporated to ensure consistent adaptation to new reactor conditions. The framework is evaluated on three fundamental reactor types, showing superior few-shot adaptation compared to traditional approaches. By combining meta-learning with physics-informed techniques, this work paves the way for a more generalizable modeling framework in chemical engineering applications. <div>
arXiv:2405.11752v3 Announce Type: replace 
Abstract: Developing accurate models for chemical reactors is often challenging due to the complexity of reaction kinetics and process dynamics. Traditional approaches require retraining models for each new system, limiting generalizability and efficiency. In this work, we take a step toward foundation models for chemical reactor modeling by introducing a neural network framework that generalizes across diverse reactor types and rapidly adapts to new chemical processes. Our approach leverages meta-learning to pretrain the model on a broad set of reactor dynamics, enabling efficient adaptation to unseen reactions with minimal data. To further enhance generalizability, we incorporate physics-informed fine-tuning, ensuring physically consistent adaptation to new reactor conditions. Our framework is evaluated across three integer-order fundamental reactor types - continuous stirred tank reactors, batch reactors, and plug flow reactors - demonstrating superior few-shot adaptation compared to conventional data-driven, physics-informed, and transfer learning approaches. By combining meta-learning with physics-informed adaptation, this work lays the foundation for a generalizable modeling framework, advancing the development of foundation models for chemical engineering applications. Source code is available at https://github.com/killingbear999/chemical-reactor-foundation-model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2406.00367</link>
<guid>https://arxiv.org/abs/2406.00367</guid>
<content:encoded><![CDATA[
<div> RoBERTa-BiLSTM, sentiment analysis, deep learning, Transformer, latent intentions
<br />
Summary:
The article introduces a hybrid deep learning model, RoBERTa-BiLSTM, combining RoBERTa and BiLSTM for sentiment analysis. Challenges in sentiment analysis include lexical diversity, long dependencies, unknown symbols, and imbalanced datasets. Existing models like BERT and RoBERTa face issues with processing time due to sequential nature. The RoBERTa-BiLSTM model aims to address these challenges by leveraging RoBERTa for word embeddings and BiLSTM for contextual semantics. Experimental results on IMDb, Twitter US Airline, and Sentiment140 datasets show RoBERTa-BiLSTM outperforming baseline models like BERT and RoBERTa, achieving high accuracies and F1 scores. The model achieves accuracies of 80.74%, 92.36%, and 82.25% on the respective datasets, with corresponding F1 scores. This hybrid approach demonstrates improved performance in sentiment analysis tasks. 
<br /> <div>
arXiv:2406.00367v2 Announce Type: replace-cross 
Abstract: Effectively analyzing the comments to uncover latent intentions holds immense value in making strategic decisions across various domains. However, several challenges hinder the process of sentiment analysis including the lexical diversity exhibited in comments, the presence of long dependencies within the text, encountering unknown symbols and words, and dealing with imbalanced datasets. Moreover, existing sentiment analysis tasks mostly leveraged sequential models to encode the long dependent texts and it requires longer execution time as it processes the text sequentially. In contrast, the Transformer requires less execution time due to its parallel processing nature. In this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM, which combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with Bidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to generate meaningful word embedding vectors, while BiLSTM effectively captures the contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid model leverages the strengths of both sequential and Transformer models to enhance performance in sentiment analysis. We conducted experiments using datasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the proposed model against existing state-of-the-art methods. Our experimental findings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models (e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies of 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140 datasets, respectively. Additionally, the model achieves F1-scores of 80.73%, 92.35%, and 82.25% on the same datasets, respectively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Digital Twins with Quantified Uncertainty for Patient-Specific Decision Making in Oncology</title>
<link>https://arxiv.org/abs/2505.08927</link>
<guid>https://arxiv.org/abs/2505.08927</guid>
<content:encoded><![CDATA[
<div> methodology, digital twins, personalized medicine, uncertainty quantification, tumor progression

Summary: 
This study introduces an end-to-end methodology for personalized modeling in biomedicine, using digital twins to improve individual patient outcomes by combining patient data with mechanistic models of disease progression. The approach integrates longitudinal non-invasive imaging data with a reaction-diffusion model to estimate and predict spatiotemporal tumor progression while considering patient-specific anatomy. By solving a statistical inverse problem, imaging data inform spatially varying model parameters. An efficient parallel implementation of the forward model and scalable Bayesian posterior distribution approximation enable rigorous uncertainty quantification due to sparse, noisy measurements. The methodology is validated using synthetic data on a virtual patient to account for model limitations, noise level, and data frequency. Decision-making applications are demonstrated by evaluating the impact of imaging frequency and optimal experimental design. Clinical relevance is illustrated through model validation on a cohort of patients with longitudinal imaging data, showcasing the potential for risk-informed personalized medicine. 

<br /><br />Summary: <div>
arXiv:2505.08927v1 Announce Type: new 
Abstract: Quantifying the uncertainty in predictive models is critical for establishing trust and enabling risk-informed decision making for personalized medicine. In contrast to one-size-fits-all approaches that seek to mitigate risk at the population level, digital twins enable personalized modeling thereby potentially improving individual patient outcomes. Realizing digital twins in biomedicine requires scalable and efficient methods to integrate patient data with mechanistic models of disease progression. This study develops an end-to-end data-to-decisions methodology that combines longitudinal non-invasive imaging data with mechanistic models to estimate and predict spatiotemporal tumor progression accounting for patient-specific anatomy. Through the solution of a statistical inverse problem, imaging data inform the spatially varying parameters of a reaction-diffusion model of tumor progression. An efficient parallel implementation of the forward model coupled with a scalable approximation of the Bayesian posterior distribution enables rigorous, but tractable, quantification of uncertainty due to the sparse, noisy measurements. The methodology is verified on a virtual patient with synthetic data to control for model inadequacy, noise level, and the frequency of data collection. The application to decision-making is illustrated by evaluating the importance of imaging frequency and formulating an optimal experimental design question. The clinical relevance is demonstrated through a model validation study on a cohort of patients with publicly available longitudinal imaging data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of the initial post-buckling response of trusses and frames by an asymptotic approach</title>
<link>https://arxiv.org/abs/2505.09373</link>
<guid>https://arxiv.org/abs/2505.09373</guid>
<content:encoded><![CDATA[
<div> asymptotic theory, post-buckling, trusses, frames, topology optimization
<br />
Summary: 
The article explores the application of asymptotic post-buckling theory in sizing and topology optimization of trusses and frames, highlighting its potential benefits and existing computational challenges. By incorporating the lowest two asymptotic coefficients in the optimization formulation, representing initial post-buckling slope and curvature, designers can control the post-buckling response and reduce imperfection sensitivity in optimized designs. The asymptotic expansion enables the approximation of structural nonlinear response, allowing optimization for specific measures of nonlinear mechanical performance such as end-compliance or complementary work. The study demonstrates the effective use of the asymptotic method in including post-buckling constraints in structural optimization through examples of linear and nonlinear compliance minimization for trusses and frames. <div>
arXiv:2505.09373v1 Announce Type: new 
Abstract: Asymptotic post-buckling theory is applied to sizing and topology optimization of trusses and frames, exploring its potential and current computational difficulties. We show that a designs' post-buckling response can be controlled by including the lowest two asymptotic coefficients, representing the initial post-buckling slope and curvature, in the optimization formulation. This also reduces the imperfection sensitivity of the optimized design. The asymptotic expansion can further be used to approximate the structural nonlinear response, and then to optimize for a given measure of the nonlinear mechanical performance such as, for example, end-compliance or complementary work. Examples of linear and nonlinear compliance minimization of trusses and frames show the effective use of the asymptotic method for including post-buckling constraints in structural optimization.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radon Exposure Dataset</title>
<link>https://arxiv.org/abs/2505.09489</link>
<guid>https://arxiv.org/abs/2505.09489</guid>
<content:encoded><![CDATA[
<div> Keywords: radon, lung cancer, dataset, modeling, prediction

Summary: 
The study focuses on creating a comprehensive dataset for modeling and predicting household radon concentrations at small scales in Pennsylvania and Utah. It aims to identify at-risk populations in areas with high radon levels by examining geological and demographic factors. The dataset combines information on temperature, geochemistry, soil characteristics, and demographic variables such as heating fuel used and building age. This data serves as a foundational resource for future studies and can be scaled up to predict radon exposure potential at a national level. By identifying populations at risk, this research helps in mitigating the risk of lung cancer due to elevated radon levels in homes.<br /><br />Summary: <div>
arXiv:2505.09489v1 Announce Type: new 
Abstract: Exposure to elevated radon levels in the home is one of the leading causes of lung cancer in the world. The following study describes the creation of a comprehensive, state-level dataset designed to enable the modeling and prediction of household radon concentrations at Zip Code Tabulation Area (ZCTA) and sub-kilometer scales. Details include the data collection and processing involved in compiling physical and demographic factors for Pennsylvania and Utah. Attempting to mitigate this risk requires identifying the underlying geological causes and the populations that might be at risk. This work focuses on identifying at-risk populations throughout Pennsylvania and Utah, where radon levels are some of the highest in the country. The resulting dataset harmonizes geological and demographic factors from various sources and spatial resolutions, including temperature, geochemistry, and soil characteristics. Demographic variables such as the household heating fuel used, the age of building, and the housing type provide further insight into which populations could be most susceptible in areas with potentially high radon levels. This dataset also serves as a foundational resource for two other studies conducted by the authors. The resolution of the data provides a novel approach to predicting potential radon exposure, and the data processing conducted for these states can be scaled up to larger spatial resolutions (e.g., the Contiguous United States [CONUS]) and allow for a broad reclassification of radon exposure potential in the United States.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep-MacroFin: Informed Equilibrium Neural Network for Continuous Time Economic Models</title>
<link>https://arxiv.org/abs/2408.10368</link>
<guid>https://arxiv.org/abs/2408.10368</guid>
<content:encoded><![CDATA[
<div> Framework, Deep learning, Partial differential equations, Continuous time economics, Neural networks

Summary: 
The paper introduces Deep-MacroFin, a framework utilizing deep learning techniques to solve partial differential equations, specifically focusing on continuous time economic models. The framework incorporates Multi-Layer Perceptrons and Kolmogorov- Arnold Networks, optimized using Hamilton-Jacobi-Bellman equations and algebraic equations for economic information. It aims to efficiently solve high-dimensional problems with reduced computational requirements compared to traditional methods. Deep-MacroFin offers a user-friendly implementation with significant memory and computational efficiency improvements, making it adaptable to high-dimensional systems of equations. Additionally, a time-stepping scheme enhances training stability, enabling the solution of nonlinear HJB equations and 50-dimensional economic models. <div>
arXiv:2408.10368v4 Announce Type: replace-cross 
Abstract: In this paper, we present Deep-MacroFin, a comprehensive framework designed to solve partial differential equations, with a particular focus on models in continuous time economics. This framework leverages deep learning methodologies, including Multi-Layer Perceptrons and the newly developed Kolmogorov-Arnold Networks. It is optimized using economic information encapsulated by Hamilton-Jacobi-Bellman (HJB) equations and coupled algebraic equations. The application of neural networks holds the promise of accurately resolving high-dimensional problems with fewer computational demands and limitations compared to other numerical methods. This framework can be readily adapted for systems of partial differential equations in high dimensions. Importantly, it offers a more efficient (5$\times$ less CUDA memory and 40$\times$ fewer FLOPs in 100D problems) and user-friendly implementation than existing libraries. We also incorporate a time-stepping scheme to enhance training stability for nonlinear HJB equations, enabling the solution of 50D economic models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
<link>https://arxiv.org/abs/2505.08316</link>
<guid>https://arxiv.org/abs/2505.08316</guid>
<content:encoded><![CDATA[
<div> Keywords: ventral visual stream, object recognition, relative position prediction, unsupervised task-driven method, brain similarity<br />
Summary:<br />
The article introduces a new function of the ventral visual stream (VVS) called relative position (RP) prediction, in addition to its role in object recognition. It criticizes current unsupervised task-driven methods for modeling VVS through contrastive learning, arguing that this approach may not capture the capabilities of RP prediction. To address this, a new method that integrates RP learning with contrastive learning is proposed. Experimental results demonstrate that this new approach improves object recognition performance and enhances RP predictivity, ultimately leading to better model brain similarity. These findings suggest that the VVS may play a role in location perception, particularly in RP prediction, highlighting the need for a more comprehensive understanding of its functions. <br /><br />Summary: <div>
arXiv:2505.08316v1 Announce Type: new 
Abstract: Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology and geometry optimization of grid-shells under self-weight loading</title>
<link>https://arxiv.org/abs/2505.08645</link>
<guid>https://arxiv.org/abs/2505.08645</guid>
<content:encoded><![CDATA[
<div> Keywords: grid-shell structures, optimization, connectivity, elevation, second-order cone optimization

Summary:<br /><br />This manuscript presents an approach for optimizing the connectivity and elevation of grid-shell structures under pure compression or tension, considering external loading and self-weight. The method involves solving a second-order cone optimization problem to ensure convexity and globally optimal solutions. Numerical examples demonstrate the characteristics of these optimal structures, showing the importance of simultaneous topology and geometry optimization. The study reveals that as self-weight increases, optimal topology and elevation profiles change significantly. The approach provides more accurate solutions and is vastly quicker than traditional 3D layout/truss topology optimization methods. This research underscores the significance of integrating topology and geometry optimization from the initial design phase for efficient and effective structural design. <div>
arXiv:2505.08645v1 Announce Type: new 
Abstract: This manuscript presents an approach for simultaneously optimizing the connectivity and elevation of grid-shell structures acting in pure compression (or pure tension) under the combined effects of a prescribed external loading and the design-dependent self-weight of the structure itself. The method derived herein involves solving a second-order cone optimization problem, thereby ensuring convexity and obtaining globally optimal results for a given discretization of the design domain. Several numerical examples are presented, illustrating characteristics of this class of optimal structures. It is found that, as self-weight becomes more significant, both the optimal topology and the optimal elevation profile of the structure change, highlighting the importance of optimizing both topology and geometry simultaneously from the earliest stages of design. It is shown that this approach can obtain solutions with greater accuracy and several orders of magnitude more quickly than a standard 3D layout/truss topology optimization approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Value of Information-based assessment of strain-based thickness loss monitoring in ship hull structures</title>
<link>https://arxiv.org/abs/2505.07427</link>
<guid>https://arxiv.org/abs/2505.07427</guid>
<content:encoded><![CDATA[
<div> Bayesian analysis, Structural Health Monitoring, ship hulls, corrosion-induced thickness loss, decision analysis<br />
<br />
Summary: This study explores the value of information from Structural Health Monitoring (SHM) systems monitoring corrosion-induced thickness loss (CITL) in ship hulls. The research utilizes a Bayesian pre-posterior decision analysis to quantify the benefits of SHM in optimizing hull maintenance. Decision consequence cost functions based on exceedance probabilities relative to a target CITL threshold are defined, allowing for practical decision-making based on risk perception. A high-fidelity numerical model of a commercial vessel is used to compare the benefits of different CITL monitoring strategies, including strain-based SHM and traditional on-site inspections. The findings provide insights into the potential of SHM in enhancing maintenance practices for ship structures. <br /><br /> <div>
arXiv:2505.07427v1 Announce Type: cross 
Abstract: Recent advances in Structural Health Monitoring (SHM) have attracted industry interest, yet real-world applications, such as in ship structures remain scarce. Despite SHM's potential to optimise maintenance, its adoption in ships is limited due to the lack of clearly quantifiable benefits for hull maintenance. This study employs a Bayesian pre-posterior decision analysis to quantify the value of information (VoI) from SHM systems monitoring corrosion-induced thickness loss (CITL) in ship hulls, in a first-of-its-kind analysis for ship structures. We define decision-making consequence cost functions based on exceedance probabilities relative to a target CITL threshold, which can be set by the decision-maker. This introduces a practical aspect to our framework, that enables implicitly modelling the decision-maker's risk perception. We apply this framework to a large-scale, high-fidelity numerical model of a commercial vessel and examine the relative benefits of different CITL monitoring strategies, including strain-based SHM and traditional on-site inspections.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUEST: QUantum-Enhanced Shared Transportation</title>
<link>https://arxiv.org/abs/2505.08074</link>
<guid>https://arxiv.org/abs/2505.08074</guid>
<content:encoded><![CDATA[
<div> framework, shared transportation, Quantum-Enhanced Shared Transportation, windbreaker, windsurfer
Summary:
Quantum-Enhanced Shared Transportation introduces Windbreaking-as-a-Service (WaaS) for shared transportation using larger "windbreaker" vehicles to provide aerodynamic shelter for smaller "windsurfer" vehicles. The computational framework QUEST solves the matching and assignment problems by formulating them as a mixed-integer quadratic problem and encoding them as a Quadratic Unconstrained Binary Optimization for quantum processing. Classical methods such as the Hungarian Algorithm and quantum algorithms like Quantum Approximate Optimization Algorithm (QAOA) are used to solve the assignment problem. The quantum implementation successfully identifies the optimal assignment, demonstrating the effectiveness of the QUEST pipeline for controlled prototypes. This study paves the way for addressing more complex scenarios and leveraging quantum technologies for large-scale shared-transportation instances. <br /><br />Summary: <div>
arXiv:2505.08074v1 Announce Type: cross 
Abstract: We introduce ``Windbreaking-as-a-Service'' (WaaS) as an innovative approach to shared transportation in which larger ``windbreaker'' vehicles provide aerodynamic shelter for ``windsurfer'' vehicles, thereby reducing drag and fuel consumption. As a computational framework to solve the large-scale matching and assignment problems that arise in WaaS, we present \textbf{QUEST} (Quantum-Enhanced Shared Transportation). Specifically, we formulate the pairing of windbreakers and windsurfers -- subject to timing, speed, and vehicle-class constraints -- as a mixed-integer quadratic problem (MIQP). Focusing on a single-segment prototype, we verify the solution classically via the Hungarian Algorithm, a Gurobi-based solver, and brute-force enumeration of binary vectors. We then encode the problem as a Quadratic Unconstrained Binary Optimization (QUBO) and map it to an Ising Hamiltonian, enabling the use of the Quantum Approximate Optimization Algorithm (QAOA) and other quantum and classical annealing technologies. Our quantum implementation successfully recovers the optimal assignment identified by the classical methods, confirming the soundness of the QUEST pipeline for a controlled prototype. While QAOA and other quantum heuristics do not guarantee a resolution of the fundamental complexity barriers, this study illustrates how the WaaS problem can be systematically translated into a quantum-ready model. It also lays the groundwork for addressing multi-segment scenarios and potentially leveraging quantum advantage for large-scale shared-transportation instances.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations</title>
<link>https://arxiv.org/abs/2505.08740</link>
<guid>https://arxiv.org/abs/2505.08740</guid>
<content:encoded><![CDATA[
<div> sensitivity-based regularization, Fourier Neural Operator, inverse problems, parameter inversion, high-dimensional parameter spaces <br />
Summary: <br />
The article introduces Sensitivity-Constrained Fourier Neural Operators (SC-FNO) to address limitations faced by deep learning frameworks like the Fourier Neural Operator (FNO) in solving parametric differential equations. SC-FNO includes a sensitivity-based regularization strategy that improves accuracy in predicting solution paths and outperforms standard FNO and FNO with physics-informed regularization. It is particularly effective in parameter inversion tasks and can handle high-dimensional parameter spaces with up to 82 parameters. SC-FNO reduces data and training requirements while achieving high performance across different types of differential equations and neural operators. Although there is a slight increase in training time, the benefits of SC-FNO are significant in terms of accuracy and scalability. The code and selected experiments for SC-FNO are available on GitHub for further exploration and implementation. <div>
arXiv:2505.08740v1 Announce Type: cross 
Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are fundamental in science and engineering. While deep learning frameworks such as the Fourier Neural Operator (FNO) can efficiently approximate solutions, they struggle with inverse problems, sensitivity estimation (du/dp), and concept drift. We address these limitations by introducing a sensitivity-based regularization strategy, called Sensitivity-Constrained Fourier Neural Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths and consistently outperforms standard FNO and FNO with physics-informed regularization. It improves performance in parameter inversion tasks, scales to high-dimensional parameter spaces (tested with up to 82 parameters), and reduces both data and training requirements. These gains are achieved with a modest increase in training time (30% to 130% per epoch) and generalize across various types of differential equations and neural operators. Code and selected experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles</title>
<link>https://arxiv.org/abs/2505.08782</link>
<guid>https://arxiv.org/abs/2505.08782</guid>
<content:encoded><![CDATA[
<div> Quantum Machine Learning, NISQ devices, multi-chip ensemble VQC framework, scalability, trainability, noise resilience<br />
Summary:<br />
The article introduces a multi-chip ensemble VQC framework to address limitations of noisy intermediate-scale quantum (NISQ) devices in Quantum Machine Learning (QML). This framework partitions computations across smaller quantum chips to enhance scalability, trainability, and noise resilience. It mitigates barren plateaus, reduces quantum error bias and variance, and maintains robust generalization through controlled entanglement. The framework is designed to work with current and future quantum hardware, making it suitable for scalable QML on near-term devices. Experimental validation was done on standard benchmark datasets (MNIST, FashionMNIST, CIFAR-10) and a real-world dataset (PhysioNet EEG), demonstrating the framework's potential in enabling practical QML applications. <br /><br />Summary: <div>
arXiv:2505.08782v1 Announce Type: cross 
Abstract: Quantum Machine Learning (QML) holds significant promise for solving computational challenges across diverse domains. However, its practical deployment is constrained by the limitations of noisy intermediate-scale quantum (NISQ) devices, including noise, limited scalability, and trainability issues in variational quantum circuits (VQCs). We introduce the multi-chip ensemble VQC framework, which partitions high-dimensional computations across smaller quantum chips to enhance scalability, trainability, and noise resilience. We show that this approach mitigates barren plateaus, reduces quantum error bias and variance, and maintains robust generalization through controlled entanglement. Designed to align with current and emerging quantum hardware, the framework demonstrates strong potential for enabling scalable QML on near-term devices, as validated by experiments on standard benchmark datasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet EEG).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAAC panels can suddenly collapse before any warning of corrosion-induced surface cracking</title>
<link>https://arxiv.org/abs/2505.06294</link>
<guid>https://arxiv.org/abs/2505.06294</guid>
<content:encoded><![CDATA[
<div> Keywords: RAAC, corrosion, modeling, porosity, collapse  

<br /><br />Summary: The study addresses the issue of reinforced autoclaved aerated concrete (RAAC) panel collapses, which have gained significant attention due to safety concerns. A lack of detailed experimental data and the lengthy process of replicating natural corrosion has underscored the importance of computational modeling in this context. Researchers suspect that the high porosity of RAAC contributes to corrosion concealment; however, existing models for corrosion-induced cracking have limited capabilities in incorporating concrete porosity effects. To improve this, the authors introduce an enriched model that integrates analytical solutions of reactive transport equations with a porosity-dependent diffusivity description. This innovative approach allows for the first computational exploration of corrosion concealment in RAAC panels. The findings reveal that RAAC panels can experience sudden collapses without visible indicators of surface cracking due to corrosion. Additionally, the research identifies the specific conditions that increase the likelihood of sudden collapse, enhancing the understanding of RAAC durability under corrosive environments. This work provides crucial insights for engineers and researchers concerned with the structural integrity and safety of RAAC constructions. <div>
arXiv:2505.06294v1 Announce Type: new 
Abstract: The collapse of reinforced autoclaved aerated concrete (RAAC) panels has attracted considerable public and academic interest. As detailed experimental data are not yet available and replicating the natural corrosion process requires years or decades, computational modelling is essential to understand under which conditions corrosion remains concealed. The very high porosity of RAAC is widely suspected to be a major contributing factor. However, current corrosion-induced cracking models are known to struggle with capturing the role of concrete porosity. To remedy this critical deficiency, we propose to enrich corrosion-induced cracking modelling with the analytical solution of reactive transport equations governing the precipitation of rust and a porosity-dependent description of diffusivity. With this, the corrosion concealment in RAAC panels is studied computationally for the first time, revealing that RAAC panels can suddenly collapse before any warning of corrosion-induced surface cracking and allowing to map the conditions most likely to result in sudden collapse.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New DAPO Algorithm for Stock Trading</title>
<link>https://arxiv.org/abs/2505.06408</link>
<guid>https://arxiv.org/abs/2505.06408</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, financial trading, Dynamic Sampling Policy Optimization, Large Language Models, trading agent <br />
Summary:
The study explores the application of reinforcement learning techniques, specifically the Dynamic Sampling Policy Optimization (DAPO), in financial trading using large language models (LLMs) for risk and sentiment analysis. By combining an improved Group Relative Policy Optimization (GRPO) algorithm with LLM-based signals, a trading agent was developed and tested on the NASDAQ-100 index. Results showed a cumulative return of 230.49% and an information ratio of 0.37, surpassing the CPPO-DeepSeek baseline. Additionally, the agent's training time was reduced from 8 to 2.5 hours over 100 epochs, with decreased RAM usage. This RL-LLM framework demonstrates potential for data-efficient trading agents, offering scalability and improved performance in financial markets. <br /> <div>
arXiv:2505.06408v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning, such as Dynamic Sampling Policy Optimization (DAPO), show strong performance when paired with large language models (LLMs). Motivated by this success, we ask whether similar gains can be realized in financial trading. We design a trading agent that combines an improved Group Relative Policy Optimization (GRPO) algorithm, augmented with ideas from DAPO, with LLM-based risk and sentiment signals extracted from financial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains a cumulative return of 230.49 percent and an information ratio of 0.37, outperforming the CPPO-DeepSeek baseline. It also cuts training time from about 8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. The proposed RL-LLM framework offers a scalable path toward data-efficient trading agents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Ternary Encoding for High-Speed Data Transmission in 3D-Integrated Circuits Using Inductive Coupling Links</title>
<link>https://arxiv.org/abs/2505.06908</link>
<guid>https://arxiv.org/abs/2505.06908</guid>
<content:encoded><![CDATA[
<div> Inductive coupling links, 3D-integrated circuits, ternary signalling scheme, crosstalk reduction, electromagnetic interference<br />
<br />
Summary:<br />
This paper introduces a novel ternary signalling scheme for inductive coupling links (ICLs) in 3D-integrated circuits (3D-ICs) to mitigate crosstalk and electromagnetic interference. By utilizing three voltage levels (-V, 0V, +V) instead of the traditional binary approach, this scheme enhances signal separation, decreases crosstalk, and enhances signal integrity. The ternary system allows for increased bandwidth efficiency and reduced power consumption as compared to Non-Return to Zero (NRZ) systems due to fewer signal transitions. A modified H-Bridge transmitter is used to generate ternary symbols by regulating current flow based on binary-to-ternary mapping. Initial simulations corroborate the effectiveness of the scheme, demonstrating minimized power consumption and higher data rates in comparison with NRZ. This innovative approach presents potential benefits for high-performance computing and Internet of Things (IoT) devices in 3D-IC settings, providing enhanced noise resilience, reduced power consumption, and heightened communication efficiency. <div>
arXiv:2505.06908v1 Announce Type: new 
Abstract: This paper proposes a ternary signalling scheme for inductive coupling links (ICLs) in 3D-integrated circuits (3D-ICs) to reduce crosstalk and electromagnetic interference in multi-stacked chip communications. By converting binary data into ternary sequences with three voltage levels (-V, 0V, +V), the approach enhances signal separation, reduces crosstalk, and improves signal integrity. Unlike traditional Non-Return to Zero (NRZ) systems, the ternary scheme increases bandwidth efficiency and reduces power consumption through fewer signal transitions. A modified H-Bridge transmitter generates ternary symbols by controlling current flow based on binary-to-ternary mapping. Preliminary simulations validate the efficiency of the scheme, showing reduced power consumption and higher data rates compared to NRZ. This approach shows promise for high-performance computing and IoT devices in 3D-IC environments, offering enhanced noise resilience, lower power usage, and improved communication efficiency.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative study of Bitcoin and Ripple cryptocurrencies trading using Deep Reinforcement Learning algorithms</title>
<link>https://arxiv.org/abs/2505.07660</link>
<guid>https://arxiv.org/abs/2505.07660</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Reinforcement Learning, Bitcoin, Ripple, Trading Strategies

<br /><br />Summary: 
This research focuses on the application of Artificial Intelligence (AI) in automated trading, specifically targeting the challenges presented by the volatile and dynamic nature of financial asset prices. The study emphasizes developing an innovative rule-based strategy to train Deep Reinforcement Learning (DRL) models for trading Bitcoin (BTC) and Ripple (XRP). Key methodologies employed include Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, and Advantage Actor-Critic algorithms, all aimed at deriving optimal trading policies. To assess the effectiveness of the proposed DRL approach, the primary performance metrics utilized are portfolio wealth and trade signals. Experimental results reveal that both Dueling and Double Deep Q-Networks exhibit superior performance when applied to XRP, as evidenced by increased portfolio wealth. The research highlights the potential of AI-driven techniques in improving trading strategies and their adaptability to fluctuating market conditions. All code related to the study is made publicly accessible through a GitHub repository, fostering transparency and collaboration within the research community. <div>
arXiv:2505.07660v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has demonstrated remarkable success across various applications. In light of this trend, the field of automated trading has developed a keen interest in leveraging AI techniques to forecast the future prices of financial assets. This interest stems from the need to address trading challenges posed by the inherent volatility and dynamic nature of asset prices. However, crafting a flawless strategy becomes a formidable task when dealing with assets characterized by intricate and ever-changing price dynamics. To surmount these formidable challenges, this research employs an innovative rule-based strategy approach to train Deep Reinforcement Learning (DRL). This application is carried out specifically in the context of trading Bitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration of Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, alongside the Advantage Actor-Critic algorithms. Each of them aims to yield an optimal policy for our application. To evaluate the effectiveness of our Deep Reinforcement Learning (DRL) approach, we rely on portfolio wealth and the trade signal as performance metrics. The experimental outcomes highlight that Duelling and Double Deep Q-Network outperformed when using XRP with the increasing of the portfolio wealth. All codes are available in this \href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\color{blue}Github link}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALFEE: Adaptive Large Foundation Model for EEG Representation</title>
<link>https://arxiv.org/abs/2505.06291</link>
<guid>https://arxiv.org/abs/2505.06291</guid>
<content:encoded><![CDATA[
<div> framework, EEG, representation, transformer, pretraining
Summary:
The article introduces the Adaptive Large Foundation model for EEG signal representation (ALFEE) framework, which aims to enhance the representation learning for EEG signals. The framework consists of a hybrid transformer architecture with two learning stages. It utilizes a hybrid attention mechanism to separate channel-wise feature aggregation and temporal dynamics modeling, allowing for robust representation of EEG signals with variable channel configurations. ALFEE includes a channel encoder, temporal encoder, and hybrid decoder to optimize task prediction, channel and temporal mask reconstruction, and temporal forecasting during pretraining. The framework also incorporates full-model adaptation during fine-tuning to improve performance across multiple tasks. Experimental results on six downstream EEG tasks demonstrate the superior performance of ALFEE over existing models. The ALFEE framework provides a scalable foundation for biological signal analysis and is available for implementation on GitHub. 
<br /><br />Summary: <div>
arXiv:2505.06291v1 Announce Type: cross 
Abstract: While foundation models excel in text, image, and video domains, the critical biological signals, particularly electroencephalography(EEG), remain underexplored. EEG benefits neurological research with its high temporal resolution, operational practicality, and safety profile. However, low signal-to-noise ratio, inter-subject variability, and cross-paradigm differences hinder the generalization of current models. Existing methods often employ simplified strategies, such as a single loss function or a channel-temporal joint representation module, and suffer from a domain gap between pretraining and evaluation tasks that compromises efficiency and adaptability. To address these limitations, we propose the Adaptive Large Foundation model for EEG signal representation(ALFEE) framework, a novel hybrid transformer architecture with two learning stages for robust EEG representation learning. ALFEE employs a hybrid attention that separates channel-wise feature aggregation from temporal dynamics modeling, enabling robust EEG representation with variable channel configurations. A channel encoder adaptively compresses variable channel information, a temporal encoder captures task-guided evolution, and a hybrid decoder reconstructs signals in both temporal and frequency domains. During pretraining, ALFEE optimizes task prediction, channel and temporal mask reconstruction, and temporal forecasting to enhance multi-scale and multi-channel representation. During fine-tuning, a full-model adaptation with a task-specific token dictionary and a cross-attention layer boosts performance across multiple tasks. After 25,000 hours of pretraining, extensive experimental results on six downstream EEG tasks demonstrate the superior performance of ALFEE over existing models. Our ALFEE framework establishes a scalable foundation for biological signal analysis with implementation at https://github.com/xw1216/ALFEE.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations</title>
<link>https://arxiv.org/abs/2505.06502</link>
<guid>https://arxiv.org/abs/2505.06502</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Learning, Generative Adversarial Networks, Super Resolution, Physical Consistency, Scientific Applications

<br /><br />Summary: 
Machine Learning, specifically Generative Adversarial Networks (GANs), has significantly transformed Super Resolution (SR) image techniques. However, existing generated images often lack physical meaningfulness, crucial for scientific applications. The proposed PC-SRGAN addresses this limitation by enhancing image resolution while ensuring physical consistency, leading to interpretable simulations. This approach demonstrates notable improvements in the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure when compared to traditional methods, even achieving effective results with limited training data (utilizing just 13% of the data required for SRGAN). In addition to SR, PC-SRGAN incorporates physically meaningful machine learning by integrating numerically justified time integrators and advanced quality metrics, aiming for reliable and causal models in scientific fields. A key benefit of PC-SRGAN over conventional SR methods is its physical consistency, positioning it as a viable surrogate model for time-dependent problems. Ultimately, this advancement supports scientific machine learning by enhancing accuracy and efficiency in image processing, improving process understanding, and offering a broader range of applications in scientific research. The source codes and data will be publicly accessible at the provided GitHub link upon paper acceptance. <div>
arXiv:2505.06502v1 Announce Type: cross 
Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. The source codes and data will be made publicly available at https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery</title>
<link>https://arxiv.org/abs/2505.06795</link>
<guid>https://arxiv.org/abs/2505.06795</guid>
<content:encoded><![CDATA[
<div> Keywords: Commodity price volatility, multi-horizon forecasting, Regularized Sparse Autoencoder, interpretable latent drivers, deep learning framework

<br /><br />Summary: This paper addresses the challenges posed by commodity price volatility, emphasizing the need for accurate multi-horizon forecasting. Predicting prices of commodities such as copper and crude oil is complicated by various factors, including macroeconomic indicators, supply and demand dynamics, and geopolitical events. Existing forecasting models often lack transparency, which limits their strategic applications. To tackle this issue, the authors introduce the Regularized Sparse Autoencoder (RSAE), a deep learning framework that facilitates simultaneous forecasting across multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series data. Notably, L1 regularization is applied to the latent vector of the model to enforce sparsity, leading to more parsimonious explanations of underlying market dynamics, including demand and supply shocks. Drawing insights from energy-based models and sparse coding, the RSAE achieves a balance between predictive accuracy and the ability to learn interpretable representations. Furthermore, evaluations on historical data for Copper and Crude Oil indicate that the RSAE not only provides competitive forecasting accuracy but also offers data-driven insights into price dynamics, distinguishing it from traditional black-box forecasting approaches. <div>
arXiv:2505.06795v1 Announce Type: cross 
Abstract: Commodity price volatility creates economic challenges, necessitating accurate multi-horizon forecasting. Predicting prices for commodities like copper and crude oil is complicated by diverse interacting factors (macroeconomic, supply/demand, geopolitical, etc.). Current models often lack transparency, limiting strategic use. This paper presents a Regularized Sparse Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon commodity price prediction and discovery of interpretable latent market drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series. Crucially, L1 regularization ($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity, promoting parsimonious explanations of market dynamics through learned factors representing underlying drivers (e.g., demand, supply shocks). Drawing from energy-based models and sparse coding, the RSAE optimizes predictive accuracy while learning sparse representations. Evaluated on historical Copper and Crude Oil data with numerous indicators, our findings indicate the RSAE offers competitive multi-horizon forecasting accuracy and data-driven insights into price dynamics via its interpretable latent space, a key advantage over traditional black-box approaches.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?</title>
<link>https://arxiv.org/abs/2505.07078</link>
<guid>https://arxiv.org/abs/2505.07078</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, asset pricing, stock trading, backtesting framework, market regime analysis

Summary: 
The study evaluates the effectiveness of Large Language Models (LLMs) in timing-based investment strategies across a broader range of symbols and over a longer timeframe using the FINSABER backtesting framework. Results show that previously reported advantages of LLM strategies diminish significantly when evaluated over two decades and on over 100 symbols. The analysis reveals that LLM strategies are overly conservative in bull markets, leading to underperformance against passive benchmarks, and overly aggressive in bear markets, resulting in heavy losses. The study highlights the importance of developing LLM strategies that prioritize trend detection and incorporate regime-aware risk controls rather than solely focusing on complex frameworks. 

<br /><br />Summary: <div>
arXiv:2505.07078v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating many-engine spacecraft: Exceeding 100 trillion grid points via information~geometric regularization and the MFC flow solver</title>
<link>https://arxiv.org/abs/2505.07392</link>
<guid>https://arxiv.org/abs/2505.07392</guid>
<content:encoded><![CDATA[
<div> method, exascale simulations, compressible fluid flows, rocket craft, computational cost <br />
Summary:
This work presents a method for exascale simulations of high-speed compressible fluid flows, specifically targeting multi-engine rocket craft simulations. By optimizing the implementation through information geometric regularization and unified addressing on tightly coupled CPU-GPU platforms, significant improvements in computational cost and memory footprint are achieved. The need for numerical shock capturing is eliminated, enabling the simulation of fluid flows at unprecedented scales. The use of linear stencil algorithms, despite being memory-bound, results in faster wall clock times compared to baseline numerics. This allows for CFD simulations with over 100 trillion grid points, surpassing existing state-of-the-art simulations. Ideal weak scaling is demonstrated on OLCF Frontier and CSCS Alps supercomputers, showcasing the scalability of the proposed method on large-scale platforms. <div>
arXiv:2505.07392v1 Announce Type: cross 
Abstract: This work proposes a method and optimized implementation for exascale simulations of high-speed compressible fluid flows, enabling the simulation of multi-engine rocket craft at an unprecedented scale. We significantly improve upon the state-of-the-art in terms of computational cost and memory footprint through a carefully crafted implementation of the recently proposed information geometric regularization, which eliminates the need for numerical shock capturing. Unified addressing on tightly coupled CPU--GPU platforms increases the total problem size with negligible performance hit. Despite linear stencil algorithms being memory-bound, we achieve wall clock times that are four times faster than optimized baseline numerics. This enables the execution of CFD simulations at more than 100 trillion grid points, surpassing the largest state-of-the-art publicly available simulations by an order of magnitude. Ideal weak scaling is demonstrated on OLCF Frontier and CSCS Alps using the full system, entailing 37.8K AMD MI250X GPUs (Frontier) or 9.2K NVIDIA GH200 superchips (Alps).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transient Nonlinear Electrothermal Adjoint Sensitivity Analysis for HVDC Cable Joints</title>
<link>https://arxiv.org/abs/2405.14284</link>
<guid>https://arxiv.org/abs/2405.14284</guid>
<content:encoded><![CDATA[
<div> Adjoint variable method, coupled nonlinear transient electrothermal problems, sensitivities, high voltage direct current cable joints, material sensitivities <br />
Summary: <br />
Efficient computation of sensitivities is crucial for designing and optimizing high voltage direct current cable joints. This study introduces the adjoint variable method for solving coupled nonlinear transient electrothermal problems to efficiently compute material sensitivities for a 320kV cable joint specimen. The results are compared with sensitivities obtained using the direct sensitivity method, validating the effectiveness of the proposed approach. The method can handle a large number of design parameters, providing a robust tool for optimizing cable joint designs. This research contributes to improving the efficiency and accuracy of high voltage cable joint design processes, ultimately leading to better performance and reliability of electrical transmission systems. <div>
arXiv:2405.14284v3 Announce Type: replace 
Abstract: Efficient computation of sensitivities is a promising approach for efficiently of designing and optimizing high voltage direct current cable joints. This paper presents the adjoint variable method for coupled nonlinear transient electrothermal problems as an efficient approach to compute sensitivities with respect to a large number of design parameters. The method is used to compute material sensitivities of a 320kV high voltage direct current cable joint specimen. The results are validated against sensitivities obtained via the direct sensitivity method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Martinize2 and Vermouth: Unified Framework for Topology Generation</title>
<link>https://arxiv.org/abs/2212.01191</link>
<guid>https://arxiv.org/abs/2212.01191</guid>
<content:encoded><![CDATA[
<div> Keywords: force field, molecular dynamics, Martini simulations, high-throughput, Vermouth library

Summary:
The article discusses the development of the Vermouth python library for preparing, running, and analyzing Martini simulations of complex systems. The Martinize2 program, a part of Vermouth, is highlighted for its ability to handle protonation states, post-translation modifications, and structural biases such as the elastic network. It can also convert non-protein molecules like ligands. The Vermouth library addresses the need for automation tools in high-throughput simulations and studies of complex cellular systems. Demonstrated with two high-complexity benchmarks, Martinize2 successfully converts protein structures to coarse-grained resolution from databases like I-TASSER and AlphaFold. These conversions showcase the library's capability to ensure input structure quality for safeguarding high-throughput applications.

<br /><br />Summary: <div>
arXiv:2212.01191v4 Announce Type: replace-cross 
Abstract: Ongoing advances in force field and computer hardware development enable the use of molecular dynamics (MD) to simulate increasingly complex systems with the ultimate goal of reaching cellular complexity. At the same time, rational design by high-throughput (HT) simulations is another forefront of MD. In these areas, the Martini coarse-grained force field, especially the latest version (i.e. v3), is being actively explored because it offers an enhanced spatial-temporal resolution. However, the automation tools for preparing simulations with the Martini force field, accompanying the previous version, were not designed for HT simulations or studies of complex cellular systems. Therefore, they become a major limiting factor. To address these shortcomings, we present the open-source Vermouth python library. Vermouth is designed to become the unified framework for developing programs, which prepare, run, and analyze Martini simulations of complex systems. To demonstrate the power of the Vermouth library, the Martinize2 program is showcased as a generalization of the martinize script, originally aimed to set up simulations of proteins. In contrast to the previous version, Martinize2 automatically handles protonation states in proteins and post-translation modifications, offers more options to fine-tune structural biases such as the elastic network (EN), and can convert non-protein molecules such as ligands. Finally, Martinize2 is used in two high-complexity benchmarks. The entire I-TASSER protein template database as well as a subset of 200,000 structures from the AlphaFold Protein Structure Database are converted to CG resolution and we illustrate how the checks on input structure quality can safeguard high-throughput applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships and Improved GRU</title>
<link>https://arxiv.org/abs/2409.08282</link>
<guid>https://arxiv.org/abs/2409.08282</guid>
<content:encoded><![CDATA[
<div> Keywords: stock price prediction, deep learning, GRU, long short-term relationships, algorithmic trading  

<br /><br />Summary:  
Stock price prediction is a complex challenge attracting significant attention within finance. Recent advances in deep learning and graph neural networks have emphasized exploring inter-stock relationships. However, most existing models primarily focus on short-term dynamics and tend to neglect the intricate nonlinear characteristics and higher-order interactions in the market. To address this, the paper introduces the LSR-IGRU model, aimed at improving stock price trend predictions. This model constructs a long short-term relationship matrix utilizing secondary industry information for long-term connections and overnight price data for short-term dynamics. The GRU inputs are enhanced at each step to better combine temporal and relationship data, boosting prediction accuracy. Extensive experimentation across various stock market datasets from China and the United States confirms the LSR-IGRU model's superiority over leading baseline models. Additionally, the model has been successfully implemented in a financial company's algorithmic trading system, significantly outperforming other methods in generating cumulative portfolio returns. The research's codebase is publicly available at the provided GitHub link. <div>
arXiv:2409.08282v3 Announce Type: replace-cross 
Abstract: Stock price prediction is a challenging problem in the field of finance and receives widespread attention. In recent years, with the rapid development of technologies such as deep learning and graph neural networks, more research methods have begun to focus on exploring the interrelationships between stocks. However, existing methods mostly focus on the short-term dynamic relationships of stocks and directly integrating relationship information with temporal information. They often overlook the complex nonlinear dynamic characteristics and potential higher-order interaction relationships among stocks in the stock market. Therefore, we propose a stock price trend prediction model named LSR-IGRU in this paper, which is based on long short-term stock relationships and an improved GRU input. Firstly, we construct a long short-term relationship matrix between stocks, where secondary industry information is employed for the first time to capture long-term relationships of stocks, and overnight price information is utilized to establish short-term relationships. Next, we improve the inputs of the GRU model at each step, enabling the model to more effectively integrate temporal information and long short-term relationship information, thereby significantly improving the accuracy of predicting stock trend changes. Finally, through extensive experiments on multiple datasets from stock markets in China and the United States, we validate the superiority of the proposed LSR-IGRU model over the current state-of-the-art baseline models. We also apply the proposed model to the algorithmic trading system of a financial company, achieving significantly higher cumulative portfolio returns compared to other baseline methods. Our sources are released at https://github.com/ZP1481616577/Baselines_LSR-IGRU.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfitted finite element modelling of surface-bulk viscous flows in animal cells</title>
<link>https://arxiv.org/abs/2505.05723</link>
<guid>https://arxiv.org/abs/2505.05723</guid>
<content:encoded><![CDATA[
<div> Keywords: unfitted finite element framework, surface-bulk problems, cortex-cytoplasm interactions, cell division, computational modelling

Summary: 
This work introduces a novel unfitted finite element framework for simulating surface-bulk problems in time-dependent domains, specifically focusing on fluid-fluid interactions in animal cells. The framework addresses the challenges of modelling cortical contractions that induce surface flows and intracellular flow within large animal cells. By combining the trace finite element method for surface flows with the aggregated finite element method for bulk flows, the framework enables accurate and stable simulations on fixed Cartesian grids without remeshing. Additionally, it incorporates mechanochemical feedback through the surface transport of a molecular regulator of active tension. The method's accuracy and stability are validated through numerical experiments, showcasing its ability to capture phenomena such as self-organized pattern formation, curvature-driven relaxation, and cell cleavage. This innovative framework provides a versatile tool for studying complex morphogenetic processes in animal cells. 

<br /><br />Summary: <div>
arXiv:2505.05723v1 Announce Type: new 
Abstract: This work presents a novel unfitted finite element framework to simulate coupled surface-bulk problems in time-dependent domains, focusing on fluid-fluid interactions in animal cells between the actomyosin cortex and the cytoplasm. The cortex, a thin layer beneath the plasma membrane, provides structural integrity and drives shape changes by generating surface contractile forces akin to tension. Cortical contractions generate Marangoni-like surface flows and induce intracellular cytoplasmic flows that are essential for processes such as cell division, migration, and polarization, particularly in large animal cells. Despite its importance, the spatiotemporal regulation of cortex-cytoplasm interactions remains poorly understood and computational modelling can be very challenging because surface-bulk dynamics often lead to large cell deformations. To address these challenges, we propose a sharp-interface framework that uniquely combines the trace finite element method for surface flows with the aggregated finite element method for bulk flows. This approach enables accurate and stable simulations on fixed Cartesian grids without remeshing. The model also incorporates mechanochemical feedback through the surface transport of a molecular regulator of active tension. We solve the resulting mixed-dimensional system on a fixed Cartesian grid using a level-set-based method to track the evolving surface. Numerical experiments validate the accuracy and stability of the method, capturing phenomena such as self-organised pattern formation, curvature-driven relaxation, and cell cleavage. This novel framework offers a powerful and extendable tool for investigating increasingly complex morphogenetic processes in animal cells.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading Under Uncertainty: A Distribution-Based Strategy for Futures Markets Using FutureQuant Transformer</title>
<link>https://arxiv.org/abs/2505.05595</link>
<guid>https://arxiv.org/abs/2505.05595</guid>
<content:encoded><![CDATA[
<div> Transformer model, futures trading, price predictions, attention mechanisms, risk management 
Summary:
The FutureQuant Transformer model is introduced to predict future price ranges and volatility in futures trading. Unlike traditional models, it focuses on forecasting ranges and volatility rather than point predictions, providing richer insights for trading strategies. By leveraging attention mechanisms to analyze complex data like real-time Limit Order Books, the model significantly improves decision-making and risk management in trading. Through its ability to learn intricate market patterns, it achieved an average gain of 0.1193% per 30-minute trade, outperforming state-of-the-art models. The model uses factors such as RSI, ATR, and Bollinger Bands in a simple algorithm to make accurate predictions, marking a substantial advancement in predictive analytics for futures trading. 
<br /><br />Summary: <div>
arXiv:2505.05595v1 Announce Type: cross 
Abstract: In the complex landscape of traditional futures trading, where vast data and variables like real-time Limit Order Books (LOB) complicate price predictions, we introduce the FutureQuant Transformer model, leveraging attention mechanisms to navigate these challenges. Unlike conventional models focused on point predictions, the FutureQuant model excels in forecasting the range and volatility of future prices, thus offering richer insights for trading strategies. Its ability to parse and learn from intricate market patterns allows for enhanced decision-making, significantly improving risk management and achieving a notable average gain of 0.1193% per 30-minute trade over state-of-the-art models with a simple algorithm using factors such as RSI, ATR, and Bollinger Bands. This innovation marks a substantial leap forward in predictive analytics within the volatile domain of futures trading.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion</title>
<link>https://arxiv.org/abs/2505.05605</link>
<guid>https://arxiv.org/abs/2505.05605</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, Conversion Prediction, Embedding Tables, Multi-Task Model, Overfitting<br />
<br />
Summary: 
Deep learning models for conversion prediction in online advertising often require complex training to predict multiple objectives. The use of embedding tables encoding high cardinality categorical features can enhance model performance but poses challenges due to gradient sparsity and label sparsity. Pinterest Ads Conversion models utilize Sparse Optimizer to improve convergence speed and address multi-epoch overfitting. The severity of overfitting varies across objectives in a multi-task model based on label sparsity. A new approach using frequency-adaptive learning rates on embedding tables is proposed to mitigate multi-epoch overfitting, compared to re-initialization. Evaluation on a large-scale production dataset demonstrates the effectiveness of these techniques in enhancing model performance without sacrificing accuracy.<br /><br />Summary: <div>
arXiv:2505.05605v1 Announce Type: cross 
Abstract: Deep learning for conversion prediction has found widespread applications in online advertising. These models have become more complex as they are trained to jointly predict multiple objectives such as click, add-to-cart, checkout and other conversion types. Additionally, the capacity and performance of these models can often be increased with the use of embedding tables that encode high cardinality categorical features such as advertiser, user, campaign, and product identifiers (IDs). These embedding tables can be pre-trained, but also learned end-to-end jointly with the model to directly optimize the model objectives. Training these large tables is challenging due to: gradient sparsity, the high cardinality of the categorical features, the non-uniform distribution of IDs and the very high label sparsity. These issues make training prone to both slow convergence and overfitting after the first epoch. Previous works addressed the multi-epoch overfitting issue by using: stronger feature hashing to reduce cardinality, filtering of low frequency IDs, regularization of the embedding tables, re-initialization of the embedding tables after each epoch, etc. Some of these techniques reduce overfitting at the expense of reduced model performance if used too aggressively. In this paper, we share key learnings from the development of embedding table optimization and multi-epoch training in Pinterest Ads Conversion models. We showcase how our Sparse Optimizer speeds up convergence, and how multi-epoch overfitting varies in severity between different objectives in a multi-task model depending on label sparsity. We propose a new approach to deal with multi-epoch overfitting: the use of a frequency-adaptive learning rate on the embedding tables and compare it to embedding re-initialization. We evaluate both methods offline using an industrial large-scale production dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing GPU Energy Usage in Exascale-Ready Portable Science Applications</title>
<link>https://arxiv.org/abs/2505.05623</link>
<guid>https://arxiv.org/abs/2505.05623</guid>
<content:encoded><![CDATA[
<div> quantum Monte Carlo, adaptive mesh, GPU energy usage, exascale-ready applications, mixed-precision

Summary:<br />
The study focuses on analyzing the GPU energy usage of two exascale-ready applications, QMCPACK and AMReX-Castro, representing particle and mesh solvers, respectively. Using NVIDIA's A100 and H100 GPUs and AMD's MI250X GPUs, power, temperature, utilization, and energy traces were examined. The research highlights the energy-saving potential of mixed-precision computations, with savings ranging from 6-25% for QMCPACK and 45% for AMReX-Castro. Challenges remain in the AMD tooling for Frontier GPUs, while NVML query resolutions exhibit little variability. The study underscores the importance of application-specific metrics in informing energy-performance trade-offs and optimizing future supercomputer architectures in the post-Moore era.<br /> 

Summary: <div>
arXiv:2505.05623v1 Announce Type: cross 
Abstract: We characterize the GPU energy usage of two widely adopted exascale-ready applications representing two classes of particle and mesh solvers: (i) QMCPACK, a quantum Monte Carlo package, and (ii) AMReX-Castro, an adaptive mesh astrophysical code. We analyze power, temperature, utilization, and energy traces from double-/single (mixed)-precision benchmarks on NVIDIA's A100 and H100 and AMD's MI250X GPUs using queries in NVML and rocm smi lib, respectively. We explore application-specific metrics to provide insights on energy vs. performance trade-offs. Our results suggest that mixed-precision energy savings range between 6-25% on QMCPACK and 45% on AMReX-Castro. Also there are still gaps in the AMD tooling on Frontier GPUs that need to be understood, while query resolutions on NVML have little variability between 1 ms and 1 s. Overall, application level knowledge is crucial to define energy-cost/science-benefit opportunities for the codesign of future supercomputer architectures in the post-Moore era.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions</title>
<link>https://arxiv.org/abs/2505.05784</link>
<guid>https://arxiv.org/abs/2505.05784</guid>
<content:encoded><![CDATA[
<div> Framework, High-frequency trading, Imitation learning, Flow matching policy, Market conditions

Summary:
FlowHFT is an imitation learning framework designed for high-frequency trading (HFT) that addresses the limitations of traditional HFT approaches. It simultaneously learns strategies from various expert models, allowing for adaptive adjustment of investment decisions based on market conditions. The framework incorporates a grid-search fine-tuning mechanism to refine strategies and outperform expert strategies in complex market scenarios. Through testing in multiple market environments, FlowHFT consistently achieves superior performance compared to individual expert models, showcasing its effectiveness in dynamic and diverse market conditions. The flow matching policy utilized in FlowHFT enables the framework to learn trading strategies under different market scenarios, demonstrating its applicability in stochastic market environments. This innovative approach offers a promising solution for HFT strategies in real-world markets. 

<br /><br />Summary: <div>
arXiv:2505.05784v1 Announce Type: cross 
Abstract: High-frequency trading (HFT) is an investing strategy that continuously monitors market states and places bid and ask orders at millisecond speeds. Traditional HFT approaches fit models with historical data and assume that future market states follow similar patterns. This limits the effectiveness of any single model to the specific conditions it was trained for. Additionally, these models achieve optimal solutions only under specific market conditions, such as assumptions about stock price's stochastic process, stable order flow, and the absence of sudden volatility. Real-world markets, however, are dynamic, diverse, and frequently volatile. To address these challenges, we propose the FlowHFT, a novel imitation learning framework based on flow matching policy. FlowHFT simultaneously learns strategies from numerous expert models, each proficient in particular market scenarios. As a result, our framework can adaptively adjust investment decisions according to the prevailing market state. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism. This allows it to refine strategies and achieve superior performance even in complex or extreme market scenarios where expert strategies may be suboptimal. We test FlowHFT in multiple market environments. We first show that flow matching policy is applicable in stochastic market environments, thus enabling FlowHFT to learn trading strategies under different market conditions. Notably, our single framework consistently achieves performance superior to the best expert for each market condition.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using iterated local alignment to aggregate trajectory data into a traffic flow map</title>
<link>https://arxiv.org/abs/2406.17500</link>
<guid>https://arxiv.org/abs/2406.17500</guid>
<content:encoded><![CDATA[
<div> Keywords: vehicle trajectories, traffic flow maps, measurement noise, local alignment algorithms, spatial resolution

Summary: 
Vehicle trajectories are valuable data for generating traffic flow maps at various scales. However, the presence of measurement noise can make small-scale aggregation challenging. To address this issue, new local alignment algorithms are introduced to align road segments for accurate flow mapping. These algorithms use inferred road segments as reference points and iterate through the process to compute locally aligned flow maps. Testing on synthetic and real-world data shows that the locally aligned maps provide precise flow aggregation at multiple scales, enhancing the accuracy and spatial resolution of static and interactive maps. The innovative approach offers a solution to the noise-related challenges in small-scale flow aggregation, making it possible to generate detailed and reliable traffic flow maps for effective traffic management and planning.<br /><br />Summary: <div>
arXiv:2406.17500v4 Announce Type: replace-cross 
Abstract: Vehicle trajectories, with their detailed geolocations, are a promising data source to compute traffic flow maps at scales ranging from the city/regional level to the road level. The main obstacle is that trajectory data are prone to measurement noise. While this is negligible for city level large-scale flow aggregation, it poses substantial difficulties for road level small-scale aggregation. To overcome these difficulties, we introduce innovative local alignment algorithms, where we infer road segments to serve as local reference segments, and proceed to align nearby road segments to them. We deploy these algorithms in an iterative workflow to compute locally aligned flow maps. By applying this workflow to synthetic and empirical trajectories, we verify that our locally aligned flow maps provide high levels of accuracy and spatial resolution of flow aggregation at multiple scales for static and interactive maps.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed solution reconstruction in elasticity and heat transfer using the explicit constraint force method</title>
<link>https://arxiv.org/abs/2505.04875</link>
<guid>https://arxiv.org/abs/2505.04875</guid>
<content:encoded><![CDATA[
<div> Keywords: physics-informed neural networks, solution reconstruction, interpretability, robustness, explicit constraint force method

<br /><br />Summary: This article addresses the challenges faced by physics-informed neural networks (PINNs) in solution reconstruction, which estimates the complete state of a physical system from sparse measurements. A key issue identified is that the governing equations used may not align with the actual physical phenomena, leading to potential failures in fulfilling the criteria of interpretability, robustness, and data consistency. These criteria are essential for assessing reconstruction quality, ensuring that results arent overly dependent on the choice of physics loss, and enabling the unique recovery of physics parameters. The study demonstrates that conventional physics loss formulations create varying "constraint forces," which are additional source terms that influence the reconstructed solution. To mitigate these issues, the authors propose an "explicit constraint force method" (ECFM) that provides better control over the introduced source terms. By adhering to the outlined criteria, the ECFM allows for more reliable and customizable reconstructions from noisy data, even when the physics parameterization does not match the measured system accurately. This approach enhances the performance and interpretability of PINNs in practical applications. <div>
arXiv:2505.04875v1 Announce Type: new 
Abstract: One use case of ``physics-informed neural networks'' (PINNs) is solution reconstruction, which aims to estimate the full-field state of a physical system from sparse measurements. Parameterized governing equations of the system are used in tandem with the measurements to regularize the regression problem. However, in real-world solution reconstruction problems, the parameterized governing equation may be inconsistent with the physical phenomena that give rise to the measurement data. We show that due to assuming consistency between the true and parameterized physics, PINNs-based approaches may fail to satisfy three basic criteria of interpretability, robustness, and data consistency. As we argue, these criteria ensure that (i) the quality of the reconstruction can be assessed, (ii) the reconstruction does not depend strongly on the choice of physics loss, and (iii) that in certain situations, the physics parameters can be uniquely recovered. In the context of elasticity and heat transfer, we demonstrate how standard formulations of the physics loss and techniques for constraining the solution to respect the measurement data lead to different ``constraint forces" -- which we define as additional source terms arising from the constraints -- and that these constraint forces can significantly influence the reconstructed solution. To avoid the potentially substantial influence of the choice of physics loss and method of constraint enforcement on the reconstructed solution, we propose the ``explicit constraint force method'' (ECFM) to gain control of the source term introduced by the constraint. We then show that by satisfying the criteria of interpretability, robustness, and data consistency, this approach leads to more predictable and customizable reconstructions from noisy measurement data, even when the parameterization of the missing physics is inconsistent with the measured system.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermoelastic Kirchhoff Plate: A Novel Model for Shot Peen Forming Metal Panels</title>
<link>https://arxiv.org/abs/2505.05236</link>
<guid>https://arxiv.org/abs/2505.05236</guid>
<content:encoded><![CDATA[
<div> Keywords: shot peen forming, torque, bending equations, Kirchhoff plate, Monte Carlo methods  

<br /><br />Summary: The study explores shot peen forming, a technique where high-velocity steel pellets impact aluminum panels, causing localized plastic deformation. This process enhances the fatigue properties of the material and introduces a residual stress distribution that results in bending, which can be interpreted as the application of spatially varying torques. The authors develop bending equations for a thermally loaded homogeneous Kirchhoff plate to predict the effects of shot peen forming. A novel test method is introduced to extract an equivalent applied torque from the bending response of uniformly shot peened plates, simplifying the accounting for surface plasticity. This extracted torque serves as an input for a model predicting the formation of rectangular plates under varied shot peen conditions. An experimental design is created and executed to assess the models validity against actual shot peen operations. Additionally, uncertainty within the experimental results is quantified using Monte Carlo methods, providing insights into the reliability of the findings and the effectiveness of the proposed modeling approach in capturing the influence of shot peening on panel deformation. <div>
arXiv:2505.05236v1 Announce Type: new 
Abstract: A common technique used in factories to shape metal panels is shot peen forming, where the panel is sprayed with a high-velocity stream of small steel pellets called shot. The impacts between the hard steel shot and softer aluminum panel cause localized plastic deformation, both improving the fatigue properties of the material's surface and imparting a residual stress distribution that results in bending. Thus, a torque is associated with the through-thickness shot peen stress distribution. We conceptualize shot peen forming as the application of spatially varying torques, which are modeled with the input of applied temperatures. In this paper, we derive the bending equations for a thermally loaded homogeneous Kirchhoff plate in order to predict the effects of shot peen forming. A simple test is devised to extract the value of an equivalent applied torque from the bending response of uniformly shot peened plates, which circumvents the difficulty of accounting for surface plasticity. This torque can be used as an input to a model which predicts the shape of rectangular plates under more complicated shot peen conditions. An experiment is designed and carried out which investigates the agreement between the model and real shot peen operations. The effect of uncertainty in the experiment is estimated with Monte Carlo methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Stock Market Prediction Using Long Short-Term Memory Networks: A Comprehensive Deep Learning Framework</title>
<link>https://arxiv.org/abs/2505.05325</link>
<guid>https://arxiv.org/abs/2505.05325</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market, LSTM, predict, sentiment analysis, web application

<br /><br />Summary: 
This paper tackles the challenge of predicting stock market movements, highlighting the volatility and complexity of financial time series data. The study employs a deep learning framework using Long Short-Term Memory (LSTM) networks to forecast closing stock prices for leading technology companies: Apple, Google, Microsoft, and Amazon, all listed on NASDAQ. Historical stock data was sourced from Yahoo Finance and then processed through normalization and feature engineering techniques to enhance model performance. The proposed LSTM model achieved a Mean Absolute Percentage Error (MAPE) of 2.72 on unseen test data, showing a considerable improvement over traditional models like ARIMA. To further refine predictive accuracy, the researchers incorporated sentiment scores derived from real-time news articles and social media, analyzed with the VADER sentiment analysis tool. Additionally, a web application was developed to provide real-time visualizations of stock price forecasts, making the model's insights accessible to both individual and institutional investors. This research illustrates the effectiveness of LSTM networks in capturing complex financial sequences and presents a novel hybrid methodology that integrates time series forecasting with sentiment analysis for enhanced predictive capability. <div>
arXiv:2505.05325v1 Announce Type: new 
Abstract: Predicting stock market movements remains a persistent challenge due to the inherently volatile, non-linear, and stochastic nature of financial time series data. This paper introduces a deep learning-based framework employing Long Short-Term Memory (LSTM) networks to forecast the closing stock prices of major technology firms: Apple, Google, Microsoft, and Amazon, listed on NASDAQ. Historical data was sourced from Yahoo Finance and processed using normalization and feature engineering techniques. The proposed model achieves a Mean Absolute Percentage Error (MAPE) of 2.72 on unseen test data, significantly outperforming traditional models like ARIMA. To further enhance predictive accuracy, sentiment scores were integrated using real-time news articles and social media data, analyzed through the VADER sentiment analysis tool. A web application was also developed to provide real-time visualizations of stock price forecasts, offering practical utility for both individual and institutional investors. This research demonstrates the strength of LSTM networks in modeling complex financial sequences and presents a novel hybrid approach combining time series modeling with sentiment analysis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatMMFuse: Multi-Modal Fusion model for Material Property Prediction</title>
<link>https://arxiv.org/abs/2505.04634</link>
<guid>https://arxiv.org/abs/2505.04634</guid>
<content:encoded><![CDATA[
<div> Keywords: graph-based encoding, multi-modal fusion, Crystal Graph Convolution Network, SciBERT, zero-shot performance

<br /><br />Summary: The paper presents Material Multi-Modal Fusion (MatMMFuse), a novel model that combines graph-based and text-based representations for enhanced material property prediction. By integrating local feature learning from the Crystal Graph Convolution Network (CGCNN) and global information from the SciBERT model, MatMMFuse utilizes a multi-head attention mechanism for effective fusion. The model is trained end-to-end using data from the Materials Project Dataset and demonstrates significant improvements over the individual CGCNN and SciBERT models across key properties: formation energy, band gap, energy above hull, and Fermi energy. Notably, the model shows a 40% enhancement compared to the CGCNN and a 68% improvement over the SciBERT model for predicting formation energy per atom. Furthermore, MatMMFuse exhibits impressive zero-shot performance on specialized datasets, including Perovskites, Chalcogenides, and the Jarvis Dataset, outperforming the standalone CGCNN and SciBERT models. This capability allows researchers to apply the model in industrial settings where obtaining training data can be costly. <div>
arXiv:2505.04634v1 Announce Type: cross 
Abstract: The recent progress of using graph based encoding of crystal structures for high throughput material property prediction has been quite successful. However, using a single modality model prevents us from exploiting the advantages of an enhanced features space by combining different representations. Specifically, pre-trained Large language models(LLMs) can encode a large amount of knowledge which is beneficial for training of models. Moreover, the graph encoder is able to learn the local features while the text encoder is able to learn global information such as space group and crystal symmetry. In this work, we propose Material Multi-Modal Fusion(MatMMFuse), a fusion based model which uses a multi-head attention mechanism for the combination of structure aware embedding from the Crystal Graph Convolution Network (CGCNN) and text embeddings from the SciBERT model. We train our model in an end-to-end framework using data from the Materials Project Dataset. We show that our proposed model shows an improvement compared to the vanilla CGCNN and SciBERT model for all four key properties: formation energy, band gap, energy above hull and fermi energy. Specifically, we observe an improvement of 40% compared to the vanilla CGCNN model and 68% compared to the SciBERT model for predicting the formation energy per atom. Importantly, we demonstrate the zero shot performance of the trained model on small curated datasets of Perovskites, Chalcogenides and the Jarvis Dataset. The results show that the proposed model exhibits better zero shot performance than the individual plain vanilla CGCNN and SciBERT model. This enables researchers to deploy the model for specialized industrial applications where collection of training data is prohibitively expensive.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
<link>https://arxiv.org/abs/2505.04846</link>
<guid>https://arxiv.org/abs/2505.04846</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Augmented Generation, HiPerRAG, high performance computing, multimodal document parsing, scientific question answering

<br /><br />Summary: The exponential growth of scientific literature has led to challenges such as underutilized discoveries and limited interdisciplinary collaboration. Retrieval Augmented Generation (RAG) has the potential to enhance the factuality of Large Language Models (LLMs) by effectively processing vast amounts of information. However, scaling RAG to manage millions of articles presents significant hurdles, including high computational costs and complex algorithmic requirements for aligning nuanced scientific content. To tackle these issues, the authors introduce HiPerRAG, a RAG workflow leveraging high-performance computing (HPC) to index and retrieve knowledge from over 3.6 million scientific articles. HiPerRAG incorporates Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy using contrastive learning and late-interaction techniques. The system demonstrates robust performance on existing scientific question answering benchmarks, achieving 90% accuracy on SciQ and 76% on PubMedQA, surpassing both specialized models like PubMedGPT and commercial LLMs such as GPT-4. By utilizing thousands of GPUs on advanced supercomputers, HiPerRAG facilitates million document-scale RAG workflows, promoting the unity of scientific knowledge and fostering interdisciplinary innovation. <div>
arXiv:2505.04846v1 Announce Type: cross 
Abstract: The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Direct-adjoint Approach for Material Point Model Calibration with Application to Plasticity</title>
<link>https://arxiv.org/abs/2501.04584</link>
<guid>https://arxiv.org/abs/2501.04584</guid>
<content:encoded><![CDATA[
<div> Keywords: elastoplastic, calibration, optimization, Hessian, automatic differentiation  

<br /><br />Summary: This paper introduces a novel method for calibrating material parameters in local elastoplastic constitutive models by framing the calibration as a constrained optimization problem. The evolution equations of the constitutive model at a single material point serve as constraints for this optimization. The aim is to minimize the objective function that measures the discrepancy between predicted stress outcomes from the model and actual experimental data. To enhance the calibration process's efficiency, a new direct-adjoint approach is employed to compute the Hessian of the objective function, which is crucial for implementing second-order optimization techniques. Additionally, automatic differentiation is utilized for calculating both gradient and Hessian values accurately. The paper includes two numerical examples to validate the accuracy of the Hessian matrices. The results demonstrate that the Newton-Raphson algorithm significantly outperforms first-order gradient-based methods such as L-BFGS-B in terms of efficiency and effectiveness, confirming the advantages of the proposed calibration approach for elastoplastic models. <div>
arXiv:2501.04584v2 Announce Type: replace 
Abstract: This paper proposes a new approach for the calibration of material parameters in local elastoplastic constitutive models. The calibration is posed as a constrained optimization problem, where the constitutive model evolution equations for a single material point serve as constraints. The objective function quantifies the mismatch between the stress predicted by the model and corresponding experimental measurements. To improve calibration efficiency, a novel direct-adjoint approach is presented to compute the Hessian of the objective function, which enables the use of second-order optimization algorithms. Automatic differentiation is used for gradient and Hessian computations. Two numerical examples are employed to validate the Hessian matrices and to demonstrate that the Newton-Raphson algorithm consistently outperforms gradient-based algorithms such as L-BFGS-B.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact nonlinear state estimation</title>
<link>https://arxiv.org/abs/2310.10976</link>
<guid>https://arxiv.org/abs/2310.10976</guid>
<content:encoded><![CDATA[
<div> Keywords: data assimilation, generative AI, Conjugate Transform Filter, nonlinear estimation, Earth system models  

<br /><br />Summary: The article discusses the limitations of traditional data assimilation (DA) methods in geosciences, which often rely on Gaussian assumptions that can lead to analysis biases and poor forecasts. To address these challenges, the authors introduce a novel nonlinear estimation theory inspired by advancements in generative artificial intelligence (AI). They present the Conjugate Transform Filter (CTF), which generalizes the Kalman filter to accommodate non-Gaussian distributions. This new filter is designed to maintain statistical relationships in prior states and effectively converge to accurate observations. Additionally, the article introduces an ensemble approximation of the CTF, termed the Ensemble Conjugate Transform Filter (ECTF), which is validated through idealized statistical experiments involving bounded quantities with non-Gaussian distributionscommon in Earth system models. The findings indicate that ECTF performs optimally when observation errors are minor compared to forecast uncertainties and when there are strong nonlinear dependencies among state variables. Ultimately, this new filtering theory presents promising pathways for enhancing conventional DA methods by integrating them with principles from AI, paving the way for improved accuracy in data assimilation tasks in geosciences. <div>
arXiv:2310.10976v2 Announce Type: replace-cross 
Abstract: The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and validated using idealized statistical experiments that feature bounded quantities with non-Gaussian distributions, a prevalent challenge in Earth system models. Results from these experiments indicate that the greatest benefits from ECTF occur when observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Ultimately, the new filtering theory offers exciting avenues for improving conventional DA algorithms through their principled integration with AI techniques.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-device Anomaly Detection in Conveyor Belt Operations</title>
<link>https://arxiv.org/abs/2411.10729</link>
<guid>https://arxiv.org/abs/2411.10729</guid>
<content:encoded><![CDATA[
<div> Keywords: conveyor belts, anomaly detection, pattern recognition, machine learning, mining operations  

<br /><br />Summary: Conveyor belts play a vital role in mining operations by facilitating the efficient movement of bulk materials, thereby enhancing productivity. While previous studies have focused on detecting anomalies in specific conveyor components, understanding root causes such as production changes and operator errors is essential. Continuous monitoring of conveyor belt work cycles remains nascent, requiring robust solutions. This study introduces two novel methods for classifying normal and abnormal duty cycles, based on a recently proposed anomaly detection technique. The methods utilize threshold-based detection, manually extracted features, pattern-matching, and supervised tiny machine learning models, including decision tree and random forest, among others. A comprehensive evaluation demonstrates that both proposed methods outperform the existing technique on two datasets. The heuristic rule-based method excels on training dataset with performance metrics of 97.3% for normal and 80.2% for abnormal cycles, while the ML-based approach scores 91.3% and 67.9% respectively on a dataset influenced by machine aging. Implemented on low-power microcontrollers, the methods achieve real-time operation with energy consumption of just 13.3 and 20.6 J during inference, presenting a significant advancement in conveyor belt monitoring solutions. <div>
arXiv:2411.10729v2 Announce Type: replace-cross 
Abstract: Conveyor belts are crucial in mining operations by enabling the continuous and efficient movement of bulk materials over long distances, which directly impacts productivity. While detecting anomalies in specific conveyor belt components has been widely studied, identifying the root causes of these failures, such as changing production conditions and operator errors, remains critical. Continuous monitoring of mining conveyor belt work cycles is still at an early stage and requires robust solutions. Recently, an anomaly detection method for duty cycle operations of a mining conveyor belt has been proposed. Based on its limited performance and unevaluated long-term proper operation, this study proposes two novel methods for classifying normal and abnormal duty cycles. The proposed approaches are pattern recognition systems that make use of threshold-based duty-cycle detection mechanisms, manually extracted features, pattern-matching, and supervised tiny machine learning models. The explored low-computational models include decision tree, random forest, extra trees, extreme gradient boosting, Gaussian naive Bayes, and multi-layer perceptron. A comprehensive evaluation of the former and proposed approaches is carried out on two datasets. Both proposed methods outperform the former method, with the best-performing approach being dataset-dependent. The heuristic rule-based approach achieves the highest performance in the same dataset used for algorithm training, with 97.3% for normal cycles and 80.2% for abnormal cycles. The ML-based approach performs better on a dataset including the effects of machine aging, scoring 91.3% for normal cycles and 67.9% for abnormal cycles. Implemented on two low-power microcontrollers, the methods demonstrate efficient, real-time operation with energy consumption of 13.3 and 20.6 ${\mu}$J during inference. These results offer valuable insights for detecting ...
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers</title>
<link>https://arxiv.org/abs/2505.04018</link>
<guid>https://arxiv.org/abs/2505.04018</guid>
<content:encoded><![CDATA[
<div> deep learning, graph neural networks, transformers, modal identification, structural health monitoring

Summary: 
This study introduces a novel deep learning framework that combines graph neural networks, transformers, and a physics-informed loss function for modal identification in a population of structures. The transformer module breaks down multiple degrees-of-freedom measurements into single-degree-of-freedom modal responses, aiding in the identification of natural frequencies and damping ratios. Concurrently, the graph neural network captures structural configurations and identifies mode shapes corresponding to the decomposed modal responses. The model is trained in an unsupervised manner, utilizing modal decomposition theory and the independence of structural modes for guidance without labeled data. Validation through simulations and experiments shows its ability to accurately decompose dynamic responses and identify modal properties from sparse measurements, even with external load and structural variations. Comparative analyses against existing techniques highlight its superior performance, making it an attractive option for population-based structural health monitoring. 

<br /><br />Summary: <div>
arXiv:2505.04018v1 Announce Type: new 
Abstract: Modal identification is crucial for structural health monitoring and structural control, providing critical insights into structural dynamics and performance. This study presents a novel deep learning framework that integrates graph neural networks (GNNs), transformers, and a physics-informed loss function to achieve modal decomposition and identification across a population of structures. The transformer module decomposes multi-degrees-of-freedom (MDOF) structural dynamic measurements into single-degree-of-freedom (SDOF) modal responses, facilitating the identification of natural frequencies and damping ratios. Concurrently, the GNN captures the structural configurations and identifies mode shapes corresponding to the decomposed SDOF modal responses. The proposed model is trained in a purely physics-informed and unsupervised manner, leveraging modal decomposition theory and the independence of structural modes to guide learning without the need for labeled data. Validation through numerical simulations and laboratory experiments demonstrates its effectiveness in accurately decomposing dynamic responses and identifying modal properties from sparse structural dynamic measurements, regardless of variations in external loads or structural configurations. Comparative analyses against established modal identification techniques and model variations further underscore its superior performance, positioning it as a favorable approach for population-based structural health monitoring.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yield and Buckling Stress Limits in Topology Optimization of Multiscale Structures</title>
<link>https://arxiv.org/abs/2505.04353</link>
<guid>https://arxiv.org/abs/2505.04353</guid>
<content:encoded><![CDATA[
<div> Topology optimization, yield stress, local buckling, global buckling, multiscale analysis
<br />
Summary: 
This study introduces an extension of multiscale topology optimization by incorporating yield stress and local/global buckling considerations into the design process. The new framework integrates yield stress limits as constraints or objectives alongside existing buckling constraints, refining the optimization process to meet mechanical performance criteria and material yield constraints. Local density-dependent yield surfaces are established based on local yield estimates, then combined with buckling criteria to obtain topology optimized designs considering yield and buckling failure. This integration is crucial for ensuring structural integrity and durability in real-world scenarios. Numerical examples show that optimized designs are influenced by the stiffness to yield ratio of the building material. Despite the assumption of scale separation, de-homogenized structures closely match homogenized predictions even at coarse length scales. 
<br /> <div>
arXiv:2505.04353v1 Announce Type: new 
Abstract: This study presents an extension of multiscale topology optimization by integrating both yield stress and local/global buckling considerations into the design process. Building upon established multiscale methodologies, we develop a new framework incorporating yield stress limits either as constraints or objectives alongside previously established local and global buckling constraints. This approach significantly refines the optimization process, ensuring that the resulting designs meet mechanical performance criteria and adhere to critical material yield constraints. First, we establish local density-dependent von Mises yield surfaces based on local yield estimates from homogenization-based analysis to predict the local yield limits of the homogenized materials. Then, these local Yield-based Load Factors (YLFs) are combined with local and global buckling criteria to obtain topology optimized designs that consider yield and buckling failure on all levels. This integration is crucial for the practical application of optimized structures in real-world scenarios, where material yield and stability behavior critically influence structural integrity and durability. Numerical examples demonstrate how optimized designs depend on the stiffness to yield ratio of the considered building material. Despite the foundational assumption of separation of scales, the de-homogenized structures, even at relatively coarse length scales, exhibit a high degree of agreement with the corresponding homogenized predictions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery Scheme in Mobile Crowdsensing</title>
<link>https://arxiv.org/abs/2505.04361</link>
<guid>https://arxiv.org/abs/2505.04361</guid>
<content:encoded><![CDATA[
<div> Privacy-Preserving Truth Discovery Mobile Crowdsensing Reputation Data Quality<br />
<br />
Summary: The article introduces a Reputation and Data Privacy-Preserving-based Truth Discovery (RDPP-TD) scheme to enhance data quality in mobile crowdsensing (MCS). Existing truth discovery methods often result in low data quality as they only consider current-round data. The proposed RDPP-TD scheme integrates Reputation-based Truth Discovery (RTD) with a Reputation and Data Privacy-Preserving (RDPP) approach to estimate truth more accurately and ensure privacy protection for worker data and reputation values. By combining RTD with RDPP, the scheme evaluates worker reliability in a privacy-preserving manner and supports reputation-based worker recruitment and rewards. The comprehensive theoretical analysis and experiments show that RDPP-TD improves data quality by up to 33.3% while providing strong privacy protection in MCS. <div>
arXiv:2505.04361v1 Announce Type: new 
Abstract: Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS). However, existing TD methods, including privacy-preserving TD approaches, estimate the truth by weighting only the data submitted in the current round, which often results in low data quality. Moreover, there is a lack of effective TD methods that preserve both reputation and data privacy. To address these issues, a Reputation and Data Privacy-Preserving based Truth Discovery (RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD) approach, which integrates the weight of current-round data with workers' reputation values to estimate the truth, thereby achieving more accurate results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which ensures privacy preservation for sensing data and reputation values. First, the RDPP approach, when seamlessly integrated with RTD, can effectively evaluate the reliability of workers and their sensing data in a privacy-preserving manner. Second, the RDPP scheme supports reputation-based worker recruitment and rewards, ensuring high-quality data collection while incentivizing workers to provide accurate information. Comprehensive theoretical analysis and extensive experiments based on real-world datasets demonstrate that the proposed RDPP-TD scheme provides strong privacy protection and improves data quality by up to 33.3%.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Execution Welfare Across Solver-based DEXes</title>
<link>https://arxiv.org/abs/2503.00738</link>
<guid>https://arxiv.org/abs/2503.00738</guid>
<content:encoded><![CDATA[
<div> auctions, decentralized exchanges, solver-based protocols, execution welfare, liquidity profile

Summary:
Solver-based protocols in decentralized exchanges have shown to improve execution welfare for end users compared to traditional routing methods like Uniswap V2 or V3. The study analyzed data for different asset pairs (USDC-WETH and PEPE-WETH) and found that solver-based platforms such as CoWSwap, 1inchFusion, and UniswapX, offer varying levels of execution welfare. While USDC-WETH, a short-tail asset, benefited significantly from solver-based trading, the impact was less pronounced for PEPE-WETH, a long-tail asset. The study also highlighted potential inefficiencies in solver market structure, liquidity profiles, and competition dynamics among solvers. These insights underscore the advantages of solver-based protocols in improving execution outcomes but also raise concerns about market concentration and competition dynamics in decentralized exchanges. 

<br /><br />Summary: <div>
arXiv:2503.00738v3 Announce Type: replace 
Abstract: Decentralized exchanges (DEXes) have evolved dramatically since the introduction of Automated Market Makers (AMMs). In recent years, solver-based protocols have emerged as an alternative venue aiming to introduce competition for routing, access to offchain liquidity, and thereby improve end-user execution. Currently, these solver auctions are hosted on opaque backends, and the extent of price improvement they provide to end users remains unclear.
  We conduct an empirical study of the execution welfare that these protocols bring to users by analyzing data across different asset profiles (USDC-WETH and PEPE-WETH). Our results indicate that, compared to vanilla routing through Uniswap V2 or V3, solver-based protocols effectively enhance execution welfare for end users on DEXes within certain trade size ranges. This effect is most pronounced with USDC-WETH, a short-tail asset, and somewhat less significant with PEPE-WETH, a long-tail asset.
  Additionally, we identify execution welfare discrepancies across solver-based platforms (e.g., CoWSwap, 1inchFusion, UniswapX), revealing potential inefficiencies due to solver market structure, variations in liquidity profile and inventory depth among solvers. These insights highlight both the advantages and challenges of solver-based trading, underscoring its role in improving execution outcomes while raising concerns about market concentration and competition dynamics.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto.gov: Learning-based Governance for Decentralized Finance (DeFi)</title>
<link>https://arxiv.org/abs/2302.09551</link>
<guid>https://arxiv.org/abs/2302.09551</guid>
<content:encoded><![CDATA[
<div> Keywords: DeFi, governance, Auto$.$gov, deep Q-network, reinforcement learning<br />
<br />
Summary: 
Auto$.$gov is a novel governance framework for decentralized finance (DeFi) that utilizes deep Q-network reinforcement learning to automate parameter adjustments. Traditional DeFi governance methods are manual and prone to human bias and financial risks. Auto$.$gov addresses these issues by using RL to make data-driven decisions and adapt to market conditions. In simulated tests based on the Aave lending protocol, Auto$.$gov successfully prevented funds loss from price oracle attacks. Real-world tests showed that Auto$.$gov outperformed benchmark approaches by 14% and the static baseline model by tenfold in terms of protocol profitability. This innovative governance model enhances the security, profitability, and sustainability of DeFi protocols, offering a more efficient and effective alternative to traditional governance methods.<br /> 
Summary: <div>
arXiv:2302.09551v4 Announce Type: replace-cross 
Abstract: Decentralized finance (DeFi) is an integral component of the blockchain ecosystem, enabling a range of financial activities through smart-contract-based protocols. Traditional DeFi governance typically involves manual parameter adjustments by protocol teams or token holder votes, and is thus prone to human bias and financial risks, undermining the system's integrity and security. While existing efforts aim to establish more adaptive parameter adjustment schemes, there remains a need for a governance model that is both more efficient and resilient to significant market manipulations. In this paper, we introduce "Auto$.$gov", a learning-based governance framework that employs a deep Qnetwork (DQN) reinforcement learning (RL) strategy to perform semi-automated, data-driven parameter adjustments. We create a DeFi environment with an encoded action-state space akin to the Aave lending protocol for simulation and testing purposes, where Auto$.$gov has demonstrated the capability to retain funds that would have otherwise been lost to price oracle attacks. In tests with real-world data, Auto$.$gov outperforms the benchmark approaches by at least 14% and the static baseline model by tenfold, in terms of the preset performance metric--protocol profitability. Overall, the comprehensive evaluations confirm that Auto$.$gov is more efficient and effective than traditional governance methods, thereby enhancing the security, profitability, and ultimately, the sustainability of DeFi protocols.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Parallel Simulation of Malignant Pleural Mesothelioma via Adaptive Domain Partitioning -- an Efficiency Analysis Study</title>
<link>https://arxiv.org/abs/2505.03067</link>
<guid>https://arxiv.org/abs/2505.03067</guid>
<content:encoded><![CDATA[
<div> framework, Malignant Pleural Mesothelioma, tumour growth, parallel efficiency analysis, Computational resources optimization

Summary:<br />
The article introduces a novel framework for simulating the growth of Malignant Pleural Mesothelioma (MPM) tumors using a Cellular Potts Model (CPM) coupled with partial differential equations (PDEs). A dynamic bounding box is applied to the simulation domain, based on CT scan data, to reduce memory and CPU overhead. This adaptive partitioning allows for efficient use of computational resources by reducing the 3D domain over which the PDEs are solved. The PDEs, representing oxygen, nutrients, and cytokines, are solved using the finite-volume method with a first-order implicit Euler scheme. Parallelization is achieved using mpi4py library, LinearGMRESSolver, and PETSc for efficient convergence. Parallel computation results in reduced solving time compared to serial computation, with optimizations enhancing memory usage and load balancing among cores. <div>
arXiv:2505.03067v1 Announce Type: new 
Abstract: A novel parallel efficiency analysis on a framework for simulating the growth of Malignant Pleural Mesothelioma (MPM) tumours is presented. Proliferation of MPM tumours in the pleural space is simulated using a Cellular Potts Model (CPM) coupled with partial differential equations (PDEs). Using segmented lung data from CT scans, an environment is set up with artificial tumour data in the pleural space, representing the simulation domain, onto which a dynamic bounding box is applied to restrict computations to the region of interest, dramatically reducing memory and CPU overhead. This adaptive partitioning of the domain enables efficient use of computational resources by reducing the three-dimensional (3D) domain over which the PDEs are to be solved. The PDEs, representing oxygen, nutrients, and cytokines, are solved using the finite-volume method with a first-order implicit Euler scheme. Parallelization is realized using the public Python library mpi4py in combination with LinearGMRESSolver and PETSc for efficient convergence. Performance analyses have shown that parallelization achieves a reduced solving time compared to serial computation. Also, optimizations enable efficient use of available memory and improved load balancing amongst the cores.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Applied to Short-term Solar PV Power Output Forecasting</title>
<link>https://arxiv.org/abs/2505.03188</link>
<guid>https://arxiv.org/abs/2505.03188</guid>
<content:encoded><![CDATA[
<div> Convolutional Neural Networks, Solar Photovoltaic Power Output, Transformer Architecture, Nowcast Model, Forecast Model
Summary:
The study focuses on predicting and forecasting solar PV power output using a transformer architecture and fully-connected layer. The research addresses the challenge of uncertainty in solar PV output due to weather conditions like cloud cover, which can vary over short and long timescales. By utilizing one year of image data and experimenting with different learning rates and batch sizes, the transformer architecture shows promising results in predicting PV output. However, it performs less effectively on days with clear skies compared to the baseline model. Reliable forecasts of renewable energy generation are vital for electricity market balance and supply reliability. This research contributes to improving the accuracy of solar PV power output predictions, particularly in the context of fluctuating weather conditions.<br /><br />Summary: <div>
arXiv:2505.03188v1 Announce Type: new 
Abstract: Reliable forecasts of the power output from variable renewable energy generators like solar photovoltaic systems are important to balancing load on real-time electricity markets and ensuring electricity supply reliability. However, solar PV power output is highly uncertain, with significant variations occurring over both longer (daily or seasonally) and shorter (within minutes) timescales due to weather conditions, especially cloud cover. This paper builds on existing work that uses convolutional neural networks in the computer vision task of predicting (in a Nowcast model) and forecasting (in a Forecast model) solar PV power output (Stanford EAO SUNSET Model). A pure transformer architecture followed by a fully-connected layer is applied to one year of image data with experiments run on various combinations of learning rate and batch size. We find that the transformer architecture performs almost as well as the baseline model in the PV output prediction task. However, it performs worse on sunny days.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-efficient inverse design of spinodoid metamaterials</title>
<link>https://arxiv.org/abs/2505.03415</link>
<guid>https://arxiv.org/abs/2505.03415</guid>
<content:encoded><![CDATA[
<div> surrogate model, spinodoid metamaterials, structure-property linkages, data-efficient, multi-objective inverse design
<br />
Summary:
This study introduces a data-efficient and accurate surrogate model for spinodoid metamaterials' structure-property linkages, using only 75 data points, a substantial reduction compared to previous works. By employing a neural network-based surrogate model that satisfies specific requirements such as equivariance with respect to permutations of structure parameters, the research team successfully creates a differentiable surrogate of the forward model. This model enables gradient-based optimization for inverse design problems, demonstrating reliable results in various complex tasks. The data efficiency achieved in this study opens up possibilities for inverse design applications involving nonlinear mechanical behavior, where the limitation of data availability has been a challenge. <div>
arXiv:2505.03415v1 Announce Type: new 
Abstract: We create an data-efficient and accurate surrogate model for structure-property linkages of spinodoid metamaterials with only 75 data points -- far fewer than the several thousands used in prior works -- and demonstrate its use in multi-objective inverse design. The inverse problem of finding a material microstructure that leads to given bulk properties is of great interest in mechanics and materials science. These inverse design tasks often require a large dataset, which can become unaffordable when considering material behavior that requires more expensive simulations or experiments. We generate a data-efficient surrogate for the mapping between the characteristics of the local material structure and the effective elasticity tensor and use it to inversely design structures with multiple objectives simultaneously. The presented neural network-based surrogate model achieves its data efficiency by inherently satisfying certain requirements, such as equivariance with respect to permutations of structure parameters, which avoids having to learn them from data. The resulting surrogate of the forward model is differentiable, allowing its direct use in gradient-based optimization for the inverse design problem. We demonstrate in three inverse design tasks of varying complexity that this approach yields reliable results while requiring significantly less training data than previous approaches based on neural-network surrogates. This paves the way for inverse design involving nonlinear mechanical behavior, where data efficiency is currently the limiting factor.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithm Selection in Short-Range Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2505.03438</link>
<guid>https://arxiv.org/abs/2505.03438</guid>
<content:encoded><![CDATA[
<div> algorithm selection, molecular dynamics simulations, performance prediction, fuzzy logic, random forest

Summary:
Three algorithm selection strategies for Molecular Dynamics simulations were investigated in this work: performance prediction using past data, fuzzy logic-based expert knowledge approach, and data-driven random forest approach. These strategies achieved speedups of up to 4.05 compared to previous methods and 1.25 compared to a static algorithm configuration selection. The study demonstrated the effectiveness of dynamic algorithm selection in improving simulation performance. The practicality of the strategies was also discussed in relation to their performance, emphasizing the feasibility of implementing such solutions in real-world scenarios. Overall, the research showcased the benefits of dynamic algorithm selection in enhancing the efficiency of particle simulations, offering significant speed improvements while maintaining practicality and ease of implementation. 

<br /><br />Summary: <div>
arXiv:2505.03438v1 Announce Type: new 
Abstract: Numerous algorithms and parallelisations have been developed for short-range particle simulations; however, none are optimally performant for all scenarios. Such a concept led to the prior development of the particle simulation library AutoPas, which implemented many of these algorithms and parallelisations and could select and tune these over the course of the simulation as the scenario changed. Prior works have, however, used only naive approaches to the algorithm selection problem, which can lead to significant overhead from trialling poorly performing algorithmic configurations.
  In this work, we investigate this problem in the case of Molecular Dynamics simulations. We present three algorithm selection strategies: an approach which makes performance predictions from past data, an expert-knowledge fuzzy logic-based approach, and a data-driven random forest-based approach. We demonstrate that these approaches can achieve speedups of up to 4.05 compared to prior approaches and 1.25 compared to a perfect configuration selection without dynamic algorithm selection. In addition, we discuss the practicality of the strategies in comparison to their performance, to highlight the tractability of such solutions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation-Based Equations for Propagation Constant in Uniform or Periodic Transmission</title>
<link>https://arxiv.org/abs/2401.06165</link>
<guid>https://arxiv.org/abs/2401.06165</guid>
<content:encoded><![CDATA[
<div> simulation-based equations, propagation constant, uniform structures, periodic structures, FPPS<br />
<br />
Summary: The article presents simulation-based equations for calculating propagation constants in uniform or periodic structures using a Field Propagation Parameter Splitter (FPPS) model. The FPPS model is based on field distributions obtained from a driven-mode solver, allowing for the separation of forward and backward waves within structures. The FPPS is tested on various structures including waveguides, closed structures, and open radiation structures, showing its ease of use and adaptability compared to other methods. The model's effectiveness is verified through comparisons with eigenmode solvers, equivalent network methods, and spectral domain integral equation methods. The FPPS can also be applied to open radiating structures and multi-dimensional periodic/uniform structures. <div>
arXiv:2401.06165v2 Announce Type: replace 
Abstract: In this work, simulation-based equations to calculate propagation constant in uniform or periodic structures (SES) are deduced and verified through simulations in various types of structures. The modeling of those structures are essentially based on field distributions from a driven-mode solver, and the field distributions are used as the input parameters of the FPPS. It allows the separation of forward and backward waves from a total wave inside such a uniform or periodic structure, and thus it can be used to calculate the propagation constants inside both uniform and periodic structures even with a strong reflection. In order to test the performance and function of the FPPS, it has been applied to a variety of typical structures, including uniform waveguides, lossfree closed structures, lossy closed structures, and open radiation structures, and compared with the results of eigenmode solvers, equivalent network methods, and spectral domain integral equation methods. The comparison shows the easy-to-use and adaptable nature of the FPPS. the FPPS. This FPPS could be also applied to open radiating structures, and even multi-dimensional periodic/uniform structures.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precision Glass Thermoforming Assisted by Neural Networks</title>
<link>https://arxiv.org/abs/2411.06762</link>
<guid>https://arxiv.org/abs/2411.06762</guid>
<content:encoded><![CDATA[
<div> surrogate model, glass thermoforming, precision, neural network, predictive<br />
<br />
Summary: 
This study presents a surrogate model based on a dimensionless back-propagation neural network (BPNN) to predict form errors in glass thermoforming processes. Traditional trial-and-error methods can be time-consuming and costly, leading to inefficiencies in precision glass product development. The surrogate model uses geometric features and process parameters as inputs to accurately predict forming errors, allowing for adjustments in mold design. The model was tested using simulation and industrial data, showing promising results in predicting form errors with reasonable accuracy. Despite potential discrepancies in industrial training data due to perception errors and mold fabrication errors, the surrogate model demonstrated practicality for implementation in the glass-manufacturing industry. <div>
arXiv:2411.06762v2 Announce Type: replace 
Abstract: Many glass products require thermoformed geometry with high precision. However, the traditional approach of developing a thermoforming process through trials and errors can cause large waste of time and resources and often end up with unsuccessfulness. Hence, there is a need to develop an efficient predictive model, replacing the costly simulations or experiments, to assist the design of precision glass thermoforming. In this work, we report a surrogate model, based on a dimensionless back-propagation neural network (BPNN), that can adequately predict the form errors and thus compensate for these errors in mold design using geometric features and process parameters as inputs. Our trials with simulation and industrial data indicate that the surrogate model can predict forming errors with adequate accuracy. Although perception errors (mold designers' decisions) and mold fabrication errors make the industrial training data less reliable than simulation data, our preliminary training and testing results still achieved a reasonable consistency with industrial data, suggesting that the surrogate models are directly implementable in the glass-manufacturing industry.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Reinforcement Learning in Quantitative Finance: A Survey</title>
<link>https://arxiv.org/abs/2408.10932</link>
<guid>https://arxiv.org/abs/2408.10932</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, finance, Quantitative Finance, machine learning, financial markets

Summary: 
This survey reviews 167 publications on the application of Reinforcement Learning (RL) in finance. RL has shown significant progress in the past decade and is gaining traction in the financial sector due to its dynamic approach and integration with machine learning techniques. Financial markets, known for their complexity and randomness, provide a challenging environment for RL applications. The survey examines various RL frameworks and applications in Quantitative Finance, highlighting the strengths and weaknesses of existing methods. Transfer learning, meta-learning, and multi-agent solutions are identified as key components advancing RL in finance. The survey also explores emerging themes in RL applications and proposes future research directions in this field. <div>
arXiv:2408.10932v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Asset Pricing: Integrating FinBERT-Based Sentiment Quantification with the Fama--French Five-Factor Model</title>
<link>https://arxiv.org/abs/2505.01432</link>
<guid>https://arxiv.org/abs/2505.01432</guid>
<content:encoded><![CDATA[
<div> FinBERT, sentiment factors, multi-factor asset pricing models, Fama French five-factor regression, market volatility <br />
<br />
Summary:This paper investigates the impact of text-derived, time-varying sentiment factors on stock returns using FinBERT. Through a comprehensive study covering 2020-2022, a dynamic sentiment index and its volatility are constructed from financial news and social media data. Results show that sentiment positively influences returns in normal market conditions but its effect varies under extreme volatility. The study reveals the time-varying nature of sentiment sensitivity and demonstrates improved abnormal returns prediction during the Federal Reserve rate hike event in June 15, 2022, using a sentiment-augmented five-factor model. These findings advocate for the integration of high-frequency sentiment analysis in traditional asset pricing models, offering valuable insights for investors and regulators.<br /><br /> <div>
arXiv:2505.01432v1 Announce Type: new 
Abstract: This paper presents a comprehensive study on the integration of text-derived, time-varying sentiment factors into traditional multi-factor asset pricing models. Leveraging FinBERT, a domain-specific deep learning language model, we construct a dynamic sentiment index and its volatility from large-scale financial news and social media data covering 2020 to 2022. By embedding these sentiment measures into the Fama French five-factor regression, we rigorously examine whether sentiment significantly explains variations in daily stock returns and how its impact evolves across different market volatility regimes. Empirical results demonstrate that sentiment has a consistently positive impact on returns during normal periods, while its effect is amplified or even reversed under extreme market conditions. Rolling regressions reveal the time-varying nature of sentiment sensitivity, and an event study around the June 15, 2022 Federal Reserve 75 basis point rate hike shows that a sentiment-augmented five-factor model better explains abnormal returns relative to the baseline model. Our findings support the incorporation of high-frequency, NLP-derived sentiment into classical asset pricing frameworks and suggest implications for investors and regulators.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimising Kernel-based Multivariate Statistical Process Control</title>
<link>https://arxiv.org/abs/2505.01556</link>
<guid>https://arxiv.org/abs/2505.01556</guid>
<content:encoded><![CDATA[
<div> kernel MSPC, multivariate statistical process control, kernel functions, kernel flows, Gaussian Process Regression

Summary:
Kernel MSPC is a framework for monitoring complex processes by analyzing multiple process variables simultaneously. Kernel MSPC enhances process monitoring capabilities by capturing non-linear relationships using kernel functions. This study proposes optimizing kernel MSPC parameters using Kernel Flows, a kernel learning methodology. The methodology also utilizes kernel combinations to learn the optimal kernel type and individual kernel parameters for each variable. The proposed approach is evaluated using cases from the Tennessee Eastman Process benchmark and successfully detects faults that were not identified in the original study. The study demonstrates the effectiveness of the proposed optimization technique and the incorporation of kernel combinations for improved process monitoring in complex systems. <br /><br />Summary: <div>
arXiv:2505.01556v1 Announce Type: new 
Abstract: Multivariate Statistical Process Control (MSPC) is a framework for monitoring and diagnosing complex processes by analysing the relationships between multiple process variables simultaneously. Kernel MSPC extends the methodology by leveraging kernel functions to capture non-linear relationships between the data, enhancing the process monitoring capabilities. However, optimising the kernel MSPC parameters, such as the kernel type and kernel parameters, is often done in literature in time-consuming and non-procedural manners such as cross-validation or grid search. In the present paper, we propose optimising the kernel MSPC parameters with Kernel Flows (KF), a recent kernel learning methodology introduced for Gaussian Process Regression (GPR). Apart from the optimisation technique, the novelty of the study resides also in the utilisation of kernel combinations for learning the optimal kernel type, and introduces individual kernel parameters for each variable. The proposed methodology is evaluated with multiple cases from the benchmark Tennessee Eastman Process. The faults are detected for all evaluated cases, including the ones not detected in the original study.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction</title>
<link>https://arxiv.org/abs/2505.01781</link>
<guid>https://arxiv.org/abs/2505.01781</guid>
<content:encoded><![CDATA[
<div> Keywords: Mean-Variance model, Black-Litterman model, Singular Spectrum analysis, Multivariate Aligned Empirical Mode Decomposition, Temporal Convolutional Networks

Summary:
The traditional Mean-Variance model is limited by sensitivity to input parameters and lack of flexibility. In contrast, the Black-Litterman model combines market equilibrium returns with investors' subjective views, showing promise in asset price prediction. A novel hybrid deep learning model incorporating Singular Spectrum analysis, Multivariate Aligned Empirical Mode Decomposition, and Temporal Convolutional Networks is proposed to enhance prediction accuracy. Experimental results demonstrate noise reduction preprocessing improves model performance significantly. The hybrid model outperforms three benchmark models in multivariate decomposition. An investment portfolio constructed using 20 NASDAQ 100 index stocks shows that when combined with the Black-Litterman model, the hybrid forecasting model produces better returns and risk control capabilities than Mean-Variance, Equal-Weighted, and Market-Weighted models over a short holding period. 

<br /><br />Summary: <div>
arXiv:2505.01781v1 Announce Type: new 
Abstract: The sensitivity to input parameters and lack of flexibility limits the traditional Mean-Variance model. In contrast, the Black-Litterman model has attracted widespread attention by integrating market equilibrium returns with investors' subjective views. This paper proposes a novel hybrid deep learning model combining Singular Spectrum analysis (SSA), Multivariate Aligned Empirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks (TCNs), aiming to improve the prediction accuracy of asset prices and thus enhance the ability of the Black-Litterman model to generate subjective views. Experimental results show that noise reduction pre-processing can improve the model's accuracy, and the prediction performance of the proposed model is significantly better than that of three multivariate decomposition benchmark models. We construct an investment portfolio by using 20 representative stocks from the NASDAQ 100 index. By combining the hybrid forecasting model with the Black-Litterman model, the generated investment portfolio exhibits better returns and risk control capabilities than the Mean-Variance, Equal-Weighted, and Market-Weighted models in the short holding period.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computational framework for predicting the effect of surface roughness in fatigue</title>
<link>https://arxiv.org/abs/2505.01871</link>
<guid>https://arxiv.org/abs/2505.01871</guid>
<content:encoded><![CDATA[
<div> Keywords: surface roughness, fatigue life, phase field model, stochastic nature, failure strength <br />
Summary: Surface roughness significantly affects the fatigue life of structural components and can be quantified using the surface factor. A numerical framework based on the phase field method has been developed to estimate the surface factor considering the stochastic nature of roughness. The model's validity is confirmed through experimental data validation. The study explores the impact of key parameters on the fatigue life of rough surfaces, including surface topology and failure strength. Notably, an increase in average surface roughness coupled with a decrease in the correlation length of the surface profile results in a pronounced effect on fatigue life. This effect is more prominent at higher failure strengths. <br /><br /> <div>
arXiv:2505.01871v1 Announce Type: new 
Abstract: Surface roughness is a critical factor influencing the fatigue life of structural components. Its effect is commonly quantified using a correction coefficient known as the surface factor. In this paper, a phase field based numerical framework is proposed to estimate the surface factor while accounting for the stochastic nature of surface roughness. The model is validated against existing experimental data. Furthermore, we investigate the influence of key parameters on the fatigue life of rough surfaces, such as surface topology and failure strength. An important effect of surface roughness is observed when the average surface roughness increases and the correlation length of the surface profile decreases. This effect becomes more pronounced with higher failure strengths.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Scheme of Electromagnetic Scattering From Scatterers With Incomplete Profiles</title>
<link>https://arxiv.org/abs/2505.02086</link>
<guid>https://arxiv.org/abs/2505.02086</guid>
<content:encoded><![CDATA[
<div> deep learning, electromagnetic scattering, incomplete profile, limited scattering data, forward and inverse problems

Summary:
A new deep learning scheme is proposed to solve electromagnetic scattering problems where the profile of the dielectric scatterer is incomplete. The scheme utilizes a limited amount of scattering data to compensate for the missing profile information. Existing solvers struggle to handle this situation effectively. The proposed scheme addresses this challenge by simultaneously solving the forward and inverse scattering problems. By using deep learning, the EM forward scattering from an incompletely known dielectric scatterer is derived, and numerical experiments are conducted to demonstrate the scheme's performance for both 2-D and 3-D EM scattering problems. The results showcase the effectiveness of the proposed deep learning-based approach in recovering the unknown parts of the scatterer profile accurately. 

<br /><br />Summary: <div>
arXiv:2505.02086v1 Announce Type: new 
Abstract: A deep learning scheme is proposed to solve the electromagnetic (EM) scattering problems where the profile of the dielectric scatterer of interest is incomplete. As a compensation, a limited amount of scattering data is provided, which is in principle containing sufficient information associated with the missing part of the profile. The existing solvers can hardly realize the compensation if the known part of the profile and the scattering data are combined straightforwardly. On one hand, the well-developed forward solvers have no mechanism to accept the scattering data, which can recover the unknown part of the profile if properly used. On the other hand, the existing solvers for inverse problems cannot retrieve the complete profile with an acceptable accuracy from the limited amount of scattering data, even when the available part of the profile can be fed into the solvers. This work aims to handle the difficulty. To this end, the EM forward scattering from an incompletely known dielectric scatterer is derived. A scheme based on DL is then proposed where the forward and inverse scattering problems are solved simultaneously. Numerical experiments are conducted to demonstrate the performance of the proposed DL-based scheme for both two-dimensional (2-D) and three-dimensional (3-D) EM scattering problems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning-Aided Approach for Estimating Field Permeability Map by Fusing Well Logs, Well Tests, and Seismic Data</title>
<link>https://arxiv.org/abs/2505.02093</link>
<guid>https://arxiv.org/abs/2505.02093</guid>
<content:encoded><![CDATA[
<div> Data fusion, permeability maps, reservoir simulation, convolutional neural network, Western Siberia<br />
<br />
Summary: 
Obtaining reliable permeability maps for oil reservoirs is essential for accurate reservoir simulation models and recovery strategies. Existing methods face challenges due to the integration of various data sources and lack of direct inter-well space information. This study proposes a novel data-fusion approach to predict two-dimensional permeability maps across the entire reservoir area. Utilizing non-parametric regression with a customized kernel shape, incorporating well logs, tests, and seismic data. A convolutional neural network processes seismic data and integrates it with other sources using a multi-stage fusion procedure to enhance the training dataset and construct the permeability map. Testing on a real oil reservoir in Western Siberia shows the developed map aligns with well permeability estimations and significantly improves inter-well space permeability predictions through seismic data integration. <br /><br /> <div>
arXiv:2505.02093v1 Announce Type: new 
Abstract: Obtaining reliable permeability maps of oil reservoirs is crucial for building a robust and accurate reservoir simulation model and, therefore, designing effective recovery strategies. This problem, however, remains challenging, as it requires the integration of various data sources by experts from different disciplines. Moreover, there are no sources to provide direct information about the inter-well space. In this work, a new method based on the data-fusion approach is proposed for predicting two-dimensional permeability maps on the whole reservoir area. This method utilizes non-parametric regression with a custom kernel shape accounting for different data sources: well logs, well tests, and seismics. A convolutional neural network is developed to process seismic data and then incorporate it with other sources. A multi-stage data fusion procedure helps to artificially increase the training dataset for the seismic interpretation model and finally to construct the adequate permeability map. The proposed methodology of permeability map construction from different sources was tested on a real oil reservoir located in Western Siberia. The results demonstrate that the developed map perfectly corresponds to the permeability estimations in the wells, and the inter-well space permeability predictions are considerably improved through the incorporation of the seismic data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking</title>
<link>https://arxiv.org/abs/2505.02139</link>
<guid>https://arxiv.org/abs/2505.02139</guid>
<content:encoded><![CDATA[
<div> representation learning, limit order book, financial market, LOBench, China A-share market<br />
Summary:<br />
The paper introduces a systematic comparative study of limit order book (LOB) representation learning to extract transferable, compact features capturing essential LOB properties. A standardized benchmark called LOBench is presented with curated datasets, unified preprocessing, and strong baselines using real China A-share market data. The study shows the sufficiency and necessity of LOB representations for various downstream tasks, highlighting their advantages over traditional task-specific end-to-end models and advanced representation learning models for general time series. This work establishes a reproducible framework and provides clear guidelines for future research. The datasets and code are publicly available at the provided link, allowing for further exploration and validation of the proposed methods.<br /> <div>
arXiv:2505.02139v1 Announce Type: new 
Abstract: The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constrains, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at https://github.com/financial-simulation-lab/LOBench.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
<div> fantasy football, integer programming, hybrid scoring metric, artificial intelligence, Premier League

Summary:
The paper introduces novel deterministic and robust integer programming models for selecting the optimal starting eleven and captain in fantasy football, a billion-dollar industry. A hybrid scoring metric is proposed using an interpretable artificial intelligence framework, leading to the highest scores while maintaining consistent performance. The models' performance is evaluated using data from the 2023/24 Premier League season, showing effectiveness during out-of-sample periods. Strategic averaging techniques for estimating cost vectors and the proposed hybrid approach are found to be successful. The paper provides insights into optimal formations and player selections, offering valuable strategies for fantasy football enthusiasts. <div>
arXiv:2505.02170v1 Announce Type: new 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Constrained by a fixed budget, decision-makers draft a squad whose players are expected to perform well in the upcoming weeks to maximize total points. This paper proposes novel deterministic and robust integer programming models that select the optimal starting eleven and the captain. A new hybrid scoring metric is constructed using an interpretable artificial intelligence framework and underlying match performance data. Several objective functions and estimation techniques are introduced for the programming model. To the best of my knowledge, this is the first study to approach fantasy football through this lens. The models' performance is evaluated using data from the 2023/24 Premier League season. Results indicate that the proposed hybrid method achieved the highest score while maintaining consistent performance. Utilizing the Monte Carlo simulation, the strategic choice of averaging techniques for estimating cost vectors, and the proposed hybrid approach are shown to be effective during the out-of-sample period. This paper also provides a thorough analysis of the optimal formations and players selected by the models, offering valuable insights into effective fantasy football strategies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Method for Optimizing Submarine Search and Rescue Strategy Under Environmental Uncertainty</title>
<link>https://arxiv.org/abs/2505.02186</link>
<guid>https://arxiv.org/abs/2505.02186</guid>
<content:encoded><![CDATA[
<div> framework, dynamic analysis, Monte Carlo method, Bayesian method, economic optimization
Summary:
The article presents a hybrid algorithm framework for locating and rescuing deep-sea submersibles in uncertain ocean environments. By combining dynamic analysis, Monte Carlo, and Bayesian methods, a probabilistic prediction approach is used to improve search efficiency. The Monte Carlo method is employed to account for environmental variability, enhancing location prediction accuracy. Bayesian grid research and probabilistic updating are integrated based on trajectory predictions, with Bayesian filtering for complex scenarios. Economic optimization is conducted through cost-benefit analysis using the entropy weight method, and the CER is applied for evaluation. This comprehensive approach aims to maximize the rate of successful rescues while minimizing costs, addressing the challenges of deep-sea submersible rescue operations effectively. <br /><br /> <div>
arXiv:2505.02186v1 Announce Type: new 
Abstract: When coping with the urgent challenge of locating and rescuing a deep-sea submersible in the event of communication or power failure, environmental uncertainty in the ocean can not be ignored. However, classic physical models are limited to deterministic scenarios. Therefore, we present a hybrid algorithm framework combined with dynamic analysis for target submarine, Monte Carlo and Bayesian method for conducting a probabilistic prediction to improve the search efficiency. Herein, the Monte Carlo is performed to overcome the environmental variability to improve the accuracy in location prediction. According to the trajectory prediction, we integrated the Bayesian based grid research and probabilistic updating. For more complex situations, we introduced the Bayesian filtering. Aiming to maximize the rate of successful rescue and costs, the economic optimization is performed utilizing the cost-benefit analysis based on entropy weight method and the CER is applied for evaluation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the Dynamics of Complex System via Multiscale Diffusion Autoencoder</title>
<link>https://arxiv.org/abs/2505.02450</link>
<guid>https://arxiv.org/abs/2505.02450</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiscale Diffusion Prediction Network, complex systems, latent space, spatiotemporal evolution, graph neural network

Summary:
The article introduces a new method, the Multiscale Diffusion Prediction Network (MDPNet), for predicting the dynamics of complex systems. Existing methods often overlook the multiscale structure of complex systems, leading to inaccuracies in predictions of spatiotemporal evolution. MDPNet aims to address this by leveraging the multiscale structure to discover the latent space of intrinsic dynamics. It utilizes a multiscale diffusion autoencoder to encode multiscale features and guide the diffusion model for reliable reconstruction. Additionally, an attention-based graph neural ordinary differential equation is introduced to model the co-evolution across different scales. The proposed method shows promising results in extensive evaluations on representative systems, achieving a significant average prediction error reduction of 53.23% compared to baseline methods. It also demonstrates superior robustness and generalization, highlighting its potential for various scientific and engineering applications. 

<br /><br />Summary: <div>
arXiv:2505.02450v1 Announce Type: new 
Abstract: Predicting the dynamics of complex systems is crucial for various scientific and engineering applications. The accuracy of predictions depends on the model's ability to capture the intrinsic dynamics. While existing methods capture key dynamics by encoding a low-dimensional latent space, they overlook the inherent multiscale structure of complex systems, making it difficult to accurately predict complex spatiotemporal evolution. Therefore, we propose a Multiscale Diffusion Prediction Network (MDPNet) that leverages the multiscale structure of complex systems to discover the latent space of intrinsic dynamics. First, we encode multiscale features through a multiscale diffusion autoencoder to guide the diffusion model for reliable reconstruction. Then, we introduce an attention-based graph neural ordinary differential equation to model the co-evolution across different scales. Extensive evaluations on representative systems demonstrate that the proposed method achieves an average prediction error reduction of 53.23% compared to baselines, while also exhibiting superior robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Compression for Time Series Modelling: A Case Study of Smart Grid Demand Forecasting</title>
<link>https://arxiv.org/abs/2505.02606</link>
<guid>https://arxiv.org/abs/2505.02606</guid>
<content:encoded><![CDATA[
<div> wavelet-based compression, time series forecasting, smart energy systems, data management, high-frequency data <br />
<br />
Summary: <br />
Efficient time series forecasting is crucial for smart energy systems, but the increasing amount of high-frequency data poses storage and transmission challenges. This study investigates the use of Discrete Wavelet Transform (DWT)-based data compression to address these challenges while maintaining forecasting accuracy. By applying biorthogonal wavelets at different compression rates, the study evaluates the impact on three forecasting models: Ordinary Least Squares (OLS), XGBoost, and Time Series Dense Encoder (TiDE). Results show that XGBoost is robust to compression artifacts, while OLS is sensitive to smooth wavelets and high compression rates. TiDE demonstrates some variability but remains competitive. The study suggests that wavelet-based compression can efficiently manage data in smart energy systems without compromising forecasting accuracy, with potential applications in climate modeling, water supply systems, and industrial operations. <div>
arXiv:2505.02606v1 Announce Type: new 
Abstract: Efficient time series forecasting is essential for smart energy systems, enabling accurate predictions of energy demand, renewable resource availability, and grid stability. However, the growing volume of high-frequency data from sensors and IoT devices poses challenges for storage and transmission. This study explores Discrete Wavelet Transform (DWT)-based data compression as a solution to these challenges while ensuring forecasting accuracy. A case study of a seawater supply system in Hirtshals, Denmark, operating under dynamic weather, operational schedules, and seasonal trends, is used for evaluation.
  Biorthogonal wavelets of varying orders were applied to compress data at different rates. Three forecasting models - Ordinary Least Squares (OLS), XGBoost, and the Time Series Dense Encoder (TiDE) - were tested to assess the impact of compression on forecasting performance. Lossy compression rates up to $r_{\mathrm{lossy}} = 0.999$ were analyzed, with the Normalized Mutual Information (NMI) metric quantifying the relationship between compression and information retention. Results indicate that wavelet-based compression can retain essential features for accurate forecasting when applied carefully.
  XGBoost proved highly robust to compression artifacts, maintaining stable performance across diverse compression rates. In contrast, OLS demonstrated sensitivity to smooth wavelets and high compression rates, while TiDE showed some variability but remained competitive. This study highlights the potential of wavelet-based compression for scalable, efficient data management in smart energy systems without sacrificing forecasting accuracy. The findings are relevant to other fields requiring high-frequency time series forecasting, including climate modeling, water supply systems, and industrial operations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components</title>
<link>https://arxiv.org/abs/2505.01627</link>
<guid>https://arxiv.org/abs/2505.01627</guid>
<content:encoded><![CDATA[
<div> Automated Classification, Large Language Models, Function-Based Design, Mechanical Assembly Parts, Domain Adaptation <br />
Summary: <br />
The study introduces a novel framework utilizing Large Language Models (LLMs) for automated classification of mechanical assembly parts' functions in the conceptual design phase of product development. Functional modeling, a critical aspect of early-phase engineering, is often hindered by the lack of structured functional data. Large Language Models (LLMs) show promise in addressing this gap by automating function annotation through domain adaptation (DA) using fine-tuning. The study showcases the effectiveness of fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR) to enhance the semantic representation of mechanical parts. Evaluation on the A Big CAD (ABC) dataset demonstrates that domain-adapted LLMs can generate high-quality functional data, improving early design decision-making and supporting more effective design exploration. <br /> <div>
arXiv:2505.01627v1 Announce Type: cross 
Abstract: The conceptual design phase represents a critical early stage in the product development process, where designers generate potential solutions that meet predefined design specifications based on functional requirements. Functional modeling, a foundational aspect of this phase, enables designers to reason about product functions before specific structural details are determined. A widely adopted approach to functional modeling is the Function-Behavior-Structure (FBS) framework, which supports the transformation of functional intent into behavioral and structural descriptions. However, the effectiveness of function-based design is often hindered by the lack of well-structured and comprehensive functional data. This scarcity can negatively impact early design decision-making and hinder the development of accurate behavioral models. Recent advances in Large Language Models (LLMs), such as those based on GPT architectures, offer a promising avenue to address this gap. LLMs have demonstrated significant capabilities in language understanding and natural language processing (NLP), making them suitable for automated classification tasks. This study proposes a novel LLM-based domain adaptation (DA) framework using fine-tuning for the automated classification of mechanical assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the traditionally manual and subjective process of function annotation can be improved in both accuracy and consistency. A case study demonstrates fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the domain-adapted LLM can generate high-quality functional data, enhancing the semantic representation of mechanical parts and supporting more effective design exploration in early-phase engineering.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strengthening Infrastructure Resilience to Hurricanes by Modeling Transportation and Electric Power Network Interdependencies</title>
<link>https://arxiv.org/abs/2404.12978</link>
<guid>https://arxiv.org/abs/2404.12978</guid>
<content:encoded><![CDATA[
<div> agent-based model, resilience, hurricane, infrastructure disruptions, interdependencies

Summary:
The study introduces an agent-based model (ABM) to analyze community resilience to hurricane-induced infrastructure disruptions, specifically focusing on the interconnections between electric power and transportation networks. Agents in the ABM represent different system components, such as electric power network, transportation network, hazards, and households. By considering interactions within and among systems, the model simulates household resilience during a hurricane in Miami-Dade County, Florida. The model incorporates two key interdependencies: the role of transportation in fuel delivery to power plants and restoration teams' access, as well as the impact of power outages on transportation network components. Validated against Hurricane Irma data, the ABM demonstrates the effectiveness of a traffic lights-based restoration strategy, prioritizing signal recovery to minimize traffic disruptions and accelerate household power restoration. The study emphasizes the importance of timely traffic signal restoration, road accessibility for restoration teams, and uninterrupted fuel transportation for efficient power restoration in hurricane scenarios. 

<br /><br />Summary: <div>
arXiv:2404.12978v2 Announce Type: replace 
Abstract: This study presents an agent-based model (ABM) developed to simulate the resilience of a community to hurricane-induced infrastructure disruptions, focusing on the interdependencies between electric power and transportation networks. In this ABM approach, agents represent the components of a system, where interactions within a system shape intra-dependency of a system and interactions among systems shape interdependencies. To study household resilience subject to a hurricane, a library of agents has been created including electric power network, transportation network, wind/flooding hazards, and household agents. The ABM is applied over the household and infrastructure data from a community (Zip code 33147) in Miami-Dade County, Florida. Interdependencies between the two networks are modeled in two ways, (i) representing the role of transportation in fuel delivery to power plants and restoration teams' access, (ii) impact of power outage on transportation network components. Restoring traffic signals quickly is crucial as their outage can slow down traffic and increase the chance of crashes. We simulate three restoration strategies: component based, distance based, and traffic lights based restoration. The model is validated against Hurricane Irma data, showing consistent behavior with varying hazard intensities. Scenario analyses explore the impact of restoration strategies, road accessibility, and wind speed intensities on power restoration. Results demonstrate that a traffic lights based restoration strategy efficiently prioritizes signal recovery without delaying household power restoration time. Restoration of power services will be faster if restoration teams do not need to wait due to inaccessible roads and fuel transportation to power plants is not delayed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A finite strain model for fiber angle plasticity of textile fabrics based on isogeometric shell finite elements</title>
<link>https://arxiv.org/abs/2412.20131</link>
<guid>https://arxiv.org/abs/2412.20131</guid>
<content:encoded><![CDATA[
<div> shear elastoplasticity model, textile fabrics, anisotropic Kirchhoff-Love shells, plasticity, frictional sliding <br />
Summary: <br />
This work introduces a shear elastoplasticity model for textile fabrics based on anisotropic Kirchhoff-Love shells with embedded fiber bending. The model accounts for rotational inter-ply frictional sliding between fiber families in textile composites experiencing large deformation, emphasizing dry fabrics like woven and non-crimp fabrics. Utilizing relative angles between fiber families as strain measures, the model is formulated using surface invariants without thickness integration. A yield function with isotropic hardening and simple evolution equation is proposed, calibrated using the picture frame test, and validated with experimental data like the bias extension test. The elastoplastic model's accuracy is confirmed through good agreement with experimental results, and its application to 3D shell problems is demonstrated. <div>
arXiv:2412.20131v2 Announce Type: replace 
Abstract: This work presents a shear elastoplasticity model for textile fabrics within the theoretical framework of anisotropic Kirchhoff-Love shells with bending of embedded fibers proposed by Duong et al. (2023). The plasticity model aims at capturing the rotational inter-ply frictional sliding between fiber families in textile composites undergoing large deformation. Such effects are usually dominant in dry textile fabrics such as woven and non-crimp fabrics. The model explicitly uses relative angles between fiber families as strain measures for the kinematics. The plasticity model is formulated directly with surface invariants without resorting to thickness integration. Motivated by experimental observations from the picture frame test, a yield function is proposed with isotropic hardening and a simple evolution equation. A classical return mapping algorithm is employed to solve the elastoplastic problem within the isogeometric finite shell element formulation of Duong et al. (2022). The verification of the implementation is facilitated by the analytical solution for the picture frame test. The proposed plasticity model is calibrated from the picture frame test and is then validated by the bias extension test, considering available experimental data for different samples from the literature. Good agreement between model prediction and experimental data is obtained. Finally, the applicability of the elastoplasticity model to 3D shell problems is demonstrated.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskLabs: Predicting Financial Risk Using Large Language Model based on Multimodal and Multi-Sources Data</title>
<link>https://arxiv.org/abs/2404.07452</link>
<guid>https://arxiv.org/abs/2404.07452</guid>
<content:encoded><![CDATA[
<div> framework, LLMs, financial risk prediction, RiskLabs, multimodal data <br />
Summary: <br />
This paper introduces RiskLabs, a framework that utilizes large language models (LLMs) to predict financial risks by integrating multimodal financial data sources such as Earnings Conference Calls (ECCs), market-related time series data, and contextual news data. The study demonstrates the effectiveness of RiskLabs in forecasting market volatility and variance. It examines the contributions of different data sources to financial risk assessment and emphasizes the importance of LLMs in this process. The paper also discusses the challenges associated with using LLMs for financial risk prediction and explores the potential benefits of combining LLMs with multimodal data for enhanced risk analysis. <div>
arXiv:2404.07452v2 Announce Type: replace-cross 
Abstract: The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering, and stock movement prediction (binary classification), the application of LLMs to financial risk prediction remains underexplored. Addressing this gap, in this paper, we introduce RiskLabs, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely integrates multimodal financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data to improve financial risk prediction. Empirical results demonstrate RiskLabs' effectiveness in forecasting both market volatility and variance. Through comparative experiments, we examine the contributions of different data sources to financial risk assessment and highlight the crucial role of LLMs in this process. We also discuss the challenges associated with using LLMs for financial risk prediction and explore the potential of combining them with multimodal data for this purpose.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spectral-based Physics-informed Finite Operator Learning for Prediction of Mechanical Behavior of Microstructures</title>
<link>https://arxiv.org/abs/2410.19027</link>
<guid>https://arxiv.org/abs/2410.19027</guid>
<content:encoded><![CDATA[
<div> operator learning, spectral methods, heterogeneous materials, physics-informed, microstructures<br />
<br />
Summary: A novel physics-informed operator learning technique based on spectral methods introduces the Lippmann-Schwinger operator in Fourier space to model heterogeneous materials. The method accelerates training by enabling gradient construction on a fixed discretization in Fourier space and maps microstructure shapes to mechanical responses without labeled data. Training minimizes equilibrium in Fourier space under loading conditions, ensuring periodicity. Physically constrained and diverse training data enhance accuracy, although performance may degrade for out-of-distribution microstructures. Integration of a Fourier Neural Operator improves accuracy in predicting stress fields and offers zero-shot super-resolution capabilities in heterogeneous domains. Extension to handle 3D problems and adaptation to finite elasticity demonstrate robustness in handling nonlinear mechanical behavior. This framework shows promise for efficient and scalable prediction of mechanical responses in complex material systems while reducing training time for physics-informed neural operators. <br /><br /> <div>
arXiv:2410.19027v3 Announce Type: replace-cross 
Abstract: A novel physics-informed operator learning technique based on spectral methods is introduced to model the complex behavior of heterogeneous materials. The Lippmann-Schwinger operator in Fourier space is employed to construct physical constraints with minimal computational overhead, effectively eliminating the need for automatic differentiation. The introduced methodology accelerates the training process by enabling gradient construction on a fixed, finite discretization in Fourier space. Later, the spectral physics-informed finite operator learning (SPiFOL) framework is built based on this discretization and trained to map the arbitrary shape of microstructures to their mechanical responses (strain fields) without relying on labeled data. The training is done by minimizing equilibrium in Fourier space concerning the macroscopic loading condition, which also guarantees the periodicity. SPiFOL, as a physics-informed operator learning method, enables rapid predictions through forward inference after training. To ensure accuracy, we incorporate physical constraints and diversify the training data. However, performance may still degrade for out-of-distribution microstructures. SPiFOL is further enhanced by integrating a Fourier Neural Operator (FNO). Compared to the standard data-driven FNO, SPiFOL shows higher accuracy in predicting stress fields and provides nearly resolution-independent results. Additionally, its zero-shot super-resolution capabilities are explored in heterogeneous domains. Finally, SPiFOL is extended to handle 3D problems and further adapted to finite elasticity, demonstrating the robustness of the framework in handling nonlinear mechanical behavior. The framework shows great potential for efficient and scalable prediction of mechanical responses in complex material systems while also reducing the training time required for training physics-informed neural operators.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction</title>
<link>https://arxiv.org/abs/2501.01010</link>
<guid>https://arxiv.org/abs/2501.01010</guid>
<content:encoded><![CDATA[
<div> State Space Model, cryptocurrency, Bitcoin, forecasting, financial data
<br />
Summary:
<br />
- The study addresses the challenge of predicting Bitcoin prices due to market volatility and dynamics.
- Traditional models like ARIMA and neural networks struggle with regime shifts and dependencies in data.
- The proposed CryptoMamba, a Mamba-based State Space Model, effectively captures long-range dependencies.
- CryptoMamba outperforms previous models in accuracy and generalizability across market conditions.
- The model's practical utility is demonstrated through accurate forecasts translating into financial gains, making it advantageous for stock and cryptocurrency price forecasting tasks. 
<br /> <div>
arXiv:2501.01010v2 Announce Type: replace-cross 
Abstract: Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming physics-informed machine learning to convex optimization</title>
<link>https://arxiv.org/abs/2505.01047</link>
<guid>https://arxiv.org/abs/2505.01047</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Physics-Informed, Convex Optimization, B-splines, Adaptive Knot Optimization

Summary:
Convex-PIML is a framework proposed to address optimization challenges in Physics-Informed Machine Learning (PIML). By transforming PIML into convex optimization, the framework overcomes limitations and enables effective integration of data with physical laws. The use of linear combination of B-splines promotes the convexity of the loss function, allowing well-established convex optimization algorithms to be utilized for efficient solutions. An adaptive knot optimization method tackles the spectral bias issue of PIML, enhancing performance. The framework is theoretically guaranteed and tested across scenarios with various physical priors, demonstrating effective solution of optimization problems. Convex-PIML shows promise for diverse applications by offering a comprehensive approach to combining data and physics for scientific problem-solving. <br /><br />Summary: <div>
arXiv:2505.01047v1 Announce Type: new 
Abstract: Physics-Informed Machine Learning (PIML) offers a powerful paradigm of integrating data with physical laws to address important scientific problems, such as parameter estimation, inferring hidden physics, equation discovery, and state prediction, etc. However, PIML still faces many serious optimization challenges that significantly restrict its applications. In this study, we propose a comprehensive framework that transforms PIML to convex optimization to overcome all these limitations, referred to as Convex-PIML. The linear combination of B-splines is utilized to approximate the data, promoting the convexity of the loss function. By replacing the non-convex components of the loss function with convex approximations, the problem is further converted into a sequence of successively refined approximated convex optimization problems. This conversion allows the use of well-established convex optimization algorithms, obtaining solutions effectively and efficiently. Furthermore, an adaptive knot optimization method based on error estimate is introduced to mitigate the spectral bias issue of PIML, further improving the performance. The proposed theoretically guaranteed framework is tested in scenarios with distinct types of physical prior. The results indicate that optimization problems are effectively solved in these scenarios, highlighting the potential of the framework for broad applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced-order structure-property linkages for stochastic metamaterials</title>
<link>https://arxiv.org/abs/2505.01283</link>
<guid>https://arxiv.org/abs/2505.01283</guid>
<content:encoded><![CDATA[
<div> additive manufacturing, mechanical metamaterials, unit cell geometries, materials informatics framework, Gaussian process regression

Summary:<br />
The article discusses the efficient design and evaluation of mechanical metamaterials using additive manufacturing. It emphasizes the need to establish connections between unit cell designs and their mechanical properties, which can be computationally intensive. The study employs principal component analysis to identify key features from a large dataset of 2D metamaterials. Fast Fourier transform-based simulations are utilized to calculate the effective elastic stiffness of different unit cell designs. Gaussian process regression is then applied to create reduced-order models mapping designs to their elastic constants. The research demonstrates the creation of robust structure-property maps through a low-dimensional representation of the dataset. Additionally, an active learning approach is used to train a surrogate model with minimal data points, showcasing the ability to generate accurate maps with a small fraction of the original dataset. <div>
arXiv:2505.01283v1 Announce Type: new 
Abstract: The capabilities of additive manufacturing have facilitated the design and production of mechanical metamaterials with diverse unit cell geometries. Establishing linkages between the vast design space of unit cells and their effective mechanical properties is critical for the efficient design and performance evaluation of such metamaterials. However, physics-based simulations of metamaterial unit cells across the entire design space are computationally expensive, necessitating a materials informatics framework to efficiently capture complex structure-property relationships. In this work, principal component analysis of 2-point correlation functions is performed to extract the salient features from a large dataset of randomly generated 2D metamaterials. Physics-based simulations are performed using a fast Fourier transform (FFT)-based homogenization approach to efficiently compute the homogenized effective elastic stiffness across the extensive unit cell designs. Subsequently, Gaussian process regression is used to generate reduced-order surrogates, mapping unit cell designs to their homogenized effective elastic constant. It is demonstrated that the adopted workflow enables a high-value low-dimensional representation of the voluminous stochastic metamaterial dataset, facilitating the construction of robust structure-property maps. Finally, an uncertainty-based active learning framework is utilized to train a surrogate model with a significantly smaller number of data points compared to the original full dataset. It is shown that a dataset as small as $0.61\%$ of the entire dataset is sufficient to generate accurate and robust structure-property maps.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimICD: A Closed-Loop Simulation Framework For ICD Therapy</title>
<link>https://arxiv.org/abs/2505.01371</link>
<guid>https://arxiv.org/abs/2505.01371</guid>
<content:encoded><![CDATA[
<div> simulation, ICD behavior, cardiac electrophysiology, arrhythmic episodes, therapy progression

Summary:
SimICD is a new simulation tool that integrates virtual ICD logic algorithms with cardiac electrophysiology simulations to simulate therapy progression decisions during arrhythmic episodes. This tool fills a gap in available models by allowing for the testing of ICD functionality in a controlled environment before clinical use. The simulations conducted with SimICD demonstrate the realistic simulation of cardiac signals and ICD responses that align with real-world devices' logic. This enables the reprogramming of ICD parameters to adapt to specific tachy-arrhythmia episodes, improving treatment customization and efficacy. Overall, SimICD facilitates virtual studies of ICD behavior, enhancing the understanding and testing of device functionality for better clinical outcomes. 

<br /><br />Summary: <div>
arXiv:2505.01371v1 Announce Type: new 
Abstract: Virtual studies of ICD behaviour are crucial for testing device functionality in a controlled environment prior to clinical application. Although previous works have shown the viability of using in silico testing for diagnosis, there is a notable gap in available models that can simulate therapy progression decisions during arrhythmic episodes. This work introduces SimICD, a simulation tool which combines virtual ICD logic algorithms with cardiac electrophysiology simulations in a feedback loop, allowing the progression of ICD therapy protocols to be simulated for a range of tachy-arrhythmia episodes. Using a cohort of virtual patients, we demonstrate the ability of SimICD to simulate realistic cardiac signals and ICD responses that align with the logic of real-world devices, facilitating the reprogramming of ICD parameters to adapt to specific episodes.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design for a Digital Twin in Clinical Patient Care</title>
<link>https://arxiv.org/abs/2505.01206</link>
<guid>https://arxiv.org/abs/2505.01206</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Twins, clinical patient care, knowledge graphs, ensemble learning, decision-making <br />
Summary:<br />
Digital Twins have the potential to revolutionize personalized clinical patient care by integrating knowledge graphs and ensemble learning to create a comprehensive reflection of a patient's clinical journey. These Digital Twins are predictive, modular, evolving, informed, interpretable, and explainable, providing valuable insights for clinicians to make informed decisions. By incorporating these elements, Digital Twins can cater to a variety of medical fields, from oncology to epidemiology, offering a versatile tool for healthcare professionals. This innovative approach ensures that Digital Twins are not only accurate and predictive but also adaptable to changing patient needs. Overall, this new design for Digital Twins has the capacity to significantly enhance clinical decision-making processes and improve patient outcomes.<br />Summary: <div>
arXiv:2505.01206v1 Announce Type: cross 
Abstract: Digital Twins hold great potential to personalize clinical patient care, provided the concept is translated to meet specific requirements dictated by established clinical workflows. We present a generalizable Digital Twin design combining knowledge graphs and ensemble learning to reflect the entire patient's clinical journey and assist clinicians in their decision-making. Such Digital Twins can be predictive, modular, evolving, informed, interpretable and explainable with applications ranging from oncology to epidemiology.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks</title>
<link>https://arxiv.org/abs/2402.08978</link>
<guid>https://arxiv.org/abs/2402.08978</guid>
<content:encoded><![CDATA[
<div> visualization, financial cluster analysis, quantitative analysis, qualitative analysis, multi-view clustering

Summary: 
Prismatic is a visual analytics system designed to assist investors in financial cluster analysis by integrating quantitative and qualitative analysis. It addresses challenges such as numerous pairwise comparisons and dynamic correlations across different time periods. Prismatic features dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation processes. By using a multi-view clustering approach, it combines data-driven clusters with knowledge-driven similarity, providing a more nuanced understanding of business correlations. The system offers well-coordinated visual views to facilitate a comprehensive interpretation of intertwined quantitative and qualitative features. Case studies on formulating concept stocks and interviews with domain experts demonstrate the effectiveness and usefulness of Prismatic in discovering investment alternatives and managing risks in the financial market. <div>
arXiv:2402.08978v2 Announce Type: replace-cross 
Abstract: Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural Networks</title>
<link>https://arxiv.org/abs/2409.07769</link>
<guid>https://arxiv.org/abs/2409.07769</guid>
<content:encoded><![CDATA[
<div> Graph neural network, mesh-based super-resolution, fluid flows, multiscale model, Reynolds number
<br />
Summary:
<br />
This study presents a novel approach using a graph neural network (GNN) for mesh-based three-dimensional super-resolution of fluid flows. The GNN operates on localized meshes of elements to improve accuracy. The architecture includes coarse-scale and fine-scale processors separated by a graph unpooling layer for multiscale modeling. Results from simulations of Taylor-Green Vortex and backward-facing step flow demonstrate the GNN's ability to produce accurate super-resolved fields compared to coarse-scale and multiscale models. Reconstruction errors increase with higher Reynolds numbers. Additionally, the GNN shows promising capabilities for geometry extrapolation in cross-mesh scenarios. <div>
arXiv:2409.07769v4 Announce Type: replace-cross 
Abstract: A graph neural network (GNN) approach is introduced in this work which enables mesh-based three-dimensional super-resolution of fluid flows. In this framework, the GNN is designed to operate not on the full mesh-based field at once, but on localized meshes of elements (or cells) directly. To facilitate mesh-based GNN representations in a manner similar to spectral (or finite) element discretizations, a baseline GNN layer (termed a message passing layer, which updates local node properties) is modified to account for synchronization of coincident graph nodes, rendering compatibility with commonly used element-based mesh connectivities. The architecture is multiscale in nature, and is comprised of a combination of coarse-scale and fine-scale message passing layer sequences (termed processors) separated by a graph unpooling layer. The coarse-scale processor embeds a query element (alongside a set number of neighboring coarse elements) into a single latent graph representation using coarse-scale synchronized message passing over the element neighborhood, and the fine-scale processor leverages additional message passing operations on this latent graph to correct for interpolation errors. Demonstration studies are performed using hexahedral mesh-based data from Taylor-Green Vortex and backward-facing step flow simulations at Reynolds numbers of 1600 and 3200. Through analysis of both global and local errors, the results ultimately show how the GNN is able to produce accurate super-resolved fields compared to targets in both coarse-scale and multiscale model configurations. Reconstruction errors for fixed architectures were found to increase in proportion to the Reynolds number. Geometry extrapolation studies on a separate cavity flow configuration show promising cross-mesh capabilities of the super-resolution strategy.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Multimodal Multiscale Data Fusion for Digital Twins in Aerosol Jet Electronics Printing</title>
<link>https://arxiv.org/abs/2505.00176</link>
<guid>https://arxiv.org/abs/2505.00176</guid>
<content:encoded><![CDATA[
<div> Aerosol Jet Printing, Additive Manufacturing, Machine Learning, Process-Structure-Property Modeling, Diffusion Models<br />
<br />
Summary: This study introduces a novel generative modeling methodology using diffusion models to fuse multimodal and multiscale Process-Structure-Property (PSP) data in Aerosol Jet Printing (AJP). The method aims to enhance manufacturing by quantitatively connecting process parameters, structural features, and material properties. Current machine learning approaches for AJP face limitations in handling complex data, highlighting the need for comprehensive analysis through fusion methods. The proposed approach registers and fuses optical microscopy and confocal profilometry data from AJP, capturing intricate PSP relationships and providing insights into dynamic manufacturing systems' digital twins. The results demonstrate effective fusion and fine-tuning steps, offering a deeper understanding of complex AJP processes.<br /><br />Summary: <div>
arXiv:2505.00176v1 Announce Type: new 
Abstract: The rising demand for high-value electronics necessitates advanced manufacturing techniques capable of meeting stringent specifications for precise, complex, and compact devices, driving the shift toward innovative additive manufacturing (AM) solutions. Aerosol Jet Printing (AJP) is a versatile AM technique that utilizes aerosolized functional materials to accurately print intricate patterns onto diverse substrates. Machine learning (ML)- based Process-Structure-Property (PSP) modeling is essential for enhancing AJP manufacturing, as it quantitatively connects process parameters, structural features, and resulting material properties. However, current ML approaches for modeling PSP relationships in AJP face significant limitations in handling multimodal and multiscale data, underscoring a critical need for generative methods capable of comprehensive analysis through multimodal and multiscale fusion. To address this challenge, this study introduces a novel generative modeling methodology leveraging diffusion models for PSP data fusion in AJP. The proposed method integrates multimodal, multiscale PSP features in two phases: (1) registering the features, and (2) fusing them to generate causal relationships between PSP attributes. A case study demonstrates the registration and fusion of optical microscopy (OM) images and confocal profilometry (CP) data from AJP, along with the fine-tuning of the fusion step. The results effectively capture complex PSP relationships, offering deeper insights into digital twins of dynamic manufacturing systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Thermal Control Based on Spatial Thermal Comfort with Reconstructed Environmental Data</title>
<link>https://arxiv.org/abs/2505.00468</link>
<guid>https://arxiv.org/abs/2505.00468</guid>
<content:encoded><![CDATA[
<div> Reconstruction, Thermal comfort, Predicted Mean Vote (PMV), Gappy Proper Orthogonal Decomposition (Gappy POD), Multi-occupant living lab environment

Summary:
The study introduces a novel method for estimating thermal comfort by incorporating spatial environmental data reconstructed using the Gappy Proper Orthogonal Decomposition (Gappy POD) algorithm. This addresses limitations of fixed-location sensors by enabling accurate reconstruction of indoor temperature fields. A group PMV-based control framework is developed to consider the thermal comfort of multiple occupants, allowing for individual and group-level thermal condition calculations. Experimental results demonstrate the effectiveness of the Gappy POD algorithm in temperature reconstruction with a low average relative error. The spatial variability in PMV values based on occupant location highlights the importance of adaptive thermal control strategies. The study underscores the significance of adaptive thermal control strategies that consider both spatial and individual variability for enhanced occupant-centric building operations. <br /><br />Summary: <div>
arXiv:2505.00468v1 Announce Type: new 
Abstract: Achieving thermal comfort while maintaining energy efficiency is a critical objective in building system control. Conventional thermal comfort models, such as the Predicted Mean Vote (PMV), rely on both environmental and personal variables. However, the use of fixed-location sensors limits the ability to capture spatial variability, which reduces the accuracy of occupant-specific comfort estimation. To address this limitation, this study proposes a new PMV estimation method that incorporates spatial environmental data reconstructed using the Gappy Proper Orthogonal Decomposition (Gappy POD) algorithm. In addition, a group PMV-based control framework is developed to account for the thermal comfort of multiple occupants. The Gappy POD method enables fast and accurate reconstruction of indoor temperature fields from sparse sensor measurements. Using these reconstructed fields and occupant location data, spatially resolved PMV values are calculated. Group-level thermal conditions are then derived through statistical aggregation methods and used to control indoor temperature in a multi-occupant living lab environment. Experimental results show that the Gappy POD algorithm achieves an average relative error below 3\% in temperature reconstruction. PMV distributions varied by up to 1.26 scale units depending on occupant location. Moreover, thermal satisfaction outcomes varied depending on the group PMV method employed. These findings underscore the importance for adaptive thermal control strategies that incorporate both spatial and individual variability, offering valuable insights for future occupant-centric building operations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review</title>
<link>https://arxiv.org/abs/2505.00210</link>
<guid>https://arxiv.org/abs/2505.00210</guid>
<content:encoded><![CDATA[
<div> machine learning, manufacturing, control systems, generative models, process monitoring<br />
<br />
Summary: This review explores the integration of generative machine learning (ML) in dynamic manufacturing processes to enhance in-situ monitoring and control systems. The complex characteristics of manufacturing systems, including time-varying parameters and uncertainties, necessitate advanced control techniques that can respond in real-time while maintaining product quality. The review categorizes approaches into Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated methods, highlighting the potential of generative ML in decision-making, process guidance, simulation, and digital twins. However, challenges such as the separation between generation and control functions, lack of physical understanding of manufacturing phenomena, and model adaptation from other domains need to be addressed. Future research directions focus on developing integrated frameworks that combine generative ML and control technologies to effectively address the dynamic complexities of modern manufacturing systems. <div>
arXiv:2505.00210v1 Announce Type: cross 
Abstract: Dynamic manufacturing processes exhibit complex characteristics defined by time-varying parameters, nonlinear behaviors, and uncertainties. These characteristics require sophisticated in-situ monitoring techniques utilizing multimodal sensor data and adaptive control systems that can respond to real-time feedback while maintaining product quality. Recently, generative machine learning (ML) has emerged as a powerful tool for modeling complex distributions and generating synthetic data while handling these manufacturing uncertainties. However, adopting these generative technologies in dynamic manufacturing systems lacks a functional control-oriented perspective to translate their probabilistic understanding into actionable process controls while respecting constraints. This review presents a functional classification of Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated approaches, offering a perspective for understanding existing ML-enhanced control systems and incorporating generative ML. The analysis of generative ML architectures within this framework demonstrates control-relevant properties and potential to extend current ML-enhanced approaches where conventional methods prove insufficient. We show generative ML's potential for manufacturing control through decision-making applications, process guidance, simulation, and digital twins, while identifying critical research gaps: separation between generation and control functions, insufficient physical understanding of manufacturing phenomena, and challenges adapting models from other domains. To address these challenges, we propose future research directions aimed at developing integrated frameworks that combine generative ML and control technologies to address the dynamic complexities of modern manufacturing systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subspace-Distance-Enabled Active Learning for Efficient Data-Driven Model Reduction of Parametric Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.00460</link>
<guid>https://arxiv.org/abs/2505.00460</guid>
<content:encoded><![CDATA[
<div> active learning, data-driven model reduction, parametric data-driven reduced-order model, proper orthogonal decomposition, subspace-distance-enabled active learning

Summary:<br />
In scenarios where high-fidelity dynamical systems require repetitive evaluation across a wide range of parameter configurations without access to governing equations, data-driven model reduction techniques are preferred. This study introduces an active learning method for constructing a parametric data-driven reduced-order model by selecting crucial parameter samples from the parameter domain. The approach involves representing high-fidelity solution snapshots in parameter-specific linear subspaces using proper orthogonal decomposition, with the relative distance between these subspaces guiding the active learning process. A distance metric is provided for comparing similarity between linear subspaces of different dimensions. The proposed subspace-distance-enabled active learning (SDE-AL) framework is successfully applied to enhance existing reduced-order modeling methods through active-learning-driven extensions. Positive results are demonstrated for two parametric physical models, showcasing the effectiveness of the SDE-AL approach.  <div>
arXiv:2505.00460v1 Announce Type: cross 
Abstract: In situations where the solution of a high-fidelity dynamical system needs to be evaluated repeatedly, over a vast pool of parametric configurations and in absence of access to the underlying governing equations, data-driven model reduction techniques are preferable. We propose a novel active learning approach to build a parametric data-driven reduced-order model (ROM) by greedily picking the most important parameter samples from the parameter domain. As a result, during the ROM construction phase, the number of high-fidelity solutions dynamically grow in a principled fashion. The high-fidelity solution snapshots are expressed in several parameter-specific linear subspaces, with the help of proper orthogonal decomposition (POD), and the relative distance between these subspaces is used as a guiding mechanism to perform active learning. For successfully achieving this, we provide a distance measure to evaluate the similarity between pairs of linear subspaces with different dimensions, and also show that this distance measure is a metric. The usability of the proposed subspace-distance-enabled active learning (SDE-AL) framework is demonstrated by augmenting two existing non-intrusive reduced-order modeling approaches, and providing their active-learning-driven (ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN. Furthermore, we report positive results for two parametric physical models, highlighting the efficiency of the proposed SDE-AL approach.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks</title>
<link>https://arxiv.org/abs/2505.00530</link>
<guid>https://arxiv.org/abs/2505.00530</guid>
<content:encoded><![CDATA[
<div> Keywords: SMILES-based molecule generation, deep reinforcement learning, catastrophic forgetting, molecule validity, exploration mechanisms

Summary:<br />
The article introduces a novel RL algorithm, Partial SMILES Validation-PPO (PSV-PPO), designed to address catastrophic forgetting during molecule generation in drug discovery. By incorporating real-time partial SMILES validation at each step of the sequence generation process, PSV-PPO prevents the deterioration of molecule validity while promoting exploration. Unlike traditional approaches that validate molecules only after completion, PSV-PPO evaluates potential branches at each step, ensuring high validity rates even during aggressive exploration. Experimental results on benchmark datasets show that PSV-PPO reduces invalid structures while maintaining competitive performance. The framework of PSV-PPO can be extended to incorporate additional domain knowledge, enhancing RL applications in drug discovery. <br /><br />Summary: <div>
arXiv:2505.00530v1 Announce Type: cross 
Abstract: SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMM-based DEX on the XRP Ledger</title>
<link>https://arxiv.org/abs/2312.13749</link>
<guid>https://arxiv.org/abs/2312.13749</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Market Maker, Decentralized Exchanges, XRP Ledger, Agent-based simulations, Continuous Auction Mechanism

Summary:
The study compares an Automated Market Maker (AMM)-based Decentralized Exchange (DEX) implementation on the XRP Ledger (XRPL) against a Generic AMM-based DEX on Ethereum. Through agent-based simulations using real market data, the XRPL-AMM-DEX demonstrates superior price synchronization, reduced slippage, and improved returns due to lower fees and shorter block times on the XRPL. The study also highlights the benefits of the integrated Continuous Auction Mechanism (CAM) in mitigating impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). This comparative analysis is the first to explore protocol-level and smart contract AMM-based DEX implementations and validate theoretical auction mechanisms through simulations. <div>
arXiv:2312.13749v4 Announce Type: replace 
Abstract: Automated Market Maker (AMM)-based Decentralized Exchanges (DEXs) are crucial in Decentralized Finance (DeFi), but Ethereum implementations suffer from high transaction costs and price synchronization challenges. To address these limitations, we compare the XRP Ledger (XRPL)-AMM-Decentralized Exchange (DEX), a protocol-level implementation, against a Generic AMM-based DEX (G-AMM-DEX) on Ethereum, akin to Uniswap's V2 AMM implementation, through agent-based simulations using real market data and multiple volatility scenarios generated via Geometric Brownian Motion (GBM). Results demonstrate that the XRPL-AMM-DEX achieves superior price synchronization, reduced slippage, and improved returns due to XRPL's lower fees and shorter block times, with benefits amplifying during market volatility. The integrated Continuous Auction Mechanism (CAM) further mitigates impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). To the best of our knowledge, this study represents the first comparative analysis between protocol-level and smart contract AMM-based DEX implementations and the first agent-based simulation validating theoretical auction mechanisms for AMM-based DEXs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing</title>
<link>https://arxiv.org/abs/2504.21317</link>
<guid>https://arxiv.org/abs/2504.21317</guid>
<content:encoded><![CDATA[
<div> redundancy, machine learning, additive manufacturing, process monitoring, mitigation

Summary:<br />
- Redundancy in machine learning-based additive manufacturing process monitoring systems can lead to increased costs, compromised performance, and high computational requirements.
- This paper defines redundancy at sample-level, feature-level, and model-level and proposes a multi-level redundancy mitigation framework.
- The framework includes methods such as data registration, downscaling, cross-modality knowledge transfer, and model pruning to reduce redundancy and improve model performance.
- In a case study for in-situ defect detection in directed energy deposition, the proposed approach showed a 91% reduction in latency, a 47% decrease in error rate, and a 99.4% reduction in storage requirements.
- The framework also allows for lower sensor costs and energy consumption, resulting in a lightweight, cost-effective, and scalable monitoring system.

<br /><br />Summary: <div>
arXiv:2504.21317v1 Announce Type: new 
Abstract: The deployment of machine learning (ML)-based process monitoring systems has significantly advanced additive manufacturing (AM) by enabling real-time defect detection, quality assessment, and process optimization. However, redundancy is a critical yet often overlooked challenge in the deployment and operation of ML-based AM process monitoring systems. Excessive redundancy leads to increased equipment costs, compromised model performance, and high computational requirements, posing barriers to industrial adoption. However, existing research lacks a unified definition of redundancy and a systematic framework for its evaluation and mitigation. This paper defines redundancy in ML-based AM process monitoring and categorizes it into sample-level, feature-level, and model-level redundancy. A comprehensive multi-level redundancy mitigation (MLRM) framework is proposed, incorporating advanced methods such as data registration, downscaling, cross-modality knowledge transfer, and model pruning to systematically reduce redundancy while improving model performance. The framework is validated through an ML-based in-situ defect detection case study for directed energy deposition (DED), demonstrating a 91% reduction in latency, a 47% decrease in error rate, and a 99.4% reduction in storage requirements. Additionally, the proposed approach lowers sensor costs and energy consumption, enabling a lightweight, cost-effective, and scalable monitoring system. By defining redundancy and introducing a structured mitigation framework, this study establishes redundancy analysis and mitigation as a key enabler of efficient ML-based process monitoring in production environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Analysis and Implementation of Cryptocurrency Systems on Blockchain 2.0</title>
<link>https://arxiv.org/abs/2504.21367</link>
<guid>https://arxiv.org/abs/2504.21367</guid>
<content:encoded><![CDATA[
<div> blockchain, decentralization, smart contracts, security, cryptocurrency
<br />
Summary: 
Blockchain technology has revolutionized decentralization by providing a trust system through cryptography and computing power. Smart contracts have further expanded blockchain application possibilities by enabling automatic execution based on predefined triggers. However, the programmability of smart contracts introduces security vulnerabilities. This article delves into the technical details of blockchain 2.0, focusing on Ethereum, and explains the operation principles of contract virtual machines. It discusses how cryptocurrencies are constructed and operated on blockchain 2.0, highlighting common security issues and solutions. Drawing on research and on-chain practices, this comprehensive perspective aims to enhance the understanding of cryptocurrency technology on blockchain 2.0 and offers insights for creating more secure cryptocurrency contracts. <div>
arXiv:2504.21367v1 Announce Type: new 
Abstract: Blockchain technology has set off a wave of decentralization in the world since its birth. The trust system constructed by blockchain technology based on cryptography algorithm and computing power provides a practical and powerful solution to solve the trust problem in human society. In order to make more convenient use of the characteristics of blockchain and build applications on it, smart contracts appear. By defining some trigger automatic execution contracts, the application space of blockchain is expanded and the foundation for the rapid development of blockchain is laid. This is blockchain 2.0. However, the programmability of smart contracts also introduces vulnerabilities. In order to cope with the insufficient security guarantee of high-value application networks running on blockchain 2.0 and smart contracts, this article will be represented by Ethereum to introduce the technical details of understanding blockchain 2.0 and the operation principle of contract virtual machines, and explain how cryptocurrencies based on blockchain 2.0 are constructed and operated. The common security problems and solutions are also discussed. Based on relevant research and on-chain practice, this paper provides a complete and comprehensive perspective to understanding cryptocurrency technology based on blockchain 2.0 and provides a reference for building more secure cryptocurrency contracts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level datasets training method in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2504.21328</link>
<guid>https://arxiv.org/abs/2504.21328</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, PDEs, Multi-grid method, High-frequency components <br />
Summary: Physics-Informed Neural Networks (PINNs) have shown promise in solving PDEs but struggle with stiff and high-frequency problems, leading to accuracy issues. An alternative approach inspired by the multi-grid method is proposed to address these challenges. By training with different levels of samples, errors of varying frequencies can be efficiently removed, improving accuracy without complex tuning. The method is tested on 1D and 2D equations with high-frequency components and Lid-driven cavity flows at different Reynolds numbers. Results show a 30% to 60% accuracy improvement and success in solving complex high-frequency PDEs up to Re=5000. Synergies with transfer learning are also explored for more challenging problems. <div>
arXiv:2504.21328v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks have emerged as a promising methodology for solving PDEs, gaining significant attention in computer science and various physics-related fields. Despite being demonstrated the ability to incorporate the physics of laws for versatile applications, PINNs still struggle with the challenging problems which are stiff to be solved and/or have high-frequency components in the solutions, resulting in accuracy and convergence issues. It may not only increase computational costs, but also lead to accuracy loss or solution divergence. In this study, an alternative approach is proposed to mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD community, the underlying idea of the current approach is to efficiently remove different frequency errors via training with different levels of training samples, resulting in a simpler way to improve the training accuracy without spending time in fine-tuning of neural network structures, loss weights as well as hyperparameters. To demonstrate the efficacy of current approach, we first investigate canonical 1D ODE with high-frequency component and 2D convection-diffusion equation with V-cycle training strategy. Finally, the current method is employed for the classical benchmark problem of steady Lid-driven cavity flows at different Reynolds numbers, to investigate the applicability and efficacy for the problem involved multiple modes of high and low frequency. By virtue of various training sequence modes, improvement through predictions lead to 30% to 60% accuracy improvement. We also investigate the synergies between current method and transfer learning techniques for more challenging problems (i.e., higher Re). From the present results, it also revealed that the current framework can produce good predictions even for the case of Re=5000, demonstrating the ability to solve complex high-frequency PDEs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation</title>
<link>https://arxiv.org/abs/2504.21574</link>
<guid>https://arxiv.org/abs/2504.21574</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, financial ecosystem, cybersecurity, ethical risks, regulatory landscape

Summary: 
Generative Artificial Intelligence (GenAI) is revolutionizing the financial sector by enhancing customer engagement, automating workflows, and extracting insights from vast data. This survey explores how banks, insurers, asset managers, and fintech firms globally are adopting GenAI tools like large language models. While driving innovation, GenAI poses cybersecurity threats like AI-generated phishing and ethical concerns around bias and data misuse. The evolving regulatory landscape is analyzed, including initiatives for risk-based AI governance by major financial regulators. Best practices for secure and responsible GenAI adoption are proposed, such as explainability techniques and human oversight. By drawing on academic research, industry cases, and policy frameworks, this chapter offers insights on leveraging GenAI's potential while managing its complex risks.<br /><br />Summary: <div>
arXiv:2504.21574v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence (GenAI) is rapidly reshaping the global financial landscape, offering unprecedented opportunities to enhance customer engagement, automate complex workflows, and extract actionable insights from vast financial data. This survey provides an overview of GenAI adoption across the financial ecosystem, examining how banks, insurers, asset managers, and fintech startups worldwide are integrating large language models and other generative tools into their operations. From AI-powered virtual assistants and personalized financial advisory to fraud detection and compliance automation, GenAI is driving innovation across functions. However, this transformation comes with significant cybersecurity and ethical risks. We discuss emerging threats such as AI-generated phishing, deepfake-enabled fraud, and adversarial attacks on AI systems, as well as concerns around bias, opacity, and data misuse. The evolving global regulatory landscape is explored in depth, including initiatives by major financial regulators and international efforts to develop risk-based AI governance. Finally, we propose best practices for secure and responsible adoption - including explainability techniques, adversarial testing, auditability, and human oversight. Drawing from academic literature, industry case studies, and policy frameworks, this chapter offers a perspective on how the financial sector can harness GenAI's transformative potential while navigating the complex risks it introduces.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of HPC-Accelerated CFD in National Security and Defense</title>
<link>https://arxiv.org/abs/2504.07837</link>
<guid>https://arxiv.org/abs/2504.07837</guid>
<content:encoded><![CDATA[
<div> HPC, Computational Fluid Dynamics, defense applications, open-source frameworks, MPI domain decomposition, GPU acceleration, hybrid parallelism, research voids, exascale readiness, machine learning surrogate models<br />
Summary: <br />
This review discusses the use of High-Performance Computing (HPC) and Computational Fluid Dynamics (CFD) in defense-related national security applications. It examines the utilization of open-source CFD frameworks such as OpenFOAM, SU2, and ADflow in security-sensitive simulations. The review also explores how HPC techniques like MPI domain decomposition and GPU acceleration, along with hybrid parallelism, enhance open-source frameworks for managing large defense CFD simulations. Additionally, it addresses the technological advancements and research voids driving the field's development, categorizing scientific contributions into air, maritime, and space domains. The review highlights modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers, showcasing how custom open-source solutions support workflows for rapid completion of multi-million cell simulations. Finally, it discusses new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to improve CFD use in defense research and development. <div>
arXiv:2504.07837v2 Announce Type: replace 
Abstract: Using High-Performance Computing (HPC), Computational Fluid Dynamics (CFD) now serves as an essential component in defense-related national security applications including missile interception and hypersonic propulsion as well as naval stealth optimization and urban hazard dispersion. This review combines two decades of open-source and public-domain research on HPC-accelerated CFD in defense, addressing three key questions: Which security-sensitive simulations have utilized open-source CFD frameworks such as OpenFOAM, SU2 and ADflow? Which HPC techniques, such as MPI domain decomposition and GPU acceleration together with hybrid parallelism best enhance open-source frameworks to manage large defense CFD simulations? Which technological advancements and research voids currently drive the directional development of the field? Examining several research studies sourced from NASA, DoD HPC centers, and academic institutions, scientific contributions have been classified into air, maritime, and space domains. Modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers show how custom open-source solutions support workflows with rapid completion of multi-million cell simulations. The conclusion highlights new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to deliver practical insights for improving CFD use in defense research and development.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews</title>
<link>https://arxiv.org/abs/2502.05439</link>
<guid>https://arxiv.org/abs/2502.05439</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic systems, financial services industry, modeling crew, model risk management, autonomous decision-making<br />
Summary: 
This paper delves into the utilization of agentic systems in the financial services sector, employing teams of artificial intelligence agents to carry out intricate modeling and model risk management tasks with human oversight. The modeling crew, comprising a judge agent and specialized agents, executes a spectrum of activities from data analysis to model evaluation. Likewise, the model risk management crew, under the watchful eye of a judge agent, ensures compliance, replicability, and soundness of models. Numerical examples applied in credit card fraud detection, credit card approval, and portfolio credit risk modeling demonstrate the efficacy and reliability of these agentic crews. The integration of human judgment with machine efficiency showcases the potential of agentic systems in driving innovation and automation in financial decision-making processes.<br /><br />Summary: <div>
arXiv:2502.05439v2 Announce Type: replace-cross 
Abstract: The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews with human-in-the-loop module that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a judge agent and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection/hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a judge agent along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLiB: High-fidelity differentiable modeling of lithium-ion batteries and efficient gradient-based parameter identification</title>
<link>https://arxiv.org/abs/2504.20674</link>
<guid>https://arxiv.org/abs/2504.20674</guid>
<content:encoded><![CDATA[
<div> DiffLiB, LIB simulation, gradient-based inverse parameter identification, automatic differentiation, advanced computational framework<br />
Summary:<br />
DiffLiB is introduced as a high-fidelity LIB simulation framework that utilizes advanced differentiable programming techniques for efficient gradient-based inverse parameter identification in the complex DFN model. Customized automatic differentiation rules are defined to enable efficient gradient-based optimization, improving computational performance significantly compared to gradient-free methods. The framework shows excellent agreement in forward predictions, maintaining low terminal voltage discrepancies. In parameter identification tasks using measured voltage data, DiffLiB demonstrates superior computational performance with significantly fewer forward predictions and less computational time required. These results highlight DiffLiB as a versatile and powerful computational tool for advanced LIB development. <br /> <div>
arXiv:2504.20674v1 Announce Type: new 
Abstract: The physics-based Doyle-Fuller-Newman (DFN) model, widely adopted for its precise electrochemical modeling, stands out among various simulation models of lithium-ion batteries (LIBs). Although the DFN model is powerful in forward predictive analysis, the inverse identification of its model parameters has remained a long-standing challenge. The numerous unknown parameters associated with the nonlinear, time-dependent, and multi-scale DFN model are extremely difficult to be determined accurately and efficiently, hindering the practical use of such battery simulation models in industrial applications. To tackle this challenge, we introduce DiffLiB, a high-fidelity finite-element-based LIB simulation framework, equipped with advanced differentiable programming techniques so that efficient gradient-based inverse parameter identification is enabled. Customized automatic differentiation rules are defined by identifying the VJP (vector-Jacobian product) structure in the chain rule and implemented using adjoint-based implicit differentiation methods. Four numerical examples, including both 2D and 3D forward predictions and inverse parameter identification, are presented to validate the accuracy and computational efficiency of DiffLiB. Benchmarking against COMSOL demonstrates excellent agreement in forward predictions, with terminal voltage discrepancies maintaining a root-mean-square error (RMSE) below 2 mV across all test conditions. In parameter identification tasks using experimentally measured voltage data, the proposed gradient-based optimization scheme achieves superior computational performance, with 96% fewer forward predictions and 72% less computational time compared with gradient-free approaches. These results demonstrate that DiffLiB is a versatile and powerful computational framework for the development of advanced LIBs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Heterogeneity within Elastic and Inelastic Discrete Mechanical Models</title>
<link>https://arxiv.org/abs/2504.20861</link>
<guid>https://arxiv.org/abs/2504.20861</guid>
<content:encoded><![CDATA[
<div> Keywords: heterogeneous media, elastic behavior, fracture behavior, homogenization, randomization <br />
Summary: 
The study examines the elastic and fracture behaviors of discrete models of heterogeneous media with different forms of randomization. These models achieve homogeneity through volumetric-deviatoric decomposition or stress homogenization methods. It is observed that stress oscillations in heterogeneous geometric structures cannot be accurately replicated by randomizing elastic parameters in homogeneous models. Additionally, the macroscopic response to uniaxial tension shows differences between homogenized and standard materials, with the homogenized material exhibiting higher peak stress and steeper softening. However, randomizing elastic parameters and adjusting inelastic parameters can bring the macroscopic response closer to the standard material, despite differing damage distributions. This research sheds light on the potential for controlled random assignment of heterogeneity in homogeneous models and provides valuable insights into the behavior of these materials. <br /><br /> <div>
arXiv:2504.20861v1 Announce Type: new 
Abstract: The study investigates the elastic and fracture behaviors of discrete, elastically homogeneous models of heterogeneous media. The homogeneity is accomplished either by volumetric-deviatoric decomposition of constitutive function or by an auxiliary stress homogenization method. The elastic parameters of the homogenized material models are randomly varied in space to introduce heterogeneity independently of the geometric properties of the discrete model. Several forms of randomization are investigated using statistical properties of nodal stress oscillations in periodic representative volume elements (RVEs). It is found that the stress oscillations present in discrete models built on heterogeneous geometric structures with standard constitutive models cannot be replicated by randomization of the elastically homogeneous discrete system. The marginal distributions as well as dependencies between stress tensor components cannot be adequately matched.
  With respect to quasi-brittle fracture behavior, the macroscopic response of the different models is studied for the load case of uniaxial tension. The elastically homogenized material provides higher peak stress occurring at lower strain levels and a steeper softening phase, compared to the standard material. Randomization of the elastic material parameters, as well as adjustment of inelastic material parameters, brings the macroscopic response of the homogenized material close to that of the standard material, although the damage distribution prior to the strain localization differs. These findings provide insight into the potential for controlled, random assignment of heterogeneity in homogeneous models, using physically-based discretizations of material structure with standard constitutive models for comparison.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale modelling of thermally stressed superelastic polyimide</title>
<link>https://arxiv.org/abs/2504.20123</link>
<guid>https://arxiv.org/abs/2504.20123</guid>
<content:encoded><![CDATA[
<div> Keywords: thermo-mechanical processes, superelastic polyimide, multiscale approach, smoothed particle hydrodynamics, molecular dynamics

Summary: 
Thermo-mechanical processes at the atomistic scale in superelastic polyimide are studied using a sequential multiscale approach. The continuum-scale smoothed particle hydrodynamics (SPH) model is coupled with atomistic molecular dynamics (MD) to investigate thermal expansion and stress relaxation. Constitutive modelling integrates thermo-mechanical properties derived from MD simulations. Benchmark tests on heat transfer validate the results. Simulation of the insulation capabilities of superelastic polyimide on an aluminium plate shows significant reduction in thermal stress, strain, and temperature development. The multiscale method effectively captures thermo-mechanical interactions in superelastic polyimide, demonstrating its potential for exploring material behavior under thermal stress. <div>
arXiv:2504.20123v1 Announce Type: cross 
Abstract: Many thermo-mechanical processes, such as thermal expansion and stress relaxation, originate at the atomistic scale. We develop a sequential multiscale approach to study thermally stressed superelastic polyimide to explore these effects. The continuum-scale smoothed particle hydrodynamics (SPH) model is coupled with atomistic molecular dynamics (MD) through constitutive modelling, where thermo-mechanical properties and equations of state are derived from MD simulations. The results are verified through benchmark problems of heat transfer. Finally, we analyse the insulating capabilities of superelastic polyimide by simulating the thermal response of an aluminium plate. The result shows a considerable reduction in the thermal stress, strain and temperature field development in the aluminium plate when superelastic polyimide is used as an insulator. The present work demonstrates the effectiveness of the multi-scale method in capturing thermo-mechanical interactions in superelastic polyimide.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling</title>
<link>https://arxiv.org/abs/2504.20586</link>
<guid>https://arxiv.org/abs/2504.20586</guid>
<content:encoded><![CDATA[
<div> Monte Carlo method, capacitance extraction, variance reduction, layout-dependent effects, floating random walk <br />
<br />
Summary: 
The article introduces a new variance reduction method for floating random walk-based capacitance extraction, a method commonly used in integrated circuit analysis. This new method, designed to address challenges in ever-denser process technologies and layout-dependent effects, offers a significant improvement in extraction efficiency. It complements existing mathematical formulations for variance reduction and has been shown to reduce variance in all extractions, particularly those affected by layout-dependent effects. Numerical experiments demonstrate that the new method can decrease the number of walks required by up to 30% and reduce overall extraction times even further compared to previously proposed variance reduction techniques for floating random walks. This advancement in capacitance extraction techniques could have a significant impact on the efficiency and accuracy of integrated circuit analysis for complex designs. <br /> <div>
arXiv:2504.20586v1 Announce Type: cross 
Abstract: Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 30% in number of walks necessary and even more in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open FinLLM Leaderboard: Towards Financial AI Readiness</title>
<link>https://arxiv.org/abs/2501.10963</link>
<guid>https://arxiv.org/abs/2501.10963</guid>
<content:encoded><![CDATA[
<div> Keywords: FinLLMs, multimodal capabilities, open leaderboard, financial tasks, AI models performance <br />
Summary: Financial large language models (FinLLMs), equipped with multimodal capabilities, are poised to transform various applications in business, finance, accounting, and auditing. To drive real-world adoption, robust benchmarks for assessing FinLLMs' and FinAgents' performance are essential. An open FinLLM leaderboard, established in collaboration with Linux Foundation and Hugging Face, offers a platform for evaluating and comparing AI models across a wide range of financial tasks. By democratizing access to financial knowledge and intelligence, these advancements can empower chatbots or agents to elevate individuals' analytical proficiency to a professional level in a matter of months. The open leaderboard is inclusive of contributions from academia, the open-source community, industry, and stakeholders, with a focus on continually updating with new datasets, tasks, and models. By fostering a collaborative and transparent ecosystem, the aim is to advance readiness for financial AI solutions. <br /><br />Summary: <div>
arXiv:2501.10963v2 Announce Type: replace 
Abstract: Financial large language models (FinLLMs) with multimodal capabilities are envisioned to revolutionize applications across business, finance, accounting, and auditing. However, real-world adoption requires robust benchmarks of FinLLMs' and FinAgents' performance. Maintaining an open leaderboard is crucial for encouraging innovative adoption and improving model effectiveness. In collaboration with Linux Foundation and Hugging Face, we create an open FinLLM leaderboard, which serves as an open platform for assessing and comparing AI models' performance on a wide spectrum of financial tasks. By demoncratizing access to advances of financial knowledge and intelligence, a chatbot or agent may enhance the analytical capabilities of the general public to a professional level within a few months of usage. This open leaderboard welcomes contributions from academia, open-source community, industry, and stakeholders. In particular, we encourage contributions of new datasets, tasks, and models for continual update. Through fostering a collaborative and open ecosystem, we seek to promote financial AI readiness.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression</title>
<link>https://arxiv.org/abs/2504.18925</link>
<guid>https://arxiv.org/abs/2504.18925</guid>
<content:encoded><![CDATA[
<div> Neural Voxel Contextual Coding, Vector Quantization Contextual Coding, 4DGS data compression, multi-rate compression, storage reduction<br />
Summary:<br />
This study introduces 4DGS-CC, a contextual coding framework for compressing 4D Gaussian Splatting (4DGS) data to address storage challenges. The framework decomposes the 4DGS data into 4D neural voxels and 3DGS components for efficient compression. It leverages Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC) to compress the data, achieving a storage reduction of approximately 12 times while maintaining rendering fidelity. The approach separates temporal and spatial dimensions in the data decomposition process and utilizes prior information for contextual coding using NVCC. Additionally, a codebook is employed to store spherical harmonics information from canonical 3DGS, which is compressed using VQCC with auxiliary hyperpriors. The integrated NVCC and VQCC enable tailored multi-rate compression of 4DGS data, making it suitable for specific storage requirements. Extensive experiments validate the effectiveness of the proposed method in achieving significant storage savings without compromising data quality. <br /><br /> <div>
arXiv:2504.18925v1 Announce Type: new 
Abstract: Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints.Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively.Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding.Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook.By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error</title>
<link>https://arxiv.org/abs/2504.19963</link>
<guid>https://arxiv.org/abs/2504.19963</guid>
<content:encoded><![CDATA[
<div> probabilistic model, subspaces, principal component analysis, model reduction, computational mechanics
<br />
Summary:<br />
This paper introduces a probabilistic model of subspaces based on probabilistic principal component analysis (PCA). The method uses quantities derived from probabilistic PCA to construct distributions of the sample matrix and the principal subspaces in an embedding space. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition. The constructed stochastic subspace can help characterize model-form uncertainty in computational mechanics. The method is justified by probabilistic PCA, satisfying linear constraints like boundary conditions, and has only one hyperparameter, simplifying training. The algorithm is easy to implement. Comparisons with existing approaches show promising results in low-dimensional visualization, parametric static problems, and dynamics modeling of space structures. <div>
arXiv:2504.19963v1 Announce Type: new 
Abstract: This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We compare the proposed method with existing approaches in a low-dimensional visualization example and a parametric static problem, and demonstrate its performance in a dynamics model of a space structure.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantBench: Benchmarking AI Methods for Quantitative Investment</title>
<link>https://arxiv.org/abs/2504.18600</link>
<guid>https://arxiv.org/abs/2504.18600</guid>
<content:encoded><![CDATA[
<div> benchmark, artificial intelligence, quantitative investment, industry practices, QuantBench  
Summary:  
QuantBench is introduced as a benchmark platform for AI in quantitative investment, aiming to bridge the gap between academic research and industry practices. It offers standardization aligned with industry standards, flexibility for integrating AI algorithms, and full coverage of the quantitative investment process. Using QuantBench, empirical studies highlight the importance of continual learning to address distribution shifts, improved methods for modeling relational financial data, and robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common platform for evaluation, QuantBench fosters collaboration between researchers and practitioners, accelerating progress in the field of AI for quantitative investment, similar to the impact of benchmark platforms in other domains such as computer vision and natural language processing. <div>
arXiv:2504.18600v1 Announce Type: cross 
Abstract: The field of artificial intelligence (AI) in quantitative investment has seen significant advancements, yet it lacks a standardized benchmark aligned with industry practices. This gap hinders research progress and limits the practical application of academic innovations. We present QuantBench, an industrial-grade benchmark platform designed to address this critical need. QuantBench offers three key strengths: (1) standardization that aligns with quantitative investment industry practices, (2) flexibility to integrate various AI algorithms, and (3) full-pipeline coverage of the entire quantitative investment process. Our empirical studies using QuantBench reveal some critical research directions, including the need for continual learning to address distribution shifts, improved methods for modeling relational financial data, and more robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common ground for evaluation and fostering collaboration between researchers and practitioners, QuantBench aims to accelerate progress in AI for quantitative investment, similar to the impact of benchmark platforms in computer vision and natural language processing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Open-Source Software Is Less Likely to Become Abandoned Than One Might Think! Lessons from Curating a Catalog of Maintained Scientific Software</title>
<link>https://arxiv.org/abs/2504.18971</link>
<guid>https://arxiv.org/abs/2504.18971</guid>
<content:encoded><![CDATA[
<div> classification, scientific software, longevity, survival models, open-source

Summary:
Using large language models, the study classifies over 18,000 scientific software projects, analyzing their attributes to understand factors affecting longevity. Infrastructural layers, downstream dependencies, publication mentions, and government participants are linked to longer lifespans, while newer projects with academic participants have shorter lifespans. Despite common perceptions, scientific projects have a longer lifespan than non-scientific open-source projects. The curated dataset provides a valuable resource for future research on scientific software, offering insights that could help prolong the lifespan of both scientific and non-scientific software projects. <div>
arXiv:2504.18971v1 Announce Type: cross 
Abstract: Scientific software is essential to scientific innovation and in many ways it is distinct from other types of software. Abandoned (or unmaintained), buggy, and hard to use software, a perception often associated with scientific software can hinder scientific progress, yet, in contrast to other types of software, its longevity is poorly understood. Existing data curation efforts are fragmented by science domain and/or are small in scale and lack key attributes. We use large language models to classify public software repositories in World of Code into distinct scientific domains and layers of the software stack, curating a large and diverse collection of over 18,000 scientific software projects. Using this data, we estimate survival models to understand how the domain, infrastructural layer, and other attributes of scientific software affect its longevity. We further obtain a matched sample of non-scientific software repositories and investigate the differences. We find that infrastructural layers, downstream dependencies, mentions of publications, and participants from government are associated with a longer lifespan, while newer projects with participants from academia had shorter lifespan. Against common expectations, scientific projects have a longer lifetime than matched non-scientific open-source software projects. We expect our curated attribute-rich collection to support future research on scientific software and provide insights that may help extend longevity of both scientific and other projects.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spreading of highly cohesive metal powders with transverse oscillation kinematics</title>
<link>https://arxiv.org/abs/2504.18981</link>
<guid>https://arxiv.org/abs/2504.18981</guid>
<content:encoded><![CDATA[
<div> powder bed additive manufacturing, laser powder bed fusion, binder jetting, transverse oscillation kinematic, dense powder layers<br />
<br />
Summary: 
The study examines the challenges of spreading fine powders in powder bed additive manufacturing processes and proposes a transverse oscillation kinematic for powder spreading. Computational simulations using a DEM-FEM framework show that transverse oscillation of a non-rotating roller can facilitate the spreading of dense powder layers with high packing fractions. Experimental validation confirms the computational results, with high packing fractions achieved for transverse oscillation frequencies above 200 Hz. Statistical analysis demonstrates that increasing transverse surface velocity improves layer uniformity and reduces cracking defects. The proposed transverse oscillation kinematic has the potential to produce thin and consistently uniform powder layers in additive manufacturing processes, offering a promising solution for handling highly cohesive powders. <br /><br /> <div>
arXiv:2504.18981v1 Announce Type: cross 
Abstract: Powder bed additive manufacturing processes such as laser powder bed fusion (LPBF) or binder jetting (BJ) benefit from using fine (D50 $\leq20~\mu m$) powders. However, the increasing level of cohesion with decreasing particle size makes spreading a uniform and continuous layer challenging. As a result, LPBF typically employs a coarser size distribution, and rotating roller mechanisms are used in BJ machines, that can create wave-like surface profiles due to roller run-out.
  In this work, a transverse oscillation kinematic for powder spreading is proposed, explored computationally, and validated experimentally. Simulations are performed using an integrated discrete element-finite element (DEM-FEM) framework and predict that transverse oscillation of a non-rotating roller facilitates the spreading of dense powder layers (beyond 50% packing fraction) with a high level of robustness to kinematic parameters. The experimental study utilizes a custom-built mechanized powder spreading testbed and X-ray transmission imaging for the analysis of spread powder layers. Experimental results generally validate the computational results, however, also exhibit parasitic layer cracking. For transverse oscillation frequencies above 200 Hz, powder layers of high packing fraction (between 50-60%) were formed, and for increased layer thicknesses, highly uniform and continuous layers were deposited. Statistical analysis of the experimental powder layer morphology as a function of kinematic spreading parameters revealed that an increasing transverse surface velocity improves layer uniformity and reduces cracking defects. This suggests that with minor improvements to the machine design, the proposed transverse oscillation kinematic has the potential to result in thin and consistently uniform powder layers of highly cohesive powder.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Bayesian Optimal Experimental Design with Normalizing Flows</title>
<link>https://arxiv.org/abs/2404.13056</link>
<guid>https://arxiv.org/abs/2404.13056</guid>
<content:encoded><![CDATA[
<div> Bayesian optimal experimental design, variational OED, normalizing flows, Monte Carlo estimators, gradient-based optimization<br />
<br />
Summary: The study introduces a novel approach, vOED-NFs, which utilizes normalizing flows (NFs) to enhance variational optimal experimental design (vOED) by approximating posterior distributions. The method employs NFs with a conditional invertible neural network architecture and includes a summary network for data dimension reduction. Monte Carlo estimators and gradient expressions enable simultaneous optimization of variational parameters and design variables. The algorithm is validated on benchmark problems and applied to scenarios involving cathodic electrophoretic deposition and stochastic modeling of aphid population. Results demonstrate that a composition of 4-5 coupling layers reduces EIG estimation bias, while NFs provide accurate approximations of posterior distributions, effectively capturing non-Gaussian and multi-modal features. The vOED-NFs approach offers a computationally efficient and accurate method for Bayesian optimal experimental design without the need for explicit likelihood evaluations. <br /><br /> <div>
arXiv:2404.13056v2 Announce Type: replace-cross 
Abstract: Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Modeling of Lipid Nanoparticle Formation for the Delivery of Nucleic Acid Therapeutics</title>
<link>https://arxiv.org/abs/2408.08577</link>
<guid>https://arxiv.org/abs/2408.08577</guid>
<content:encoded><![CDATA[
<div> Keywords: nucleic acids, lipid nanoparticles, mechanistic modeling, process development, process control<br />
Summary: 
Nucleic acids, including mRNA, are a promising therapeutic modality for treating various diseases. Lipid nanoparticles (LNPs) have been used as a delivery system for nucleic acids in COVID-19 vaccines. However, understanding the formation and structure of LNPs is challenging, especially during scale-up of manufacturing processes. Mathematical and computational methods offer a way to improve understanding of LNP formation and aid in process development and control. This article discusses strategies for mechanistic modeling of LNP formation, starting with predicting important physicochemical properties of the species involved. It outlines a framework for constructing models of reactor- and particle-scale processes, linking insights from the models to product quality attributes and process understanding. Finally, the article explores using these models to guide advanced process control and optimization strategies.<br /><br />Summary: <div>
arXiv:2408.08577v2 Announce Type: replace-cross 
Abstract: Nucleic acids such as mRNA have emerged as a promising therapeutic modality with the capability of addressing a wide range of diseases. Lipid nanoparticles (LNPs) as a delivery platform for nucleic acids were used in the COVID-19 vaccines and have received much attention. While modern manufacturing processes which involve rapidly mixing an organic stream containing the lipids with an aqueous stream containing the nucleic acids are conceptually straightforward, detailed understanding of LNP formation and structure is still limited and scale-up can be challenging. Mathematical and computational methods are a promising avenue for deepening scientific understanding of the LNP formation process and facilitating improved process development and control. This article describes strategies for the mechanistic modeling of LNP formation, starting with strategies to estimate and predict important physicochemical properties of the various species such as diffusivities and solubilities. Subsequently, a framework is outlined for constructing mechanistic models of reactor- and particle-scale processes. Insights gained from the various models are mapped back to product quality attributes and process insights. Lastly, the use of the models to guide development of advanced process control and optimization strategies is discussed.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatternPaint: Practical Layout Pattern Generation Using Diffusion-Based Inpainting</title>
<link>https://arxiv.org/abs/2409.01348</link>
<guid>https://arxiv.org/abs/2409.01348</guid>
<content:encoded><![CDATA[
<div> diffusion-based framework, VLSI layout patterns, design for manufacturing, few-shot finetuning, technology nodes 
Summary:
PatternPaint is a diffusion-based framework designed for generating diverse VLSI layout patterns essential for design for manufacturing. It addresses the challenge of limited design-rule-compliant training samples and simplifies complex layout pattern generation through inpainting processes. The framework incorporates a template-based denoising scheme and utilizes few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results demonstrate the effectiveness of PatternPaint in generating legal patterns in complex 2D metal interconnect design rule settings for sub-3nm technology nodes. It achieves high diversity scores and improves legality rates significantly through few-shot finetuning. This approach offers a production-ready solution for layout pattern generation in the development of new technology nodes. 
<br /><br />Summary: <div>
arXiv:2409.01348v4 Announce Type: replace-cross 
Abstract: Generating diverse VLSI layout patterns is essential for various downstream tasks in design for manufacturing, as design rules continually evolve during the development of new technology nodes. However, existing training-based methods for layout pattern generation rely on large datasets. In practical scenarios, especially when developing a new technology node, obtaining such extensive layout data is challenging. Consequently, training models with large datasets becomes impractical, limiting the scalability and adaptability of prior approaches. To this end, we propose PatternPaint, a diffusion-based framework capable of generating legal patterns with limited design-rule-compliant training samples. PatternPaint simplifies complex layout pattern generation into a series of inpainting processes with a template-based denoising scheme. Furthermore, we perform few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results show that using a sub-3nm technology node (Intel 18A), our model is the only one that can generate legal patterns in complex 2D metal interconnect design rule settings among all previous works and achieves a high diversity score. Additionally, our few-shot finetuning can boost the legality rate with 1.87X improvement compared to the original pretrained model. As a result, we demonstrate a production-ready approach for layout pattern generation in developing new technology nodes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets</title>
<link>https://arxiv.org/abs/2410.13105</link>
<guid>https://arxiv.org/abs/2410.13105</guid>
<content:encoded><![CDATA[
<div> Decentralized Finance, DeFi, lending, algorithm-driven, liquidity pools <br />
<br />
Summary: 
This work proposes a dynamic model for the DeFi lending market, incorporating evolving demand and supply curves and an adaptive interest rate controller to respond to market changes in real-time. The Recursive Least Squares algorithm is used to track external market conditions, ensuring stable utilization and managing default and liquidation risks. The algorithm provides theoretical guarantees on interest rate convergence and utilization stability while also addressing vulnerability to adversarial manipulation. Two approaches are proposed to mitigate manipulation: detecting extreme fluctuations and enhancing elasticity through interest rate derivative markets. The dynamic model shows low error rates on real data and the interest rate controller outperforms static curve protocols in optimizing utilization and reducing liquidations. <div>
arXiv:2410.13105v4 Announce Type: replace-cross 
Abstract: Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller tracks the external market and achieves stable utilization, while also controlling default and liquidation risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. We propose two complementary approaches to mitigating adversarial manipulation: an algorithmic method that detects extreme demand and supply fluctuations and a market-based strategy that enhances elasticity, potentially via interest rate derivative markets. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Evaluation of Variational Quantum Eigensolver and Quantum Dynamics Algorithms on the Advection-Diffusion Equation</title>
<link>https://arxiv.org/abs/2503.24045</link>
<guid>https://arxiv.org/abs/2503.24045</guid>
<content:encoded><![CDATA[
<div> Variational Quantum Eigensolver, linear advection-diffusion equation, quantum algorithms, partial differential equations, noiseless simulation<br />
<br />
Summary: 
The study investigates the performance of quantum algorithms in solving a linear one-dimensional advection-diffusion equation. It compares Variational Quantum Eigensolver (VQE) with Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS) on small quantum hardware. While VQE on a noiseless simulator achieves high accuracy with low infidelities, the dynamics algorithms suffer from errors due to noise and limited shot statistics on hardware. VQE outperforms the dynamics methods in terms of accuracy as it reaches low infidelities with moderate circuit depths. The comparison provides insights into the accuracy and resource demands of different algorithms for solving partial differential equations. The study concludes with a discussion on potential extensions to higher-dimensional and nonlinear PDEs relevant to engineering and finance. <br /><br /> <div>
arXiv:2503.24045v2 Announce Type: replace-cross 
Abstract: We investigate the potential of near-term quantum algorithms for solving partial differential equations (PDEs), focusing on a linear one-dimensional advection-diffusion equation as a test case. This study benchmarks a ground-state algorithm, Variational Quantum Eigensolver (VQE), against three leading quantum dynamics algorithms, Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS), applied to the same PDE on small quantum hardware. While Trotterization is fully quantum, VarQTE and AVQDS are variational algorithms that reduce circuit depth for noisy intermediate-scale quantum (NISQ) devices. However, hardware results from these dynamics methods show sizable errors due to noise and limited shot statistics. To establish a noise-free performance baseline, we implement the VQE-based solver on a noiseless statevector simulator. Our results show VQE can reach final-time infidelities as low as ${O}(10^{-9})$ with $N=4$ qubits and moderate circuit depths, outperforming hardware-deployed dynamics methods that show infidelities $\gtrsim 10^{-1}$. By comparing noiseless VQE to shot-based and hardware-run algorithms, we assess their accuracy and resource demands, providing a baseline for future quantum PDE solvers. We conclude with a discussion of limitations and potential extensions to higher-dimensional, nonlinear PDEs relevant to engineering and finance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTFinRAG: Interactive Modularized Financial RAG Benchmark</title>
<link>https://arxiv.org/abs/2504.18024</link>
<guid>https://arxiv.org/abs/2504.18024</guid>
<content:encoded><![CDATA[
<div> Keywords: financial sectors, language model technologies, SMARTFinRAG, evaluation paradigm, open-source architecture<br />
Summary: SMARTFinRAG is a new platform designed to address gaps in assessing specialized RAG systems in the financial sector. It introduces a modular architecture that allows components to be interchanged during runtime, a document-centric evaluation paradigm that creates domain-specific QA pairs from financial documents, and an intuitive interface to facilitate research-implementation integration. The evaluation of SMARTFinRAG shows variations in retrieval efficacy and response quality across different configurations. The platform's open-source architecture promotes transparent and reproducible research, while also helping financial institutions overcome deployment challenges when implementing RAG systems. This innovative platform aims to enhance the adoption and evaluation of language model technologies in the financial industry. <br /><br /> <div>
arXiv:2504.18024v1 Announce Type: new 
Abstract: Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Governing Equations of Geomagnetic Storm Dynamics with Symbolic Regression</title>
<link>https://arxiv.org/abs/2504.18461</link>
<guid>https://arxiv.org/abs/2504.18461</guid>
<content:encoded><![CDATA[
<div> solar wind, geomagnetic storms, Disturbance Storm Time index, symbolic regression, PySR framework

Summary:
The study focuses on using symbolic regression to create data-driven equations that describe the behavior of the Disturbance Storm Time (Dst) index during geomagnetic storms. By analyzing historical data from the NASA OMNIweb database, including various solar wind parameters, the study aims to develop models that accurately predict the evolution of the Dst index. The models generated by the PySR framework showcase a hierarchy of complexity levels and outperform traditional empirical models in terms of accuracy and interpretability. The evaluation of the models on historical storm events, such as the 2003 Halloween Storm and the 2015 St. Patrick's Day Storm, demonstrates their effectiveness in capturing nonlinear dependencies and thresholding effects in Dst evolution. Overall, the study provides valuable insights into the mechanisms driving geomagnetic storms and offers interpretable mathematical expressions for predicting their intensity. 

<br /><br />Summary: <div>
arXiv:2504.18461v1 Announce Type: new 
Abstract: Geomagnetic storms are large-scale disturbances of the Earth's magnetosphere driven by solar wind interactions, posing significant risks to space-based and ground-based infrastructure. The Disturbance Storm Time (Dst) index quantifies geomagnetic storm intensity by measuring global magnetic field variations. This study applies symbolic regression to derive data-driven equations describing the temporal evolution of the Dst index. We use historical data from the NASA OMNIweb database, including solar wind density, bulk velocity, convective electric field, dynamic pressure, and magnetic pressure. The PySR framework, an evolutionary algorithm-based symbolic regression library, is used to identify mathematical expressions linking dDst/dt to key solar wind. The resulting models include a hierarchy of complexity levels and enable a comparison with well-established empirical models such as the Burton-McPherron-Russell and O'Brien-McPherron models. The best-performing symbolic regression models demonstrate superior accuracy in most cases, particularly during moderate geomagnetic storms, while maintaining physical interpretability. Performance evaluation on historical storm events includes the 2003 Halloween Storm, the 2015 St. Patrick's Day Storm, and a 2017 moderate storm. The results provide interpretable, closed-form expressions that capture nonlinear dependencies and thresholding effects in Dst evolution.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAQGA: A Quantum-Enhanced Genetic Algorithm with Novel Entanglement-Aware Crossovers</title>
<link>https://arxiv.org/abs/2504.17923</link>
<guid>https://arxiv.org/abs/2504.17923</guid>
<content:encoded><![CDATA[
<div> genetic algorithms, combinatorial optimization, portfolio optimization, quantum computing, quantum circuits

Summary:
- Genetic algorithms are effective in complex optimization problems, such as portfolio optimization.
- Quantum computing can address challenging tasks, and quantum genetic algorithms combine the benefits of both approaches.
- The proposed quantum genetic algorithm introduces a novel crossover strategy generating quantum circuits from binary solutions.
- It encodes entanglement patterns from parent solutions to enhance performance without significantly increasing circuit depth.
- Testing on a portfolio optimization problem using IBM's 127 qubits Eagle processor and simulators shows a significant improvement in fitness values compared to classical and quantum-inspired genetic algorithms, highlighting the potential of quantum computers in solving real-world combinatorial optimization problems.<br /><br /> <div>
arXiv:2504.17923v1 Announce Type: cross 
Abstract: Genetic algorithms are highly effective optimization techniques for many computationally challenging problems, including combinatorial optimization tasks like portfolio optimization. Quantum computing has also shown potential in addressing these complex challenges. Combining these approaches, quantum genetic algorithms leverage the principles of superposition and entanglement to enhance the performance of classical genetic algorithms. In this work, we propose a novel quantum genetic algorithm introducing an innovative crossover strategy to generate quantum circuits from a binary solution. We incorporate a heuristic method to encode entanglement patterns from parent solutions into circuits for the next generation. Our algorithm advances quantum genetic algorithms by utilizing a limited number of entanglements, enabling efficient exploration of optimal solutions without significantly increasing circuit depth, making it suitable for near-term applications. We test this approach on a portfolio optimization problem using an IBM 127 qubits Eagle processor (ibm_quebec) and simulators. Compared to state-of-the-art algorithms, our results show that the proposed method improves fitness values by 33.6% over classical genetic algorithm and 37.2% over quantum-inspired genetic algorithm, using the same iteration counts and population sizes with real quantum hardware employing 100 qubits. These findings highlight the potential of current quantum computers to address real-world utility-scale combinatorial optimization problems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach For Bitcoin Forecasting</title>
<link>https://arxiv.org/abs/2504.18206</link>
<guid>https://arxiv.org/abs/2504.18206</guid>
<content:encoded><![CDATA[
<div> Keywords: Bitcoin, cryptocurrency, time series, machine learning, directional accuracy

Summary:
In this study, the researchers focus on Bitcoin, a popular cryptocurrency, and investigate the use of different time series data along with machine learning algorithms to forecast its price movements. The analysis reveals that incorporating the Open, High, and Low prices of Bitcoin significantly improves directional accuracy. The Low price, in particular, plays a crucial role in enhancing the forecast accuracy when used in combination with a Gated Recurrent Unit network and a baseline forecast. The study also finds that other Bitcoin-related features, apart from price data, have minimal impact on prediction accuracy. Overall, the proposed method displays comparable performance to existing approaches in terms of directional accuracy, emphasizing the importance of considering specific time series data and machine learning techniques for forecasting cryptocurrency prices.<br /><br />Summary: <div>
arXiv:2504.18206v1 Announce Type: cross 
Abstract: Bitcoin is one of the cryptocurrencies that is gaining more popularity in recent years. Previous studies have shown that closing price alone is not enough to forecast stock market series. We introduce a new set of time series and demonstrate that a subset is necessary to improve directional accuracy based on a machine learning ensemble. In our experiments, we study which time series and machine learning algorithms deliver the best results. We found that the most relevant time series that contribute to improving directional accuracy are Open, High and Low, with the largest contribution of Low in combination with an ensemble of Gated Recurrent Unit network and a baseline forecast. The relevance of other Bitcoin-related features that are not price-related is negligible. The proposed method delivers similar performance to the state-of-the-art when observing directional accuracy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid modelling of reactive transport in porous media using machine learning: limitations and solutions</title>
<link>https://arxiv.org/abs/2405.14548</link>
<guid>https://arxiv.org/abs/2405.14548</guid>
<content:encoded><![CDATA[
<div> machine learning, reactive transport, porous media, geochemical reactions, cation exchange problem

Summary:
Machine learning models are explored as replacements for a geochemical module in simulating reactive transport in porous media. Testing on a cation exchange problem reveals that while the surrogate models perform well in isolated predictions, they struggle with rollout predictions over successive time steps. By incorporating physics-based constraints and tailored dataset generation strategies, accurate rollout predictions are achieved. The study highlights the limitation of machine learning surrogates in predicting over multiple time steps, even for a simple sorption equilibrium reaction like the cation exchange problem. However, with the addition of physics-based modifications, these limitations can be overcome. The research provides a detailed analysis of these limitations and potential mitigation strategies.<br /><br /> <div>
arXiv:2405.14548v2 Announce Type: replace 
Abstract: Reactive transport in porous media plays a pivotal role in subsurface reservoir processes, influencing fluid properties and geochemical characteristics. However, coupling fluid flow and transport with geochemical reactions is computationally intensive, requiring geochemical calculations at each grid cell and each time step within a discretized simulation domain. Although recent advancements have integrated machine learning techniques as surrogates for geochemical simulations, ensuring computational efficiency and accuracy remains a challenge. This work investigates machine learning models as replacements for a geochemical module in a simulation of reactive transport in porous media. As a proof of concept, we test this approach on a well-documented cation exchange problem. While the surrogate models excel in isolated predictions, they fall short in rollout predictions over successive time steps. By introducing modifications, including physics-based constraints and tailored dataset generation strategies, we show that machine learning surrogates can achieve accurate rollout predictions. Our findings emphasize that even for a simple sorption equilibrium reaction (cation exchange problem), machine learning surrogates alone fail in predicting over successive time-steps. Incorporating simple physics-based modifications enables us to overcome this limitation. A detailed analysis of the limitations and potential mitigation strategies is presented in this work.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Function-coherent gambles</title>
<link>https://arxiv.org/abs/2503.01855</link>
<guid>https://arxiv.org/abs/2503.01855</guid>
<content:encoded><![CDATA[
<div> function-coherent gambles, non-linear utility, intertemporal choice, discounting, desirability framework
Summary:
The paper introduces function-coherent gambles, a generalization of the desirable gambles framework that allows for non-linear utility functions. Core axioms for function-coherence are established, and a representation theorem is proven, linking acceptable gambles to continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, such as hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. The integration of these alternative discounting models within the function-coherent framework provides a unified treatment for modeling complex patterns of time preference. This approach bridges the gap between normative theory and real-world behavior in intertemporal decision-making under uncertainty, offering theoretical foundations for understanding sophisticated time preferences within the desirability paradigm. <br /><br />Summary: <div>
arXiv:2503.01855v2 Announce Type: replace-cross 
Abstract: The desirable gambles framework provides a foundational approach to imprecise probability theory but relies heavily on linear utility assumptions. This paper introduces function-coherent gambles, a generalization that accommodates non-linear utility while preserving essential rationality properties. We establish core axioms for function-coherence and prove a representation theorem that characterizes acceptable gambles through continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, including hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. We demonstrate how these alternatives to constant-rate exponential discounting can be integrated within the function-coherent framework. This unified treatment provides theoretical foundations for modeling sophisticated patterns of time preference within the desirability paradigm, bridging a gap between normative theory and observed behavior in intertemporal decision-making under genuine uncertainty.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Entropy Stable Formulation of Two-equation Turbulence Models with Particular Reference to the k-epsilon Model</title>
<link>https://arxiv.org/abs/2504.17110</link>
<guid>https://arxiv.org/abs/2504.17110</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical algorithms, partial differential equations, entropy production inequality, turbulence models, k-epsilon model

Summary:
This article discusses the importance of incorporating nonlinear physical stability principles, such as the entropy production inequality, in the design of numerical algorithms for partial differential equations. By introducing space-time averaging and defining entropy variables, a symmetric system of advective-diffusive equations can be derived for turbulence models, including the k-epsilon model. Positivity and symmetry constraints are necessary for the turbulence diffusivity coefficients and source terms to ensure the design of entropy producing two-equation turbulence models. This approach emphasizes the use of physical principles over artificial viscosity for designing robust algorithms. <div>
arXiv:2504.17110v1 Announce Type: new 
Abstract: Consistency and stability are two essential ingredients in the design of numerical algorithms for partial differential equations. Robust algorithms can be developed by incorporating nonlinear physical stability principles in their design, such as the entropy production inequality (i.e., the Clausius-Duhem inequality or second law of thermodynamics), rather than by simply adding artificial viscosity (a common approach). This idea is applied to the k-epsilon and two-equation turbulence models by introducing space-time averaging. Then, a set of entropy variables can be defined which leads to a symmetric system of advective-diffusive equations. Positivity and symmetry of the equations require certain constraints on the turbulence diffusivity coefficients and the turbulence source terms. With these, we are able to design entropy producing two-equation turbulence models and, in particular, the k-epsilon model.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Stock Prices for Enhanced Multi-Step Forecast and Prediction</title>
<link>https://arxiv.org/abs/2504.17313</link>
<guid>https://arxiv.org/abs/2504.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: stock price forecasting, stock price prediction, PCIE model, tokenization method, multi-step prediction<br />
Summary: <br />
Effective stock price forecasting and prediction are essential for investors and policymakers but are challenging due to the dynamic nature of stock price data. Forecasting and prediction targets have distinct statistical characteristics and multi-step approaches provide richer information but are more difficult. The Patched Channel Integration Encoder (PCIE) model is introduced to address these challenges by utilizing multiple stock channels and a novel tokenization method. The tokenization process involves univariate patching and temporal learning to reduce cumulative errors. Experimental results demonstrate that PCIE outperforms current state-of-the-art models in both forecast and prediction tasks. <div>
arXiv:2504.17313v1 Announce Type: new 
Abstract: Effective stock price forecasting (estimating future prices) and prediction (estimating future price changes) are pivotal for investors, regulatory agencies, and policymakers. These tasks enable informed decision-making, risk management, strategic planning, and superior portfolio returns. Despite their importance, forecasting and prediction are challenging due to the dynamic nature of stock price data, which exhibit significant temporal variations in distribution and statistical properties. Additionally, while both forecasting and prediction targets are derived from the same dataset, their statistical characteristics differ significantly. Forecasting targets typically follow a log-normal distribution, characterized by significant shifts in mean and variance over time, whereas prediction targets adhere to a normal distribution. Furthermore, although multi-step forecasting and prediction offer a broader perspective and richer information compared to single-step approaches, it is much more challenging due to factors such as cumulative errors and long-term temporal variance. As a result, many previous works have tackled either single-step stock price forecasting or prediction instead. To address these issues, we introduce a novel model, termed Patched Channel Integration Encoder (PCIE), to tackle both stock price forecasting and prediction. In this model, we utilize multiple stock channels that cover both historical prices and price changes, and design a novel tokenization method to effectively embed these channels in a cross-channel and temporally efficient manner. Specifically, the tokenization process involves univariate patching and temporal learning with a channel-mixing encoder to reduce cumulative errors. Comprehensive experiments validate that PCIE outperforms current state-of-the-art models in forecast and prediction tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems</title>
<link>https://arxiv.org/abs/2504.17354</link>
<guid>https://arxiv.org/abs/2504.17354</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, effective contact area, rough surfaces, machine learning algorithms, multi-query contexts
<br />
Summary: 
This study presents a surrogate modeling framework for predicting the effective contact area in rough surface contact using fast-to-evaluate machine learning algorithms. The models are trained on a precomputed dataset of imposed load and roughness parameters to predict the effective contact area efficiently. The Kernel Ridge Regressor is identified as the best trade-off between accuracy and efficiency, making it suitable for general-purpose surrogate modeling. The Gaussian Process Regressor is also effective for uncertainty quantification tasks. The models' generalization capability is validated on unseen simulation scenarios, demonstrating their transferability to new configurations. While database generation is a significant cost in the process, the overall approach is practical and efficient for multi-query tasks, even after accounting for initial expenses. <br /><br />Summary: <div>
arXiv:2504.17354v1 Announce Type: new 
Abstract: The effective contact area in rough surface contact plays a critical role in multi-physics phenomena such as wear, sealing, and thermal or electrical conduction. Although accurate numerical methods, like the Boundary Element Method (BEM), are available to compute this quantity, their high computational cost limits their applicability in multi-query contexts, such as uncertainty quantification, parameter identification, and multi-scale algorithms, where many repeated evaluations are required. This study proposes a surrogate modeling framework for predicting the effective contact area using fast-to-evaluate data-driven techniques. Various machine learning algorithms are trained on a precomputed dataset, where the inputs are the imposed load and statistical roughness parameters, and the output is the corresponding effective contact area. All models undergo hyperparameter optimization to enable fair comparisons in terms of predictive accuracy and computational efficiency, evaluated using established quantitative metrics. Among the models, the Kernel Ridge Regressor demonstrates the best trade-off between accuracy and efficiency, achieving high predictive accuracy, low prediction time, and minimal training overhead-making it a strong candidate for general-purpose surrogate modeling. The Gaussian Process Regressor provides an attractive alternative when uncertainty quantification is required, although it incurs additional computational cost due to variance estimation. The generalization capability of the Kernel Ridge model is validated on an unseen simulation scenario, confirming its ability to transfer to new configurations. Database generation constitutes the dominant cost in the surrogate modeling process. Nevertheless, the approach proves practical and efficient for multi-query tasks, even when accounting for this initial expense.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>polyGen: A Learning Framework for Atomic-level Polymer Structure Generation</title>
<link>https://arxiv.org/abs/2504.17656</link>
<guid>https://arxiv.org/abs/2504.17656</guid>
<content:encoded><![CDATA[
<div> Latent diffusion model, polymer structures, generative algorithms, molecular encoding, structure matching criteria 

Summary: 

polyGen is introduced as a novel approach to generating realistic polymer structures using a latent diffusion model. Leveraging a molecular encoding that captures polymer connectivity, polyGen can generate diverse conformations of both linear chains and complex branched structures. The model shows improvement in joint learning between similar chemical structures through training augmentation with DFT-optimized molecular structures. However, its performance decreases when handling repeat units with a high atom count. polyGen represents a paradigm shift in atomic-level structure generation for polymer science, providing the first proof-of-concept for predicting realistic atomic-level polymer conformations while considering their intrinsic structural flexibility. <div>
arXiv:2504.17656v1 Announce Type: new 
Abstract: Synthetic polymeric materials underpin fundamental technologies in the energy, electronics, consumer goods, and medical sectors, yet their development still suffers from prolonged design timelines. Although polymer informatics tools have supported speedup, polymer simulation protocols continue to face significant challenges: on-demand generation of realistic 3D atomic structures that respect the conformational diversity of polymer structures. Generative algorithms for 3D structures of inorganic crystals, bio-polymers, and small molecules exist, but have not addressed synthetic polymers. In this work, we introduce polyGen, the first latent diffusion model designed specifically to generate realistic polymer structures from minimal inputs such as the repeat unit chemistry alone, leveraging a molecular encoding that captures polymer connectivity throughout the architecture. Due to a scarce dataset of only 3855 DFT-optimized polymer structures, we augment our training with DFT-optimized molecular structures, showing improvement in joint learning between similar chemical structures. We also establish structure matching criteria to benchmark our approach on this novel problem. polyGen effectively generates diverse conformations of both linear chains and complex branched structures, though its performance decreases when handling repeat units with a high atom count. Given these initial results, polyGen represents a paradigm shift in atomic-level structure generation for polymer science-the first proof-of-concept for predicting realistic atomic-level polymer conformations while accounting for their intrinsic structural flexibility.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy</title>
<link>https://arxiv.org/abs/2504.17124</link>
<guid>https://arxiv.org/abs/2504.17124</guid>
<content:encoded><![CDATA[
<div> X-ray absorption near edge structure (XANES) spectroscopy, adaptive sampling methods, Bayesian optimization, absorption edge, pre-edge peaks <br />
<br />Summary: <br />
X-ray absorption near edge structure (XANES) spectroscopy is a valuable technique for analyzing chemical states in materials, but traditional data collection methods can be time-consuming. This study introduces a knowledge-injected Bayesian optimization approach for adaptive XANES data collection, taking into account spectral features like absorption edges and pre-edge peaks. The method efficiently reconstructs absorption edges with high accuracy using only 15-20% of the measurement points needed in conventional sampling. It can determine the x-ray energy of sharp peaks with minimal errors, achieving overall root-mean-square errors below 0.005 compared to traditional methods. The experiments on battery materials and catalysts demonstrate its effectiveness for both static and dynamic XANES measurements, enhancing data collection efficiency and enabling better time resolution for tracking chemical changes. This automated approach reduces common errors in XANES experiments and facilitates dynamic studies requiring high temporal resolution. <div>
arXiv:2504.17124v1 Announce Type: cross 
Abstract: X-ray absorption near edge structure (XANES) spectroscopy is a powerful technique for characterizing the chemical state and symmetry of individual elements within materials, but requires collecting data at many energy points which can be time-consuming. While adaptive sampling methods exist for efficiently collecting spectroscopic data, they often lack domain-specific knowledge about XANES spectra structure. Here we demonstrate a knowledge-injected Bayesian optimization approach for adaptive XANES data collection that incorporates understanding of spectral features like absorption edges and pre-edge peaks. We show this method accurately reconstructs the absorption edge of XANES spectra using only 15-20% of the measurement points typically needed for conventional sampling, while maintaining the ability to determine the x-ray energy of the sharp peak after absorption edge with errors less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and overall root-mean-square errors less than 0.005 compared to compared to traditionally sampled spectra. Our experiments on battery materials and catalysts demonstrate the method's effectiveness for both static and dynamic XANES measurements, improving data collection efficiency and enabling better time resolution for tracking chemical changes. This approach advances the degree of automation in XANES experiments reducing the common errors of under- or over-sampling points in near the absorption edge and enabling dynamic experiments that require high temporal resolution or limited measurement time.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints</title>
<link>https://arxiv.org/abs/2504.17142</link>
<guid>https://arxiv.org/abs/2504.17142</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, microelectronic components, multiphysics constraints, design optimization, high-dimensional solution space
Summary:
This study focuses on using reinforcement learning techniques for designing microelectronic components under multiphysics constraints. Traditional global optimization approaches are limited when dealing with intricate solution spaces and constraints. The complexity of microelectronic component design, such as ASICs and HI interposers, poses challenges for conventional methods. The study explores optimizing interconnect geometry for ASIC chips and component placement on a HI interposer while meeting thermoelastic and design constraints. The placement problem involves a high-dimensional solution space, highlighting the need for advanced techniques like reinforcement learning. By developing and testing an RL-based framework, the study aims to enhance the design and optimization processes for microelectronic components with multiphysics constraints. <div>
arXiv:2504.17142v1 Announce Type: cross 
Abstract: This study focuses on the development of reinforcement learning based techniques for the design of microelectronic components under multiphysics constraints. While traditional design approaches based on global optimization approaches are effective when dealing with a small number of design parameters, as the complexity of the solution space and of the constraints increases different techniques are needed. This is an important reason that makes the design and optimization of microelectronic components (characterized by large solution space and multiphysics constraints) very challenging for traditional methods. By taking as prototypical elements an application-specific integrated circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and numerically test an optimization framework based on reinforcement learning (RL). More specifically, we consider the optimization of the bonded interconnect geometry for an ASIC chip as well as the placement of components on a HI interposer while satisfying thermoelastic and design constraints. This placement problem is particularly interesting because it features a high-dimensional solution space.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection, Classification and Prevalence of Self-Admitted Aging Debt</title>
<link>https://arxiv.org/abs/2504.17428</link>
<guid>https://arxiv.org/abs/2504.17428</guid>
<content:encoded><![CDATA[
<div> Aging Debt, Self-Admitted Aging Debt, software aging, source code comments, OSS repositories <br />
Summary:<br />
The study introduces Aging Debt (AD) as a concept to represent increased maintenance efforts in software. It focuses on Self-Admitted Aging Debt (SAAD) observed in source code comments to detect and measure AD in software. A taxonomy is developed to categorize temporal software aging into Active and Dormant types. Analysis of over 9,000+ OSS repositories shows that more than 21% exhibit SAAD, with Dormant AD being prevalent. The study highlights the critical aspect of software maintenance and the importance of addressing evolutionary indicators like source code comments in software aging research. The proposed taxonomy can assist researchers in detailed software aging studies and help practitioners in developing proactive maintenance strategies. <br /> <div>
arXiv:2504.17428v1 Announce Type: cross 
Abstract: Context: Previous research on software aging is limited with focus on dynamic runtime indicators like memory and performance, often neglecting evolutionary indicators like source code comments and narrowly examining legacy issues within the TD context. Objective: We introduce the concept of Aging Debt (AD), representing the increased maintenance efforts and costs needed to keep software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed in source code comments left by software developers. Method: We employ a mixed-methods approach, combining qualitative and quantitative analyses to detect and measure AD in software. This includes framing SAAD patterns from the source code comments after analysing the source code context, then utilizing the SAAD patterns to detect SAAD comments. In the process, we develop a taxonomy for SAAD that reflects the temporal aging of software and its associated debt. Then we utilize the taxonomy to quantify the different types of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes temporal software aging into Active and Dormant types. Our extensive analysis of over 9,000+ Open Source Software (OSS) repositories reveals that more than 21% repositories exhibit signs of SAAD as observed from our gold standard SAAD dataset. Notably, Dormant AD emerges as the predominant category, highlighting a critical but often overlooked aspect of software maintenance. Conclusion: As software volume grows annually, so do evolutionary aging and maintenance challenges; our proposed taxonomy can aid researchers in detailed software aging studies and help practitioners develop improved and proactive maintenance strategies.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An approach based on metaheuristic algorithms to the timetabling problem in deregulated railway markets</title>
<link>https://arxiv.org/abs/2504.17455</link>
<guid>https://arxiv.org/abs/2504.17455</guid>
<content:encoded><![CDATA[
<div> Genetic Algorithm, train timetabling, liberalized railway markets, optimization, scheduling<br />
<br />
Summary: <br />
The paper addresses the train timetabling problem in liberalized railway markets, focusing on maximizing infrastructure capacity and revenue optimization for infrastructure managers and railway undertakings. A modular simulation framework is introduced to simulate deregulated railway systems, evaluating ten metaheuristic algorithms using the MEALPY Python library. Results show that the Genetic Algorithm outperforms other algorithms in revenue optimization, convergence speed, and schedule adherence. Alternative algorithms, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, exhibit slower convergence and higher variability. The study highlights the trade-off between scheduling more trains and meeting requested times, offering insights into solving complex scheduling challenges in deregulated railway systems. <div>
arXiv:2504.17455v1 Announce Type: cross 
Abstract: The train timetabling problem in liberalized railway markets represents a challenge to the coordination between infrastructure managers and railway undertakings. Efficient scheduling is critical in maximizing infrastructure capacity and utilization while adhering as closely as possible to the requests of railway undertakings. These objectives ultimately contribute to maximizing the infrastructure manager's revenues. This paper sets out a modular simulation framework to reproduce the dynamics of deregulated railway systems. Ten metaheuristic algorithms using the MEALPY Python library are then evaluated in order to optimize train schedules in the liberalized Spanish railway market. The results show that the Genetic Algorithm outperforms others in revenue optimization, convergence speed, and schedule adherence. Alternatives, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, show slower convergence and higher variability. The results emphasize the trade-off between scheduling more trains and adhering to requested times, providing insights into solving complex scheduling problems in deregulated railway systems.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Equitable Rail Service Allocation Through Fairness-Oriented Timetabling in Liberalized Markets</title>
<link>https://arxiv.org/abs/2504.17489</link>
<guid>https://arxiv.org/abs/2504.17489</guid>
<content:encoded><![CDATA[
<div> Keywords: European rail transport, liberalization, infrastructure capacity, railway market, equity metrics 

Summary: 
In the changing landscape of European rail transport due to liberalization, railway undertakings now compete for limited infrastructure capacity to offer their services. The equitable allocation of infrastructure by the infrastructure manager is crucial for the efficiency and sustainability of this competitive environment. A methodology utilizing Jain, Gini, and Atkinson equity metrics is proposed to address the rail service allocation problem in a liberalized railway market. The results from computational tests demonstrate that this methodology promotes equitable planning across various competitiveness scenarios. This stands in contrast to approaches solely focused on maximizing the infrastructure manager's profit without considering fair allocation. The study supports the use of the proposed methodology and equity metrics as effective tools for planning and decision-making within a liberalized railway market. 

<br /><br />Summary: <div>
arXiv:2504.17489v1 Announce Type: cross 
Abstract: Over the last few decades, European rail transport has undergone major changes as part of the process of liberalization set out in European regulations. In this context of liberalization, railway undertakings compete with each other for the limited infrastructure capacity available to offer their rail services. The infrastructure manager is responsible for the equitable allocation of infrastructure between all companies in the market, which is essential to ensure the efficiency and sustainability of this competitive ecosystem. In this paper, a methodology based on Jain, Gini and Atkinson equity metrics is used to solve the rail service allocation problem in a liberalized railway market, analyzing the solutions obtained. The results show that the proposed methodology and the equity metrics used allow for equitable planning in different competitiveness scenarios. These results contrast with solutions where the objective of the infrastructure manager is to maximize its own profit, without regard for the equitable allocation of infrastructure. Therefore, the computational tests support the methodology and metrics used as a planning and decision support tool in a liberalized railway market.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and generalizable protein-ligand binding affinity prediction with geometric deep learning</title>
<link>https://arxiv.org/abs/2504.16261</link>
<guid>https://arxiv.org/abs/2504.16261</guid>
<content:encoded><![CDATA[
<div> deep learning, protein-ligand binding, affinity prediction, interatomic potential, machine learning<br />
Summary:<br />
The article introduces IPBind, a computational method based on geometric deep learning, for predicting protein-ligand binding affinity. While existing algorithms struggle with novel protein-ligand complexes, IPBind leverages interatomic potential to make robust predictions. Experimental results on binding affinity prediction benchmarks show the effectiveness and universality of IPBind, providing atom-level insights. This work underscores the benefits of using machine learning interatomic potential in protein-ligand binding affinity prediction. <div>
arXiv:2504.16261v1 Announce Type: new 
Abstract: Protein-ligand binding complexes are ubiquitous and essential to life. Protein-ligand binding affinity prediction (PLA) quantifies the binding strength between ligands and proteins, providing crucial insights for discovering and designing potential candidate ligands. While recent advances have been made in predicting protein-ligand complex structures, existing algorithms for interaction and affinity prediction suffer from a sharp decline in performance when handling ligands bound with novel unseen proteins. We propose IPBind, a geometric deep learning-based computational method, enabling robust predictions by leveraging interatomic potential between complex's bound and unbound status. Experimental results on widely used binding affinity prediction benchmarks demonstrate the effectiveness and universality of IPBind. Meanwhile, it provides atom-level insights into prediction. This work highlights the advantage of leveraging machine learning interatomic potential for predicting protein-ligand binding affinity.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Different Transformer Model Structures for Stock Prediction</title>
<link>https://arxiv.org/abs/2504.16361</link>
<guid>https://arxiv.org/abs/2504.16361</guid>
<content:encoded><![CDATA[
<div> Transformer, stock index prediction, model architectures, forecasting, ProbSparse attention

Summary:
- The paper compares different Transformer model architectures for stock index prediction, highlighting the importance of architectural choices in predictive accuracy. 
- Existing studies often treat Transformers as black boxes, overlooking the impact of specific structural designs on performance.
- Five Transformer structures were evaluated: encoder-only, decoder-only, Vanilla Transformer, Vanilla Transformer without embedding layers, and Vanilla Transformer with ProbSparse attention.
- Results indicate that Transformer models generally outperform traditional approaches, with the decoder-only structure being the most effective in all scenarios.
- The Transformer with ProbSparse attention showed the poorest performance in most cases. 

<br /><br />Summary: <div>
arXiv:2504.16361v1 Announce Type: new 
Abstract: This paper compares different Transformer model architectures for stock index prediction. While many studies have shown that Transformers perform well in stock price forecasting, few have explored how different structural designs impact performance. Most existing works treat the Transformer as a black box, overlooking how specific architectural choices may affect predictive accuracy. However, understanding these differences is critical for developing more effective forecasting models. This study aims to identify which Transformer variant is most suitable for stock forecasting. This study evaluates five Transformer structures: (1) encoder-only Transformer, (2) decoder-only Transformer, (3) Vanilla Transformer (encoder + decoder), (4) Vanilla Transformer without embedding layers, and (5) Vanilla Transformer with ProbSparse attention. Results show that Transformer-based models generally outperform traditional approaches. Transformer with decoder only structure outperforms all other models in all scenarios. Transformer with ProbSparse attention has the worst performance in almost all cases.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preconditioning Natural and Second Order Gradient Descent in Quantum Optimization: A Performance Benchmark</title>
<link>https://arxiv.org/abs/2504.16518</link>
<guid>https://arxiv.org/abs/2504.16518</guid>
<content:encoded><![CDATA[
<div> optimization, parametric quantum circuits, noisy gradient evaluations, curvature information, BFGS update rule <br />
Summary: <br />
The optimization of parametric quantum circuits faces challenges due to the non-convex nature of the objective function, noisy gradient evaluations, and barren plateaus. Selecting the right classical optimizer is crucial in quantum-classical applications. Incorporating curvature information in the parameter update can aid in faster convergence. Quasi-Newton and quantum natural gradient methods show promise in this regard but have drawbacks. A study evaluates the performance of optimizers on MaxCut problems using a shallow QAOA algorithm. To address noise sensitivity and iteration cost, a novel approach called secant-penalization in the BFGS update rule (SP-BFGS) is introduced, leading to improved outcomes for QAOA optimization problems. This method stabilizes BFGS updates against gradient noise, showcasing potential for optimizing parametric quantum circuits in noisy environments. <br /> <div>
arXiv:2504.16518v1 Announce Type: new 
Abstract: The optimization of parametric quantum circuits is technically hindered by three major obstacles: the non-convex nature of the objective function, noisy gradient evaluations, and the presence of barren plateaus. As a result, the selection of classical optimizer becomes a critical factor in assessing and exploiting quantum-classical applications. One promising approach to tackle these challenges involves incorporating curvature information into the parameter update. The most prominent methods in this field are quasi-Newton and quantum natural gradient methods, which can facilitate faster convergence compared to first-order approaches. Second order methods however exhibit a significant trade-off between computational cost and accuracy, as well as heightened sensitivity to noise. This study evaluates the performance of three families of optimizers on synthetically generated MaxCut problems on a shallow QAOA algorithm. To address noise sensitivity and iteration cost, we demonstrate that incorporating secant-penalization in the BFGS update rule (SP-BFGS) yields improved outcomes for QAOA optimization problems, introducing a novel approach to stabilizing BFGS updates against gradient noise.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields</title>
<link>https://arxiv.org/abs/2504.16600</link>
<guid>https://arxiv.org/abs/2504.16600</guid>
<content:encoded><![CDATA[
<div> Keywords: passive implants, magnetic fields, safety assessment, numerical approach, thermal diffusion <br />
Summary: <br />
- Safety assessment of patients with one-dimensional structured passive implants exposed to low or medium frequency magnetic fields poses challenges due to different length scales. <br />
- A novel numerical approach is proposed, solving three-dimensional and one-dimensional coupled problems, considering thermal diffusion through metallic implants for improved accuracy. <br />
- Results from measurements on a cranial plate exposed to a magnetic field show a 25% improvement in accuracy compared to the method based on thermal seeds. <br />
- Application of the proposed method in a magnetic hyperthermia case study predicts a 10% lower temperature increase near the implant compared to the overestimation by relying on thermal seeds. <br /> <div>
arXiv:2504.16600v1 Announce Type: new 
Abstract: Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Software Engineering Research on Jupyter Notebook</title>
<link>https://arxiv.org/abs/2504.16180</link>
<guid>https://arxiv.org/abs/2504.16180</guid>
<content:encoded><![CDATA[
<div> Trends, gaps, methodologies, human-computer interaction, reusability <br />
Summary:<br />
The study analyzed software engineering research on Jupyter notebooks, finding that most publications are in human-computer interaction venues rather than traditional software engineering ones. Various software engineering topics were addressed, but solutions for testing, refactoring, and documentation specific to notebooks are lacking. Only a small percentage of studies provide reusable code, and many replication packages are not stored in permanent repositories. Future research opportunities include developing automatic testing frameworks, refactoring clones between notebooks, and generating group documentation for coherent code cells. <div>
arXiv:2504.16180v1 Announce Type: cross 
Abstract: Context: Jupyter Notebook has emerged as a versatile tool that transforms how researchers, developers, and data scientists conduct and communicate their work. As the adoption of Jupyter notebooks continues to rise, so does the interest from the software engineering research community in improving the software engineering practices for Jupyter notebooks.
  Objective: The purpose of this study is to analyze trends, gaps, and methodologies used in software engineering research on Jupyter notebooks.
  Method: We selected 146 relevant publications from the DBLP Computer Science Bibliography up to the end of 2024, following established systematic literature review guidelines. We explored publication trends, categorized them based on software engineering topics, and reported findings based on those topics.
  Results: The most popular venues for publishing software engineering research on Jupyter notebooks are related to human-computer interaction instead of traditional software engineering venues. Researchers have addressed a wide range of software engineering topics on notebooks, such as code reuse, readability, and execution environment. Although reusability is one of the research topics for Jupyter notebooks, only 64 of the 146 studies can be reused based on their provided URLs. Additionally, most replication packages are not hosted on permanent repositories for long-term availability and adherence to open science principles.
  Conclusion: Solutions specific to notebooks for software engineering issues, including testing, refactoring, and documentation, are underexplored. Future research opportunities exist in automatic testing frameworks, refactoring clones between notebooks, and generating group documentation for coherent code cells.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Doubly Stochastic Transformers</title>
<link>https://arxiv.org/abs/2504.16275</link>
<guid>https://arxiv.org/abs/2504.16275</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, Softmax, Sinkhorn algorithm, doubly stochastic matrix, variational quantum circuit <br />
Summary: 
The study introduces a hybrid classical-quantum Doubly Stochastic Transformer (QDSFormer) that incorporates a variational quantum circuit in place of the Softmax in the attention layer of the Transformer model. The parametric quantum circuit used in the QDSFormer enhances the expressive power of the model, resulting in more diverse Doubly Stochastic Matrices (DSMs) that preserve information better than classical operators. Through experiments on small-scale object recognition tasks, the QDSFormer outperforms standard Vision Transformers and other doubly stochastic Transformers. Additionally, a quantum-inspired doubly stochastic Transformer based on QR decomposition is also compared. The QDSFormer demonstrates improved training stability and lower performance variation, suggesting that it could mitigate instability during training on small-scale data. These findings highlight the potential of quantum-inspired approaches in enhancing the performance and stability of Transformer models.
<br /><br />Summary: <div>
arXiv:2504.16275v1 Announce Type: cross 
Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often destabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard Vision Transformer and other doubly stochastic Transformers. Beyond the established Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. The QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixing Data-Driven and Physics-Based Constitutive Models using Uncertainty-Driven Phase Fields</title>
<link>https://arxiv.org/abs/2504.16713</link>
<guid>https://arxiv.org/abs/2504.16713</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, multiscale models, data-driven, phase-field model, Gaussian Process surrogate <br />
Summary:
This article presents an adaptive mixture approach for accelerating multiscale models using data-driven surrogate modeling techniques. The approach involves using a fast probabilistic surrogate model for most of the computational domain, switching to the true high-fidelity model only when necessary. By incorporating phases in the computational domain and utilizing a phase-field model driven by surrogate uncertainty, the transition between models is smooth and accurate. The method reduces the time required to collect a large training dataset, while still maintaining high accuracy in simulations. The study compares this approach to a purely local model and demonstrates its effectiveness using a Gaussian Process surrogate for an elasto-plastic material. The adaptive mixture of models shows great potential for speeding up multiscale simulations without compromising accuracy or stability. <br /> <div>
arXiv:2504.16713v1 Announce Type: cross 
Abstract: There is a high interest in accelerating multiscale models using data-driven surrogate modeling techniques. Creating a large training dataset encompassing all relevant load scenarios is essential for a good surrogate, yet the computational cost of producing this data quickly becomes a limiting factor. Commonly, a pre-trained surrogate is used throughout the computational domain. Here, we introduce an alternative adaptive mixture approach that uses a fast probabilistic surrogate model as constitutive model when possible, but resorts back to the true high-fidelity model when necessary. The surrogate is thus not required to be accurate for every possible load condition, enabling a significant reduction in the data collection time. We achieve this by creating phases in the computational domain corresponding to the different models. These phases evolve using a phase-field model driven by the surrogate uncertainty. When the surrogate uncertainty becomes large, the phase-field model causes a local transition from the surrogate to the high-fidelity model, maintaining a highly accurate simulation. We discuss the requirements of this approach to achieve accurate and numerically stable results and compare the phase-field model to a purely local approach that does not enforce spatial smoothness for the phase mixing. Using a Gaussian Process surrogate for an elasto-plastic material, we demonstrate the potential of this mixture of models to accelerate multiscale simulations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements in Constitutive Model Calibration: Leveraging the Power of Full-Field DIC Measurements and In-Situ Load Path Selection for Reliable Parameter Inference</title>
<link>https://arxiv.org/abs/2411.07310</link>
<guid>https://arxiv.org/abs/2411.07310</guid>
<content:encoded><![CDATA[
<div> calibration, material characterization, computational engineering, Bayesian inference, uncertainty quantification
<br />
Summary:
Interlaced Characterization and Calibration (ICC) presents an improved workflow for material model calibration, addressing current limitations. It efficiently uses full-field data for calibration, aligns collected data with optimal experimental design, quantifies parameter uncertainty through Bayesian inference, and incorporates a real-time feedback loop. Demonstrated on an aluminum cruciform specimen, ICC utilizes Bayesian optimal experimental design for selecting load steps, principal component analysis for reducing data dimensions, and fast surrogate models for computational efficiency. This framework allows for reliable calibration of high-fidelity constitutive models with quantified uncertainty. By advancing the state-of-the-art in material characterization and model calibration, ICC supports credible decision-making in solid mechanics modeling, potentially increasing modeling agility. 
<br /> <div>
arXiv:2411.07310v3 Announce Type: replace 
Abstract: Accurate material characterization and model calibration are essential for computationally-supported engineering decisions. Current characterization and calibration methods (1) use simplified test specimen geometries and global data, (2) cannot guarantee that sufficient characterization data is collected for a specific model of interest, (3) use deterministic methods that provide best-fit parameter values with no uncertainty quantification, and (4) are sequential, inflexible, and time-consuming. This work brings together several recent advancements into an improved workflow called Interlaced Characterization and Calibration that advances the state-of-the-art in constitutive model calibration. The ICC paradigm (1) efficiently uses full-field data to calibrate a high-fidelity material model, (2) aligns the data needed with the data collected with an optimal experimental design protocol, (3) quantifies parameter uncertainty through Bayesian inference, and (4) incorporates these advances into a quasi real-time feedback loop. The ICC framework is demonstrated on the calibration of a material model using simulated full-field data for an aluminum cruciform specimen being deformed bi-axially. The cruciform is actively driven through the myopically optimal load path using Bayesian optimal experimental design, which selects load steps that yield the maximum expected information gain. To aid in numerical stability and preserve computational resources, the full-field data is dimensionally reduced via principal component analysis, and fast surrogate models which approximate the input-output relationships of the expensive finite element model are used. The tools demonstrated here show that high-fidelity constitutive models can be efficiently and reliably calibrated with quantified uncertainty, thus supporting credible decision-making and potentially increasing the agility of solid mechanics modeling.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>