<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CE updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CE</link>

<item>
<title>Optimization of Functional Materials Design with Optimal Initial Data in Surrogate-Based Active Learning</title>
<link>https://arxiv.org/abs/2506.03329</link>
<guid>https://arxiv.org/abs/2506.03329</guid>
<content:encoded><![CDATA[
<div> Keywords: functional materials, optimization, data-driven algorithms, surrogate-based active learning, quantum computing

Summary: 
This study focuses on optimizing functional materials through data-driven algorithms, which efficiently explore complex design spaces by learning relationships between material structures and performance metrics. Surrogate-based active learning, coupled with quantum computing, is highlighted as a cost-effective approach for material optimization. The use of a special surrogate model called quadratic unconstrained binary optimization is emphasized. The research investigates the impact of initial data sizes on optimization efficiency, showing that adequate initial data is crucial for achieving fast convergence and reducing computational costs. Averaged piecewise linear regression is used to identify the optimal initiation points for convergence, emphasizing the importance of proper initial data in efficient optimization of functional materials. This work contributes to improving optimization processes for functional materials by ensuring faster convergence and reduced computational expenses in surrogate-based active learning. 

<br><br>Summary: <div>
arXiv:2506.03329v1 Announce Type: new 
Abstract: The optimization of functional materials is important to enhance their properties, but their complex geometries pose great challenges to optimization. Data-driven algorithms efficiently navigate such complex design spaces by learning relationships between material structures and performance metrics to discover high-performance functional materials. Surrogate-based active learning, continually improving its surrogate model by iteratively including high-quality data points, has emerged as a cost-effective data-driven approach. Furthermore, it can be coupled with quantum computing to enhance optimization processes, especially when paired with a special form of surrogate model ($i.e.$, quadratic unconstrained binary optimization), formulated by factorization machine. However, current practices often overlook the variability in design space sizes when determining the initial data size for optimization. In this work, we investigate the optimal initial data sizes required for efficient convergence across various design space sizes. By employing averaged piecewise linear regression, we identify initiation points where convergence begins, highlighting the crucial role of employing adequate initial data in achieving efficient optimization. These results contribute to the efficient optimization of functional materials by ensuring faster convergence and reducing computational costs in surrogate-based active learning.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the robustness of Dirichlet-Neumann coupling schemes for fluid-structure-interaction problems with nearly-closed fluid domains</title>
<link>https://arxiv.org/abs/2506.04027</link>
<guid>https://arxiv.org/abs/2506.04027</guid>
<content:encoded><![CDATA[
<div> Dirichlet-Neumann split, incompressible fluid, added-mass effect, flow resistance, nearly-closed fluid-domain<br>
<br>
Summary:<br>
Partitioned methods for fluid-structure interaction (FSI) typically use a Dirichlet-Neumann (DN) split for interface conditions. However, for nearly-closed fluid domains with incompressible fluids and Robin conditions, the DN scheme can become unstable due to increasing flow resistance. Convergence deteriorates as resistance increases, leading to instability at high resistances. This instability is linked to an added-damping effect, affecting the convergence rate of the partitioned method. Understanding this effect can improve the robustness and efficiency of FSI simulations, especially for applications like valves. The analysis also sheds light on the incompressibility dilemma for FSI problems with nearly closed fluid domains. Numerical experiments confirm these findings in more complex scenarios, highlighting the challenges and potential solutions for such FSI problems. <div>
arXiv:2506.04027v1 Announce Type: new 
Abstract: Partitioned methods for fluid-structure interaction (FSI) involve solving the structural and flow problems sequentially. These methods allow for separate settings for the fluid and solid subsystems and thus modularity, enabling reuse of advanced commercial and open-source software. Most partitioned FSI schemes apply a Dirichlet-Neumann (DN) split of the interface conditions. The DN scheme is adequate in a wide range of applications, but it is sensitive to the added-mass effect, and it is susceptible to the incompressibility dilemma, i.e. it completely fails for FSI problems with an incompressible fluid furnished with Dirichlet boundary conditions on the part of its boundary complementary to the interface. In this paper, we show that if the fluid is incompressible and the fluid domain is nearly-closed, i.e. it carries Dirichlet conditions except for a permeable part of the boundary carrying a Robin condition, then the DN partitioned approach is sensitive to the flow resistance at the permeable part, and convergence of the partitioned approach deteriorates as the flow resistance increases. The DN scheme then becomes unstable in the limit as the flow resistance passes to infinity. Based on a simple model problem, we show that in the nearly-closed case, the convergence rate of the DN partitioned method depends on a so-called added-damping effect. The analysis gives insights that can aid to improve robustness and efficiency of partitioned method for FSI problems with contact, e.g. valve applications. In addition, the results elucidate the incompressibility dilemma as a limit of the added-damping effect passing to infinity, and the corresponding challenges related to FSI problems with nearly closed fluid-domain configurations. Via numerical experiments, we consider the generalization of the results of the simple model problem to more complex nearly-closed FSI problems.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk and Reward of Transitioning from a National to a Zonal Electricity Market in Great Britain</title>
<link>https://arxiv.org/abs/2506.04107</link>
<guid>https://arxiv.org/abs/2506.04107</guid>
<content:encoded><![CDATA[
<div> consumer savings, producer surplus impacts, socioeconomic benefits, zonal market, electricity market  

Summary:  
The study evaluates the potential benefits of transitioning from a single-price national wholesale market to a zonal market design in Great Britain. Using an open-source electricity market model, the analysis shows that a six-zone market could result in significant consumer savings of around £9.4/MWh annually, totaling over £2.3 billion per year. However, generators in northern regions may experience revenue reductions of 30-40%. Policy interventions could mitigate these negative impacts, allowing for up to 97% restoration of national market revenues for affected units while still preserving around £3.1/MWh in consumer savings. The current system could achieve an annual welfare gain of £380-£770 million through operational efficiency improvements alone during 2022-2024, with potential annual benefits exceeding £1-2 billion beyond 2029. These benefits outweigh potential downsides associated with increased capital costs. <div>
arXiv:2506.04107v1 Announce Type: cross 
Abstract: More spatially granular electricity wholesale markets promise more efficient operation and better asset siting in highly renewable power systems. Great Britain is considering moving from its current single-price national wholesale market to a zonal design. Existing studies reach varying and difficult-to-reconcile conclusions about the desirability of a zonal market in GB, partly because they rely on models that vary in their transparency and assumptions about future power systems. Using a novel open-source electricity market model, calibrated to match observed network behaviour, this article quantifies consumer savings, unit-level producer surplus impacts, and broader socioeconomic benefits that would have arisen had a six-zone market operated in Great Britain during 2022-2024. In the absence of mitigating policies, it is estimated that during those three years GB consumers would save approximately {\pounds}9.4/MWh (equalling an average of more than {\pounds}2.3B per year), but generators in northern regions would experience revenue reductions of 30-40\%. Policy interventions can restore these units' national market revenues to up to 97\% while still preserving around {\pounds}3.1/MWh in consumer savings (about {\pounds}750M per year). It is further estimated that the current system could achieve approximately {\pounds}380-{\pounds}770 million in annual welfare gain during 2022-2024 through improved operational efficiency alone. The drivers behind these benefits, notably wind curtailment volumes, are expected to become more pronounced towards 2030, suggesting that purely operationally achieved annual benefits of around {\pounds}1-2 billion beyond 2029 are likely. It is found that the scale of these benefits would outweigh the potential downsides related to increases in the cost of capital that have been estimated elsewhere.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.04171</link>
<guid>https://arxiv.org/abs/2506.04171</guid>
<content:encoded><![CDATA[
<div> Flow-Based Generative Models, Physics-Constrained Inference, Partial Differential Equations, Constraint Satisfaction, Zero-Shot Inference<br>
Summary:<br>
The article introduces Physics-Constrained Flow Matching (PCFM), a method for enforcing nonlinear constraints in pretrained flow-based generative models. Existing methods struggle to enforce physical constraints effectively, but PCFM addresses this by guiding the sampling process with physics-based corrections while maintaining alignment with learned flow. The framework outperforms both unconstrained and constrained baselines on various PDEs, including those with shocks and sharp features. PCFM ensures exact satisfaction of constraints at the final solution. This approach presents a general framework for enforcing hard constraints in scientific and general-purpose generative models, particularly valuable in applications where constraint satisfaction is critical. <br><br>Summary: <div>
arXiv:2506.04171v1 Announce Type: cross 
Abstract: Deep generative models have recently been applied to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty-aware inference. However, enforcing physical constraints, such as conservation laws (linear and nonlinear) and physical consistencies, remains challenging. Existing methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints. In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints. Empirically, PCFM outperforms both unconstrained and constrained baselines on a range of PDEs, including those with shocks, discontinuities, and sharp features, while ensuring exact constraint satisfaction at the final solution. Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models, especially in applications where constraint satisfaction is essential.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Multimodal Financial Foundation Models (MFFMs): Progress, Prospects, and Challenges</title>
<link>https://arxiv.org/abs/2506.01973</link>
<guid>https://arxiv.org/abs/2506.01973</guid>
<content:encoded><![CDATA[
<div> Keywords: FinLLMs, MFFMs, financial data, multimodal, research

Summary:
Financial Large Language Models (FinLLMs) and Multimodal Financial Foundation Models (MFFMs) are revolutionizing the analysis of financial data. While FinLLMs focus on language-centric approaches, MFFMs can process a wide range of multimodal financial data, offering a more comprehensive understanding of complex financial tasks. The progress and potential of MFFMs were discussed in a position paper presented at the MFFM Workshop at the ACM International Conference on AI in Finance 2024. Ongoing research on FinAgents at the SecureFinAI Lab at Columbia University aims to further explore the capabilities of MFFMs. By leveraging diverse data sources such as fundamental data, market data, and alternative data, MFFMs have the potential to streamline financial operations and investment processes. The Github repository for MFFMs provides a platform for collaboration and development in this emerging field. <div>
arXiv:2506.01973v1 Announce Type: new 
Abstract: Financial Large Language Models (FinLLMs), such as open FinGPT and proprietary BloombergGPT, have demonstrated great potential in select areas of financial services. Beyond this earlier language-centric approach, Multimodal Financial Foundation Models (MFFMs) can digest interleaved multimodal financial data, including fundamental data, market data, data analytics, macroeconomic, and alternative data (e.g., natural language, audio, images, and video). In this position paper, presented at the MFFM Workshop joined with ACM International Conference on AI in Finance (ICAIF) 2024, we describe the progress, prospects, and challenges of MFFMs. This paper also highlights ongoing research on FinAgents in the \textbf{SecureFinAI Lab}\footnote{\https://openfin.engineering.columbia.edu/} at Columbia University. We believe that MFFMs will enable a deeper understanding of the underlying complexity associated with numerous financial tasks and data, streamlining the operation of financial services and investment processes. Github Repo https://github.com/Open-Finance-Lab/Awesome-MFFMs/.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Polymer Property Predictions</title>
<link>https://arxiv.org/abs/2506.02129</link>
<guid>https://arxiv.org/abs/2506.02129</guid>
<content:encoded><![CDATA[
<div> Machine learning, polymer informatics, large language models, thermal properties, molecular embeddings
Summary: 
- The study explores the use of large language models (LLMs) in predicting key thermal properties in polymer science, comparing them to traditional fingerprinting-based methods.
- LLaMA-3 outperforms GPT-3.5 in predictive accuracy, likely due to its open-source architecture.
- Single-task learning proves more effective than multi-task learning, as LLMs struggle with capturing cross-property correlations.
- Analysis of molecular embeddings shows limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features.
- The findings provide guidance on selecting LLMs for polymer informatics, highlighting the interplay between molecular embeddings and natural language processing. 

<br /><br />Summary: <div>
arXiv:2506.02129v1 Announce Type: new 
Abstract: Machine learning has revolutionized polymer science by enabling rapid property prediction and generative design. Large language models (LLMs) offer further opportunities in polymer informatics by simplifying workflows that traditionally rely on large labeled datasets, handcrafted representations, and complex feature engineering. LLMs leverage natural language inputs through transfer learning, eliminating the need for explicit fingerprinting and streamlining training. In this study, we finetune general purpose LLMs -- open-source LLaMA-3-8B and commercial GPT-3.5 -- on a curated dataset of 11,740 entries to predict key thermal properties: glass transition, melting, and decomposition temperatures. Using parameter-efficient fine-tuning and hyperparameter optimization, we benchmark these models against traditional fingerprinting-based approaches -- Polymer Genome, polyGNN, and polyBERT -- under single-task (ST) and multi-task (MT) learning. We find that while LLM-based methods approach traditional models in performance, they generally underperform in predictive accuracy and efficiency. LLaMA-3 consistently outperforms GPT-3.5, likely due to its tunable open-source architecture. Additionally, ST learning proves more effective than MT, as LLMs struggle to capture cross-property correlations, a key strength of traditional methods. Analysis of molecular embeddings reveals limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features and domain-specific embeddings. These findings provide insight into the interplay between molecular embeddings and natural language processing, guiding LLM selection for polymer informatics.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singularity Blockchain Key Management via non-custodial key management</title>
<link>https://arxiv.org/abs/2506.02282</link>
<guid>https://arxiv.org/abs/2506.02282</guid>
<content:encoded><![CDATA[
<div> web3 wallets, user identity, blockchain, key management, non-custodial<br />
<br />
Summary: <br />
Web3 wallets play a crucial role in managing user identity on the blockchain by storing and providing access to private keys. Key management schemes can be either custodial or non-custodial, with the latter placing the burden of key storage and recovery on the user. Existing non-custodial schemes often require users to remember seed phrases, leading to onboarding challenges and the risk of asset loss if the key is forgotten. This paper introduces a novel non-custodial key management approach that allows users to back up and recover their private key using third-party sign-in methods such as google-oAuth. By enabling users to securely backup their keys through independent authentication methods, this technique aims to enhance user experience and reduce the likelihood of key loss. <div>
arXiv:2506.02282v1 Announce Type: new 
Abstract: web3 wallets are key to managing user identity on blockchain. The main purpose of a web3 wallet application is to manage the private key for the user and provide an interface to interact with the blockchain. The key management scheme ( KMS ) used by the wallet to store and recover the private key can be either custodial, where the keys are permissioned and in custody of the wallet provider or noncustodial where the keys are in custody of the user. The existing non-custodial key management schemes tend to offset the burden of storing and recovering the key entirely on the user by asking them to remember seed-phrases. This creates onboarding hassles for the user and introduces the risk that the user may lose their assets if they forget or lose their seedphrase/private key. In this paper, we propose a novel method of backing up user keys using a non-custodial key management technique that allows users to save and recover a backup of their private key using any independent sign-in method such as google-oAuth or other 3P oAuth.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the fracture mechanics validity of small scale tests</title>
<link>https://arxiv.org/abs/2506.02538</link>
<guid>https://arxiv.org/abs/2506.02538</guid>
<content:encoded><![CDATA[
<div> Keywords: fracture behaviour, micro-scale mechanical tests, crack growth, material properties, hydrogen embrittlement

Summary: 
This study focuses on conducting small-scale tests to understand the fracture behavior of materials. Using numerical and semi-analytical approaches, the researchers determine the conditions necessary for a valid and quantitative fracture experiment, considering factors such as sample geometry, material properties, and crack lengths. They establish the maximum value of the J-integral, known as Jmax, where fracture must occur for accurate results. Maps are generated to show the maximum valid J value as a function of yield strength, strain hardening, and sample size, providing guidance for conducting experiments. The analysis is extended to metals embrittled by hydrogen exposure, overlaying the response of these materials on the established maps to determine the conditions required for obtaining quantitative insight into such materials. <div>
arXiv:2506.02538v1 Announce Type: new 
Abstract: There is growing interest in conducting small-scale tests to gain additional insight into the fracture behaviour of components across a wide range of materials. For example, micro-scale mechanical tests inside of a microscope (\emph{in situ}) enable direct, high-resolution observation of the interplay between crack growth and microstructural phenomena (e.g., dislocation behaviour or the fracture resistance of a particular interface), and sub-size samples are increasingly used when only a limited amount of material is available. However, to obtain quantitative insight and extract relevant fracture parameters, the sample must be sufficiently large for a $J$- (HRR) or a $K$-field to exist. We conduct numerical and semi-analytical studies to map the conditions (sample geometry, material) that result in a valid, quantitative fracture experiment. Specifically, for a wide range of material properties, crack lengths and sample dimensions, we establish the maximum value of the $J$-integral where an HRR field ceases to exist (i.e., the maximum $J$ value at which fracture must occur for the test to be valid, $J_\mathrm{max}$). Maps are generated to establish the maximum valid $J$ value ($J_\mathrm{max}$) as a function of yield strength, strain hardening and minimum sample size. These maps are then used to discuss the existing experimental literature and provide guidance on how to conduct quantitative experiments. Finally, our study is particularised to the analysis of metals that have been embrittled due to hydrogen exposure. The response of relevant materials under hydrogen-containing environments are superimposed on the aforementioned maps, determining the conditions that will enable quantitative insight.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching Location Representation with Detailed Semantic Information</title>
<link>https://arxiv.org/abs/2506.02744</link>
<guid>https://arxiv.org/abs/2506.02744</guid>
<content:encoded><![CDATA[
<div> Embeddings, Urban Modeling, Contrastive Learning, Point-of-Interest, Multimodal<br />
<br />
Summary: 
The study introduces CaLLiPer+, an urban modeling approach that integrates Point-of-Interest names with categorical labels in a contrastive learning framework. The model shows improved performance in land use classification and socioeconomic status mapping compared to baseline methods, with gains of 4% to 11%. By incorporating POI names, the model enhances location retrieval and captures complex urban concepts accurately. Ablation studies demonstrate the complementary role of POI names and the benefits of using pretrained text encoders for spatial representations. The research underscores the significance of integrating fine-grained semantic attributes and multimodal learning techniques for advancing urban foundation models. <div>
arXiv:2506.02744v1 Announce Type: new 
Abstract: Spatial representations that capture both structural and semantic characteristics of urban environments are essential for urban modeling. Traditional spatial embeddings often prioritize spatial proximity while underutilizing fine-grained contextual information from places. To address this limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that systematically integrates Point-of-Interest (POI) names alongside categorical labels within a multimodal contrastive learning framework. We evaluate its effectiveness on two downstream tasks, land use classification and socioeconomic status distribution mapping, demonstrating consistent performance gains of 4% to 11% over baseline methods. Additionally, we show that incorporating POI names enhances location retrieval, enabling models to capture complex urban concepts with greater precision. Ablation studies further reveal the complementary role of POI names and the advantages of leveraging pretrained text encoders for spatial representations. Overall, our findings highlight the potential of integrating fine-grained semantic attributes and multimodal learning techniques to advance the development of urban foundation models.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Aware Density Estimation in Multiple Dimensions</title>
<link>https://arxiv.org/abs/2506.02323</link>
<guid>https://arxiv.org/abs/2506.02323</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization, probability densities, multidimensional problems, splines, PET rebinning  
Summary:  
- The article presents an optimization problem to estimate probability densities in multidimensional problems with uneven sampling probability.  
- Detector sensitivity is considered as an heterogeneous density, utilizing splines on a grid for computational speed and flexible boundary conditions.  
- The method uses nuclear norm regularization on the spline's Hessian to promote sparsity, making it spatially adaptive and stable against the choice of regularization parameter.  
- The computational pipeline is tested on standard densities, with provided software for implementation.  
- A new approach to PET rebinning is showcased as an application of the framework.<br /><br />Summary: <div>
arXiv:2506.02323v1 Announce Type: cross 
Abstract: We formulate an optimization problem to estimate probability densities in the context of multidimensional problems that are sampled with uneven probability. It considers detector sensitivity as an heterogeneous density and takes advantage of the computational speed and flexible boundary conditions offered by splines on a grid. We choose to regularize the Hessian of the spline via the nuclear norm to promote sparsity. As a result, the method is spatially adaptive and stable against the choice of the regularization parameter, which plays the role of the bandwidth. We test our computational pipeline on standard densities and provide software. We also present a new approach to PET rebinning as an application of our framework.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning</title>
<link>https://arxiv.org/abs/2506.02485</link>
<guid>https://arxiv.org/abs/2506.02485</guid>
<content:encoded><![CDATA[
<div> Generative AI, Wildfire prediction, Multimodal approaches, 2D fire spread forecasting, 3D simulations 

Summary:
Generative AI models like GANs and VAEs show promise in improving wildfire prediction by integrating multimodal data and generating diverse scenarios. These models can enhance 2D fire spread forecasting and enable more realistic 3D simulations. A human-AI collaboration framework using large language models aids in automated knowledge extraction and literature synthesis. Five key visions for integrating generative AI into wildfire management include multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. The challenges of implementing these visions are also addressed, with proposed solutions to overcome them.<br /><br />Summary: <div>
arXiv:2506.02485v1 Announce Type: cross 
Abstract: Wildfires continue to inflict devastating human, environmental, and economic losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and the urgent demand for more effective response strategies. While physics-based and deep learning models have advanced wildfire simulation, they face critical limitations in predicting and visualizing multimodal fire spread in real time, particularly in both 2D and 3D spatial domains using dynamically updated GIS data. These limitations hinder timely emergency response, infrastructure protection, and community safety. Generative AI has recently emerged as a transformative approach across research and industry. Models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and diffusion-based architectures offer distinct advantages over traditional methods, including the integration of multimodal data, generation of diverse scenarios under uncertainty, and improved modeling of wildfire dynamics across spatial and temporal scales. This position paper advocates for the adoption of generative AI as a foundational framework for wildfire prediction. We explore how such models can enhance 2D fire spread forecasting and enable more realistic, scalable 3D simulations. Additionally, we employ a novel human-AI collaboration framework using large language models (LLMs) for automated knowledge extraction, literature synthesis, and bibliometric mapping. Looking ahead, we identify five key visions for integrating generative AI into wildfire management: multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. We also address three major challenges accompanying these opportunities and propose potential solutions to support their implementation.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression</title>
<link>https://arxiv.org/abs/2506.02678</link>
<guid>https://arxiv.org/abs/2506.02678</guid>
<content:encoded><![CDATA[
<div> Dynamic ratio-based training, Large Language Models, efficient language reasoning, inference, System-1, System-2 <br />
<br />
Dynamic ratio-based training is proposed in this work to improve the efficiency of language reasoning in large language models. The method continuously balances the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. The approach is validated on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B models across a variety of benchmarks with varying difficulty levels. Results show a reduction of nearly 40% in the number of output tokens while maintaining reasoning accuracy. The research presents an innovative solution to the challenge of performing efficient language reasoning, particularly during inference with long outputs, without the need for sophisticated data annotations or interpolation between models. Code and data for the research will be made available soon. <br /><br />Summary: <div>
arXiv:2506.02678v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mesoscale phase-field model of intergranular liquid lithium corrosion of ferritic/martensitic steels</title>
<link>https://arxiv.org/abs/2506.02776</link>
<guid>https://arxiv.org/abs/2506.02776</guid>
<content:encoded><![CDATA[
<div> phase-field model, intergranular corrosion, ferritic/martensitic steels, liquid lithium, chromium concentration

Summary:
The article presents a phase-field model for simulating intergranular corrosion in ferritic/martensitic steels exposed to liquid lithium. By tracking the chromium concentration in the material, mass transport within the metal and liquid phases is analyzed. The model effectively captures intergranular corrosion by enhancing chromium diffusion along grain boundaries without the need for specific treatment. Results from simulations align closely with experimental measurements of weight loss and corrosion depth in a 9 wt% Cr steel at 600°C. A sensitivity analysis reveals the influence of microstructural factors such as near-surface grain density and grain size on the corrosion process. The study also evaluates the impact of saturation on corrosion behavior. Overall, near-surface grain density is identified as a critical factor, while grain size is found to influence susceptibility to intergranular corrosion. <div>
arXiv:2506.02776v1 Announce Type: cross 
Abstract: A phase-field model is developed to simulate intergranular corrosion of ferritic/martensitic steels exposed to liquid lithium. The chromium concentration of the material is used to track the mass transport within the metal and liquid (corrosive) phase. The framework naturally captures intergranular corrosion by enhancing the diffusion of chromium along grain boundaries relative to the grain bulk with no special treatment for the corrosion front evolution. The formulation applies to arbitrary 2D and 3D polycrystalline geometries. The framework reproduces experimental measurements of weight loss and corrosion depth for a 9 wt\% Cr ferritic/martensitic steel exposed to static lithium at 600 $^\circ$C. A sensitivity analysis, varying near-surface grain density, grain size, and chromium depletion thickness, highlights the microstructural influence in the corrosion process. Moreover, the significance of saturation is considered and evaluated. Simulation results show that near-surface grain density is a deciding factor, whereas grain size dictates the susceptibility to intergranular corrosion.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.02911</link>
<guid>https://arxiv.org/abs/2506.02911</guid>
<content:encoded><![CDATA[
<div> cell type annotation, single-cell RNA sequencing data, CellPuzzles, large language models, batch-level accuracy

Summary:
Cell type annotation plays a critical role in analyzing single-cell RNA sequencing data heterogeneity. Current foundation models lack the ability to consider batch-level cellular context and provide explanatory reasoning in cell type annotation tasks. To address this, the CellPuzzles task was introduced to mimic expert annotation workflows, requiring unique cell type assignment across batches of cells. Existing large language models struggle with this task, with limited accuracy. In response, Cell-o1, a 7B large language model, was developed through supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 surpasses previous models, demonstrating superior performance and generalization across different contexts. The training dynamics and reasoning behaviors of Cell-o1 provide insights into improved batch-level annotation performance and expert-like reasoning strategies. <div>
arXiv:2506.02911v1 Announce Type: cross 
Abstract: Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to optimize convex risk measures: The cases of utility-based shortfall risk and optimized certainty equivalent risk</title>
<link>https://arxiv.org/abs/2506.01101</link>
<guid>https://arxiv.org/abs/2506.01101</guid>
<content:encoded><![CDATA[
<div> risk measures, estimation, optimization, gradient estimators, stochastic gradient algorithm

Summary: 
The article introduces the estimation and optimization of convex risk measures, specifically utility-based shortfall risk (UBSR) and Optimized Certainty Equivalent (OCE) risk, covering unbounded random variables. It extends various risk measures like entropic risk and Value-at-Risk. Non-asymptotic bounds are derived for mean absolute error and mean-squared error in estimation using sample average approximation (SAA) estimators. Expressions for UBSR and OCE gradients under smooth parameterization are provided, with gradient estimators proposed using the SAA estimator of UBSR and non-asymptotic bounds on error. A stochastic gradient algorithm is developed for optimization using these gradient estimators. Non-asymptotic convergence rate bounds are derived for the optimization of UBSR and OCE risk measures. <div>
arXiv:2506.01101v1 Announce Type: new 
Abstract: We consider the problems of estimation and optimization of two popular convex risk mea- sures: utility-based shortfall risk (UBSR) and Optimized Certainty Equivalent (OCE) risk. We extend these risk measures to cover possibly unbounded random variables. We cover prominent risk measures like the entropic risk, expectile risk, monotone mean-variance risk, Value-at-Risk, and Conditional Value-at-Risk as few special cases of either the UBSR or the OCE risk. In the context of estimation, we derive non-asymptotic bounds on the mean absolute error (MAE) and mean-squared error (MSE) of the classical sample average approximation (SAA) estimators of both, the UBSR and the OCE. Next, in the context of optimization, we derive expressions for the UBSR gradient and the OCE gradient under a smooth parameterization. Utilizing these expres- sions, we propose gradient estimators for both, the UBSR and the OCE. We use the SAA estimator of UBSR in both these gradient estimators, and derive non-asymptotic bounds on MAE and MSE for the proposed gradient estimation schemes. We incorporate the aforementioned gradient estima- tors into a stochastic gradient (SG) algorithm for optimization. Finally, we derive non-asymptotic bounds that quantify the rate of convergence of our SG algorithm for the optimization of the UBSR and the OCE risk measure
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast numerical generation of Lie closure</title>
<link>https://arxiv.org/abs/2506.01120</link>
<guid>https://arxiv.org/abs/2506.01120</guid>
<content:encoded><![CDATA[
<div> Keywords: Lie algebra, matrix, numerical construction, quantum computing, linear independence<br />
Summary:<br />
The article discusses the importance of finding the Lie-algebraic closure of matrices in quantum computing and quantum control. Analytically determining the closure is challenging for most cases, leading to a need for numerical construction. The standard algorithm for this construction relies on a subroutine to check linear independence, which can be computationally intensive. The authors present efficient methods for linear independence checks that reduce computational complexity and memory usage. One of these methods is implemented and validated against known results. These new algorithms allow for the exploration of Lie closure in larger system sizes that were previously unattainable, opening up possibilities for numerical studies in quantum computing and control applications. <div>
arXiv:2506.01120v1 Announce Type: new 
Abstract: Finding the Lie-algebraic closure of a handful of matrices has important applications in quantum computing and quantum control. For most realistic cases, the closure cannot be determined analytically, necessitating an explicit numerical construction. The standard construction algorithm makes repeated calls to a subroutine that determines whether a matrix is linearly independent from a potentially large set of matrices. Because the common implementation of this subroutine has a high complexity, the construction of Lie closure is practically limited to trivially small matrix sizes. We present efficient alternative methods of linear independence check that simultaneously reduce the computational complexity and memory footprint. An implementation of one of the methods is validated against known results. Our new algorithms enable numerical studies of Lie closure in larger system sizes than was previously possible.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging ML-AI Techniques for Analog and RF EDA</title>
<link>https://arxiv.org/abs/2506.00007</link>
<guid>https://arxiv.org/abs/2506.00007</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, EDA workflows, analog design, RF circuits, optimization techniques

Summary: 
This survey delves into the integration of machine learning (ML) into electronic design automation (EDA) workflows specifically tailored for analog and RF circuits. The challenges unique to analog design, such as complex constraints, nonlinear design spaces, and high computational costs, are addressed. The review encompasses state-of-the-art ML and optimization techniques for various circuit tasks, including constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey emphasizes how ML can enhance automation, elevate design quality, and reduce time-to-market while meeting desired circuit specifications. In addition, emerging trends and cross-cutting challenges, like robustness to variations and considerations of interconnect parasitics, are explored. Overall, the survey underscores the potential of ML to revolutionize analog and RF circuit design by optimizing workflow efficiency and achieving superior design outcomes. 

<br /><br />Summary: <div>
arXiv:2506.00007v1 Announce Type: cross 
Abstract: This survey explores the integration of machine learning (ML) into EDA workflows for analog and RF circuits, addressing challenges unique to analog design, which include complex constraints, nonlinear design spaces, and high computational costs. State-of-the-art learning and optimization techniques are reviewed for circuit tasks such as constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey highlights the capability of ML to enhance automation, improve design quality, and reduce time-to-market while meeting the target specifications of an analog or RF circuit. Emerging trends and cross-cutting challenges, including robustness to variations and considerations of interconnect parasitics, are also discussed.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Vessel Behavior using AIS Trajectory Data and Markovian Models in the Gulf of St. Lawrence</title>
<link>https://arxiv.org/abs/2506.00025</link>
<guid>https://arxiv.org/abs/2506.00025</guid>
<content:encoded><![CDATA[
<div> Keywords: maritime mobility, spatio-temporal analysis, vessel movement patterns, discrete-time Markov chains, COVID-19 pandemic <br />
Summary: <br />
This article presents a spatio-temporal analytical framework using discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, focusing on changes during the COVID-19 pandemic. The ocean space is divided into hexagonal cells, and mobility signatures for different vessel types are constructed based on cell transitions and dwell time. Origin-destination matrices and spatial transition probability models are developed to understand vessel dynamics at different time scales. The study reveals consistent mobility signatures for specific vessel types across different regions, suggesting underlying behavioral patterns. During the pandemic, passenger and fishing vessels show significant temporal deviations, reflecting the impact of social isolation measures and operational restrictions on non-essential maritime activities in the region. These findings contribute to a better understanding of maritime mobility patterns and highlight the influence of external factors on vessel movements. <br /> <div>
arXiv:2506.00025v1 Announce Type: cross 
Abstract: Maritime Mobility is at the center of the global economy, and analyzing and understanding such data at scale is critical for ocean conservation and governance. Accordingly, this work introduces a spatio-temporal analytical framework based on discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, emphasizing changes induced during the COVID-19 pandemic. We discretize the ocean space into hexagonal cells and construct mobility signatures for individual vessel types using the frequency of cell transitions and the dwell time within each cell. These features are used to build origin-destination matrices and spatial transition probability models that characterize vessel dynamics at different temporal resolutions. Under multiple vessel types, we contribute with a temporal evolution analysis of mobility patterns during pandemic times, highlighting significant but transient changes to recurring transportation behaviors. Our findings indicate vessel-specific mobility signatures consistent across spatially disjoint regions, suggesting that those are latent behavioral invariants. Besides, we observe significant temporal deviations among passenger and fishing vessels during the pandemic, indicating a strong influence of social isolation policies and operational limitations imposed on non-essential maritime activity in this region.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risks of AI-driven product development and strategies for their mitigation</title>
<link>https://arxiv.org/abs/2506.00047</link>
<guid>https://arxiv.org/abs/2506.00047</guid>
<content:encoded><![CDATA[
<div> progressing, automated product development, risks, mitigation strategies, AI-driven product development
<br />
Summary:
Humanity is moving towards automated product development to accelerate technological progress, but this trend poses risks that must be addressed. To mitigate these risks, principles for safer AI-driven product development are outlined, emphasizing human oversight, accountability, and explainable design. The risk assessment includes technical risks affecting product quality and safety, as well as sociotechnical risks impacting society. While AI-driven product development is still evolving, this discussion aims to balance opportunities and risks without hindering progress in understanding, norm-setting, and regulation. <div>
arXiv:2506.00047v1 Announce Type: cross 
Abstract: Humanity is progressing towards automated product development, a trend that promises faster creation of better products and thus the acceleration of technological progress. However, increasing reliance on non-human agents for this process introduces many risks. This perspective aims to initiate a discussion on these risks and appropriate mitigation strategies. To this end, we outline a set of principles for safer AI-driven product development which emphasize human oversight, accountability, and explainable design, among others. The risk assessment covers both technical risks which affect product quality and safety, and sociotechnical risks which affect society. While AI-driven product development is still in its early stages, this discussion will help balance its opportunities and risks without delaying essential progress in understanding, norm-setting, and regulation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance</title>
<link>https://arxiv.org/abs/2506.00494</link>
<guid>https://arxiv.org/abs/2506.00494</guid>
<content:encoded><![CDATA[
<div> Keywords: Soft Fin-Ray grippers, multi-objective optimization, finite element method, multilayer perception, non-dominated sorting genetic algorithm

Summary:<br />
Soft Fin-Ray grippers are effective for delicate manipulation but face challenges in modeling grasp force and deformation for design purposes. The study uses finite element method (FEM) to estimate deflections and contact forces of the gripper when grasping cylindrical objects, creating a dataset for predicting contact force and tip displacement using a multilayer perception (MLP). The dataset includes design variables related to beam thickness and spacing, with target features of maximum contact forces and tip displacements. A multi-objective optimization problem is addressed, balancing the trade-off between force and delicate manipulation. The non-dominated sorting genetic algorithm (NSGA-II) is used to find optimized design solutions. The methodologies presented in the study can enhance the design and gripping performance of soft robotic grippers, aiding in choosing designs suitable for both delicate grasping and high-force applications.<br />Summary: <div>
arXiv:2506.00494v1 Announce Type: cross 
Abstract: Soft Fin-Ray grippers can perform delicate and careful manipulation, which has caused notable attention in different fields. These grippers can handle objects of various forms and sizes safely. The internal structure of the Fin-Ray finger plays a significant role in its adaptability and grasping performance. However, modeling the non-linear grasp force and deformation behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger becomes more rigid and capable of exerting higher forces, it becomes less delicate in handling objects. The contrast between these two objectives gives rise to a multi-objective optimization problem. In this study, we employ finite element method (FEM) to estimate the deflections and contact forces of the Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a multilayer perception (MLP) for prediction of the contact force and the tip displacement. The FEM dataset consists of three input and four target features. The three input features of the MLP and optimization design variables are the thickness of the front and supporting beams, the thickness of the cross beams, and the equal spacing between the cross beams. In addition, the target features are the maximum contact forces and maximum tip displacements in x- and y-directions. The magnitude of maximum contact force and magnitude of maximum tip displacement are the two objectives, showing the trade-off between force and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized set of solutions are found using multi-objective optimal techniques. We use non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our findings demonstrate that our methodologies can be used to improve the design and gripping performance of soft robotic grippers, helping us to choose a design not only for delicate grasping but also for high-force applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling the Spread of Epidemics on Networks with Differential Privacy</title>
<link>https://arxiv.org/abs/2506.00745</link>
<guid>https://arxiv.org/abs/2506.00745</guid>
<content:encoded><![CDATA[
<div> strategies, epidemic spread, vaccination, contact network, differential privacy<br />
<br />
Summary: Designing effective vaccination strategies for controlling epidemic spread on heterogeneous contact networks is crucial, especially when sensitive information is involved and privacy guarantees are needed. This study introduces $(\varepsilon,\delta)$-differentially private algorithms for reducing the maximum degree and spectral radius to design optimal vaccination strategies. A private algorithm for the multi-set multi-cover problem is developed to control network properties while preserving privacy. The tradeoff between privacy and utility of these algorithms is evaluated on various synthetic and real-world networks, demonstrating their effectiveness in controlling epidemic spread while maintaining privacy. <div>
arXiv:2506.00745v1 Announce Type: cross 
Abstract: Designing effective strategies for controlling epidemic spread by vaccination is an important question in epidemiology, especially in the early stages when vaccines are limited. This is a challenging question when the contact network is very heterogeneous, and strategies based on controlling network properties, such as the degree and spectral radius, have been shown to be effective. Implementation of such strategies requires detailed information on the contact structure, which might be sensitive in many applications. Our focus here is on choosing effective vaccination strategies when the edges are sensitive and differential privacy guarantees are needed. Our main contributions are $(\varepsilon,\delta)$-differentially private algorithms for designing vaccination strategies by reducing the maximum degree and spectral radius. Our key technique is a private algorithm for the multi-set multi-cover problem, which we use for controlling network properties. We evaluate privacy-utility tradeoffs of our algorithms on multiple synthetic and real-world networks, and show their effectiveness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking</title>
<link>https://arxiv.org/abs/2506.01093</link>
<guid>https://arxiv.org/abs/2506.01093</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time transaction monitoring, graph-based modeling, narrative field embedding, generative explanation, financial compliance <br />
Summary: 
The paper introduces a real-time transaction monitoring framework that combines graph-based modeling, narrative field embedding, and generative explanation to automate financial compliance. The system creates dynamic transaction graphs, extracts features, and detects suspicious behavior using a graph neural network. It also generates natural language explanations aligned with regulatory clauses for flagged transactions. Experimental results on simulated financial data demonstrate high performance metrics with a 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation confirms the quality and interpretability of generated justifications. The study showcases the potential of integrating graph intelligence and generative models for explainable compliance in high-risk financial settings. <br /><br />Summary: <div>
arXiv:2506.01093v1 Announce Type: cross 
Abstract: This paper presents a real-time transaction monitoring framework that integrates graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. The system constructs dynamic transaction graphs, extracts structural and contextual features, and classifies suspicious behavior using a graph neural network. A retrieval-augmented generation module generates natural language explanations aligned with regulatory clauses for each flagged transaction. Experiments conducted on a simulated stream of financial data show that the proposed method achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation further confirms the quality and interpretability of generated justifications. The findings demonstrate the potential of combining graph intelligence and generative models to support explainable, audit-ready compliance in high-risk financial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents</title>
<link>https://arxiv.org/abs/2506.01900</link>
<guid>https://arxiv.org/abs/2506.01900</guid>
<content:encoded><![CDATA[
<div> framework, autonomous LLM agents, cost optimization, task outsourcing, agent economies

Summary:
COALESCE is a framework designed to optimize resource utilization in autonomous Large Language Model (LLM) agents by enabling them to outsource specific subtasks to cost-effective third-party agents. The framework incorporates hybrid skill representation, dynamic skill discovery, task decomposition, cost comparison models, decision-making algorithms, and a communication protocol. The theoretical simulations show a 41.8% cost reduction potential, while empirical validation confirms a 20.3% cost reduction with epsilon-greedy exploration. The framework aims to leverage open standards like Google's Agent2Agent protocol to foster efficient agent interactions, reduce operational costs, enhance scalability, and create specialized agent economies. By facilitating a dynamic market for agent capabilities, COALESCE enables complex LLM agent functionalities to become more accessible and economically viable. 

<br /><br />Summary: <div>
arXiv:2506.01900v1 Announce Type: cross 
Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Guided Diffusion Model for Accelerating Computational Fluid Dynamics</title>
<link>https://arxiv.org/abs/2504.04375</link>
<guid>https://arxiv.org/abs/2504.04375</guid>
<content:encoded><![CDATA[
<div> diffusion models, fluid dynamics computation, machine learning, numerical solvers, turbulent flow<br />
Summary:<br />
Machine learning methods, such as diffusion models, aim to accelerate high-fidelity fluid dynamics computation by utilizing low-fidelity data produced by numerical solvers. However, existing approaches struggle to reconstruct fine-scale details when using solver-generated low-fidelity inputs. To address this issue, SG-Diff, a novel diffusion model, is proposed. It incorporates an Importance Weight strategy during training to focus on intricate fluid details and a Predictor-Corrector-Advancer SDE solver to embed physical guidance into the diffusion sampling process. Experimental results on turbulent flow datasets demonstrate SG-Diff's effectiveness in achieving more accurate reconstructions compared to state-of-the-art baselines. <br />Summary: <div>
arXiv:2504.04375v2 Announce Type: replace 
Abstract: Machine learning methods, such as diffusion models, are widely explored as a promising way to accelerate high-fidelity fluid dynamics computation via a super-resolution process from faster-to-compute low-fidelity input. However, existing approaches usually make impractical assumptions that the low-fidelity data is down-sampled from high-fidelity data. In reality, low-fidelity data is produced by numerical solvers that use a coarser resolution. Solver-generated low-fidelity data usually sacrifices fine-grained details, such as small-scale vortices compared to high-fidelity ones. Our findings show that SOTA diffusion models struggle to reconstruct fine-scale details when faced with solver-generated low-fidelity inputs. To bridge this gap, we propose SG-Diff, a novel diffusion model for reconstruction, where both low-fidelity inputs and high-fidelity targets are generated from numerical solvers. We propose an \textit{Importance Weight} strategy during training that serves as a form of self-guidance, focusing on intricate fluid details, and a \textit{Predictor-Corrector-Advancer} SDE solver that embeds physical guidance into the diffusion sampling process. Together, these techniques steer the diffusion model toward more accurate reconstructions. Experimental results on four 2D turbulent flow datasets demonstrate the efficacy of \model~against state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Business Analytics: A Clash of Expectations and Reality</title>
<link>https://arxiv.org/abs/2205.09337</link>
<guid>https://arxiv.org/abs/2205.09337</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, machine learning, structured datasets, gradient boosting, business analytics

Summary:
Deep learning, a popular tool in the field of artificial intelligence and machine learning, faces challenges in widespread adoption within business analytics. These challenges include computational complexity, lack of big data architecture, black-box transparency, skill shortages, and leadership commitment. Despite its benefits, deep learning may not outperform traditional machine learning models for structured datasets with fixed-length feature vectors. The study suggests that gradient boosting models are more suitable for making predictions on structured datasets in business analytics. This finding highlights the importance of viewing deep learning as a complement to existing machine learning models rather than a universal solution. The paper provides insights from empirical studies in three industry use cases, discussing practical implications and outlining future research directions. 

<br /><br />Summary: <div>
arXiv:2205.09337v2 Announce Type: replace-cross 
Abstract: Our fast-paced digital economy shaped by global competition requires increased data-driven decision-making based on artificial intelligence (AI) and machine learning (ML). The benefits of deep learning (DL) are manifold, but it comes with limitations that have, so far, interfered with widespread industry adoption. This paper explains why DL, despite its popularity, has difficulties speeding up its adoption within business analytics. It is shown that the adoption of deep learning is not only affected by computational complexity, lacking big data architecture, lack of transparency (black-box), skill shortage, and leadership commitment, but also by the fact that DL does not outperform traditional ML models in the case of structured datasets with fixed-length feature vectors. Deep learning should be regarded as a powerful addition to the existing body of ML models instead of a one size fits all solution. The results strongly suggest that gradient boosting can be seen as the go-to model for predictions on structured datasets within business analytics. In addition to the empirical study based on three industry use cases, the paper offers a comprehensive discussion of those results, practical implications, and a roadmap for future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated machine learning: AI-driven decision making in business analytics</title>
<link>https://arxiv.org/abs/2205.10538</link>
<guid>https://arxiv.org/abs/2205.10538</guid>
<content:encoded><![CDATA[
<div> AutoML, industrial machine learning, H2O AutoML framework, business analytics, automated decision-making <br />
Summary: <br />
The rise of AI-driven decision-making in the business world has led to a surge in interest in industrial machine learning applications. The shortage of analytics experts can be addressed by enhancing the user-friendliness of ML frameworks. Automated machine learning (AutoML) offers a solution by providing automated off-the-shelf solutions for model selection and hyperparameter tuning. In a study comparing the H2O AutoML framework with a manually tuned stacked ML model, the manual model outperformed in all three case studies, but the H2O AutoML package showed promising results. It is fast, easy to use, and delivers reliable results close to a professionally tuned model. This tool can aid in fast prototyping, shorten development cycles, and bridge the gap between demand and supply for ML experts. AutoML has the potential to empower individuals in an increasingly automated and digital world. <br /> <div>
arXiv:2205.10538v2 Announce Type: replace-cross 
Abstract: The realization that AI-driven decision-making is indispensable in today's fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards automated decisions in business analytics. Finally, AutoML has the potential to foster human empowerment in a world that is rapidly becoming more automated and digital.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETS-C: Leveraging Text Embedding for Time Series Classification</title>
<link>https://arxiv.org/abs/2407.06533</link>
<guid>https://arxiv.org/abs/2407.06533</guid>
<content:encoded><![CDATA[
<div> Keywords: language modeling, time series data, text embedding model, convolutional neural networks, lightweight model architecture <br />
Summary: 
This study introduces a novel approach for time series classification using a text embedding model in conjunction with a lightweight classification head. While previous methods focused on fine-tuning large language models for time series data, this new approach, called LETS-C, outperforms the state-of-the-art models in classification accuracy. The LETS-C model combines text embeddings with a simple classification head composed of convolutional neural networks and multilayer perceptron. By utilizing text embeddings to encode time series data, the LETS-C model achieves high performance while keeping a lightweight architecture. Through extensive experiments on a well-established time series classification benchmark, it was found that LETS-C requires only 14.5% of the trainable parameters compared to the current SOTA model. This suggests that leveraging text embedding models for time series classification tasks presents a promising direction for achieving high performance with a simpler model architecture. <br /><br />Summary: <div>
arXiv:2407.06533v2 Announce Type: replace-cross 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much is Enough? The Diminishing Returns of Tokenization Training Data</title>
<link>https://arxiv.org/abs/2502.20273</link>
<guid>https://arxiv.org/abs/2502.20273</guid>
<content:encoded><![CDATA[
<div> tokenization, hyperparameter, training data size, BPE, UnigramLM, WordPiece
<br />
Summary: 
This study examines the impact of tokenizer training data size on tokenization quality in natural language processing. By training BPE, UnigramLM, and WordPiece tokenizers on varying English training data sizes from 1GB to 900GB, it is found that improvements in tokenization quality diminish beyond 150GB due to constraints introduced by the pre-tokenization stage. The saturation effect is observed in both English and Russian data, indicating a practical limit to tokenization quality improvements achievable through increasing data size. This insight can guide the optimization of tokenization processes, reducing the computational resources needed for training on large corpora. Further research directions in tokenization algorithms are suggested based on these findings. 
<br /> <div>
arXiv:2502.20273v2 Announce Type: replace-cross 
Abstract: Tokenization, a crucial initial step in natural language processing, is governed by several key parameters, such as the tokenization algorithm, vocabulary size, pre-tokenization strategy, inference strategy, and training data corpus. This paper investigates the impact of an often-overlooked hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and WordPiece tokenizers across various vocabulary sizes using English training data ranging from 1GB to 900GB. Our findings reveal diminishing returns as training data size increases beyond roughly 150GB, suggesting a practical limit to the improvements in tokenization quality achievable through additional data. We analyze this phenomenon and attribute the saturation effect to constraints introduced by the pre-tokenization stage. We then demonstrate the extent to which these findings can generalize by experimenting on data in Russian, a language typologically distant from English. While the limit appears to materialize at a later phase of pre-training, around 200GB, it is in fact observed. These results provide valuable insights for optimizing the tokenization process by reducing the compute required for training on large corpora and suggest promising directions for future research in tokenization algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singularity Protocol for Cross Chain AMM without Intermediate Tokens or Bridges</title>
<link>https://arxiv.org/abs/2505.24337</link>
<guid>https://arxiv.org/abs/2505.24337</guid>
<content:encoded><![CDATA[
<div> AMMs, decentralized exchange, cross-chain swaps, liquidity, blockchain<br />
Summary:<br />
Automated Market Makers (AMMs) have revolutionized decentralized exchanges but face scalability challenges with cross-chain swaps. The current double-sided AMMs are inefficient and introduce risks like volatility and blockchain issues. This paper proposes a new class of AMMs that eliminate the need for intermediate tokens or bridging, enabling efficient cross-chain swaps with lower gas requirements. The new technology is based on an invariant that does not rely on bi-state dependency between assets being swapped. This innovation supports cross-chain swaps across various blockchain layers and offers a more streamlined and secure method for value transfer swaps. The proposed solution addresses the limitations of existing AMMs and provides a promising approach for improving liquidity and efficiency in cross-chain transactions. <br /><br /> <div>
arXiv:2505.24337v1 Announce Type: new 
Abstract: Automated Market Makers (AMMs) are decentralized exchange protocols that provide continuous access to token liquidity without the need for order books or traditional market makers. However, this innovation has failed to scale when it comes to cross-chain swaps. Modern cross-chain swaps employ double-sided AMMs, which are not only inefficient due to liquidity fragmentation but also require an intermediate token. This introduces inherent volatility risk as well as blockchain and bridging risk, especially in the case of wrapped tokens. This paper describes the inefficiencies of existing AMM invariants, particularly their mixed polynomial nature, and derives a new class of AMMs that do not have bi-state dependency between the assets being swapped. We propose a novel method of value transfer swaps using the described invariant that mitigates the need for bi-state dependency and eliminates the need for intermediate tokens or bridging. Furthermore, we show how this mechanism enables efficient cross-chain swaps with lower gas requirements and no bridging risks. The proposed technology is designed to support cross-chain swaps across any permutation of L1, L2, and L3 blockchains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Black Box: Interpretability of LLMs in Finance</title>
<link>https://arxiv.org/abs/2505.24650</link>
<guid>https://arxiv.org/abs/2505.24650</guid>
<content:encoded><![CDATA[
<div> interpretability, financial services, large language models, transparency, regulatory compliance

Summary: 
This paper introduces the concept of mechanistic interpretability in the context of Large Language Models (LLMs) in the financial services sector. LLMs have shown great potential in various financial tasks but their complexity and lack of transparency raise concerns in the regulated financial industry. Mechanistic interpretability offers a transparent way to understand LLM behavior by reverse-engineering their internal workings, providing insights into how specific features influence predictions and allowing for the modification of model behavior. The paper explores the theoretical aspects of mechanistic interpretability and demonstrates its practical relevance through financial use cases such as trading strategies, sentiment analysis, bias detection, and hallucination detection. The adoption of advanced interpretability tools is expected to be crucial as LLM usage increases, ensuring that AI systems in finance are ethical, transparent, and compliant with evolving regulations. The paper emphasizes how these techniques can address interpretability requirements for regulatory and compliance purposes in the financial sector. 

<br /><br />Summary: <div>
arXiv:2505.24650v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a spectrum of tasks in financial services, including report generation, chatbots, sentiment analysis, regulatory compliance, investment advisory, financial knowledge retrieval, and summarization. However, their intrinsic complexity and lack of transparency pose significant challenges, especially in the highly regulated financial sector, where interpretability, fairness, and accountability are critical. As far as we are aware, this paper presents the first application in the finance domain of understanding and utilizing the inner workings of LLMs through mechanistic interpretability, addressing the pressing need for transparency and control in AI systems. Mechanistic interpretability is the most intuitive and transparent way to understand LLM behavior by reverse-engineering their internal workings. By dissecting the activations and circuits within these models, it provides insights into how specific features or components influence predictions - making it possible not only to observe but also to modify model behavior. In this paper, we explore the theoretical aspects of mechanistic interpretability and demonstrate its practical relevance through a range of financial use cases and experiments, including applications in trading strategies, sentiment analysis, bias, and hallucination detection. While not yet widely adopted, mechanistic interpretability is expected to become increasingly vital as adoption of LLMs increases. Advanced interpretability tools can ensure AI systems remain ethical, transparent, and aligned with evolving financial regulations. In this paper, we have put special emphasis on how these techniques can help unlock interpretability requirements for regulatory and compliance purposes - addressing both current needs and anticipating future expectations from financial regulators globally.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Bayesian multi-fidelity inverse analysis for expensive and non-differentiable physics-based simulations in high stochastic dimensions</title>
<link>https://arxiv.org/abs/2505.24708</link>
<guid>https://arxiv.org/abs/2505.24708</guid>
<content:encoded><![CDATA[
<div> Bayesian, multi-fidelity, inverse analysis, high-dimensional, computational<br />
Summary:<br />
This article introduces a novel approach called Bayesian multi-fidelity inverse analysis (BMFIA) to address the challenges of high-dimensional Bayesian inverse analysis for computationally demanding, nonlinear physics-based high-fidelity models. The method leverages simpler lower-fidelity models designed to provide model derivatives, learning a probabilistic dependence between the lower and higher-fidelity models. This allows for statistically correcting the inaccurate lower-fidelity responses in an altered likelihood formulation. BMFIA is fully differentiable and can be applied to a wide range of scenarios, including finely-resolved spatial reconstruction problems for nonlinear and transient coupled poro-elastic media physics. The approach is demonstrated to solve Bayesian inverse problems efficiently, even with a small amount of data, making it a valuable tool for addressing complex multi-physics problems. <div>
arXiv:2505.24708v1 Announce Type: new 
Abstract: High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible for computationally demanding, nonlinear physics-based high-fidelity (HF) models. Usually, the use of more efficient gradient-based inference schemes is impeded if the multi-physics models are provided by complex legacy codes. Adjoint-based derivatives are either exceedingly cumbersome to derive or non-existent for practically relevant large-scale nonlinear and coupled multi-physics problems. Similarly, holistic automated differentiation w.r.t. primary variables of multi-physics codes is usually not yet an option and requires extensive code restructuring if not considered from the outset in the software design. This absence of differentiability further exacerbates the already present computational challenges. To overcome the existing limitations, we propose a novel inference approach called Bayesian multi-fidelity inverse analysis (BMFIA), which leverages simpler and computationally cheaper lower-fidelity (LF) models that are designed to provide model derivatives. BMFIA learns a simple, probabilistic dependence of the LF and HF models, which is then employed in an altered likelihood formulation to statistically correct the inaccurate LF response. From a Bayesian viewpoint, this dependence represents a multi-fidelity conditional density (discriminative model). We demonstrate how this multi-fidelity conditional density can be learned robustly in the small data regime from only a few HF and LF simulations (50 to 300), which would not be sufficient for naive surrogate approaches. The formulation is fully differentiable and allows the flexible design of a wide range of LF models. We demonstrate that BMFIA solves Bayesian inverse problems for scenarios that used to be prohibitive, such as finely-resolved spatial reconstruction problems for nonlinear and transient coupled poro-elastic media physics.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Soupability of Documents in State Space Models</title>
<link>https://arxiv.org/abs/2505.24033</link>
<guid>https://arxiv.org/abs/2505.24033</guid>
<content:encoded><![CDATA[
<div> Keywords: Structured State Space Models, document souping, Mamba2 models, multi-hop QA, long-document reasoning

Summary:
Structured State Space Models (SSMs) are investigated to determine if their hidden states can be merged post-hoc to support downstream reasoning. A strategy called document souping is proposed, where documents are encoded independently and their representations are pooled into a single context state. This approach allows for modular encoding and reuse without the need to reprocess the full input for each query. Mamba2 models are modified to produce soupable representations, enabling support for multi-hop QA, sparse retrieval, and long-document reasoning with high accuracy. In experiments on HotpotQA, souping ten independently encoded documents achieves performance near that of a cross-encoder trained on the same inputs. Overall, this method demonstrates the effectiveness of combining independently encoded document representations for improved downstream reasoning tasks. 

Summary: <div>
arXiv:2505.24033v1 Announce Type: cross 
Abstract: We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Proximity: A Graph-Based Approach to Blockchain Fraud Prevention</title>
<link>https://arxiv.org/abs/2505.24284</link>
<guid>https://arxiv.org/abs/2505.24284</guid>
<content:encoded><![CDATA[
<div> Keywords: fraud-deterrent, public blockchains, transaction proximity, Easily Attainable Identities (EAIs), directed graph analysis

Summary: 
This paper presents a fraud-deterrent access validation system for public blockchains using Transaction Proximity and Easily Attainable Identities (EAIs) concepts. The system analyzes transaction patterns to identify wallets closely connected to centralized exchanges, aiming to prevent fraudulent activities. The analysis of the Ethereum blockchain reveals a high percentage of large USDC wallets being EAI or within one transaction hop of an EAI. Moreover, a significant number of past exploits were found to not involve EAIs, highlighting the need for such a validation system. Three implementation approaches are proposed, balancing gas cost and privacy considerations. This approach allows for programmatic compliance without restricting access or sharing personal information, maintaining blockchain openness and enabling protocols to implement customized validation systems.<br /><br />Summary: <div>
arXiv:2505.24284v1 Announce Type: cross 
Abstract: This paper introduces a fraud-deterrent access validation system for public blockchains, leveraging two complementary concepts: "Transaction Proximity", which measures the distance between wallets in the transaction graph, and "Easily Attainable Identities (EAIs)", wallets with direct transaction connections to centralized exchanges. Recognizing the limitations of traditional approaches like blocklisting (reactive, slow) and strict allow listing (privacy-invasive, adoption barriers), we propose a system that analyzes transaction patterns to identify wallets with close connections to centralized exchanges.
  Our directed graph analysis of the Ethereum blockchain reveals that 56% of large USDC wallets (with a lifetime maximum balance greater than \$10,000) are EAI and 88% are within one transaction hop of an EAI. For transactions exceeding \$2,000, 91% involve at least one EAI. Crucially, an analysis of past exploits shows that 83% of the known exploiter addresses are not EAIs, with 21% being more than five hops away from any regulated exchange. We present three implementation approaches with varying gas cost and privacy tradeoffs, demonstrating that EAI-based access control can potentially prevent most of these incidents while preserving blockchain openness. Importantly, our approach does not restrict access or share personally identifiable information, but it provides information for protocols to implement their own validation or risk scoring systems based on specific needs. This middle-ground solution enables programmatic compliance while maintaining the core values of open blockchain.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking for Attention: Randomized Attention Test Design for Validator Monitoring in Optimistic Rollups</title>
<link>https://arxiv.org/abs/2505.24393</link>
<guid>https://arxiv.org/abs/2505.24393</guid>
<content:encoded><![CDATA[
<div> scalability, blockchain, Optimistic Rollups, Randomized Attention Test, game-theoretic analysis

Summary:
Optimistic Rollups (ORUs) enhance blockchain scalability but face the verifier's dilemma due to a lack of mechanisms ensuring validator attentiveness. The Randomized Attention Test (RAT) protocol is introduced to challenge validators in ORUs, verifying their liveness and readiness. Game-theoretic analysis shows that an Ideal Security Equilibrium can be achieved with RAT, where validators are attentive and proposers honest. This equilibrium is attainable with low penalties for non-responsive validators and a low attention test frequency. RAT serves as a practical mechanism to enforce validator diligence, increasing the security and integrity of ORU systems without significant additional costs. <div>
arXiv:2505.24393v1 Announce Type: cross 
Abstract: Optimistic Rollups (ORUs) significantly enhance blockchain scalability but inherently suffer from the verifier's dilemma, particularly concerning validator attentiveness. Current systems lack mechanisms to proactively ensure validators are diligently monitoring L2 state transitions, creating a vulnerability where fraudulent states could be finalized. This paper introduces the Randomized Attention Test (RAT), a novel L1-based protocol designed to probabilistically challenge validators in ORUs, thereby verifying their liveness and computational readiness. Our game-theoretic analysis demonstrates that an Ideal Security Equilibrium, where all validators are attentive and proposers are honest, can be achieved with RAT. Notably, this equilibrium is attainable and stable with relatively low economic penalties (e.g., under $1000) for non-responsive validators and a low attention test frequency (e.g., under 1% per epoch). RAT thus provides a crucial, practical mechanism to enforce validator diligence, fortifying the overall security and integrity of ORU systems with minimizing additional costs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV</title>
<link>https://arxiv.org/abs/2505.24781</link>
<guid>https://arxiv.org/abs/2505.24781</guid>
<content:encoded><![CDATA[
<div> Regularized Tyler's M-estimator, shrinkage coefficient, leave-one-out cross-validation, computational efficiency, high-dimensional data<br />
<br />
Summary: <br />
The study focuses on estimating a shrinkage coefficient for Regularized Tyler's M-estimator using leave-one-out cross-validation (LOOCV) log-likelihood loss. The proposed approach aims to find an optimal shrinkage coefficient by solving a selected objective function, enhancing computational efficiency by approximating the LOOCV log-likelihood loss. This approximation significantly reduces the running time complexity for the LOOCV procedure by O(n), offering a faster computation of the LOOCV estimate. The efficiency and accuracy of the method were demonstrated through synthetic high-dimensional data and real datasets for object recognition, face recognition, and handwritten digit recognition. Results indicate the proposed approach is both efficient and more precise compared to existing methods for shrinkage coefficient estimation. <div>
arXiv:2505.24781v1 Announce Type: cross 
Abstract: We consider the problem of estimating a regularization parameter, or a shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator (RTME). In particular, we propose to estimate an optimal shrinkage coefficient by setting $\alpha$ as the solution to a suitably chosen objective function; namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since LOOCV is computationally prohibitive even for moderate sample size $n$, we propose a computationally efficient approximation for the LOOCV log-likelihood loss that eliminates the need for invoking the RTME procedure $n$ times for each sample left out during the LOOCV procedure. This approximation yields an $O(n)$ reduction in the running time complexity for the LOOCV procedure, which results in a significant speedup for computing the LOOCV estimate. We demonstrate the efficiency and accuracy of the proposed approach on synthetic high-dimensional data sampled from heavy-tailed elliptical distributions, as well as on real high-dimensional datasets for object recognition, face recognition, and handwritten digit's recognition. Our experiments show that the proposed approach is efficient and consistently more accurate than other methods in the literature for shrinkage coefficient estimation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpolating Neural Network-Tensor Decomposition (INN-TD): a scalable and interpretable approach for large-scale physics-based problems</title>
<link>https://arxiv.org/abs/2503.02041</link>
<guid>https://arxiv.org/abs/2503.02041</guid>
<content:encoded><![CDATA[
<div> Interpolating Neural Network-Tensor Decomposition, scalable, interpretable, machine learning, finite element methods, large-scale physical systems
Summary:
Interpolating Neural Network-Tensor Decomposition (INN-TD) is introduced as a framework that combines machine learning and finite element methods to model large-scale physical systems accurately and efficiently. By incorporating locally supported interpolation functions from finite element methods into the network architecture, INN-TD achieves a sparse learning structure, leading to enhanced accuracy, faster training/solving speed, and reduced memory usage. This framework is well-suited for addressing large-scale high-dimensional parametric partial differential equations in physical problems that require high precision in tasks such as training, solving, and inverse optimization. Its effectiveness lies in its ability to provide interpretable solutions for industrial problems while maintaining computational efficiency and accuracy in modeling complex physics-based systems. <br /><br />Summary: <div>
arXiv:2503.02041v3 Announce Type: replace 
Abstract: Deep learning has been extensively employed as a powerful function approximator for modeling physics-based problems described by partial differential equations (PDEs). Despite their popularity, standard deep learning models often demand prohibitively large computational resources and yield limited accuracy when scaling to large-scale, high-dimensional physical problems. Their black-box nature further hinders the application in industrial problems where interpretability and high precision are critical. To overcome these challenges, this paper introduces Interpolating Neural Network-Tensor Decomposition (INN-TD), a scalable and interpretable framework that has the merits of both machine learning and finite element methods for modeling large-scale physical systems. By integrating locally supported interpolation functions from finite element into the network architecture, INN-TD achieves a sparse learning structure with enhanced accuracy, faster training/solving speed, and reduced memory footprint. This makes it particularly effective for tackling large-scale high-dimensional parametric PDEs in training, solving, and inverse optimization tasks in physical problems where high precision is required.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data</title>
<link>https://arxiv.org/abs/2505.22670</link>
<guid>https://arxiv.org/abs/2505.22670</guid>
<content:encoded><![CDATA[
<div> semantic-spatial-topological, BIM models, network-based representation, IFC, design patterns

Summary:
This study introduces a unified network-based representation method for BIM models to capture complex spatial and topological relationships between components. By extending the IFC standard, the method incorporates local spatial relationships and topological connections, enriching the network structure. This approach enhances understanding of component interactions, dependencies, and design patterns in BIM models. The proposed representation method effectively captures semantic, topological, and spatial relationships, offering significant potential for learning design patterns in construction industry. <div>
arXiv:2505.22670v1 Announce Type: new 
Abstract: Building Information Modeling (BIM) has revolutionized the construction industry by providing a comprehensive digital representation of building structures throughout their lifecycle. However, existing research lacks effective methods for capturing the complex spatial and topological relationships between components in BIM models, which are essential for understanding design patterns and enhancing decision-making. This study proposes a unified network-based representation method that integrates the "semantic-spatial-topological" multi-dimensional design features of BIM models. By extending the IFC (Industry Foundation Classes) standard, we introduce local spatial relationships and topological connections between components to enrich the network structure. This representation method enables a more detailed understanding of component interactions, dependencies, and implicit design patterns, effectively capturing the semantic, topological, and spatial relationships in BIM, and holds significant potential for the representation and learning of design patterns.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive analysis of PINNs: Variants, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2505.22761</link>
<guid>https://arxiv.org/abs/2505.22761</guid>
<content:encoded><![CDATA[
<div> Physics Informed Neural Networks (PINNs), differential equations, architecture, variants, application, challenges

Summary:
Physics Informed Neural Networks (PINNs) are a powerful computational tool for solving differential equations. This survey provides a comprehensive overview of PINNs, covering architecture, variants, applications, real-world use cases, and challenges. Existing surveys lack detail, but this one offers a thorough analysis of PINNs, including recent advancements and research. Three main contributions are discussed: an in-depth look at PINNs architecture and variants, performance analysis on various equations and applications, and a discussion of current issues and future research directions. This survey aims to standardize and popularize the use of PINNs by addressing key areas and providing valuable insights for the field moving forward.<br /><br />Summary: <div>
arXiv:2505.22761v1 Announce Type: new 
Abstract: Physics Informed Neural Networks (PINNs) have been emerging as a powerful computational tool for solving differential equations. However, the applicability of these models is still in its initial stages and requires more standardization to gain wider popularity. Through this survey, we present a comprehensive overview of PINNs approaches exploring various aspects related to their architecture, variants, areas of application, real-world use cases, challenges, and so on. Even though existing surveys can be identified, they fail to provide a comprehensive view as they primarily focus on either different application scenarios or limit their study to a superficial level. This survey attempts to bridge the gap in the existing literature by presenting a detailed analysis of all these factors combined with recent advancements and state-of-the-art research in PINNs. Additionally, we discuss prevalent challenges in PINNs implementation and present some of the future research directions as well. The overall contributions of the survey can be summarised into three sections: A detailed overview of PINNs architecture and variants, a performance analysis of PINNs on different equations and application domains highlighting their features. Finally, we present a detailed discussion of current issues and future research directions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution analysis of software quality metrics in an open-source java project: A case study on TestNG</title>
<link>https://arxiv.org/abs/2505.22884</link>
<guid>https://arxiv.org/abs/2505.22884</guid>
<content:encoded><![CDATA[
<div> Keywords: software quality, TestNG, Java, object-oriented metrics, static analysis<br />
Summary:<br />
This study examines the evolution of software quality metrics in five versions of the TestNG Java testing framework. Using Understand, key object-oriented metrics were analyzed, showing trends such as increased stability and maintainability. The results suggest ongoing development, refactoring, and architectural improvements have contributed to TestNG's maturity over time. This study offers insights on design evolution and recommendations for maintaining code quality in similar projects. <div>
arXiv:2505.22884v1 Announce Type: cross 
Abstract: Software quality is critical in modern software engineering, especially in large and evolving codebases. This study analyzes the evolution of software quality metrics in five successive versions of the open-source Java testing framework TestNG. Using the static analysis tool Understand, eleven key object-oriented metrics, including cyclomatic complexity, class coupling, and lines of code, were extracted for each version. Statistical and visual analyses reveal structural trends over time. The results indicate that TestNG has matured into a more stable and maintainable framework, reflecting ongoing development, refactoring, and architectural improvements. This study provides insights into design evolution and offers recommendations for maintaining code quality in similar projects.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Be.FM: Open Foundation Models for Human Behavior</title>
<link>https://arxiv.org/abs/2505.23058</link>
<guid>https://arxiv.org/abs/2505.23058</guid>
<content:encoded><![CDATA[
<div> modeling, human behavior, decision-making, benchmark tasks, behavioral science

Summary:
Be.FM is introduced as an open foundation model for human behavior modeling, using large language models and fine-tuning on diverse behavioral data. It has the potential to predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge. Through comprehensive benchmark tasks, Be.FM demonstrates its capabilities in understanding and predicting human decision-making. This opens up possibilities for leveraging foundation models in various fields to gain insights into human behavior. <div>
arXiv:2505.23058v1 Announce Type: cross 
Abstract: Despite their success in numerous fields, the potential of foundation models for modeling and understanding human behavior remains largely unexplored. We introduce Be.FM, one of the first open foundation models designed for human behavior modeling. Built upon open-source large language models and fine-tuned on a diverse range of behavioral data, Be.FM can be used to understand and predict human decision-making. We construct a comprehensive set of benchmark tasks for testing the capabilities of behavioral foundation models. Our results demonstrate that Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid subgradient and simulated annealing method for hemivariational inequalities</title>
<link>https://arxiv.org/abs/2505.23676</link>
<guid>https://arxiv.org/abs/2505.23676</guid>
<content:encoded><![CDATA[
<div> global aggregate subgradient method, hemivariational inequality problems, contact mechanics, local minimization algorithm, performance comparison

Summary:
The paper introduces a global aggregate subgradient method for solving hemivariational inequality problems in contact mechanics. It combines global search capabilities to determine starting points for a local minimization algorithm. The algorithm incorporates null steps, using aggregate and current iteration subgradients to find the search direction, as well as serious steps. The method's performance is evaluated against other solvers using a representative contact mechanics problem. The study showcases the effectiveness of the proposed approach in efficiently solving complex contact mechanics problems compared to existing methods. <div>
arXiv:2505.23676v1 Announce Type: cross 
Abstract: In this paper, we employ a global aggregate subgradient method for the numerical solution of hemivariational inequality problems arising in contact mechanics. The method integrates a global search procedure to identify starting points for a local minimization algorithm. The algorithm consists of two types of steps: null steps and serious steps. In each null step, only two subgradients are utilized: the aggregate subgradient and the subgradient computed at the current iteration point, which together determine the search direction. Furthermore, we compare the performance of the proposed method with selected solvers using a representative contact mechanics problem as a case study.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computerized Modeling of Electrophysiology and Pathoelectrophysiology of the Atria -- How Much Detail is Needed?</title>
<link>https://arxiv.org/abs/2505.23717</link>
<guid>https://arxiv.org/abs/2505.23717</guid>
<content:encoded><![CDATA[
<div> Keywords: computerized modeling, atrial electrophysiology, arrhythmias, fibrotic tissue, atrial ablation<br />
Summary:<br />
This review focuses on computerized modeling of human atrial electrophysiology, specifically addressing common arrhythmias like atrial flutter and atrial fibrillation. The key question is identifying the necessary components for accurate simulation of arrhythmogenic tissue modifications, including remodeling, cardiomyopathy, and fibrosis. It examines the balance between model complexity and computational efficiency, emphasizing the risks of oversimplification and excessive detail. Various aspects of atrial modeling, from cellular to whole atria levels, are covered, considering factors like atrial geometry, fiber direction, anisotropy, and wall thickness. The impact of different modeling approaches and the latest advances in modeling fibrotic tissue are discussed, along with verification and validation methods. The use of these models in planning atrial ablation strategies, both personalized and cohort-based, is highlighted, stressing the importance of integrating experimental data and clinical validation for improved patient outcomes.<br /> <div>
arXiv:2505.23717v1 Announce Type: cross 
Abstract: This review focuses on the computerized modeling of the electrophysiology of the human atria, emphasizing the simulation of common arrhythmias such as atrial flutter (AFlut) and atrial fibrillation (AFib). Which components of the model are necessary to accurately model arrhythmogenic tissue modifications, including remodeling, cardiomyopathy, and fibrosis, to ensure reliable simulations? The central question explored is the level of detail required for trustworthy simulations for a specific context of use. The review discusses the balance between model complexity and computational efficiency, highlighting the risks of oversimplification and excessive detail. It covers various aspects of atrial modeling, from cellular to whole atria levels, including the influence of atrial geometry, fiber direction, anisotropy, and wall thickness on simulation outcomes. The article also examines the impact of different modeling approaches, such as volumetric 3D models, bilayer models, and single surface models, on the realism of simulations. In addition, it reviews the latest advances in the modeling of fibrotic tissue and the verification and validation of atrial models. The intended use of these models in planning and optimization of atrial ablation strategies is discussed, with a focus on personalized modeling for individual patients and cohort-based approaches for broader applications. The review concludes by emphasizing the importance of integrating experimental data and clinical validation to enhance the utility of computerized atrial models to improve patient outcomes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A blockchain-based intelligent recommender system framework for enhancing supply chain resilience</title>
<link>https://arxiv.org/abs/2404.00306</link>
<guid>https://arxiv.org/abs/2404.00306</guid>
<content:encoded><![CDATA[
<div> Intelligent recommender system technology, blockchain technology, smart contract, supply chain disruption, system dynamics simulation<br />
<br />
Summary: 
This research proposes a data-driven supply chain disruption response framework utilizing intelligent recommender system (IRS) technology integrated with blockchain (BLC) technology. A smart contract prototype demonstrates information exchange within a BLC network. An industrial case study implementation validates the BLC-IRS framework's effectiveness in responding to disruptions. A system dynamics simulation model confirms the framework's ability to mitigate disruptions in the supply chain response phase. This approach provides a practical digital solution for supply chain resilience, allowing participants to react swiftly and effectively to disruptions. By utilizing synthetic technologies, the BLC-IRS framework enhances the SCRes community's ability to access supplementary resource information in a secure and real-time manner following disruptions. <div>
arXiv:2404.00306v3 Announce Type: replace 
Abstract: This research proposed a data-driven supply chain disruption response baseline framework based on intelligent recommender system technology as an initial SCRes reactive solution. To improve the data quality and reliability of the proposed IRS as a stable, secure, and resilient decision support system, blockchain technology is integrated into the baseline architecture. The smart contract is prototyped to demonstrate the information exchange mechanism under a BLC network environment. The BLC-IRS framework is then implemented with an industrial case to demonstrate its executable function. A system dynamics (SD) simulation model is adopted to validate the BLC-IRS framework as an effective digital SCRes enhancement measure. The simulation results indicated that the proposed BLC-IRS framework can be effectively implemented as a SC disruption mitigation measure in the SCRes response phase as reactive measure, enabling SC participants to react better to SC disruptions at the physical level. Compared to previous studies that limited at the conceptual level as the proactive SCRes measure with a standalone fashion, the developed BLC-IRS contributes an executable SCRes digital solution with synthetic technologies as a reactive SCRes measure for the SCRes community, by identifying the internal and external supplementary resource information in an agile, safe, and real-time manner after SC disruption.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Perishable and Non-Perishable Product Assignment to Packaging Lines in a Sustainable Manufacturing System: An AUGMECON2VIKOR Algorithm</title>
<link>https://arxiv.org/abs/2410.21844</link>
<guid>https://arxiv.org/abs/2410.21844</guid>
<content:encoded><![CDATA[
<div> manufacturing systems, food industry, mathematical model, optimization, perishable products<br />
<br />
Summary: 
This study introduces a new mathematical model and assignment approach to optimize manufacturing systems for perishable, non-perishable, and hybrid products in the food industry. The model considers three objective functions: minimizing production costs, maximizing product quality, and reducing CO2 emissions. Comparing the proposed AUGMECON2VIKOR model to AUGMECON2, the former outperforms in generating superior Pareto solutions across all objectives. Additionally, a sensitivity analysis demonstrates the positive environmental impact, influencing cost and quality factors. By leveraging knowledge discovery and a tailored assignment model, this study offers a comprehensive approach to address the unique constraints of perishable products and enhance operational efficiency in manufacturing systems. <div>
arXiv:2410.21844v2 Announce Type: replace-cross 
Abstract: Identifying appropriate manufacturing systems for products can be considered a pivotal manufacturing task contributing to the optimization of operational and planning activities. It has gained importance in the food industry due to the distinct constraints and considerations posed by perishable and non-perishable items in this problem. Hence, this study proposes a new mathematical model according to knowledge discovery as well as an assignment model to optimize manufacturing systems for perishable, non-perishable, and hybrid products tailored to meet their unique characteristics. In the presented model, three objective functions are taken into account: (1) minimizing production costs by assigning the products to the right set of manufacturing systems, (2) maximizing the product quality by assigning the products to the systems, and (3) minimizing total CO2 emissions of the machines. A numerical example is utilized to evaluate the performance of AUGMECON2VIKOR compared to AUGMECON2. The results show that AUGMECON2VIKOR obtains superior Pareto solutions across all objective functions. Furthermore, the sensitivity analysis explores the positive green impacts, influencing both cost and quality.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Representation Learning for fMRI-based Neurological Disorder Identification</title>
<link>https://arxiv.org/abs/2412.16197</link>
<guid>https://arxiv.org/abs/2412.16197</guid>
<content:encoded><![CDATA[
<div> representation learning, meta-learning, self-supervised learning, functional Magnetic Resonance Imaging, neurological disorders<br />
Summary:<br />
The study addresses challenges in identifying neurological disorders due to data heterogeneity and scarcity. It introduces a novel representation learning approach that combines meta-learning and self-supervised learning to enhance generalization from normal to clinical features. By leveraging self-supervised learning on control data and incorporating meta-learning, the model can generalize to clinical tasks with limited training data. The approach is applied to four different clinical datasets for neurological disorder classification, demonstrating its effectiveness for diverse tasks. The public availability of the code allows for further exploration and application of the representation learning strategy. <div>
arXiv:2412.16197v2 Announce Type: replace-cross 
Abstract: Despite the impressive advances achieved using deep learning for functional brain activity analysis, the heterogeneity of functional patterns and the scarcity of imaging data still pose challenges in tasks such as identifying neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while data may be abundantly available from healthy controls, clinical data is often scarce, especially for rare diseases, limiting the ability of models to identify clinically-relevant features. We overcome this limitation by introducing a novel representation learning strategy integrating meta-learning with self-supervised learning to improve the generalization from normal to clinical features. This approach enables generalization to challenging clinical tasks featuring scarce training data. We achieve this by leveraging self-supervised learning on the control dataset to focus on inherent features that are not limited to a particular supervised task and incorporating meta-learning to improve the generalization across domains. To explore the generalizability of the learned representations to unseen clinical applications, we apply the model to four distinct clinical datasets featuring scarce and heterogeneous data for neurological disorder classification. Results demonstrate the superiority of our representation learning strategy on diverse clinically-relevant tasks. Code is publicly available at https://github.com/wenhui0206/MeTSK/tree/main
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling extreme events and intermittency in turbulent diffusion with a mean gradient</title>
<link>https://arxiv.org/abs/2505.21688</link>
<guid>https://arxiv.org/abs/2505.21688</guid>
<content:encoded><![CDATA[
<div> Keywords: passive tracer transport, turbulent flows, intermittency, extreme events, stochastic dynamics<br />
<br />
Summary: 
This study examines the statistical properties of passive tracer transport in turbulent flows with a mean gradient, focusing on tracer intermittency and extreme events. An analytically tractable model is developed, combining zonal and shear velocity components with linear and nonlinear stochastic dynamics. By formulating the model in Fourier space, the researchers derive a straightforward explicit solution for the tracer invariant statistics. They identify the resonance condition responsible for non-Gaussian behavior and bursts in the tracer, pinpointing the occurrence of peak tracer variance when the zonal flow and shear flow phase speeds are equal. Through numerical experiments across various regimes, the study validates these findings and showcases how the velocity field and stochasticity influence tracer extremes. These results offer valuable insights into the mechanisms governing turbulent tracer transport, which can significantly impact uncertainty quantification and data assimilation in geophysical and environmental applications.<br /><br />Summary: <div>
arXiv:2505.21688v1 Announce Type: new 
Abstract: We study the statistical properties of passive tracer transport in turbulent flows with a mean gradient, emphasizing tracer intermittency and extreme events. An analytically tractable model is developed, coupling zonal and shear velocity components with both linear and nonlinear stochastic dynamics. Formulating the model in Fourier space, a simple explicit solution for the tracer invariant statistics is derived. Through this model we identify the resonance condition responsible for non-Gaussian behavior and bursts in the tracer. Resonant conditions, that lead to a peak in the tracer variance, occur when the zonal flow and the shear flow phase speeds are equivalent. Numerical experiments across a range of regimes, including different energy spectra and zonal flow models, are performed to validate these findings and demonstrate how the velocity field and stochasticity determines tracer extremes. These results provide additional insight into the mechanisms underlying turbulent tracer transport, with implications for uncertainty quantification and data assimilation in geophysical and environmental applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero</title>
<link>https://arxiv.org/abs/2505.21536</link>
<guid>https://arxiv.org/abs/2505.21536</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, circular economy, materials, thermodynamics, Python library
<br />
Summary: 
CiRL is a deep reinforcement learning library that focuses on the circularity of solid and fluid materials to address the challenges of achieving a net zero target. It integrates DRL into material circularity design using thermodynamical material networks. The library features CE-oriented environments in the state-space form, based on the Stable-Baselines3 Python library, and developed in Google Colaboratory for accessibility to researchers. By leveraging DRL algorithms in the context of circular economy, CiRL aims to support the transition towards a more sustainable and efficient use of finite raw materials in modern society. <div>
arXiv:2505.21536v1 Announce Type: cross 
Abstract: The demand of finite raw materials will keep increasing as they fuel modern society. Simultaneously, solutions for stopping carbon emissions in the short term are not available, thus making the net zero target extremely challenging to achieve at scale. The circular economy (CE) paradigm is gaining attention as a solution to address climate change and the uncertainties of supplies of critical materials. Hence, in this paper, we introduce CiRL, a deep reinforcement learning (DRL) library of environments focused on the circularity of both solid and fluid materials. The integration of DRL into the design of material circularity is possible thanks to the formalism of thermodynamical material networks, which is underpinned by compartmental dynamical thermodynamics. Along with the focus on circularity, this library has three more features: the new CE-oriented environments are in the state-space form, which is typically used in dynamical systems analysis and control designs; it is based on a state-of-the-art Python library of DRL algorithms, namely, Stable-Baselines3; and it is developed in Google Colaboratory to be accessible to researchers from different disciplines and backgrounds as is often the case for circular economy researchers and engineers. CiRL is publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-Capping Metric Evaluation for Improving Energy Efficiency</title>
<link>https://arxiv.org/abs/2505.21758</link>
<guid>https://arxiv.org/abs/2505.21758</guid>
<content:encoded><![CDATA[
<div> power-scaling management, resource utilization, energy-performance metrics, GPU power-capping, exascale applications

Summary:
This paper delves into the optimization of power-scaling management and resource utilization in high-performance computing systems running at exascale. By leveraging integrated CPU-GPU power management on architectures like the NVIDIA GH200 superchip, the study evaluates energy-performance metrics considering simultaneous CPU and GPU power-capping effects. Focusing on the Locally Self-Consistent Multiple Scattering (LSMS) application, the research identifies potential opportunities for energy savings in exascale applications. The results demonstrate that GPU task-specific dynamic power-cap adjustments, combined with integrated CPU-GPU power steering, can enhance energy utilization for certain GPU tasks. These findings lay the foundation for future adaptive optimization strategies in exascale computing, emphasizing the significance of even small reductions in energy consumption for overall system efficiency. <div>
arXiv:2505.21758v1 Announce Type: cross 
Abstract: With high-performance computing systems now running at exascale, optimizing power-scaling management and resource utilization has become more critical than ever. This paper explores runtime power-capping optimizations that leverage integrated CPU-GPU power management on architectures like the NVIDIA GH200 superchip. We evaluate energy-performance metrics that account for simultaneous CPU and GPU power-capping effects by using two complementary approaches: speedup-energy-delay and a Euclidean distance-based multi-objective optimization method. By targeting a mostly compute-bound exascale science application, the Locally Self-Consistent Multiple Scattering (LSMS), we explore challenging scenarios to identify potential opportunities for energy savings in exascale applications, and we recognize that even modest reductions in energy consumption can have significant overall impacts. Our results highlight how GPU task-specific dynamic power-cap adjustments combined with integrated CPU-GPU power steering can improve the energy utilization of certain GPU tasks, thereby laying the groundwork for future adaptive optimization strategies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2505.21887</link>
<guid>https://arxiv.org/abs/2505.21887</guid>
<content:encoded><![CDATA[
<div> Keywords: Robust routing, uncertainty, logistics, benchmark, stochastic dynamics 

Summary:
SVRPBench introduces an open benchmark for robust routing under uncertainty in logistics, specifically focusing on high-fidelity stochastic dynamics in vehicle routing at an urban scale. This benchmark comprises over 500 instances with realistic delivery conditions, including time-dependent congestion, delays, accidents, and time windows for customers. The dataset simulates constraint-rich scenarios such as multi-depot and multi-vehicle setups. Benchmarking results demonstrate that classical and metaheuristic methods outperform state-of-the-art RL solvers like POMO and AM under distributional shift. The findings highlight the importance of designing solvers that can generalize beyond synthetic assumptions and adapt to real-world uncertainty. The release of the dataset and evaluation suite enables reproducible research in the field of robust routing. <br /><br />Summary: SVRPBench provides an open benchmark for robust routing under uncertainty in real-world logistics, showcasing the challenges faced by state-of-the-art RL solvers and the need for adaptive solutions to dynamic stochastic conditions. <div>
arXiv:2505.21887v1 Announce Type: cross 
Abstract: Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</title>
<link>https://arxiv.org/abs/2505.21923</link>
<guid>https://arxiv.org/abs/2505.21923</guid>
<content:encoded><![CDATA[
<div> machine learning, analog circuits, topology selection, parameter inference, layout feasibility

Summary:<br />
The article introduces FALCON, a machine learning framework for automated analog circuit synthesis. FALCON guides the circuit design process by first selecting an appropriate circuit topology based on performance specifications. It then uses a graph neural network to infer circuit parameters and predict performance. The design process is constrained by layout considerations and design rules to ensure feasibility. FALCON was trained and evaluated on a large dataset of analog mm-wave circuits, demonstrating high accuracy in topology inference and performance prediction. The automated design process is efficient, completing in under 1 second per instance. Overall, FALCON shows promise as a practical and scalable tool for end-to-end analog circuit design automation.<br /> <div>
arXiv:2505.21923v1 Announce Type: cross 
Abstract: Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99\% accuracy in topology inference, <10\% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data</title>
<link>https://arxiv.org/abs/2505.22252</link>
<guid>https://arxiv.org/abs/2505.22252</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, cheminformatics, drug discovery, Explainable AI, Graph Neural Networks

Summary:
The study focuses on the importance of understanding the rationale behind predictions made by deep learning models in cheminformatics and drug discovery. Existing evaluation frameworks for Explainable AI (XAI) in this field lack real-world data and fail to capture the complexity of actual scenarios. To address this gap, the researchers introduce a new benchmark called B-XAIC, which uses real-world molecular data and diverse tasks with known ground-truth rationales. By evaluating XAI methods for Graph Neural Networks (GNNs) using B-XAIC, the study reveals the limitations of current methods in the molecular domain. This benchmark serves as a valuable resource for enhancing the faithfulness of explanations in XAI and improving the interpretability of models used in cheminformatics and drug discovery. <div>
arXiv:2505.22252v1 Announce Type: cross 
Abstract: Understanding the reasoning behind deep learning model predictions is crucial in cheminformatics and drug discovery, where molecular design determines their properties. However, current evaluation frameworks for Explainable AI (XAI) in this domain often rely on artificial datasets or simplified tasks, employing data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness. To address this, we introduce B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through a comprehensive evaluation using B-XAIC, we reveal limitations of existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain. This benchmark provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation</title>
<link>https://arxiv.org/abs/2505.22391</link>
<guid>https://arxiv.org/abs/2505.22391</guid>
<content:encoded><![CDATA[
<div> Keywords: generative modeling, diffusion models, PDE constraints, Physics-Informed Distillation, inverse problem solving<br />
Summary: 
Physics-based generative modeling offers advantages in handling partial observations and addressing both forward and inverse problems. Diffusion models have been increasingly used for modeling physical systems governed by partial differential equations (PDEs). However, a trade-off exists when enforcing PDE constraints on clean samples, leading to reduced generative accuracy. To address this, a post-hoc distillation approach called Physics-Informed Distillation of Diffusion Models (PIDDM) is proposed. This method enforces PDE constraints after the diffusion process, improving PDE satisfaction and supporting both forward and inverse problem solving. Experimental results demonstrate that PIDDM outperforms recent baselines like PIDM and DiffusionPDE, with lower computation overhead. The approach provides insights into more efficient ways of integrating physical constraints into diffusion models.<br /><br />Summary: <div>
arXiv:2505.22391v1 Announce Type: cross 
Abstract: Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2L Translation Operators for Kernel Independent Fast Multipole Methods on Modern Architectures</title>
<link>https://arxiv.org/abs/2408.07436</link>
<guid>https://arxiv.org/abs/2408.07436</guid>
<content:encoded><![CDATA[
<div> Multipole-to-Local Translation Operators, Kernel-independent Fast Multipole Method, Benchmarking, BLAS-based M2L, Randomized Low-rank Compression<br />
<br />
Summary:<br />
Hardware trends prioritize data reuse in algorithms. This study focuses on high-performance Multipole-to-Local (M2L) translation operators for the kernel-independent Fast Multipole Method (kiFMM). The traditional M2L approach is bandwidth-limited, presenting a bottleneck in the FMM. While FFT-based M2L implementations are efficient, they lack operational intensity and require specific optimizations. In contrast, BLAS-based M2L with randomized low-rank compression offers competitive performance, portability, and a simpler implementation leveraging existing BLAS infrastructure. A Rust-based implementation allows seamless strategy switching for fair benchmarking. CPU results show that FFT-based M2L excels in low-accuracy or dynamic simulations, whereas BLAS-based M2L is more effective in high-accuracy settings for static distributions, despite higher setup costs that are offset in many practical FMM applications. <br /> <div>
arXiv:2408.07436v4 Announce Type: replace 
Abstract: Hardware trends favor algorithm designs that maximize data reuse per FLOP. We develop and benchmark high-performance Multipole-to-Local (M2L) translation operators for the kernel-independent Fast Multipole Method (kiFMM), a widely adopted FMM variant that supports a broad class of kernels and has been favored by recent implementations for its simple specification. Naively implemented, M2L is bandwidth-limited and therefore a key bottleneck in the FMM. State-of-the-art FFT-based M2L implementations, though elegant and with a fast setup time, suffer from low operational intensity and require architecture-specific optimizations. We demonstrate that a BLAS-based M2L, combined with randomized low-rank compression, achieves competitive performance with greater portability and a simpler implementation leveraging existing BLAS infrastructure, at the cost of higher setup times-especially for high-accuracy settings in double precision. Our Rust-based implementation enables seamless switching between strategies for fair benchmarking. Results on CPUs show that FFT-based M2L is favorable in low-accuracy settings or dynamic particle simulations, while BLAS-based M2L is favored for high-accuracy settings for static particle distributions, where its higher setup costs are amortized in many practical applications of the FMM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving precision of A/B experiments using trigger intensity</title>
<link>https://arxiv.org/abs/2411.03530</link>
<guid>https://arxiv.org/abs/2411.03530</guid>
<content:encoded><![CDATA[
<div> randomized controlled experiment, A/B experiment, signal-to-noise ratio, sampling, bias

Summary: 
- Online randomized controlled experiments are widely used in industry to measure causal changes but often lack statistical significance due to low signal-to-noise ratios.
- Traditional methods focus on trigger observations where treatment and control models differ, leading to a costly process.
- A proposed sampling-based evaluation method reduces costs by introducing bias inversely proportional to the number of observations sampled.
- Simulation results show that bias effectively reduces to zero with a limited number of observations sampled.
- Empirical data demonstrates a 36.48% reduction in standard error with partial knowledge evaluation. 
<br /><br /> <div>
arXiv:2411.03530v2 Announce Type: replace-cross 
Abstract: In industry, online randomized controlled experiment (a.k.a. A/B experiment) is a standard approach to measure the impact of a causal change. These experiments have small treatment effect to reduce the potential blast radius. As a result, these experiments often lack statistical significance due to low signal-to-noise ratio. A standard approach for improving the precision (or reducing the standard error) focuses only on the trigger observations, where the output of the treatment and the control model are different. Although evaluation with full information about trigger observations (full knowledge) improves the precision, detecting all such trigger observations is a costly affair. In this paper, we propose a sampling based evaluation method (partial knowledge) to reduce this cost. The randomness of sampling introduces bias in the estimated outcome. We theoretically analyze this bias and show that the bias is inversely proportional to the number of observations used for sampling. We also compare the proposed evaluation methods using simulation and empirical data. In simulation, bias in evaluation with partial knowledge effectively reduces to zero when a limited number of observations (<= 0.1%) are sampled for trigger estimation. In empirical setup, evaluation with partial knowledge reduces the standard error by 36.48%.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIT-BO: High-Dimensional Bayesian Optimization with Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2505.20685</link>
<guid>https://arxiv.org/abs/2505.20685</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, high-dimensional spaces, Gradient-Informed Bayesian Optimization, Tabular Foundation Models, pre-trained model<br />
<br />
Summary: 
The paper introduces Gradient-Informed Bayesian Optimization using Tabular Foundation Models (GIT-BO) to address challenges in high-dimensional Bayesian optimization. GIT-BO leverages a pre-trained tabular foundation model to identify low-dimensional subspaces for optimization using gradient information. By creating a gradient-informed diagnostic matrix, the most sensitive directions of the model's predictions are identified for adaptive optimization without repeated model retraining. Extensive evaluation across 23 benchmarks shows GIT-BO outperforms existing Gaussian process-based methods in scalability and optimization performance, especially in high dimensions up to 500. This work showcases the effectiveness of foundation models with gradient-informed adaptive subspace identification as competitive alternatives for high-dimensional Bayesian optimization tasks. <br /> <div>
arXiv:2505.20685v1 Announce Type: new 
Abstract: Bayesian optimization (BO) effectively optimizes expensive black-box functions but faces significant challenges in high-dimensional spaces (dimensions exceeding 100) due to the curse of dimensionality. Existing high-dimensional BO methods typically leverage low-dimensional embeddings or structural assumptions to mitigate this challenge, yet these approaches frequently incur considerable computational overhead and rigidity due to iterative surrogate retraining and fixed assumptions. To address these limitations, we propose Gradient-Informed Bayesian Optimization using Tabular Foundation Models (GIT-BO), an approach that utilizes a pre-trained tabular foundation model (TFM) as a surrogate, leveraging its gradient information to adaptively identify low-dimensional subspaces for optimization. We propose a way to exploit internal gradient computations from the TFM's forward pass by creating a gradient-informed diagnostic matrix that reveals the most sensitive directions of the TFM's predictions, enabling optimization in a continuously re-estimated active subspace without the need for repeated model retraining. Extensive empirical evaluation across 23 synthetic and real-world benchmarks demonstrates that GIT-BO consistently outperforms four state-of-the-art Gaussian process-based high-dimensional BO methods, showing superior scalability and optimization performances, especially as dimensionality increases up to 500 dimensions. This work establishes foundation models, augmented with gradient-informed adaptive subspace identification, as highly competitive alternatives to traditional Gaussian process-based approaches for high-dimensional Bayesian optimization tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced and mixed precision turbulent flow simulations using explicit finite difference schemes</title>
<link>https://arxiv.org/abs/2505.20911</link>
<guid>https://arxiv.org/abs/2505.20911</guid>
<content:encoded><![CDATA[
<div> Keywords: reduced precision computing, mixed precision arithmetic, compressible turbulent flow simulations, explicit finite difference schemes, performance gains

Summary:
Reduced and mixed precision computing is being increasingly utilized in high-performance computing (HPC) for enhanced computational efficiency, especially on modern hardware like GPUs. The study focuses on applying mixed precision arithmetic in compressible turbulent flow simulations using explicit finite difference schemes. They modify the OPS and OpenSBLI frameworks to allow for customizable precision levels, enabling precise control over precision allocation for various tasks. Through numerical experiments on the Taylor-Green vortex benchmark, the researchers showcase significant performance improvements with mixed precision strategies like half-single and single-double combinations, maintaining numerical accuracy. Pure half-precision computations, however, exhibit unacceptable accuracy degradation, emphasizing the importance of careful precision selection. The study demonstrates that mixed precision configurations can decrease memory usage and communication overhead, resulting in noticeable speedups, particularly on multi-CPU and multi-GPU systems. 

<br /><br />Summary: <div>
arXiv:2505.20911v1 Announce Type: new 
Abstract: The use of reduced and mixed precision computing has gained increasing attention in high-performance computing (HPC) as a means to improve computational efficiency, particularly on modern hardware architectures like GPUs. In this work, we explore the application of mixed precision arithmetic in compressible turbulent flow simulations using explicit finite difference schemes. We extend the OPS and OpenSBLI frameworks to support customizable precision levels, enabling fine-grained control over precision allocation for different computational tasks. Through a series of numerical experiments on the Taylor-Green vortex benchmark, we demonstrate that mixed precision strategies, such as half-single and single-double combinations, can offer significant performance gains without compromising numerical accuracy. However, pure half-precision computations result in unacceptable accuracy loss, underscoring the need for careful precision selection. Our results show that mixed precision configurations can reduce memory usage and communication overhead, leading to notable speedups, particularly on multi-CPU and multi-GPU systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limitations of Nyquist Criteria in the Discretization of 2D Electromagnetic Integral Equations at High Frequency: Spectral Insights into Pollution Effects</title>
<link>https://arxiv.org/abs/2505.20942</link>
<guid>https://arxiv.org/abs/2505.20942</guid>
<content:encoded><![CDATA[
<div> Boundary Integral Equations, Boundary Element Methods, Spectral Analysis, Solution Accuracy, Electromagnetic Scattering<br />
<br />
Summary: The article discusses the use of boundary integral equations in modeling wave phenomena in various fields like elastic, acoustic, or electromagnetic. The focus is on analyzing the impact of Boundary Element Methods (BEMs) discretization on solution accuracy, particularly in electromagnetic scattering from a conducting cylinder. The study examines both ill-conditioned and well-conditioned equations, identifying a form of pollution affecting accuracy in different ways. The research proposes a solution strategy to mitigate this pollution problem. Through rigorous spectral analysis, the article provides deep insight into the root causes of numerical pollution in BEMs, highlighting the importance of understanding how discretization affects solution accuracy in boundary value problems. <div>
arXiv:2505.20942v1 Announce Type: new 
Abstract: The use of boundary integral equations in modeling boundary value problems-such as elastic, acoustic, or electromagnetic ones-is well established in the literature and widespread in practical applications. These equations are typically solved numerically using boundary element methods (BEMs), which generally provide accurate and reliable solutions. When the frequency of the wave phenomenon under study increases, the discretization of the problem is typically chosen to maintain a fixed number of unknowns per wavelength. Under these conditions, the BEM over finite-dimensional subspaces of piecewise polynomial basis functions is commonly believed to provide a bounded solution accuracy. If proven, this would constitute a significant advantage of the BEM with respect to finite element and finite difference time domain methods, which, in contrast, are affected by numerical pollution. In this work, we conduct a rigorous spectral analysis of some of the most commonly used boundary integral operators and examine the impact of the BEM discretization on the solution accuracy of widely used integral equations modeling two-dimensional electromagnetic scattering from a perfectly electrically conducting cylinder. We consider both ill-conditioned and well-conditioned equations, the latter being characterized by solution operators bounded independently of frequency. Our analysis, which is capable of tracking the effects of BEM discretization on compositions and sums of different operators, reveals a form of pollution that affects, in different measures, equations of both kinds. After elucidating the mechanism by which the BEM discretization impacts accuracy, we propose a solution strategy that can cure the pollution problem thus evidenced. The defining strength of the proposed theoretical model lies in its capacity to deliver deep insight into the root causes of the phenomenon.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of the Past: An AI-Enabled Pipeline for Traffic Simulation from Noisy, Multimodal Detector Data and Stakeholder Feedback</title>
<link>https://arxiv.org/abs/2505.21349</link>
<guid>https://arxiv.org/abs/2505.21349</guid>
<content:encoded><![CDATA[
<div> computer vision, combinatorial optimization, large language models, traffic simulation, data-driven

Summary:
The article introduces a new approach to designing traffic simulations that accurately reflect real-world traffic conditions. The proposed pipeline involves three steps: using computer vision for vehicle counting from camera footage, applying combinatorial optimization for vehicle route generation from multimodal data, and utilizing large language models for iterative simulation refinement based on natural language feedback. Through testing on a road network in Strongsville, Ohio, the framework successfully captures the city's traffic patterns in a detailed simulation. The pipeline's flexibility allows for generalization to other municipalities with varying levels of data and infrastructure availability. <div>
arXiv:2505.21349v1 Announce Type: new 
Abstract: How can a traffic simulation be designed to faithfully reflect real-world traffic conditions? Past data-driven approaches to traffic simulation in the literature have relied on unrealistic or suboptimal heuristics. They also fail to adequately account for the effects of uncertainty and multimodality in the data on simulation outcomes. In this work, we integrate advances in AI to construct a three-step, end-to-end pipeline for generating a traffic simulation from detector data: computer vision for vehicle counting from camera footage, combinatorial optimization for vehicle route generation from multimodal data, and large language models for iterative simulation refinement from natural language feedback. Using a road network from Strongsville, Ohio as a testbed, we demonstrate that our pipeline can accurately capture the city's traffic patterns in a granular simulation. Beyond Strongsville, our traffic simulation framework can be generalized to other municipalities with different levels of data and infrastructure availability.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information</title>
<link>https://arxiv.org/abs/2505.20650</link>
<guid>https://arxiv.org/abs/2505.20650</guid>
<content:encoded><![CDATA[
<div> FinTagging, XBRL benchmark, large language models, structured information extraction, semantic alignment <br />
Summary: <br />
FinTagging is introduced as a comprehensive XBRL benchmark for evaluating language models' capabilities in financial reporting. It consists of two subtasks, FinNI for entity extraction and FinCL for concept alignment, emphasizing the extraction and alignment of facts within the US-GAAP taxonomy. Large language models exhibit strong extraction abilities but struggle with fine-grained concept alignment, especially in distinguishing closely related taxonomy entries. The study underscores the importance of improved semantic reasoning and schema-aware modeling for accurate financial disclosure. The code is available on GitHub, and the data can be accessed through the Hugging Face repository. <div>
arXiv:2505.20650v1 Announce Type: cross 
Abstract: We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction</title>
<link>https://arxiv.org/abs/2505.21109</link>
<guid>https://arxiv.org/abs/2505.21109</guid>
<content:encoded><![CDATA[
<div> adaptation, language models, hallucination issues, Small Language Graph, lightweight

Summary:<br />
- The study addresses challenges in adapting large language models, focusing on reducing computational resources and minimizing hallucination issues.
- The Small Language Graph (SLG) approach, based on a graph structure with lightweight expert nodes, outperformed traditional fine-tuning methods by 3 times on the Exact Match metric.
- SLG also demonstrated a 1.7 times faster fine-tuning process compared to stand-alone models.
- The findings suggest that SLG could enable small to medium-sized engineering companies to leverage generative AI technologies without the need for expensive computational resources.
- The graph architecture and small expert nodes offer potential for distributed AI systems, potentially reducing the reliance on costly centralized compute clusters. 

<br /><br /> <div>
arXiv:2505.21109v1 Announce Type: cross 
Abstract: Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap Between Data-Driven And Theory-Driven Modelling - Leveraging Causal Machine Learning for Integrative Modelling of Dynamical Systems</title>
<link>https://arxiv.org/abs/2410.09516</link>
<guid>https://arxiv.org/abs/2410.09516</guid>
<content:encoded><![CDATA[
<div> causal feature selection, domain knowledge, time-series data, machine learning, predictive robustness<br />
<br />
Summary: 
This study explores the use of causal feature selection with domain knowledge in improving machine learning applications, specifically in the context of a data center system. Traditional machine learning techniques often face challenges of overfitting and unreliable predictions in novel conditions. By incorporating causality into the modeling process, predictive robustness can be enhanced. The study compares causal feature selection with traditional feature selection methods using simulated time-series data. Results show that predictions based on causal features are more robust, highlighting the potential benefits of combining causal discovery algorithms with human expertise. This approach can help address the time-consuming process of manually constructing causal graphs, particularly in complex time series with numerous variables. <div>
arXiv:2410.09516v3 Announce Type: replace 
Abstract: Classical machine learning techniques often struggle with overfitting and unreliable predictions when exposed to novel conditions. Introducing causality into the modelling process offers a promising way to mitigate these challenges by enhancing predictive robustness. However, constructing an initial causal graph manually using domain knowledge is time-consuming, particularly in complex time series with numerous variables. To address this, causal discovery algorithms can provide a preliminary causal structure that domain experts can refine. This study investigates causal feature selection with domain knowledge using a data center system as an example. We use simulated time-series data to compare different causal feature selection with traditional machine-learning feature selection methods. Our results show that predictions based on causal features are more robust compared to those derived from traditional methods. These findings underscore the potential of combining causal discovery algorithms with human expertise to improve machine learning applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fusion with Relational Learning for Molecular Property Prediction</title>
<link>https://arxiv.org/abs/2410.12128</link>
<guid>https://arxiv.org/abs/2410.12128</guid>
<content:encoded><![CDATA[
<div> Graph-based molecular representation learning, multimodal fusion, relational learning, drug discovery, materials science
<br />
Summary: 
MMFRL (Multimodal Fusion with Relational Learning for Molecular Property Prediction) is introduced as a novel framework to enhance graph-based molecular representation learning. It addresses challenges in molecular property prediction by incorporating multimodal fusion at different stages such as early, intermediate, and late. The method improves embedding initialization through multimodal pretraining using relational learning. Extensive experiments on MoleculeNet benchmarks show that MMFRL outperforms existing methods significantly, allowing for task-specific optimizations. The explainability of MMFRL provides valuable chemical insights, enhancing its potential for real-world drug discovery applications. <div>
arXiv:2410.12128v2 Announce Type: replace 
Abstract: Graph based molecular representation learning is essential for accurately predicting molecular properties in drug discovery and materials science; however, it faces significant challenges due to the intricate relationships among molecules and the limited chemical knowledge utilized during training. While contrastive learning is often employed to handle molecular relationships, its reliance on binary metrics is insufficient for capturing the complexity of these interactions. Multimodal fusion has gained attention for property reasoning, but previous work has explored only a limited range of modalities, and the optimal stages for fusing different modalities in molecular property tasks remain underexplored. In this paper, we introduce MMFRL (Multimodal Fusion with Relational Learning for Molecular Property Prediction), a novel framework designed to overcome these limitations. Our method enhances embedding initialization through multimodal pretraining using relational learning. We also conduct a systematic investigation into the impact of modality fusion at different stages such as early, intermediate, and late, highlighting their advantages and shortcomings. Extensive experiments on MoleculeNet benchmarks demonstrate that MMFRL significantly outperforms existing methods. Furthermore, MMFRL enables task-specific optimizations. Additionally, the explainability of MMFRL provides valuable chemical insights, emphasizing its potential to enhance real-world drug discovery applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A batch production scheduling problem in a reconfigurable hybrid manufacturing-remanufacturing system</title>
<link>https://arxiv.org/abs/2504.00605</link>
<guid>https://arxiv.org/abs/2504.00605</guid>
<content:encoded><![CDATA[
<div> sustainability, remanufacturing, Hybrid Manufacturing-Remanufacturing System, Reconfigurable Manufacturing System, production scheduling

Summary:
The study focuses on production scheduling in a Hybrid Manufacturing-Remanufacturing System (HMRS) with non-identical parallel reconfigurable machines and batch orders. Models using Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) are developed, with a computationally efficient Logic-based Benders Decomposition (LBBD) method for solution. The LBBD approach outperforms MILP, CP, and warm-started MILP models, achieving an average gap of about 2%. The study highlights the benefits of utilizing Reconfigurable Manufacturing System (RMS) technologies in HMRSs and provides actionable managerial insights for scheduling in such systems. It addresses the complexity of production management in HMRSs and showcases the importance of customized production capabilities for increased flexibility and efficiency in processing both new products and End-of-Life (EOL) products in a shared facility. 

<br /><br />Summary: <div>
arXiv:2504.00605v2 Announce Type: replace 
Abstract: In recent years, remanufacturing of End-of-Life (EOL) products has been adopted by manufacturing sectors as a competent practice to enhance their sustainability and market share. Due to the mass customization of products and high volatility of market, processing of new products and remanufacturing of EOLs in the same shared facility, namely Hybrid Manufacturing-Remanufacturing System (HMRS), is a mean to keep such production efficient. Accordingly, customized production capabilities are required to increase flexibility, which can be effectively provided under the Reconfigurable Manufacturing System (RMS) paradigm. Despite the advantages of utilizing RMS technologies in HMRSs, production management of such systems suffers excessive complexity. Hence, this study concentrates on the production scheduling of an HMRS consisting of non-identical parallel reconfigurable machines where the orders can be grouped into batches. In this regard, Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) models are devised to formulate the problem. Furthermore, a computationally efficient solution method is developed based on a Logic-based Benders Decomposition (LBBD) approach. The warm start technique is also implemented by providing a decent initial solution to the MILP model. Computational experiments attest to the LBBD method's superiority over the MILP, CP, and warm-started MILP models by obtaining an average gap of about 2%, besides it yields actionable managerial insights for scheduling in HMRSs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-Enhanced Monte Carlo: A Machine Learning View on Control Variate</title>
<link>https://arxiv.org/abs/2412.11257</link>
<guid>https://arxiv.org/abs/2412.11257</guid>
<content:encoded><![CDATA[
<div> ML, Monte Carlo, Prediction-Enhanced, Variance Reduction, Simulation

Summary:
Prediction-Enhanced Monte Carlo (PEMC) is introduced as a framework that uses machine learning models as predictors to reduce variance and runtime in complex simulation tasks. It overcomes the computational challenges of traditional Monte Carlo methods by leveraging modern ML surrogates for unbiased evaluation. PEMC acts as a modernized control variate approach, considering overall computation-cost-aware variance reduction. It demonstrates efficacy in various scenarios, including equity derivatives, interest rate derivatives, and healthcare decision-making processes. In equity derivatives, it reduces variance for variance swaps under stochastic local volatility models. In interest rate derivatives, it enhances swaption pricing under the HJM interest-rate model. In healthcare decision-making, it assists in ambulance dispatch and hospital load balancing by providing accurate mortality rate estimates. PEMC consistently improves performance by reducing variance while maintaining unbiasedness, highlighting its potential as a valuable enhancement to standard Monte Carlo methods. 

<br /><br />Summary: 
Keywords: ML, Monte Carlo, Prediction-Enhanced, Variance Reduction, Simulation <div>
arXiv:2412.11257v2 Announce Type: replace-cross 
Abstract: For many complex simulation tasks spanning areas such as healthcare, engineering, and finance, Monte Carlo (MC) methods are invaluable due to their unbiased estimates and precise error quantification. Nevertheless, Monte Carlo simulations often become computationally prohibitive, especially for nested, multi-level, or path-dependent evaluations lacking effective variance reduction techniques. While machine learning (ML) surrogates appear as natural alternatives, naive replacements typically introduce unquantifiable biases. We address this challenge by introducing Prediction-Enhanced Monte Carlo (PEMC), a framework that leverages modern ML models as learned predictors, using cheap and parallelizable simulation as features, to output unbiased evaluation with reduced variance and runtime. PEMC can also be viewed as a "modernized" view of control variates, where we consider the overall computation-cost-aware variance reduction instead of per-replication reduction, while bypassing the closed-form mean function requirement and maintaining the advantageous unbiasedness and uncertainty quantifiability of Monte Carlo.
  We illustrate PEMC's broader efficacy and versatility through three examples: first, equity derivatives such as variance swaps under stochastic local volatility models; second, interest rate derivatives such as swaption pricing under the Heath-Jarrow-Morton (HJM) interest-rate model. Finally, we showcase PEMC in a socially significant context - ambulance dispatch and hospital load balancing - where accurate mortality rate estimates are key for ethically sensitive decision-making. Across these diverse scenarios, PEMC consistently reduces variance while preserving unbiasedness, highlighting its potential as a powerful enhancement to standard Monte Carlo baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Beyond Words: MatVQA for Challenging Visual-Scientific Reasoning in Materials Science</title>
<link>https://arxiv.org/abs/2505.18319</link>
<guid>https://arxiv.org/abs/2505.18319</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Materials Science, MatVQA, Scientific Reasoning, Benchmarking

Summary:
MatVQA is a new benchmark designed to address the limitations of current materials science evaluation datasets by integrating visual and language modalities for scientific reasoning. The dataset features 1325 questions across four structure-property-performance reasoning tasks, encouraging MLLMs to perform fine-grained visual analysis of material imagery. By benchmarking 17 MLLMs on MatVQA, significant gaps in multimodal reasoning capabilities were revealed. The benchmark data and evaluation code are publicly available to facilitate further research in applying MLLMs to complex materials science problems.<br /><br />Summary: <div>
arXiv:2505.18319v1 Announce Type: new 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) that integrate vision and language modalities has unlocked new potentials for scientific reasoning, outperforming prior benchmarks in both natural language and coding domains. Current materials science evaluation datasets such as MaScQA and SciQA remain largely text-based and fail to capture the visual and research-level analytic complexity required in materials discovery and design. We introduce MatVQA, a scalable benchmark specifically designed to address this gap. Generated via an automated pipeline, MArxivAgent, from recent materials literature, MatVQA features 1325 questions across four critical structure-property-performance (SPP) reasoning tasks. Uniquely, MatVQA employs an iterative process to eliminate textual shortcuts, compelling MLLMs to perform fine-grained, low-level visual analysis of material imagery (e.g., microscopy, diffraction patterns) integrated with multi-step scientific reasoning. Benchmarking 17 open- and closed-source MLLMs on MatVQA reveals substantial gaps in current multimodal reasoning capabilities. MatVQA benchmark data, along with evaluation code, is publicly available in \href{https://anonymous.4open.science/r/matvqa-1E01}{https://anonymous.4open.science/r/matvqa-1E01/README.md} to catalyze further research in applying MLLMs to complex materials science problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AERO: An autonomous platform for continuous research</title>
<link>https://arxiv.org/abs/2505.18408</link>
<guid>https://arxiv.org/abs/2505.18408</guid>
<content:encoded><![CDATA[
<div> automated research, data infrastructure, collaboration, epidemiology, public health

Summary: 
The article introduces AERO, an automated research and data sharing platform developed to support cross-sector investigations during the COVID-19 pandemic. AERO allows for the automatic ingestion, validation, and transformation of monitored data for analysis, as well as the execution of analyses and data sharing among different entities. Leveraging capabilities from the Globus platform and GitHub, AERO facilitates automation, distributed execution, data sharing, and authentication. The implementation of AERO has been demonstrated with two public health surveillance applications and benchmarking with a synthetic application. Users can access these applications for testing purposes. AERO addresses the need for new data infrastructure in rapidly evolving situations, such as public health emergencies, to enable continuous, distributed, and multi-disciplinary collaboration. <div>
arXiv:2505.18408v1 Announce Type: new 
Abstract: The COVID-19 pandemic highlighted the need for new data infrastructure, as epidemiologists and public health workers raced to harness rapidly evolving data, analytics, and infrastructure in support of cross-sector investigations. To meet this need, we developed AERO, an automated research and data sharing platform for continuous, distributed, and multi-disciplinary collaboration. In this paper, we describe the AERO design and how it supports the automatic ingestion, validation, and transformation of monitored data into a form suitable for analysis; the automated execution of analyses on this data; and the sharing of data among different entities. We also describe how our AERO implementation leverages capabilities provided by the Globus platform and GitHub for automation, distributed execution, data sharing, and authentication. We present results obtained with an instance of AERO running two public health surveillance applications and demonstrate benchmarking results with a synthetic application, all of which are publicly available for testing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Equilibrium: Non-Equilibrium Foundations Should Underpin Generative Processes in Complex Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.18621</link>
<guid>https://arxiv.org/abs/2505.18621</guid>
<content:encoded><![CDATA[
<div> Generative models, non-equilibrium physics, complex dynamical systems, empirical experiments, scientific modeling <br />
<br />
Generative models inspired by non-equilibrium physics are proposed as essential for better modeling complex dynamical systems due to limitations of classical equilibrium-based models. These non-equilibrium frameworks naturally capture evolving distributions and non-stationary behavior. Empirical experiments on a dynamic system validate the effectiveness of non-equilibrium generative models in tracking temporal evolution and adapting to changing landscapes. Future directions include integrating non-equilibrium principles with generative AI to simulate rare events, infer underlying mechanisms, and represent multi-scale dynamics across scientific domains. Embracing non-equilibrium physics is deemed necessary for generative AI to serve as a scientific modeling tool, offering new capabilities for simulating, understanding, and controlling complex systems.<br /><br />Summary: <div>
arXiv:2505.18621v1 Announce Type: new 
Abstract: This position paper argues that next-generation non-equilibrium-inspired generative models will provide the essential foundation for better modeling real-world complex dynamical systems. While many classical generative algorithms draw inspiration from equilibrium physics, they are fundamentally limited in representing systems with transient, irreversible, or far-from-equilibrium behavior. We show that non-equilibrium frameworks naturally capture non-equilibrium processes and evolving distributions. Through empirical experiments on a dynamic Printz potential system, we demonstrate that non-equilibrium generative models better track temporal evolution and adapt to non-stationary landscapes. We further highlight future directions such as integrating non-equilibrium principles with generative AI to simulate rare events, inferring underlying mechanisms, and representing multi-scale dynamics across scientific domains. Our position is that embracing non-equilibrium physics is not merely beneficial--but necessary--for generative AI to serve as a scientific modeling tool, offering new capabilities for simulating, understanding, and controlling complex systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A high-order matrix-free adaptive solver for the shallow water equations with irregular bathymetry</title>
<link>https://arxiv.org/abs/2505.18743</link>
<guid>https://arxiv.org/abs/2505.18743</guid>
<content:encoded><![CDATA[
<div> AMR, coastal engineering, Discontinuous Galerkin method, deal.II library, parallelization<br />
<br />
Summary: <br />
The article introduces a new Adaptive Mesh Refinement (AMR) solver for coastal engineering applications. It is based on the Discontinuous Galerkin (DG) method and implemented in the deal.II library, offering efficient parallelization and handling of non-conforming meshes. The method is well-balanced, adaptable to realistic bathymetry data without regularity assumptions, and conservatively discretizes transported chemical species. Idealized benchmarks validate the approach, demonstrating its potential for accurate and efficient adaptive simulations of coastal flows. The solver's capabilities are further evidenced through experiments on realistic bathymetries and complex domains. <div>
arXiv:2505.18743v1 Announce Type: new 
Abstract: We present the first step in the development of an Adaptive Mesh Refinement (AMR) solver for coastal engineering applications, based on a high-order Discontinuous Galerkin (DG) method as implemented in the deal.II library. This environment provides efficient and native parallelization techniques and automatically handles non-conforming meshes to implement both static and dynamic AMR approaches. The proposed method is automatically well-balanced, allows the use of realistic bathymetry data without any regularity assumption, and includes a consistent conservative discretization for transported chemical species. Numerical experiments on idealized benchmarks validate the proposed approach, while results obtained on realistic bathymetries and complex domains show its potential for accurate and efficient adaptive simulations of coastal flows.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoofNet: A Global Multimodal Dataset for Roof Material Classification</title>
<link>https://arxiv.org/abs/2505.19358</link>
<guid>https://arxiv.org/abs/2505.19358</guid>
<content:encoded><![CDATA[
<div> Dataset, RoofNet, Earth Observation imagery, roofing types, global exposure datasets
<br />
Summary: 
RoofNet is a new dataset that combines Earth Observation imagery with curated text annotations to classify roofing materials in buildings around the world. With over 51,500 samples from 184 sites, RoofNet labels 14 key roofing types, such as asphalt shingles and clay tiles, to enhance global exposure datasets. By fine-tuning a vision-language model on a subset of annotated images, RoofNet provides accurate predictions on roofing materials across different regions. The dataset includes rich metadata like roof shape, footprint area, solar panel presence, and mixed materials indicators. RoofNet's AI-driven risk assessment capabilities make it valuable for insurance underwriting, disaster preparedness, and infrastructure policy planning. It serves as a benchmark for evaluating model generalization and offers insights for decision-making in various sectors. 
<br /> <div>
arXiv:2505.19358v1 Announce Type: new 
Abstract: Natural disasters are increasing in frequency and severity, causing hundreds of billions of dollars in damage annually and posing growing threats to infrastructure and human livelihoods. Accurate data on roofing materials is critical for modeling building vulnerability to natural hazards such as earthquakes, floods, wildfires, and hurricanes, yet such data remain unavailable. To address this gap, we introduce RoofNet, the largest and most geographically diverse novel multimodal dataset to date, comprising over 51,500 samples from 184 geographically diverse sites pairing high-resolution Earth Observation (EO) imagery with curated text annotations for global roof material classification. RoofNet includes geographically diverse satellite imagery labeled with 14 key roofing types -- such as asphalt shingles, clay tiles, and metal sheets -- and is designed to enhance the fidelity of global exposure datasets through vision-language modeling (VLM). We sample EO tiles from climatically and architecturally distinct regions to construct a representative dataset. A subset of 6,000 images was annotated in collaboration with domain experts to fine-tune a VLM. We used geographic- and material-aware prompt tuning to enhance class separability. The fine-tuned model was then applied to the remaining EO tiles, with predictions refined through rule-based and human-in-the-loop verification. In addition to material labels, RoofNet provides rich metadata including roof shape, footprint area, solar panel presence, and indicators of mixed roofing materials (e.g., HVAC systems). RoofNet supports scalable, AI-driven risk assessment and serves as a downstream benchmark for evaluating model generalization across regions -- offering actionable insights for insurance underwriting, disaster preparedness, and infrastructure policy planning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Finite Element Neural Network (IFENN) for Phase-Field Fracture with Minimal Input and Generalized Geometry-Load Handling</title>
<link>https://arxiv.org/abs/2505.19566</link>
<guid>https://arxiv.org/abs/2505.19566</guid>
<content:encoded><![CDATA[
<div> novel formulation, phase-field fracture propagation, Integrated Finite Element Neural Network (IFENN), physics-informed convolutional networks (PICNNs), unsupervised training<br />
Summary:<br />
The article presents a new approach for modeling phase-field fracture propagation using the Integrated Finite Element Neural Network (IFENN) framework. This hybrid solver scheme combines neural networks as PDE solvers within FEM, improving accuracy and speed of predictions. The novel formulation involves using physics-informed convolutional networks (PICNNs) to calculate the phase-field variable while solving equilibrium equations with FEM. By eliminating temporal features and focusing on spatial coupling between strain energy density and phase-field variable, the training process is significantly reduced to just 5 minutes. The trained PICNN embedded within IFENN can simulate crack propagation in various scenarios with high accuracy, including rectangular domains, multiple interacting cracks, and different mesh densities. This breakthrough in hybrid modeling offers a physics-consistent solution for fracture and coupled problems. <br /><br />Summary: <div>
arXiv:2505.19566v1 Announce Type: new 
Abstract: We present a novel formulation for modeling phase-field fracture propagation based on the Integrated Finite Element Neural Network (IFENN) framework. IFENN is a hybrid solver scheme that utilizes neural networks as PDE solvers within FEM, preserving accuracy via residual minimization while achieving speed-up via swift network predictions and reduction of the size of system of equations in coupled problems. In this work, we introduce a radically new formulation of IFENN in which the phase-field variable is calculated using physics-informed convolutional networks (PICNNs), while the equilibrium equation is still solved using FEM to maintain the solver robustness. Unlike conventional approaches, which rely on sequence or time-dependent models, we eliminate the need to include temporal features in the training setup and inference stage. Instead, we show that it is sufficient to learn only the spatial coupling between the strain energy density and the phase-field variable in the vicinity of the fracture process zone, and utilize this information along the advancing crack simulation. We train a single CNN in a purely physics-based, unsupervised manner on just two load increments from a single-notch tension problem, with a total training time of only 5 minutes. Following this exceptionally minimal and fast training, we show that the same PICNN can (when embedded within IFENN) model crack propagation in a very wide range of unseen scenarios, including arbitrarily rectangular domains, single and multiple interacting cracks, varying mesh densities, and arbitrary loading paths. The proposed formulation delivers breakthroughs that address many of the limitations in the existing literature of hybrid modeling, introducing a new paradigm for the development of generalizable, physics-consistent hybrid models that are applicable to fracture and other coupled problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets</title>
<link>https://arxiv.org/abs/2505.19819</link>
<guid>https://arxiv.org/abs/2505.19819</guid>
<content:encoded><![CDATA[
<div> datasets, LoRA methods, financial tasks, performance gains, democratize financial intelligence <br />
<br />
Keywords: datasets, LoRA methods, financial tasks, performance gains, democratize financial intelligence <br />
<br />
Summary: 
The paper introduces the FinLoRA project, which explores the efficacy of low-rank adaptation (LoRA) methods in high-stakes financial domains. The project curated 19 datasets covering various financial applications, including four novel XBRL analysis datasets based on SEC filings. Five LoRA methods and five base Large Language Models (LLMs) were evaluated, with LoRA methods showing substantial performance gains averaging 36% over base models. Extensive experimental results in terms of accuracy, F1, and BERTScore were provided, along with information on computational costs during fine-tuning and inference stages. The FinLoRA project aims to democratize financial intelligence by providing an affordable and scalable approach for the general public. The datasets, LoRA adapters, code, and documentation are available on GitHub at https://github.com/Open-Finance-Lab/FinLoRA. <br /> <div>
arXiv:2505.19819v1 Announce Type: new 
Abstract: Low-rank adaptation (LoRA) methods show great potential for scaling pre-trained general-purpose Large Language Models (LLMs) to hundreds or thousands of use scenarios. However, their efficacy in high-stakes domains like finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings. In this paper, we present the open-source FinLoRA project that benchmarks LoRA methods on both general and highly professional financial tasks. First, we curated 19 datasets covering diverse financial applications; in particular, we created four novel XBRL analysis datasets based on 150 SEC filings. Second, we evaluated five LoRA methods and five base LLMs. Finally, we provide extensive experimental results in terms of accuracy, F1, and BERTScore and report computational cost in terms of time and GPU memory during fine-tuning and inference stages. We find that LoRA methods achieved substantial performance gains of 36\% on average over base models. Our FinLoRA project provides an affordable and scalable approach to democratize financial intelligence to the general public. Datasets, LoRA adapters, code, and documentation are available at https://github.com/Open-Finance-Lab/FinLoRA
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision</title>
<link>https://arxiv.org/abs/2505.18181</link>
<guid>https://arxiv.org/abs/2505.18181</guid>
<content:encoded><![CDATA[
<div> dataset, machine learning, NMR spectroscopy, molecular representation, annotated

Summary:
The article introduces 2DNMRGym, a dataset for machine learning-based molecular representation learning in two-dimensional Nuclear Magnetic Resonance (NMR) spectroscopy. The dataset contains over 22,000 Heteronuclear Single Quantum Coherence (HSQC) spectra along with corresponding molecular graphs and SMILES strings. Utilizing a surrogate supervision setup, models are trained on algorithm-generated annotations and evaluated on human-annotated gold-standard labels, allowing for rigorous assessment of model generalization. Benchmark results using 2D and 3D Graph Neural Network (GNN) and GNN transformer models demonstrate promising performance. This dataset supports scalable model training and provides a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. The data and code are open-source and available on Huggingface and Github. 

<br /><br />Summary: <div>
arXiv:2505.18181v1 Announce Type: cross 
Abstract: Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy, particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays a critical role in elucidating molecular structures, interactions, and electronic properties. However, accurately interpreting 2D NMR data remains labor-intensive and error-prone, requiring highly trained domain experts, especially for complex molecules. Machine Learning (ML) holds significant potential in 2D NMR analysis by learning molecular representations and recognizing complex patterns from data. However, progress has been limited by the lack of large-scale and high-quality annotated datasets. In this work, we introduce 2DNMRGym, the first annotated experimental dataset designed for ML-based molecular representation learning in 2D NMR. It includes over 22,000 HSQC spectra, along with the corresponding molecular graphs and SMILES strings. Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained using algorithm-generated annotations derived from a previously validated method and evaluated on a held-out set of human-annotated gold-standard labels. This enables rigorous assessment of a model's ability to generalize from imperfect supervision to expert-level interpretation. We provide benchmark results using a series of 2D and 3D GNN and GNN transformer models, establishing a strong foundation for future work. 2DNMRGym supports scalable model training and introduces a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. Our data and code is open-source and available on Huggingface and Github.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Optimization Algorithms for Energy Management Systems in Microgrids: A Control Strategy Based on a PHIL System</title>
<link>https://arxiv.org/abs/2505.18210</link>
<guid>https://arxiv.org/abs/2505.18210</guid>
<content:encoded><![CDATA[
<div> optimization, microgrid, power hardware, renewable energy sources, energy management

Summary:<br />
- An adaptive multi-objective optimization approach was implemented in a real-time power hardware-in-loop configuration for a microgrid with various energy resources.
- The approach effectively balanced factors such as fuel consumption, load mismatch, power quality, battery degradation, and the use of renewable energy sources.
- Real-time experimental data was used for dynamic system state updates, with adaptive preference-based selection adjusted based on battery charging thresholds.
- The technique integrated six technical objectives and complex constraints, aiding in practical microgrid decision making and dynamic energy system optimization.
- The energy management process successfully maximized photovoltaic production, minimized power mismatch, and stabilized battery state of charge in different conditions, outperforming a baseline system without optimization techniques.<br /><br /> <div>
arXiv:2505.18210v1 Announce Type: cross 
Abstract: In this research a real time power hardware in loop configuration has been implemented for an microgrid with the combination of distribution energy resources such as photovoltaic, grid tied inverter, battery, utility grid, and a diesel generator. This paper introduces an unique adaptive multi-objective optimization approach that employs weighted optimization techniques for real-time microgrid systems. The aim is to effectively balance various factors including fuel consumption, load mismatch, power quality, battery degradation, and the utilization of renewable energy sources. A real time experimental data from power hardware in loop system has been used for dynamically updating system states. The adaptive preference-based selection method are adjusted based on state of battery charging thresholds. The technique has been integrated with six technical objectives and complex constraints. This approach helps to practical microgrid decision making and optimization of dynamic energy systems. The energy management process were also able to maximize photovoltaic production where minimizing power mismatch, stabilizing battery state of charge under different condition. The research results were also compared with the baseline system without optimization techniques, and a reliable outcome was found.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
<div> PINNs, immersed boundary method, fluid-structure interaction, neural network architectures, adaptive activation functions <br />
Summary:
Neural network architectures combining physics-informed neural networks (PINNs) with the immersed boundary method (IBM) were proposed for fluid-structure interaction (FSI) problems. Two architectures, Single-FSI and Eulerian-Lagrangian, were compared using standard Tanh and adaptive B-spline activation functions. The Eulerian-Lagrangian architecture showed superior performance, with the adaptive B-spline activation improving accuracy near boundaries. While velocity field prediction was successful, pressure recovery proved challenging without explicit force-coupling constraints. The study emphasized the importance of domain-specific architectural design and adaptive activation functions in modeling FSI within the PINN framework. <br /> <div>
arXiv:2505.18565v1 Announce Type: cross 
Abstract: We introduce neural network architectures that combine physics-informed neural networks (PINNs) with the immersed boundary method (IBM) to solve fluid-structure interaction (FSI) problems. Our approach features two distinct architectures: a Single-FSI network with a unified parameter space, and an innovative Eulerian-Lagrangian network that maintains separate parameter spaces for fluid and structure domains. We study each architecture using standard Tanh and adaptive B-spline activation functions. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that the Eulerian-Lagrangian architecture performs significantly better. The adaptive B-spline activation further enhances accuracy by providing locality-aware representation near boundaries. While our methodology shows promising results in predicting the velocity field, pressure recovery remains challenging due to the absence of explicit force-coupling constraints in the current formulation. Our findings underscore the importance of domain-specific architectural design and adaptive activation functions for modeling FSI problems within the PINN framework.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete gradient methods for port-Hamiltonian differential-algebraic equations</title>
<link>https://arxiv.org/abs/2505.18810</link>
<guid>https://arxiv.org/abs/2505.18810</guid>
<content:encoded><![CDATA[
<div> discrete gradient methods, nonlinear port-Hamiltonian differential-algebraic equations, numerical scheme, Dirac-dissipative structures, multibody system dynamics 

Summary: 
Discrete gradient methods are effective for time discretization of dynamical systems, preserving structure regardless of the total energy form. This study focuses on applying these methods to nonlinear port-Hamiltonian differential-algebraic equations, commonly used in modeling physical systems. A novel numerical scheme is introduced for semi-explicit differential-algebraic equations, with the utilization of discrete gradient pairs and Dirac-dissipative structures in general settings. The behavior under system transformations is analyzed, showing that these equations can be represented as a parametrized port-Hamiltonian semi-explicit system and an unstructured equation under specific conditions. The application to multibody system dynamics is demonstrated through numerical results, showcasing the efficiency of the approach. <div>
arXiv:2505.18810v1 Announce Type: cross 
Abstract: Discrete gradient methods are a powerful tool for the time discretization of dynamical systems, since they are structure-preserving regardless of the form of the total energy. In this work, we discuss the application of discrete gradient methods to the system class of nonlinear port-Hamiltonian differential-algebraic equations - as they emerge from the port- and energy-based modeling of physical systems in various domains. We introduce a novel numerical scheme tailored for semi-explicit differential-algebraic equations and further address more general settings using the concepts of discrete gradient pairs and Dirac-dissipative structures. Additionally, the behavior under system transformations is investigated and we demonstrate that under suitable assumptions port-Hamiltonian differential-algebraic equations admit a representation which consists of a parametrized port-Hamiltonian semi-explicit system and an unstructured equation. Finally, we present the application to multibody system dynamics and discuss numerical results to demonstrate the capabilities of our approach.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search</title>
<link>https://arxiv.org/abs/2505.19209</link>
<guid>https://arxiv.org/abs/2505.19209</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, scientific hypothesis discovery, combinatorial optimization, hierarchical search method, chemistry literature

Summary:
1. The study introduces the task of fine-grained scientific hypothesis discovery, aiming to generate detailed and experimentally actionable hypotheses from initial research directions.
2. The research explores how to utilize an LLM's internal heuristics to formulate the most promising hypothesis based on its own scoring system.
3. The alignment between LLM-generated hypotheses and ground-truth hypotheses is investigated to evaluate the quality of the generated hypotheses.
4. Comparisons are made between using an ensemble of diverse LLMs and repeated instances of the strongest LLM to shape the reward landscape for hypothesis generation.
5. The study also analyzes the effectiveness of an ensemble of identical LLMs in providing a reliable reward landscape for hypothesis discovery.
6. The proposed hierarchical search method incrementally adds details to hypotheses, leading to a smoother reward landscape and more effective optimization.
7. Empirical evaluations on a chemistry literature benchmark demonstrate that the hierarchical search method consistently outperforms strong baselines. 

<br /><br />Summary: <div>
arXiv:2505.19209v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the novel task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent chemistry literature show that our method consistently outperforms strong baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid Development of Efficient Participant-Specific Computational Models of the Wrist</title>
<link>https://arxiv.org/abs/2505.19282</link>
<guid>https://arxiv.org/abs/2505.19282</guid>
<content:encoded><![CDATA[
<div> Keywords: computational modeling, hand and wrist injuries, finite element models, participant-specific, ligament injury

Summary:
Computational modeling plays a crucial role in developing treatment options for hand and wrist injuries. However, the current models are limited, often relying on average material properties from literature. A novel automated workflow has been developed to create participant-specific finite element models using non-linear morphing techniques and algorithmic approaches. By utilizing four-dimensional computed tomography (4DCT) data, three participant-specific models were created within 2 hours, allowing for individual simulations to be performed in just 45 seconds. The models were used to investigate clinical questions such as optimizing ligament properties to participant-specific kinematics and conducting Monte Carlo analysis on the effects of ligament injury on joint contact pressure. This work paves the way for future patient-specific modeling of hand and wrist injuries, offering a more personalized approach to treatment and understanding the impacts of injuries on joint health. 

<br /><br />Summary: <div>
arXiv:2505.19282v1 Announce Type: cross 
Abstract: While computational modeling may help to develop new treatment options for hand and wrist injuries, at present, few models exist. The time and expertise required to develop and use these models is considerable. Moreover, most do not allow for variation of material properties, instead relying on literature reported averages. We have developed a novel automated workflow combining non-linear morphing techniques with various algorithmic techniques to create participant-specific finite element models. Using this workflow, three participant-specific models were created from our existing four-dimensional computed tomography (4DCT) data. These were then used to perform two analyses to demonstrate the usefulness of the models to investigate clinical questions, namely optimization of ligament properties to participant-specific kinematics, and Monte Carlo (MC) analysis of the impacts of ligament injury on joint contact pressure, as an analogue for joint injury that may lead to osteoarthritis. Participant-specific models can be created in 2 hours and individual simulations performed in 45 seconds. This work lays the groundwork for future patient-specific modeling of the hand and wrist.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs</title>
<link>https://arxiv.org/abs/2505.19457</link>
<guid>https://arxiv.org/abs/2505.19457</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, finance, evaluation, reasoning <br /> 
<br />
Summary: 
The article introduces BizFinBench, a benchmark designed to evaluate Large Language Models (LLMs) in financial applications. It includes 6,781 annotated queries in Chinese across various dimensions such as numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering. The benchmark utilizes both objective and subjective metrics and introduces IteraJudge, a method to reduce bias when LLMs serve as evaluators. The evaluation of 25 models shows no single model excels in all tasks, revealing distinct capability patterns. In Numerical Calculation, Claude-3.5-Sonnet and DeepSeek-R1 lead, while in Reasoning, proprietary models outperform open-source models. Information Extraction shows the largest performance spread, while Prediction Recognition performance variance is minimal. The study highlights that while current LLMs handle routine finance queries well, they struggle with complex scenarios that require cross-concept reasoning. BizFinBench aims to provide a rigorous benchmark for future research in the financial domain. <div>
arXiv:2505.19457v1 Announce Type: cross 
Abstract: Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients</title>
<link>https://arxiv.org/abs/2505.19538</link>
<guid>https://arxiv.org/abs/2505.19538</guid>
<content:encoded><![CDATA[
<div> Knowledge bases, experiential knowledge, DoctorRAG, retrieval precision, Med-TextGrad <br />
Summary: <br />
Existing medical reasoning systems often focus on knowledge bases, neglecting the importance of experiential knowledge from patient cases. DoctorRAG is introduced as a framework that combines clinical knowledge and case-based experience to mimic doctor-like reasoning. It enhances retrieval precision by tagging concepts and employing a hybrid retrieval mechanism from knowledge sources and patient data. The integration of the Med-TextGrad module ensures the output aligns with retrieved knowledge and patient queries. Experimental results on diverse datasets show DoctorRAG outperforms traditional RAG models, with iterative refinements further improving performance. The approach generates more accurate, relevant, and comprehensive responses, highlighting progress towards developing medical reasoning systems that more closely mimic human clinical reasoning. <div>
arXiv:2505.19538v1 Announce Type: cross 
Abstract: Existing medical RAG systems mainly leverage knowledge from medical knowledge bases, neglecting the crucial role of experiential knowledge derived from similar patient cases -- a key component of human clinical reasoning. To bridge this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like reasoning by integrating both explicit clinical knowledge and implicit case-based experience. DoctorRAG enhances retrieval precision by first allocating conceptual tags for queries and knowledge sources, together with a hybrid retrieval mechanism from both relevant knowledge and patient. In addition, a Med-TextGrad module using multi-agent textual gradients is integrated to ensure that the final output adheres to the retrieved knowledge and patient query. Comprehensive experiments on multilingual, multitask datasets demonstrate that DoctorRAG significantly outperforms strong baseline RAG models and gains improvements from iterative refinements. Our approach generates more accurate, relevant, and comprehensive responses, taking a step towards more doctor-like medical reasoning systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based Visual Pathway Delineation</title>
<link>https://arxiv.org/abs/2505.19733</link>
<guid>https://arxiv.org/abs/2505.19733</guid>
<content:encoded><![CDATA[
<div> Framework, visual pathway, MRI, feature decomposition, semi-supervised<br />
<br />
Summary: 
Accurately delineating the visual pathway (VP) using multi-parametric MR imaging data is crucial for understanding the human visual system and diagnosing related disorders. Existing methods struggle with complex cross-sequence relationships and the need for large labeled datasets. To address these challenges, a novel semi-supervised framework is proposed. It includes a correlation-constrained feature decomposition (CFD) to capture unique MRI sequence characteristics and aid in information fusion. Additionally, a consistency-based sample enhancement (CSE) module generates edge information from unlabeled data to mitigate the limited labeled data issue. The framework outperforms seven state-of-the-art approaches in VP delineation on public and in-house datasets, showcasing its effectiveness in leveraging multi-parametric MRI data for improved visual pathway delineation. <br /> <div>
arXiv:2505.19733v1 Announce Type: cross 
Abstract: Accurately delineating the visual pathway (VP) is crucial for understanding the human visual system and diagnosing related disorders. Exploring multi-parametric MR imaging data has been identified as an important way to delineate VP. However, due to the complex cross-sequence relationships, existing methods cannot effectively model the complementary information from different MRI sequences. In addition, these existing methods heavily rely on large training data with labels, which is labor-intensive and time-consuming to obtain. In this work, we propose a novel semi-supervised multi-parametric feature decomposition framework for VP delineation. Specifically, a correlation-constrained feature decomposition (CFD) is designed to handle the complex cross-sequence relationships by capturing the unique characteristics of each MRI sequence and easing the multi-parametric information fusion process. Furthermore, a consistency-based sample enhancement (CSE) module is developed to address the limited labeled data issue, by generating and promoting meaningful edge information from unlabeled data. We validate our framework using two public datasets, and one in-house Multi-Shell Diffusion MRI (MDM) dataset. Experimental results demonstrate the superiority of our approach in terms of delineation performance when compared to seven state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Approach to Credit Prediction with Learnable Prompts for Multi-scale Temporal Representation Learning</title>
<link>https://arxiv.org/abs/2404.13004</link>
<guid>https://arxiv.org/abs/2404.13004</guid>
<content:encoded><![CDATA[
<div> Keywords: credit scoring, deep learning, FinLangNet, multi-scale distributions, time series classification

Summary:
FinLangNet is a novel approach for credit scoring that utilizes deep learning to generate multi-scale distributions of a user's future behavior by transforming tabular data into sequential representations. By incorporating prompt-based training inspired by Large Language Models, FinLangNet introduces prompts at different levels to capture user behavior effectively. Experimental results show that FinLangNet outperforms traditional methods like XGBoost, achieving significant improvements in performance metrics such as KS metric and relative bad debt rate. Moreover, FinLangNet demonstrates superior performance in time series classification tasks on public UEA archives, highlighting its scalability and adaptability in financial scenarios.<br /><br />Summary: <div>
arXiv:2404.13004v4 Announce Type: replace 
Abstract: Recent industrial credit scoring models remain heavily reliant on manually tuned statistical learning methods. While deep learning offers promising solutions, its effectiveness is often limited by the complexity of financial data, particularly in long-horizon scenarios. In this work, we propose FinLangNet, which addresses credit scoring by reframing it as the task of generating multi-scale distributions of a user's future behavior. Within this framework, tabular data is transformed into sequential representations, enabling the generation of user embeddings across multiple temporal scales. Inspired by the recent success of prompt-based training in Large Language Models (LLMs), FinLangNet also introduces two types of prompts to model and capture user behavior at both the feature-granularity and user-granularity levels. Experimental results demonstrate that FinLangNet outperforms the online XGBoost benchmark, achieving a 7.2\% improvement in KS metric performance and a 9.9\% reduction in the relative bad debt rate. Furthermore, FinLangNet exhibits superior performance on public UEA archives, underscoring its scalability and adaptability in time series classification tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Pathways in Reaction Networks guided by Energy Barriers using Integer Linear Programming</title>
<link>https://arxiv.org/abs/2504.10609</link>
<guid>https://arxiv.org/abs/2504.10609</guid>
<content:encoded><![CDATA[
<div> methodology, pathways, reaction networks, kinetic information, energy barriers 
Summary: 
This study introduces a computational methodology for exploring synthesis pathways in chemical reaction networks. The approach incorporates integer linear programming and directed hypergraphs to model reaction networks. Multiple pathways meeting search criteria can be ranked based on an objective function maximizing pathway probability. An automated pipeline estimates energy barriers for reactions in the network. This methodology allows for flexible and kinetically informed pathway exploration on large reaction networks, even when lacking kinetic annotations. It can be applied to networks generated via molecular space expansion approaches. <div>
arXiv:2504.10609v2 Announce Type: replace 
Abstract: Analyzing synthesis pathways for target molecules in a chemical reaction network annotated with information on the kinetics of individual reactions is an area of active study. This work presents a computational methodology for searching for pathways in reaction networks which is based on integer linear programming and the modeling of reaction networks by directed hypergraphs. Often multiple pathways fit the given search criteria. To rank them, we develop an objective function based on physical arguments maximizing the probability of the pathway. We furthermore develop an automated pipeline to estimate the energy barriers of individual reactions in reaction networks. Combined, the methodology facilitates flexible and kinetically informed pathway investigations on large reaction networks by computational means, even for networks coming without kinetic annotation, such as those created via generative approaches for expanding molecular spaces.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models</title>
<link>https://arxiv.org/abs/2411.02083</link>
<guid>https://arxiv.org/abs/2411.02083</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, quantitative reasoning, Number Token Loss, regression-like loss, token level<br />
Summary:<br />
Language models excel at text generation but struggle with quantitative reasoning due to a lack of inductive bias for numbers. The Cross Entropy loss, designed for nominal scale data, hinders number token proximity understanding. To address this, a Number Token Loss (NTL) is proposed, minimizing Lp norm or Wasserstein distance between real and predicted number tokens. NTL enhances math-related task performance without increasing runtime, even matching regression head performance in a direct comparison. Scaling to 3B parameter models shows improved performance, showcasing potential for seamless integration into Large Language Models (LLMs). This work aims to inspire LLM developers to enhance pretraining objectives. <div>
arXiv:2411.02083v2 Announce Type: replace-cross 
Abstract: While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the Cross Entropy loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the Lp norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the Cross Entropy objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope that this work can inspire LLM developers to improve their pretraining objectives. The code is available via: https://tum-ai.github.io/number-token-loss/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemToolAgent: The Impact of Tools on Language Agents for Chemistry Problem Solving</title>
<link>https://arxiv.org/abs/2411.07228</link>
<guid>https://arxiv.org/abs/2411.07228</guid>
<content:encoded><![CDATA[
<div> ChemToolAgent, Chemistry tasks, Large language models, Evaluation, Specialized tools<br />
Summary:<br />
ChemToolAgent, an enhanced chemistry agent over ChemCrow, was developed to improve large language models (LLMs) for chemistry problem solving. Comprehensive evaluation showed that while ChemToolAgent did not consistently outperform LLMs without tools, it excelled in specialized chemistry tasks like synthesis prediction when augmented with specialized tools. However, for general chemistry questions like those in exams, agents' ability to reason correctly with chemistry knowledge proved to be more critical, and tool augmentation did not always provide added benefits. Error analysis with a chemistry expert supported these findings, highlighting the importance of considering task specificity and the role of tools in enhancing LLMs for diverse chemistry tasks. <div>
arXiv:2411.07228v3 Announce Type: replace-cross 
Abstract: To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemToolAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemToolAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents' ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Diffusion Autoencoder for Test-time Adapting Prediction of Complex Systems</title>
<link>https://arxiv.org/abs/2505.17459</link>
<guid>https://arxiv.org/abs/2505.17459</guid>
<content:encoded><![CDATA[
<div> SparseDiff, test-time adaptation, sparse encoder, graph neural ordinary differential equation, spatial interactions <br />
Summary: SparseDiff is a novel approach for predicting the behavior of complex systems by dynamically updating the encoding scheme to capture emergent spatiotemporal structures. It utilizes a codebook-based sparse encoder to create a sparse graph topology of the spatial domain, coupled with a graph neural ordinary differential equation to model dynamics and a diffusion decoder for reconstruction. This method enables autoregressive prediction of spatiotemporal evolution while adapting the sparse topological structure to accommodate emerging spatial patterns through adaptive re-encoding. Extensive evaluations show that SparseDiff significantly reduces prediction errors compared to baselines, requiring only a fraction of the spatial resolution. <div>
arXiv:2505.17459v1 Announce Type: new 
Abstract: Predicting the behavior of complex systems is critical in many scientific and engineering domains, and hinges on the model's ability to capture their underlying dynamics. Existing methods encode the intrinsic dynamics of high-dimensional observations through latent representations and predict autoregressively. However, these latent representations lose the inherent spatial structure of spatiotemporal dynamics, leading to the predictor's inability to effectively model spatial interactions and neglect emerging dynamics during long-term prediction. In this work, we propose SparseDiff, introducing a test-time adaptation strategy to dynamically update the encoding scheme to accommodate emergent spatiotemporal structures during the long-term evolution of the system. Specifically, we first design a codebook-based sparse encoder, which coarsens the continuous spatial domain into a sparse graph topology. Then, we employ a graph neural ordinary differential equation to model the dynamics and guide a diffusion decoder for reconstruction. SparseDiff autoregressively predicts the spatiotemporal evolution and adjust the sparse topological structure to adapt to emergent spatiotemporal patterns by adaptive re-encoding. Extensive evaluations on representative systems demonstrate that SparseDiff achieves an average prediction error reduction of 49.99\% compared to baselines, requiring only 1\% of the spatial resolution.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Imputation before Prediction: A New Computational Paradigm for De Novo Peptide Sequencing</title>
<link>https://arxiv.org/abs/2505.17524</link>
<guid>https://arxiv.org/abs/2505.17524</guid>
<content:encoded><![CDATA[
<div> Keywords: de novo peptide sequencing, missing fragmentation, latent imputation, set-prediction, performance improvement

Summary:
De novo peptide sequencing is a critical technique for determining peptide sequences directly from mass spectrometry data. The new computational approach, LIPNovo, addresses the challenge of missing fragmentation in spectra by imputing missing information in the latent space using theoretical peak profiles of target peptides. By framing the imputation as a set-prediction problem and utilizing learnable peak queries, LIPNovo effectively supplements missing data during inference, leading to improved performance. Experimental results on benchmark datasets show that LIPNovo surpasses existing methods significantly. The code for LIPNovo is available on GitHub for further exploration and application. 

<br /><br />Summary: De novo peptide sequencing is vital for peptide identification from mass spectrometry data. LIPNovo introduces a novel approach to handle missing fragmentation by imputing information in the latent space, enhancing performance through set-prediction techniques. Experimentally, LIPNovo outperforms state-of-the-art methods, underscoring its effectiveness in peptide sequencing tasks. The availability of the code on GitHub enables wider adoption and exploration of LIPNovo's capabilities. <div>
arXiv:2505.17524v1 Announce Type: new 
Abstract: De novo peptide sequencing is a fundamental computational technique for ascertaining amino acid sequences of peptides directly from tandem mass spectrometry data, eliminating the need for reference databases. Cutting-edge models usually encode the observed mass spectra into latent representations from which peptides are predicted autoregressively. However, the issue of missing fragmentation, attributable to factors such as suboptimal fragmentation efficiency and instrumental constraints, presents a formidable challenge in practical applications. To tackle this obstacle, we propose a novel computational paradigm called \underline{\textbf{L}}atent \underline{\textbf{I}}mputation before \underline{\textbf{P}}rediction (LIPNovo). LIPNovo is devised to compensate for missing fragmentation information within observed spectra before executing the final peptide prediction. Rather than generating raw missing data, LIPNovo performs imputation in the latent space, guided by the theoretical peak profile of the target peptide sequence. The imputation process is conceptualized as a set-prediction problem, utilizing a set of learnable peak queries to reason about the relationships among observed peaks and directly generate the latent representations of theoretical peaks through optimal bipartite matching. In this way, LIPNovo manages to supplement missing information during inference and thus boosts performance. Despite its simplicity, experiments on three benchmark datasets demonstrate that LIPNovo outperforms state-of-the-art methods by large margins. Code is available at \href{https://github.com/usr922/LIPNovo}{https://github.com/usr922/LIPNovo}.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4D-CTA Image and geometry dataset for kinematic analysis of abdominal aortic aneurysms</title>
<link>https://arxiv.org/abs/2505.17647</link>
<guid>https://arxiv.org/abs/2505.17647</guid>
<content:encoded><![CDATA[
<div> Dataset, Abdominal Aortic Aneurysm, 4D-CTA, Biomechanics, Image Registration

Summary:<br /><br />This article presents a dataset of 4D-CTA images and patient-specific AAA geometries from ten patients for kinematic analysis of abdominal aortic aneurysms (AAA). The dataset includes images captured throughout the cardiac cycle, allowing for the study of AAA wall displacement and strain. Synthetic ground truth data from Patient 1's image is also included for method verification. The dataset facilitates non-invasive analysis of AAA kinematics using an image registration-based approach. The use of open-source file formats enhances the applicability and reusability of the dataset in AAA biomechanics studies. The research was conducted at the ISML-UWA, using images acquired at Fiona Stanley Hospital in Western Australia. This dataset provides valuable information for understanding the biomechanics of AAA and could aid in the development of improved diagnostic and treatment strategies.<br />Summary: <div>
arXiv:2505.17647v1 Announce Type: new 
Abstract: This article presents a dataset used in the article "Kinematics of Abdominal Aortic Aneurysms" [arXiv:2405.13377], published in the Journal of Biomechanics. The dataset is publicly available for download from the Zenodo data repository (https://doi.org/10.5281/zenodo.15477710). The dataset includes time-resolved 3D computed tomography angiography (4D-CTA) images of abdominal aortic aneurysm (AAA) captured throughout the cardiac cycle from ten patients diagnosed with AAA, along with ten patient-specific AAA geometries extracted from these images. Typically, the 4D-CTA dataset for each patient contains ten electrocardiogram (ECG)-gated 3D-CTA image frames acquired over a cardiac cycle, capturing both the systolic and diastolic phases of the AAA configuration. For method verification, the dataset also includes synthetic ground truth data generated from Patient 1's 3D-CTA AAA image in the diastolic phase. The ground truth data includes the patient-specific finite element (FE) biomechanical model and a synthetic systolic 3D-CTA image. The synthetic systolic image was generated by warping Patient 1's diastolic 3D-CTA image using the realistic displacement field obtained from the AAA biomechanical FE model. The images were acquired at Fiona Stanley Hospital in Western Australia and provided to the researchers at the Intelligent Systems for Medicine Laboratory at The University of Western Australia (ISML-UWA), where image-based AAA kinematic analysis was performed. Our dataset enabled the analysis of AAA wall displacement and strain throughout the cardiac cycle using a non-invasive, in vivo, image registration-based approach. The use of widely adopted, open-source file formats (NRRD for images and STL for geometries) facilitates broad applicability and reusability in AAA biomechanics studies that require patient-specific geometry and information about AAA kinematics during cardiac cycle.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A brief review of the Deep BSDE method for solving high-dimensional partial differential equations</title>
<link>https://arxiv.org/abs/2505.17032</link>
<guid>https://arxiv.org/abs/2505.17032</guid>
<content:encoded><![CDATA[
<div> Deep BSDE method, high-dimensional PDEs, numerical computation, deep learning techniques, neural networks <br />
<br />
Summary: 
The article discusses the challenges posed by high-dimensional partial differential equations (PDEs) in traditional numerical computation methods. It introduces the Deep BSDE method, which utilizes deep learning techniques to effectively solve nonlinear PDEs in high dimensions. Since its inception in 2017, the Deep BSDE method has generated widespread interest in using neural networks for tackling high-dimensional PDEs. The article briefly outlines the method, its subsequent developments, and highlights the active research being conducted in this area. The Deep BSDE method has opened up new possibilities for solving complex PDEs in very high dimensions, offering a promising approach for addressing the curse of dimensionality in numerical computations. Future directions for research in this field are also discussed, suggesting potential advancements and applications of deep learning techniques in solving high-dimensional PDEs. <div>
arXiv:2505.17032v1 Announce Type: cross 
Abstract: High-dimensional partial differential equations (PDEs) pose significant challenges for numerical computation due to the curse of dimensionality, which limits the applicability of traditional mesh-based methods. Since 2017, the Deep BSDE method has introduced deep learning techniques that enable the effective solution of nonlinear PDEs in very high dimensions. This innovation has sparked considerable interest in using neural networks for high-dimensional PDEs, making it an active area of research. In this short review, we briefly sketch the Deep BSDE method, its subsequent developments, and future directions for the field.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</title>
<link>https://arxiv.org/abs/2505.17050</link>
<guid>https://arxiv.org/abs/2505.17050</guid>
<content:encoded><![CDATA[
<div> Keywords: Project-Based Learning, Multimodal Large Language Models, PBLBench, Analytic Hierarchy Process, Education

Summary:
PBLBench is a new benchmark designed to evaluate complex reasoning tasks in Project-Based Learning using multimodal large language models. The benchmark challenges models with tasks that mirror those handled by human experts, assessing their performance using structured and weighted evaluation criteria derived from the Analytic Hierarchy Process. The study tested 15 leading models and found that even the most advanced ones achieved only 59% rank accuracy, highlighting the difficulty of the benchmark. This indicates the challenges presented by real-world educational tasks and the need for more capable AI agents to assist teachers effectively. PBLBench aims to improve teacher workload and enhance educational productivity by providing a rigorous evaluation platform for multimodal large language models in educational settings.<br /><br />Summary: <div>
arXiv:2505.17050v1 Announce Type: cross 
Abstract: Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback</title>
<link>https://arxiv.org/abs/2505.17873</link>
<guid>https://arxiv.org/abs/2505.17873</guid>
<content:encoded><![CDATA[
<div> simulator, experiment-guided ranking, hypothesis, chemistry, automated scientific discovery

Summary:
The paper introduces the concept of experiment-guided ranking in the context of hypothesis prioritization in natural sciences. Existing approaches in automated scientific discovery focus on pre-experiment ranking without considering empirical outcomes. To address this gap, the authors propose a simulator that models hypothesis performance based on similarity to a known ground truth hypothesis, incorporating noise. They validate the simulator using a dataset of chemistry hypotheses with experimentally reported outcomes. The proposed method clusters hypotheses based on functional characteristics and uses insights from simulated experimental feedback to prioritize candidates. Experimental results demonstrate that the method outperforms pre-experiment baselines and strong ablations in hypothesis ranking. <div>
arXiv:2505.17873v1 Announce Type: cross 
Abstract: Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on large language model's internal reasoning without incorporating empirical outcomes from experiments. We introduce the task of experiment-guided ranking, which aims to prioritize candidate hypotheses based on the results of previously tested ones. However, developing such strategies is challenging due to the impracticality of repeatedly conducting real experiments in natural science domains. To address this, we propose a simulator grounded in three domain-informed assumptions, modeling hypothesis performance as a function of similarity to a known ground truth hypothesis, perturbed by noise. We curate a dataset of 124 chemistry hypotheses with experimentally reported outcomes to validate the simulator. Building on this simulator, we develop a pseudo experiment-guided ranking method that clusters hypotheses by shared functional characteristics and prioritizes candidates based on insights derived from simulated experimental feedback. Experiments show that our method outperforms pre-experiment baselines and strong ablations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SINDyG: Sparse Identification of Nonlinear Dynamical Systems from Graph-Structured Data</title>
<link>https://arxiv.org/abs/2409.04463</link>
<guid>https://arxiv.org/abs/2409.04463</guid>
<content:encoded><![CDATA[
<div> Sparse Identification of Nonlinear Dynamical Systems, Machine Learning, Sparsity-promoting techniques, Network structure, Neuronal dynamics

Summary: 
The article introduces a new method called Sparse Identification of Nonlinear Dynamical Systems from Graph-structured data (SINDyG), which incorporates network structure into sparse regression for identifying model parameters that explain underlying network dynamics. Unlike existing methods, SINDyG considers interactions between subsystems, allowing for capturing small changes in emergent system behavior. The method is applied to neuronal dynamics, modeling macroscopic oscillations of a neuron population with the extended Stuart-Landau equation. Computational experiments demonstrate improved accuracy and simplicity in identifying network dynamics compared to the original SINDy approach. This innovative approach of combining machine learning with sparsity-promoting techniques opens up new possibilities for extracting governing equations from data in various scientific fields. <br /><br />Summary: <div>
arXiv:2409.04463v3 Announce Type: replace-cross 
Abstract: The combination of machine learning (ML) and sparsity-promoting techniques is enabling direct extraction of governing equations from data, revolutionizing computational modeling in diverse fields of science and engineering. The discovered dynamical models could be used to address challenges in climate science, neuroscience, ecology, finance, epidemiology, and beyond. However, most existing sparse identification methods for discovering dynamical systems treat the whole system as one without considering the interactions between subsystems. As a result, such models are not able to capture small changes in the emergent system behavior. To address this issue, we developed a new method called Sparse Identification of Nonlinear Dynamical Systems from Graph-structured data (SINDyG), which incorporates the network structure into sparse regression to identify model parameters that explain the underlying network dynamics. We showcase the application of our proposed method using several case studies of neuronal dynamics, where we model the macroscopic oscillation of a population of neurons using the extended Stuart-Landau (SL) equation and utilize the SINDyG method to identify the underlying nonlinear dynamics. Our extensive computational experiments validate the improved accuracy and simplicity of discovered network dynamics when compared to the original SINDy approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A finite element solver for a thermodynamically consistent electrolyte model</title>
<link>https://arxiv.org/abs/2505.16296</link>
<guid>https://arxiv.org/abs/2505.16296</guid>
<content:encoded><![CDATA[
<div> finite element solver, electrolyte model, multicomponent ionic transport, non-equilibrium thermodynamics, FEniCSx platform
Summary:<br /><br />In this study, a finite element solver for a thermodynamically consistent electrolyte model is presented. The model accurately captures multicomponent ionic transport, incorporating steric effects, solvation, and pressure coupling. Rooted in non-equilibrium thermodynamics, the model enforces mass conservation, charge neutrality, and entropy production. It surpasses classical frameworks by using modified partial mass balances, the electrostatic Poisson equation, and a momentum balance with electrostatic potential, atomic fractions, and pressure. The solver, implemented in FEniCSx, handles one- and two-dimensional problems efficiently with various boundary conditions. It demonstrates excellent convergence behavior and robustness, validated against benchmark problems. Simulations showcase critical electrolyte phenomena such as electric double layer formation, rectification behavior, and the impacts of solvation number, Debye length, and compressibility. The modular variational formulation allows extension to complex electrochemical systems with multiple ionic species of asymmetric valences.<br /><br /> <div>
arXiv:2505.16296v1 Announce Type: new 
Abstract: In this study, we present a finite element solver for a thermodynamically consistent electrolyte model that accurately captures multicomponent ionic transport by incorporating key physical phenomena such as steric effects, solvation, and pressure coupling. The model is rooted in the principles of non-equilibrium thermodynamics and strictly enforces mass conservation, charge neutrality, and entropy production. It extends beyond classical frameworks like the Nernst-Planck system by employing modified partial mass balances, the electrostatic Poisson equation, and a momentum balance expressed in terms of electrostatic potential, atomic fractions, and pressure, thereby enhancing numerical stability and physical consistency. Implemented using the FEniCSx platform, the solver efficiently handles one- and two-dimensional problems with varied boundary conditions and demonstrates excellent convergence behavior and robustness. Validation against benchmark problems confirms its improved physical fidelity, particularly in regimes characterized by high ionic concentrations and strong electrochemical gradients. Simulation results reveal critical electrolyte phenomena, including electric double layer formation, rectification behavior, and the effects of solvation number, Debye length, and compressibility. The solver's modular variational formulation facilitates its extension to complex electrochemical systems involving multiple ionic species with asymmetric valences.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling</title>
<link>https://arxiv.org/abs/2505.16573</link>
<guid>https://arxiv.org/abs/2505.16573</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Stock Price Prediction, Cross-Stock Trend Integration, Collaborative Learning, Data Privacy<br />
<br />
Summary: 
The article introduces a novel approach, Cross-Stock Trend Integration (CSTI), for stock price prediction by merging local stock patterns into a global model. Inspired by Federated Learning (FL), this method allows collaborative learning across distributed datasets without sharing raw data, maintaining data privacy. Individual stock models are trained separately and then merged to create a unified global model, which is fine-tuned on specific stock data for local relevance. CSTI enables parallel training, optimizing computational resources and reducing training time. Extensive experiments show that CSTI outperforms benchmark models and enhances predictive capabilities of existing approaches, providing a robust alternative to single-stock learning methodologies. <div>
arXiv:2505.16573v1 Announce Type: new 
Abstract: Stock price prediction is a critical area of financial forecasting, traditionally approached by training models using the historical price data of individual stocks. While these models effectively capture single-stock patterns, they fail to leverage potential correlations among stock trends, which could improve predictive performance. Current single-stock learning methods are thus limited in their ability to provide a broader understanding of price dynamics across multiple stocks. To address this, we propose a novel method that merges local patterns into a global understanding through cross-stock pattern integration. Our strategy is inspired by Federated Learning (FL), a paradigm designed for decentralized model training. FL enables collaborative learning across distributed datasets without sharing raw data, facilitating the aggregation of global insights while preserving data privacy. In our adaptation, we train models on individual stock data and iteratively merge them to create a unified global model. This global model is subsequently fine-tuned on specific stock data to retain local relevance. The proposed strategy enables parallel training of individual stock models, facilitating efficient utilization of computational resources and reducing overall training time. We conducted extensive experiments to evaluate the proposed method, demonstrating that it outperforms benchmark models and enhances the predictive capabilities of state-of-the-art approaches. Our results highlight the efficacy of Cross-Stock Trend Integration (CSTI) in advancing stock price prediction, offering a robust alternative to traditional single-stock learning methodologies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Integration Strategies for ESD Protection and Termination in High-Speed LVDS Systems</title>
<link>https://arxiv.org/abs/2505.16200</link>
<guid>https://arxiv.org/abs/2505.16200</guid>
<content:encoded><![CDATA[
<div> Keywords: Electrostatic Discharge (ESD), protection diodes, termination resistors, Low Voltage Differential Signaling (LVDS), signal integrity maintenance<br />
Summary:<br />
This technical article delves into the integration strategies for ESD protection diodes and termination resistors in LVDS designs. It covers critical aspects such as protection mechanisms, design considerations, impedance matching, and placement optimization techniques. The article emphasizes the significance of maintaining signal integrity and ensuring protection effectiveness in LVDS systems. It provides detailed analyses of layout considerations and advanced design strategies to address common integration challenges. The importance of balancing protection requirements with signal integrity demands is highlighted, with practical guidelines offered for implementing robust high-speed digital systems. The article discusses various methodologies for optimizing performance and validating designs, offering designers a comprehensive framework for creating reliable LVDS systems. <div>
arXiv:2505.16200v1 Announce Type: cross 
Abstract: This technical article explores comprehensive strategies for integrating Electrostatic Discharge (ESD) protection diodes and termination resistors in LowVoltage Differential Signaling (LVDS) designs. The article examines critical aspects of protection mechanisms, design considerations, impedance matching, and placement optimization techniques. Through detailed analysis of layout considerations and advanced design strategies, the article presents solutions for common integration challenges. It emphasizes the importance of signal integrity maintenance and protection effectiveness while providing practical guidelines for implementing robust LVDS systems. Various methodologies for performance optimization and validation are discussed, offering designers a thorough framework for creating reliable high-speed digital systems that balance protection requirements with signal integrity demands.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGLDBench: A Benchmark Suite for Stress-Guided Lightweight 3D Designs</title>
<link>https://arxiv.org/abs/2501.03068</link>
<guid>https://arxiv.org/abs/2501.03068</guid>
<content:encoded><![CDATA[
<div> benchmark, material layout strategies, stiff, lightweight designs, 3D domains, stress-guided<br />
<br />
Summary: 
The Stress-Guided Lightweight Design Benchmark (SGLDBench) is introduced as a benchmark suite for assessing material layout strategies in 3D domains to create stiff, lightweight designs. It includes six reference strategies and a multigrid elasticity solver for efficient execution and stiffness validation. The benchmark enables systematic comparison of design strategies based on mechanical properties, supports diverse load conditions, and offers high-resolution designs and stiffness analysis. Visual analysis is emphasized to understand the relationship between design geometry and stress distribution, providing insights into design strategy properties and behaviors. The benchmark's features are showcased through experiments comparing reference strategy results in terms of geometric and mechanical properties. <div>
arXiv:2501.03068v2 Announce Type: replace 
Abstract: We introduce the Stress-Guided Lightweight Design Benchmark (SGLDBench), a comprehensive benchmark suite for applying and evaluating material layout strategies to generate stiff, lightweight designs in 3D domains. SGLDBench provides a seamlessly integrated simulation and analysis framework, including six reference strategies and a scalable multigrid elasticity solver to efficiently execute these strategies and validate the stiffness of their results. This facilitates the systematic analysis and comparison of design strategies based on the mechanical properties they achieve. SGLDBench enables the evaluation of diverse load conditions and, through the tight integration of the solver, supports high-resolution designs and stiffness analysis. Additionally, SGLDBench emphasizes visual analysis to explore the relationship between the geometric structure of a design and the distribution of stresses, offering insights into the specific properties and behaviors of different design strategies. SGLDBench's specific features are highlighted through several experiments, comparing the results of reference strategies with respect to geometric and mechanical properties.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stochastic Dynamic Network Model of the Space Environment</title>
<link>https://arxiv.org/abs/2411.03173</link>
<guid>https://arxiv.org/abs/2411.03173</guid>
<content:encoded><![CDATA[
<div> network model, space environment, stochastic dynamics, species, collision avoidance

Summary:
The article proposes a network model to study the space environment as a dynamic system with different species of objects represented as nodes connected by stochastic links. Stochastic dynamic equations are derived to describe the evolution of the network, replicating existing results on the space environment's evolution. The analysis of the network structure identifies critical species and orbit regimes that impact the environment the most. The concept of carrying capacity in space is introduced based on the stability of network equilibria. Using current object populations and launch traffic forecasts, the model demonstrates how different policies can affect collision avoidance and post-mission disposal maneuvers. The proposed network model provides a framework for understanding and managing the space environment effectively. 

<br /><br />Summary: <div>
arXiv:2411.03173v2 Announce Type: replace-cross 
Abstract: This work proposes to model the space environment as a stochastic dynamic network where each node is a group of objects of a given class, or species, and their relationship is represented by stochastic links. A set of stochastic dynamic equations, governing the evolution of the network, are derived from the network structure and topology. It will be shown that the proposed system of stochastic dynamic equations well reproduces existing results on the evolution of the space environment. The analysis of the structure of the network and relationships among node can help to understand which species of objects and orbit regimes are more critical and affect the most the future evolution of the space environment. In analogy with ecological networks, we develop a theory of the carrying capacity of space based on the stability of equilibria of the network dynamics. Some examples are presented starting from the current population of resident objects and different launch traffic forecast models. It will be shown how the proposed network model can be used to study the effect of the adoption of different policies on the execution of collision avoidance and post mission disposal manoeuvres.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deployment of Traditional and Hybrid Machine Learning for Critical Heat Flux Prediction in the CTF Thermal Hydraulics Code</title>
<link>https://arxiv.org/abs/2505.14701</link>
<guid>https://arxiv.org/abs/2505.14701</guid>
<content:encoded><![CDATA[
<div> machine learning, critical heat flux, hybrid models, subchannel code, nuclear reactors

Summary:
- The study focuses on predicting critical heat flux (CHF) using machine learning (ML) models, crucial for safety in nuclear reactors.
- Traditional empirical correlations for CHF prediction often show discrepancies, prompting the need for more reliable methods.
- Hybrid models, combining data-driven ML with physics-based models, show improved accuracy over conventional methods.
- Integration of ML-based CHF models into subchannel codes proves effective in enhancing prediction performance.
- The study's results demonstrate that ML-based models, integrated with hybrid approaches, can reduce CHF overprediction and enhance overall accuracy, highlighting their potential in improving operational efficiency and safety in nuclear reactor systems.<br /><br />Summary: <div>
arXiv:2505.14701v1 Announce Type: new 
Abstract: Critical heat flux (CHF) marks the transition from nucleate to film boiling, where heat transfer to the working fluid can rapidly deteriorate. Accurate CHF prediction is essential for efficiency, safety, and preventing equipment damage, particularly in nuclear reactors. Although widely used, empirical correlations frequently exhibit discrepancies in comparison with experimental data, limiting their reliability in diverse operational conditions. Traditional machine learning (ML) approaches have demonstrated the potential for CHF prediction but have often suffered from limited interpretability, data scarcity, and insufficient knowledge of physical principles. Hybrid model approaches, which combine data-driven ML with physics-based models, mitigate these concerns by incorporating prior knowledge of the domain. This study integrated a purely data-driven ML model and two hybrid models (using the Biasi and Bowring CHF correlations) within the CTF subchannel code via a custom Fortran framework. Performance was evaluated using two validation cases: a subset of the Nuclear Regulatory Commission CHF database and the Bennett dryout experiments. In both cases, the hybrid models exhibited significantly lower error metrics in comparison with conventional empirical correlations. The pure ML model remained competitive with the hybrid models. Trend analysis of error parity indicates that ML-based models reduce the tendency for CHF overprediction, improving overall accuracy. These results demonstrate that ML-based CHF models can be effectively integrated into subchannel codes and can potentially increase performance in comparison with conventional methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local-Global Associative Frames for Symmetry-Preserving Crystal Structure Modeling</title>
<link>https://arxiv.org/abs/2505.15315</link>
<guid>https://arxiv.org/abs/2505.15315</guid>
<content:encoded><![CDATA[
<div> Keywords: crystal structures, invariance, symmetry, frames, crystal property prediction

Summary:
In the study of crystal structures, maintaining invariance to rotational transformations is essential for accurate property prediction. Traditional approaches using global or local frames have limitations in capturing both local structure heterogeneity and preserving crystal symmetry. To address this, the proposed Symmetry-Preserving Frames (SPFrame) method constructs invariant local frames while integrating global structural information to enforce invariance to SO(3) rotations. The SPFrame approach outperforms existing techniques and baselines in crystal property prediction tasks. By combining local and global frame information, SPFrame achieves superior performance in capturing the complexity of crystal structures while maintaining symmetry, demonstrating its efficacy in improving predictive accuracy for various crystal properties. 

<br /><br />Summary: <div>
arXiv:2505.15315v1 Announce Type: new 
Abstract: Crystal structures are defined by the periodic arrangement of atoms in 3D space, inherently making them equivariant to SO(3) group. A fundamental requirement for crystal property prediction is that the model's output should remain invariant to arbitrary rotational transformations of the input structure. One promising strategy to achieve this invariance is to align the given crystal structure into a canonical orientation with appropriately computed rotations, or called frames. However, existing work either only considers a global frame or solely relies on more advanced local frames based on atoms' local structure. A global frame is too coarse to capture the local structure heterogeneity of the crystal, while local frames may inadvertently disrupt crystal symmetry, limiting their expressivity. In this work, we revisit the frame design problem for crystalline materials and propose a novel approach to construct expressive Symmetry-Preserving Frames, dubbed as SPFrame, for modeling crystal structures. Specifically, this local-global associative frame constructs invariant local frames rather than equivariant ones, thereby preserving the symmetry of the crystal. In parallel, it integrates global structural information to construct an equivariant global frame to enforce SO(3) invariance. Extensive experimental results demonstrate that SPFrame consistently outperforms traditional frame construction techniques and existing crystal property prediction baselines across multiple benchmark tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Open Earth Science as Fast and Accessible as Natural Language</title>
<link>https://arxiv.org/abs/2505.15690</link>
<guid>https://arxiv.org/abs/2505.15690</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural language processing, Earth observation, Large Language Models, Data analysis, Software framework

Summary: 
This study explores the feasibility of using Large Language Models (LLMs) for natural-language-driven earth observation data analysis. The research focuses on achieving high accuracy, interactive latencies, low costs, and open source software. Techniques such as model scaling, prompt optimization, and inference-time scaling optimization are employed to achieve near 100% accuracy across multiple metrics. The analysis also considers cost, latency, and maintainability of the techniques. Opportunities for further research, framework development, and ongoing work towards a comprehensive solution are identified. Collaboration and contributions are encouraged for the advancement of this field.

<br /><br />Summary: <div>
arXiv:2505.15690v1 Announce Type: new 
Abstract: Is natural-language-driven earth observation data analysis now feasible with the assistance of Large Language Models (LLMs)? For open science in service of public interest, feasibility requires reliably high accuracy, interactive latencies, low (sustainable) costs, open LLMs, and openly maintainable software -- hence, the challenge. What are the techniques and programming system requirements necessary for satisfying these constraints, and what is the corresponding development and maintenance burden in practice? This study lays the groundwork for exploring these questions, introducing an impactful earth science use-case, and providing a software framework with evaluation data and metrics, along with initial results from employing model scaling, prompt-optimization, and inference-time scaling optimization techniques. While we attain high accuracy (near 100%) across 10 of 11 metrics, the analysis further considers cost (token-spend), latency, and maintainability across this space of techniques. Finally, we enumerate opportunities for further research, general programming and evaluation framework development, and ongoing work for a comprehensive, deployable solution. This is a call for collaboration and contribution.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
<link>https://arxiv.org/abs/2505.15155</link>
<guid>https://arxiv.org/abs/2505.15155</guid>
<content:encoded><![CDATA[
<div> Framework, Quantitative Finance, Automation, Factor-model, Co-optimization
Summary: 
RD-Agent(Q) is a data-centric multi-agent framework designed to automate the research and development of quantitative strategies in financial markets. It breaks down the quant process into two stages: Research and Development, connected through a feedback loop with a multi-armed bandit scheduler for direction selection. The framework sets goal-aligned prompts, formulates hypotheses, and maps them to tasks in the Research stage, while using a code-generation agent, Co-STEER, to implement task-specific code in the Development stage. Empirical results show RD-Agent(Q) achieving higher annualized returns than classical factor libraries with fewer factors, and outperforming deep time-series models on real markets. The joint factor-model optimization provides a balance between predictive accuracy and strategy robustness. The code for RD-Agent(Q) is available on GitHub at: https://github.com/microsoft/RD-Agent.<br /><br />Summary: <div>
arXiv:2505.15155v1 Announce Type: cross 
Abstract: Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility. Despite advances in large language models and multi-agent systems, current quantitative research pipelines suffer from limited automation, weak interpretability, and fragmented coordination across key components such as factor mining and model innovation. In this paper, we propose R&amp;D-Agent for Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization. RD-Agent(Q) decomposes the quant process into two iterative stages: a Research stage that dynamically sets goal-aligned prompts, formulates hypotheses based on domain priors, and maps them to concrete tasks, and a Development stage that employs a code-generation agent, Co-STEER, to implement task-specific code, which is then executed in real-market backtests. The two stages are connected through a feedback stage that thoroughly evaluates experimental outcomes and informs subsequent iterations, with a multi-armed bandit scheduler for adaptive direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher annualized returns than classical factor libraries using 70% fewer factors, and outperforms state-of-the-art deep time-series models on real markets. Its joint factor-model optimization delivers a strong balance between predictive accuracy and strategy robustness. Our code is available at: https://github.com/microsoft/RD-Agent.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2505.15228</link>
<guid>https://arxiv.org/abs/2505.15228</guid>
<content:encoded><![CDATA[
<div> Keywords: CP-KAN, neural architecture, Chebyshev polynomial, QUBO, regression tasks <br />
Summary: <br />
- This article introduces the cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a neural architecture that combines Chebyshev polynomial basis functions and quadratic unconstrained binary optimization (QUBO).
- The primary contribution is a reformulation of the degree selection problem as a QUBO task, which significantly reduces complexity and allows for efficient degree selection across neurons.
- CP-KAN performs well in regression tasks with limited data, showing robustness to input scales and natural regularization properties from its polynomial basis.
- The architecture is theoretically linked to properties of financial time series, demonstrating its potential for analyzing financial data.
- Empirical validation across different domains shows that CP-KAN performs competitively compared to traditional architectures, particularly in scenarios where data efficiency and numerical stability are crucial. <div>
arXiv:2505.15228v1 Announce Type: cross 
Abstract: We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a neural architecture combining Chebyshev polynomial basis functions and quadratic unconstrained binary optimization (QUBO). Our primary contribution involves reformulating the degree selection problem as a QUBO task, reducing the complexity from $O(D^N)$ to a single optimization step per layer. This approach enables efficient degree selection across neurons while maintaining computational tractability. The architecture performs well in regression tasks with limited data, showing good robustness to input scales and natural regularization properties from its polynomial basis. Additionally, theoretical analysis establishes connections between CP-KAN's performance and properties of financial time series. Our empirical validation across multiple domains demonstrates competitive performance compared to several traditional architectures tested, especially in scenarios where data efficiency and numerical stability are important. Our implementation, including strategies for managing computational overhead in larger networks is available in Ref.~\citep{cpkan_implementation}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization of Probability Distributions via Divide-and-Conquer: Convergence and Error Propagation under Distributional Arithmetic Operations</title>
<link>https://arxiv.org/abs/2505.15283</link>
<guid>https://arxiv.org/abs/2505.15283</guid>
<content:encoded><![CDATA[
<div> algorithm, distribution, approximation, stability, convergence
<br />
Summary: 
This article examines a divide-and-conquer algorithm for approximating continuous one-dimensional probability distributions with finite mean. It conducts a numerical comparison with existing schemes, emphasizing the stability of discrete approximations during arithmetic operations. The study establishes an upper bound for the approximation error based on the Wasserstein-1 distance, applicable to all continuous distributions with finite mean. The algorithm often achieves optimal convergence rates, and numerical tests demonstrate its superior stability compared to other methods when subjected to arithmetic operations. <div>
arXiv:2505.15283v1 Announce Type: cross 
Abstract: This article studies a general divide-and-conquer algorithm for approximating continuous one-dimensional probability distributions with finite mean. The article presents a numerical study that compares pre-existing approximation schemes with a special focus on the stability of the discrete approximations when they undergo arithmetic operations. The main results are a simple upper bound of the approximation error in terms of the Wasserstein-1 distance that is valid for all continuous distributions with finite mean. In many use-cases, the studied method achieve optimal rate of convergence, and numerical experiments show that the algorithm is more stable than pre-existing approximation schemes in the context of arithmetic operations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elasto-acoustic wave propagation in geophysical media using hybrid high-order methods on general meshes</title>
<link>https://arxiv.org/abs/2505.15771</link>
<guid>https://arxiv.org/abs/2505.15771</guid>
<content:encoded><![CDATA[
<div> Keywords: HHO methods, elasto-acoustic waves, explicit schemes, implicit schemes, numerical simulations
<br />
Summary:
Hybrid high-order (HHO) methods are investigated for the space semi-discretization of coupled elasto-acoustic waves using explicit and implicit Runge-Kutta schemes for time discretization. The implementation of these schemes requires static condensation of face and cell unknowns with block-diagonal matrices. CFL stability limits of explicit schemes are estimated, and a comparison between explicit and implicit schemes shows the competitiveness of implicit schemes in various scenarios. Simulations in a 2D geophysical model demonstrate the geometrical flexibility of the HHO method in handling hybrid and nonconforming meshes, yielding results comparable to spectral element software. Overall, the study highlights the effectiveness of HHO methods in accurately simulating coupled elasto-acoustic waves with efficient time discretization strategies. 
<br /> <div>
arXiv:2505.15771v1 Announce Type: cross 
Abstract: Hybrid high-order (HHO) methods are numerical methods characterized by several interesting properties such as local conservativity, geometric flexibility and high-order accuracy. Here, HHO schemes are studied for the space semi-discretization of coupled elasto-acoustic waves in the time domain using a first-order formulation. Explicit and singly diagonal implicit Runge--Kutta (ERK & SDIRK) schemes are used for the time discretization. We show that an efficient implementation of explicit (resp. implicit) time schemes calls for a static condensation of the face (resp. cell) unknowns. Crucially, both static condensation procedures only involve block-diagonal matrices. Then, we provide numerical estimates for the CFL stability limit of ERK schemes and present a comparative study on the efficiency of explicit versus implicit schemes. Our findings indicate that implicit time schemes remain competitive in many situations. Finally, simulations in a 2D realistic geophysical configuration are performed, illustrating the geometrical flexibility of the HHO method: both hybrid (triangular and quadrangular) and nonconforming (with hanging nodes) meshes are easily handled, delivering results of comparable accuracy to a reference spectral element software based on tensorized elements.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Prediction-Assisted Safe Reinforcement Learning for Electric Vehicle Charging Station Recommendation in Dynamically Coupled Transportation-Power Systems</title>
<link>https://arxiv.org/abs/2407.20679</link>
<guid>https://arxiv.org/abs/2407.20679</guid>
<content:encoded><![CDATA[
<div> Recommendation, Electric Vehicles, Transportation-Power Systems, Reinforcement Learning, Constrained Markov Decision Process

Summary:
- The study tackles the en-route charging station recommendation problem for electric vehicles in dynamically coupled transportation-power systems.
- The objective is to maximize overall traffic efficiency while ensuring power grid safety, a novel approach in existing literature.
- The problem is formulated as a constrained Markov decision process (CMDP) and addressed using an online prediction-assisted safe reinforcement learning (OP-SRL) method.
- Challenges of constrained optimization and uncertain delays are overcome through innovative approaches such as Lagrangian method and online sequence-to-sequence predictor.
- Comprehensive experimental studies demonstrate the superior performance of the proposed method in road network efficiency, power grid safety, and EV user satisfaction.
<br /><br />Summary: <div>
arXiv:2407.20679v2 Announce Type: replace 
Abstract: With the proliferation of electric vehicles (EVs), the transportation network and power grid become increasingly interdependent and coupled via charging stations. The concomitant growth in charging demand has posed challenges for both networks, highlighting the importance of charging coordination. Existing literature largely overlooks the interactions between power grid security and traffic efficiency. In view of this, we study the en-route charging station (CS) recommendation problem for EVs in dynamically coupled transportation-power systems. The system-level objective is to maximize the overall traffic efficiency while ensuring the safety of the power grid. This problem is for the first time formulated as a constrained Markov decision process (CMDP), and an online prediction-assisted safe reinforcement learning (OP-SRL) method is proposed to learn the optimal and secure policy by extending the PPO method. To be specific, we mainly address two challenges. First, the constrained optimization problem is converted into an equivalent unconstrained optimization problem by applying the Lagrangian method. Second, to account for the uncertain long-time delay between performing CS recommendation and commencing charging, we put forward an online sequence-to-sequence (Seq2Seq) predictor for state augmentation to guide the agent in making forward-thinking decisions. Finally, we conduct comprehensive experimental studies based on the Nguyen-Dupuis network and a large-scale real-world road network, coupled with IEEE 33-bus and IEEE 69-bus distribution systems, respectively. Results demonstrate that the proposed method outperforms baselines in terms of road network efficiency, power grid safety, and EV user satisfaction. The case study on the real-world network also illustrates the applicability in the practical context.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Method for Satellite Pattern-of-Life Identification</title>
<link>https://arxiv.org/abs/2412.10814</link>
<guid>https://arxiv.org/abs/2412.10814</guid>
<content:encoded><![CDATA[
<div> machine learning, satellite behavior, pattern-of-life identification, diffusion model, data sampling rates

Summary:
The article introduces a novel diffusion-based method for satellite pattern-of-life (PoL) identification, which overcomes limitations of existing approaches by eliminating the need for manual refinement or domain-specific knowledge. The method combines a multivariate time-series encoder with a diffusion model to capture hidden representations of satellite positional data and generate PoL labels. It achieves high identification quality and robustness even with varying data sampling rates, demonstrating potential for practical satellite behavior pattern identification and mission deployment. This innovative approach outperforms traditional methods and addresses challenges in analyzing satellite behaviors, offering a promising solution for space safety and satellite monitoring. The Expert-ML method, previously developed, is also discussed as a domain expertise-informed machine learning approach for PoL identification. <div>
arXiv:2412.10814v2 Announce Type: replace-cross 
Abstract: Satellite pattern-of-life (PoL) identification is crucial for space safety and satellite monitoring, involving the analysis of typical satellite behaviors such as station-keeping, drift, etc. However, existing PoL identification methods remain underdeveloped due to the complexity of aerospace systems, variability in satellite behaviors, and fluctuating observation sampling rates. In a first attempt, we developed a domain expertise-informed machine learning method (Expert-ML) to combine satellite orbital movement knowledge and machine learning models. The Expert-ML method achieved high accuracy results in simulation data and real-world data with normal sampling rate. However, this approach lacks of generality as it requires domain expertise and its performance degraded significantly when data sampling rate varied. To achieve generality, we propose a novel diffusion-based PoL identification method. Distinct from prior approaches, the proposed method leverages a diffusion model to achieve end-to-end identification without manual refinement or domain-specific knowledge. Specifically, we employ a multivariate time-series encoder to capture hidden representations of satellite positional data. The encoded features are subsequently incorporated as conditional information in the denoising process to generate PoL labels. Through experimentation across real-world satellite settings, our proposed diffusion-based method demonstrates its high identification quality and provides a robust solution even with reduced data sampling rates, indicating its great potential in practical satellite behavior pattern identification, tracking and related mission deployment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Dynamical Systems across Environments via Diffusive Model Weight Generation</title>
<link>https://arxiv.org/abs/2505.13919</link>
<guid>https://arxiv.org/abs/2505.13919</guid>
<content:encoded><![CDATA[
<div> Data-driven methods, dynamic behaviors, cross-environment prediction, model weight generation, \texttt{EnvAd-Diff}<br />
<br />
Summary: 
The study focuses on using data-driven methods to predict physical dynamics in different environments. It addresses the issue of prediction functions failing when transferred to unseen environments by proposing a model weight generation method called \texttt{EnvAd-Diff}. The method operates in the weight space of the dynamic function to generate suitable weights based on environmental conditions for zero-shot prediction. By training expert prediction functions on dynamic trajectories from visible environments, a model zoo is created to construct sample pairs of prediction function weights and their corresponding environments. A latent space diffusion model conditioned on the environment is then trained to model the joint distribution of weights and environments. The study also introduces a physics-informed surrogate label to distinguish different environments. Generalization experiments across multiple systems show that the 1M parameter prediction function generated by \texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model. <div>
arXiv:2505.13919v1 Announce Type: new 
Abstract: Data-driven methods offer an effective equation-free solution for predicting physical dynamics. However, the same physical system can exhibit significantly different dynamic behaviors in various environments. This causes prediction functions trained for specific environments to fail when transferred to unseen environments. Therefore, cross-environment prediction requires modeling the dynamic functions of different environments. In this work, we propose a model weight generation method, \texttt{EnvAd-Diff}. \texttt{EnvAd-Diff} operates in the weight space of the dynamic function, generating suitable weights from scratch based on environmental condition for zero-shot prediction. Specifically, we first train expert prediction functions on dynamic trajectories from a limited set of visible environments to create a model zoo, thereby constructing sample pairs of prediction function weights and their corresponding environments. Subsequently, we train a latent space diffusion model conditioned on the environment to model the joint distribution of weights and environments. Considering the lack of environmental prior knowledge in real-world scenarios, we propose a physics-informed surrogate label to distinguish different environments. Generalization experiments across multiple systems demonstrate that a 1M parameter prediction function generated by \texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLUTUS Open Source -- Breaking Barriers in Algorithmic Trading</title>
<link>https://arxiv.org/abs/2505.14050</link>
<guid>https://arxiv.org/abs/2505.14050</guid>
<content:encoded><![CDATA[
<div> Keywords: Algorithmic trading, reproducibility, standardization, collaboration, PLUTUS

Summary: 
Algorithmic trading has traditionally been a secretive and fragmented domain, lacking transparency, standardization, and collaboration. The PLUTUS Open Source initiative, sponsored by ALGOTRADE, aims to reshape this landscape by promoting openness, structure, and collaboration within the algorithmic trading ecosystem. PLUTUS introduces a reproducibility standard, a modular development framework, and a suite of community-built reference strategies, providing a systematic approach to designing, testing, and documenting trading algorithms for users regardless of their technical or financial background. The initiative invites contributions from the research and trading communities to build a transparent and inclusive future for algorithmic trading. The foundational structure of PLUTUS is outlined, along with working examples that adhere to the PLUTUS standard. By promoting reproducibility, standardization, and collaboration, PLUTUS aims to drive a positive transformation in the algorithmic trading industry. 

Summary: <br /><br />PLUTUS Open Source aims to reshape the algorithmic trading ecosystem by promoting openness, structure, and collaboration. The initiative introduces a reproducibility standard, a modular development framework, and a suite of community-built reference strategies. PLUTUS provides a systematic approach to designing, testing, and documenting trading algorithms, accessible to users with varying technical and financial backgrounds. By inviting contributions from the research and trading communities, PLUTUS aims to build a transparent and inclusive future for algorithmic trading. The initiative's foundational structure and working examples adhering to the PLUTUS standard are presented as a foundation for driving positive change in the industry. <div>
arXiv:2505.14050v1 Announce Type: new 
Abstract: Algorithmic trading has long been an opaque, fragmented domain, guarded by secrecy and built around proprietary systems. In contrast to the open, collaborative evolution in fields like machine learning or software engineering, the algorithmic trading ecosystem has been slow to adopt reproducibility, standardization, and shared infrastructure. This paper introduces PLUTUS Open Source, an initiative sponsored by ALGOTRADE to reshape this landscape through openness, structure, and collaboration. PLUTUS combines a reproducibility standard, a modular development framework, and a growing suite of community-built reference strategies. The project provides a systematic approach to designing, testing, and documenting trading algorithms, regardless of the user's technical or financial background. We outline the motivation behind the initiative, present its foundational structure, and showcase working examples that adhere to the PLUTUS standard. We also invite the broader research and trading communities to contribute, iterate, and help build a transparent and inclusive future for algorithmic trading.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order, mixed-hybrid finite elements for Kirchhoff-Love shells</title>
<link>https://arxiv.org/abs/2505.14115</link>
<guid>https://arxiv.org/abs/2505.14115</guid>
<content:encoded><![CDATA[
<div> mixed-hybrid method, Kirchhoff-Love shells, Lagrange elements, hybridization, numerical analyses <br />
Summary: 
A novel mixed-hybrid method for Kirchhoff-Love shells is introduced, allowing the use of classical Lagrange elements in numerical analyses. Unlike displacement-based formulations, this method features displacements and moments as primary unknowns, reducing continuity requirements and enabling equal-order interpolations. Hybridization simplifies static condensation while introducing rotational degrees of freedom as Lagrange multipliers. The formulation, based on Tangential Differential Calculus, is applicable for explicit and implicit shell geometries. Mechanical boundary conditions are considered, and numerical results show optimal convergence rates for smooth solutions. New benchmark test cases are proposed to assess the method's effectiveness. <div>
arXiv:2505.14115v1 Announce Type: new 
Abstract: A novel mixed-hybrid method for Kirchhoff-Love shells is proposed that enables the use of classical, possibly higher-order Lagrange elements in numerical analyses. In contrast to purely displacement-based formulations that require higher continuity of shape functions as in IGA, the mixed formulation features displacements and moments as primary unknowns. Thereby the continuity requirements are reduced, allowing equal-order interpolations of the displacements and moments. Hybridization enables an element-wise static condensation of the degrees of freedom related to the moments, at the price of introducing (significantly less) rotational degrees of freedom acting as Lagrange multipliers to weakly enforce the continuity of tangential moments along element edges. The mixed model is formulated coordinate-free based on the Tangential Differential Calculus, making it applicable for explicitly and implicitly defined shell geometries. All mechanically relevant boundary conditions are considered. Numerical results confirm optimal higher-order convergence rates whenever the mechanical setup allows for sufficiently smooth solutions; new benchmark test cases of this type are proposed.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Maxwell Tomography Using the Volume-Surface Integral Equation for Improved Estimation of Electrical Properties</title>
<link>https://arxiv.org/abs/2505.14546</link>
<guid>https://arxiv.org/abs/2505.14546</guid>
<content:encoded><![CDATA[
<div> Tomography, Maxwell, Electrical properties, MRI, VSIE<br />
<br />
Summary:
Global Maxwell Tomography (GMT) is a method for estimating electrical properties (EP) from magnetic resonance measurements. A novel version of GMT using the volume-surface integral equation (VSIE) recalculates coil currents based on updated EP estimates, yielding more accurate reconstructions. Simulation and experimental results showed that VSIE-based GMT outperformed the traditional VIE-based method, with improvements of at least 12% in simulations and relative differences of 13-26% in experiments compared to probe-measured values. By accounting for the effect of EP on coil currents, VSIE-based GMT does not rely on an initial EP estimate, making it more suitable for experimental reconstructions. The study highlights the significance of using VSIE for enhancing GMT performance in noninvasive EP estimation. <br /><br /> <div>
arXiv:2505.14546v1 Announce Type: new 
Abstract: Objective: Global Maxwell Tomography (GMT) is a noninvasive inverse optimization method for the estimation of electrical properties (EP) from magnetic resonance (MR) measurements. GMT uses the volume integral equation (VIE) in the forward problem and assumes that the sample has negligible effect on the coil currents. Consequently, GMT calculates the coil's incident fields with an initial EP distribution and keeps them constant for all optimization iterations. This can lead to erroneous reconstructions. This work introduces a novel version of GMT that replaces VIE with the volume-surface integral equation (VSIE), which recalculates the coil currents at every iteration based on updated EP estimates before computing the associated fields. Methods: We simulated an 8-channel transceiver coil array for 7 T brain imaging and reconstructed the EP of a realistic head model using VSIE-based GMT. We built the coil, collected experimental MR measurements, and reconstructed EP of a two-compartment phantom. Results: In simulations, VSIE-based GMT outperformed VIE-based GMT by at least 12% for both EP. In experiments, the relative difference with respect to probe-measured EP values in the inner (outer) compartment was 13% (26%) and 17% (33%) for the permittivity and conductivity, respectively. Conclusion: The use of VSIE over VIE enhances GMT's performance by accounting for the effect of the EP on the coil currents. Significance: VSIE-based GMT does not rely on an initial EP estimate, rendering it more suitable for experimental reconstructions compared to the VIE-based GMT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact</title>
<link>https://arxiv.org/abs/2505.13469</link>
<guid>https://arxiv.org/abs/2505.13469</guid>
<content:encoded><![CDATA[
<div> fair lending, machine learning, algorithmic fairness, profit margins, default rates 
Summary:
- The study investigates the tradeoff between enforcing fairness in lending algorithms and maximizing profitability in financial institutions by using machine learning models.
- Simulations on synthetic data reflecting real-world lending patterns reveal that equal opportunity constraints generally have a lower impact on profit margins compared to demographic parity.
- Surprisingly, removing protected attributes from the model through fairness through unawareness results in improved fairness and profitability metrics.
- Fair lending can be profitable under specific economic conditions, and the study also examines the feature-specific drivers of unfairness in lending algorithms.
- The findings provide practical guidance for designing lending algorithms that balance ethical concerns with business objectives. 
<br /><br />Summary: <div>
arXiv:2505.13469v1 Announce Type: cross 
Abstract: As financial institutions increasingly rely on machine learning models to automate lending decisions, concerns about algorithmic fairness have risen. This paper explores the tradeoff between enforcing fairness constraints (such as demographic parity or equal opportunity) and maximizing lender profitability. Through simulations on synthetic data that reflects real-world lending patterns, we quantify how different fairness interventions impact profit margins and default rates. Our results demonstrate that equal opportunity constraints typically impose lower profit costs than demographic parity, but surprisingly, removing protected attributes from the model (fairness through unawareness) outperforms explicit fairness interventions in both fairness and profitability metrics. We further identify the specific economic conditions under which fair lending becomes profitable and analyze the feature-specific drivers of unfairness. These findings offer practical guidance for designing lending algorithms that balance ethical considerations with business objectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Methods for Model Pruning and Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.14052</link>
<guid>https://arxiv.org/abs/2505.14052</guid>
<content:encoded><![CDATA[
<div> Pruning, Language models, Model optimization, Computational complexity, Knowledge distillation
<br />
Summary:
MAMA Pruning is introduced as an enhanced method for model pruning in large language models like R1 or o3-mini. The technique aims to remove neurons and connections that are unlikely to contribute significantly during human-computer interaction. By analyzing movement and magnitude of weights and biases in the pre-training phase, as well as GRPO rewards in the post-training phase, MAMA Pruning effectively reduces model size and computational complexity. The method maintains performance comparable to unpruned models even at extreme pruning levels. Experimental results demonstrate the superiority of MAMA Pruning over existing methods across various pruning levels and computational linguistics tasks.
<br /><br />Summary: <div>
arXiv:2505.14052v1 Announce Type: cross 
Abstract: Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Evaluation of a Microservices Cloud Framework for Online Travel Platforms</title>
<link>https://arxiv.org/abs/2505.14508</link>
<guid>https://arxiv.org/abs/2505.14508</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Travel Agents, Microservices architecture, Cloud computing, Fault Tolerance, Cost-effective analysis<br />
Summary: <br />
The paper discusses the use of Microservices architecture in handling online travel agents globally, highlighting the benefits of flexibility and efficiency in managing large amounts of data. The Microservices Cloud Framework for Online Travel Platforms (MCF-OTP) is designed to improve performance, flexibility, and maintenance by utilizing cloud computing and microservice technologies. The framework aims to enhance fault tolerance and response time, surpassing traditional monolithic designs in scalability and uptime. Cost-effective analysis shows a net gain from startup fees and operational costs. The cloud-based environment helps reduce costs and optimize resource allocation, ultimately increasing efficiency in managing online travel platforms. <div>
arXiv:2505.14508v1 Announce Type: cross 
Abstract: Handling online travel agents globally requires efficient and flexible software solution architectures. When it needs to handle thousands of agents and billions of clients data globally. Microservices architecture is used to break down a large program into numerous, smaller services which can run individually and perform individual tasks. This paper analyses and integrates a unique Microservices Cloud Framework designed to support Online Travel Platforms (MCF-OTP). MCF-OTPs main goal is to increase the performance, flexibility, and maintenance of online travel platforms via cloud computing and microservice technologies. Large-scale travel apps, including managing numerous data sources, dealing with traffic peaks, and providing fault tolerance, can be addressed by the suggested framework. The framework increases good interpretation between flawless data synchronization, microservices, and dynamic scaling based on demand technology. An organization framework that optimizes service borders and minimizes inter-service dependencies is recommended. Thus, this can result in elevated development adaptability. In this research, the principal goal is to evaluate MCF-OTPs efficiency using the indicators of fault tolerance and response time. It is indicated by the findings that the MCF-OTP structure excels traditional monolithic designs in terms of dependability and scalability, managing traffic spikes seamlessly and decreasing downtime. The cost-effective analysis helps ascertain the net gain attained by the startup fees and the ongoing operational costs. The cloud-based environment is used to reduce the fracture cost which also helps to increase the efficiency of resource allocation, according to the research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions</title>
<link>https://arxiv.org/abs/2410.03779</link>
<guid>https://arxiv.org/abs/2410.03779</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, mesh-based physical simulation, EvoMesh, anisotropic message passing, adaptively guided, physical inputs<br />
<br />
Summary:<br />
EvoMesh is introduced as a fully differentiable framework for mesh-based physical simulation that learns graph hierarchies and physical dynamics together. It utilizes direction-specific aggregation of dynamic features and node selection probabilities for hierarchical levels based on physical context, enabling flexible message shortcuts and long-range dependency capture. Experimental results on various datasets demonstrate EvoMesh's superior performance over fixed-hierarchy message passing networks. The code for EvoMesh is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2410.03779v2 Announce Type: replace-cross 
Abstract: Graph neural networks have been a powerful tool for mesh-based physical simulation. To efficiently model large-scale systems, existing methods mainly employ hierarchical graph structures to capture multi-scale node relations. However, these graph hierarchies are typically manually designed and fixed, limiting their ability to adapt to the evolving dynamics of complex physical systems. We propose EvoMesh, a fully differentiable framework that jointly learns graph hierarchies and physical dynamics, adaptively guided by physical inputs. EvoMesh introduces anisotropic message passing, which enables direction-specific aggregation of dynamic features between nodes within each hierarchy, while simultaneously learning node selection probabilities for the next hierarchical level based on physical context. This design creates more flexible message shortcuts and enhances the model's capacity to capture long-range dependencies. Extensive experiments on five benchmark physical simulation datasets show that EvoMesh outperforms recent fixed-hierarchy message passing networks by large margins. Code is available at https://github.com/hbell99/EvoMesh.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries</title>
<link>https://arxiv.org/abs/2410.19236</link>
<guid>https://arxiv.org/abs/2410.19236</guid>
<content:encoded><![CDATA[
<div> algorithm, SHAP zero, Shapley values, biological sequences, interpretability

Summary:
SHAP zero is a novel algorithm that addresses the need for interpretable predictions in machine learning models for biological sequences. By amortizing the computational cost of Shapley value computation across large-scale datasets, SHAP zero enables near-zero marginal cost for future queries. It leverages the sparse Fourier transform of the model to uncover high-order feature interactions and efficiently explain predictions in models of guide RNA efficacy, DNA repair outcomes, and protein fitness. The algorithm significantly accelerates the process of extracting global biological insights and reveals rich combinatorial interactions that were previously inaccessible at scale. This work paves the way for scalable and efficient interpretability of black-box sequence models in biology.<br /><br />Summary: <div>
arXiv:2410.19236v3 Announce Type: replace-cross 
Abstract: The growing adoption of machine learning models for biological sequences has intensified the need for interpretable predictions, with Shapley values emerging as a theoretically grounded standard for model explanation. While effective for local explanations of individual input sequences, scaling Shapley-based interpretability to extract global biological insights requires evaluating thousands of sequences--incurring exponential computational cost per query. We introduce SHAP zero, a novel algorithm that amortizes the cost of Shapley value computation across large-scale biological datasets. After a one-time model sketching step, SHAP zero enables near-zero marginal cost for future queries by uncovering an underexplored connection between Shapley values, high-order feature interactions, and the sparse Fourier transform of the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and protein fitness, SHAP zero explains predictions orders of magnitude faster than existing methods, recovering rich combinatorial interactions previously inaccessible at scale. This work opens the door to principled, efficient, and scalable interpretability for black-box sequence models in biology.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced Feature Engineering for Multi-Factor Electricity Price Predictions</title>
<link>https://arxiv.org/abs/2505.11890</link>
<guid>https://arxiv.org/abs/2505.11890</guid>
<content:encoded><![CDATA[
<div> forecasting, electricity price, volatility, New South Wales, machine learning

Summary:<br /><br />
- Traditional forecasting models struggle in capturing the complex dynamics of electricity markets, leading to unreliable predictions in volatile markets like New South Wales.
- FAEP, a Feature-Augmented Electricity Price Prediction framework, addresses these challenges by leveraging Large Language Models (LLMs) and advanced feature engineering.
- By incorporating external features such as weather data and price volatility jumps, FAEP enhances prediction accuracy.
- Utilizing Retrieval-Augmented Generation (RAG) for feature extraction and a hybrid XGBoost-LSTM model, FAEP outperforms other models in the NSW electricity market.
- Experimental results demonstrate the state-of-the-art performance of FAEP in electricity price prediction, showcasing the efficiency of LLM-enhanced feature engineering and hybrid machine learning architectures. 

<br /><br />Summary: Hello, please summarize this article in en language, first extract 5 keywords, output in the same line, then line break, write a summary containing all the points in 200 words in en, output in order by points, and output in the following format '<br /><br />Summary:' , <br /> is the line break of HTML, 2 must be retained when output, and must be before the word 'Summary:' <div>
arXiv:2505.11890v1 Announce Type: new 
Abstract: Accurately forecasting electricity price volatility is crucial for effective risk management and decision-making. Traditional forecasting models often fall short in capturing the complex, non-linear dynamics of electricity markets, particularly when external factors like weather conditions and market volatility are involved. These limitations hinder their ability to provide reliable predictions in markets with high volatility, such as the New South Wales (NSW) electricity market. To address these challenges, we introduce FAEP, a Feature-Augmented Electricity Price Prediction framework. FAEP leverages Large Language Models (LLMs) combined with advanced feature engineering to enhance prediction accuracy. By incorporating external features such as weather data and price volatility jumps, and utilizing Retrieval-Augmented Generation (RAG) for effective feature extraction, FAEP overcomes the shortcomings of traditional approaches. A hybrid XGBoost-LSTM model in FAEP further refines these augmented features, resulting in a more robust prediction framework. Experimental results demonstrate that FAEP achieves state-of-art (SOTA) performance compared to other electricity price prediction models in the Australian New South Wale electricity market, showcasing the efficiency of LLM-enhanced feature engineering and hybrid machine learning architectures.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-based Discovery of Intrinsic Coordinates and Governing Equations from High-Dimensional Data</title>
<link>https://arxiv.org/abs/2505.11940</link>
<guid>https://arxiv.org/abs/2505.11940</guid>
<content:encoded><![CDATA[
<div> Keywords: governing equations, high-dimensional data, multimodal large language models, zero-shot method, spatial perception <br />
<br />
Summary: 
Discovering governing equations from high-dimensional scientific data is a challenging task due to the exponential expansion of the equation space. This paper introduces a novel zero-shot approach using multimodal large language models (MLLM) to automatically identify physical coordinates and equations from data. By enhancing MLLM's spatial perception with visual prompts and leveraging its domain knowledge, the proposed method effectively navigates the equation space. Evaluation on simulated and real data shows improved accuracy in discovering physical coordinates and equations. Long-term extrapolation accuracy is enhanced by approximately 26.96% compared to the baseline, showcasing the efficiency of the approach in understanding the evolution of systems. <div>
arXiv:2505.11940v1 Announce Type: new 
Abstract: Discovering governing equations from scientific data is crucial for understanding the evolution of systems, and is typically framed as a search problem within a candidate equation space. However, the high-dimensional nature of dynamical systems leads to an exponentially expanding equation space, making the search process extremely challenging. The visual perception and pre-trained scientific knowledge of multimodal large language models (MLLM) hold promise for providing effective navigation in high-dimensional equation spaces. In this paper, we propose a zero-shot method based on MLLM for automatically discovering physical coordinates and governing equations from high-dimensional data. Specifically, we design a series of enhanced visual prompts for MLLM to enhance its spatial perception. In addition, MLLM's domain knowledge is employed to navigate the search process within the equation space. Quantitative and qualitative evaluations on two representative types of systems demonstrate that the proposed method effectively discovers the physical coordinates and equations from both simulated and real experimental data, with long-term extrapolation accuracy improved by approximately 26.96% compared to the baseline.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Gas Well Performance with Decline Curve Analysis: A Case Study on Semutang Gas Field</title>
<link>https://arxiv.org/abs/2505.12333</link>
<guid>https://arxiv.org/abs/2505.12333</guid>
<content:encoded><![CDATA[
<div> DCA, production forecasting, gas reservoir, Semutang gas field, decline curve parameters <br />
<br />
Summary: <br />
Decline-curve analysis (DCA) is an important method for forecasting production and estimating reserves in gas reservoirs. This study applied DCA to predict future production performance and estimate ultimate recovery for a well in the Semutang gas field in Bangladesh. Different decline curve models were used, with the hyperbolic model providing the most accurate forecast. It is crucial to select the right decline model for precise production forecasting and reserve estimation, essential for effective reservoir management and resource optimization. The study emphasizes the significance of accurate decline curve analysis in the oil and gas industry. <div>
arXiv:2505.12333v1 Announce Type: new 
Abstract: Decline-curve analysis (DCA) is a widely utilized method for production forecasting and estimating remaining reserves in gas reservoir. Based on the assumptions that past production trend can be mathematically characterized and used to predict future performance. It relies on historical production data and assumes that production methods remain unchanged throughout the analysis. This method is particularly valuable due to its accuracy in forecasting and its broad acceptance within the industry. Wells in the same geographical area and producing from similar geological formations often exhibit similar decline curve parameters. This study applies DCA to forecast the future production performance and estimate the ultimate recovery for the Semutang gas field's well 5 in Bangladesh. Using historical production data, decline curves were generated based on exponential, hyperbolic, and harmonic model equations. The cumulative production estimations were 11,139.34 MMSCF for the exponential model, 11,620.26 MMSCF for the hyperbolic model, and 14,021.92 MMSCF for the harmonic model. In terms of the well's productive life, the estimates were 335.13 days, 1,152 days, and 22,611 days, respectively. Among these models, the hyperbolic decline provided the most realistic forecast, closely aligning with observed production trend. The study highlights the importance of selecting an appropriate decline model for accurate production forecasting and reserve estimation, which is essential for effective reservoir management and resource optimization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seismic analysis based on a new interval method with incomplete information</title>
<link>https://arxiv.org/abs/2505.12607</link>
<guid>https://arxiv.org/abs/2505.12607</guid>
<content:encoded><![CDATA[
<div> Keywords: seismic analysis, interval uncertainty, Monte Carlo simulation, covariance matrix adaptation evolution strategy, computational efficiency <br />
Summary: 
This study focuses on describing time-variant uncertain seismic accelerations using the minimum interval radius-based interval process (MRIP) based on the convex model for seismic analysis in engineering structures. To enhance computational efficiency in uncertainty analysis, the paper introduces the improved covariance matrix adaptation evolution strategy (CMA-ES) called DES-ES, which exhibits higher efficiency. Additionally, a computational framework named DES-ES-SS is proposed to leverage response dependency and further enhance computational efficiency while maintaining accuracy in interval uncertainty analysis of seismic structures under stationary or non-stationary seismic acceleration conditions. Numerical experiments validate the effectiveness of DES-ES-SS in improving computational efficiency without compromising accuracy. <br /><br /> <div>
arXiv:2505.12607v1 Announce Type: new 
Abstract: For seismic analysis in engineering structures, it is essential to consider the dynamic responses under seismic excitation, necessitating the description of seismic accelerations. Limit seismics samples lead to incomplete uncertainty information, which is described by the non-probabilistic method reasonable. This study employs the minimum interval radius-based interval process (MRIP) based on the convex model to describe the time-variant uncertain seismic acceleration, subsequently conducting uncertainty analysis for seismic structures. However, the Monte Carlo simulation for uncertainty analysis requires extensive deterministic computations to ensure accuracy, exhibiting poor computational efficiency. To address this issue, this paper first improves the covariance matrix adaptation evolution strategy (CMA-ES) through the dynamic evolution sequence, proposing DES-ES, whose efficiency is validated to be higher than that of CMA-ES. Furthermore, leveraging the dependency of the responses, a computational framework named DES-ES-SS is proposed. Numerical experiments demonstrate that DES-ES-SS improves computational efficiency while maintaining the accuracy of the interval uncertainty analysis of the seismic structures whether the seismic acceleration is stationary or non-stationary.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit differentiation with second-order derivatives and benchmarks in finite-element-based differentiable physics</title>
<link>https://arxiv.org/abs/2505.12646</link>
<guid>https://arxiv.org/abs/2505.12646</guid>
<content:encoded><![CDATA[
<div> differentiable programming, automatic differentiation, second-order derivatives, finite-element-based, PDE-constrained optimization

Summary:
This paper introduces a framework for computing second-order derivatives (Hessians) for implicit functions in finite-element-based differentiable physics. By utilizing primitive automatic differentiation tools, the authors develop an algorithm for efficiently calculating Hessian-vector products. Validation against finite difference approximations shows accurate results. Benchmark tests across various problem types demonstrate the advantages of using second-order information in optimization. The Newton-CG method with exact Hessians proves beneficial for nonlinear inverse problems like traction force identification and shape optimization, while the L-BFGS-B method is suitable for linear cases. These findings establish a solid foundation for incorporating second-order implicit differentiation into differentiable physics engines, enhancing optimization speed and reliability.<br /><br />Summary: <div>
arXiv:2505.12646v1 Announce Type: new 
Abstract: Differentiable programming is revolutionizing computational science by enabling automatic differentiation (AD) of numerical simulations. While first-order gradients are well-established, second-order derivatives (Hessians) for implicit functions in finite-element-based differentiable physics remain underexplored. This work bridges this gap by deriving and implementing a framework for implicit Hessian computation in PDE-constrained optimization problems. We leverage primitive AD tools (Jacobian-vector product/vector-Jacobian product) to build an algorithm for Hessian-vector products and validate the accuracy against finite difference approximations. Four benchmarks spanning linear/nonlinear, 2D/3D, and single/coupled-variable problems demonstrate the utility of second-order information. Results show that the Newton-CG method with exact Hessians accelerates convergence for nonlinear inverse problems (e.g., traction force identification, shape optimization), while the L-BFGS-B method suffices for linear cases. Our work provides a robust foundation for integrating second-order implicit differentiation into differentiable physics engines, enabling faster and more reliable optimization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid Topology Estimation using an Information Theoretic Approach</title>
<link>https://arxiv.org/abs/2505.11517</link>
<guid>https://arxiv.org/abs/2505.11517</guid>
<content:encoded><![CDATA[
<div> mutual information, power grid, graph modeling, maximum spanning tree, IEEE networks

Summary:
The article introduces an information-theoretic approach to estimate the topology of a power grid by modeling it as a graph and analyzing voltage magnitude data of individual nodes. The mutual information between pairs of nodes is computed using different approximation methods, leading to the estimation of the power grid topology through a maximum spanning tree based on mutual information, generated with the Chow-Liu algorithm. The approach is successfully optimized and validated on IEEE networks created with MATPOWER and GridLAB-D data, as well as on networks from the European Union Joint Research Council. The experiments and results demonstrate the effectiveness of this method in accurately estimating power grid topologies. <br /><br />Summary: <div>
arXiv:2505.11517v1 Announce Type: cross 
Abstract: The topology of a power grid is estimated using an information theoretic approach. By modeling the grid as a graph and using voltage magnitude data of individual nodes in the grid, the mutual information between pairs of nodes is computed using different approximation methods. Using the well-known Chow-Liu algorithm, a maximum spanning tree based on mutual information is computed to estimate the power grid topology. Experiments and results are presented to optimize this approach with success shown for IEEE networks generated with MATPOWER and data generated using GridLAB-D. The algorithm is then cross-validated on IEEE networks generated by the European Union Joint Research Council.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASL-HJX: A Comprehensive Guide to Solving Deterministic and Stochastic Hamilton-Jacobi Equations</title>
<link>https://arxiv.org/abs/2505.11527</link>
<guid>https://arxiv.org/abs/2505.11527</guid>
<content:encoded><![CDATA[
<div> framework, Hamilton-Jacobi equations, numerical methods, operator splitting techniques, neuroscience<br />
Summary:<br />
CASL-HJX is a computational framework designed for solving deterministic and stochastic Hamilton-Jacobi equations in two spatial dimensions. It integrates numerical methods for hyperbolic PDEs with operator splitting techniques and implements implicit methods for second-order derivative terms to ensure convergence to viscosity solutions. The high-performance C++ core efficiently handles mixed-order derivative systems with time-varying dynamics, making it suitable for real-world applications. The solver's versatility is demonstrated through tutorial examples and applications in neuroscience, enabling the design of energy-efficient controllers for regulating neural populations. CASL-HJX's modular architecture allows researchers to define computational domains, configure problems, and execute simulations with high numerical accuracy. It serves as a robust tool for managing uncertainty in complex dynamical systems, bridging the gap between deterministic control methods and stochastic models. <div>
arXiv:2505.11527v1 Announce Type: cross 
Abstract: CASL-HJX is a computational framework designed for solving deterministic and stochastic Hamilton-Jacobi equations in two spatial dimensions. It provides a flexible and efficient approach to modeling front propagation problems, optimal control problems, and stochastic Hamilton-Jacobi Bellman equations. The framework integrates numerical methods for hyperbolic PDEs with operator splitting techniques and implements implicit methods for second-order derivative terms, ensuring convergence to viscosity solutions while achieving global rather than local optimization. Built with a high-performance C++ core, CASL-HJX efficiently handles mixed-order derivative systems with time-varying dynamics, making it suitable for real-world applications across multiple domains. We demonstrate the solver's versatility through tutorial examples covering various PDEs and through applications in neuroscience, where it enables the design of energy-efficient controllers for regulating neural populations to mitigate pathological synchrony. While our examples focus on these applications, the mathematical foundation of the solver makes it applicable to problems in finance, engineering, and machine learning. The modular architecture allows researchers to define computational domains, configure problems, and execute simulations with high numerical accuracy. CASL-HJX bridges the gap between deterministic control methods and stochastic models, providing a robust tool for managing uncertainty in complex dynamical systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median</title>
<link>https://arxiv.org/abs/2505.11725</link>
<guid>https://arxiv.org/abs/2505.11725</guid>
<content:encoded><![CDATA[
<div> bootstrap, quantiles, central limit theorem, moment condition, asymptotic distributions

Summary:
This paper investigates the m-out-of-n bootstrap method for estimating sample quantiles, providing parameter-free guarantees for its soundness. A central limit theorem is established for a data-driven estimator under a mild moment condition, without the need for unknown nuisance parameters. The paper presents a counter-example demonstrating the tightness of the moment assumption and derives an Edgeworth expansion for exact convergence rates, along with a Berry Esseen bound for bootstrap approximation errors. The results are illustrated through applications to practical statistics like quantiles for random walk Metropolis-Hastings and rewards of ergodic Markov decision processes, showcasing the theory's utility in modern estimation and learning tasks.<br /><br />Summary: <div>
arXiv:2505.11725v1 Announce Type: cross 
Abstract: The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet (1992), approximates the distribution of a statistic by repeatedly drawing m subsamples (with m much smaller than n) without replacement from an original sample of size n. It is now routinely used for robust inference with heavy-tailed data, bandwidth selection, and other large-sample applications. Despite its broad applicability across econometrics, biostatistics, and machine learning, rigorous parameter-free guarantees for the soundness of the m-out-of-n bootstrap when estimating sample quantiles have remained elusive.
  This paper establishes such guarantees by analyzing the estimator of sample quantiles obtained from m-out-of-n resampling of a dataset of size n. We first prove a central limit theorem for a fully data-driven version of the estimator that holds under a mild moment condition and involves no unknown nuisance parameters. We then show that the moment assumption is essentially tight by constructing a counter-example in which the CLT fails. Strengthening the assumptions slightly, we derive an Edgeworth expansion that provides exact convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap approximation error. Finally, we illustrate the scope of our results by deriving parameter-free asymptotic distributions for practical statistics, including the quantiles for random walk Metropolis-Hastings and the rewards of ergodic Markov decision processes, thereby demonstrating the usefulness of our theory in modern estimation and learning tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
<link>https://arxiv.org/abs/2505.12638</link>
<guid>https://arxiv.org/abs/2505.12638</guid>
<content:encoded><![CDATA[
<div> ChromFound, scATAC-seq, single-cell, regulatory mechanisms, cell identification<br />
Summary:<br />
ChromFound is a foundation model designed for scATAC-seq data analysis, addressing challenges such as high dimensionality and lack of standardized OCR representation. It utilizes a hybrid architecture and genome-aware tokenization to capture genome-wide regulatory signals, trained on a large dataset for diverse tasks. The model demonstrates robust zero-shot performance, universal cell representation generation, and transferability in cell type annotation and cross-omics prediction. By identifying enhancer-gene links missed by other methods, ChromFound provides insights into disease risk variants in the noncoding genome. The model offers a promising framework for deciphering regulatory mechanisms and understanding complex chromatin landscapes in single-cell studies. <br /><br />Summary: <div>
arXiv:2505.12638v1 Announce Type: cross 
Abstract: The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present \textbf{ChromFound}, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Modeling of Random Fields from Limited Data via Constrained Latent Flow Matching</title>
<link>https://arxiv.org/abs/2505.13007</link>
<guid>https://arxiv.org/abs/2505.13007</guid>
<content:encoded><![CDATA[
<div> framework, generative modeling, random fields, latent flow matching, variational autoencoder <br />
Summary: 
- The article discusses a novel framework for generative modeling of random fields that incorporates domain knowledge to supplement limited data.
- The approach uses latent flow matching to operate on compressed function representations in the latent space of a pre-trained VAE.
- It includes a function decoder within the VAE and integrates physical/statistical constraints into the training process.
- The framework learns a latent function representation that generates continuous random field samples satisfying domain-specific constraints, even with sparse data.
- Two applications, wind velocity field reconstruction and material property inference, demonstrate the effectiveness of the framework in achieving improved reconstruction accuracy and inference with small training datasets. 
<br /> <div>
arXiv:2505.13007v1 Announce Type: cross 
Abstract: Deep generative models are promising tools for science and engineering, but their reliance on abundant, high-quality data limits applicability. We present a novel framework for generative modeling of random fields (probability distributions over continuous functions) that incorporates domain knowledge to supplement limited, sparse, and indirect data. The foundation of the approach is latent flow matching, where generative modeling occurs on compressed function representations in the latent space of a pre-trained variational autoencoder (VAE). Innovations include the adoption of a function decoder within the VAE and integration of physical/statistical constraints into the VAE training process. In this way, a latent function representation is learned that yields continuous random field samples satisfying domain-specific constraints when decoded, even in data-limited regimes. Efficacy is demonstrated on two challenging applications: wind velocity field reconstruction from sparse sensors and material property inference from a limited number of indirect measurements. Results show that the proposed framework achieves significant improvements in reconstruction accuracy compared to unconstrained methods and enables effective inference with relatively small training datasets that is intractable without constraints.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System reduction-based approximate reanalysis method for statically indeterminate structures with high-rank modification</title>
<link>https://arxiv.org/abs/2302.02171</link>
<guid>https://arxiv.org/abs/2302.02171</guid>
<content:encoded><![CDATA[
<div> spectral decomposition, structural reanalysis, high-rank modification, approximate static reanalysis, iterative solution <br />
Summary: <br />
The article proposes a novel approximate static reanalysis method for statically indeterminate structures with high-rank modification. The method involves dividing the structure into the basis system and additional components, rewriting equilibrium equations, and establishing a reduced equation system using spectral decomposition. The reduced system is solved using a pre-conditioned iterative solution algorithm to obtain approximate solutions for modified structures. The method shows excellent computational performance for structures with homogeneous material and functionally graded beams. The combination of system reduction and iterative solution technology proves to be an effective approach for developing high-performance reanalysis methods. <div>
arXiv:2302.02171v2 Announce Type: replace 
Abstract: Efficient structural reanalysis for high-rank modification plays an important role in engineering computations which require repeated evaluations of structural responses, such as structural optimization and probabilistic analysis. To improve the efficiency of engineering computations, a novel approximate static reanalysis method based on system reduction and iterative solution is proposed for statically indeterminate structures with high-rank modification. In this approach, a statically indeterminate structure is divided into the basis system and the additional components. Subsequently, the structural equilibrium equations are rewritten as the equation system with the stiffness matrix of the basis system and the pseudo forces derived from the additional elements. With the introduction of spectral decomposition, a reduced equation system with the element forces of the additional elements as the unknowns is established. Then, the approximate solutions of the modified structure can be obtained by solving the reduced equation system through a pre-conditioned iterative solution algorithm. The computational costs of the proposed method and the other two reanalysis methods are compared and numerical examples including static reanalysis and static nonlinear analysis are presented. The results demonstrate that the proposed method has excellent computational performance for both the structures with homogeneous material and structures composed of functionally graded beams. Meanwhile, the superiority of the proposed method indicates that the combination of system reduction and pre-conditioned iterative solution technology is an effective way to develop high-performance reanalysis methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSP-free adaptive Kriging surrogate model method for reliability analysis with small failure probability</title>
<link>https://arxiv.org/abs/2304.07010</link>
<guid>https://arxiv.org/abs/2304.07010</guid>
<content:encoded><![CDATA[
<div> Active learning reliability method, Kriging, Monte Carlo Simulation, Candidate Sample Pool, Particle Swarm Optimization

Summary:
The paper introduces a new method, CSP-free AK-MCS, to enhance reliability analysis in engineering. The method eliminates the reliance on Candidate Sample Pool (CSP) size, especially for systems with low failure probabilities. It consists of two stages: constructing a surrogate model and using Monte Carlo simulation. Surrogate model refinement is done iteratively with representative samples selected through Particle Swarm Optimization (PSO) algorithm. Adjustments like penalty intensity control and density control optimize the objective function, balancing accuracy and efficiency. Numerical examples prove the effectiveness of CSP-free AK-MCS in handling small failure probabilities, showing improved performance compared to conventional methods. This innovative approach overcomes limitations and provides exceptional results for reliability analysis. 

<br /><br />Summary: <div>
arXiv:2304.07010v5 Announce Type: replace 
Abstract: In the field of reliability engineering, the Active learning reliability method combining Kriging and Monte Carlo Simulation (AK-MCS) has been developed and demonstrated to be effective in reliability analysis. However, the performance of AK-MCS is sensitive to the size of Candidate Sample Pool (CSP), particularly for systems with small failure probabilities. To address the limitations of conventional AK-MCS that relies on CSP, this paper proposes a CSP-free AK-MCS. The proposed methodology consists of two stages: surrogate model construction and Monte Carlo simulation for estimating the failure probability. In the stage of surrogate model construction, the surrogate model is iteratively refined based on the representative samples selected by solving the optimization problem facilitated by Particle Swarm Optimization (PSO) algorithm. To achieve an optimal balance between solution accuracy and efficiency, the penalty intensity control and the density control for the experimental design points are introduced to modify the objective function in optimization. The performance of the proposed methodology is evaluated using numerical examples, and results indicate that by leveraging an optimization algorithm to select representative samples, the proposed CSP-free AK-MCS overcomes the limitations of conventional CSP-based AK-MCS and exhibits exceptional performance in addressing small failure probabilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A cable finite element formulation based on exact tension field for static nonlinear analysis of cable structures</title>
<link>https://arxiv.org/abs/2401.05609</link>
<guid>https://arxiv.org/abs/2401.05609</guid>
<content:encoded><![CDATA[
<div> beam theory, cable structures, finite element model, nonlinear analysis, numerical precision <br />
Summary: 
This paper introduces a numerically exact finite element model for analyzing the static nonlinear behavior of cable structures. The model accurately calculates the tension field by incorporating the geometrically exact beam theory and fundamental mechanical properties of cables. Unlike previous approaches, this model prioritizes numerical precision and universal applicability, deriving linearized equations with implicit expressions that include integrals. It also considers the variation in cross-sectional stiffness along the cable's length. The formulation ensures equilibrium and compatibility within cable elements, enabling accurate computation of internal forces, deformation states, and the unstrained length of the cable. The study discusses the implementation of solutions using the complete tangent matrix and internal iterations within elements. Numerical examples validate the effectiveness of the proposed cable element. <div>
arXiv:2401.05609v5 Announce Type: replace 
Abstract: This paper presents a numerically exact cable finite element model for static nonlinear analysis of cable structures. The model derives the exact expression of the tension field using the geometrically exact beam theory coupled with the fundamental mechanical characteristics of cables. The equations for the cable element are formulated by addressing the equilibrium conditions at the element boundaries and ensuring compatibility within the element. Unlike previous studies that typically provide explicit expressions for cable models, this study develops a formulation that emphasizes numerical precision and broad applicability. It achieves this by deriving linearized equations with implicit expressions incorporating integrals. The proposed model accurately computes internal forces and deformation states, and determines the unstrained length of the cable. Additionally, it accounts for the variability in cross-sectional stiffness along the cable's length. The paper discusses solution implementations using the complete tangent matrix and element internal iterations. The effectiveness of the proposed cable element is demonstrated through numerical examples.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enforced Interface Constraints for Domain Decomposition Method of Discrete Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.10925</link>
<guid>https://arxiv.org/abs/2505.10925</guid>
<content:encoded><![CDATA[
<div> Discrete Physics-Informed Neural Network, Domain Decomposition Method, Gaussian quadrature, Interface constraints, Computational efficiency<br />
<br />
Summary:
The study introduces a discrete physics-informed neural network (dPINN) framework with enforced interface constraints (EIC) for modeling physical systems using the domain decomposition method (DDM). The dPINN accurately calculates system energy through element-wise integration using Gaussian quadrature. The EIC enforces interfacial displacement constraints to ensure physical field continuity without auxiliary sampling or loss penalties, supporting independent meshing in each subdomain. By eliminating weak spatial constraints (WSC), the EIC-dPINN provides more stable and physically consistent predictions. Extensive numerical experiments in two and three dimensions confirm the framework's accuracy and showcase computational efficiency gains through parallel training. The results illustrate the framework's scalability, robustness, and potential for solving large-scale, geometrically complex problems.<br /><br />Summary: <div>
arXiv:2505.10925v1 Announce Type: new 
Abstract: This study presents a discrete physics-informed neural network (dPINN) framework, enhanced with enforced interface constraints (EIC), for modeling physical systems using the domain decomposition method (DDM). Built upon finite element-style mesh discretization, the dPINN accurately evaluates system energy through Gaussian quadrature-based element-wise integration. To ensure physical field continuity across subdomain interfaces, the EIC mechanism enforces interfacial displacement constraints without requiring auxiliary sampling or loss penalties.This formulation supports independent meshing in each subdomain, simplifying preprocessing and improving computational flexibility. Additionally, by eliminating the influence of weak spatial constraints (WSC) commonly observed in traditional PINNs, the EIC-dPINN delivers more stable and physically consistent predictions.Extensive two- and three-dimensional numerical experiments validate the proposed framework's accuracy and demonstrate the computational efficiency gains achieved through parallel training. The results highlight the framework's scalability, robustness, and potential for solving large-scale, geometrically complex problems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking</title>
<link>https://arxiv.org/abs/2505.11065</link>
<guid>https://arxiv.org/abs/2505.11065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, DeepFund, real-time market data, fund investment, evaluation

Summary:<br />
Large Language Models (LLMs) have shown promise in financial tasks but their effectiveness in fund investment remains unexplored. To address limitations in evaluating LLM-driven trading strategies, a new benchmark tool called DeepFund is introduced. DeepFund directly connects with real-time stock market data to prevent information leakage and provide fair evaluations. Test results on nine top LLMs reveal challenges in various investment dimensions, showcasing practical limitations in active fund management. Even advanced models like DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses in the real-time evaluation environment of DeepFund. This highlights the current constraints of LLMs in active fund management.<br /> <div>
arXiv:2505.11065v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated notable capabilities across financial tasks, including financial report summarization, earnings call transcript analysis, and asset classification. However, their real-world effectiveness in managing complex fund investment remains inadequately assessed. A fundamental limitation of existing benchmarks for evaluating LLM-driven trading strategies is their reliance on historical back-testing, inadvertently enabling LLMs to "time travel"-leveraging future information embedded in their training corpora, thus resulting in possible information leakage and overly optimistic performance estimates. To address this issue, we introduce DeepFund, a live fund benchmark tool designed to rigorously evaluate LLM in real-time market conditions. Utilizing a multi-agent architecture, DeepFund connects directly with real-time stock market data-specifically data published after each model pretraining cutoff-to ensure fair and leakage-free evaluations. Empirical tests on nine flagship LLMs from leading global institutions across multiple investment dimensions-including ticker-level analysis, investment decision-making, portfolio management, and risk control-reveal significant practical challenges. Notably, even cutting-edge models such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses within DeepFund real-time evaluation environment, underscoring the present limitations of LLMs for active fund management. Our code is available at https://github.com/HKUSTDial/DeepFund.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment</title>
<link>https://arxiv.org/abs/2505.11194</link>
<guid>https://arxiv.org/abs/2505.11194</guid>
<content:encoded><![CDATA[
<div> sequence-to-text model, protein function prediction, multimodal, contrastive alignment learning, fine-tuning

Summary:
Prot2Text-V2 is a novel multimodal sequence-to-text model that generates natural language descriptions of protein function directly from amino acid sequences. This model combines a protein language encoder with a decoder-only language model through a modality projector, improving cross-modal learning through Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE). The model is trained on curated entries from SwissProt and performs well under low-homology conditions. Prot2Text-V2 outperforms traditional and LLM-based baselines across various metrics. The combination of contrasting mean- and std-pooled protein embeddings with text representations, along with instruction-based fine-tuning using LoRA, results in accurate protein function description generation conditioned on the protein sequence.<br /><br />Summary: <div>
arXiv:2505.11194v1 Announce Type: new 
Abstract: Predicting protein function from sequence is a central challenge in computational biology. While existing methods rely heavily on structured ontologies or similarity-based techniques, they often lack the flexibility to express structure-free functional descriptions and novel biological functions. In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text model that generates free-form natural language descriptions of protein function directly from amino acid sequences. Our method combines a protein language model as a sequence encoder (ESM-3B) and a decoder-only language model (LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A key innovation is our Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE), which improves cross-modal learning by matching mean- and std-pooled protein embeddings with text representations via contrastive loss. After the alignment phase, we apply instruction-based fine-tuning using LoRA on the decoder to teach the model how to generate accurate protein function descriptions conditioned on the protein sequence. We train Prot2Text-V2 on about 250K curated entries from SwissProt and evaluate it under low-homology conditions, where test sequences have low similarity with training samples. Prot2Text-V2 consistently outperforms traditional and LLM-based baselines across various metrics.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11208</link>
<guid>https://arxiv.org/abs/2505.11208</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, analog circuit design, variation-aware design, robustness, PVT variations

Summary:
GLOVA is introduced as an analog circuit sizing framework to address challenges in variation-aware design. It utilizes risk-sensitive reinforcement learning to improve robustness against PVT variations, with an ensemble-based critic for efficient learning. A $\mu$-$\sigma$ evaluation and simulation reordering method is proposed for design verification, reducing simulation costs. GLOVA supports industrial-level PVT variation evaluation methods, including corner simulation and global/local Monte Carlo simulations. Compared to existing frameworks, GLOVA demonstrates significant improvements in sample efficiency and time, achieving up to 80.5$\times$ improvement in sample efficiency and 76.0$\times$ reduction in time. <div>
arXiv:2505.11208v1 Announce Type: cross 
Abstract: Analog/mixed-signal circuit design encounters significant challenges due to performance degradation from process, voltage, and temperature (PVT) variations. To achieve commercial-grade reliability, iterative manual design revisions and extensive statistical simulations are required. While several studies have aimed to automate variation aware analog design to reduce time-to-market, the substantial mismatches in real-world wafers have not been thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing framework that effectively manages the impact of diverse random mismatches to improve robustness against PVT variations. In the proposed approach, risk-sensitive reinforcement learning is leveraged to account for the reliability bound affected by PVT variations, and ensemble-based critic is introduced to achieve sample-efficient learning. For design verification, we also propose $\mu$-$\sigma$ evaluation and simulation reordering method to reduce simulation costs of identifying failed designs. GLOVA supports verification through industrial-level PVT variation evaluation methods, including corner simulation as well as global and local Monte Carlo (MC) simulations. Compared to previous state-of-the-art variation-aware analog sizing frameworks, GLOVA achieves up to 80.5$\times$ improvement in sample efficiency and 76.0$\times$ reduction in time.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets</title>
<link>https://arxiv.org/abs/2502.01506</link>
<guid>https://arxiv.org/abs/2502.01506</guid>
<content:encoded><![CDATA[
<div> Keywords: social emergence, large language model agents, multi-agent framework, socio-economic systems, emergent phenomena

Summary: 
 This study introduces TwinMarket, a new framework that uses large language model agents to simulate socio-economic systems. Traditional modeling approaches struggle to capture the complexity of human behavior, but TwinMarket's use of LLMs allows for more realistic simulations. By focusing on individual behaviors and their interactions, the framework demonstrates how collective dynamics and emergent phenomena can arise. The experiments conducted in a simulated stock market environment show how individual actions can lead to group behaviors, resulting in outcomes such as financial bubbles and recessions. This approach provides valuable insights into the intricate relationship between individual decision-making and collective socio-economic patterns. Overall, TwinMarket offers a promising avenue for studying social emergence and understanding the complexities of human behavior in socio-economic systems. 

<br /><br /> <div>
arXiv:2502.01506v3 Announce Type: replace 
Abstract: The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Network Structure Search with Program Synthesis</title>
<link>https://arxiv.org/abs/2502.02711</link>
<guid>https://arxiv.org/abs/2502.02711</guid>
<content:encoded><![CDATA[
<div> tensor networks, data compression, program synthesis, constraint-based assessment, search efficiency 

Summary: 
Tensor networks are effective for compressing multi-dimensional data, but finding the optimal structure is challenging. This study proposes a novel approach that views tensor network structure search as a program synthesis problem. By establishing a link between transformation programs and network structures, the method reduces the search space using output-directed splits. A synthesis algorithm identifies promising network candidates through constraint solving, minimizing the need for costly tensor decomposition. Experimental results demonstrate a significant improvement in search speed, achieving compression ratios surpassing state-of-the-art methods by 1.5 to 3 times. The approach also scales effectively to larger tensors and generalizes well to similar data, outperforming generic structures with compression ratios up to 2.4 times better. This enhanced efficiency and effectiveness make the proposed method a valuable tool for data compression tasks. 

<br /><br />Summary: <div>
arXiv:2502.02711v3 Announce Type: replace 
Abstract: Tensor networks provide a powerful framework for compressing multi-dimensional data. The optimal tensor network structure for a given data tensor depends on both data characteristics and specific optimality criteria, making tensor network structure search a difficult problem. Existing solutions typically rely on sampling and compressing numerous candidate structures; these procedures are computationally expensive and therefore limiting for practical applications. We address this challenge by viewing tensor network structure search as a program synthesis problem and introducing an efficient constraint-based assessment method that avoids costly tensor decomposition. Specifically, we establish a correspondence between transformation programs and network structures. We also design a novel operation named output-directed splits to reduce the search space without hindering expressiveness. We then propose a synthesis algorithm to identify promising network candidates through constraint solving, and avoid tensor decomposition for all but the most promising candidates. Experimental results show that our approach improves search speed by up to $10\times$ and achieves compression ratios $1.5\times$ to $3\times$ better than state-of-the-art. Notably, our approach scales to larger tensors that are unattainable by prior work. Furthermore, the discovered topologies generalize well to similar data, yielding compression ratios up to $ 2.4\times$ better than a generic structure while the runtime remains around $110$ seconds.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Market Environments for FinRL Contests</title>
<link>https://arxiv.org/abs/2504.02281</link>
<guid>https://arxiv.org/abs/2504.02281</guid>
<content:encoded><![CDATA[
<div> Contests, FinRL, financial tasks, datasets, evaluation protocols<br />
<br />
Summary: Financial reinforcement learning (FinRL) has shown promise in sequential decision-making in finance, but faces challenges in real-world trading. To address this, three FinRL Contests were organized from 2023 to 2025, covering various financial tasks with standardized definitions, datasets, environments, and baselines. Over 200 participants from 100 institutions and 22 countries joined, with open-source starter kits provided for reproducibility. The contests focused on stock trading, order execution, cryptocurrency trading, and the use of large language model-generated signals. The efforts included task formulations, data curation pipelines, environment implementations, evaluation protocols, participant performance analysis, and organizational insights. These benchmarking initiatives have enhanced the understanding and development of FinRL methods for financial applications. <br /><br /> <div>
arXiv:2504.02281v3 Announce Type: replace 
Abstract: Financial reinforcement learning (FinRL) has emerged as a promising paradigm for sequential decision-making in financial engineering. However, applying RL in real-world trading tasks remains challenging due to the non-stationarity of financial data, low signal-to-noise ratios, and various market frictions. Although numerous FinRL methods have been developed for tasks such as trading and portfolio management, the lack of standardized task definitions, datasets, environments, and baselines has hindered consistent evaluation and reproducibility. To bridge this gap, we organized three FinRL Contests from 2023 to 2025, covering a diverse range of financial tasks such as stock trading, order execution, cryptocurrency trading, and the use of large language model (LLM)-generated signals. These contests attracted 200 participants from over 100 institutions across 22 countries. To promote reproduction, we provided open-source starter kits featuring GPU-optimized parallel market environments and comprehensive documentation. In this paper, we summarize these benchmarking efforts, detailing task formulations, data curation pipelines, environment implementations, evaluation protocols, participant performance, and key organizational insights.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promise of Data-Driven Modeling and Decision Support for Precision Oncology and Theranostics</title>
<link>https://arxiv.org/abs/2505.09899</link>
<guid>https://arxiv.org/abs/2505.09899</guid>
<content:encoded><![CDATA[
<div> Keywords: Cancer, Theranostics, Precision oncology, Reinforcement learning, Data-driven decision support 

Summary: 
This paper discusses the potential of theranostics in precision oncology by combining molecular imaging with targeted therapy for personalized cancer treatment. It explores current data-driven decision support applications with a focus on reinforcement learning to optimize patient-specific care plans for cancer treatment. The study reviews the training environments, state-space representation, performance evaluation criteria, and the measurement of risk and reward in precision oncology applications. It identifies key challenges in the field and proposes a framework that integrates data-driven modeling with reinforcement learning-based decision support to optimize radiopharmaceutical therapy dosing. The framework utilizes advanced technologies such as Neural Ordinary Differential Equations and Physics-Informed Neural Networks to enhance Physiologically Based Pharmacokinetic models. By employing reinforcement learning algorithms, it iteratively refines treatment policies based on individual patient data, aiming to improve outcomes in cancer therapy. 

<br /><br />Summary: <div>
arXiv:2505.09899v1 Announce Type: new 
Abstract: Cancer remains a leading cause of death worldwide, necessitating personalized treatment approaches to improve outcomes. Theranostics, combining molecular-level imaging with targeted therapy, offers potential for precision oncology but requires optimized, patient-specific care plans. This paper investigates state-of-the-art data-driven decision support applications with a reinforcement learning focus in precision oncology. We review current applications, training environments, state-space representation, performance evaluation criteria, and measurement of risk and reward, highlighting key challenges. We propose a framework integrating data-driven modeling with reinforcement learning-based decision support to optimize radiopharmaceutical therapy dosing, addressing identified challenges and setting directions for future research. The framework leverages Neural Ordinary Differential Equations and Physics-Informed Neural Networks to enhance Physiologically Based Pharmacokinetic models while applying reinforcement learning algorithms to iteratively refine treatment policies based on patient-specific data.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical regularized Hierarchical Generative Model for Metallic Glass Structural Generation and Energy Prediction</title>
<link>https://arxiv.org/abs/2505.09977</link>
<guid>https://arxiv.org/abs/2505.09977</guid>
<content:encoded><![CDATA[
<div> variational autoencoder, disordered materials, glass, generative AI models, physics-informed regularizers <br />
Summary: 
The article introduces GlassVAE, a hierarchical graph variational autoencoder designed to handle the complexity of disordered materials like glasses. GlassVAE uses graph representations to create rotation, translation, and permutation invariant embeddings of atomic configurations, enabling efficient generation of realistic structures and exploration of the glass energy landscape. To ensure structural realism and physical fidelity, GlassVAE is augmented with two physics-informed regularizers: a radial distribution function (RDF) loss and an energy regression loss. The regularizers play a crucial role in enhancing the accuracy of the model. By encoding high-dimensional atomistic data into a compact latent vector and decoding it to generate structures with accurate energy predictions, GlassVAE provides a rapid, physics-aware approach for modeling and designing disordered materials. <br /><br /> <div>
arXiv:2505.09977v1 Announce Type: new 
Abstract: Disordered materials such as glasses, unlike crystals, lack long range atomic order and have no periodic unit cells, yielding a high dimensional configuration space with widely varying properties. The complexity not only increases computational costs for atomistic simulations but also makes it difficult for generative AI models to deliver accurate property predictions and realistic structure generation. In this work, we introduce GlassVAE, a hierarchical graph variational autoencoder that uses graph representations to learn compact, rotation, translation, and permutation invariant embeddings of atomic configurations. The resulting structured latent space not only enables efficient generation of novel, physically plausible structures but also supports exploration of the glass energy landscape. To enforce structural realism and physical fidelity, we augment GlassVAE with two physics informed regularizers, a radial distribution function (RDF) loss that captures characteristic short and medium range ordering and an energy regression loss that reflects the broad configurational energetics. Both theoretical analysis and experimental results highlight the critical impact of these regularizers. By encoding high dimensional atomistic data into a compact latent vector and decoding it into structures with accurate energy predictions, GlassVAE provides a fast, physics aware path for modeling and designing disordered materials.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Based Aerospace Engineering -- A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2505.10142</link>
<guid>https://arxiv.org/abs/2505.10142</guid>
<content:encoded><![CDATA[
<div> Knowledge-Based Engineering, aerospace industry, knowledge management, SWARM-SLR, knowledge graph<br />
<br />
Summary: 
The study explores state-of-the-art knowledge management practices in aerospace engineering. It involves an extensive review of over 1,000 articles using SWARM-SLR methodology, qualitative analysis of 164 selected articles, and input from aerospace engineering domain experts. The research results include the creation of a knowledge graph comprising over 700 aerospace engineering processes, software, and data, formalized in the Web Ontology Language (OWL) and mapped to Wikidata entries. The knowledge graph is available on the Open Research Knowledge Graph (ORKG) and an aerospace Wikibase for further collaboration and knowledge exchange. The study highlights the importance of structured, semantic-based approaches in managing aerospace engineering knowledge, aiming to improve design processes, foster collaboration, and promote sustainable aviation practices. The intermediate and final artifacts of the study are documented in a Zenodo dataset for wider dissemination and reuse. <div>
arXiv:2505.10142v1 Announce Type: new 
Abstract: The aerospace industry operates at the frontier of technological innovation while maintaining high standards regarding safety and reliability. In this environment, with an enormous potential for re-use and adaptation of existing solutions and methods, Knowledge-Based Engineering (KBE) has been applied for decades. The objective of this study is to identify and examine state-of-the-art knowledge management practices in the field of aerospace engineering. Our contributions include: 1) A SWARM-SLR of over 1,000 articles with qualitative analysis of 164 selected articles, supported by two aerospace engineering domain expert surveys. 2) A knowledge graph of over 700 knowledge-based aerospace engineering processes, software, and data, formalized in the interoperable Web Ontology Language (OWL) and mapped to Wikidata entries where possible. The knowledge graph is represented on the Open Research Knowledge Graph (ORKG), and an aerospace Wikibase, for reuse and continuation of structuring aerospace engineering knowledge exchange. 3) Our resulting intermediate and final artifacts of the knowledge synthesis, available as a Zenodo dataset. This review sets a precedent for structured, semantic-based approaches to managing aerospace engineering knowledge. By advancing these principles, research, and industry can achieve more efficient design processes, enhanced collaboration, and a stronger commitment to sustainable aviation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Space-Time Multigrid Methods Suitable for Topology Optimisation of Transient Heat Conduction</title>
<link>https://arxiv.org/abs/2505.10168</link>
<guid>https://arxiv.org/abs/2505.10168</guid>
<content:encoded><![CDATA[
<div> topology optimization, transient heat conduction, Space-Time MultiGrid methods, diffusivity, adjoint problem <br />
Summary: <br />
This paper introduces Space-Time MultiGrid (STMG) methods for topology optimization in transient heat conduction problems. The methods employ pointwise smoothers and uniform Cartesian space-time meshes, with a coarsening strategy based on the geometric mean of minimum and maximum diffusivity for high contrast problems. Different discretization methods for coarse levels were tested, with averaging thermal resistivities on finer levels showing best results for one-dimensional cases. A proposed coarsening strategy ensuring spatial resolution on coarse grids yielded mixed results. The STMG methods successfully served as a solver for one-dimensional topology optimization, including solving the adjoint problem. The methods were robust and converged reliably during optimization cycles. They proved effective for the adjoint problem even when the prolongation operator only forwards information in time, despite the adjoint problem moving backwards in time. <div>
arXiv:2505.10168v1 Announce Type: new 
Abstract: This paper presents Space-Time MultiGrid (STMG) methods which are suitable for performing topology optimisation of transient heat conduction problems. The proposed methods use a pointwise smoother and uniform Cartesian space-time meshes. For problems with high contrast in the diffusivity, it was found that it is beneficial to define a coarsening strategy based on the geometric mean of the minimum and maximum diffusivity. However, other coarsening strategies may be better for other smoothers. Several methods of discretising the coarse levels were tested. Of these, it was best to use a method which averages the thermal resistivities on the finer levels. However, this was likely a consequence of the fact that only one spatial dimension was considered for the test problems. A second coarsening strategy was proposed which ensures spatial resolution on the coarse grids. Mixed results were found for this strategy. The proposed STMG methods were used as a solver for a one-dimensional topology optimisation problem. In this context, the adjoint problem was also solved using the STMG methods. The STMG methods were sufficiently robust for this application, since they converged during every optimisation cycle. It was found that the STMG methods also work for the adjoint problem when the prolongation operator only sends information forwards in time, even although the direction of time for the adjoint problem is backwards.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture</title>
<link>https://arxiv.org/abs/2505.09907</link>
<guid>https://arxiv.org/abs/2505.09907</guid>
<content:encoded><![CDATA[
<div> Keywords: Hass avocados, price forecasting, deep learning, TCN-MLP-Attention Architecture, agricultural markets <br />
Summary:  
- The study addresses the importance of price forecasting for high-value crops like Hass avocados due to the demand for healthy foods.
- The proposed hybrid deep learning model, TCN-MLP-Attention Architecture, combines TCN, MLP, and an Attention mechanism to handle complex price fluctuations influenced by various factors.
- The dataset consists of over 50,000 records of Hass avocado sales in the U.S. from 2015 to 2018, including variables like sales volume, average price, region, weather, and variety type.
- After systematic preprocessing, the model demonstrated excellent predictive performance, outperforming traditional methods with an RMSE of 1.23 and an MSE of 1.51.
- The research offers a scalable and effective approach for time series forecasting in agricultural markets, providing valuable insights for intelligent supply chain management and price strategy optimization. 

<br /><br />Summary: <div>
arXiv:2505.09907v1 Announce Type: cross 
Abstract: With the growing demand for healthy foods, agricultural product price forecasting has become increasingly important. Hass avocados, as a high-value crop, exhibit complex price fluctuations influenced by factors such as seasonality, region, and weather. Traditional prediction models often struggle with highly nonlinear and dynamic data. To address this, we propose a hybrid deep learning model, TCN-MLP-Attention Architecture, combining Temporal Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for dynamic feature weighting. The dataset used covers over 50,000 records of Hass avocado sales across the U.S. from 2015 to 2018, including variables such as sales volume, average price, time, region, weather, and variety type, collected from point-of-sale systems and the Hass Avocado Board. After systematic preprocessing, including missing value imputation and feature normalization, the proposed model was trained and evaluated. Experimental results demonstrate that the TCN-MLP-Attention model achieves excellent predictive performance, with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods. This research provides a scalable and effective approach for time series forecasting in agricultural markets and offers valuable insights for intelligent supply chain management and price strategy optimization.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Foundation Model for Chemical Reactor Modeling: Meta-Learning with Physics-Informed Adaptation</title>
<link>https://arxiv.org/abs/2405.11752</link>
<guid>https://arxiv.org/abs/2405.11752</guid>
<content:encoded><![CDATA[
<div> neural network framework, chemical reactors, meta-learning, physics-informed, generalizable

Summary:
This work introduces a neural network framework for chemical reactor modeling that can generalize across different reactor types and quickly adapt to new chemical processes. By leveraging meta-learning, the model is pretrained on a wide range of reactor dynamics, allowing for efficient adaptation to unseen reactions with minimal data. Additionally, physics-informed fine-tuning is incorporated to ensure consistent adaptation to new reactor conditions. The framework is evaluated on three fundamental reactor types, showing superior few-shot adaptation compared to traditional approaches. By combining meta-learning with physics-informed techniques, this work paves the way for a more generalizable modeling framework in chemical engineering applications. <div>
arXiv:2405.11752v3 Announce Type: replace 
Abstract: Developing accurate models for chemical reactors is often challenging due to the complexity of reaction kinetics and process dynamics. Traditional approaches require retraining models for each new system, limiting generalizability and efficiency. In this work, we take a step toward foundation models for chemical reactor modeling by introducing a neural network framework that generalizes across diverse reactor types and rapidly adapts to new chemical processes. Our approach leverages meta-learning to pretrain the model on a broad set of reactor dynamics, enabling efficient adaptation to unseen reactions with minimal data. To further enhance generalizability, we incorporate physics-informed fine-tuning, ensuring physically consistent adaptation to new reactor conditions. Our framework is evaluated across three integer-order fundamental reactor types - continuous stirred tank reactors, batch reactors, and plug flow reactors - demonstrating superior few-shot adaptation compared to conventional data-driven, physics-informed, and transfer learning approaches. By combining meta-learning with physics-informed adaptation, this work lays the foundation for a generalizable modeling framework, advancing the development of foundation models for chemical engineering applications. Source code is available at https://github.com/killingbear999/chemical-reactor-foundation-model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2406.00367</link>
<guid>https://arxiv.org/abs/2406.00367</guid>
<content:encoded><![CDATA[
<div> RoBERTa-BiLSTM, sentiment analysis, deep learning, Transformer, latent intentions
<br />
Summary:
The article introduces a hybrid deep learning model, RoBERTa-BiLSTM, combining RoBERTa and BiLSTM for sentiment analysis. Challenges in sentiment analysis include lexical diversity, long dependencies, unknown symbols, and imbalanced datasets. Existing models like BERT and RoBERTa face issues with processing time due to sequential nature. The RoBERTa-BiLSTM model aims to address these challenges by leveraging RoBERTa for word embeddings and BiLSTM for contextual semantics. Experimental results on IMDb, Twitter US Airline, and Sentiment140 datasets show RoBERTa-BiLSTM outperforming baseline models like BERT and RoBERTa, achieving high accuracies and F1 scores. The model achieves accuracies of 80.74%, 92.36%, and 82.25% on the respective datasets, with corresponding F1 scores. This hybrid approach demonstrates improved performance in sentiment analysis tasks. 
<br /> <div>
arXiv:2406.00367v2 Announce Type: replace-cross 
Abstract: Effectively analyzing the comments to uncover latent intentions holds immense value in making strategic decisions across various domains. However, several challenges hinder the process of sentiment analysis including the lexical diversity exhibited in comments, the presence of long dependencies within the text, encountering unknown symbols and words, and dealing with imbalanced datasets. Moreover, existing sentiment analysis tasks mostly leveraged sequential models to encode the long dependent texts and it requires longer execution time as it processes the text sequentially. In contrast, the Transformer requires less execution time due to its parallel processing nature. In this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM, which combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with Bidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to generate meaningful word embedding vectors, while BiLSTM effectively captures the contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid model leverages the strengths of both sequential and Transformer models to enhance performance in sentiment analysis. We conducted experiments using datasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the proposed model against existing state-of-the-art methods. Our experimental findings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models (e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies of 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140 datasets, respectively. Additionally, the model achieves F1-scores of 80.73%, 92.35%, and 82.25% on the same datasets, respectively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Digital Twins with Quantified Uncertainty for Patient-Specific Decision Making in Oncology</title>
<link>https://arxiv.org/abs/2505.08927</link>
<guid>https://arxiv.org/abs/2505.08927</guid>
<content:encoded><![CDATA[
<div> methodology, digital twins, personalized medicine, uncertainty quantification, tumor progression

Summary: 
This study introduces an end-to-end methodology for personalized modeling in biomedicine, using digital twins to improve individual patient outcomes by combining patient data with mechanistic models of disease progression. The approach integrates longitudinal non-invasive imaging data with a reaction-diffusion model to estimate and predict spatiotemporal tumor progression while considering patient-specific anatomy. By solving a statistical inverse problem, imaging data inform spatially varying model parameters. An efficient parallel implementation of the forward model and scalable Bayesian posterior distribution approximation enable rigorous uncertainty quantification due to sparse, noisy measurements. The methodology is validated using synthetic data on a virtual patient to account for model limitations, noise level, and data frequency. Decision-making applications are demonstrated by evaluating the impact of imaging frequency and optimal experimental design. Clinical relevance is illustrated through model validation on a cohort of patients with longitudinal imaging data, showcasing the potential for risk-informed personalized medicine. 

<br /><br />Summary: <div>
arXiv:2505.08927v1 Announce Type: new 
Abstract: Quantifying the uncertainty in predictive models is critical for establishing trust and enabling risk-informed decision making for personalized medicine. In contrast to one-size-fits-all approaches that seek to mitigate risk at the population level, digital twins enable personalized modeling thereby potentially improving individual patient outcomes. Realizing digital twins in biomedicine requires scalable and efficient methods to integrate patient data with mechanistic models of disease progression. This study develops an end-to-end data-to-decisions methodology that combines longitudinal non-invasive imaging data with mechanistic models to estimate and predict spatiotemporal tumor progression accounting for patient-specific anatomy. Through the solution of a statistical inverse problem, imaging data inform the spatially varying parameters of a reaction-diffusion model of tumor progression. An efficient parallel implementation of the forward model coupled with a scalable approximation of the Bayesian posterior distribution enables rigorous, but tractable, quantification of uncertainty due to the sparse, noisy measurements. The methodology is verified on a virtual patient with synthetic data to control for model inadequacy, noise level, and the frequency of data collection. The application to decision-making is illustrated by evaluating the importance of imaging frequency and formulating an optimal experimental design question. The clinical relevance is demonstrated through a model validation study on a cohort of patients with publicly available longitudinal imaging data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of the initial post-buckling response of trusses and frames by an asymptotic approach</title>
<link>https://arxiv.org/abs/2505.09373</link>
<guid>https://arxiv.org/abs/2505.09373</guid>
<content:encoded><![CDATA[
<div> asymptotic theory, post-buckling, trusses, frames, topology optimization
<br />
Summary: 
The article explores the application of asymptotic post-buckling theory in sizing and topology optimization of trusses and frames, highlighting its potential benefits and existing computational challenges. By incorporating the lowest two asymptotic coefficients in the optimization formulation, representing initial post-buckling slope and curvature, designers can control the post-buckling response and reduce imperfection sensitivity in optimized designs. The asymptotic expansion enables the approximation of structural nonlinear response, allowing optimization for specific measures of nonlinear mechanical performance such as end-compliance or complementary work. The study demonstrates the effective use of the asymptotic method in including post-buckling constraints in structural optimization through examples of linear and nonlinear compliance minimization for trusses and frames. <div>
arXiv:2505.09373v1 Announce Type: new 
Abstract: Asymptotic post-buckling theory is applied to sizing and topology optimization of trusses and frames, exploring its potential and current computational difficulties. We show that a designs' post-buckling response can be controlled by including the lowest two asymptotic coefficients, representing the initial post-buckling slope and curvature, in the optimization formulation. This also reduces the imperfection sensitivity of the optimized design. The asymptotic expansion can further be used to approximate the structural nonlinear response, and then to optimize for a given measure of the nonlinear mechanical performance such as, for example, end-compliance or complementary work. Examples of linear and nonlinear compliance minimization of trusses and frames show the effective use of the asymptotic method for including post-buckling constraints in structural optimization.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radon Exposure Dataset</title>
<link>https://arxiv.org/abs/2505.09489</link>
<guid>https://arxiv.org/abs/2505.09489</guid>
<content:encoded><![CDATA[
<div> Keywords: radon, lung cancer, dataset, modeling, prediction

Summary: 
The study focuses on creating a comprehensive dataset for modeling and predicting household radon concentrations at small scales in Pennsylvania and Utah. It aims to identify at-risk populations in areas with high radon levels by examining geological and demographic factors. The dataset combines information on temperature, geochemistry, soil characteristics, and demographic variables such as heating fuel used and building age. This data serves as a foundational resource for future studies and can be scaled up to predict radon exposure potential at a national level. By identifying populations at risk, this research helps in mitigating the risk of lung cancer due to elevated radon levels in homes.<br /><br />Summary: <div>
arXiv:2505.09489v1 Announce Type: new 
Abstract: Exposure to elevated radon levels in the home is one of the leading causes of lung cancer in the world. The following study describes the creation of a comprehensive, state-level dataset designed to enable the modeling and prediction of household radon concentrations at Zip Code Tabulation Area (ZCTA) and sub-kilometer scales. Details include the data collection and processing involved in compiling physical and demographic factors for Pennsylvania and Utah. Attempting to mitigate this risk requires identifying the underlying geological causes and the populations that might be at risk. This work focuses on identifying at-risk populations throughout Pennsylvania and Utah, where radon levels are some of the highest in the country. The resulting dataset harmonizes geological and demographic factors from various sources and spatial resolutions, including temperature, geochemistry, and soil characteristics. Demographic variables such as the household heating fuel used, the age of building, and the housing type provide further insight into which populations could be most susceptible in areas with potentially high radon levels. This dataset also serves as a foundational resource for two other studies conducted by the authors. The resolution of the data provides a novel approach to predicting potential radon exposure, and the data processing conducted for these states can be scaled up to larger spatial resolutions (e.g., the Contiguous United States [CONUS]) and allow for a broad reclassification of radon exposure potential in the United States.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep-MacroFin: Informed Equilibrium Neural Network for Continuous Time Economic Models</title>
<link>https://arxiv.org/abs/2408.10368</link>
<guid>https://arxiv.org/abs/2408.10368</guid>
<content:encoded><![CDATA[
<div> Framework, Deep learning, Partial differential equations, Continuous time economics, Neural networks

Summary: 
The paper introduces Deep-MacroFin, a framework utilizing deep learning techniques to solve partial differential equations, specifically focusing on continuous time economic models. The framework incorporates Multi-Layer Perceptrons and Kolmogorov- Arnold Networks, optimized using Hamilton-Jacobi-Bellman equations and algebraic equations for economic information. It aims to efficiently solve high-dimensional problems with reduced computational requirements compared to traditional methods. Deep-MacroFin offers a user-friendly implementation with significant memory and computational efficiency improvements, making it adaptable to high-dimensional systems of equations. Additionally, a time-stepping scheme enhances training stability, enabling the solution of nonlinear HJB equations and 50-dimensional economic models. <div>
arXiv:2408.10368v4 Announce Type: replace-cross 
Abstract: In this paper, we present Deep-MacroFin, a comprehensive framework designed to solve partial differential equations, with a particular focus on models in continuous time economics. This framework leverages deep learning methodologies, including Multi-Layer Perceptrons and the newly developed Kolmogorov-Arnold Networks. It is optimized using economic information encapsulated by Hamilton-Jacobi-Bellman (HJB) equations and coupled algebraic equations. The application of neural networks holds the promise of accurately resolving high-dimensional problems with fewer computational demands and limitations compared to other numerical methods. This framework can be readily adapted for systems of partial differential equations in high dimensions. Importantly, it offers a more efficient (5$\times$ less CUDA memory and 40$\times$ fewer FLOPs in 100D problems) and user-friendly implementation than existing libraries. We also incorporate a time-stepping scheme to enhance training stability for nonlinear HJB equations, enabling the solution of 50D economic models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
<link>https://arxiv.org/abs/2505.08316</link>
<guid>https://arxiv.org/abs/2505.08316</guid>
<content:encoded><![CDATA[
<div> Keywords: ventral visual stream, object recognition, relative position prediction, unsupervised task-driven method, brain similarity<br />
Summary:<br />
The article introduces a new function of the ventral visual stream (VVS) called relative position (RP) prediction, in addition to its role in object recognition. It criticizes current unsupervised task-driven methods for modeling VVS through contrastive learning, arguing that this approach may not capture the capabilities of RP prediction. To address this, a new method that integrates RP learning with contrastive learning is proposed. Experimental results demonstrate that this new approach improves object recognition performance and enhances RP predictivity, ultimately leading to better model brain similarity. These findings suggest that the VVS may play a role in location perception, particularly in RP prediction, highlighting the need for a more comprehensive understanding of its functions. <br /><br />Summary: <div>
arXiv:2505.08316v1 Announce Type: new 
Abstract: Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology and geometry optimization of grid-shells under self-weight loading</title>
<link>https://arxiv.org/abs/2505.08645</link>
<guid>https://arxiv.org/abs/2505.08645</guid>
<content:encoded><![CDATA[
<div> Keywords: grid-shell structures, optimization, connectivity, elevation, second-order cone optimization

Summary:<br /><br />This manuscript presents an approach for optimizing the connectivity and elevation of grid-shell structures under pure compression or tension, considering external loading and self-weight. The method involves solving a second-order cone optimization problem to ensure convexity and globally optimal solutions. Numerical examples demonstrate the characteristics of these optimal structures, showing the importance of simultaneous topology and geometry optimization. The study reveals that as self-weight increases, optimal topology and elevation profiles change significantly. The approach provides more accurate solutions and is vastly quicker than traditional 3D layout/truss topology optimization methods. This research underscores the significance of integrating topology and geometry optimization from the initial design phase for efficient and effective structural design. <div>
arXiv:2505.08645v1 Announce Type: new 
Abstract: This manuscript presents an approach for simultaneously optimizing the connectivity and elevation of grid-shell structures acting in pure compression (or pure tension) under the combined effects of a prescribed external loading and the design-dependent self-weight of the structure itself. The method derived herein involves solving a second-order cone optimization problem, thereby ensuring convexity and obtaining globally optimal results for a given discretization of the design domain. Several numerical examples are presented, illustrating characteristics of this class of optimal structures. It is found that, as self-weight becomes more significant, both the optimal topology and the optimal elevation profile of the structure change, highlighting the importance of optimizing both topology and geometry simultaneously from the earliest stages of design. It is shown that this approach can obtain solutions with greater accuracy and several orders of magnitude more quickly than a standard 3D layout/truss topology optimization approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Value of Information-based assessment of strain-based thickness loss monitoring in ship hull structures</title>
<link>https://arxiv.org/abs/2505.07427</link>
<guid>https://arxiv.org/abs/2505.07427</guid>
<content:encoded><![CDATA[
<div> Bayesian analysis, Structural Health Monitoring, ship hulls, corrosion-induced thickness loss, decision analysis<br />
<br />
Summary: This study explores the value of information from Structural Health Monitoring (SHM) systems monitoring corrosion-induced thickness loss (CITL) in ship hulls. The research utilizes a Bayesian pre-posterior decision analysis to quantify the benefits of SHM in optimizing hull maintenance. Decision consequence cost functions based on exceedance probabilities relative to a target CITL threshold are defined, allowing for practical decision-making based on risk perception. A high-fidelity numerical model of a commercial vessel is used to compare the benefits of different CITL monitoring strategies, including strain-based SHM and traditional on-site inspections. The findings provide insights into the potential of SHM in enhancing maintenance practices for ship structures. <br /><br /> <div>
arXiv:2505.07427v1 Announce Type: cross 
Abstract: Recent advances in Structural Health Monitoring (SHM) have attracted industry interest, yet real-world applications, such as in ship structures remain scarce. Despite SHM's potential to optimise maintenance, its adoption in ships is limited due to the lack of clearly quantifiable benefits for hull maintenance. This study employs a Bayesian pre-posterior decision analysis to quantify the value of information (VoI) from SHM systems monitoring corrosion-induced thickness loss (CITL) in ship hulls, in a first-of-its-kind analysis for ship structures. We define decision-making consequence cost functions based on exceedance probabilities relative to a target CITL threshold, which can be set by the decision-maker. This introduces a practical aspect to our framework, that enables implicitly modelling the decision-maker's risk perception. We apply this framework to a large-scale, high-fidelity numerical model of a commercial vessel and examine the relative benefits of different CITL monitoring strategies, including strain-based SHM and traditional on-site inspections.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUEST: QUantum-Enhanced Shared Transportation</title>
<link>https://arxiv.org/abs/2505.08074</link>
<guid>https://arxiv.org/abs/2505.08074</guid>
<content:encoded><![CDATA[
<div> framework, shared transportation, Quantum-Enhanced Shared Transportation, windbreaker, windsurfer
Summary:
Quantum-Enhanced Shared Transportation introduces Windbreaking-as-a-Service (WaaS) for shared transportation using larger "windbreaker" vehicles to provide aerodynamic shelter for smaller "windsurfer" vehicles. The computational framework QUEST solves the matching and assignment problems by formulating them as a mixed-integer quadratic problem and encoding them as a Quadratic Unconstrained Binary Optimization for quantum processing. Classical methods such as the Hungarian Algorithm and quantum algorithms like Quantum Approximate Optimization Algorithm (QAOA) are used to solve the assignment problem. The quantum implementation successfully identifies the optimal assignment, demonstrating the effectiveness of the QUEST pipeline for controlled prototypes. This study paves the way for addressing more complex scenarios and leveraging quantum technologies for large-scale shared-transportation instances. <br /><br />Summary: <div>
arXiv:2505.08074v1 Announce Type: cross 
Abstract: We introduce ``Windbreaking-as-a-Service'' (WaaS) as an innovative approach to shared transportation in which larger ``windbreaker'' vehicles provide aerodynamic shelter for ``windsurfer'' vehicles, thereby reducing drag and fuel consumption. As a computational framework to solve the large-scale matching and assignment problems that arise in WaaS, we present \textbf{QUEST} (Quantum-Enhanced Shared Transportation). Specifically, we formulate the pairing of windbreakers and windsurfers -- subject to timing, speed, and vehicle-class constraints -- as a mixed-integer quadratic problem (MIQP). Focusing on a single-segment prototype, we verify the solution classically via the Hungarian Algorithm, a Gurobi-based solver, and brute-force enumeration of binary vectors. We then encode the problem as a Quadratic Unconstrained Binary Optimization (QUBO) and map it to an Ising Hamiltonian, enabling the use of the Quantum Approximate Optimization Algorithm (QAOA) and other quantum and classical annealing technologies. Our quantum implementation successfully recovers the optimal assignment identified by the classical methods, confirming the soundness of the QUEST pipeline for a controlled prototype. While QAOA and other quantum heuristics do not guarantee a resolution of the fundamental complexity barriers, this study illustrates how the WaaS problem can be systematically translated into a quantum-ready model. It also lays the groundwork for addressing multi-segment scenarios and potentially leveraging quantum advantage for large-scale shared-transportation instances.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations</title>
<link>https://arxiv.org/abs/2505.08740</link>
<guid>https://arxiv.org/abs/2505.08740</guid>
<content:encoded><![CDATA[
<div> sensitivity-based regularization, Fourier Neural Operator, inverse problems, parameter inversion, high-dimensional parameter spaces <br />
Summary: <br />
The article introduces Sensitivity-Constrained Fourier Neural Operators (SC-FNO) to address limitations faced by deep learning frameworks like the Fourier Neural Operator (FNO) in solving parametric differential equations. SC-FNO includes a sensitivity-based regularization strategy that improves accuracy in predicting solution paths and outperforms standard FNO and FNO with physics-informed regularization. It is particularly effective in parameter inversion tasks and can handle high-dimensional parameter spaces with up to 82 parameters. SC-FNO reduces data and training requirements while achieving high performance across different types of differential equations and neural operators. Although there is a slight increase in training time, the benefits of SC-FNO are significant in terms of accuracy and scalability. The code and selected experiments for SC-FNO are available on GitHub for further exploration and implementation. <div>
arXiv:2505.08740v1 Announce Type: cross 
Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are fundamental in science and engineering. While deep learning frameworks such as the Fourier Neural Operator (FNO) can efficiently approximate solutions, they struggle with inverse problems, sensitivity estimation (du/dp), and concept drift. We address these limitations by introducing a sensitivity-based regularization strategy, called Sensitivity-Constrained Fourier Neural Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths and consistently outperforms standard FNO and FNO with physics-informed regularization. It improves performance in parameter inversion tasks, scales to high-dimensional parameter spaces (tested with up to 82 parameters), and reduces both data and training requirements. These gains are achieved with a modest increase in training time (30% to 130% per epoch) and generalize across various types of differential equations and neural operators. Code and selected experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles</title>
<link>https://arxiv.org/abs/2505.08782</link>
<guid>https://arxiv.org/abs/2505.08782</guid>
<content:encoded><![CDATA[
<div> Quantum Machine Learning, NISQ devices, multi-chip ensemble VQC framework, scalability, trainability, noise resilience<br />
Summary:<br />
The article introduces a multi-chip ensemble VQC framework to address limitations of noisy intermediate-scale quantum (NISQ) devices in Quantum Machine Learning (QML). This framework partitions computations across smaller quantum chips to enhance scalability, trainability, and noise resilience. It mitigates barren plateaus, reduces quantum error bias and variance, and maintains robust generalization through controlled entanglement. The framework is designed to work with current and future quantum hardware, making it suitable for scalable QML on near-term devices. Experimental validation was done on standard benchmark datasets (MNIST, FashionMNIST, CIFAR-10) and a real-world dataset (PhysioNet EEG), demonstrating the framework's potential in enabling practical QML applications. <br /><br />Summary: <div>
arXiv:2505.08782v1 Announce Type: cross 
Abstract: Quantum Machine Learning (QML) holds significant promise for solving computational challenges across diverse domains. However, its practical deployment is constrained by the limitations of noisy intermediate-scale quantum (NISQ) devices, including noise, limited scalability, and trainability issues in variational quantum circuits (VQCs). We introduce the multi-chip ensemble VQC framework, which partitions high-dimensional computations across smaller quantum chips to enhance scalability, trainability, and noise resilience. We show that this approach mitigates barren plateaus, reduces quantum error bias and variance, and maintains robust generalization through controlled entanglement. Designed to align with current and emerging quantum hardware, the framework demonstrates strong potential for enabling scalable QML on near-term devices, as validated by experiments on standard benchmark datasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet EEG).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAAC panels can suddenly collapse before any warning of corrosion-induced surface cracking</title>
<link>https://arxiv.org/abs/2505.06294</link>
<guid>https://arxiv.org/abs/2505.06294</guid>
<content:encoded><![CDATA[
<div> Keywords: RAAC, corrosion, modeling, porosity, collapse  

<br /><br />Summary: The study addresses the issue of reinforced autoclaved aerated concrete (RAAC) panel collapses, which have gained significant attention due to safety concerns. A lack of detailed experimental data and the lengthy process of replicating natural corrosion has underscored the importance of computational modeling in this context. Researchers suspect that the high porosity of RAAC contributes to corrosion concealment; however, existing models for corrosion-induced cracking have limited capabilities in incorporating concrete porosity effects. To improve this, the authors introduce an enriched model that integrates analytical solutions of reactive transport equations with a porosity-dependent diffusivity description. This innovative approach allows for the first computational exploration of corrosion concealment in RAAC panels. The findings reveal that RAAC panels can experience sudden collapses without visible indicators of surface cracking due to corrosion. Additionally, the research identifies the specific conditions that increase the likelihood of sudden collapse, enhancing the understanding of RAAC durability under corrosive environments. This work provides crucial insights for engineers and researchers concerned with the structural integrity and safety of RAAC constructions. <div>
arXiv:2505.06294v1 Announce Type: new 
Abstract: The collapse of reinforced autoclaved aerated concrete (RAAC) panels has attracted considerable public and academic interest. As detailed experimental data are not yet available and replicating the natural corrosion process requires years or decades, computational modelling is essential to understand under which conditions corrosion remains concealed. The very high porosity of RAAC is widely suspected to be a major contributing factor. However, current corrosion-induced cracking models are known to struggle with capturing the role of concrete porosity. To remedy this critical deficiency, we propose to enrich corrosion-induced cracking modelling with the analytical solution of reactive transport equations governing the precipitation of rust and a porosity-dependent description of diffusivity. With this, the corrosion concealment in RAAC panels is studied computationally for the first time, revealing that RAAC panels can suddenly collapse before any warning of corrosion-induced surface cracking and allowing to map the conditions most likely to result in sudden collapse.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New DAPO Algorithm for Stock Trading</title>
<link>https://arxiv.org/abs/2505.06408</link>
<guid>https://arxiv.org/abs/2505.06408</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, financial trading, Dynamic Sampling Policy Optimization, Large Language Models, trading agent <br />
Summary:
The study explores the application of reinforcement learning techniques, specifically the Dynamic Sampling Policy Optimization (DAPO), in financial trading using large language models (LLMs) for risk and sentiment analysis. By combining an improved Group Relative Policy Optimization (GRPO) algorithm with LLM-based signals, a trading agent was developed and tested on the NASDAQ-100 index. Results showed a cumulative return of 230.49% and an information ratio of 0.37, surpassing the CPPO-DeepSeek baseline. Additionally, the agent's training time was reduced from 8 to 2.5 hours over 100 epochs, with decreased RAM usage. This RL-LLM framework demonstrates potential for data-efficient trading agents, offering scalability and improved performance in financial markets. <br /> <div>
arXiv:2505.06408v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning, such as Dynamic Sampling Policy Optimization (DAPO), show strong performance when paired with large language models (LLMs). Motivated by this success, we ask whether similar gains can be realized in financial trading. We design a trading agent that combines an improved Group Relative Policy Optimization (GRPO) algorithm, augmented with ideas from DAPO, with LLM-based risk and sentiment signals extracted from financial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains a cumulative return of 230.49 percent and an information ratio of 0.37, outperforming the CPPO-DeepSeek baseline. It also cuts training time from about 8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. The proposed RL-LLM framework offers a scalable path toward data-efficient trading agents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Ternary Encoding for High-Speed Data Transmission in 3D-Integrated Circuits Using Inductive Coupling Links</title>
<link>https://arxiv.org/abs/2505.06908</link>
<guid>https://arxiv.org/abs/2505.06908</guid>
<content:encoded><![CDATA[
<div> Inductive coupling links, 3D-integrated circuits, ternary signalling scheme, crosstalk reduction, electromagnetic interference<br />
<br />
Summary:<br />
This paper introduces a novel ternary signalling scheme for inductive coupling links (ICLs) in 3D-integrated circuits (3D-ICs) to mitigate crosstalk and electromagnetic interference. By utilizing three voltage levels (-V, 0V, +V) instead of the traditional binary approach, this scheme enhances signal separation, decreases crosstalk, and enhances signal integrity. The ternary system allows for increased bandwidth efficiency and reduced power consumption as compared to Non-Return to Zero (NRZ) systems due to fewer signal transitions. A modified H-Bridge transmitter is used to generate ternary symbols by regulating current flow based on binary-to-ternary mapping. Initial simulations corroborate the effectiveness of the scheme, demonstrating minimized power consumption and higher data rates in comparison with NRZ. This innovative approach presents potential benefits for high-performance computing and Internet of Things (IoT) devices in 3D-IC settings, providing enhanced noise resilience, reduced power consumption, and heightened communication efficiency. <div>
arXiv:2505.06908v1 Announce Type: new 
Abstract: This paper proposes a ternary signalling scheme for inductive coupling links (ICLs) in 3D-integrated circuits (3D-ICs) to reduce crosstalk and electromagnetic interference in multi-stacked chip communications. By converting binary data into ternary sequences with three voltage levels (-V, 0V, +V), the approach enhances signal separation, reduces crosstalk, and improves signal integrity. Unlike traditional Non-Return to Zero (NRZ) systems, the ternary scheme increases bandwidth efficiency and reduces power consumption through fewer signal transitions. A modified H-Bridge transmitter generates ternary symbols by controlling current flow based on binary-to-ternary mapping. Preliminary simulations validate the efficiency of the scheme, showing reduced power consumption and higher data rates compared to NRZ. This approach shows promise for high-performance computing and IoT devices in 3D-IC environments, offering enhanced noise resilience, lower power usage, and improved communication efficiency.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative study of Bitcoin and Ripple cryptocurrencies trading using Deep Reinforcement Learning algorithms</title>
<link>https://arxiv.org/abs/2505.07660</link>
<guid>https://arxiv.org/abs/2505.07660</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Reinforcement Learning, Bitcoin, Ripple, Trading Strategies

<br /><br />Summary: 
This research focuses on the application of Artificial Intelligence (AI) in automated trading, specifically targeting the challenges presented by the volatile and dynamic nature of financial asset prices. The study emphasizes developing an innovative rule-based strategy to train Deep Reinforcement Learning (DRL) models for trading Bitcoin (BTC) and Ripple (XRP). Key methodologies employed include Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, and Advantage Actor-Critic algorithms, all aimed at deriving optimal trading policies. To assess the effectiveness of the proposed DRL approach, the primary performance metrics utilized are portfolio wealth and trade signals. Experimental results reveal that both Dueling and Double Deep Q-Networks exhibit superior performance when applied to XRP, as evidenced by increased portfolio wealth. The research highlights the potential of AI-driven techniques in improving trading strategies and their adaptability to fluctuating market conditions. All code related to the study is made publicly accessible through a GitHub repository, fostering transparency and collaboration within the research community. <div>
arXiv:2505.07660v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has demonstrated remarkable success across various applications. In light of this trend, the field of automated trading has developed a keen interest in leveraging AI techniques to forecast the future prices of financial assets. This interest stems from the need to address trading challenges posed by the inherent volatility and dynamic nature of asset prices. However, crafting a flawless strategy becomes a formidable task when dealing with assets characterized by intricate and ever-changing price dynamics. To surmount these formidable challenges, this research employs an innovative rule-based strategy approach to train Deep Reinforcement Learning (DRL). This application is carried out specifically in the context of trading Bitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration of Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, alongside the Advantage Actor-Critic algorithms. Each of them aims to yield an optimal policy for our application. To evaluate the effectiveness of our Deep Reinforcement Learning (DRL) approach, we rely on portfolio wealth and the trade signal as performance metrics. The experimental outcomes highlight that Duelling and Double Deep Q-Network outperformed when using XRP with the increasing of the portfolio wealth. All codes are available in this \href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\color{blue}Github link}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALFEE: Adaptive Large Foundation Model for EEG Representation</title>
<link>https://arxiv.org/abs/2505.06291</link>
<guid>https://arxiv.org/abs/2505.06291</guid>
<content:encoded><![CDATA[
<div> framework, EEG, representation, transformer, pretraining
Summary:
The article introduces the Adaptive Large Foundation model for EEG signal representation (ALFEE) framework, which aims to enhance the representation learning for EEG signals. The framework consists of a hybrid transformer architecture with two learning stages. It utilizes a hybrid attention mechanism to separate channel-wise feature aggregation and temporal dynamics modeling, allowing for robust representation of EEG signals with variable channel configurations. ALFEE includes a channel encoder, temporal encoder, and hybrid decoder to optimize task prediction, channel and temporal mask reconstruction, and temporal forecasting during pretraining. The framework also incorporates full-model adaptation during fine-tuning to improve performance across multiple tasks. Experimental results on six downstream EEG tasks demonstrate the superior performance of ALFEE over existing models. The ALFEE framework provides a scalable foundation for biological signal analysis and is available for implementation on GitHub. 
<br /><br />Summary: <div>
arXiv:2505.06291v1 Announce Type: cross 
Abstract: While foundation models excel in text, image, and video domains, the critical biological signals, particularly electroencephalography(EEG), remain underexplored. EEG benefits neurological research with its high temporal resolution, operational practicality, and safety profile. However, low signal-to-noise ratio, inter-subject variability, and cross-paradigm differences hinder the generalization of current models. Existing methods often employ simplified strategies, such as a single loss function or a channel-temporal joint representation module, and suffer from a domain gap between pretraining and evaluation tasks that compromises efficiency and adaptability. To address these limitations, we propose the Adaptive Large Foundation model for EEG signal representation(ALFEE) framework, a novel hybrid transformer architecture with two learning stages for robust EEG representation learning. ALFEE employs a hybrid attention that separates channel-wise feature aggregation from temporal dynamics modeling, enabling robust EEG representation with variable channel configurations. A channel encoder adaptively compresses variable channel information, a temporal encoder captures task-guided evolution, and a hybrid decoder reconstructs signals in both temporal and frequency domains. During pretraining, ALFEE optimizes task prediction, channel and temporal mask reconstruction, and temporal forecasting to enhance multi-scale and multi-channel representation. During fine-tuning, a full-model adaptation with a task-specific token dictionary and a cross-attention layer boosts performance across multiple tasks. After 25,000 hours of pretraining, extensive experimental results on six downstream EEG tasks demonstrate the superior performance of ALFEE over existing models. Our ALFEE framework establishes a scalable foundation for biological signal analysis with implementation at https://github.com/xw1216/ALFEE.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations</title>
<link>https://arxiv.org/abs/2505.06502</link>
<guid>https://arxiv.org/abs/2505.06502</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Learning, Generative Adversarial Networks, Super Resolution, Physical Consistency, Scientific Applications

<br /><br />Summary: 
Machine Learning, specifically Generative Adversarial Networks (GANs), has significantly transformed Super Resolution (SR) image techniques. However, existing generated images often lack physical meaningfulness, crucial for scientific applications. The proposed PC-SRGAN addresses this limitation by enhancing image resolution while ensuring physical consistency, leading to interpretable simulations. This approach demonstrates notable improvements in the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure when compared to traditional methods, even achieving effective results with limited training data (utilizing just 13% of the data required for SRGAN). In addition to SR, PC-SRGAN incorporates physically meaningful machine learning by integrating numerically justified time integrators and advanced quality metrics, aiming for reliable and causal models in scientific fields. A key benefit of PC-SRGAN over conventional SR methods is its physical consistency, positioning it as a viable surrogate model for time-dependent problems. Ultimately, this advancement supports scientific machine learning by enhancing accuracy and efficiency in image processing, improving process understanding, and offering a broader range of applications in scientific research. The source codes and data will be publicly accessible at the provided GitHub link upon paper acceptance. <div>
arXiv:2505.06502v1 Announce Type: cross 
Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. The source codes and data will be made publicly available at https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery</title>
<link>https://arxiv.org/abs/2505.06795</link>
<guid>https://arxiv.org/abs/2505.06795</guid>
<content:encoded><![CDATA[
<div> Keywords: Commodity price volatility, multi-horizon forecasting, Regularized Sparse Autoencoder, interpretable latent drivers, deep learning framework

<br /><br />Summary: This paper addresses the challenges posed by commodity price volatility, emphasizing the need for accurate multi-horizon forecasting. Predicting prices of commodities such as copper and crude oil is complicated by various factors, including macroeconomic indicators, supply and demand dynamics, and geopolitical events. Existing forecasting models often lack transparency, which limits their strategic applications. To tackle this issue, the authors introduce the Regularized Sparse Autoencoder (RSAE), a deep learning framework that facilitates simultaneous forecasting across multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series data. Notably, L1 regularization is applied to the latent vector of the model to enforce sparsity, leading to more parsimonious explanations of underlying market dynamics, including demand and supply shocks. Drawing insights from energy-based models and sparse coding, the RSAE achieves a balance between predictive accuracy and the ability to learn interpretable representations. Furthermore, evaluations on historical data for Copper and Crude Oil indicate that the RSAE not only provides competitive forecasting accuracy but also offers data-driven insights into price dynamics, distinguishing it from traditional black-box forecasting approaches. <div>
arXiv:2505.06795v1 Announce Type: cross 
Abstract: Commodity price volatility creates economic challenges, necessitating accurate multi-horizon forecasting. Predicting prices for commodities like copper and crude oil is complicated by diverse interacting factors (macroeconomic, supply/demand, geopolitical, etc.). Current models often lack transparency, limiting strategic use. This paper presents a Regularized Sparse Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon commodity price prediction and discovery of interpretable latent market drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series. Crucially, L1 regularization ($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity, promoting parsimonious explanations of market dynamics through learned factors representing underlying drivers (e.g., demand, supply shocks). Drawing from energy-based models and sparse coding, the RSAE optimizes predictive accuracy while learning sparse representations. Evaluated on historical Copper and Crude Oil data with numerous indicators, our findings indicate the RSAE offers competitive multi-horizon forecasting accuracy and data-driven insights into price dynamics via its interpretable latent space, a key advantage over traditional black-box approaches.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?</title>
<link>https://arxiv.org/abs/2505.07078</link>
<guid>https://arxiv.org/abs/2505.07078</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, asset pricing, stock trading, backtesting framework, market regime analysis

Summary: 
The study evaluates the effectiveness of Large Language Models (LLMs) in timing-based investment strategies across a broader range of symbols and over a longer timeframe using the FINSABER backtesting framework. Results show that previously reported advantages of LLM strategies diminish significantly when evaluated over two decades and on over 100 symbols. The analysis reveals that LLM strategies are overly conservative in bull markets, leading to underperformance against passive benchmarks, and overly aggressive in bear markets, resulting in heavy losses. The study highlights the importance of developing LLM strategies that prioritize trend detection and incorporate regime-aware risk controls rather than solely focusing on complex frameworks. 

<br /><br />Summary: <div>
arXiv:2505.07078v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating many-engine spacecraft: Exceeding 100 trillion grid points via information~geometric regularization and the MFC flow solver</title>
<link>https://arxiv.org/abs/2505.07392</link>
<guid>https://arxiv.org/abs/2505.07392</guid>
<content:encoded><![CDATA[
<div> method, exascale simulations, compressible fluid flows, rocket craft, computational cost <br />
Summary:
This work presents a method for exascale simulations of high-speed compressible fluid flows, specifically targeting multi-engine rocket craft simulations. By optimizing the implementation through information geometric regularization and unified addressing on tightly coupled CPU-GPU platforms, significant improvements in computational cost and memory footprint are achieved. The need for numerical shock capturing is eliminated, enabling the simulation of fluid flows at unprecedented scales. The use of linear stencil algorithms, despite being memory-bound, results in faster wall clock times compared to baseline numerics. This allows for CFD simulations with over 100 trillion grid points, surpassing existing state-of-the-art simulations. Ideal weak scaling is demonstrated on OLCF Frontier and CSCS Alps supercomputers, showcasing the scalability of the proposed method on large-scale platforms. <div>
arXiv:2505.07392v1 Announce Type: cross 
Abstract: This work proposes a method and optimized implementation for exascale simulations of high-speed compressible fluid flows, enabling the simulation of multi-engine rocket craft at an unprecedented scale. We significantly improve upon the state-of-the-art in terms of computational cost and memory footprint through a carefully crafted implementation of the recently proposed information geometric regularization, which eliminates the need for numerical shock capturing. Unified addressing on tightly coupled CPU--GPU platforms increases the total problem size with negligible performance hit. Despite linear stencil algorithms being memory-bound, we achieve wall clock times that are four times faster than optimized baseline numerics. This enables the execution of CFD simulations at more than 100 trillion grid points, surpassing the largest state-of-the-art publicly available simulations by an order of magnitude. Ideal weak scaling is demonstrated on OLCF Frontier and CSCS Alps using the full system, entailing 37.8K AMD MI250X GPUs (Frontier) or 9.2K NVIDIA GH200 superchips (Alps).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transient Nonlinear Electrothermal Adjoint Sensitivity Analysis for HVDC Cable Joints</title>
<link>https://arxiv.org/abs/2405.14284</link>
<guid>https://arxiv.org/abs/2405.14284</guid>
<content:encoded><![CDATA[
<div> Adjoint variable method, coupled nonlinear transient electrothermal problems, sensitivities, high voltage direct current cable joints, material sensitivities <br />
Summary: <br />
Efficient computation of sensitivities is crucial for designing and optimizing high voltage direct current cable joints. This study introduces the adjoint variable method for solving coupled nonlinear transient electrothermal problems to efficiently compute material sensitivities for a 320kV cable joint specimen. The results are compared with sensitivities obtained using the direct sensitivity method, validating the effectiveness of the proposed approach. The method can handle a large number of design parameters, providing a robust tool for optimizing cable joint designs. This research contributes to improving the efficiency and accuracy of high voltage cable joint design processes, ultimately leading to better performance and reliability of electrical transmission systems. <div>
arXiv:2405.14284v3 Announce Type: replace 
Abstract: Efficient computation of sensitivities is a promising approach for efficiently of designing and optimizing high voltage direct current cable joints. This paper presents the adjoint variable method for coupled nonlinear transient electrothermal problems as an efficient approach to compute sensitivities with respect to a large number of design parameters. The method is used to compute material sensitivities of a 320kV high voltage direct current cable joint specimen. The results are validated against sensitivities obtained via the direct sensitivity method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Martinize2 and Vermouth: Unified Framework for Topology Generation</title>
<link>https://arxiv.org/abs/2212.01191</link>
<guid>https://arxiv.org/abs/2212.01191</guid>
<content:encoded><![CDATA[
<div> Keywords: force field, molecular dynamics, Martini simulations, high-throughput, Vermouth library

Summary:
The article discusses the development of the Vermouth python library for preparing, running, and analyzing Martini simulations of complex systems. The Martinize2 program, a part of Vermouth, is highlighted for its ability to handle protonation states, post-translation modifications, and structural biases such as the elastic network. It can also convert non-protein molecules like ligands. The Vermouth library addresses the need for automation tools in high-throughput simulations and studies of complex cellular systems. Demonstrated with two high-complexity benchmarks, Martinize2 successfully converts protein structures to coarse-grained resolution from databases like I-TASSER and AlphaFold. These conversions showcase the library's capability to ensure input structure quality for safeguarding high-throughput applications.

<br /><br />Summary: <div>
arXiv:2212.01191v4 Announce Type: replace-cross 
Abstract: Ongoing advances in force field and computer hardware development enable the use of molecular dynamics (MD) to simulate increasingly complex systems with the ultimate goal of reaching cellular complexity. At the same time, rational design by high-throughput (HT) simulations is another forefront of MD. In these areas, the Martini coarse-grained force field, especially the latest version (i.e. v3), is being actively explored because it offers an enhanced spatial-temporal resolution. However, the automation tools for preparing simulations with the Martini force field, accompanying the previous version, were not designed for HT simulations or studies of complex cellular systems. Therefore, they become a major limiting factor. To address these shortcomings, we present the open-source Vermouth python library. Vermouth is designed to become the unified framework for developing programs, which prepare, run, and analyze Martini simulations of complex systems. To demonstrate the power of the Vermouth library, the Martinize2 program is showcased as a generalization of the martinize script, originally aimed to set up simulations of proteins. In contrast to the previous version, Martinize2 automatically handles protonation states in proteins and post-translation modifications, offers more options to fine-tune structural biases such as the elastic network (EN), and can convert non-protein molecules such as ligands. Finally, Martinize2 is used in two high-complexity benchmarks. The entire I-TASSER protein template database as well as a subset of 200,000 structures from the AlphaFold Protein Structure Database are converted to CG resolution and we illustrate how the checks on input structure quality can safeguard high-throughput applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships and Improved GRU</title>
<link>https://arxiv.org/abs/2409.08282</link>
<guid>https://arxiv.org/abs/2409.08282</guid>
<content:encoded><![CDATA[
<div> Keywords: stock price prediction, deep learning, GRU, long short-term relationships, algorithmic trading  

<br /><br />Summary:  
Stock price prediction is a complex challenge attracting significant attention within finance. Recent advances in deep learning and graph neural networks have emphasized exploring inter-stock relationships. However, most existing models primarily focus on short-term dynamics and tend to neglect the intricate nonlinear characteristics and higher-order interactions in the market. To address this, the paper introduces the LSR-IGRU model, aimed at improving stock price trend predictions. This model constructs a long short-term relationship matrix utilizing secondary industry information for long-term connections and overnight price data for short-term dynamics. The GRU inputs are enhanced at each step to better combine temporal and relationship data, boosting prediction accuracy. Extensive experimentation across various stock market datasets from China and the United States confirms the LSR-IGRU model's superiority over leading baseline models. Additionally, the model has been successfully implemented in a financial company's algorithmic trading system, significantly outperforming other methods in generating cumulative portfolio returns. The research's codebase is publicly available at the provided GitHub link. <div>
arXiv:2409.08282v3 Announce Type: replace-cross 
Abstract: Stock price prediction is a challenging problem in the field of finance and receives widespread attention. In recent years, with the rapid development of technologies such as deep learning and graph neural networks, more research methods have begun to focus on exploring the interrelationships between stocks. However, existing methods mostly focus on the short-term dynamic relationships of stocks and directly integrating relationship information with temporal information. They often overlook the complex nonlinear dynamic characteristics and potential higher-order interaction relationships among stocks in the stock market. Therefore, we propose a stock price trend prediction model named LSR-IGRU in this paper, which is based on long short-term stock relationships and an improved GRU input. Firstly, we construct a long short-term relationship matrix between stocks, where secondary industry information is employed for the first time to capture long-term relationships of stocks, and overnight price information is utilized to establish short-term relationships. Next, we improve the inputs of the GRU model at each step, enabling the model to more effectively integrate temporal information and long short-term relationship information, thereby significantly improving the accuracy of predicting stock trend changes. Finally, through extensive experiments on multiple datasets from stock markets in China and the United States, we validate the superiority of the proposed LSR-IGRU model over the current state-of-the-art baseline models. We also apply the proposed model to the algorithmic trading system of a financial company, achieving significantly higher cumulative portfolio returns compared to other baseline methods. Our sources are released at https://github.com/ZP1481616577/Baselines_LSR-IGRU.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfitted finite element modelling of surface-bulk viscous flows in animal cells</title>
<link>https://arxiv.org/abs/2505.05723</link>
<guid>https://arxiv.org/abs/2505.05723</guid>
<content:encoded><![CDATA[
<div> Keywords: unfitted finite element framework, surface-bulk problems, cortex-cytoplasm interactions, cell division, computational modelling

Summary: 
This work introduces a novel unfitted finite element framework for simulating surface-bulk problems in time-dependent domains, specifically focusing on fluid-fluid interactions in animal cells. The framework addresses the challenges of modelling cortical contractions that induce surface flows and intracellular flow within large animal cells. By combining the trace finite element method for surface flows with the aggregated finite element method for bulk flows, the framework enables accurate and stable simulations on fixed Cartesian grids without remeshing. Additionally, it incorporates mechanochemical feedback through the surface transport of a molecular regulator of active tension. The method's accuracy and stability are validated through numerical experiments, showcasing its ability to capture phenomena such as self-organized pattern formation, curvature-driven relaxation, and cell cleavage. This innovative framework provides a versatile tool for studying complex morphogenetic processes in animal cells. 

<br /><br />Summary: <div>
arXiv:2505.05723v1 Announce Type: new 
Abstract: This work presents a novel unfitted finite element framework to simulate coupled surface-bulk problems in time-dependent domains, focusing on fluid-fluid interactions in animal cells between the actomyosin cortex and the cytoplasm. The cortex, a thin layer beneath the plasma membrane, provides structural integrity and drives shape changes by generating surface contractile forces akin to tension. Cortical contractions generate Marangoni-like surface flows and induce intracellular cytoplasmic flows that are essential for processes such as cell division, migration, and polarization, particularly in large animal cells. Despite its importance, the spatiotemporal regulation of cortex-cytoplasm interactions remains poorly understood and computational modelling can be very challenging because surface-bulk dynamics often lead to large cell deformations. To address these challenges, we propose a sharp-interface framework that uniquely combines the trace finite element method for surface flows with the aggregated finite element method for bulk flows. This approach enables accurate and stable simulations on fixed Cartesian grids without remeshing. The model also incorporates mechanochemical feedback through the surface transport of a molecular regulator of active tension. We solve the resulting mixed-dimensional system on a fixed Cartesian grid using a level-set-based method to track the evolving surface. Numerical experiments validate the accuracy and stability of the method, capturing phenomena such as self-organised pattern formation, curvature-driven relaxation, and cell cleavage. This novel framework offers a powerful and extendable tool for investigating increasingly complex morphogenetic processes in animal cells.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading Under Uncertainty: A Distribution-Based Strategy for Futures Markets Using FutureQuant Transformer</title>
<link>https://arxiv.org/abs/2505.05595</link>
<guid>https://arxiv.org/abs/2505.05595</guid>
<content:encoded><![CDATA[
<div> Transformer model, futures trading, price predictions, attention mechanisms, risk management 
Summary:
The FutureQuant Transformer model is introduced to predict future price ranges and volatility in futures trading. Unlike traditional models, it focuses on forecasting ranges and volatility rather than point predictions, providing richer insights for trading strategies. By leveraging attention mechanisms to analyze complex data like real-time Limit Order Books, the model significantly improves decision-making and risk management in trading. Through its ability to learn intricate market patterns, it achieved an average gain of 0.1193% per 30-minute trade, outperforming state-of-the-art models. The model uses factors such as RSI, ATR, and Bollinger Bands in a simple algorithm to make accurate predictions, marking a substantial advancement in predictive analytics for futures trading. 
<br /><br />Summary: <div>
arXiv:2505.05595v1 Announce Type: cross 
Abstract: In the complex landscape of traditional futures trading, where vast data and variables like real-time Limit Order Books (LOB) complicate price predictions, we introduce the FutureQuant Transformer model, leveraging attention mechanisms to navigate these challenges. Unlike conventional models focused on point predictions, the FutureQuant model excels in forecasting the range and volatility of future prices, thus offering richer insights for trading strategies. Its ability to parse and learn from intricate market patterns allows for enhanced decision-making, significantly improving risk management and achieving a notable average gain of 0.1193% per 30-minute trade over state-of-the-art models with a simple algorithm using factors such as RSI, ATR, and Bollinger Bands. This innovation marks a substantial leap forward in predictive analytics within the volatile domain of futures trading.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion</title>
<link>https://arxiv.org/abs/2505.05605</link>
<guid>https://arxiv.org/abs/2505.05605</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, Conversion Prediction, Embedding Tables, Multi-Task Model, Overfitting<br />
<br />
Summary: 
Deep learning models for conversion prediction in online advertising often require complex training to predict multiple objectives. The use of embedding tables encoding high cardinality categorical features can enhance model performance but poses challenges due to gradient sparsity and label sparsity. Pinterest Ads Conversion models utilize Sparse Optimizer to improve convergence speed and address multi-epoch overfitting. The severity of overfitting varies across objectives in a multi-task model based on label sparsity. A new approach using frequency-adaptive learning rates on embedding tables is proposed to mitigate multi-epoch overfitting, compared to re-initialization. Evaluation on a large-scale production dataset demonstrates the effectiveness of these techniques in enhancing model performance without sacrificing accuracy.<br /><br />Summary: <div>
arXiv:2505.05605v1 Announce Type: cross 
Abstract: Deep learning for conversion prediction has found widespread applications in online advertising. These models have become more complex as they are trained to jointly predict multiple objectives such as click, add-to-cart, checkout and other conversion types. Additionally, the capacity and performance of these models can often be increased with the use of embedding tables that encode high cardinality categorical features such as advertiser, user, campaign, and product identifiers (IDs). These embedding tables can be pre-trained, but also learned end-to-end jointly with the model to directly optimize the model objectives. Training these large tables is challenging due to: gradient sparsity, the high cardinality of the categorical features, the non-uniform distribution of IDs and the very high label sparsity. These issues make training prone to both slow convergence and overfitting after the first epoch. Previous works addressed the multi-epoch overfitting issue by using: stronger feature hashing to reduce cardinality, filtering of low frequency IDs, regularization of the embedding tables, re-initialization of the embedding tables after each epoch, etc. Some of these techniques reduce overfitting at the expense of reduced model performance if used too aggressively. In this paper, we share key learnings from the development of embedding table optimization and multi-epoch training in Pinterest Ads Conversion models. We showcase how our Sparse Optimizer speeds up convergence, and how multi-epoch overfitting varies in severity between different objectives in a multi-task model depending on label sparsity. We propose a new approach to deal with multi-epoch overfitting: the use of a frequency-adaptive learning rate on the embedding tables and compare it to embedding re-initialization. We evaluate both methods offline using an industrial large-scale production dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing GPU Energy Usage in Exascale-Ready Portable Science Applications</title>
<link>https://arxiv.org/abs/2505.05623</link>
<guid>https://arxiv.org/abs/2505.05623</guid>
<content:encoded><![CDATA[
<div> quantum Monte Carlo, adaptive mesh, GPU energy usage, exascale-ready applications, mixed-precision

Summary:<br />
The study focuses on analyzing the GPU energy usage of two exascale-ready applications, QMCPACK and AMReX-Castro, representing particle and mesh solvers, respectively. Using NVIDIA's A100 and H100 GPUs and AMD's MI250X GPUs, power, temperature, utilization, and energy traces were examined. The research highlights the energy-saving potential of mixed-precision computations, with savings ranging from 6-25% for QMCPACK and 45% for AMReX-Castro. Challenges remain in the AMD tooling for Frontier GPUs, while NVML query resolutions exhibit little variability. The study underscores the importance of application-specific metrics in informing energy-performance trade-offs and optimizing future supercomputer architectures in the post-Moore era.<br /> 

Summary: <div>
arXiv:2505.05623v1 Announce Type: cross 
Abstract: We characterize the GPU energy usage of two widely adopted exascale-ready applications representing two classes of particle and mesh solvers: (i) QMCPACK, a quantum Monte Carlo package, and (ii) AMReX-Castro, an adaptive mesh astrophysical code. We analyze power, temperature, utilization, and energy traces from double-/single (mixed)-precision benchmarks on NVIDIA's A100 and H100 and AMD's MI250X GPUs using queries in NVML and rocm smi lib, respectively. We explore application-specific metrics to provide insights on energy vs. performance trade-offs. Our results suggest that mixed-precision energy savings range between 6-25% on QMCPACK and 45% on AMReX-Castro. Also there are still gaps in the AMD tooling on Frontier GPUs that need to be understood, while query resolutions on NVML have little variability between 1 ms and 1 s. Overall, application level knowledge is crucial to define energy-cost/science-benefit opportunities for the codesign of future supercomputer architectures in the post-Moore era.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions</title>
<link>https://arxiv.org/abs/2505.05784</link>
<guid>https://arxiv.org/abs/2505.05784</guid>
<content:encoded><![CDATA[
<div> Framework, High-frequency trading, Imitation learning, Flow matching policy, Market conditions

Summary:
FlowHFT is an imitation learning framework designed for high-frequency trading (HFT) that addresses the limitations of traditional HFT approaches. It simultaneously learns strategies from various expert models, allowing for adaptive adjustment of investment decisions based on market conditions. The framework incorporates a grid-search fine-tuning mechanism to refine strategies and outperform expert strategies in complex market scenarios. Through testing in multiple market environments, FlowHFT consistently achieves superior performance compared to individual expert models, showcasing its effectiveness in dynamic and diverse market conditions. The flow matching policy utilized in FlowHFT enables the framework to learn trading strategies under different market scenarios, demonstrating its applicability in stochastic market environments. This innovative approach offers a promising solution for HFT strategies in real-world markets. 

<br /><br />Summary: <div>
arXiv:2505.05784v1 Announce Type: cross 
Abstract: High-frequency trading (HFT) is an investing strategy that continuously monitors market states and places bid and ask orders at millisecond speeds. Traditional HFT approaches fit models with historical data and assume that future market states follow similar patterns. This limits the effectiveness of any single model to the specific conditions it was trained for. Additionally, these models achieve optimal solutions only under specific market conditions, such as assumptions about stock price's stochastic process, stable order flow, and the absence of sudden volatility. Real-world markets, however, are dynamic, diverse, and frequently volatile. To address these challenges, we propose the FlowHFT, a novel imitation learning framework based on flow matching policy. FlowHFT simultaneously learns strategies from numerous expert models, each proficient in particular market scenarios. As a result, our framework can adaptively adjust investment decisions according to the prevailing market state. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism. This allows it to refine strategies and achieve superior performance even in complex or extreme market scenarios where expert strategies may be suboptimal. We test FlowHFT in multiple market environments. We first show that flow matching policy is applicable in stochastic market environments, thus enabling FlowHFT to learn trading strategies under different market conditions. Notably, our single framework consistently achieves performance superior to the best expert for each market condition.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using iterated local alignment to aggregate trajectory data into a traffic flow map</title>
<link>https://arxiv.org/abs/2406.17500</link>
<guid>https://arxiv.org/abs/2406.17500</guid>
<content:encoded><![CDATA[
<div> Keywords: vehicle trajectories, traffic flow maps, measurement noise, local alignment algorithms, spatial resolution

Summary: 
Vehicle trajectories are valuable data for generating traffic flow maps at various scales. However, the presence of measurement noise can make small-scale aggregation challenging. To address this issue, new local alignment algorithms are introduced to align road segments for accurate flow mapping. These algorithms use inferred road segments as reference points and iterate through the process to compute locally aligned flow maps. Testing on synthetic and real-world data shows that the locally aligned maps provide precise flow aggregation at multiple scales, enhancing the accuracy and spatial resolution of static and interactive maps. The innovative approach offers a solution to the noise-related challenges in small-scale flow aggregation, making it possible to generate detailed and reliable traffic flow maps for effective traffic management and planning.<br /><br />Summary: <div>
arXiv:2406.17500v4 Announce Type: replace-cross 
Abstract: Vehicle trajectories, with their detailed geolocations, are a promising data source to compute traffic flow maps at scales ranging from the city/regional level to the road level. The main obstacle is that trajectory data are prone to measurement noise. While this is negligible for city level large-scale flow aggregation, it poses substantial difficulties for road level small-scale aggregation. To overcome these difficulties, we introduce innovative local alignment algorithms, where we infer road segments to serve as local reference segments, and proceed to align nearby road segments to them. We deploy these algorithms in an iterative workflow to compute locally aligned flow maps. By applying this workflow to synthetic and empirical trajectories, we verify that our locally aligned flow maps provide high levels of accuracy and spatial resolution of flow aggregation at multiple scales for static and interactive maps.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed solution reconstruction in elasticity and heat transfer using the explicit constraint force method</title>
<link>https://arxiv.org/abs/2505.04875</link>
<guid>https://arxiv.org/abs/2505.04875</guid>
<content:encoded><![CDATA[
<div> Keywords: physics-informed neural networks, solution reconstruction, interpretability, robustness, explicit constraint force method

<br /><br />Summary: This article addresses the challenges faced by physics-informed neural networks (PINNs) in solution reconstruction, which estimates the complete state of a physical system from sparse measurements. A key issue identified is that the governing equations used may not align with the actual physical phenomena, leading to potential failures in fulfilling the criteria of interpretability, robustness, and data consistency. These criteria are essential for assessing reconstruction quality, ensuring that results aren’t overly dependent on the choice of physics loss, and enabling the unique recovery of physics parameters. The study demonstrates that conventional physics loss formulations create varying "constraint forces," which are additional source terms that influence the reconstructed solution. To mitigate these issues, the authors propose an "explicit constraint force method" (ECFM) that provides better control over the introduced source terms. By adhering to the outlined criteria, the ECFM allows for more reliable and customizable reconstructions from noisy data, even when the physics parameterization does not match the measured system accurately. This approach enhances the performance and interpretability of PINNs in practical applications. <div>
arXiv:2505.04875v1 Announce Type: new 
Abstract: One use case of ``physics-informed neural networks'' (PINNs) is solution reconstruction, which aims to estimate the full-field state of a physical system from sparse measurements. Parameterized governing equations of the system are used in tandem with the measurements to regularize the regression problem. However, in real-world solution reconstruction problems, the parameterized governing equation may be inconsistent with the physical phenomena that give rise to the measurement data. We show that due to assuming consistency between the true and parameterized physics, PINNs-based approaches may fail to satisfy three basic criteria of interpretability, robustness, and data consistency. As we argue, these criteria ensure that (i) the quality of the reconstruction can be assessed, (ii) the reconstruction does not depend strongly on the choice of physics loss, and (iii) that in certain situations, the physics parameters can be uniquely recovered. In the context of elasticity and heat transfer, we demonstrate how standard formulations of the physics loss and techniques for constraining the solution to respect the measurement data lead to different ``constraint forces" -- which we define as additional source terms arising from the constraints -- and that these constraint forces can significantly influence the reconstructed solution. To avoid the potentially substantial influence of the choice of physics loss and method of constraint enforcement on the reconstructed solution, we propose the ``explicit constraint force method'' (ECFM) to gain control of the source term introduced by the constraint. We then show that by satisfying the criteria of interpretability, robustness, and data consistency, this approach leads to more predictable and customizable reconstructions from noisy measurement data, even when the parameterization of the missing physics is inconsistent with the measured system.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermoelastic Kirchhoff Plate: A Novel Model for Shot Peen Forming Metal Panels</title>
<link>https://arxiv.org/abs/2505.05236</link>
<guid>https://arxiv.org/abs/2505.05236</guid>
<content:encoded><![CDATA[
<div> Keywords: shot peen forming, torque, bending equations, Kirchhoff plate, Monte Carlo methods  

<br /><br />Summary: The study explores shot peen forming, a technique where high-velocity steel pellets impact aluminum panels, causing localized plastic deformation. This process enhances the fatigue properties of the material and introduces a residual stress distribution that results in bending, which can be interpreted as the application of spatially varying torques. The authors develop bending equations for a thermally loaded homogeneous Kirchhoff plate to predict the effects of shot peen forming. A novel test method is introduced to extract an equivalent applied torque from the bending response of uniformly shot peened plates, simplifying the accounting for surface plasticity. This extracted torque serves as an input for a model predicting the formation of rectangular plates under varied shot peen conditions. An experimental design is created and executed to assess the model’s validity against actual shot peen operations. Additionally, uncertainty within the experimental results is quantified using Monte Carlo methods, providing insights into the reliability of the findings and the effectiveness of the proposed modeling approach in capturing the influence of shot peening on panel deformation. <div>
arXiv:2505.05236v1 Announce Type: new 
Abstract: A common technique used in factories to shape metal panels is shot peen forming, where the panel is sprayed with a high-velocity stream of small steel pellets called shot. The impacts between the hard steel shot and softer aluminum panel cause localized plastic deformation, both improving the fatigue properties of the material's surface and imparting a residual stress distribution that results in bending. Thus, a torque is associated with the through-thickness shot peen stress distribution. We conceptualize shot peen forming as the application of spatially varying torques, which are modeled with the input of applied temperatures. In this paper, we derive the bending equations for a thermally loaded homogeneous Kirchhoff plate in order to predict the effects of shot peen forming. A simple test is devised to extract the value of an equivalent applied torque from the bending response of uniformly shot peened plates, which circumvents the difficulty of accounting for surface plasticity. This torque can be used as an input to a model which predicts the shape of rectangular plates under more complicated shot peen conditions. An experiment is designed and carried out which investigates the agreement between the model and real shot peen operations. The effect of uncertainty in the experiment is estimated with Monte Carlo methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Stock Market Prediction Using Long Short-Term Memory Networks: A Comprehensive Deep Learning Framework</title>
<link>https://arxiv.org/abs/2505.05325</link>
<guid>https://arxiv.org/abs/2505.05325</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market, LSTM, predict, sentiment analysis, web application

<br /><br />Summary: 
This paper tackles the challenge of predicting stock market movements, highlighting the volatility and complexity of financial time series data. The study employs a deep learning framework using Long Short-Term Memory (LSTM) networks to forecast closing stock prices for leading technology companies: Apple, Google, Microsoft, and Amazon, all listed on NASDAQ. Historical stock data was sourced from Yahoo Finance and then processed through normalization and feature engineering techniques to enhance model performance. The proposed LSTM model achieved a Mean Absolute Percentage Error (MAPE) of 2.72 on unseen test data, showing a considerable improvement over traditional models like ARIMA. To further refine predictive accuracy, the researchers incorporated sentiment scores derived from real-time news articles and social media, analyzed with the VADER sentiment analysis tool. Additionally, a web application was developed to provide real-time visualizations of stock price forecasts, making the model's insights accessible to both individual and institutional investors. This research illustrates the effectiveness of LSTM networks in capturing complex financial sequences and presents a novel hybrid methodology that integrates time series forecasting with sentiment analysis for enhanced predictive capability. <div>
arXiv:2505.05325v1 Announce Type: new 
Abstract: Predicting stock market movements remains a persistent challenge due to the inherently volatile, non-linear, and stochastic nature of financial time series data. This paper introduces a deep learning-based framework employing Long Short-Term Memory (LSTM) networks to forecast the closing stock prices of major technology firms: Apple, Google, Microsoft, and Amazon, listed on NASDAQ. Historical data was sourced from Yahoo Finance and processed using normalization and feature engineering techniques. The proposed model achieves a Mean Absolute Percentage Error (MAPE) of 2.72 on unseen test data, significantly outperforming traditional models like ARIMA. To further enhance predictive accuracy, sentiment scores were integrated using real-time news articles and social media data, analyzed through the VADER sentiment analysis tool. A web application was also developed to provide real-time visualizations of stock price forecasts, offering practical utility for both individual and institutional investors. This research demonstrates the strength of LSTM networks in modeling complex financial sequences and presents a novel hybrid approach combining time series modeling with sentiment analysis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatMMFuse: Multi-Modal Fusion model for Material Property Prediction</title>
<link>https://arxiv.org/abs/2505.04634</link>
<guid>https://arxiv.org/abs/2505.04634</guid>
<content:encoded><![CDATA[
<div> Keywords: graph-based encoding, multi-modal fusion, Crystal Graph Convolution Network, SciBERT, zero-shot performance

<br /><br />Summary: The paper presents Material Multi-Modal Fusion (MatMMFuse), a novel model that combines graph-based and text-based representations for enhanced material property prediction. By integrating local feature learning from the Crystal Graph Convolution Network (CGCNN) and global information from the SciBERT model, MatMMFuse utilizes a multi-head attention mechanism for effective fusion. The model is trained end-to-end using data from the Materials Project Dataset and demonstrates significant improvements over the individual CGCNN and SciBERT models across key properties: formation energy, band gap, energy above hull, and Fermi energy. Notably, the model shows a 40% enhancement compared to the CGCNN and a 68% improvement over the SciBERT model for predicting formation energy per atom. Furthermore, MatMMFuse exhibits impressive zero-shot performance on specialized datasets, including Perovskites, Chalcogenides, and the Jarvis Dataset, outperforming the standalone CGCNN and SciBERT models. This capability allows researchers to apply the model in industrial settings where obtaining training data can be costly. <div>
arXiv:2505.04634v1 Announce Type: cross 
Abstract: The recent progress of using graph based encoding of crystal structures for high throughput material property prediction has been quite successful. However, using a single modality model prevents us from exploiting the advantages of an enhanced features space by combining different representations. Specifically, pre-trained Large language models(LLMs) can encode a large amount of knowledge which is beneficial for training of models. Moreover, the graph encoder is able to learn the local features while the text encoder is able to learn global information such as space group and crystal symmetry. In this work, we propose Material Multi-Modal Fusion(MatMMFuse), a fusion based model which uses a multi-head attention mechanism for the combination of structure aware embedding from the Crystal Graph Convolution Network (CGCNN) and text embeddings from the SciBERT model. We train our model in an end-to-end framework using data from the Materials Project Dataset. We show that our proposed model shows an improvement compared to the vanilla CGCNN and SciBERT model for all four key properties: formation energy, band gap, energy above hull and fermi energy. Specifically, we observe an improvement of 40% compared to the vanilla CGCNN model and 68% compared to the SciBERT model for predicting the formation energy per atom. Importantly, we demonstrate the zero shot performance of the trained model on small curated datasets of Perovskites, Chalcogenides and the Jarvis Dataset. The results show that the proposed model exhibits better zero shot performance than the individual plain vanilla CGCNN and SciBERT model. This enables researchers to deploy the model for specialized industrial applications where collection of training data is prohibitively expensive.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
<link>https://arxiv.org/abs/2505.04846</link>
<guid>https://arxiv.org/abs/2505.04846</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Augmented Generation, HiPerRAG, high performance computing, multimodal document parsing, scientific question answering

<br /><br />Summary: The exponential growth of scientific literature has led to challenges such as underutilized discoveries and limited interdisciplinary collaboration. Retrieval Augmented Generation (RAG) has the potential to enhance the factuality of Large Language Models (LLMs) by effectively processing vast amounts of information. However, scaling RAG to manage millions of articles presents significant hurdles, including high computational costs and complex algorithmic requirements for aligning nuanced scientific content. To tackle these issues, the authors introduce HiPerRAG, a RAG workflow leveraging high-performance computing (HPC) to index and retrieve knowledge from over 3.6 million scientific articles. HiPerRAG incorporates Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy using contrastive learning and late-interaction techniques. The system demonstrates robust performance on existing scientific question answering benchmarks, achieving 90% accuracy on SciQ and 76% on PubMedQA, surpassing both specialized models like PubMedGPT and commercial LLMs such as GPT-4. By utilizing thousands of GPUs on advanced supercomputers, HiPerRAG facilitates million document-scale RAG workflows, promoting the unity of scientific knowledge and fostering interdisciplinary innovation. <div>
arXiv:2505.04846v1 Announce Type: cross 
Abstract: The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Direct-adjoint Approach for Material Point Model Calibration with Application to Plasticity</title>
<link>https://arxiv.org/abs/2501.04584</link>
<guid>https://arxiv.org/abs/2501.04584</guid>
<content:encoded><![CDATA[
<div> Keywords: elastoplastic, calibration, optimization, Hessian, automatic differentiation  

<br /><br />Summary: This paper introduces a novel method for calibrating material parameters in local elastoplastic constitutive models by framing the calibration as a constrained optimization problem. The evolution equations of the constitutive model at a single material point serve as constraints for this optimization. The aim is to minimize the objective function that measures the discrepancy between predicted stress outcomes from the model and actual experimental data. To enhance the calibration process's efficiency, a new direct-adjoint approach is employed to compute the Hessian of the objective function, which is crucial for implementing second-order optimization techniques. Additionally, automatic differentiation is utilized for calculating both gradient and Hessian values accurately. The paper includes two numerical examples to validate the accuracy of the Hessian matrices. The results demonstrate that the Newton-Raphson algorithm significantly outperforms first-order gradient-based methods such as L-BFGS-B in terms of efficiency and effectiveness, confirming the advantages of the proposed calibration approach for elastoplastic models. <div>
arXiv:2501.04584v2 Announce Type: replace 
Abstract: This paper proposes a new approach for the calibration of material parameters in local elastoplastic constitutive models. The calibration is posed as a constrained optimization problem, where the constitutive model evolution equations for a single material point serve as constraints. The objective function quantifies the mismatch between the stress predicted by the model and corresponding experimental measurements. To improve calibration efficiency, a novel direct-adjoint approach is presented to compute the Hessian of the objective function, which enables the use of second-order optimization algorithms. Automatic differentiation is used for gradient and Hessian computations. Two numerical examples are employed to validate the Hessian matrices and to demonstrate that the Newton-Raphson algorithm consistently outperforms gradient-based algorithms such as L-BFGS-B.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact nonlinear state estimation</title>
<link>https://arxiv.org/abs/2310.10976</link>
<guid>https://arxiv.org/abs/2310.10976</guid>
<content:encoded><![CDATA[
<div> Keywords: data assimilation, generative AI, Conjugate Transform Filter, nonlinear estimation, Earth system models  

<br /><br />Summary: The article discusses the limitations of traditional data assimilation (DA) methods in geosciences, which often rely on Gaussian assumptions that can lead to analysis biases and poor forecasts. To address these challenges, the authors introduce a novel nonlinear estimation theory inspired by advancements in generative artificial intelligence (AI). They present the Conjugate Transform Filter (CTF), which generalizes the Kalman filter to accommodate non-Gaussian distributions. This new filter is designed to maintain statistical relationships in prior states and effectively converge to accurate observations. Additionally, the article introduces an ensemble approximation of the CTF, termed the Ensemble Conjugate Transform Filter (ECTF), which is validated through idealized statistical experiments involving bounded quantities with non-Gaussian distributions—common in Earth system models. The findings indicate that ECTF performs optimally when observation errors are minor compared to forecast uncertainties and when there are strong nonlinear dependencies among state variables. Ultimately, this new filtering theory presents promising pathways for enhancing conventional DA methods by integrating them with principles from AI, paving the way for improved accuracy in data assimilation tasks in geosciences. <div>
arXiv:2310.10976v2 Announce Type: replace-cross 
Abstract: The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and validated using idealized statistical experiments that feature bounded quantities with non-Gaussian distributions, a prevalent challenge in Earth system models. Results from these experiments indicate that the greatest benefits from ECTF occur when observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Ultimately, the new filtering theory offers exciting avenues for improving conventional DA algorithms through their principled integration with AI techniques.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-device Anomaly Detection in Conveyor Belt Operations</title>
<link>https://arxiv.org/abs/2411.10729</link>
<guid>https://arxiv.org/abs/2411.10729</guid>
<content:encoded><![CDATA[
<div> Keywords: conveyor belts, anomaly detection, pattern recognition, machine learning, mining operations  

<br /><br />Summary: Conveyor belts play a vital role in mining operations by facilitating the efficient movement of bulk materials, thereby enhancing productivity. While previous studies have focused on detecting anomalies in specific conveyor components, understanding root causes such as production changes and operator errors is essential. Continuous monitoring of conveyor belt work cycles remains nascent, requiring robust solutions. This study introduces two novel methods for classifying normal and abnormal duty cycles, based on a recently proposed anomaly detection technique. The methods utilize threshold-based detection, manually extracted features, pattern-matching, and supervised tiny machine learning models, including decision tree and random forest, among others. A comprehensive evaluation demonstrates that both proposed methods outperform the existing technique on two datasets. The heuristic rule-based method excels on training dataset with performance metrics of 97.3% for normal and 80.2% for abnormal cycles, while the ML-based approach scores 91.3% and 67.9% respectively on a dataset influenced by machine aging. Implemented on low-power microcontrollers, the methods achieve real-time operation with energy consumption of just 13.3 and 20.6 μJ during inference, presenting a significant advancement in conveyor belt monitoring solutions. <div>
arXiv:2411.10729v2 Announce Type: replace-cross 
Abstract: Conveyor belts are crucial in mining operations by enabling the continuous and efficient movement of bulk materials over long distances, which directly impacts productivity. While detecting anomalies in specific conveyor belt components has been widely studied, identifying the root causes of these failures, such as changing production conditions and operator errors, remains critical. Continuous monitoring of mining conveyor belt work cycles is still at an early stage and requires robust solutions. Recently, an anomaly detection method for duty cycle operations of a mining conveyor belt has been proposed. Based on its limited performance and unevaluated long-term proper operation, this study proposes two novel methods for classifying normal and abnormal duty cycles. The proposed approaches are pattern recognition systems that make use of threshold-based duty-cycle detection mechanisms, manually extracted features, pattern-matching, and supervised tiny machine learning models. The explored low-computational models include decision tree, random forest, extra trees, extreme gradient boosting, Gaussian naive Bayes, and multi-layer perceptron. A comprehensive evaluation of the former and proposed approaches is carried out on two datasets. Both proposed methods outperform the former method, with the best-performing approach being dataset-dependent. The heuristic rule-based approach achieves the highest performance in the same dataset used for algorithm training, with 97.3% for normal cycles and 80.2% for abnormal cycles. The ML-based approach performs better on a dataset including the effects of machine aging, scoring 91.3% for normal cycles and 67.9% for abnormal cycles. Implemented on two low-power microcontrollers, the methods demonstrate efficient, real-time operation with energy consumption of 13.3 and 20.6 ${\mu}$J during inference. These results offer valuable insights for detecting ...
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers</title>
<link>https://arxiv.org/abs/2505.04018</link>
<guid>https://arxiv.org/abs/2505.04018</guid>
<content:encoded><![CDATA[
<div> deep learning, graph neural networks, transformers, modal identification, structural health monitoring

Summary: 
This study introduces a novel deep learning framework that combines graph neural networks, transformers, and a physics-informed loss function for modal identification in a population of structures. The transformer module breaks down multiple degrees-of-freedom measurements into single-degree-of-freedom modal responses, aiding in the identification of natural frequencies and damping ratios. Concurrently, the graph neural network captures structural configurations and identifies mode shapes corresponding to the decomposed modal responses. The model is trained in an unsupervised manner, utilizing modal decomposition theory and the independence of structural modes for guidance without labeled data. Validation through simulations and experiments shows its ability to accurately decompose dynamic responses and identify modal properties from sparse measurements, even with external load and structural variations. Comparative analyses against existing techniques highlight its superior performance, making it an attractive option for population-based structural health monitoring. 

<br /><br />Summary: <div>
arXiv:2505.04018v1 Announce Type: new 
Abstract: Modal identification is crucial for structural health monitoring and structural control, providing critical insights into structural dynamics and performance. This study presents a novel deep learning framework that integrates graph neural networks (GNNs), transformers, and a physics-informed loss function to achieve modal decomposition and identification across a population of structures. The transformer module decomposes multi-degrees-of-freedom (MDOF) structural dynamic measurements into single-degree-of-freedom (SDOF) modal responses, facilitating the identification of natural frequencies and damping ratios. Concurrently, the GNN captures the structural configurations and identifies mode shapes corresponding to the decomposed SDOF modal responses. The proposed model is trained in a purely physics-informed and unsupervised manner, leveraging modal decomposition theory and the independence of structural modes to guide learning without the need for labeled data. Validation through numerical simulations and laboratory experiments demonstrates its effectiveness in accurately decomposing dynamic responses and identifying modal properties from sparse structural dynamic measurements, regardless of variations in external loads or structural configurations. Comparative analyses against established modal identification techniques and model variations further underscore its superior performance, positioning it as a favorable approach for population-based structural health monitoring.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yield and Buckling Stress Limits in Topology Optimization of Multiscale Structures</title>
<link>https://arxiv.org/abs/2505.04353</link>
<guid>https://arxiv.org/abs/2505.04353</guid>
<content:encoded><![CDATA[
<div> Topology optimization, yield stress, local buckling, global buckling, multiscale analysis
<br />
Summary: 
This study introduces an extension of multiscale topology optimization by incorporating yield stress and local/global buckling considerations into the design process. The new framework integrates yield stress limits as constraints or objectives alongside existing buckling constraints, refining the optimization process to meet mechanical performance criteria and material yield constraints. Local density-dependent yield surfaces are established based on local yield estimates, then combined with buckling criteria to obtain topology optimized designs considering yield and buckling failure. This integration is crucial for ensuring structural integrity and durability in real-world scenarios. Numerical examples show that optimized designs are influenced by the stiffness to yield ratio of the building material. Despite the assumption of scale separation, de-homogenized structures closely match homogenized predictions even at coarse length scales. 
<br /> <div>
arXiv:2505.04353v1 Announce Type: new 
Abstract: This study presents an extension of multiscale topology optimization by integrating both yield stress and local/global buckling considerations into the design process. Building upon established multiscale methodologies, we develop a new framework incorporating yield stress limits either as constraints or objectives alongside previously established local and global buckling constraints. This approach significantly refines the optimization process, ensuring that the resulting designs meet mechanical performance criteria and adhere to critical material yield constraints. First, we establish local density-dependent von Mises yield surfaces based on local yield estimates from homogenization-based analysis to predict the local yield limits of the homogenized materials. Then, these local Yield-based Load Factors (YLFs) are combined with local and global buckling criteria to obtain topology optimized designs that consider yield and buckling failure on all levels. This integration is crucial for the practical application of optimized structures in real-world scenarios, where material yield and stability behavior critically influence structural integrity and durability. Numerical examples demonstrate how optimized designs depend on the stiffness to yield ratio of the considered building material. Despite the foundational assumption of separation of scales, the de-homogenized structures, even at relatively coarse length scales, exhibit a high degree of agreement with the corresponding homogenized predictions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery Scheme in Mobile Crowdsensing</title>
<link>https://arxiv.org/abs/2505.04361</link>
<guid>https://arxiv.org/abs/2505.04361</guid>
<content:encoded><![CDATA[
<div> Privacy-Preserving Truth Discovery Mobile Crowdsensing Reputation Data Quality<br />
<br />
Summary: The article introduces a Reputation and Data Privacy-Preserving-based Truth Discovery (RDPP-TD) scheme to enhance data quality in mobile crowdsensing (MCS). Existing truth discovery methods often result in low data quality as they only consider current-round data. The proposed RDPP-TD scheme integrates Reputation-based Truth Discovery (RTD) with a Reputation and Data Privacy-Preserving (RDPP) approach to estimate truth more accurately and ensure privacy protection for worker data and reputation values. By combining RTD with RDPP, the scheme evaluates worker reliability in a privacy-preserving manner and supports reputation-based worker recruitment and rewards. The comprehensive theoretical analysis and experiments show that RDPP-TD improves data quality by up to 33.3% while providing strong privacy protection in MCS. <div>
arXiv:2505.04361v1 Announce Type: new 
Abstract: Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS). However, existing TD methods, including privacy-preserving TD approaches, estimate the truth by weighting only the data submitted in the current round, which often results in low data quality. Moreover, there is a lack of effective TD methods that preserve both reputation and data privacy. To address these issues, a Reputation and Data Privacy-Preserving based Truth Discovery (RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD) approach, which integrates the weight of current-round data with workers' reputation values to estimate the truth, thereby achieving more accurate results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which ensures privacy preservation for sensing data and reputation values. First, the RDPP approach, when seamlessly integrated with RTD, can effectively evaluate the reliability of workers and their sensing data in a privacy-preserving manner. Second, the RDPP scheme supports reputation-based worker recruitment and rewards, ensuring high-quality data collection while incentivizing workers to provide accurate information. Comprehensive theoretical analysis and extensive experiments based on real-world datasets demonstrate that the proposed RDPP-TD scheme provides strong privacy protection and improves data quality by up to 33.3%.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Execution Welfare Across Solver-based DEXes</title>
<link>https://arxiv.org/abs/2503.00738</link>
<guid>https://arxiv.org/abs/2503.00738</guid>
<content:encoded><![CDATA[
<div> auctions, decentralized exchanges, solver-based protocols, execution welfare, liquidity profile

Summary:
Solver-based protocols in decentralized exchanges have shown to improve execution welfare for end users compared to traditional routing methods like Uniswap V2 or V3. The study analyzed data for different asset pairs (USDC-WETH and PEPE-WETH) and found that solver-based platforms such as CoWSwap, 1inchFusion, and UniswapX, offer varying levels of execution welfare. While USDC-WETH, a short-tail asset, benefited significantly from solver-based trading, the impact was less pronounced for PEPE-WETH, a long-tail asset. The study also highlighted potential inefficiencies in solver market structure, liquidity profiles, and competition dynamics among solvers. These insights underscore the advantages of solver-based protocols in improving execution outcomes but also raise concerns about market concentration and competition dynamics in decentralized exchanges. 

<br /><br />Summary: <div>
arXiv:2503.00738v3 Announce Type: replace 
Abstract: Decentralized exchanges (DEXes) have evolved dramatically since the introduction of Automated Market Makers (AMMs). In recent years, solver-based protocols have emerged as an alternative venue aiming to introduce competition for routing, access to offchain liquidity, and thereby improve end-user execution. Currently, these solver auctions are hosted on opaque backends, and the extent of price improvement they provide to end users remains unclear.
  We conduct an empirical study of the execution welfare that these protocols bring to users by analyzing data across different asset profiles (USDC-WETH and PEPE-WETH). Our results indicate that, compared to vanilla routing through Uniswap V2 or V3, solver-based protocols effectively enhance execution welfare for end users on DEXes within certain trade size ranges. This effect is most pronounced with USDC-WETH, a short-tail asset, and somewhat less significant with PEPE-WETH, a long-tail asset.
  Additionally, we identify execution welfare discrepancies across solver-based platforms (e.g., CoWSwap, 1inchFusion, UniswapX), revealing potential inefficiencies due to solver market structure, variations in liquidity profile and inventory depth among solvers. These insights highlight both the advantages and challenges of solver-based trading, underscoring its role in improving execution outcomes while raising concerns about market concentration and competition dynamics.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto.gov: Learning-based Governance for Decentralized Finance (DeFi)</title>
<link>https://arxiv.org/abs/2302.09551</link>
<guid>https://arxiv.org/abs/2302.09551</guid>
<content:encoded><![CDATA[
<div> Keywords: DeFi, governance, Auto$.$gov, deep Q-network, reinforcement learning<br />
<br />
Summary: 
Auto$.$gov is a novel governance framework for decentralized finance (DeFi) that utilizes deep Q-network reinforcement learning to automate parameter adjustments. Traditional DeFi governance methods are manual and prone to human bias and financial risks. Auto$.$gov addresses these issues by using RL to make data-driven decisions and adapt to market conditions. In simulated tests based on the Aave lending protocol, Auto$.$gov successfully prevented funds loss from price oracle attacks. Real-world tests showed that Auto$.$gov outperformed benchmark approaches by 14% and the static baseline model by tenfold in terms of protocol profitability. This innovative governance model enhances the security, profitability, and sustainability of DeFi protocols, offering a more efficient and effective alternative to traditional governance methods.<br /> 
Summary: <div>
arXiv:2302.09551v4 Announce Type: replace-cross 
Abstract: Decentralized finance (DeFi) is an integral component of the blockchain ecosystem, enabling a range of financial activities through smart-contract-based protocols. Traditional DeFi governance typically involves manual parameter adjustments by protocol teams or token holder votes, and is thus prone to human bias and financial risks, undermining the system's integrity and security. While existing efforts aim to establish more adaptive parameter adjustment schemes, there remains a need for a governance model that is both more efficient and resilient to significant market manipulations. In this paper, we introduce "Auto$.$gov", a learning-based governance framework that employs a deep Qnetwork (DQN) reinforcement learning (RL) strategy to perform semi-automated, data-driven parameter adjustments. We create a DeFi environment with an encoded action-state space akin to the Aave lending protocol for simulation and testing purposes, where Auto$.$gov has demonstrated the capability to retain funds that would have otherwise been lost to price oracle attacks. In tests with real-world data, Auto$.$gov outperforms the benchmark approaches by at least 14% and the static baseline model by tenfold, in terms of the preset performance metric--protocol profitability. Overall, the comprehensive evaluations confirm that Auto$.$gov is more efficient and effective than traditional governance methods, thereby enhancing the security, profitability, and ultimately, the sustainability of DeFi protocols.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Parallel Simulation of Malignant Pleural Mesothelioma via Adaptive Domain Partitioning -- an Efficiency Analysis Study</title>
<link>https://arxiv.org/abs/2505.03067</link>
<guid>https://arxiv.org/abs/2505.03067</guid>
<content:encoded><![CDATA[
<div> framework, Malignant Pleural Mesothelioma, tumour growth, parallel efficiency analysis, Computational resources optimization

Summary:<br />
The article introduces a novel framework for simulating the growth of Malignant Pleural Mesothelioma (MPM) tumors using a Cellular Potts Model (CPM) coupled with partial differential equations (PDEs). A dynamic bounding box is applied to the simulation domain, based on CT scan data, to reduce memory and CPU overhead. This adaptive partitioning allows for efficient use of computational resources by reducing the 3D domain over which the PDEs are solved. The PDEs, representing oxygen, nutrients, and cytokines, are solved using the finite-volume method with a first-order implicit Euler scheme. Parallelization is achieved using mpi4py library, LinearGMRESSolver, and PETSc for efficient convergence. Parallel computation results in reduced solving time compared to serial computation, with optimizations enhancing memory usage and load balancing among cores. <div>
arXiv:2505.03067v1 Announce Type: new 
Abstract: A novel parallel efficiency analysis on a framework for simulating the growth of Malignant Pleural Mesothelioma (MPM) tumours is presented. Proliferation of MPM tumours in the pleural space is simulated using a Cellular Potts Model (CPM) coupled with partial differential equations (PDEs). Using segmented lung data from CT scans, an environment is set up with artificial tumour data in the pleural space, representing the simulation domain, onto which a dynamic bounding box is applied to restrict computations to the region of interest, dramatically reducing memory and CPU overhead. This adaptive partitioning of the domain enables efficient use of computational resources by reducing the three-dimensional (3D) domain over which the PDEs are to be solved. The PDEs, representing oxygen, nutrients, and cytokines, are solved using the finite-volume method with a first-order implicit Euler scheme. Parallelization is realized using the public Python library mpi4py in combination with LinearGMRESSolver and PETSc for efficient convergence. Performance analyses have shown that parallelization achieves a reduced solving time compared to serial computation. Also, optimizations enable efficient use of available memory and improved load balancing amongst the cores.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Applied to Short-term Solar PV Power Output Forecasting</title>
<link>https://arxiv.org/abs/2505.03188</link>
<guid>https://arxiv.org/abs/2505.03188</guid>
<content:encoded><![CDATA[
<div> Convolutional Neural Networks, Solar Photovoltaic Power Output, Transformer Architecture, Nowcast Model, Forecast Model
Summary:
The study focuses on predicting and forecasting solar PV power output using a transformer architecture and fully-connected layer. The research addresses the challenge of uncertainty in solar PV output due to weather conditions like cloud cover, which can vary over short and long timescales. By utilizing one year of image data and experimenting with different learning rates and batch sizes, the transformer architecture shows promising results in predicting PV output. However, it performs less effectively on days with clear skies compared to the baseline model. Reliable forecasts of renewable energy generation are vital for electricity market balance and supply reliability. This research contributes to improving the accuracy of solar PV power output predictions, particularly in the context of fluctuating weather conditions.<br /><br />Summary: <div>
arXiv:2505.03188v1 Announce Type: new 
Abstract: Reliable forecasts of the power output from variable renewable energy generators like solar photovoltaic systems are important to balancing load on real-time electricity markets and ensuring electricity supply reliability. However, solar PV power output is highly uncertain, with significant variations occurring over both longer (daily or seasonally) and shorter (within minutes) timescales due to weather conditions, especially cloud cover. This paper builds on existing work that uses convolutional neural networks in the computer vision task of predicting (in a Nowcast model) and forecasting (in a Forecast model) solar PV power output (Stanford EAO SUNSET Model). A pure transformer architecture followed by a fully-connected layer is applied to one year of image data with experiments run on various combinations of learning rate and batch size. We find that the transformer architecture performs almost as well as the baseline model in the PV output prediction task. However, it performs worse on sunny days.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-efficient inverse design of spinodoid metamaterials</title>
<link>https://arxiv.org/abs/2505.03415</link>
<guid>https://arxiv.org/abs/2505.03415</guid>
<content:encoded><![CDATA[
<div> surrogate model, spinodoid metamaterials, structure-property linkages, data-efficient, multi-objective inverse design
<br />
Summary:
This study introduces a data-efficient and accurate surrogate model for spinodoid metamaterials' structure-property linkages, using only 75 data points, a substantial reduction compared to previous works. By employing a neural network-based surrogate model that satisfies specific requirements such as equivariance with respect to permutations of structure parameters, the research team successfully creates a differentiable surrogate of the forward model. This model enables gradient-based optimization for inverse design problems, demonstrating reliable results in various complex tasks. The data efficiency achieved in this study opens up possibilities for inverse design applications involving nonlinear mechanical behavior, where the limitation of data availability has been a challenge. <div>
arXiv:2505.03415v1 Announce Type: new 
Abstract: We create an data-efficient and accurate surrogate model for structure-property linkages of spinodoid metamaterials with only 75 data points -- far fewer than the several thousands used in prior works -- and demonstrate its use in multi-objective inverse design. The inverse problem of finding a material microstructure that leads to given bulk properties is of great interest in mechanics and materials science. These inverse design tasks often require a large dataset, which can become unaffordable when considering material behavior that requires more expensive simulations or experiments. We generate a data-efficient surrogate for the mapping between the characteristics of the local material structure and the effective elasticity tensor and use it to inversely design structures with multiple objectives simultaneously. The presented neural network-based surrogate model achieves its data efficiency by inherently satisfying certain requirements, such as equivariance with respect to permutations of structure parameters, which avoids having to learn them from data. The resulting surrogate of the forward model is differentiable, allowing its direct use in gradient-based optimization for the inverse design problem. We demonstrate in three inverse design tasks of varying complexity that this approach yields reliable results while requiring significantly less training data than previous approaches based on neural-network surrogates. This paves the way for inverse design involving nonlinear mechanical behavior, where data efficiency is currently the limiting factor.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithm Selection in Short-Range Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2505.03438</link>
<guid>https://arxiv.org/abs/2505.03438</guid>
<content:encoded><![CDATA[
<div> algorithm selection, molecular dynamics simulations, performance prediction, fuzzy logic, random forest

Summary:
Three algorithm selection strategies for Molecular Dynamics simulations were investigated in this work: performance prediction using past data, fuzzy logic-based expert knowledge approach, and data-driven random forest approach. These strategies achieved speedups of up to 4.05 compared to previous methods and 1.25 compared to a static algorithm configuration selection. The study demonstrated the effectiveness of dynamic algorithm selection in improving simulation performance. The practicality of the strategies was also discussed in relation to their performance, emphasizing the feasibility of implementing such solutions in real-world scenarios. Overall, the research showcased the benefits of dynamic algorithm selection in enhancing the efficiency of particle simulations, offering significant speed improvements while maintaining practicality and ease of implementation. 

<br /><br />Summary: <div>
arXiv:2505.03438v1 Announce Type: new 
Abstract: Numerous algorithms and parallelisations have been developed for short-range particle simulations; however, none are optimally performant for all scenarios. Such a concept led to the prior development of the particle simulation library AutoPas, which implemented many of these algorithms and parallelisations and could select and tune these over the course of the simulation as the scenario changed. Prior works have, however, used only naive approaches to the algorithm selection problem, which can lead to significant overhead from trialling poorly performing algorithmic configurations.
  In this work, we investigate this problem in the case of Molecular Dynamics simulations. We present three algorithm selection strategies: an approach which makes performance predictions from past data, an expert-knowledge fuzzy logic-based approach, and a data-driven random forest-based approach. We demonstrate that these approaches can achieve speedups of up to 4.05 compared to prior approaches and 1.25 compared to a perfect configuration selection without dynamic algorithm selection. In addition, we discuss the practicality of the strategies in comparison to their performance, to highlight the tractability of such solutions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation-Based Equations for Propagation Constant in Uniform or Periodic Transmission</title>
<link>https://arxiv.org/abs/2401.06165</link>
<guid>https://arxiv.org/abs/2401.06165</guid>
<content:encoded><![CDATA[
<div> simulation-based equations, propagation constant, uniform structures, periodic structures, FPPS<br />
<br />
Summary: The article presents simulation-based equations for calculating propagation constants in uniform or periodic structures using a Field Propagation Parameter Splitter (FPPS) model. The FPPS model is based on field distributions obtained from a driven-mode solver, allowing for the separation of forward and backward waves within structures. The FPPS is tested on various structures including waveguides, closed structures, and open radiation structures, showing its ease of use and adaptability compared to other methods. The model's effectiveness is verified through comparisons with eigenmode solvers, equivalent network methods, and spectral domain integral equation methods. The FPPS can also be applied to open radiating structures and multi-dimensional periodic/uniform structures. <div>
arXiv:2401.06165v2 Announce Type: replace 
Abstract: In this work, simulation-based equations to calculate propagation constant in uniform or periodic structures (SES) are deduced and verified through simulations in various types of structures. The modeling of those structures are essentially based on field distributions from a driven-mode solver, and the field distributions are used as the input parameters of the FPPS. It allows the separation of forward and backward waves from a total wave inside such a uniform or periodic structure, and thus it can be used to calculate the propagation constants inside both uniform and periodic structures even with a strong reflection. In order to test the performance and function of the FPPS, it has been applied to a variety of typical structures, including uniform waveguides, lossfree closed structures, lossy closed structures, and open radiation structures, and compared with the results of eigenmode solvers, equivalent network methods, and spectral domain integral equation methods. The comparison shows the easy-to-use and adaptable nature of the FPPS. the FPPS. This FPPS could be also applied to open radiating structures, and even multi-dimensional periodic/uniform structures.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precision Glass Thermoforming Assisted by Neural Networks</title>
<link>https://arxiv.org/abs/2411.06762</link>
<guid>https://arxiv.org/abs/2411.06762</guid>
<content:encoded><![CDATA[
<div> surrogate model, glass thermoforming, precision, neural network, predictive<br />
<br />
Summary: 
This study presents a surrogate model based on a dimensionless back-propagation neural network (BPNN) to predict form errors in glass thermoforming processes. Traditional trial-and-error methods can be time-consuming and costly, leading to inefficiencies in precision glass product development. The surrogate model uses geometric features and process parameters as inputs to accurately predict forming errors, allowing for adjustments in mold design. The model was tested using simulation and industrial data, showing promising results in predicting form errors with reasonable accuracy. Despite potential discrepancies in industrial training data due to perception errors and mold fabrication errors, the surrogate model demonstrated practicality for implementation in the glass-manufacturing industry. <div>
arXiv:2411.06762v2 Announce Type: replace 
Abstract: Many glass products require thermoformed geometry with high precision. However, the traditional approach of developing a thermoforming process through trials and errors can cause large waste of time and resources and often end up with unsuccessfulness. Hence, there is a need to develop an efficient predictive model, replacing the costly simulations or experiments, to assist the design of precision glass thermoforming. In this work, we report a surrogate model, based on a dimensionless back-propagation neural network (BPNN), that can adequately predict the form errors and thus compensate for these errors in mold design using geometric features and process parameters as inputs. Our trials with simulation and industrial data indicate that the surrogate model can predict forming errors with adequate accuracy. Although perception errors (mold designers' decisions) and mold fabrication errors make the industrial training data less reliable than simulation data, our preliminary training and testing results still achieved a reasonable consistency with industrial data, suggesting that the surrogate models are directly implementable in the glass-manufacturing industry.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Reinforcement Learning in Quantitative Finance: A Survey</title>
<link>https://arxiv.org/abs/2408.10932</link>
<guid>https://arxiv.org/abs/2408.10932</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, finance, Quantitative Finance, machine learning, financial markets

Summary: 
This survey reviews 167 publications on the application of Reinforcement Learning (RL) in finance. RL has shown significant progress in the past decade and is gaining traction in the financial sector due to its dynamic approach and integration with machine learning techniques. Financial markets, known for their complexity and randomness, provide a challenging environment for RL applications. The survey examines various RL frameworks and applications in Quantitative Finance, highlighting the strengths and weaknesses of existing methods. Transfer learning, meta-learning, and multi-agent solutions are identified as key components advancing RL in finance. The survey also explores emerging themes in RL applications and proposes future research directions in this field. <div>
arXiv:2408.10932v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Asset Pricing: Integrating FinBERT-Based Sentiment Quantification with the Fama--French Five-Factor Model</title>
<link>https://arxiv.org/abs/2505.01432</link>
<guid>https://arxiv.org/abs/2505.01432</guid>
<content:encoded><![CDATA[
<div> FinBERT, sentiment factors, multi-factor asset pricing models, Fama French five-factor regression, market volatility <br />
<br />
Summary:This paper investigates the impact of text-derived, time-varying sentiment factors on stock returns using FinBERT. Through a comprehensive study covering 2020-2022, a dynamic sentiment index and its volatility are constructed from financial news and social media data. Results show that sentiment positively influences returns in normal market conditions but its effect varies under extreme volatility. The study reveals the time-varying nature of sentiment sensitivity and demonstrates improved abnormal returns prediction during the Federal Reserve rate hike event in June 15, 2022, using a sentiment-augmented five-factor model. These findings advocate for the integration of high-frequency sentiment analysis in traditional asset pricing models, offering valuable insights for investors and regulators.<br /><br /> <div>
arXiv:2505.01432v1 Announce Type: new 
Abstract: This paper presents a comprehensive study on the integration of text-derived, time-varying sentiment factors into traditional multi-factor asset pricing models. Leveraging FinBERT, a domain-specific deep learning language model, we construct a dynamic sentiment index and its volatility from large-scale financial news and social media data covering 2020 to 2022. By embedding these sentiment measures into the Fama French five-factor regression, we rigorously examine whether sentiment significantly explains variations in daily stock returns and how its impact evolves across different market volatility regimes. Empirical results demonstrate that sentiment has a consistently positive impact on returns during normal periods, while its effect is amplified or even reversed under extreme market conditions. Rolling regressions reveal the time-varying nature of sentiment sensitivity, and an event study around the June 15, 2022 Federal Reserve 75 basis point rate hike shows that a sentiment-augmented five-factor model better explains abnormal returns relative to the baseline model. Our findings support the incorporation of high-frequency, NLP-derived sentiment into classical asset pricing frameworks and suggest implications for investors and regulators.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimising Kernel-based Multivariate Statistical Process Control</title>
<link>https://arxiv.org/abs/2505.01556</link>
<guid>https://arxiv.org/abs/2505.01556</guid>
<content:encoded><![CDATA[
<div> kernel MSPC, multivariate statistical process control, kernel functions, kernel flows, Gaussian Process Regression

Summary:
Kernel MSPC is a framework for monitoring complex processes by analyzing multiple process variables simultaneously. Kernel MSPC enhances process monitoring capabilities by capturing non-linear relationships using kernel functions. This study proposes optimizing kernel MSPC parameters using Kernel Flows, a kernel learning methodology. The methodology also utilizes kernel combinations to learn the optimal kernel type and individual kernel parameters for each variable. The proposed approach is evaluated using cases from the Tennessee Eastman Process benchmark and successfully detects faults that were not identified in the original study. The study demonstrates the effectiveness of the proposed optimization technique and the incorporation of kernel combinations for improved process monitoring in complex systems. <br /><br />Summary: <div>
arXiv:2505.01556v1 Announce Type: new 
Abstract: Multivariate Statistical Process Control (MSPC) is a framework for monitoring and diagnosing complex processes by analysing the relationships between multiple process variables simultaneously. Kernel MSPC extends the methodology by leveraging kernel functions to capture non-linear relationships between the data, enhancing the process monitoring capabilities. However, optimising the kernel MSPC parameters, such as the kernel type and kernel parameters, is often done in literature in time-consuming and non-procedural manners such as cross-validation or grid search. In the present paper, we propose optimising the kernel MSPC parameters with Kernel Flows (KF), a recent kernel learning methodology introduced for Gaussian Process Regression (GPR). Apart from the optimisation technique, the novelty of the study resides also in the utilisation of kernel combinations for learning the optimal kernel type, and introduces individual kernel parameters for each variable. The proposed methodology is evaluated with multiple cases from the benchmark Tennessee Eastman Process. The faults are detected for all evaluated cases, including the ones not detected in the original study.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction</title>
<link>https://arxiv.org/abs/2505.01781</link>
<guid>https://arxiv.org/abs/2505.01781</guid>
<content:encoded><![CDATA[
<div> Keywords: Mean-Variance model, Black-Litterman model, Singular Spectrum analysis, Multivariate Aligned Empirical Mode Decomposition, Temporal Convolutional Networks

Summary:
The traditional Mean-Variance model is limited by sensitivity to input parameters and lack of flexibility. In contrast, the Black-Litterman model combines market equilibrium returns with investors' subjective views, showing promise in asset price prediction. A novel hybrid deep learning model incorporating Singular Spectrum analysis, Multivariate Aligned Empirical Mode Decomposition, and Temporal Convolutional Networks is proposed to enhance prediction accuracy. Experimental results demonstrate noise reduction preprocessing improves model performance significantly. The hybrid model outperforms three benchmark models in multivariate decomposition. An investment portfolio constructed using 20 NASDAQ 100 index stocks shows that when combined with the Black-Litterman model, the hybrid forecasting model produces better returns and risk control capabilities than Mean-Variance, Equal-Weighted, and Market-Weighted models over a short holding period. 

<br /><br />Summary: <div>
arXiv:2505.01781v1 Announce Type: new 
Abstract: The sensitivity to input parameters and lack of flexibility limits the traditional Mean-Variance model. In contrast, the Black-Litterman model has attracted widespread attention by integrating market equilibrium returns with investors' subjective views. This paper proposes a novel hybrid deep learning model combining Singular Spectrum analysis (SSA), Multivariate Aligned Empirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks (TCNs), aiming to improve the prediction accuracy of asset prices and thus enhance the ability of the Black-Litterman model to generate subjective views. Experimental results show that noise reduction pre-processing can improve the model's accuracy, and the prediction performance of the proposed model is significantly better than that of three multivariate decomposition benchmark models. We construct an investment portfolio by using 20 representative stocks from the NASDAQ 100 index. By combining the hybrid forecasting model with the Black-Litterman model, the generated investment portfolio exhibits better returns and risk control capabilities than the Mean-Variance, Equal-Weighted, and Market-Weighted models in the short holding period.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computational framework for predicting the effect of surface roughness in fatigue</title>
<link>https://arxiv.org/abs/2505.01871</link>
<guid>https://arxiv.org/abs/2505.01871</guid>
<content:encoded><![CDATA[
<div> Keywords: surface roughness, fatigue life, phase field model, stochastic nature, failure strength <br />
Summary: Surface roughness significantly affects the fatigue life of structural components and can be quantified using the surface factor. A numerical framework based on the phase field method has been developed to estimate the surface factor considering the stochastic nature of roughness. The model's validity is confirmed through experimental data validation. The study explores the impact of key parameters on the fatigue life of rough surfaces, including surface topology and failure strength. Notably, an increase in average surface roughness coupled with a decrease in the correlation length of the surface profile results in a pronounced effect on fatigue life. This effect is more prominent at higher failure strengths. <br /><br /> <div>
arXiv:2505.01871v1 Announce Type: new 
Abstract: Surface roughness is a critical factor influencing the fatigue life of structural components. Its effect is commonly quantified using a correction coefficient known as the surface factor. In this paper, a phase field based numerical framework is proposed to estimate the surface factor while accounting for the stochastic nature of surface roughness. The model is validated against existing experimental data. Furthermore, we investigate the influence of key parameters on the fatigue life of rough surfaces, such as surface topology and failure strength. An important effect of surface roughness is observed when the average surface roughness increases and the correlation length of the surface profile decreases. This effect becomes more pronounced with higher failure strengths.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Scheme of Electromagnetic Scattering From Scatterers With Incomplete Profiles</title>
<link>https://arxiv.org/abs/2505.02086</link>
<guid>https://arxiv.org/abs/2505.02086</guid>
<content:encoded><![CDATA[
<div> deep learning, electromagnetic scattering, incomplete profile, limited scattering data, forward and inverse problems

Summary:
A new deep learning scheme is proposed to solve electromagnetic scattering problems where the profile of the dielectric scatterer is incomplete. The scheme utilizes a limited amount of scattering data to compensate for the missing profile information. Existing solvers struggle to handle this situation effectively. The proposed scheme addresses this challenge by simultaneously solving the forward and inverse scattering problems. By using deep learning, the EM forward scattering from an incompletely known dielectric scatterer is derived, and numerical experiments are conducted to demonstrate the scheme's performance for both 2-D and 3-D EM scattering problems. The results showcase the effectiveness of the proposed deep learning-based approach in recovering the unknown parts of the scatterer profile accurately. 

<br /><br />Summary: <div>
arXiv:2505.02086v1 Announce Type: new 
Abstract: A deep learning scheme is proposed to solve the electromagnetic (EM) scattering problems where the profile of the dielectric scatterer of interest is incomplete. As a compensation, a limited amount of scattering data is provided, which is in principle containing sufficient information associated with the missing part of the profile. The existing solvers can hardly realize the compensation if the known part of the profile and the scattering data are combined straightforwardly. On one hand, the well-developed forward solvers have no mechanism to accept the scattering data, which can recover the unknown part of the profile if properly used. On the other hand, the existing solvers for inverse problems cannot retrieve the complete profile with an acceptable accuracy from the limited amount of scattering data, even when the available part of the profile can be fed into the solvers. This work aims to handle the difficulty. To this end, the EM forward scattering from an incompletely known dielectric scatterer is derived. A scheme based on DL is then proposed where the forward and inverse scattering problems are solved simultaneously. Numerical experiments are conducted to demonstrate the performance of the proposed DL-based scheme for both two-dimensional (2-D) and three-dimensional (3-D) EM scattering problems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning-Aided Approach for Estimating Field Permeability Map by Fusing Well Logs, Well Tests, and Seismic Data</title>
<link>https://arxiv.org/abs/2505.02093</link>
<guid>https://arxiv.org/abs/2505.02093</guid>
<content:encoded><![CDATA[
<div> Data fusion, permeability maps, reservoir simulation, convolutional neural network, Western Siberia<br />
<br />
Summary: 
Obtaining reliable permeability maps for oil reservoirs is essential for accurate reservoir simulation models and recovery strategies. Existing methods face challenges due to the integration of various data sources and lack of direct inter-well space information. This study proposes a novel data-fusion approach to predict two-dimensional permeability maps across the entire reservoir area. Utilizing non-parametric regression with a customized kernel shape, incorporating well logs, tests, and seismic data. A convolutional neural network processes seismic data and integrates it with other sources using a multi-stage fusion procedure to enhance the training dataset and construct the permeability map. Testing on a real oil reservoir in Western Siberia shows the developed map aligns with well permeability estimations and significantly improves inter-well space permeability predictions through seismic data integration. <br /><br /> <div>
arXiv:2505.02093v1 Announce Type: new 
Abstract: Obtaining reliable permeability maps of oil reservoirs is crucial for building a robust and accurate reservoir simulation model and, therefore, designing effective recovery strategies. This problem, however, remains challenging, as it requires the integration of various data sources by experts from different disciplines. Moreover, there are no sources to provide direct information about the inter-well space. In this work, a new method based on the data-fusion approach is proposed for predicting two-dimensional permeability maps on the whole reservoir area. This method utilizes non-parametric regression with a custom kernel shape accounting for different data sources: well logs, well tests, and seismics. A convolutional neural network is developed to process seismic data and then incorporate it with other sources. A multi-stage data fusion procedure helps to artificially increase the training dataset for the seismic interpretation model and finally to construct the adequate permeability map. The proposed methodology of permeability map construction from different sources was tested on a real oil reservoir located in Western Siberia. The results demonstrate that the developed map perfectly corresponds to the permeability estimations in the wells, and the inter-well space permeability predictions are considerably improved through the incorporation of the seismic data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking</title>
<link>https://arxiv.org/abs/2505.02139</link>
<guid>https://arxiv.org/abs/2505.02139</guid>
<content:encoded><![CDATA[
<div> representation learning, limit order book, financial market, LOBench, China A-share market<br />
Summary:<br />
The paper introduces a systematic comparative study of limit order book (LOB) representation learning to extract transferable, compact features capturing essential LOB properties. A standardized benchmark called LOBench is presented with curated datasets, unified preprocessing, and strong baselines using real China A-share market data. The study shows the sufficiency and necessity of LOB representations for various downstream tasks, highlighting their advantages over traditional task-specific end-to-end models and advanced representation learning models for general time series. This work establishes a reproducible framework and provides clear guidelines for future research. The datasets and code are publicly available at the provided link, allowing for further exploration and validation of the proposed methods.<br /> <div>
arXiv:2505.02139v1 Announce Type: new 
Abstract: The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constrains, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at https://github.com/financial-simulation-lab/LOBench.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
<div> fantasy football, integer programming, hybrid scoring metric, artificial intelligence, Premier League

Summary:
The paper introduces novel deterministic and robust integer programming models for selecting the optimal starting eleven and captain in fantasy football, a billion-dollar industry. A hybrid scoring metric is proposed using an interpretable artificial intelligence framework, leading to the highest scores while maintaining consistent performance. The models' performance is evaluated using data from the 2023/24 Premier League season, showing effectiveness during out-of-sample periods. Strategic averaging techniques for estimating cost vectors and the proposed hybrid approach are found to be successful. The paper provides insights into optimal formations and player selections, offering valuable strategies for fantasy football enthusiasts. <div>
arXiv:2505.02170v1 Announce Type: new 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Constrained by a fixed budget, decision-makers draft a squad whose players are expected to perform well in the upcoming weeks to maximize total points. This paper proposes novel deterministic and robust integer programming models that select the optimal starting eleven and the captain. A new hybrid scoring metric is constructed using an interpretable artificial intelligence framework and underlying match performance data. Several objective functions and estimation techniques are introduced for the programming model. To the best of my knowledge, this is the first study to approach fantasy football through this lens. The models' performance is evaluated using data from the 2023/24 Premier League season. Results indicate that the proposed hybrid method achieved the highest score while maintaining consistent performance. Utilizing the Monte Carlo simulation, the strategic choice of averaging techniques for estimating cost vectors, and the proposed hybrid approach are shown to be effective during the out-of-sample period. This paper also provides a thorough analysis of the optimal formations and players selected by the models, offering valuable insights into effective fantasy football strategies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Method for Optimizing Submarine Search and Rescue Strategy Under Environmental Uncertainty</title>
<link>https://arxiv.org/abs/2505.02186</link>
<guid>https://arxiv.org/abs/2505.02186</guid>
<content:encoded><![CDATA[
<div> framework, dynamic analysis, Monte Carlo method, Bayesian method, economic optimization
Summary:
The article presents a hybrid algorithm framework for locating and rescuing deep-sea submersibles in uncertain ocean environments. By combining dynamic analysis, Monte Carlo, and Bayesian methods, a probabilistic prediction approach is used to improve search efficiency. The Monte Carlo method is employed to account for environmental variability, enhancing location prediction accuracy. Bayesian grid research and probabilistic updating are integrated based on trajectory predictions, with Bayesian filtering for complex scenarios. Economic optimization is conducted through cost-benefit analysis using the entropy weight method, and the CER is applied for evaluation. This comprehensive approach aims to maximize the rate of successful rescues while minimizing costs, addressing the challenges of deep-sea submersible rescue operations effectively. <br /><br /> <div>
arXiv:2505.02186v1 Announce Type: new 
Abstract: When coping with the urgent challenge of locating and rescuing a deep-sea submersible in the event of communication or power failure, environmental uncertainty in the ocean can not be ignored. However, classic physical models are limited to deterministic scenarios. Therefore, we present a hybrid algorithm framework combined with dynamic analysis for target submarine, Monte Carlo and Bayesian method for conducting a probabilistic prediction to improve the search efficiency. Herein, the Monte Carlo is performed to overcome the environmental variability to improve the accuracy in location prediction. According to the trajectory prediction, we integrated the Bayesian based grid research and probabilistic updating. For more complex situations, we introduced the Bayesian filtering. Aiming to maximize the rate of successful rescue and costs, the economic optimization is performed utilizing the cost-benefit analysis based on entropy weight method and the CER is applied for evaluation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the Dynamics of Complex System via Multiscale Diffusion Autoencoder</title>
<link>https://arxiv.org/abs/2505.02450</link>
<guid>https://arxiv.org/abs/2505.02450</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiscale Diffusion Prediction Network, complex systems, latent space, spatiotemporal evolution, graph neural network

Summary:
The article introduces a new method, the Multiscale Diffusion Prediction Network (MDPNet), for predicting the dynamics of complex systems. Existing methods often overlook the multiscale structure of complex systems, leading to inaccuracies in predictions of spatiotemporal evolution. MDPNet aims to address this by leveraging the multiscale structure to discover the latent space of intrinsic dynamics. It utilizes a multiscale diffusion autoencoder to encode multiscale features and guide the diffusion model for reliable reconstruction. Additionally, an attention-based graph neural ordinary differential equation is introduced to model the co-evolution across different scales. The proposed method shows promising results in extensive evaluations on representative systems, achieving a significant average prediction error reduction of 53.23% compared to baseline methods. It also demonstrates superior robustness and generalization, highlighting its potential for various scientific and engineering applications. 

<br /><br />Summary: <div>
arXiv:2505.02450v1 Announce Type: new 
Abstract: Predicting the dynamics of complex systems is crucial for various scientific and engineering applications. The accuracy of predictions depends on the model's ability to capture the intrinsic dynamics. While existing methods capture key dynamics by encoding a low-dimensional latent space, they overlook the inherent multiscale structure of complex systems, making it difficult to accurately predict complex spatiotemporal evolution. Therefore, we propose a Multiscale Diffusion Prediction Network (MDPNet) that leverages the multiscale structure of complex systems to discover the latent space of intrinsic dynamics. First, we encode multiscale features through a multiscale diffusion autoencoder to guide the diffusion model for reliable reconstruction. Then, we introduce an attention-based graph neural ordinary differential equation to model the co-evolution across different scales. Extensive evaluations on representative systems demonstrate that the proposed method achieves an average prediction error reduction of 53.23% compared to baselines, while also exhibiting superior robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Compression for Time Series Modelling: A Case Study of Smart Grid Demand Forecasting</title>
<link>https://arxiv.org/abs/2505.02606</link>
<guid>https://arxiv.org/abs/2505.02606</guid>
<content:encoded><![CDATA[
<div> wavelet-based compression, time series forecasting, smart energy systems, data management, high-frequency data <br />
<br />
Summary: <br />
Efficient time series forecasting is crucial for smart energy systems, but the increasing amount of high-frequency data poses storage and transmission challenges. This study investigates the use of Discrete Wavelet Transform (DWT)-based data compression to address these challenges while maintaining forecasting accuracy. By applying biorthogonal wavelets at different compression rates, the study evaluates the impact on three forecasting models: Ordinary Least Squares (OLS), XGBoost, and Time Series Dense Encoder (TiDE). Results show that XGBoost is robust to compression artifacts, while OLS is sensitive to smooth wavelets and high compression rates. TiDE demonstrates some variability but remains competitive. The study suggests that wavelet-based compression can efficiently manage data in smart energy systems without compromising forecasting accuracy, with potential applications in climate modeling, water supply systems, and industrial operations. <div>
arXiv:2505.02606v1 Announce Type: new 
Abstract: Efficient time series forecasting is essential for smart energy systems, enabling accurate predictions of energy demand, renewable resource availability, and grid stability. However, the growing volume of high-frequency data from sensors and IoT devices poses challenges for storage and transmission. This study explores Discrete Wavelet Transform (DWT)-based data compression as a solution to these challenges while ensuring forecasting accuracy. A case study of a seawater supply system in Hirtshals, Denmark, operating under dynamic weather, operational schedules, and seasonal trends, is used for evaluation.
  Biorthogonal wavelets of varying orders were applied to compress data at different rates. Three forecasting models - Ordinary Least Squares (OLS), XGBoost, and the Time Series Dense Encoder (TiDE) - were tested to assess the impact of compression on forecasting performance. Lossy compression rates up to $r_{\mathrm{lossy}} = 0.999$ were analyzed, with the Normalized Mutual Information (NMI) metric quantifying the relationship between compression and information retention. Results indicate that wavelet-based compression can retain essential features for accurate forecasting when applied carefully.
  XGBoost proved highly robust to compression artifacts, maintaining stable performance across diverse compression rates. In contrast, OLS demonstrated sensitivity to smooth wavelets and high compression rates, while TiDE showed some variability but remained competitive. This study highlights the potential of wavelet-based compression for scalable, efficient data management in smart energy systems without sacrificing forecasting accuracy. The findings are relevant to other fields requiring high-frequency time series forecasting, including climate modeling, water supply systems, and industrial operations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components</title>
<link>https://arxiv.org/abs/2505.01627</link>
<guid>https://arxiv.org/abs/2505.01627</guid>
<content:encoded><![CDATA[
<div> Automated Classification, Large Language Models, Function-Based Design, Mechanical Assembly Parts, Domain Adaptation <br />
Summary: <br />
The study introduces a novel framework utilizing Large Language Models (LLMs) for automated classification of mechanical assembly parts' functions in the conceptual design phase of product development. Functional modeling, a critical aspect of early-phase engineering, is often hindered by the lack of structured functional data. Large Language Models (LLMs) show promise in addressing this gap by automating function annotation through domain adaptation (DA) using fine-tuning. The study showcases the effectiveness of fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR) to enhance the semantic representation of mechanical parts. Evaluation on the A Big CAD (ABC) dataset demonstrates that domain-adapted LLMs can generate high-quality functional data, improving early design decision-making and supporting more effective design exploration. <br /> <div>
arXiv:2505.01627v1 Announce Type: cross 
Abstract: The conceptual design phase represents a critical early stage in the product development process, where designers generate potential solutions that meet predefined design specifications based on functional requirements. Functional modeling, a foundational aspect of this phase, enables designers to reason about product functions before specific structural details are determined. A widely adopted approach to functional modeling is the Function-Behavior-Structure (FBS) framework, which supports the transformation of functional intent into behavioral and structural descriptions. However, the effectiveness of function-based design is often hindered by the lack of well-structured and comprehensive functional data. This scarcity can negatively impact early design decision-making and hinder the development of accurate behavioral models. Recent advances in Large Language Models (LLMs), such as those based on GPT architectures, offer a promising avenue to address this gap. LLMs have demonstrated significant capabilities in language understanding and natural language processing (NLP), making them suitable for automated classification tasks. This study proposes a novel LLM-based domain adaptation (DA) framework using fine-tuning for the automated classification of mechanical assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the traditionally manual and subjective process of function annotation can be improved in both accuracy and consistency. A case study demonstrates fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the domain-adapted LLM can generate high-quality functional data, enhancing the semantic representation of mechanical parts and supporting more effective design exploration in early-phase engineering.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strengthening Infrastructure Resilience to Hurricanes by Modeling Transportation and Electric Power Network Interdependencies</title>
<link>https://arxiv.org/abs/2404.12978</link>
<guid>https://arxiv.org/abs/2404.12978</guid>
<content:encoded><![CDATA[
<div> agent-based model, resilience, hurricane, infrastructure disruptions, interdependencies

Summary:
The study introduces an agent-based model (ABM) to analyze community resilience to hurricane-induced infrastructure disruptions, specifically focusing on the interconnections between electric power and transportation networks. Agents in the ABM represent different system components, such as electric power network, transportation network, hazards, and households. By considering interactions within and among systems, the model simulates household resilience during a hurricane in Miami-Dade County, Florida. The model incorporates two key interdependencies: the role of transportation in fuel delivery to power plants and restoration teams' access, as well as the impact of power outages on transportation network components. Validated against Hurricane Irma data, the ABM demonstrates the effectiveness of a traffic lights-based restoration strategy, prioritizing signal recovery to minimize traffic disruptions and accelerate household power restoration. The study emphasizes the importance of timely traffic signal restoration, road accessibility for restoration teams, and uninterrupted fuel transportation for efficient power restoration in hurricane scenarios. 

<br /><br />Summary: <div>
arXiv:2404.12978v2 Announce Type: replace 
Abstract: This study presents an agent-based model (ABM) developed to simulate the resilience of a community to hurricane-induced infrastructure disruptions, focusing on the interdependencies between electric power and transportation networks. In this ABM approach, agents represent the components of a system, where interactions within a system shape intra-dependency of a system and interactions among systems shape interdependencies. To study household resilience subject to a hurricane, a library of agents has been created including electric power network, transportation network, wind/flooding hazards, and household agents. The ABM is applied over the household and infrastructure data from a community (Zip code 33147) in Miami-Dade County, Florida. Interdependencies between the two networks are modeled in two ways, (i) representing the role of transportation in fuel delivery to power plants and restoration teams' access, (ii) impact of power outage on transportation network components. Restoring traffic signals quickly is crucial as their outage can slow down traffic and increase the chance of crashes. We simulate three restoration strategies: component based, distance based, and traffic lights based restoration. The model is validated against Hurricane Irma data, showing consistent behavior with varying hazard intensities. Scenario analyses explore the impact of restoration strategies, road accessibility, and wind speed intensities on power restoration. Results demonstrate that a traffic lights based restoration strategy efficiently prioritizes signal recovery without delaying household power restoration time. Restoration of power services will be faster if restoration teams do not need to wait due to inaccessible roads and fuel transportation to power plants is not delayed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A finite strain model for fiber angle plasticity of textile fabrics based on isogeometric shell finite elements</title>
<link>https://arxiv.org/abs/2412.20131</link>
<guid>https://arxiv.org/abs/2412.20131</guid>
<content:encoded><![CDATA[
<div> shear elastoplasticity model, textile fabrics, anisotropic Kirchhoff-Love shells, plasticity, frictional sliding <br />
Summary: <br />
This work introduces a shear elastoplasticity model for textile fabrics based on anisotropic Kirchhoff-Love shells with embedded fiber bending. The model accounts for rotational inter-ply frictional sliding between fiber families in textile composites experiencing large deformation, emphasizing dry fabrics like woven and non-crimp fabrics. Utilizing relative angles between fiber families as strain measures, the model is formulated using surface invariants without thickness integration. A yield function with isotropic hardening and simple evolution equation is proposed, calibrated using the picture frame test, and validated with experimental data like the bias extension test. The elastoplastic model's accuracy is confirmed through good agreement with experimental results, and its application to 3D shell problems is demonstrated. <div>
arXiv:2412.20131v2 Announce Type: replace 
Abstract: This work presents a shear elastoplasticity model for textile fabrics within the theoretical framework of anisotropic Kirchhoff-Love shells with bending of embedded fibers proposed by Duong et al. (2023). The plasticity model aims at capturing the rotational inter-ply frictional sliding between fiber families in textile composites undergoing large deformation. Such effects are usually dominant in dry textile fabrics such as woven and non-crimp fabrics. The model explicitly uses relative angles between fiber families as strain measures for the kinematics. The plasticity model is formulated directly with surface invariants without resorting to thickness integration. Motivated by experimental observations from the picture frame test, a yield function is proposed with isotropic hardening and a simple evolution equation. A classical return mapping algorithm is employed to solve the elastoplastic problem within the isogeometric finite shell element formulation of Duong et al. (2022). The verification of the implementation is facilitated by the analytical solution for the picture frame test. The proposed plasticity model is calibrated from the picture frame test and is then validated by the bias extension test, considering available experimental data for different samples from the literature. Good agreement between model prediction and experimental data is obtained. Finally, the applicability of the elastoplasticity model to 3D shell problems is demonstrated.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskLabs: Predicting Financial Risk Using Large Language Model based on Multimodal and Multi-Sources Data</title>
<link>https://arxiv.org/abs/2404.07452</link>
<guid>https://arxiv.org/abs/2404.07452</guid>
<content:encoded><![CDATA[
<div> framework, LLMs, financial risk prediction, RiskLabs, multimodal data <br />
Summary: <br />
This paper introduces RiskLabs, a framework that utilizes large language models (LLMs) to predict financial risks by integrating multimodal financial data sources such as Earnings Conference Calls (ECCs), market-related time series data, and contextual news data. The study demonstrates the effectiveness of RiskLabs in forecasting market volatility and variance. It examines the contributions of different data sources to financial risk assessment and emphasizes the importance of LLMs in this process. The paper also discusses the challenges associated with using LLMs for financial risk prediction and explores the potential benefits of combining LLMs with multimodal data for enhanced risk analysis. <div>
arXiv:2404.07452v2 Announce Type: replace-cross 
Abstract: The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering, and stock movement prediction (binary classification), the application of LLMs to financial risk prediction remains underexplored. Addressing this gap, in this paper, we introduce RiskLabs, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely integrates multimodal financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data to improve financial risk prediction. Empirical results demonstrate RiskLabs' effectiveness in forecasting both market volatility and variance. Through comparative experiments, we examine the contributions of different data sources to financial risk assessment and highlight the crucial role of LLMs in this process. We also discuss the challenges associated with using LLMs for financial risk prediction and explore the potential of combining them with multimodal data for this purpose.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spectral-based Physics-informed Finite Operator Learning for Prediction of Mechanical Behavior of Microstructures</title>
<link>https://arxiv.org/abs/2410.19027</link>
<guid>https://arxiv.org/abs/2410.19027</guid>
<content:encoded><![CDATA[
<div> operator learning, spectral methods, heterogeneous materials, physics-informed, microstructures<br />
<br />
Summary: A novel physics-informed operator learning technique based on spectral methods introduces the Lippmann-Schwinger operator in Fourier space to model heterogeneous materials. The method accelerates training by enabling gradient construction on a fixed discretization in Fourier space and maps microstructure shapes to mechanical responses without labeled data. Training minimizes equilibrium in Fourier space under loading conditions, ensuring periodicity. Physically constrained and diverse training data enhance accuracy, although performance may degrade for out-of-distribution microstructures. Integration of a Fourier Neural Operator improves accuracy in predicting stress fields and offers zero-shot super-resolution capabilities in heterogeneous domains. Extension to handle 3D problems and adaptation to finite elasticity demonstrate robustness in handling nonlinear mechanical behavior. This framework shows promise for efficient and scalable prediction of mechanical responses in complex material systems while reducing training time for physics-informed neural operators. <br /><br /> <div>
arXiv:2410.19027v3 Announce Type: replace-cross 
Abstract: A novel physics-informed operator learning technique based on spectral methods is introduced to model the complex behavior of heterogeneous materials. The Lippmann-Schwinger operator in Fourier space is employed to construct physical constraints with minimal computational overhead, effectively eliminating the need for automatic differentiation. The introduced methodology accelerates the training process by enabling gradient construction on a fixed, finite discretization in Fourier space. Later, the spectral physics-informed finite operator learning (SPiFOL) framework is built based on this discretization and trained to map the arbitrary shape of microstructures to their mechanical responses (strain fields) without relying on labeled data. The training is done by minimizing equilibrium in Fourier space concerning the macroscopic loading condition, which also guarantees the periodicity. SPiFOL, as a physics-informed operator learning method, enables rapid predictions through forward inference after training. To ensure accuracy, we incorporate physical constraints and diversify the training data. However, performance may still degrade for out-of-distribution microstructures. SPiFOL is further enhanced by integrating a Fourier Neural Operator (FNO). Compared to the standard data-driven FNO, SPiFOL shows higher accuracy in predicting stress fields and provides nearly resolution-independent results. Additionally, its zero-shot super-resolution capabilities are explored in heterogeneous domains. Finally, SPiFOL is extended to handle 3D problems and further adapted to finite elasticity, demonstrating the robustness of the framework in handling nonlinear mechanical behavior. The framework shows great potential for efficient and scalable prediction of mechanical responses in complex material systems while also reducing the training time required for training physics-informed neural operators.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction</title>
<link>https://arxiv.org/abs/2501.01010</link>
<guid>https://arxiv.org/abs/2501.01010</guid>
<content:encoded><![CDATA[
<div> State Space Model, cryptocurrency, Bitcoin, forecasting, financial data
<br />
Summary:
<br />
- The study addresses the challenge of predicting Bitcoin prices due to market volatility and dynamics.
- Traditional models like ARIMA and neural networks struggle with regime shifts and dependencies in data.
- The proposed CryptoMamba, a Mamba-based State Space Model, effectively captures long-range dependencies.
- CryptoMamba outperforms previous models in accuracy and generalizability across market conditions.
- The model's practical utility is demonstrated through accurate forecasts translating into financial gains, making it advantageous for stock and cryptocurrency price forecasting tasks. 
<br /> <div>
arXiv:2501.01010v2 Announce Type: replace-cross 
Abstract: Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming physics-informed machine learning to convex optimization</title>
<link>https://arxiv.org/abs/2505.01047</link>
<guid>https://arxiv.org/abs/2505.01047</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Physics-Informed, Convex Optimization, B-splines, Adaptive Knot Optimization

Summary:
Convex-PIML is a framework proposed to address optimization challenges in Physics-Informed Machine Learning (PIML). By transforming PIML into convex optimization, the framework overcomes limitations and enables effective integration of data with physical laws. The use of linear combination of B-splines promotes the convexity of the loss function, allowing well-established convex optimization algorithms to be utilized for efficient solutions. An adaptive knot optimization method tackles the spectral bias issue of PIML, enhancing performance. The framework is theoretically guaranteed and tested across scenarios with various physical priors, demonstrating effective solution of optimization problems. Convex-PIML shows promise for diverse applications by offering a comprehensive approach to combining data and physics for scientific problem-solving. <br /><br />Summary: <div>
arXiv:2505.01047v1 Announce Type: new 
Abstract: Physics-Informed Machine Learning (PIML) offers a powerful paradigm of integrating data with physical laws to address important scientific problems, such as parameter estimation, inferring hidden physics, equation discovery, and state prediction, etc. However, PIML still faces many serious optimization challenges that significantly restrict its applications. In this study, we propose a comprehensive framework that transforms PIML to convex optimization to overcome all these limitations, referred to as Convex-PIML. The linear combination of B-splines is utilized to approximate the data, promoting the convexity of the loss function. By replacing the non-convex components of the loss function with convex approximations, the problem is further converted into a sequence of successively refined approximated convex optimization problems. This conversion allows the use of well-established convex optimization algorithms, obtaining solutions effectively and efficiently. Furthermore, an adaptive knot optimization method based on error estimate is introduced to mitigate the spectral bias issue of PIML, further improving the performance. The proposed theoretically guaranteed framework is tested in scenarios with distinct types of physical prior. The results indicate that optimization problems are effectively solved in these scenarios, highlighting the potential of the framework for broad applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced-order structure-property linkages for stochastic metamaterials</title>
<link>https://arxiv.org/abs/2505.01283</link>
<guid>https://arxiv.org/abs/2505.01283</guid>
<content:encoded><![CDATA[
<div> additive manufacturing, mechanical metamaterials, unit cell geometries, materials informatics framework, Gaussian process regression

Summary:<br />
The article discusses the efficient design and evaluation of mechanical metamaterials using additive manufacturing. It emphasizes the need to establish connections between unit cell designs and their mechanical properties, which can be computationally intensive. The study employs principal component analysis to identify key features from a large dataset of 2D metamaterials. Fast Fourier transform-based simulations are utilized to calculate the effective elastic stiffness of different unit cell designs. Gaussian process regression is then applied to create reduced-order models mapping designs to their elastic constants. The research demonstrates the creation of robust structure-property maps through a low-dimensional representation of the dataset. Additionally, an active learning approach is used to train a surrogate model with minimal data points, showcasing the ability to generate accurate maps with a small fraction of the original dataset. <div>
arXiv:2505.01283v1 Announce Type: new 
Abstract: The capabilities of additive manufacturing have facilitated the design and production of mechanical metamaterials with diverse unit cell geometries. Establishing linkages between the vast design space of unit cells and their effective mechanical properties is critical for the efficient design and performance evaluation of such metamaterials. However, physics-based simulations of metamaterial unit cells across the entire design space are computationally expensive, necessitating a materials informatics framework to efficiently capture complex structure-property relationships. In this work, principal component analysis of 2-point correlation functions is performed to extract the salient features from a large dataset of randomly generated 2D metamaterials. Physics-based simulations are performed using a fast Fourier transform (FFT)-based homogenization approach to efficiently compute the homogenized effective elastic stiffness across the extensive unit cell designs. Subsequently, Gaussian process regression is used to generate reduced-order surrogates, mapping unit cell designs to their homogenized effective elastic constant. It is demonstrated that the adopted workflow enables a high-value low-dimensional representation of the voluminous stochastic metamaterial dataset, facilitating the construction of robust structure-property maps. Finally, an uncertainty-based active learning framework is utilized to train a surrogate model with a significantly smaller number of data points compared to the original full dataset. It is shown that a dataset as small as $0.61\%$ of the entire dataset is sufficient to generate accurate and robust structure-property maps.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimICD: A Closed-Loop Simulation Framework For ICD Therapy</title>
<link>https://arxiv.org/abs/2505.01371</link>
<guid>https://arxiv.org/abs/2505.01371</guid>
<content:encoded><![CDATA[
<div> simulation, ICD behavior, cardiac electrophysiology, arrhythmic episodes, therapy progression

Summary:
SimICD is a new simulation tool that integrates virtual ICD logic algorithms with cardiac electrophysiology simulations to simulate therapy progression decisions during arrhythmic episodes. This tool fills a gap in available models by allowing for the testing of ICD functionality in a controlled environment before clinical use. The simulations conducted with SimICD demonstrate the realistic simulation of cardiac signals and ICD responses that align with real-world devices' logic. This enables the reprogramming of ICD parameters to adapt to specific tachy-arrhythmia episodes, improving treatment customization and efficacy. Overall, SimICD facilitates virtual studies of ICD behavior, enhancing the understanding and testing of device functionality for better clinical outcomes. 

<br /><br />Summary: <div>
arXiv:2505.01371v1 Announce Type: new 
Abstract: Virtual studies of ICD behaviour are crucial for testing device functionality in a controlled environment prior to clinical application. Although previous works have shown the viability of using in silico testing for diagnosis, there is a notable gap in available models that can simulate therapy progression decisions during arrhythmic episodes. This work introduces SimICD, a simulation tool which combines virtual ICD logic algorithms with cardiac electrophysiology simulations in a feedback loop, allowing the progression of ICD therapy protocols to be simulated for a range of tachy-arrhythmia episodes. Using a cohort of virtual patients, we demonstrate the ability of SimICD to simulate realistic cardiac signals and ICD responses that align with the logic of real-world devices, facilitating the reprogramming of ICD parameters to adapt to specific episodes.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design for a Digital Twin in Clinical Patient Care</title>
<link>https://arxiv.org/abs/2505.01206</link>
<guid>https://arxiv.org/abs/2505.01206</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Twins, clinical patient care, knowledge graphs, ensemble learning, decision-making <br />
Summary:<br />
Digital Twins have the potential to revolutionize personalized clinical patient care by integrating knowledge graphs and ensemble learning to create a comprehensive reflection of a patient's clinical journey. These Digital Twins are predictive, modular, evolving, informed, interpretable, and explainable, providing valuable insights for clinicians to make informed decisions. By incorporating these elements, Digital Twins can cater to a variety of medical fields, from oncology to epidemiology, offering a versatile tool for healthcare professionals. This innovative approach ensures that Digital Twins are not only accurate and predictive but also adaptable to changing patient needs. Overall, this new design for Digital Twins has the capacity to significantly enhance clinical decision-making processes and improve patient outcomes.<br />Summary: <div>
arXiv:2505.01206v1 Announce Type: cross 
Abstract: Digital Twins hold great potential to personalize clinical patient care, provided the concept is translated to meet specific requirements dictated by established clinical workflows. We present a generalizable Digital Twin design combining knowledge graphs and ensemble learning to reflect the entire patient's clinical journey and assist clinicians in their decision-making. Such Digital Twins can be predictive, modular, evolving, informed, interpretable and explainable with applications ranging from oncology to epidemiology.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks</title>
<link>https://arxiv.org/abs/2402.08978</link>
<guid>https://arxiv.org/abs/2402.08978</guid>
<content:encoded><![CDATA[
<div> visualization, financial cluster analysis, quantitative analysis, qualitative analysis, multi-view clustering

Summary: 
Prismatic is a visual analytics system designed to assist investors in financial cluster analysis by integrating quantitative and qualitative analysis. It addresses challenges such as numerous pairwise comparisons and dynamic correlations across different time periods. Prismatic features dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation processes. By using a multi-view clustering approach, it combines data-driven clusters with knowledge-driven similarity, providing a more nuanced understanding of business correlations. The system offers well-coordinated visual views to facilitate a comprehensive interpretation of intertwined quantitative and qualitative features. Case studies on formulating concept stocks and interviews with domain experts demonstrate the effectiveness and usefulness of Prismatic in discovering investment alternatives and managing risks in the financial market. <div>
arXiv:2402.08978v2 Announce Type: replace-cross 
Abstract: Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural Networks</title>
<link>https://arxiv.org/abs/2409.07769</link>
<guid>https://arxiv.org/abs/2409.07769</guid>
<content:encoded><![CDATA[
<div> Graph neural network, mesh-based super-resolution, fluid flows, multiscale model, Reynolds number
<br />
Summary:
<br />
This study presents a novel approach using a graph neural network (GNN) for mesh-based three-dimensional super-resolution of fluid flows. The GNN operates on localized meshes of elements to improve accuracy. The architecture includes coarse-scale and fine-scale processors separated by a graph unpooling layer for multiscale modeling. Results from simulations of Taylor-Green Vortex and backward-facing step flow demonstrate the GNN's ability to produce accurate super-resolved fields compared to coarse-scale and multiscale models. Reconstruction errors increase with higher Reynolds numbers. Additionally, the GNN shows promising capabilities for geometry extrapolation in cross-mesh scenarios. <div>
arXiv:2409.07769v4 Announce Type: replace-cross 
Abstract: A graph neural network (GNN) approach is introduced in this work which enables mesh-based three-dimensional super-resolution of fluid flows. In this framework, the GNN is designed to operate not on the full mesh-based field at once, but on localized meshes of elements (or cells) directly. To facilitate mesh-based GNN representations in a manner similar to spectral (or finite) element discretizations, a baseline GNN layer (termed a message passing layer, which updates local node properties) is modified to account for synchronization of coincident graph nodes, rendering compatibility with commonly used element-based mesh connectivities. The architecture is multiscale in nature, and is comprised of a combination of coarse-scale and fine-scale message passing layer sequences (termed processors) separated by a graph unpooling layer. The coarse-scale processor embeds a query element (alongside a set number of neighboring coarse elements) into a single latent graph representation using coarse-scale synchronized message passing over the element neighborhood, and the fine-scale processor leverages additional message passing operations on this latent graph to correct for interpolation errors. Demonstration studies are performed using hexahedral mesh-based data from Taylor-Green Vortex and backward-facing step flow simulations at Reynolds numbers of 1600 and 3200. Through analysis of both global and local errors, the results ultimately show how the GNN is able to produce accurate super-resolved fields compared to targets in both coarse-scale and multiscale model configurations. Reconstruction errors for fixed architectures were found to increase in proportion to the Reynolds number. Geometry extrapolation studies on a separate cavity flow configuration show promising cross-mesh capabilities of the super-resolution strategy.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Multimodal Multiscale Data Fusion for Digital Twins in Aerosol Jet Electronics Printing</title>
<link>https://arxiv.org/abs/2505.00176</link>
<guid>https://arxiv.org/abs/2505.00176</guid>
<content:encoded><![CDATA[
<div> Aerosol Jet Printing, Additive Manufacturing, Machine Learning, Process-Structure-Property Modeling, Diffusion Models<br />
<br />
Summary: This study introduces a novel generative modeling methodology using diffusion models to fuse multimodal and multiscale Process-Structure-Property (PSP) data in Aerosol Jet Printing (AJP). The method aims to enhance manufacturing by quantitatively connecting process parameters, structural features, and material properties. Current machine learning approaches for AJP face limitations in handling complex data, highlighting the need for comprehensive analysis through fusion methods. The proposed approach registers and fuses optical microscopy and confocal profilometry data from AJP, capturing intricate PSP relationships and providing insights into dynamic manufacturing systems' digital twins. The results demonstrate effective fusion and fine-tuning steps, offering a deeper understanding of complex AJP processes.<br /><br />Summary: <div>
arXiv:2505.00176v1 Announce Type: new 
Abstract: The rising demand for high-value electronics necessitates advanced manufacturing techniques capable of meeting stringent specifications for precise, complex, and compact devices, driving the shift toward innovative additive manufacturing (AM) solutions. Aerosol Jet Printing (AJP) is a versatile AM technique that utilizes aerosolized functional materials to accurately print intricate patterns onto diverse substrates. Machine learning (ML)- based Process-Structure-Property (PSP) modeling is essential for enhancing AJP manufacturing, as it quantitatively connects process parameters, structural features, and resulting material properties. However, current ML approaches for modeling PSP relationships in AJP face significant limitations in handling multimodal and multiscale data, underscoring a critical need for generative methods capable of comprehensive analysis through multimodal and multiscale fusion. To address this challenge, this study introduces a novel generative modeling methodology leveraging diffusion models for PSP data fusion in AJP. The proposed method integrates multimodal, multiscale PSP features in two phases: (1) registering the features, and (2) fusing them to generate causal relationships between PSP attributes. A case study demonstrates the registration and fusion of optical microscopy (OM) images and confocal profilometry (CP) data from AJP, along with the fine-tuning of the fusion step. The results effectively capture complex PSP relationships, offering deeper insights into digital twins of dynamic manufacturing systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Thermal Control Based on Spatial Thermal Comfort with Reconstructed Environmental Data</title>
<link>https://arxiv.org/abs/2505.00468</link>
<guid>https://arxiv.org/abs/2505.00468</guid>
<content:encoded><![CDATA[
<div> Reconstruction, Thermal comfort, Predicted Mean Vote (PMV), Gappy Proper Orthogonal Decomposition (Gappy POD), Multi-occupant living lab environment

Summary:
The study introduces a novel method for estimating thermal comfort by incorporating spatial environmental data reconstructed using the Gappy Proper Orthogonal Decomposition (Gappy POD) algorithm. This addresses limitations of fixed-location sensors by enabling accurate reconstruction of indoor temperature fields. A group PMV-based control framework is developed to consider the thermal comfort of multiple occupants, allowing for individual and group-level thermal condition calculations. Experimental results demonstrate the effectiveness of the Gappy POD algorithm in temperature reconstruction with a low average relative error. The spatial variability in PMV values based on occupant location highlights the importance of adaptive thermal control strategies. The study underscores the significance of adaptive thermal control strategies that consider both spatial and individual variability for enhanced occupant-centric building operations. <br /><br />Summary: <div>
arXiv:2505.00468v1 Announce Type: new 
Abstract: Achieving thermal comfort while maintaining energy efficiency is a critical objective in building system control. Conventional thermal comfort models, such as the Predicted Mean Vote (PMV), rely on both environmental and personal variables. However, the use of fixed-location sensors limits the ability to capture spatial variability, which reduces the accuracy of occupant-specific comfort estimation. To address this limitation, this study proposes a new PMV estimation method that incorporates spatial environmental data reconstructed using the Gappy Proper Orthogonal Decomposition (Gappy POD) algorithm. In addition, a group PMV-based control framework is developed to account for the thermal comfort of multiple occupants. The Gappy POD method enables fast and accurate reconstruction of indoor temperature fields from sparse sensor measurements. Using these reconstructed fields and occupant location data, spatially resolved PMV values are calculated. Group-level thermal conditions are then derived through statistical aggregation methods and used to control indoor temperature in a multi-occupant living lab environment. Experimental results show that the Gappy POD algorithm achieves an average relative error below 3\% in temperature reconstruction. PMV distributions varied by up to 1.26 scale units depending on occupant location. Moreover, thermal satisfaction outcomes varied depending on the group PMV method employed. These findings underscore the importance for adaptive thermal control strategies that incorporate both spatial and individual variability, offering valuable insights for future occupant-centric building operations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review</title>
<link>https://arxiv.org/abs/2505.00210</link>
<guid>https://arxiv.org/abs/2505.00210</guid>
<content:encoded><![CDATA[
<div> machine learning, manufacturing, control systems, generative models, process monitoring<br />
<br />
Summary: This review explores the integration of generative machine learning (ML) in dynamic manufacturing processes to enhance in-situ monitoring and control systems. The complex characteristics of manufacturing systems, including time-varying parameters and uncertainties, necessitate advanced control techniques that can respond in real-time while maintaining product quality. The review categorizes approaches into Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated methods, highlighting the potential of generative ML in decision-making, process guidance, simulation, and digital twins. However, challenges such as the separation between generation and control functions, lack of physical understanding of manufacturing phenomena, and model adaptation from other domains need to be addressed. Future research directions focus on developing integrated frameworks that combine generative ML and control technologies to effectively address the dynamic complexities of modern manufacturing systems. <div>
arXiv:2505.00210v1 Announce Type: cross 
Abstract: Dynamic manufacturing processes exhibit complex characteristics defined by time-varying parameters, nonlinear behaviors, and uncertainties. These characteristics require sophisticated in-situ monitoring techniques utilizing multimodal sensor data and adaptive control systems that can respond to real-time feedback while maintaining product quality. Recently, generative machine learning (ML) has emerged as a powerful tool for modeling complex distributions and generating synthetic data while handling these manufacturing uncertainties. However, adopting these generative technologies in dynamic manufacturing systems lacks a functional control-oriented perspective to translate their probabilistic understanding into actionable process controls while respecting constraints. This review presents a functional classification of Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated approaches, offering a perspective for understanding existing ML-enhanced control systems and incorporating generative ML. The analysis of generative ML architectures within this framework demonstrates control-relevant properties and potential to extend current ML-enhanced approaches where conventional methods prove insufficient. We show generative ML's potential for manufacturing control through decision-making applications, process guidance, simulation, and digital twins, while identifying critical research gaps: separation between generation and control functions, insufficient physical understanding of manufacturing phenomena, and challenges adapting models from other domains. To address these challenges, we propose future research directions aimed at developing integrated frameworks that combine generative ML and control technologies to address the dynamic complexities of modern manufacturing systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subspace-Distance-Enabled Active Learning for Efficient Data-Driven Model Reduction of Parametric Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.00460</link>
<guid>https://arxiv.org/abs/2505.00460</guid>
<content:encoded><![CDATA[
<div> active learning, data-driven model reduction, parametric data-driven reduced-order model, proper orthogonal decomposition, subspace-distance-enabled active learning

Summary:<br />
In scenarios where high-fidelity dynamical systems require repetitive evaluation across a wide range of parameter configurations without access to governing equations, data-driven model reduction techniques are preferred. This study introduces an active learning method for constructing a parametric data-driven reduced-order model by selecting crucial parameter samples from the parameter domain. The approach involves representing high-fidelity solution snapshots in parameter-specific linear subspaces using proper orthogonal decomposition, with the relative distance between these subspaces guiding the active learning process. A distance metric is provided for comparing similarity between linear subspaces of different dimensions. The proposed subspace-distance-enabled active learning (SDE-AL) framework is successfully applied to enhance existing reduced-order modeling methods through active-learning-driven extensions. Positive results are demonstrated for two parametric physical models, showcasing the effectiveness of the SDE-AL approach.  <div>
arXiv:2505.00460v1 Announce Type: cross 
Abstract: In situations where the solution of a high-fidelity dynamical system needs to be evaluated repeatedly, over a vast pool of parametric configurations and in absence of access to the underlying governing equations, data-driven model reduction techniques are preferable. We propose a novel active learning approach to build a parametric data-driven reduced-order model (ROM) by greedily picking the most important parameter samples from the parameter domain. As a result, during the ROM construction phase, the number of high-fidelity solutions dynamically grow in a principled fashion. The high-fidelity solution snapshots are expressed in several parameter-specific linear subspaces, with the help of proper orthogonal decomposition (POD), and the relative distance between these subspaces is used as a guiding mechanism to perform active learning. For successfully achieving this, we provide a distance measure to evaluate the similarity between pairs of linear subspaces with different dimensions, and also show that this distance measure is a metric. The usability of the proposed subspace-distance-enabled active learning (SDE-AL) framework is demonstrated by augmenting two existing non-intrusive reduced-order modeling approaches, and providing their active-learning-driven (ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN. Furthermore, we report positive results for two parametric physical models, highlighting the efficiency of the proposed SDE-AL approach.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks</title>
<link>https://arxiv.org/abs/2505.00530</link>
<guid>https://arxiv.org/abs/2505.00530</guid>
<content:encoded><![CDATA[
<div> Keywords: SMILES-based molecule generation, deep reinforcement learning, catastrophic forgetting, molecule validity, exploration mechanisms

Summary:<br />
The article introduces a novel RL algorithm, Partial SMILES Validation-PPO (PSV-PPO), designed to address catastrophic forgetting during molecule generation in drug discovery. By incorporating real-time partial SMILES validation at each step of the sequence generation process, PSV-PPO prevents the deterioration of molecule validity while promoting exploration. Unlike traditional approaches that validate molecules only after completion, PSV-PPO evaluates potential branches at each step, ensuring high validity rates even during aggressive exploration. Experimental results on benchmark datasets show that PSV-PPO reduces invalid structures while maintaining competitive performance. The framework of PSV-PPO can be extended to incorporate additional domain knowledge, enhancing RL applications in drug discovery. <br /><br />Summary: <div>
arXiv:2505.00530v1 Announce Type: cross 
Abstract: SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMM-based DEX on the XRP Ledger</title>
<link>https://arxiv.org/abs/2312.13749</link>
<guid>https://arxiv.org/abs/2312.13749</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Market Maker, Decentralized Exchanges, XRP Ledger, Agent-based simulations, Continuous Auction Mechanism

Summary:
The study compares an Automated Market Maker (AMM)-based Decentralized Exchange (DEX) implementation on the XRP Ledger (XRPL) against a Generic AMM-based DEX on Ethereum. Through agent-based simulations using real market data, the XRPL-AMM-DEX demonstrates superior price synchronization, reduced slippage, and improved returns due to lower fees and shorter block times on the XRPL. The study also highlights the benefits of the integrated Continuous Auction Mechanism (CAM) in mitigating impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). This comparative analysis is the first to explore protocol-level and smart contract AMM-based DEX implementations and validate theoretical auction mechanisms through simulations. <div>
arXiv:2312.13749v4 Announce Type: replace 
Abstract: Automated Market Maker (AMM)-based Decentralized Exchanges (DEXs) are crucial in Decentralized Finance (DeFi), but Ethereum implementations suffer from high transaction costs and price synchronization challenges. To address these limitations, we compare the XRP Ledger (XRPL)-AMM-Decentralized Exchange (DEX), a protocol-level implementation, against a Generic AMM-based DEX (G-AMM-DEX) on Ethereum, akin to Uniswap's V2 AMM implementation, through agent-based simulations using real market data and multiple volatility scenarios generated via Geometric Brownian Motion (GBM). Results demonstrate that the XRPL-AMM-DEX achieves superior price synchronization, reduced slippage, and improved returns due to XRPL's lower fees and shorter block times, with benefits amplifying during market volatility. The integrated Continuous Auction Mechanism (CAM) further mitigates impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). To the best of our knowledge, this study represents the first comparative analysis between protocol-level and smart contract AMM-based DEX implementations and the first agent-based simulation validating theoretical auction mechanisms for AMM-based DEXs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing</title>
<link>https://arxiv.org/abs/2504.21317</link>
<guid>https://arxiv.org/abs/2504.21317</guid>
<content:encoded><![CDATA[
<div> redundancy, machine learning, additive manufacturing, process monitoring, mitigation

Summary:<br />
- Redundancy in machine learning-based additive manufacturing process monitoring systems can lead to increased costs, compromised performance, and high computational requirements.
- This paper defines redundancy at sample-level, feature-level, and model-level and proposes a multi-level redundancy mitigation framework.
- The framework includes methods such as data registration, downscaling, cross-modality knowledge transfer, and model pruning to reduce redundancy and improve model performance.
- In a case study for in-situ defect detection in directed energy deposition, the proposed approach showed a 91% reduction in latency, a 47% decrease in error rate, and a 99.4% reduction in storage requirements.
- The framework also allows for lower sensor costs and energy consumption, resulting in a lightweight, cost-effective, and scalable monitoring system.

<br /><br />Summary: <div>
arXiv:2504.21317v1 Announce Type: new 
Abstract: The deployment of machine learning (ML)-based process monitoring systems has significantly advanced additive manufacturing (AM) by enabling real-time defect detection, quality assessment, and process optimization. However, redundancy is a critical yet often overlooked challenge in the deployment and operation of ML-based AM process monitoring systems. Excessive redundancy leads to increased equipment costs, compromised model performance, and high computational requirements, posing barriers to industrial adoption. However, existing research lacks a unified definition of redundancy and a systematic framework for its evaluation and mitigation. This paper defines redundancy in ML-based AM process monitoring and categorizes it into sample-level, feature-level, and model-level redundancy. A comprehensive multi-level redundancy mitigation (MLRM) framework is proposed, incorporating advanced methods such as data registration, downscaling, cross-modality knowledge transfer, and model pruning to systematically reduce redundancy while improving model performance. The framework is validated through an ML-based in-situ defect detection case study for directed energy deposition (DED), demonstrating a 91% reduction in latency, a 47% decrease in error rate, and a 99.4% reduction in storage requirements. Additionally, the proposed approach lowers sensor costs and energy consumption, enabling a lightweight, cost-effective, and scalable monitoring system. By defining redundancy and introducing a structured mitigation framework, this study establishes redundancy analysis and mitigation as a key enabler of efficient ML-based process monitoring in production environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Analysis and Implementation of Cryptocurrency Systems on Blockchain 2.0</title>
<link>https://arxiv.org/abs/2504.21367</link>
<guid>https://arxiv.org/abs/2504.21367</guid>
<content:encoded><![CDATA[
<div> blockchain, decentralization, smart contracts, security, cryptocurrency
<br />
Summary: 
Blockchain technology has revolutionized decentralization by providing a trust system through cryptography and computing power. Smart contracts have further expanded blockchain application possibilities by enabling automatic execution based on predefined triggers. However, the programmability of smart contracts introduces security vulnerabilities. This article delves into the technical details of blockchain 2.0, focusing on Ethereum, and explains the operation principles of contract virtual machines. It discusses how cryptocurrencies are constructed and operated on blockchain 2.0, highlighting common security issues and solutions. Drawing on research and on-chain practices, this comprehensive perspective aims to enhance the understanding of cryptocurrency technology on blockchain 2.0 and offers insights for creating more secure cryptocurrency contracts. <div>
arXiv:2504.21367v1 Announce Type: new 
Abstract: Blockchain technology has set off a wave of decentralization in the world since its birth. The trust system constructed by blockchain technology based on cryptography algorithm and computing power provides a practical and powerful solution to solve the trust problem in human society. In order to make more convenient use of the characteristics of blockchain and build applications on it, smart contracts appear. By defining some trigger automatic execution contracts, the application space of blockchain is expanded and the foundation for the rapid development of blockchain is laid. This is blockchain 2.0. However, the programmability of smart contracts also introduces vulnerabilities. In order to cope with the insufficient security guarantee of high-value application networks running on blockchain 2.0 and smart contracts, this article will be represented by Ethereum to introduce the technical details of understanding blockchain 2.0 and the operation principle of contract virtual machines, and explain how cryptocurrencies based on blockchain 2.0 are constructed and operated. The common security problems and solutions are also discussed. Based on relevant research and on-chain practice, this paper provides a complete and comprehensive perspective to understanding cryptocurrency technology based on blockchain 2.0 and provides a reference for building more secure cryptocurrency contracts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level datasets training method in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2504.21328</link>
<guid>https://arxiv.org/abs/2504.21328</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, PDEs, Multi-grid method, High-frequency components <br />
Summary: Physics-Informed Neural Networks (PINNs) have shown promise in solving PDEs but struggle with stiff and high-frequency problems, leading to accuracy issues. An alternative approach inspired by the multi-grid method is proposed to address these challenges. By training with different levels of samples, errors of varying frequencies can be efficiently removed, improving accuracy without complex tuning. The method is tested on 1D and 2D equations with high-frequency components and Lid-driven cavity flows at different Reynolds numbers. Results show a 30% to 60% accuracy improvement and success in solving complex high-frequency PDEs up to Re=5000. Synergies with transfer learning are also explored for more challenging problems. <div>
arXiv:2504.21328v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks have emerged as a promising methodology for solving PDEs, gaining significant attention in computer science and various physics-related fields. Despite being demonstrated the ability to incorporate the physics of laws for versatile applications, PINNs still struggle with the challenging problems which are stiff to be solved and/or have high-frequency components in the solutions, resulting in accuracy and convergence issues. It may not only increase computational costs, but also lead to accuracy loss or solution divergence. In this study, an alternative approach is proposed to mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD community, the underlying idea of the current approach is to efficiently remove different frequency errors via training with different levels of training samples, resulting in a simpler way to improve the training accuracy without spending time in fine-tuning of neural network structures, loss weights as well as hyperparameters. To demonstrate the efficacy of current approach, we first investigate canonical 1D ODE with high-frequency component and 2D convection-diffusion equation with V-cycle training strategy. Finally, the current method is employed for the classical benchmark problem of steady Lid-driven cavity flows at different Reynolds numbers, to investigate the applicability and efficacy for the problem involved multiple modes of high and low frequency. By virtue of various training sequence modes, improvement through predictions lead to 30% to 60% accuracy improvement. We also investigate the synergies between current method and transfer learning techniques for more challenging problems (i.e., higher Re). From the present results, it also revealed that the current framework can produce good predictions even for the case of Re=5000, demonstrating the ability to solve complex high-frequency PDEs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation</title>
<link>https://arxiv.org/abs/2504.21574</link>
<guid>https://arxiv.org/abs/2504.21574</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, financial ecosystem, cybersecurity, ethical risks, regulatory landscape

Summary: 
Generative Artificial Intelligence (GenAI) is revolutionizing the financial sector by enhancing customer engagement, automating workflows, and extracting insights from vast data. This survey explores how banks, insurers, asset managers, and fintech firms globally are adopting GenAI tools like large language models. While driving innovation, GenAI poses cybersecurity threats like AI-generated phishing and ethical concerns around bias and data misuse. The evolving regulatory landscape is analyzed, including initiatives for risk-based AI governance by major financial regulators. Best practices for secure and responsible GenAI adoption are proposed, such as explainability techniques and human oversight. By drawing on academic research, industry cases, and policy frameworks, this chapter offers insights on leveraging GenAI's potential while managing its complex risks.<br /><br />Summary: <div>
arXiv:2504.21574v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence (GenAI) is rapidly reshaping the global financial landscape, offering unprecedented opportunities to enhance customer engagement, automate complex workflows, and extract actionable insights from vast financial data. This survey provides an overview of GenAI adoption across the financial ecosystem, examining how banks, insurers, asset managers, and fintech startups worldwide are integrating large language models and other generative tools into their operations. From AI-powered virtual assistants and personalized financial advisory to fraud detection and compliance automation, GenAI is driving innovation across functions. However, this transformation comes with significant cybersecurity and ethical risks. We discuss emerging threats such as AI-generated phishing, deepfake-enabled fraud, and adversarial attacks on AI systems, as well as concerns around bias, opacity, and data misuse. The evolving global regulatory landscape is explored in depth, including initiatives by major financial regulators and international efforts to develop risk-based AI governance. Finally, we propose best practices for secure and responsible adoption - including explainability techniques, adversarial testing, auditability, and human oversight. Drawing from academic literature, industry case studies, and policy frameworks, this chapter offers a perspective on how the financial sector can harness GenAI's transformative potential while navigating the complex risks it introduces.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of HPC-Accelerated CFD in National Security and Defense</title>
<link>https://arxiv.org/abs/2504.07837</link>
<guid>https://arxiv.org/abs/2504.07837</guid>
<content:encoded><![CDATA[
<div> HPC, Computational Fluid Dynamics, defense applications, open-source frameworks, MPI domain decomposition, GPU acceleration, hybrid parallelism, research voids, exascale readiness, machine learning surrogate models<br />
Summary: <br />
This review discusses the use of High-Performance Computing (HPC) and Computational Fluid Dynamics (CFD) in defense-related national security applications. It examines the utilization of open-source CFD frameworks such as OpenFOAM, SU2, and ADflow in security-sensitive simulations. The review also explores how HPC techniques like MPI domain decomposition and GPU acceleration, along with hybrid parallelism, enhance open-source frameworks for managing large defense CFD simulations. Additionally, it addresses the technological advancements and research voids driving the field's development, categorizing scientific contributions into air, maritime, and space domains. The review highlights modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers, showcasing how custom open-source solutions support workflows for rapid completion of multi-million cell simulations. Finally, it discusses new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to improve CFD use in defense research and development. <div>
arXiv:2504.07837v2 Announce Type: replace 
Abstract: Using High-Performance Computing (HPC), Computational Fluid Dynamics (CFD) now serves as an essential component in defense-related national security applications including missile interception and hypersonic propulsion as well as naval stealth optimization and urban hazard dispersion. This review combines two decades of open-source and public-domain research on HPC-accelerated CFD in defense, addressing three key questions: Which security-sensitive simulations have utilized open-source CFD frameworks such as OpenFOAM, SU2 and ADflow? Which HPC techniques, such as MPI domain decomposition and GPU acceleration together with hybrid parallelism best enhance open-source frameworks to manage large defense CFD simulations? Which technological advancements and research voids currently drive the directional development of the field? Examining several research studies sourced from NASA, DoD HPC centers, and academic institutions, scientific contributions have been classified into air, maritime, and space domains. Modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers show how custom open-source solutions support workflows with rapid completion of multi-million cell simulations. The conclusion highlights new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to deliver practical insights for improving CFD use in defense research and development.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews</title>
<link>https://arxiv.org/abs/2502.05439</link>
<guid>https://arxiv.org/abs/2502.05439</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic systems, financial services industry, modeling crew, model risk management, autonomous decision-making<br />
Summary: 
This paper delves into the utilization of agentic systems in the financial services sector, employing teams of artificial intelligence agents to carry out intricate modeling and model risk management tasks with human oversight. The modeling crew, comprising a judge agent and specialized agents, executes a spectrum of activities from data analysis to model evaluation. Likewise, the model risk management crew, under the watchful eye of a judge agent, ensures compliance, replicability, and soundness of models. Numerical examples applied in credit card fraud detection, credit card approval, and portfolio credit risk modeling demonstrate the efficacy and reliability of these agentic crews. The integration of human judgment with machine efficiency showcases the potential of agentic systems in driving innovation and automation in financial decision-making processes.<br /><br />Summary: <div>
arXiv:2502.05439v2 Announce Type: replace-cross 
Abstract: The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews with human-in-the-loop module that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a judge agent and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection/hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a judge agent along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLiB: High-fidelity differentiable modeling of lithium-ion batteries and efficient gradient-based parameter identification</title>
<link>https://arxiv.org/abs/2504.20674</link>
<guid>https://arxiv.org/abs/2504.20674</guid>
<content:encoded><![CDATA[
<div> DiffLiB, LIB simulation, gradient-based inverse parameter identification, automatic differentiation, advanced computational framework<br />
Summary:<br />
DiffLiB is introduced as a high-fidelity LIB simulation framework that utilizes advanced differentiable programming techniques for efficient gradient-based inverse parameter identification in the complex DFN model. Customized automatic differentiation rules are defined to enable efficient gradient-based optimization, improving computational performance significantly compared to gradient-free methods. The framework shows excellent agreement in forward predictions, maintaining low terminal voltage discrepancies. In parameter identification tasks using measured voltage data, DiffLiB demonstrates superior computational performance with significantly fewer forward predictions and less computational time required. These results highlight DiffLiB as a versatile and powerful computational tool for advanced LIB development. <br /> <div>
arXiv:2504.20674v1 Announce Type: new 
Abstract: The physics-based Doyle-Fuller-Newman (DFN) model, widely adopted for its precise electrochemical modeling, stands out among various simulation models of lithium-ion batteries (LIBs). Although the DFN model is powerful in forward predictive analysis, the inverse identification of its model parameters has remained a long-standing challenge. The numerous unknown parameters associated with the nonlinear, time-dependent, and multi-scale DFN model are extremely difficult to be determined accurately and efficiently, hindering the practical use of such battery simulation models in industrial applications. To tackle this challenge, we introduce DiffLiB, a high-fidelity finite-element-based LIB simulation framework, equipped with advanced differentiable programming techniques so that efficient gradient-based inverse parameter identification is enabled. Customized automatic differentiation rules are defined by identifying the VJP (vector-Jacobian product) structure in the chain rule and implemented using adjoint-based implicit differentiation methods. Four numerical examples, including both 2D and 3D forward predictions and inverse parameter identification, are presented to validate the accuracy and computational efficiency of DiffLiB. Benchmarking against COMSOL demonstrates excellent agreement in forward predictions, with terminal voltage discrepancies maintaining a root-mean-square error (RMSE) below 2 mV across all test conditions. In parameter identification tasks using experimentally measured voltage data, the proposed gradient-based optimization scheme achieves superior computational performance, with 96% fewer forward predictions and 72% less computational time compared with gradient-free approaches. These results demonstrate that DiffLiB is a versatile and powerful computational framework for the development of advanced LIBs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Heterogeneity within Elastic and Inelastic Discrete Mechanical Models</title>
<link>https://arxiv.org/abs/2504.20861</link>
<guid>https://arxiv.org/abs/2504.20861</guid>
<content:encoded><![CDATA[
<div> Keywords: heterogeneous media, elastic behavior, fracture behavior, homogenization, randomization <br />
Summary: 
The study examines the elastic and fracture behaviors of discrete models of heterogeneous media with different forms of randomization. These models achieve homogeneity through volumetric-deviatoric decomposition or stress homogenization methods. It is observed that stress oscillations in heterogeneous geometric structures cannot be accurately replicated by randomizing elastic parameters in homogeneous models. Additionally, the macroscopic response to uniaxial tension shows differences between homogenized and standard materials, with the homogenized material exhibiting higher peak stress and steeper softening. However, randomizing elastic parameters and adjusting inelastic parameters can bring the macroscopic response closer to the standard material, despite differing damage distributions. This research sheds light on the potential for controlled random assignment of heterogeneity in homogeneous models and provides valuable insights into the behavior of these materials. <br /><br /> <div>
arXiv:2504.20861v1 Announce Type: new 
Abstract: The study investigates the elastic and fracture behaviors of discrete, elastically homogeneous models of heterogeneous media. The homogeneity is accomplished either by volumetric-deviatoric decomposition of constitutive function or by an auxiliary stress homogenization method. The elastic parameters of the homogenized material models are randomly varied in space to introduce heterogeneity independently of the geometric properties of the discrete model. Several forms of randomization are investigated using statistical properties of nodal stress oscillations in periodic representative volume elements (RVEs). It is found that the stress oscillations present in discrete models built on heterogeneous geometric structures with standard constitutive models cannot be replicated by randomization of the elastically homogeneous discrete system. The marginal distributions as well as dependencies between stress tensor components cannot be adequately matched.
  With respect to quasi-brittle fracture behavior, the macroscopic response of the different models is studied for the load case of uniaxial tension. The elastically homogenized material provides higher peak stress occurring at lower strain levels and a steeper softening phase, compared to the standard material. Randomization of the elastic material parameters, as well as adjustment of inelastic material parameters, brings the macroscopic response of the homogenized material close to that of the standard material, although the damage distribution prior to the strain localization differs. These findings provide insight into the potential for controlled, random assignment of heterogeneity in homogeneous models, using physically-based discretizations of material structure with standard constitutive models for comparison.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale modelling of thermally stressed superelastic polyimide</title>
<link>https://arxiv.org/abs/2504.20123</link>
<guid>https://arxiv.org/abs/2504.20123</guid>
<content:encoded><![CDATA[
<div> Keywords: thermo-mechanical processes, superelastic polyimide, multiscale approach, smoothed particle hydrodynamics, molecular dynamics

Summary: 
Thermo-mechanical processes at the atomistic scale in superelastic polyimide are studied using a sequential multiscale approach. The continuum-scale smoothed particle hydrodynamics (SPH) model is coupled with atomistic molecular dynamics (MD) to investigate thermal expansion and stress relaxation. Constitutive modelling integrates thermo-mechanical properties derived from MD simulations. Benchmark tests on heat transfer validate the results. Simulation of the insulation capabilities of superelastic polyimide on an aluminium plate shows significant reduction in thermal stress, strain, and temperature development. The multiscale method effectively captures thermo-mechanical interactions in superelastic polyimide, demonstrating its potential for exploring material behavior under thermal stress. <div>
arXiv:2504.20123v1 Announce Type: cross 
Abstract: Many thermo-mechanical processes, such as thermal expansion and stress relaxation, originate at the atomistic scale. We develop a sequential multiscale approach to study thermally stressed superelastic polyimide to explore these effects. The continuum-scale smoothed particle hydrodynamics (SPH) model is coupled with atomistic molecular dynamics (MD) through constitutive modelling, where thermo-mechanical properties and equations of state are derived from MD simulations. The results are verified through benchmark problems of heat transfer. Finally, we analyse the insulating capabilities of superelastic polyimide by simulating the thermal response of an aluminium plate. The result shows a considerable reduction in the thermal stress, strain and temperature field development in the aluminium plate when superelastic polyimide is used as an insulator. The present work demonstrates the effectiveness of the multi-scale method in capturing thermo-mechanical interactions in superelastic polyimide.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling</title>
<link>https://arxiv.org/abs/2504.20586</link>
<guid>https://arxiv.org/abs/2504.20586</guid>
<content:encoded><![CDATA[
<div> Monte Carlo method, capacitance extraction, variance reduction, layout-dependent effects, floating random walk <br />
<br />
Summary: 
The article introduces a new variance reduction method for floating random walk-based capacitance extraction, a method commonly used in integrated circuit analysis. This new method, designed to address challenges in ever-denser process technologies and layout-dependent effects, offers a significant improvement in extraction efficiency. It complements existing mathematical formulations for variance reduction and has been shown to reduce variance in all extractions, particularly those affected by layout-dependent effects. Numerical experiments demonstrate that the new method can decrease the number of walks required by up to 30% and reduce overall extraction times even further compared to previously proposed variance reduction techniques for floating random walks. This advancement in capacitance extraction techniques could have a significant impact on the efficiency and accuracy of integrated circuit analysis for complex designs. <br /> <div>
arXiv:2504.20586v1 Announce Type: cross 
Abstract: Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 30% in number of walks necessary and even more in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open FinLLM Leaderboard: Towards Financial AI Readiness</title>
<link>https://arxiv.org/abs/2501.10963</link>
<guid>https://arxiv.org/abs/2501.10963</guid>
<content:encoded><![CDATA[
<div> Keywords: FinLLMs, multimodal capabilities, open leaderboard, financial tasks, AI models performance <br />
Summary: Financial large language models (FinLLMs), equipped with multimodal capabilities, are poised to transform various applications in business, finance, accounting, and auditing. To drive real-world adoption, robust benchmarks for assessing FinLLMs' and FinAgents' performance are essential. An open FinLLM leaderboard, established in collaboration with Linux Foundation and Hugging Face, offers a platform for evaluating and comparing AI models across a wide range of financial tasks. By democratizing access to financial knowledge and intelligence, these advancements can empower chatbots or agents to elevate individuals' analytical proficiency to a professional level in a matter of months. The open leaderboard is inclusive of contributions from academia, the open-source community, industry, and stakeholders, with a focus on continually updating with new datasets, tasks, and models. By fostering a collaborative and transparent ecosystem, the aim is to advance readiness for financial AI solutions. <br /><br />Summary: <div>
arXiv:2501.10963v2 Announce Type: replace 
Abstract: Financial large language models (FinLLMs) with multimodal capabilities are envisioned to revolutionize applications across business, finance, accounting, and auditing. However, real-world adoption requires robust benchmarks of FinLLMs' and FinAgents' performance. Maintaining an open leaderboard is crucial for encouraging innovative adoption and improving model effectiveness. In collaboration with Linux Foundation and Hugging Face, we create an open FinLLM leaderboard, which serves as an open platform for assessing and comparing AI models' performance on a wide spectrum of financial tasks. By demoncratizing access to advances of financial knowledge and intelligence, a chatbot or agent may enhance the analytical capabilities of the general public to a professional level within a few months of usage. This open leaderboard welcomes contributions from academia, open-source community, industry, and stakeholders. In particular, we encourage contributions of new datasets, tasks, and models for continual update. Through fostering a collaborative and open ecosystem, we seek to promote financial AI readiness.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression</title>
<link>https://arxiv.org/abs/2504.18925</link>
<guid>https://arxiv.org/abs/2504.18925</guid>
<content:encoded><![CDATA[
<div> Neural Voxel Contextual Coding, Vector Quantization Contextual Coding, 4DGS data compression, multi-rate compression, storage reduction<br />
Summary:<br />
This study introduces 4DGS-CC, a contextual coding framework for compressing 4D Gaussian Splatting (4DGS) data to address storage challenges. The framework decomposes the 4DGS data into 4D neural voxels and 3DGS components for efficient compression. It leverages Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC) to compress the data, achieving a storage reduction of approximately 12 times while maintaining rendering fidelity. The approach separates temporal and spatial dimensions in the data decomposition process and utilizes prior information for contextual coding using NVCC. Additionally, a codebook is employed to store spherical harmonics information from canonical 3DGS, which is compressed using VQCC with auxiliary hyperpriors. The integrated NVCC and VQCC enable tailored multi-rate compression of 4DGS data, making it suitable for specific storage requirements. Extensive experiments validate the effectiveness of the proposed method in achieving significant storage savings without compromising data quality. <br /><br /> <div>
arXiv:2504.18925v1 Announce Type: new 
Abstract: Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints.Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively.Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding.Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook.By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error</title>
<link>https://arxiv.org/abs/2504.19963</link>
<guid>https://arxiv.org/abs/2504.19963</guid>
<content:encoded><![CDATA[
<div> probabilistic model, subspaces, principal component analysis, model reduction, computational mechanics
<br />
Summary:<br />
This paper introduces a probabilistic model of subspaces based on probabilistic principal component analysis (PCA). The method uses quantities derived from probabilistic PCA to construct distributions of the sample matrix and the principal subspaces in an embedding space. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition. The constructed stochastic subspace can help characterize model-form uncertainty in computational mechanics. The method is justified by probabilistic PCA, satisfying linear constraints like boundary conditions, and has only one hyperparameter, simplifying training. The algorithm is easy to implement. Comparisons with existing approaches show promising results in low-dimensional visualization, parametric static problems, and dynamics modeling of space structures. <div>
arXiv:2504.19963v1 Announce Type: new 
Abstract: This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We compare the proposed method with existing approaches in a low-dimensional visualization example and a parametric static problem, and demonstrate its performance in a dynamics model of a space structure.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantBench: Benchmarking AI Methods for Quantitative Investment</title>
<link>https://arxiv.org/abs/2504.18600</link>
<guid>https://arxiv.org/abs/2504.18600</guid>
<content:encoded><![CDATA[
<div> benchmark, artificial intelligence, quantitative investment, industry practices, QuantBench  
Summary:  
QuantBench is introduced as a benchmark platform for AI in quantitative investment, aiming to bridge the gap between academic research and industry practices. It offers standardization aligned with industry standards, flexibility for integrating AI algorithms, and full coverage of the quantitative investment process. Using QuantBench, empirical studies highlight the importance of continual learning to address distribution shifts, improved methods for modeling relational financial data, and robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common platform for evaluation, QuantBench fosters collaboration between researchers and practitioners, accelerating progress in the field of AI for quantitative investment, similar to the impact of benchmark platforms in other domains such as computer vision and natural language processing. <div>
arXiv:2504.18600v1 Announce Type: cross 
Abstract: The field of artificial intelligence (AI) in quantitative investment has seen significant advancements, yet it lacks a standardized benchmark aligned with industry practices. This gap hinders research progress and limits the practical application of academic innovations. We present QuantBench, an industrial-grade benchmark platform designed to address this critical need. QuantBench offers three key strengths: (1) standardization that aligns with quantitative investment industry practices, (2) flexibility to integrate various AI algorithms, and (3) full-pipeline coverage of the entire quantitative investment process. Our empirical studies using QuantBench reveal some critical research directions, including the need for continual learning to address distribution shifts, improved methods for modeling relational financial data, and more robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common ground for evaluation and fostering collaboration between researchers and practitioners, QuantBench aims to accelerate progress in AI for quantitative investment, similar to the impact of benchmark platforms in computer vision and natural language processing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Open-Source Software Is Less Likely to Become Abandoned Than One Might Think! Lessons from Curating a Catalog of Maintained Scientific Software</title>
<link>https://arxiv.org/abs/2504.18971</link>
<guid>https://arxiv.org/abs/2504.18971</guid>
<content:encoded><![CDATA[
<div> classification, scientific software, longevity, survival models, open-source

Summary:
Using large language models, the study classifies over 18,000 scientific software projects, analyzing their attributes to understand factors affecting longevity. Infrastructural layers, downstream dependencies, publication mentions, and government participants are linked to longer lifespans, while newer projects with academic participants have shorter lifespans. Despite common perceptions, scientific projects have a longer lifespan than non-scientific open-source projects. The curated dataset provides a valuable resource for future research on scientific software, offering insights that could help prolong the lifespan of both scientific and non-scientific software projects. <div>
arXiv:2504.18971v1 Announce Type: cross 
Abstract: Scientific software is essential to scientific innovation and in many ways it is distinct from other types of software. Abandoned (or unmaintained), buggy, and hard to use software, a perception often associated with scientific software can hinder scientific progress, yet, in contrast to other types of software, its longevity is poorly understood. Existing data curation efforts are fragmented by science domain and/or are small in scale and lack key attributes. We use large language models to classify public software repositories in World of Code into distinct scientific domains and layers of the software stack, curating a large and diverse collection of over 18,000 scientific software projects. Using this data, we estimate survival models to understand how the domain, infrastructural layer, and other attributes of scientific software affect its longevity. We further obtain a matched sample of non-scientific software repositories and investigate the differences. We find that infrastructural layers, downstream dependencies, mentions of publications, and participants from government are associated with a longer lifespan, while newer projects with participants from academia had shorter lifespan. Against common expectations, scientific projects have a longer lifetime than matched non-scientific open-source software projects. We expect our curated attribute-rich collection to support future research on scientific software and provide insights that may help extend longevity of both scientific and other projects.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spreading of highly cohesive metal powders with transverse oscillation kinematics</title>
<link>https://arxiv.org/abs/2504.18981</link>
<guid>https://arxiv.org/abs/2504.18981</guid>
<content:encoded><![CDATA[
<div> powder bed additive manufacturing, laser powder bed fusion, binder jetting, transverse oscillation kinematic, dense powder layers<br />
<br />
Summary: 
The study examines the challenges of spreading fine powders in powder bed additive manufacturing processes and proposes a transverse oscillation kinematic for powder spreading. Computational simulations using a DEM-FEM framework show that transverse oscillation of a non-rotating roller can facilitate the spreading of dense powder layers with high packing fractions. Experimental validation confirms the computational results, with high packing fractions achieved for transverse oscillation frequencies above 200 Hz. Statistical analysis demonstrates that increasing transverse surface velocity improves layer uniformity and reduces cracking defects. The proposed transverse oscillation kinematic has the potential to produce thin and consistently uniform powder layers in additive manufacturing processes, offering a promising solution for handling highly cohesive powders. <br /><br /> <div>
arXiv:2504.18981v1 Announce Type: cross 
Abstract: Powder bed additive manufacturing processes such as laser powder bed fusion (LPBF) or binder jetting (BJ) benefit from using fine (D50 $\leq20~\mu m$) powders. However, the increasing level of cohesion with decreasing particle size makes spreading a uniform and continuous layer challenging. As a result, LPBF typically employs a coarser size distribution, and rotating roller mechanisms are used in BJ machines, that can create wave-like surface profiles due to roller run-out.
  In this work, a transverse oscillation kinematic for powder spreading is proposed, explored computationally, and validated experimentally. Simulations are performed using an integrated discrete element-finite element (DEM-FEM) framework and predict that transverse oscillation of a non-rotating roller facilitates the spreading of dense powder layers (beyond 50% packing fraction) with a high level of robustness to kinematic parameters. The experimental study utilizes a custom-built mechanized powder spreading testbed and X-ray transmission imaging for the analysis of spread powder layers. Experimental results generally validate the computational results, however, also exhibit parasitic layer cracking. For transverse oscillation frequencies above 200 Hz, powder layers of high packing fraction (between 50-60%) were formed, and for increased layer thicknesses, highly uniform and continuous layers were deposited. Statistical analysis of the experimental powder layer morphology as a function of kinematic spreading parameters revealed that an increasing transverse surface velocity improves layer uniformity and reduces cracking defects. This suggests that with minor improvements to the machine design, the proposed transverse oscillation kinematic has the potential to result in thin and consistently uniform powder layers of highly cohesive powder.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Bayesian Optimal Experimental Design with Normalizing Flows</title>
<link>https://arxiv.org/abs/2404.13056</link>
<guid>https://arxiv.org/abs/2404.13056</guid>
<content:encoded><![CDATA[
<div> Bayesian optimal experimental design, variational OED, normalizing flows, Monte Carlo estimators, gradient-based optimization<br />
<br />
Summary: The study introduces a novel approach, vOED-NFs, which utilizes normalizing flows (NFs) to enhance variational optimal experimental design (vOED) by approximating posterior distributions. The method employs NFs with a conditional invertible neural network architecture and includes a summary network for data dimension reduction. Monte Carlo estimators and gradient expressions enable simultaneous optimization of variational parameters and design variables. The algorithm is validated on benchmark problems and applied to scenarios involving cathodic electrophoretic deposition and stochastic modeling of aphid population. Results demonstrate that a composition of 4-5 coupling layers reduces EIG estimation bias, while NFs provide accurate approximations of posterior distributions, effectively capturing non-Gaussian and multi-modal features. The vOED-NFs approach offers a computationally efficient and accurate method for Bayesian optimal experimental design without the need for explicit likelihood evaluations. <br /><br /> <div>
arXiv:2404.13056v2 Announce Type: replace-cross 
Abstract: Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Modeling of Lipid Nanoparticle Formation for the Delivery of Nucleic Acid Therapeutics</title>
<link>https://arxiv.org/abs/2408.08577</link>
<guid>https://arxiv.org/abs/2408.08577</guid>
<content:encoded><![CDATA[
<div> Keywords: nucleic acids, lipid nanoparticles, mechanistic modeling, process development, process control<br />
Summary: 
Nucleic acids, including mRNA, are a promising therapeutic modality for treating various diseases. Lipid nanoparticles (LNPs) have been used as a delivery system for nucleic acids in COVID-19 vaccines. However, understanding the formation and structure of LNPs is challenging, especially during scale-up of manufacturing processes. Mathematical and computational methods offer a way to improve understanding of LNP formation and aid in process development and control. This article discusses strategies for mechanistic modeling of LNP formation, starting with predicting important physicochemical properties of the species involved. It outlines a framework for constructing models of reactor- and particle-scale processes, linking insights from the models to product quality attributes and process understanding. Finally, the article explores using these models to guide advanced process control and optimization strategies.<br /><br />Summary: <div>
arXiv:2408.08577v2 Announce Type: replace-cross 
Abstract: Nucleic acids such as mRNA have emerged as a promising therapeutic modality with the capability of addressing a wide range of diseases. Lipid nanoparticles (LNPs) as a delivery platform for nucleic acids were used in the COVID-19 vaccines and have received much attention. While modern manufacturing processes which involve rapidly mixing an organic stream containing the lipids with an aqueous stream containing the nucleic acids are conceptually straightforward, detailed understanding of LNP formation and structure is still limited and scale-up can be challenging. Mathematical and computational methods are a promising avenue for deepening scientific understanding of the LNP formation process and facilitating improved process development and control. This article describes strategies for the mechanistic modeling of LNP formation, starting with strategies to estimate and predict important physicochemical properties of the various species such as diffusivities and solubilities. Subsequently, a framework is outlined for constructing mechanistic models of reactor- and particle-scale processes. Insights gained from the various models are mapped back to product quality attributes and process insights. Lastly, the use of the models to guide development of advanced process control and optimization strategies is discussed.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatternPaint: Practical Layout Pattern Generation Using Diffusion-Based Inpainting</title>
<link>https://arxiv.org/abs/2409.01348</link>
<guid>https://arxiv.org/abs/2409.01348</guid>
<content:encoded><![CDATA[
<div> diffusion-based framework, VLSI layout patterns, design for manufacturing, few-shot finetuning, technology nodes 
Summary:
PatternPaint is a diffusion-based framework designed for generating diverse VLSI layout patterns essential for design for manufacturing. It addresses the challenge of limited design-rule-compliant training samples and simplifies complex layout pattern generation through inpainting processes. The framework incorporates a template-based denoising scheme and utilizes few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results demonstrate the effectiveness of PatternPaint in generating legal patterns in complex 2D metal interconnect design rule settings for sub-3nm technology nodes. It achieves high diversity scores and improves legality rates significantly through few-shot finetuning. This approach offers a production-ready solution for layout pattern generation in the development of new technology nodes. 
<br /><br />Summary: <div>
arXiv:2409.01348v4 Announce Type: replace-cross 
Abstract: Generating diverse VLSI layout patterns is essential for various downstream tasks in design for manufacturing, as design rules continually evolve during the development of new technology nodes. However, existing training-based methods for layout pattern generation rely on large datasets. In practical scenarios, especially when developing a new technology node, obtaining such extensive layout data is challenging. Consequently, training models with large datasets becomes impractical, limiting the scalability and adaptability of prior approaches. To this end, we propose PatternPaint, a diffusion-based framework capable of generating legal patterns with limited design-rule-compliant training samples. PatternPaint simplifies complex layout pattern generation into a series of inpainting processes with a template-based denoising scheme. Furthermore, we perform few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results show that using a sub-3nm technology node (Intel 18A), our model is the only one that can generate legal patterns in complex 2D metal interconnect design rule settings among all previous works and achieves a high diversity score. Additionally, our few-shot finetuning can boost the legality rate with 1.87X improvement compared to the original pretrained model. As a result, we demonstrate a production-ready approach for layout pattern generation in developing new technology nodes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets</title>
<link>https://arxiv.org/abs/2410.13105</link>
<guid>https://arxiv.org/abs/2410.13105</guid>
<content:encoded><![CDATA[
<div> Decentralized Finance, DeFi, lending, algorithm-driven, liquidity pools <br />
<br />
Summary: 
This work proposes a dynamic model for the DeFi lending market, incorporating evolving demand and supply curves and an adaptive interest rate controller to respond to market changes in real-time. The Recursive Least Squares algorithm is used to track external market conditions, ensuring stable utilization and managing default and liquidation risks. The algorithm provides theoretical guarantees on interest rate convergence and utilization stability while also addressing vulnerability to adversarial manipulation. Two approaches are proposed to mitigate manipulation: detecting extreme fluctuations and enhancing elasticity through interest rate derivative markets. The dynamic model shows low error rates on real data and the interest rate controller outperforms static curve protocols in optimizing utilization and reducing liquidations. <div>
arXiv:2410.13105v4 Announce Type: replace-cross 
Abstract: Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller tracks the external market and achieves stable utilization, while also controlling default and liquidation risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. We propose two complementary approaches to mitigating adversarial manipulation: an algorithmic method that detects extreme demand and supply fluctuations and a market-based strategy that enhances elasticity, potentially via interest rate derivative markets. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Evaluation of Variational Quantum Eigensolver and Quantum Dynamics Algorithms on the Advection-Diffusion Equation</title>
<link>https://arxiv.org/abs/2503.24045</link>
<guid>https://arxiv.org/abs/2503.24045</guid>
<content:encoded><![CDATA[
<div> Variational Quantum Eigensolver, linear advection-diffusion equation, quantum algorithms, partial differential equations, noiseless simulation<br />
<br />
Summary: 
The study investigates the performance of quantum algorithms in solving a linear one-dimensional advection-diffusion equation. It compares Variational Quantum Eigensolver (VQE) with Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS) on small quantum hardware. While VQE on a noiseless simulator achieves high accuracy with low infidelities, the dynamics algorithms suffer from errors due to noise and limited shot statistics on hardware. VQE outperforms the dynamics methods in terms of accuracy as it reaches low infidelities with moderate circuit depths. The comparison provides insights into the accuracy and resource demands of different algorithms for solving partial differential equations. The study concludes with a discussion on potential extensions to higher-dimensional and nonlinear PDEs relevant to engineering and finance. <br /><br /> <div>
arXiv:2503.24045v2 Announce Type: replace-cross 
Abstract: We investigate the potential of near-term quantum algorithms for solving partial differential equations (PDEs), focusing on a linear one-dimensional advection-diffusion equation as a test case. This study benchmarks a ground-state algorithm, Variational Quantum Eigensolver (VQE), against three leading quantum dynamics algorithms, Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS), applied to the same PDE on small quantum hardware. While Trotterization is fully quantum, VarQTE and AVQDS are variational algorithms that reduce circuit depth for noisy intermediate-scale quantum (NISQ) devices. However, hardware results from these dynamics methods show sizable errors due to noise and limited shot statistics. To establish a noise-free performance baseline, we implement the VQE-based solver on a noiseless statevector simulator. Our results show VQE can reach final-time infidelities as low as ${O}(10^{-9})$ with $N=4$ qubits and moderate circuit depths, outperforming hardware-deployed dynamics methods that show infidelities $\gtrsim 10^{-1}$. By comparing noiseless VQE to shot-based and hardware-run algorithms, we assess their accuracy and resource demands, providing a baseline for future quantum PDE solvers. We conclude with a discussion of limitations and potential extensions to higher-dimensional, nonlinear PDEs relevant to engineering and finance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTFinRAG: Interactive Modularized Financial RAG Benchmark</title>
<link>https://arxiv.org/abs/2504.18024</link>
<guid>https://arxiv.org/abs/2504.18024</guid>
<content:encoded><![CDATA[
<div> Keywords: financial sectors, language model technologies, SMARTFinRAG, evaluation paradigm, open-source architecture<br />
Summary: SMARTFinRAG is a new platform designed to address gaps in assessing specialized RAG systems in the financial sector. It introduces a modular architecture that allows components to be interchanged during runtime, a document-centric evaluation paradigm that creates domain-specific QA pairs from financial documents, and an intuitive interface to facilitate research-implementation integration. The evaluation of SMARTFinRAG shows variations in retrieval efficacy and response quality across different configurations. The platform's open-source architecture promotes transparent and reproducible research, while also helping financial institutions overcome deployment challenges when implementing RAG systems. This innovative platform aims to enhance the adoption and evaluation of language model technologies in the financial industry. <br /><br /> <div>
arXiv:2504.18024v1 Announce Type: new 
Abstract: Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Governing Equations of Geomagnetic Storm Dynamics with Symbolic Regression</title>
<link>https://arxiv.org/abs/2504.18461</link>
<guid>https://arxiv.org/abs/2504.18461</guid>
<content:encoded><![CDATA[
<div> solar wind, geomagnetic storms, Disturbance Storm Time index, symbolic regression, PySR framework

Summary:
The study focuses on using symbolic regression to create data-driven equations that describe the behavior of the Disturbance Storm Time (Dst) index during geomagnetic storms. By analyzing historical data from the NASA OMNIweb database, including various solar wind parameters, the study aims to develop models that accurately predict the evolution of the Dst index. The models generated by the PySR framework showcase a hierarchy of complexity levels and outperform traditional empirical models in terms of accuracy and interpretability. The evaluation of the models on historical storm events, such as the 2003 Halloween Storm and the 2015 St. Patrick's Day Storm, demonstrates their effectiveness in capturing nonlinear dependencies and thresholding effects in Dst evolution. Overall, the study provides valuable insights into the mechanisms driving geomagnetic storms and offers interpretable mathematical expressions for predicting their intensity. 

<br /><br />Summary: <div>
arXiv:2504.18461v1 Announce Type: new 
Abstract: Geomagnetic storms are large-scale disturbances of the Earth's magnetosphere driven by solar wind interactions, posing significant risks to space-based and ground-based infrastructure. The Disturbance Storm Time (Dst) index quantifies geomagnetic storm intensity by measuring global magnetic field variations. This study applies symbolic regression to derive data-driven equations describing the temporal evolution of the Dst index. We use historical data from the NASA OMNIweb database, including solar wind density, bulk velocity, convective electric field, dynamic pressure, and magnetic pressure. The PySR framework, an evolutionary algorithm-based symbolic regression library, is used to identify mathematical expressions linking dDst/dt to key solar wind. The resulting models include a hierarchy of complexity levels and enable a comparison with well-established empirical models such as the Burton-McPherron-Russell and O'Brien-McPherron models. The best-performing symbolic regression models demonstrate superior accuracy in most cases, particularly during moderate geomagnetic storms, while maintaining physical interpretability. Performance evaluation on historical storm events includes the 2003 Halloween Storm, the 2015 St. Patrick's Day Storm, and a 2017 moderate storm. The results provide interpretable, closed-form expressions that capture nonlinear dependencies and thresholding effects in Dst evolution.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAQGA: A Quantum-Enhanced Genetic Algorithm with Novel Entanglement-Aware Crossovers</title>
<link>https://arxiv.org/abs/2504.17923</link>
<guid>https://arxiv.org/abs/2504.17923</guid>
<content:encoded><![CDATA[
<div> genetic algorithms, combinatorial optimization, portfolio optimization, quantum computing, quantum circuits

Summary:
- Genetic algorithms are effective in complex optimization problems, such as portfolio optimization.
- Quantum computing can address challenging tasks, and quantum genetic algorithms combine the benefits of both approaches.
- The proposed quantum genetic algorithm introduces a novel crossover strategy generating quantum circuits from binary solutions.
- It encodes entanglement patterns from parent solutions to enhance performance without significantly increasing circuit depth.
- Testing on a portfolio optimization problem using IBM's 127 qubits Eagle processor and simulators shows a significant improvement in fitness values compared to classical and quantum-inspired genetic algorithms, highlighting the potential of quantum computers in solving real-world combinatorial optimization problems.<br /><br /> <div>
arXiv:2504.17923v1 Announce Type: cross 
Abstract: Genetic algorithms are highly effective optimization techniques for many computationally challenging problems, including combinatorial optimization tasks like portfolio optimization. Quantum computing has also shown potential in addressing these complex challenges. Combining these approaches, quantum genetic algorithms leverage the principles of superposition and entanglement to enhance the performance of classical genetic algorithms. In this work, we propose a novel quantum genetic algorithm introducing an innovative crossover strategy to generate quantum circuits from a binary solution. We incorporate a heuristic method to encode entanglement patterns from parent solutions into circuits for the next generation. Our algorithm advances quantum genetic algorithms by utilizing a limited number of entanglements, enabling efficient exploration of optimal solutions without significantly increasing circuit depth, making it suitable for near-term applications. We test this approach on a portfolio optimization problem using an IBM 127 qubits Eagle processor (ibm_quebec) and simulators. Compared to state-of-the-art algorithms, our results show that the proposed method improves fitness values by 33.6% over classical genetic algorithm and 37.2% over quantum-inspired genetic algorithm, using the same iteration counts and population sizes with real quantum hardware employing 100 qubits. These findings highlight the potential of current quantum computers to address real-world utility-scale combinatorial optimization problems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach For Bitcoin Forecasting</title>
<link>https://arxiv.org/abs/2504.18206</link>
<guid>https://arxiv.org/abs/2504.18206</guid>
<content:encoded><![CDATA[
<div> Keywords: Bitcoin, cryptocurrency, time series, machine learning, directional accuracy

Summary:
In this study, the researchers focus on Bitcoin, a popular cryptocurrency, and investigate the use of different time series data along with machine learning algorithms to forecast its price movements. The analysis reveals that incorporating the Open, High, and Low prices of Bitcoin significantly improves directional accuracy. The Low price, in particular, plays a crucial role in enhancing the forecast accuracy when used in combination with a Gated Recurrent Unit network and a baseline forecast. The study also finds that other Bitcoin-related features, apart from price data, have minimal impact on prediction accuracy. Overall, the proposed method displays comparable performance to existing approaches in terms of directional accuracy, emphasizing the importance of considering specific time series data and machine learning techniques for forecasting cryptocurrency prices.<br /><br />Summary: <div>
arXiv:2504.18206v1 Announce Type: cross 
Abstract: Bitcoin is one of the cryptocurrencies that is gaining more popularity in recent years. Previous studies have shown that closing price alone is not enough to forecast stock market series. We introduce a new set of time series and demonstrate that a subset is necessary to improve directional accuracy based on a machine learning ensemble. In our experiments, we study which time series and machine learning algorithms deliver the best results. We found that the most relevant time series that contribute to improving directional accuracy are Open, High and Low, with the largest contribution of Low in combination with an ensemble of Gated Recurrent Unit network and a baseline forecast. The relevance of other Bitcoin-related features that are not price-related is negligible. The proposed method delivers similar performance to the state-of-the-art when observing directional accuracy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid modelling of reactive transport in porous media using machine learning: limitations and solutions</title>
<link>https://arxiv.org/abs/2405.14548</link>
<guid>https://arxiv.org/abs/2405.14548</guid>
<content:encoded><![CDATA[
<div> machine learning, reactive transport, porous media, geochemical reactions, cation exchange problem

Summary:
Machine learning models are explored as replacements for a geochemical module in simulating reactive transport in porous media. Testing on a cation exchange problem reveals that while the surrogate models perform well in isolated predictions, they struggle with rollout predictions over successive time steps. By incorporating physics-based constraints and tailored dataset generation strategies, accurate rollout predictions are achieved. The study highlights the limitation of machine learning surrogates in predicting over multiple time steps, even for a simple sorption equilibrium reaction like the cation exchange problem. However, with the addition of physics-based modifications, these limitations can be overcome. The research provides a detailed analysis of these limitations and potential mitigation strategies.<br /><br /> <div>
arXiv:2405.14548v2 Announce Type: replace 
Abstract: Reactive transport in porous media plays a pivotal role in subsurface reservoir processes, influencing fluid properties and geochemical characteristics. However, coupling fluid flow and transport with geochemical reactions is computationally intensive, requiring geochemical calculations at each grid cell and each time step within a discretized simulation domain. Although recent advancements have integrated machine learning techniques as surrogates for geochemical simulations, ensuring computational efficiency and accuracy remains a challenge. This work investigates machine learning models as replacements for a geochemical module in a simulation of reactive transport in porous media. As a proof of concept, we test this approach on a well-documented cation exchange problem. While the surrogate models excel in isolated predictions, they fall short in rollout predictions over successive time steps. By introducing modifications, including physics-based constraints and tailored dataset generation strategies, we show that machine learning surrogates can achieve accurate rollout predictions. Our findings emphasize that even for a simple sorption equilibrium reaction (cation exchange problem), machine learning surrogates alone fail in predicting over successive time-steps. Incorporating simple physics-based modifications enables us to overcome this limitation. A detailed analysis of the limitations and potential mitigation strategies is presented in this work.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Function-coherent gambles</title>
<link>https://arxiv.org/abs/2503.01855</link>
<guid>https://arxiv.org/abs/2503.01855</guid>
<content:encoded><![CDATA[
<div> function-coherent gambles, non-linear utility, intertemporal choice, discounting, desirability framework
Summary:
The paper introduces function-coherent gambles, a generalization of the desirable gambles framework that allows for non-linear utility functions. Core axioms for function-coherence are established, and a representation theorem is proven, linking acceptable gambles to continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, such as hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. The integration of these alternative discounting models within the function-coherent framework provides a unified treatment for modeling complex patterns of time preference. This approach bridges the gap between normative theory and real-world behavior in intertemporal decision-making under uncertainty, offering theoretical foundations for understanding sophisticated time preferences within the desirability paradigm. <br /><br />Summary: <div>
arXiv:2503.01855v2 Announce Type: replace-cross 
Abstract: The desirable gambles framework provides a foundational approach to imprecise probability theory but relies heavily on linear utility assumptions. This paper introduces function-coherent gambles, a generalization that accommodates non-linear utility while preserving essential rationality properties. We establish core axioms for function-coherence and prove a representation theorem that characterizes acceptable gambles through continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, including hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. We demonstrate how these alternatives to constant-rate exponential discounting can be integrated within the function-coherent framework. This unified treatment provides theoretical foundations for modeling sophisticated patterns of time preference within the desirability paradigm, bridging a gap between normative theory and observed behavior in intertemporal decision-making under genuine uncertainty.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Entropy Stable Formulation of Two-equation Turbulence Models with Particular Reference to the k-epsilon Model</title>
<link>https://arxiv.org/abs/2504.17110</link>
<guid>https://arxiv.org/abs/2504.17110</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical algorithms, partial differential equations, entropy production inequality, turbulence models, k-epsilon model

Summary:
This article discusses the importance of incorporating nonlinear physical stability principles, such as the entropy production inequality, in the design of numerical algorithms for partial differential equations. By introducing space-time averaging and defining entropy variables, a symmetric system of advective-diffusive equations can be derived for turbulence models, including the k-epsilon model. Positivity and symmetry constraints are necessary for the turbulence diffusivity coefficients and source terms to ensure the design of entropy producing two-equation turbulence models. This approach emphasizes the use of physical principles over artificial viscosity for designing robust algorithms. <div>
arXiv:2504.17110v1 Announce Type: new 
Abstract: Consistency and stability are two essential ingredients in the design of numerical algorithms for partial differential equations. Robust algorithms can be developed by incorporating nonlinear physical stability principles in their design, such as the entropy production inequality (i.e., the Clausius-Duhem inequality or second law of thermodynamics), rather than by simply adding artificial viscosity (a common approach). This idea is applied to the k-epsilon and two-equation turbulence models by introducing space-time averaging. Then, a set of entropy variables can be defined which leads to a symmetric system of advective-diffusive equations. Positivity and symmetry of the equations require certain constraints on the turbulence diffusivity coefficients and the turbulence source terms. With these, we are able to design entropy producing two-equation turbulence models and, in particular, the k-epsilon model.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Stock Prices for Enhanced Multi-Step Forecast and Prediction</title>
<link>https://arxiv.org/abs/2504.17313</link>
<guid>https://arxiv.org/abs/2504.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: stock price forecasting, stock price prediction, PCIE model, tokenization method, multi-step prediction<br />
Summary: <br />
Effective stock price forecasting and prediction are essential for investors and policymakers but are challenging due to the dynamic nature of stock price data. Forecasting and prediction targets have distinct statistical characteristics and multi-step approaches provide richer information but are more difficult. The Patched Channel Integration Encoder (PCIE) model is introduced to address these challenges by utilizing multiple stock channels and a novel tokenization method. The tokenization process involves univariate patching and temporal learning to reduce cumulative errors. Experimental results demonstrate that PCIE outperforms current state-of-the-art models in both forecast and prediction tasks. <div>
arXiv:2504.17313v1 Announce Type: new 
Abstract: Effective stock price forecasting (estimating future prices) and prediction (estimating future price changes) are pivotal for investors, regulatory agencies, and policymakers. These tasks enable informed decision-making, risk management, strategic planning, and superior portfolio returns. Despite their importance, forecasting and prediction are challenging due to the dynamic nature of stock price data, which exhibit significant temporal variations in distribution and statistical properties. Additionally, while both forecasting and prediction targets are derived from the same dataset, their statistical characteristics differ significantly. Forecasting targets typically follow a log-normal distribution, characterized by significant shifts in mean and variance over time, whereas prediction targets adhere to a normal distribution. Furthermore, although multi-step forecasting and prediction offer a broader perspective and richer information compared to single-step approaches, it is much more challenging due to factors such as cumulative errors and long-term temporal variance. As a result, many previous works have tackled either single-step stock price forecasting or prediction instead. To address these issues, we introduce a novel model, termed Patched Channel Integration Encoder (PCIE), to tackle both stock price forecasting and prediction. In this model, we utilize multiple stock channels that cover both historical prices and price changes, and design a novel tokenization method to effectively embed these channels in a cross-channel and temporally efficient manner. Specifically, the tokenization process involves univariate patching and temporal learning with a channel-mixing encoder to reduce cumulative errors. Comprehensive experiments validate that PCIE outperforms current state-of-the-art models in forecast and prediction tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems</title>
<link>https://arxiv.org/abs/2504.17354</link>
<guid>https://arxiv.org/abs/2504.17354</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, effective contact area, rough surfaces, machine learning algorithms, multi-query contexts
<br />
Summary: 
This study presents a surrogate modeling framework for predicting the effective contact area in rough surface contact using fast-to-evaluate machine learning algorithms. The models are trained on a precomputed dataset of imposed load and roughness parameters to predict the effective contact area efficiently. The Kernel Ridge Regressor is identified as the best trade-off between accuracy and efficiency, making it suitable for general-purpose surrogate modeling. The Gaussian Process Regressor is also effective for uncertainty quantification tasks. The models' generalization capability is validated on unseen simulation scenarios, demonstrating their transferability to new configurations. While database generation is a significant cost in the process, the overall approach is practical and efficient for multi-query tasks, even after accounting for initial expenses. <br /><br />Summary: <div>
arXiv:2504.17354v1 Announce Type: new 
Abstract: The effective contact area in rough surface contact plays a critical role in multi-physics phenomena such as wear, sealing, and thermal or electrical conduction. Although accurate numerical methods, like the Boundary Element Method (BEM), are available to compute this quantity, their high computational cost limits their applicability in multi-query contexts, such as uncertainty quantification, parameter identification, and multi-scale algorithms, where many repeated evaluations are required. This study proposes a surrogate modeling framework for predicting the effective contact area using fast-to-evaluate data-driven techniques. Various machine learning algorithms are trained on a precomputed dataset, where the inputs are the imposed load and statistical roughness parameters, and the output is the corresponding effective contact area. All models undergo hyperparameter optimization to enable fair comparisons in terms of predictive accuracy and computational efficiency, evaluated using established quantitative metrics. Among the models, the Kernel Ridge Regressor demonstrates the best trade-off between accuracy and efficiency, achieving high predictive accuracy, low prediction time, and minimal training overhead-making it a strong candidate for general-purpose surrogate modeling. The Gaussian Process Regressor provides an attractive alternative when uncertainty quantification is required, although it incurs additional computational cost due to variance estimation. The generalization capability of the Kernel Ridge model is validated on an unseen simulation scenario, confirming its ability to transfer to new configurations. Database generation constitutes the dominant cost in the surrogate modeling process. Nevertheless, the approach proves practical and efficient for multi-query tasks, even when accounting for this initial expense.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>polyGen: A Learning Framework for Atomic-level Polymer Structure Generation</title>
<link>https://arxiv.org/abs/2504.17656</link>
<guid>https://arxiv.org/abs/2504.17656</guid>
<content:encoded><![CDATA[
<div> Latent diffusion model, polymer structures, generative algorithms, molecular encoding, structure matching criteria 

Summary: 

polyGen is introduced as a novel approach to generating realistic polymer structures using a latent diffusion model. Leveraging a molecular encoding that captures polymer connectivity, polyGen can generate diverse conformations of both linear chains and complex branched structures. The model shows improvement in joint learning between similar chemical structures through training augmentation with DFT-optimized molecular structures. However, its performance decreases when handling repeat units with a high atom count. polyGen represents a paradigm shift in atomic-level structure generation for polymer science, providing the first proof-of-concept for predicting realistic atomic-level polymer conformations while considering their intrinsic structural flexibility. <div>
arXiv:2504.17656v1 Announce Type: new 
Abstract: Synthetic polymeric materials underpin fundamental technologies in the energy, electronics, consumer goods, and medical sectors, yet their development still suffers from prolonged design timelines. Although polymer informatics tools have supported speedup, polymer simulation protocols continue to face significant challenges: on-demand generation of realistic 3D atomic structures that respect the conformational diversity of polymer structures. Generative algorithms for 3D structures of inorganic crystals, bio-polymers, and small molecules exist, but have not addressed synthetic polymers. In this work, we introduce polyGen, the first latent diffusion model designed specifically to generate realistic polymer structures from minimal inputs such as the repeat unit chemistry alone, leveraging a molecular encoding that captures polymer connectivity throughout the architecture. Due to a scarce dataset of only 3855 DFT-optimized polymer structures, we augment our training with DFT-optimized molecular structures, showing improvement in joint learning between similar chemical structures. We also establish structure matching criteria to benchmark our approach on this novel problem. polyGen effectively generates diverse conformations of both linear chains and complex branched structures, though its performance decreases when handling repeat units with a high atom count. Given these initial results, polyGen represents a paradigm shift in atomic-level structure generation for polymer science-the first proof-of-concept for predicting realistic atomic-level polymer conformations while accounting for their intrinsic structural flexibility.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy</title>
<link>https://arxiv.org/abs/2504.17124</link>
<guid>https://arxiv.org/abs/2504.17124</guid>
<content:encoded><![CDATA[
<div> X-ray absorption near edge structure (XANES) spectroscopy, adaptive sampling methods, Bayesian optimization, absorption edge, pre-edge peaks <br />
<br />Summary: <br />
X-ray absorption near edge structure (XANES) spectroscopy is a valuable technique for analyzing chemical states in materials, but traditional data collection methods can be time-consuming. This study introduces a knowledge-injected Bayesian optimization approach for adaptive XANES data collection, taking into account spectral features like absorption edges and pre-edge peaks. The method efficiently reconstructs absorption edges with high accuracy using only 15-20% of the measurement points needed in conventional sampling. It can determine the x-ray energy of sharp peaks with minimal errors, achieving overall root-mean-square errors below 0.005 compared to traditional methods. The experiments on battery materials and catalysts demonstrate its effectiveness for both static and dynamic XANES measurements, enhancing data collection efficiency and enabling better time resolution for tracking chemical changes. This automated approach reduces common errors in XANES experiments and facilitates dynamic studies requiring high temporal resolution. <div>
arXiv:2504.17124v1 Announce Type: cross 
Abstract: X-ray absorption near edge structure (XANES) spectroscopy is a powerful technique for characterizing the chemical state and symmetry of individual elements within materials, but requires collecting data at many energy points which can be time-consuming. While adaptive sampling methods exist for efficiently collecting spectroscopic data, they often lack domain-specific knowledge about XANES spectra structure. Here we demonstrate a knowledge-injected Bayesian optimization approach for adaptive XANES data collection that incorporates understanding of spectral features like absorption edges and pre-edge peaks. We show this method accurately reconstructs the absorption edge of XANES spectra using only 15-20% of the measurement points typically needed for conventional sampling, while maintaining the ability to determine the x-ray energy of the sharp peak after absorption edge with errors less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and overall root-mean-square errors less than 0.005 compared to compared to traditionally sampled spectra. Our experiments on battery materials and catalysts demonstrate the method's effectiveness for both static and dynamic XANES measurements, improving data collection efficiency and enabling better time resolution for tracking chemical changes. This approach advances the degree of automation in XANES experiments reducing the common errors of under- or over-sampling points in near the absorption edge and enabling dynamic experiments that require high temporal resolution or limited measurement time.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints</title>
<link>https://arxiv.org/abs/2504.17142</link>
<guid>https://arxiv.org/abs/2504.17142</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, microelectronic components, multiphysics constraints, design optimization, high-dimensional solution space
Summary:
This study focuses on using reinforcement learning techniques for designing microelectronic components under multiphysics constraints. Traditional global optimization approaches are limited when dealing with intricate solution spaces and constraints. The complexity of microelectronic component design, such as ASICs and HI interposers, poses challenges for conventional methods. The study explores optimizing interconnect geometry for ASIC chips and component placement on a HI interposer while meeting thermoelastic and design constraints. The placement problem involves a high-dimensional solution space, highlighting the need for advanced techniques like reinforcement learning. By developing and testing an RL-based framework, the study aims to enhance the design and optimization processes for microelectronic components with multiphysics constraints. <div>
arXiv:2504.17142v1 Announce Type: cross 
Abstract: This study focuses on the development of reinforcement learning based techniques for the design of microelectronic components under multiphysics constraints. While traditional design approaches based on global optimization approaches are effective when dealing with a small number of design parameters, as the complexity of the solution space and of the constraints increases different techniques are needed. This is an important reason that makes the design and optimization of microelectronic components (characterized by large solution space and multiphysics constraints) very challenging for traditional methods. By taking as prototypical elements an application-specific integrated circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and numerically test an optimization framework based on reinforcement learning (RL). More specifically, we consider the optimization of the bonded interconnect geometry for an ASIC chip as well as the placement of components on a HI interposer while satisfying thermoelastic and design constraints. This placement problem is particularly interesting because it features a high-dimensional solution space.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection, Classification and Prevalence of Self-Admitted Aging Debt</title>
<link>https://arxiv.org/abs/2504.17428</link>
<guid>https://arxiv.org/abs/2504.17428</guid>
<content:encoded><![CDATA[
<div> Aging Debt, Self-Admitted Aging Debt, software aging, source code comments, OSS repositories <br />
Summary:<br />
The study introduces Aging Debt (AD) as a concept to represent increased maintenance efforts in software. It focuses on Self-Admitted Aging Debt (SAAD) observed in source code comments to detect and measure AD in software. A taxonomy is developed to categorize temporal software aging into Active and Dormant types. Analysis of over 9,000+ OSS repositories shows that more than 21% exhibit SAAD, with Dormant AD being prevalent. The study highlights the critical aspect of software maintenance and the importance of addressing evolutionary indicators like source code comments in software aging research. The proposed taxonomy can assist researchers in detailed software aging studies and help practitioners in developing proactive maintenance strategies. <br /> <div>
arXiv:2504.17428v1 Announce Type: cross 
Abstract: Context: Previous research on software aging is limited with focus on dynamic runtime indicators like memory and performance, often neglecting evolutionary indicators like source code comments and narrowly examining legacy issues within the TD context. Objective: We introduce the concept of Aging Debt (AD), representing the increased maintenance efforts and costs needed to keep software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed in source code comments left by software developers. Method: We employ a mixed-methods approach, combining qualitative and quantitative analyses to detect and measure AD in software. This includes framing SAAD patterns from the source code comments after analysing the source code context, then utilizing the SAAD patterns to detect SAAD comments. In the process, we develop a taxonomy for SAAD that reflects the temporal aging of software and its associated debt. Then we utilize the taxonomy to quantify the different types of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes temporal software aging into Active and Dormant types. Our extensive analysis of over 9,000+ Open Source Software (OSS) repositories reveals that more than 21% repositories exhibit signs of SAAD as observed from our gold standard SAAD dataset. Notably, Dormant AD emerges as the predominant category, highlighting a critical but often overlooked aspect of software maintenance. Conclusion: As software volume grows annually, so do evolutionary aging and maintenance challenges; our proposed taxonomy can aid researchers in detailed software aging studies and help practitioners develop improved and proactive maintenance strategies.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An approach based on metaheuristic algorithms to the timetabling problem in deregulated railway markets</title>
<link>https://arxiv.org/abs/2504.17455</link>
<guid>https://arxiv.org/abs/2504.17455</guid>
<content:encoded><![CDATA[
<div> Genetic Algorithm, train timetabling, liberalized railway markets, optimization, scheduling<br />
<br />
Summary: <br />
The paper addresses the train timetabling problem in liberalized railway markets, focusing on maximizing infrastructure capacity and revenue optimization for infrastructure managers and railway undertakings. A modular simulation framework is introduced to simulate deregulated railway systems, evaluating ten metaheuristic algorithms using the MEALPY Python library. Results show that the Genetic Algorithm outperforms other algorithms in revenue optimization, convergence speed, and schedule adherence. Alternative algorithms, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, exhibit slower convergence and higher variability. The study highlights the trade-off between scheduling more trains and meeting requested times, offering insights into solving complex scheduling challenges in deregulated railway systems. <div>
arXiv:2504.17455v1 Announce Type: cross 
Abstract: The train timetabling problem in liberalized railway markets represents a challenge to the coordination between infrastructure managers and railway undertakings. Efficient scheduling is critical in maximizing infrastructure capacity and utilization while adhering as closely as possible to the requests of railway undertakings. These objectives ultimately contribute to maximizing the infrastructure manager's revenues. This paper sets out a modular simulation framework to reproduce the dynamics of deregulated railway systems. Ten metaheuristic algorithms using the MEALPY Python library are then evaluated in order to optimize train schedules in the liberalized Spanish railway market. The results show that the Genetic Algorithm outperforms others in revenue optimization, convergence speed, and schedule adherence. Alternatives, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, show slower convergence and higher variability. The results emphasize the trade-off between scheduling more trains and adhering to requested times, providing insights into solving complex scheduling problems in deregulated railway systems.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Equitable Rail Service Allocation Through Fairness-Oriented Timetabling in Liberalized Markets</title>
<link>https://arxiv.org/abs/2504.17489</link>
<guid>https://arxiv.org/abs/2504.17489</guid>
<content:encoded><![CDATA[
<div> Keywords: European rail transport, liberalization, infrastructure capacity, railway market, equity metrics 

Summary: 
In the changing landscape of European rail transport due to liberalization, railway undertakings now compete for limited infrastructure capacity to offer their services. The equitable allocation of infrastructure by the infrastructure manager is crucial for the efficiency and sustainability of this competitive environment. A methodology utilizing Jain, Gini, and Atkinson equity metrics is proposed to address the rail service allocation problem in a liberalized railway market. The results from computational tests demonstrate that this methodology promotes equitable planning across various competitiveness scenarios. This stands in contrast to approaches solely focused on maximizing the infrastructure manager's profit without considering fair allocation. The study supports the use of the proposed methodology and equity metrics as effective tools for planning and decision-making within a liberalized railway market. 

<br /><br />Summary: <div>
arXiv:2504.17489v1 Announce Type: cross 
Abstract: Over the last few decades, European rail transport has undergone major changes as part of the process of liberalization set out in European regulations. In this context of liberalization, railway undertakings compete with each other for the limited infrastructure capacity available to offer their rail services. The infrastructure manager is responsible for the equitable allocation of infrastructure between all companies in the market, which is essential to ensure the efficiency and sustainability of this competitive ecosystem. In this paper, a methodology based on Jain, Gini and Atkinson equity metrics is used to solve the rail service allocation problem in a liberalized railway market, analyzing the solutions obtained. The results show that the proposed methodology and the equity metrics used allow for equitable planning in different competitiveness scenarios. These results contrast with solutions where the objective of the infrastructure manager is to maximize its own profit, without regard for the equitable allocation of infrastructure. Therefore, the computational tests support the methodology and metrics used as a planning and decision support tool in a liberalized railway market.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and generalizable protein-ligand binding affinity prediction with geometric deep learning</title>
<link>https://arxiv.org/abs/2504.16261</link>
<guid>https://arxiv.org/abs/2504.16261</guid>
<content:encoded><![CDATA[
<div> deep learning, protein-ligand binding, affinity prediction, interatomic potential, machine learning<br />
Summary:<br />
The article introduces IPBind, a computational method based on geometric deep learning, for predicting protein-ligand binding affinity. While existing algorithms struggle with novel protein-ligand complexes, IPBind leverages interatomic potential to make robust predictions. Experimental results on binding affinity prediction benchmarks show the effectiveness and universality of IPBind, providing atom-level insights. This work underscores the benefits of using machine learning interatomic potential in protein-ligand binding affinity prediction. <div>
arXiv:2504.16261v1 Announce Type: new 
Abstract: Protein-ligand binding complexes are ubiquitous and essential to life. Protein-ligand binding affinity prediction (PLA) quantifies the binding strength between ligands and proteins, providing crucial insights for discovering and designing potential candidate ligands. While recent advances have been made in predicting protein-ligand complex structures, existing algorithms for interaction and affinity prediction suffer from a sharp decline in performance when handling ligands bound with novel unseen proteins. We propose IPBind, a geometric deep learning-based computational method, enabling robust predictions by leveraging interatomic potential between complex's bound and unbound status. Experimental results on widely used binding affinity prediction benchmarks demonstrate the effectiveness and universality of IPBind. Meanwhile, it provides atom-level insights into prediction. This work highlights the advantage of leveraging machine learning interatomic potential for predicting protein-ligand binding affinity.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Different Transformer Model Structures for Stock Prediction</title>
<link>https://arxiv.org/abs/2504.16361</link>
<guid>https://arxiv.org/abs/2504.16361</guid>
<content:encoded><![CDATA[
<div> Transformer, stock index prediction, model architectures, forecasting, ProbSparse attention

Summary:
- The paper compares different Transformer model architectures for stock index prediction, highlighting the importance of architectural choices in predictive accuracy. 
- Existing studies often treat Transformers as black boxes, overlooking the impact of specific structural designs on performance.
- Five Transformer structures were evaluated: encoder-only, decoder-only, Vanilla Transformer, Vanilla Transformer without embedding layers, and Vanilla Transformer with ProbSparse attention.
- Results indicate that Transformer models generally outperform traditional approaches, with the decoder-only structure being the most effective in all scenarios.
- The Transformer with ProbSparse attention showed the poorest performance in most cases. 

<br /><br />Summary: <div>
arXiv:2504.16361v1 Announce Type: new 
Abstract: This paper compares different Transformer model architectures for stock index prediction. While many studies have shown that Transformers perform well in stock price forecasting, few have explored how different structural designs impact performance. Most existing works treat the Transformer as a black box, overlooking how specific architectural choices may affect predictive accuracy. However, understanding these differences is critical for developing more effective forecasting models. This study aims to identify which Transformer variant is most suitable for stock forecasting. This study evaluates five Transformer structures: (1) encoder-only Transformer, (2) decoder-only Transformer, (3) Vanilla Transformer (encoder + decoder), (4) Vanilla Transformer without embedding layers, and (5) Vanilla Transformer with ProbSparse attention. Results show that Transformer-based models generally outperform traditional approaches. Transformer with decoder only structure outperforms all other models in all scenarios. Transformer with ProbSparse attention has the worst performance in almost all cases.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preconditioning Natural and Second Order Gradient Descent in Quantum Optimization: A Performance Benchmark</title>
<link>https://arxiv.org/abs/2504.16518</link>
<guid>https://arxiv.org/abs/2504.16518</guid>
<content:encoded><![CDATA[
<div> optimization, parametric quantum circuits, noisy gradient evaluations, curvature information, BFGS update rule <br />
Summary: <br />
The optimization of parametric quantum circuits faces challenges due to the non-convex nature of the objective function, noisy gradient evaluations, and barren plateaus. Selecting the right classical optimizer is crucial in quantum-classical applications. Incorporating curvature information in the parameter update can aid in faster convergence. Quasi-Newton and quantum natural gradient methods show promise in this regard but have drawbacks. A study evaluates the performance of optimizers on MaxCut problems using a shallow QAOA algorithm. To address noise sensitivity and iteration cost, a novel approach called secant-penalization in the BFGS update rule (SP-BFGS) is introduced, leading to improved outcomes for QAOA optimization problems. This method stabilizes BFGS updates against gradient noise, showcasing potential for optimizing parametric quantum circuits in noisy environments. <br /> <div>
arXiv:2504.16518v1 Announce Type: new 
Abstract: The optimization of parametric quantum circuits is technically hindered by three major obstacles: the non-convex nature of the objective function, noisy gradient evaluations, and the presence of barren plateaus. As a result, the selection of classical optimizer becomes a critical factor in assessing and exploiting quantum-classical applications. One promising approach to tackle these challenges involves incorporating curvature information into the parameter update. The most prominent methods in this field are quasi-Newton and quantum natural gradient methods, which can facilitate faster convergence compared to first-order approaches. Second order methods however exhibit a significant trade-off between computational cost and accuracy, as well as heightened sensitivity to noise. This study evaluates the performance of three families of optimizers on synthetically generated MaxCut problems on a shallow QAOA algorithm. To address noise sensitivity and iteration cost, we demonstrate that incorporating secant-penalization in the BFGS update rule (SP-BFGS) yields improved outcomes for QAOA optimization problems, introducing a novel approach to stabilizing BFGS updates against gradient noise.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields</title>
<link>https://arxiv.org/abs/2504.16600</link>
<guid>https://arxiv.org/abs/2504.16600</guid>
<content:encoded><![CDATA[
<div> Keywords: passive implants, magnetic fields, safety assessment, numerical approach, thermal diffusion <br />
Summary: <br />
- Safety assessment of patients with one-dimensional structured passive implants exposed to low or medium frequency magnetic fields poses challenges due to different length scales. <br />
- A novel numerical approach is proposed, solving three-dimensional and one-dimensional coupled problems, considering thermal diffusion through metallic implants for improved accuracy. <br />
- Results from measurements on a cranial plate exposed to a magnetic field show a 25% improvement in accuracy compared to the method based on thermal seeds. <br />
- Application of the proposed method in a magnetic hyperthermia case study predicts a 10% lower temperature increase near the implant compared to the overestimation by relying on thermal seeds. <br /> <div>
arXiv:2504.16600v1 Announce Type: new 
Abstract: Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Software Engineering Research on Jupyter Notebook</title>
<link>https://arxiv.org/abs/2504.16180</link>
<guid>https://arxiv.org/abs/2504.16180</guid>
<content:encoded><![CDATA[
<div> Trends, gaps, methodologies, human-computer interaction, reusability <br />
Summary:<br />
The study analyzed software engineering research on Jupyter notebooks, finding that most publications are in human-computer interaction venues rather than traditional software engineering ones. Various software engineering topics were addressed, but solutions for testing, refactoring, and documentation specific to notebooks are lacking. Only a small percentage of studies provide reusable code, and many replication packages are not stored in permanent repositories. Future research opportunities include developing automatic testing frameworks, refactoring clones between notebooks, and generating group documentation for coherent code cells. <div>
arXiv:2504.16180v1 Announce Type: cross 
Abstract: Context: Jupyter Notebook has emerged as a versatile tool that transforms how researchers, developers, and data scientists conduct and communicate their work. As the adoption of Jupyter notebooks continues to rise, so does the interest from the software engineering research community in improving the software engineering practices for Jupyter notebooks.
  Objective: The purpose of this study is to analyze trends, gaps, and methodologies used in software engineering research on Jupyter notebooks.
  Method: We selected 146 relevant publications from the DBLP Computer Science Bibliography up to the end of 2024, following established systematic literature review guidelines. We explored publication trends, categorized them based on software engineering topics, and reported findings based on those topics.
  Results: The most popular venues for publishing software engineering research on Jupyter notebooks are related to human-computer interaction instead of traditional software engineering venues. Researchers have addressed a wide range of software engineering topics on notebooks, such as code reuse, readability, and execution environment. Although reusability is one of the research topics for Jupyter notebooks, only 64 of the 146 studies can be reused based on their provided URLs. Additionally, most replication packages are not hosted on permanent repositories for long-term availability and adherence to open science principles.
  Conclusion: Solutions specific to notebooks for software engineering issues, including testing, refactoring, and documentation, are underexplored. Future research opportunities exist in automatic testing frameworks, refactoring clones between notebooks, and generating group documentation for coherent code cells.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Doubly Stochastic Transformers</title>
<link>https://arxiv.org/abs/2504.16275</link>
<guid>https://arxiv.org/abs/2504.16275</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, Softmax, Sinkhorn algorithm, doubly stochastic matrix, variational quantum circuit <br />
Summary: 
The study introduces a hybrid classical-quantum Doubly Stochastic Transformer (QDSFormer) that incorporates a variational quantum circuit in place of the Softmax in the attention layer of the Transformer model. The parametric quantum circuit used in the QDSFormer enhances the expressive power of the model, resulting in more diverse Doubly Stochastic Matrices (DSMs) that preserve information better than classical operators. Through experiments on small-scale object recognition tasks, the QDSFormer outperforms standard Vision Transformers and other doubly stochastic Transformers. Additionally, a quantum-inspired doubly stochastic Transformer based on QR decomposition is also compared. The QDSFormer demonstrates improved training stability and lower performance variation, suggesting that it could mitigate instability during training on small-scale data. These findings highlight the potential of quantum-inspired approaches in enhancing the performance and stability of Transformer models.
<br /><br />Summary: <div>
arXiv:2504.16275v1 Announce Type: cross 
Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often destabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard Vision Transformer and other doubly stochastic Transformers. Beyond the established Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. The QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixing Data-Driven and Physics-Based Constitutive Models using Uncertainty-Driven Phase Fields</title>
<link>https://arxiv.org/abs/2504.16713</link>
<guid>https://arxiv.org/abs/2504.16713</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, multiscale models, data-driven, phase-field model, Gaussian Process surrogate <br />
Summary:
This article presents an adaptive mixture approach for accelerating multiscale models using data-driven surrogate modeling techniques. The approach involves using a fast probabilistic surrogate model for most of the computational domain, switching to the true high-fidelity model only when necessary. By incorporating phases in the computational domain and utilizing a phase-field model driven by surrogate uncertainty, the transition between models is smooth and accurate. The method reduces the time required to collect a large training dataset, while still maintaining high accuracy in simulations. The study compares this approach to a purely local model and demonstrates its effectiveness using a Gaussian Process surrogate for an elasto-plastic material. The adaptive mixture of models shows great potential for speeding up multiscale simulations without compromising accuracy or stability. <br /> <div>
arXiv:2504.16713v1 Announce Type: cross 
Abstract: There is a high interest in accelerating multiscale models using data-driven surrogate modeling techniques. Creating a large training dataset encompassing all relevant load scenarios is essential for a good surrogate, yet the computational cost of producing this data quickly becomes a limiting factor. Commonly, a pre-trained surrogate is used throughout the computational domain. Here, we introduce an alternative adaptive mixture approach that uses a fast probabilistic surrogate model as constitutive model when possible, but resorts back to the true high-fidelity model when necessary. The surrogate is thus not required to be accurate for every possible load condition, enabling a significant reduction in the data collection time. We achieve this by creating phases in the computational domain corresponding to the different models. These phases evolve using a phase-field model driven by the surrogate uncertainty. When the surrogate uncertainty becomes large, the phase-field model causes a local transition from the surrogate to the high-fidelity model, maintaining a highly accurate simulation. We discuss the requirements of this approach to achieve accurate and numerically stable results and compare the phase-field model to a purely local approach that does not enforce spatial smoothness for the phase mixing. Using a Gaussian Process surrogate for an elasto-plastic material, we demonstrate the potential of this mixture of models to accelerate multiscale simulations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements in Constitutive Model Calibration: Leveraging the Power of Full-Field DIC Measurements and In-Situ Load Path Selection for Reliable Parameter Inference</title>
<link>https://arxiv.org/abs/2411.07310</link>
<guid>https://arxiv.org/abs/2411.07310</guid>
<content:encoded><![CDATA[
<div> calibration, material characterization, computational engineering, Bayesian inference, uncertainty quantification
<br />
Summary:
Interlaced Characterization and Calibration (ICC) presents an improved workflow for material model calibration, addressing current limitations. It efficiently uses full-field data for calibration, aligns collected data with optimal experimental design, quantifies parameter uncertainty through Bayesian inference, and incorporates a real-time feedback loop. Demonstrated on an aluminum cruciform specimen, ICC utilizes Bayesian optimal experimental design for selecting load steps, principal component analysis for reducing data dimensions, and fast surrogate models for computational efficiency. This framework allows for reliable calibration of high-fidelity constitutive models with quantified uncertainty. By advancing the state-of-the-art in material characterization and model calibration, ICC supports credible decision-making in solid mechanics modeling, potentially increasing modeling agility. 
<br /> <div>
arXiv:2411.07310v3 Announce Type: replace 
Abstract: Accurate material characterization and model calibration are essential for computationally-supported engineering decisions. Current characterization and calibration methods (1) use simplified test specimen geometries and global data, (2) cannot guarantee that sufficient characterization data is collected for a specific model of interest, (3) use deterministic methods that provide best-fit parameter values with no uncertainty quantification, and (4) are sequential, inflexible, and time-consuming. This work brings together several recent advancements into an improved workflow called Interlaced Characterization and Calibration that advances the state-of-the-art in constitutive model calibration. The ICC paradigm (1) efficiently uses full-field data to calibrate a high-fidelity material model, (2) aligns the data needed with the data collected with an optimal experimental design protocol, (3) quantifies parameter uncertainty through Bayesian inference, and (4) incorporates these advances into a quasi real-time feedback loop. The ICC framework is demonstrated on the calibration of a material model using simulated full-field data for an aluminum cruciform specimen being deformed bi-axially. The cruciform is actively driven through the myopically optimal load path using Bayesian optimal experimental design, which selects load steps that yield the maximum expected information gain. To aid in numerical stability and preserve computational resources, the full-field data is dimensionally reduced via principal component analysis, and fast surrogate models which approximate the input-output relationships of the expensive finite element model are used. The tools demonstrated here show that high-fidelity constitutive models can be efficiently and reliably calibrated with quantified uncertainty, thus supporting credible decision-making and potentially increasing the agility of solid mechanics modeling.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Optimization of Physics-Informed Neural Networks: Advancing Generalizability by the Baldwin Effect</title>
<link>https://arxiv.org/abs/2312.03243</link>
<guid>https://arxiv.org/abs/2312.03243</guid>
<content:encoded><![CDATA[
<div> evolutionary selection, physics-informed neural networks, meta-learning, prediction accuracy, computational efficiency
<br />
Summary:
This paper introduces a novel approach to enhance the generalizability of Physics-Informed Neural Networks (PINNs) by incorporating principles of Baldwinian evolution. The proposed Baldwinian-PINNs are pre-wired with connection strengths to enable efficient learning of physics tasks. By utilizing a two-stage stochastic programming formulation that combines evolutionary selection pressure and lifetime learning, these evolved PINNs exhibit fast and accurate predictions across various physics problems. The Baldwinian-PINNs outperform state-of-the-art gradient-based meta-learning methods by achieving over a 70x improvement in accuracy while requiring 700x less computational time when solving the diffusion-reaction equation. This research marks a significant advancement in meta-learning for PINNs, providing a more generalizable and efficient approach to solving complex physics simulations. Sample codes for implementation are available on GitHub for further exploration. 
<br /> <div>
arXiv:2312.03243v3 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) are at the forefront of scientific machine learning, making possible the creation of machine intelligence that is cognizant of physical laws and able to accurately simulate them. However, today's PINNs are often trained for a single physics task and require computationally expensive re-training for each new task, even for tasks from similar physics domains. To address this limitation, this paper proposes a pioneering approach to advance the generalizability of PINNs through the framework of Baldwinian evolution. Drawing inspiration from the neurodevelopment of precocial species that have evolved to learn, predict and react quickly to their environment, we envision PINNs that are pre-wired with connection strengths inducing strong biases towards efficient learning of physics. A novel two-stage stochastic programming formulation coupling evolutionary selection pressure (based on proficiency over a distribution of physics tasks) with lifetime learning (to specialize on a sampled subset of those tasks) is proposed to instantiate the Baldwin effect. The evolved Baldwinian-PINNs demonstrate fast and physics-compliant prediction capabilities across a range of empirically challenging problem instances with more than an order of magnitude improvement in prediction accuracy at a fraction of the computation cost compared to state-of-the-art gradient-based meta-learning methods. For example, when solving the diffusion-reaction equation, a 70x improvement in accuracy was obtained while taking 700x less computational time. This paper thus marks a leap forward in the meta-learning of PINNs as generalizable physics solvers. Sample codes are available at https://github.com/chiuph/Baldwinian-PINN.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dual-stage constitutive modeling framework based on finite strain data-driven identification and physics-augmented neural networks</title>
<link>https://arxiv.org/abs/2504.15492</link>
<guid>https://arxiv.org/abs/2504.15492</guid>
<content:encoded><![CDATA[
<div> approach; hyperelastic constitutive models; Data-Driven Identification; Physics-Augmented Neural Network; Finite Element simulations <br />
Summary: 
This study introduces a novel dual-stage approach for generating hyperelastic constitutive models using only experimentally measurable data. The first step involves applying a Data-Driven Identification (DDI) formulation to identify stress-strain tuples based on measured displacement fields. In the second step, a Physics-Augmented Neural Network (PANN) is calibrated using the data set, ensuring compliance with hyperelasticity conditions while maintaining flexibility. The approach is validated through synthetic 2D data generated in virtual experiments and applied in 3D Finite Element simulations. Additionally, a real experiment with noisy data is successfully mimicked, showcasing the effectiveness of the proposed method for automated hyperelastic model generation. <br /> <div>
arXiv:2504.15492v1 Announce Type: new 
Abstract: In this contribution, we present a novel consistent dual-stage approach for the automated generation of hyperelastic constitutive models which only requires experimentally measurable data. To generate input data for our approach, an experiment with full-field measurement has to be conducted to gather testing force and corresponding displacement field of the sample. Then, in the first step of the dual-stage framework, a new finite strain Data-Driven Identification (DDI) formulation is applied. This method enables to identify tuples consisting of stresses and strains by only prescribing the applied boundary conditions and the measured displacement field. In the second step, the data set is used to calibrate a Physics-Augmented Neural Network (PANN), which fulfills all common conditions of hyperelasticity by construction and is very flexible at the same time. We demonstrate the applicability of our approach by several descriptive examples. Two-dimensional synthetic data are exemplarily generated in virtual experiments by using a reference constitutive model. The calibrated PANN is then applied in 3D Finite Element simulations. In addition, a real experiment including noisy data is mimicked.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Higher-Order Interpolation and Restriction in ExaHyPE Avoiding Non-physical Reflections</title>
<link>https://arxiv.org/abs/2504.15814</link>
<guid>https://arxiv.org/abs/2504.15814</guid>
<content:encoded><![CDATA[
<div> Wave equations, adaptive mesh refinement, ExaHyPE, ExaGRyPE, interpolation schemes<br />
Summary:<br />
Wave equations play an essential role in understanding various phenomena, but tracking them over regular meshes is computationally challenging given their large scales. Adaptive mesh refinement (AMR) helps by creating higher-resolution meshes near regions of interest. ExaHyPE and ExaGRyPE are software engines developed to solve wave problems using AMR. To advance the mesh in time, a set of higher-order interpolation schemes was introduced to address errors near AMR boundaries and improve computational efficiency. By calculating derivatives at each coarse grid cell to approximate fine cells, these new methods outperform the traditional tensor-product approach in terms of speed and accuracy. This improvement was demonstrated in a benchmark simulation of a stationary black hole, where the errors near AMR boundaries were eliminated. <div>
arXiv:2504.15814v1 Announce Type: new 
Abstract: Wave equations help us to understand phenomena ranging from earthquakes to tsunamis. These phenomena materialise over very large scales. It would be computationally infeasible to track them over a regular mesh. Yet, since the phenomena are localised, adaptive mesh refinement (AMR) can be used to construct meshes with a higher resolution close to the regions of interest. ExaHyPE is a software engine created to solve wave problems using AMR, and we use it as baseline to construct our numerical relativity application called ExaGRyPE. To advance the mesh in time, we have to interpolate and restrict along resolution transitions in each and every time step. ExaHyPE's vanilla code version uses a d-linear tensor-product approach. In benchmarks of a stationary black hole this performs slowly and leads to errors in conserved quantities near AMR boundaries. We therefore introduce a set of higher-order interpolation schemes where the derivatives are calculated at each coarse grid cell to approximate the enclosed fine cells. The resulting methods run faster than the tensor-product approach. Most importantly, when running the stationary black hole simulation using the higher order methods the errors near the AMR boundaries are removed.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AneuPy: An open source Python tool for creating simulation-ready geometries of abdominal aortic aneurysms</title>
<link>https://arxiv.org/abs/2504.15285</link>
<guid>https://arxiv.org/abs/2504.15285</guid>
<content:encoded><![CDATA[
<div> AAA, abdominal aortic aneurysm, geometrical models, open-source software, AneuPy <br />
Summary:
Abdominal aortic aneurysms (AAAs) are a serious health concern, particularly among older individuals, due to their potential for life-threatening rupture. The geometric characteristics of AAAs, such as maximum diameter and wall thickness, are vital for assessing rupture risk. However, there is a lack of open-source software for generating simulation-ready AAA geometries. AneuPy, a Python-based tool, addresses this gap by automating the generation of idealized and patient-specific AAA geometrical models with minimal input data. By facilitating the creation of simulation-ready geometries for biomechanical and hemodynamic analyses, AneuPy aims to enhance research in AAA and improve patient-specific risk assessment. Its efficiency and flexibility make it a valuable tool for modeling AAA using finite element analysis (FEA), computational fluid dynamics (CFD), or fluid-structure interaction (FSI) methods. <br /><br />Summary: <div>
arXiv:2504.15285v1 Announce Type: cross 
Abstract: Abdominal aortic aneurysms (AAAs) are localized dilations of the abdominal aorta that can lead to life-threatening rupture if left untreated. AAAs predominantly affect older individuals, with a high mortality rate upon rupture, making early diagnosis and risk assessment critical. The geometric characteristics of an AAA, such as its maximum diameter, asymmetry, and wall thickness, play a crucial role in biomechanical models used to assess rupture risk. Despite the growing use of computational modeling to study AAAs, there is a lack of open source software that facilitates the generation of simulation-ready geometries tailored for biomechanical and hemodynamic analyses. To address this need, we introduce AneuPy, an open-source Python-based tool designed to generate idealized and patient-specific AAA geometrical models. AneuPy provides an efficient and automated approach to aneurysm geometry generation, requiring minimal input data while allowing for flexible parameterization. By streamlining the creation of simulation-ready geometries for finite element analysis (FEA), computational fluid dynamics (CFD), or fluid-structure interaction (FSI) models, AneuPy aims to facilitate research in AAAs and enhance patient-specific risk assessment.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.17908</link>
<guid>https://arxiv.org/abs/2412.17908</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, data poisoning, backdoor attack, detection methods, reinforcement learning

Summary:
The article explores the potential impact of large language models utilizing reinforcement learning in various fields, such as finance, physics, and artificial intelligence ecosystems. Specifically, the focus is on a backdoor attack named FinanceLLMsBackRL, which targets data poisoning without prior triggers. This attack poses a threat to financial institutions using reinforcement learning to simulate scenarios for models before and after regular operations. The study proposes a method for detecting such attacks through dynamic systems and statistical analysis of data distribution. Through this research, the authors aim to shed light on the vulnerabilities that may arise from the integration of large language models with reinforcement learning in critical sectors. This work underscores the importance of developing robust security measures to safeguard against potential threats in the evolving landscape of artificial intelligence systems. 

<br /><br />Summary: <div>
arXiv:2412.17908v3 Announce Type: replace-cross 
Abstract: With the rapid development of generative artificial intelligence, particularly large language models a number of sub-fields of deep learning have made significant progress and are now very useful in everyday applications. For example,financial institutions simulate a wide range of scenarios for various models created by their research teams using reinforcement learning, both before production and after regular operations. In this work, we propose a backdoor attack that focuses solely on data poisoning and a method of detection by dynamic systems and statistical analysis of the distribution of data. This particular backdoor attack is classified as an attack without prior consideration or trigger, and we name it FinanceLLMsBackRL. Our aim is to examine the potential effects of large language models that use reinforcement learning systems for text production or speech recognition, finance, physics, or the ecosystem of contemporary artificial intelligence models.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Aware Compression of Plasma Distribution Functions with GPU-Accelerated Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2504.14897</link>
<guid>https://arxiv.org/abs/2504.14897</guid>
<content:encoded><![CDATA[
<div> compression, plasma simulations, Gaussian Mixture Models, in-situ, real-time <br />
<br />
Summary: 
This article introduces a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) for large-scale plasma simulations. By approximating electron and ion velocity distribution functions with GMMs, the method captures plasma features like mean velocity and temperature, enabling identification of heating processes and beam generation. The approach involves constructing a histogram to reduce computational overhead and implementing GPU-accelerated, in-situ GMM fitting within the iPIC3D simulator for real-time compression. The compressed representation is stored using the ADIOS 2 library to optimize the I/O process. Compared to other algorithms like SZ, MGARD, and BLOSC2, the GMM-based method retains a physics-based approach, preserving the physical interpretation of plasma phenomena while achieving compression ratios of up to $10^4 and processing times comparable to standard compression engines. <div>
arXiv:2504.14897v1 Announce Type: new 
Abstract: Data compression is a critical technology for large-scale plasma simulations. Storing complete particle information requires Terabyte-scale data storage, and analysis requires ad-hoc scalable post-processing tools. We propose a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) to approximate electron and ion velocity distribution functions with a number of Gaussian components. This GMM-based method allows us to capture plasma features such as mean velocity and temperature, and it enables us to identify heating processes and generate beams. We first construct a histogram to reduce computational overhead and apply GPU-accelerated, in-situ GMM fitting within \texttt{iPIC3D}, a large-scale implicit Particle-in-Cell simulator, ensuring real-time compression. The compressed representation is stored using the \texttt{ADIOS 2} library, thus optimizing the I/O process. The GPU and histogramming implementation provides a significant speed-up with respect to GMM on particles (both in time and required memory at run-time), enabling real-time compression. Compared to algorithms like SZ, MGARD, and BLOSC2, our GMM-based method has a physics-based approach, retaining the physical interpretation of plasma phenomena such as beam formation, acceleration, and heating mechanisms. Our GMM algorithm achieves a compression ratio of up to $10^4$, requiring a processing time comparable to, or even lower than, standard compression engines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A parallel implementation of reduced-order modeling of large-scale systems</title>
<link>https://arxiv.org/abs/2504.14338</link>
<guid>https://arxiv.org/abs/2504.14338</guid>
<content:encoded><![CDATA[
<div> algorithm, distributed, reduced-order models, aerospace engineering, parallel

Summary:<br />
This paper introduces distributed Operator Inference (dOpInf), a parallel algorithm designed for constructing physics-based reduced-order models (ROMs) in large-scale aerospace engineering simulations. The algorithm efficiently processes high-dimensional datasets using distributed computing, enabling the learning of structured ROMs that approximate underlying dynamical systems. dOpInf is scalable, allowing for fully parallelized reduced modeling on thousands of processors. The resulting ROMs are computationally inexpensive, suitable for tasks such as design exploration, risk assessment, and uncertainty quantification. A tutorial using a 2D Navier-Stokes flow case study is provided to guide users through implementation, making dOpInf accessible for integration into complex aerospace simulations.<br /> <div>
arXiv:2504.14338v1 Announce Type: cross 
Abstract: Motivated by the large-scale nature of modern aerospace engineering simulations, this paper presents a detailed description of distributed Operator Inference (dOpInf), a recently developed parallel algorithm designed to efficiently construct physics-based reduced-order models (ROMs) for problems with large state dimensions. One such example is the simulation of rotating detonation rocket engines, where snapshot data generated by high-fidelity large-eddy simulations have many millions of degrees of freedom. dOpInf enables, via distributed computing, the efficient processing of datasets with state dimensions that are too large to process on a single computer, and the learning of structured physics-based ROMs that approximate the dynamical systems underlying those datasets. All elements of dOpInf are scalable, leading to a fully parallelized reduced modeling approach that can scale to the thousands of processors available on leadership high-performance computing platforms. The resulting ROMs are computationally cheap, making them ideal for key engineering tasks such as design space exploration, risk assessment, and uncertainty quantification. To illustrate the practical application of dOpInf, we provide a step-by-step tutorial using a 2D Navier-Stokes flow over a step scenario as a case study. This tutorial guides users through the implementation process, making dOpInf accessible for integration into complex aerospace engineering simulations.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
<div> Dialogue framework, teaching capabilities, evaluation, language models, pedagogical effectiveness<br />
Summary:<br />
EducationQ is a multi-agent dialogue framework designed to assess the teaching capabilities of large language models (LLMs). Evaluating 14 LLMs from major AI organizations, the study found that teaching effectiveness does not solely rely on model scale or general reasoning abilities. Smaller open-source models sometimes outperformed larger commercial ones in educational contexts. The research highlighted a gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Through a mixed-methods approach combining quantitative metrics, qualitative analysis, and expert case studies, distinct pedagogical strengths of top-performing models were identified. Human expert evaluations supported the automated qualitative analysis of effective teaching behaviors. The study suggests that LLMs-as-teachers require specialized optimization beyond simple scaling, indicating a need for targeted enhancement of specific pedagogical effectiveness in future educational AI development.<br /> 
Summary: <div>
arXiv:2504.14928v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues</title>
<link>https://arxiv.org/abs/2504.14963</link>
<guid>https://arxiv.org/abs/2504.14963</guid>
<content:encoded><![CDATA[
<div> acoustic features, speaker identification, textual data, fuzzy fingerprints, pre-trained models
Summary:
This study explores the use of fuzzy fingerprints from large pre-trained models to enhance text-based speaker identification. By incorporating speaker-specific tokens and context-aware modeling, the accuracy of speaker identification from text reaches up to 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. The approach of using fuzzy fingerprints enables approximation of full fine-tuning performance with fewer hidden units, enhancing interpretability. The analysis includes a mechanism to identify speaker-agnostic lines and handle ambiguous utterances. The study highlights the challenges in text-based speaker identification and offers insights for future improvements. 
<br /><br />Summary: <div>
arXiv:2504.14963v1 Announce Type: cross 
Abstract: Speaker identification using voice recordings leverages unique acoustic features, but this approach fails when only textual data is available. Few approaches have attempted to tackle the problem of identifying speakers solely from text, and the existing ones have primarily relied on traditional methods. In this work, we explore the use of fuzzy fingerprints from large pre-trained models to improve text-based speaker identification. We integrate speaker-specific tokens and context-aware modeling, demonstrating that conversational context significantly boosts accuracy, reaching 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering improved interpretability. Finally, we analyze ambiguous utterances and propose a mechanism to detect speaker-agnostic lines. Our findings highlight key challenges and provide insights for future improvements in text-based speaker identification.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends</title>
<link>https://arxiv.org/abs/2405.08542</link>
<guid>https://arxiv.org/abs/2405.08542</guid>
<content:encoded><![CDATA[
<div> Metaverse, Industrial Production, XR, Blockchain, AI

Summary:
The article explores the potential of the Industrial Metaverse in enhancing industrial production through technologies such as XR, blockchain, AI, digital twin, and 6G. It discusses the advantages of using the Metaverse in industrial settings and the key enabling technologies for various aspects of production operations. Challenges such as security concerns, resource limitations, and interoperability issues are identified, along with existing solutions to address them. The article also outlines future research directions and open issues in the Industrial Metaverse as it continues to evolve in the industrial production field. <div>
arXiv:2405.08542v2 Announce Type: replace 
Abstract: As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks. This deployment, called the Industrial Metaverse, is used for product design, production operations, industrial quality inspection, and product testing. However, there lacks of in-depth understanding of the enabling technologies associated with the Industrial Metaverse. This encompasses both the precise industrial scenarios targeted by each technology and the potential migration of technologies developed in other domains to the industrial sector. Driven by this issue, in this article, we conduct a comprehensive survey of the state-of-the-art literature on the Industrial Metaverse. Specifically, we first analyze the advantages of the Metaverse for industrial production. Then, we review a collection of key enabling technologies of the Industrial Metaverse, including blockchain (BC), digital twin (DT), 6G, XR, and artificial intelligence (AI), and analyze how these technologies can support different aspects of industrial production. Subsequently, we present numerous formidable challenges encountered within the Industrial Metaverse, including confidentiality and security concerns, resource limitations, and interoperability constraints. Furthermore, we investigate the extant solutions devised to address them. Finally, we briefly outline several open issues and future research directions of the Industrial Metaverse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An optimization-based coupling of reduced order models with efficient reduced adjoint basis generation approach</title>
<link>https://arxiv.org/abs/2408.14450</link>
<guid>https://arxiv.org/abs/2408.14450</guid>
<content:encoded><![CDATA[
<div> Optimization-based coupling, Lagrange multiplier, multiple modeling, simulation, reduced order models<br />
<br />
Summary: Optimization-based coupling (OBC) offers a promising approach in various modeling and simulation scenarios but faces challenges in time-dependent problems due to computational costs. This paper introduces an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission issue, utilizing reduced order models to alleviate computational burdens. The "optimize-then-reduce" strategy is employed to solve the minimization problem at each time step efficiently. A key innovation is the development of a technique for effective adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. Numerical experiments validate the accuracy of the proposed approach while comparing different methods for selecting reduced order bases for adjoint systems. Criteria such as decay of snapshot energy, average iteration counts, and timings are assessed to demonstrate the effectiveness of the optimization-based ROM-ROM coupling. <div>
arXiv:2408.14450v2 Announce Type: replace 
Abstract: Optimization-based coupling (OBC) is an attractive alternative to traditional Lagrange multiplier approaches in multiple modeling and simulation contexts. However, application of OBC to time-dependent problems has been hindered by the computational cost of finding the stationary points of the associated Lagrangian, which requires primal and adjoint solves. This issue can be mitigated by using OBC in conjunction with computationally efficient reduced order models (ROM). To demonstrate the potential of this combination, in this paper we develop an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission problem. We pursue the ``optimize-then-reduce'' path towards solving the minimization problem at each timestep and solve reduced-space adjoint system of equations, where the main challenge in this formulation is the generation of adjoint snapshots and reduced bases for the adjoint systems required by the optimizer. One of the main contributions of the paper is a new technique for efficient adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. We present numerical studies demonstrating the accuracy of the approach along with comparison between various approaches for selecting a reduced order basis for the adjoint systems, including decay of snapshot energy, average iteration counts, and timings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioFit: A WebGL-Based Tool for Fast and Efficient Parameterization of Cardiac Action Potential Models to Fit User-Provided Data</title>
<link>https://arxiv.org/abs/2504.13274</link>
<guid>https://arxiv.org/abs/2504.13274</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiac action potential, Particle swarm optimization, Interactive tool, Model parameter fitting, Physiological dynamics

Summary:
Interactive browser-based tool presented in this study utilizes particle swarm optimization (PSO) algorithm to find model parameter sets that accurately reproduce cardiac dynamics from user-provided data. The tool offers rapid customization and can achieve low-error fittings in a few iterations, requiring only a few seconds of runtime on typical machines. Users can select parameters to fit, define their value ranges, and adjust PSO algorithm hyperparameters through a user-friendly webpage interface. Various models were successfully fitted to different datasets, demonstrating the tool's versatility and efficiency. Convergence of fitting is influenced by the choice of model, dataset characteristics, and PSO algorithm settings. Through these fittings, new insights were gained regarding the physiological and dynamical implications of the model parameters.  <br /><br />Summary: <div>
arXiv:2504.13274v1 Announce Type: new 
Abstract: Cardiac action potential models allow examination of a variety of cardiac dynamics, including how behavior may change under specific interventions. To study a specific scenario, including patient-specific cases, model parameter sets must be found that accurately reproduce the dynamics of interest. To facilitate this complex and time-consuming process, we present an interactive browser-based tool that uses the particle swarm optimization (PSO) algorithm implemented in JavaScript and taking advantage of the WebGL API for hardware acceleration. Our tool allows rapid customization and can find low-error fittings to user-provided voltage time series or action potential duration data from multiple cycle lengths in a few iterations (10-32), corresponding to a runtime of a few seconds on most machines. Additionally, our tool focuses on ease of use and flexibility, providing a webpage interface that allows users to select a subset of parameters to fit, set the range of values each parameter is allowed to assume, and control the PSO algorithm hyperparameters. We demonstrate our tool's utility by fitting a variety of models to different datasets, showing how convergence is affected by model choice, dataset properties, and PSO algorithmic settings, and explaining new insights gained about the physiological and dynamical roles of the model parameters.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ascribe New Dimensions to Scientific Data Visualization with VR</title>
<link>https://arxiv.org/abs/2504.13448</link>
<guid>https://arxiv.org/abs/2504.13448</guid>
<content:encoded><![CDATA[
<div> Keywords: computer mouse, scientific images, Virtual Reality, AI-driven algorithms, multimodal analysis<br />
<br />
Summary: <br />
The article discusses the limitations of the traditional 2D visualization methods in exploring complex, multi-scale scientific images using a computer mouse. It introduces ASCRIBE-VR, a Virtual Reality platform that integrates AI-driven algorithms with scientific images. ASCRIBE-VR allows for immersive and interactive visualization, supporting the analysis of advanced datasets such as X-ray CT and Magnetic Resonance. The VR tools are compatible with Meta Quest and can seamlessly explore large-scale 3D images by merging AI-generated results with VR visualization. This integration enhances scientific discovery by bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins. <div>
arXiv:2504.13448v1 Announce Type: cross 
Abstract: For over half a century, the computer mouse has been the primary tool for interacting with digital data, yet it remains a limiting factor in exploring complex, multi-scale scientific images. Traditional 2D visualization methods hinder intuitive analysis of inherently 3D structures. Virtual Reality (VR) offers a transformative alternative, providing immersive, interactive environments that enhance data comprehension. This article introduces ASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research with Immersive Browsing \& Exploration, which integrates AI-driven algorithms with scientific images. ASCRIBE-VR enables multimodal analysis, structural assessments, and immersive visualization, supporting scientific visualization of advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D imaging. Our VR tools, compatible with Meta Quest, can consume the output of our AI-based segmentation and iterative feedback processes to enable seamless exploration of large-scale 3D images. By merging AI-generated results with VR visualization, ASCRIBE-VR enhances scientific discovery, bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Models Meet Financial Data Modalities</title>
<link>https://arxiv.org/abs/2504.13521</link>
<guid>https://arxiv.org/abs/2504.13521</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithmic trading, deep learning, limit order book, high-frequency trading, predictive performance

Summary: 
This study explores the integration of deep learning models with various financial data sources to improve algorithmic trading strategies and portfolio optimization. By developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation, the researchers achieved state-of-the-art performance in high-frequency trading algorithms. This approach underscores the effectiveness of deep learning in handling structured financial data and enhancing predictive performance in trading strategies. The study highlights the potential of incorporating limit order book analysis into algorithmic trading using deep learning models. By leveraging deep learning techniques, the researchers were able to extract meaningful signals from diverse financial data sources, including candlestick charts, order statistics, traded volume data, and news flow. This research contributes to bridging the gap between deep learning and structured financial data analysis, showcasing the promise of deep learning in financial applications. 

Summary: <div>
arXiv:2504.13521v1 Announce Type: cross 
Abstract: Algorithmic trading relies on extracting meaningful signals from diverse financial data sources, including candlestick charts, order statistics on put and canceled orders, traded volume data, limit order books, and news flow. While deep learning has demonstrated remarkable success in processing unstructured data and has significantly advanced natural language processing, its application to structured financial data remains an ongoing challenge. This study investigates the integration of deep learning models with financial data modalities, aiming to enhance predictive performance in trading strategies and portfolio optimization. We present a novel approach to incorporating limit order book analysis into algorithmic trading by developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation. Our methodology for processing limit order book data achieves state-of-the-art performance in high-frequency trading algorithms, underscoring the effectiveness of deep learning in financial applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bitcoin's Edge: Embedded Sentiment in Blockchain Transactional Data</title>
<link>https://arxiv.org/abs/2504.13598</link>
<guid>https://arxiv.org/abs/2504.13598</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency, Blockchain, Natural Language Processing, Sentiment Analysis, Financial Predictions

Summary:
Blockchain technology is not only used for financial transactions but also for storing and sharing non-financial content. This hidden content can impact cryptocurrency price movements by conveying private information and shaping public sentiment. Current methods of analyzing blockchain data are limited, prompting the use of Natural Language Processing techniques to extract sentiment from transactional data. The study demonstrates the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency prices on Bitcoin and Ethereum blockchains. It uncovers an informational advantage for Bitcoin over Ethereum, showing that sentiment analysis can effectively predict price movements. This research highlights the value of blockchain sentiment analysis in enhancing financial predictions within cryptocurrency markets, providing a novel framework for leveraging freely available, transparent, and immutable data for informed decision-making in the digital asset space. 

<br /><br />Summary: <div>
arXiv:2504.13598v1 Announce Type: cross 
Abstract: Cryptocurrency blockchains, beyond their primary role as distributed payment systems, are increasingly used to store and share arbitrary content, such as text messages and files. Although often non-financial, this hidden content can impact price movements by conveying private information, shaping sentiment, and influencing public opinion. However, current analyses of such data are limited in scope and scalability, primarily relying on manual classification or hand-crafted heuristics. In this work, we address these limitations by employing Natural Language Processing techniques to analyze, detect patterns, and extract public sentiment encoded within blockchain transactional data. Using a variety of Machine Learning techniques, we showcase for the first time the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency price movements on the Bitcoin and Ethereum blockchains. Our findings shed light on a previously underexplored source of freely available, transparent, and immutable data and introduce blockchain sentiment analysis as a novel and robust framework for enhancing financial predictions in cryptocurrency markets. Incidentally, we discover an asymmetry between cryptocurrencies; Bitcoin has an informational advantage over Ethereum in that the sentiment embedded into transactional data is sufficient to predict its price movement.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems</title>
<link>https://arxiv.org/abs/2504.13768</link>
<guid>https://arxiv.org/abs/2504.13768</guid>
<content:encoded><![CDATA[
<div> Graph neural network, multi-body dynamical systems, internal loads, predictive maintenance, digital twin <br />
Summary: Equi-Euler GraphNet is proposed for accurate real-time modeling of multi-body dynamical systems. It simultaneously predicts internal forces and global trajectories, crucial for fault detection and predictive maintenance in digital twin applications. It introduces equivariant message-passing and temporal-aware iterative node update mechanisms, tailored for cylindrical roller bearings. Trained on high-fidelity simulations, it generalizes well and outperforms state-of-the-art models in trajectory prediction. Equi-Euler GraphNet achieves efficient and stable rollouts over thousands of time steps with minimal error accumulation, providing a 200x speedup over conventional solvers. This makes it a valuable tool for digital twins, design, and maintenance applications. <br /> <div>
arXiv:2504.13768v1 Announce Type: cross 
Abstract: Accurate real-time modeling of multi-body dynamical systems is essential for enabling digital twin applications across industries. While many data-driven approaches aim to learn system dynamics, jointly predicting internal loads and system trajectories remains a key challenge. This dual prediction is especially important for fault detection and predictive maintenance, where internal loads-such as contact forces-act as early indicators of faults, reflecting wear or misalignment before affecting motion. These forces also serve as inputs to degradation models (e.g., crack growth), enabling damage prediction and remaining useful life estimation. We propose Equi-Euler GraphNet, a physics-informed graph neural network (GNN) that simultaneously predicts internal forces and global trajectories in multi-body systems. In this mesh-free framework, nodes represent system components and edges encode interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an equivariant message-passing scheme, interpreting edge messages as interaction forces consistent under Euclidean transformations; and (2) a temporal-aware iterative node update mechanism, based on Euler integration, to capture influence of distant interactions over time. Tailored for cylindrical roller bearings, it decouples ring dynamics from constrained motion of rolling elements. Trained on high-fidelity multiphysics simulations, Equi-Euler GraphNet generalizes beyond the training distribution, accurately predicting loads and trajectories under unseen speeds, loads, and configurations. It outperforms state-of-the-art GNNs focused on trajectory prediction, delivering stable rollouts over thousands of time steps with minimal error accumulation. Achieving up to a 200x speedup over conventional solvers while maintaining comparable accuracy, it serves as an efficient reduced-order model for digital twins, design, and maintenance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer Encoder and Multi-features Time2Vec for Financial Prediction</title>
<link>https://arxiv.org/abs/2504.13801</link>
<guid>https://arxiv.org/abs/2504.13801</guid>
<content:encoded><![CDATA[
<div> Transformers, financial prediction, time series analysis, attention mechanism, correlation feature selection<br />
<br />
Summary:<br />
This paper introduces a novel neural network architecture that combines Time2Vec with the Encoder of the Transformer model for financial prediction. By studying different markets, a correlation feature selection method is proposed to improve the accuracy of predicting multiple stock prices. Through fine-tuning hyperparameters, the method outperforms benchmark models and traditional encoding methods like positional encoding. The integration of Time2Vec with the Transformer model allows for the capture of both short and long-range dependencies, aiding in understanding broader market trends. Selecting correlation features further enhances prediction accuracy, particularly in industries where companies exhibit correlated stock price movements. <div>
arXiv:2504.13801v1 Announce Type: cross 
Abstract: Financial prediction is a complex and challenging task of time series analysis and signal processing, expected to model both short-term fluctuations and long-term temporal dependencies. Transformers have remarkable success mostly in natural language processing using attention mechanism, which also influenced the time series community. The ability to capture both short and long-range dependencies helps to understand the financial market and to recognize price patterns, leading to successful applications of Transformers in stock prediction. Although, the previous research predominantly focuses on individual features and singular predictions, that limits the model's ability to understand broader market trends. In reality, within sectors such as finance and technology, companies belonging to the same industry often exhibit correlated stock price movements.
  In this paper, we develop a novel neural network architecture by integrating Time2Vec with the Encoder of the Transformer model. Based on the study of different markets, we propose a novel correlation feature selection method. Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a comparative analysis of our results against benchmark models. We conclude that our method outperforms other state-of-the-art encoding methods such as positional encoding, and we also conclude that selecting correlation features enhance the accuracy of predicting multiple stock prices.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding</title>
<link>https://arxiv.org/abs/2408.11363</link>
<guid>https://arxiv.org/abs/2408.11363</guid>
<content:encoded><![CDATA[
<div> Keywords: ProteinGPT, large language model, protein sequences, protein structures, protein analysis <br />
Summary: ProteinGPT is a cutting-edge multimodal large language model designed for analyzing protein sequences and structures efficiently. By integrating protein sequence and structure encoders with a large language model, ProteinGPT can provide accurate and contextually relevant responses to protein-related queries. The model was trained on a dataset of 132,092 proteins, each annotated with property tags and QA pairs, using GPT-4o for instruction tuning. Experimental results show that ProteinGPT outperforms baseline models and general-purpose LLMs in understanding and responding to protein-related questions, achieving high performance on both semantic and lexical metrics. The code and data for ProteinGPT are available on GitHub, making this powerful tool accessible to researchers in the field. <br /><br /> <div>
arXiv:2408.11363v2 Announce Type: replace-cross 
Abstract: Understanding biological processes, drug development, and biotechnological advancements requires a detailed analysis of protein structures and functions, a task that is inherently complex and time-consuming in traditional protein research. To streamline this process, we introduce ProteinGPT, a state-of-the-art multimodal large language model for proteins that enables users to upload protein sequences and/or structures for comprehensive analysis and responsive inquiries. ProteinGPT integrates protein sequence and structure encoders with linear projection layers to ensure precise representation adaptation and leverages a large language model (LLM) to generate accurate, contextually relevant responses. To train ProteinGPT, we constructed a large-scale dataset of 132,092 proteins, each annotated with 20-30 property tags and 5-10 QA pairs per protein, and optimized the instruction-tuning process using GPT-4o. Experiments demonstrate that ProteinGPT effectively generates informative responses to protein-related questions, achieving high performance on both semantic and lexical metrics and significantly outperforming baseline models and general-purpose LLMs in understanding and responding to protein-related queries. Our code and data are available at https://github.com/ProteinGPT/ProteinGPT.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Accurate Prediction of Antenna Reflection Coefficients in Planar Layered Media Environment via Generalized Scattering Matrix</title>
<link>https://arxiv.org/abs/2504.12613</link>
<guid>https://arxiv.org/abs/2504.12613</guid>
<content:encoded><![CDATA[
<div> Keywords: reflection coefficient, antenna, planar layered medium, generalized scattering matrix, numerical algorithm

Summary: 
The article presents a new numerical algorithm for evaluating the reflection coefficient of an antenna in the presence of a planar layered medium. This algorithm utilizes the antenna's generalized scattering matrix (GSM) to model the interaction between the antenna and the medium through spherical-to-planar vector wave transformations. By avoiding approximations that could compromise accuracy, the algorithm reduces algebraic complexity and significantly speeds up antenna performance evaluation. While a one-time preprocessing cost is required to obtain the antenna's GSM in free space, the numerical evaluation speed of this method surpasses that of commercial software FEKO by several orders of magnitude, maintaining high accuracy. This advancement in computational efficiency offers a promising approach for accurately evaluating antenna performance in complex scenarios involving layered media. 

<br /><br />Summary: <div>
arXiv:2504.12613v1 Announce Type: new 
Abstract: The numerical algorithm for evaluating the reflection coefficient of an antenna in the presence of the planar layered medium is reformulated using the antenna's generalized scattering matrix (GSM). The interaction between the antenna and the layered medium is modeled through spherical-to-planar vector wave transformations, ensuring no approximations that could compromise computational accuracy. This theoretical framework significantly reduces algebraic complexity, resulting in a marked increase in the speed of antenna performance evaluation. Excluding the one-time preprocessing cost of obtaining the antenna's GSM in free space, the numerical evaluation speed of this method exceeds that of the commercial software FEKO by several orders of magnitude, while maintaining nearly identical accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study</title>
<link>https://arxiv.org/abs/2504.12503</link>
<guid>https://arxiv.org/abs/2504.12503</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, engineering design, continual learning, regression tasks, catastrophic forgetting

Summary:
Continual learning (CL) is introduced to engineering design to address the challenge of incorporating new knowledge into models without retraining from scratch, which is computationally expensive. This study benchmarks various CL methods on regression tasks using five engineering datasets and establishes nine new engineering CL benchmarks. The results show that existing CL methods can improve performance compared to naive baselines, with the Replay strategy achieving comparable performance to retraining while reducing training time significantly. This points to the potential of CL in enhancing real-world engineering workflows. The code and datasets used in the study will be made available for further research and applications. 

<br /><br />Summary: <div>
arXiv:2504.12503v1 Announce Type: cross 
Abstract: Engineering problems that apply machine learning often involve computationally intensive methods but rely on limited datasets. As engineering data evolves with new designs and constraints, models must incorporate new knowledge over time. However, high computational costs make retraining models from scratch infeasible. Continual learning (CL) offers a promising solution by enabling models to learn from sequential data while mitigating catastrophic forgetting, where a model forgets previously learned mappings. This work introduces CL to engineering design by benchmarking several CL methods on representative regression tasks. We apply these strategies to five engineering datasets and construct nine new engineering CL benchmarks to evaluate their ability to address forgetting and improve generalization. Preliminary results show that applying existing CL methods to these tasks improves performance over naive baselines. In particular, the Replay strategy achieved performance comparable to retraining in several benchmarks while reducing training time by nearly half, demonstrating its potential for real-world engineering workflows. The code and datasets used in this work will be available at: https://github.com/kmsamuel/cl-for-engineering-release.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational quantum and neural quantum states algorithms for the linear complementarity problem</title>
<link>https://arxiv.org/abs/2504.08141</link>
<guid>https://arxiv.org/abs/2504.08141</guid>
<content:encoded><![CDATA[
<div> variational quantum algorithms, VQAs, hybrid quantum-classical methods, variational quantum linear solver, VQLS

Summary:
Variational quantum algorithms (VQAs) are being explored as a way to harness quantum computing while addressing current hardware limitations. This study introduces the variational quantum linear solver (VQLS) and its classical counterpart, the variational neural linear solver (VNLS), within a minimum map Newton solver for a rigid body contact model. The VNLS accurately simulates the dynamics of rigid spherical bodies during collisions, suggesting that quantum and quantum-inspired linear algebra algorithms can be effective for modeling certain physical systems. This research opens up the possibility of using VQAs and quantum-inspired classical algorithms to solve real-world problems, highlighting their potential utility in various applications. <br /><br />Summary: <div>
arXiv:2504.08141v2 Announce Type: replace 
Abstract: Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Editing for Offline Model-based Optimization</title>
<link>https://arxiv.org/abs/2405.13964</link>
<guid>https://arxiv.org/abs/2405.13964</guid>
<content:encoded><![CDATA[
<div> Offline model-based optimization, surrogate model, out-of-distribution issue, diffusion prior, Design Editing for Offline Model-based Optimization (DEMO)<br />
Summary:<br />
Offline model-based optimization (MBO) involves maximizing a black-box objective function using a dataset of designs and scores. Traditional methods use surrogate models but are prone to errors when predicting scores for new designs. To address this, DEMO introduces a diffusion prior to calibrate overly optimized designs. It generates pseudo design candidates using gradient ascent, then refines them through an editing process involving noise and denoising with the diffusion prior. Empirical evaluations across seven tasks show that DEMO, with proper tuning, achieves competitive scores compared to previous literature. <div>
arXiv:2405.13964v4 Announce Type: replace-cross 
Abstract: Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. While these pseudo design candidates contain information beyond the offline dataset, they might be invalid or have erroneously high predicted scores. Therefore, to address this challenge while utilizing the information provided by pseudo design candidates, we propose an editing process to refine these pseudo design candidates. We introduce noise to the pseudo design candidates and subsequently denoise them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. Empirical evaluations on seven offline MBO tasks show that, with properly tuned hyperparameters, DEMOs score is competitive with the best previously reported scores in the literature.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Contextual Market Equilibrium Computation through Deep Learning</title>
<link>https://arxiv.org/abs/2406.15459</link>
<guid>https://arxiv.org/abs/2406.15459</guid>
<content:encoded><![CDATA[
<div> Market equilibrium, computational economics, large-scale buyer population, contextual market model, deep learning-based method<br />
<br />
Summary: 
The paper explores the computation of market equilibrium in scenarios with a large-scale buyer population using a deep learning-based method called MarketFCNet. The approach introduces a contextual market model where buyers and goods are represented by their contexts. MarketFCNet uses a neural network to parameterize the allocation of goods to buyers based on their contexts. An efficient method is proposed to estimate the loss function for training the network, enabling optimization through gradient descent. The Nash Gap metric is introduced to measure the deviation of the allocation and prices from the market equilibrium. Experimental results show that MarketFCNet outperforms existing methods in terms of performance and running times as the market scale increases, showcasing the potential of deep learning in approximating large-scale contextual market equilibrium. <br /><br /> <div>
arXiv:2406.15459v2 Announce Type: replace-cross 
Abstract: Market equilibrium is one of the most fundamental solution concepts in economics and social optimization analysis. Existing works on market equilibrium computation primarily focus on settings with relatively few buyers. Motivated by this, our paper investigates the computation of market equilibrium in scenarios with a large-scale buyer population, where buyers and goods are represented by their contexts. Building on this realistic and generalized contextual market model, we introduce MarketFCNet, a deep learning-based method for approximating market equilibrium. We start by parameterizing the allocation of each good to each buyer using a neural network, which depends solely on the context of the buyer and the good. Next, we propose an efficient method to unbiasedly estimate the loss function of the training algorithm, enabling us to optimize the network parameters through gradient. To evaluate the approximated solution, we propose a metric called Nash Gap, which quantifies the deviation of the given allocation and price pair from the market equilibrium. Experimental results indicate that MarketFCNet delivers competitive performance and significantly lower running times compared to existing methods as the market scale expands, demonstrating the potential of deep learning-based methods to accelerate the approximation of large-scale contextual market equilibrium.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrivAerML: High-Fidelity Computational Fluid Dynamics Dataset for Road-Car External Aerodynamics</title>
<link>https://arxiv.org/abs/2408.11969</link>
<guid>https://arxiv.org/abs/2408.11969</guid>
<content:encoded><![CDATA[
<div> Machine Learning, automotive aerodynamics, open-source dataset, high-fidelity CFD, DrivAer notchback<br />
Summary:<br />
Machine Learning has the potential to transform automotive aerodynamics by providing quick flow predictions during the design phase. However, a lack of open-source training data for realistic road cars using high-fidelity CFD methods is impeding progress. To overcome this hurdle, a high-fidelity open-source dataset for automotive aerodynamics has been created. The dataset comprises 500 variants of the DrivAer notchback vehicle, generated through parametric morphing. Mesh generation and scale-resolving CFD were conducted using state-of-the-art automated workflows. The dataset, published under the CC-BY-SA license, includes geometries and detailed aerodynamic information. This initiative marks the first large public-domain dataset for complex automotive configurations derived from high-fidelity CFD simulations. <div>
arXiv:2408.11969v2 Announce Type: replace-cross 
Abstract: Machine Learning (ML) has the potential to revolutionise the field of automotive aerodynamics, enabling split-second flow predictions early in the design process. However, the lack of open-source training data for realistic road cars, using high-fidelity CFD methods, represents a barrier to their development. To address this, a high-fidelity open-source (CC-BY-SA) public dataset for automotive aerodynamics has been generated, based on 500 parametrically morphed variants of the widely-used DrivAer notchback generic vehicle. Mesh generation and scale-resolving CFD was executed using consistent and validated automatic workflows representative of the industrial state-of-the-art. Geometries and rich aerodynamic data are published in open-source formats. To our knowledge, this is the first large, public-domain dataset for complex automotive configurations generated using high-fidelity CFD.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Algorithms for Option Hedging</title>
<link>https://arxiv.org/abs/2504.05521</link>
<guid>https://arxiv.org/abs/2504.05521</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, dynamic hedging, financial assets, risk, DRL algorithms<br />
<br />
Summary: 
Dynamic hedging involves offsetting financial risk through periodic transactions. This study compares the performance of eight DRL algorithms in dynamic hedging scenarios. The algorithms include Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), various Deep Q-Learning (DQL) and Deep Deterministic Policy Gradient (DDPG) variants. The experiments use a GJR-GARCH(1,1) model to simulate the dataset and evaluate against the Black-Scholes delta hedge baseline. Results show that MCPG and PPO perform best in terms of the root semi-quadratic penalty. MCPG outperforms the baseline within the allotted computational budget due to sparse rewards in the environment. This comparison provides insights into the effectiveness of different DRL algorithms in dynamic hedging strategies, with potential implications for financial risk management. 
<br /><br />Summary: <div>
arXiv:2504.05521v2 Announce Type: replace-cross 
Abstract: Dynamic hedging is a financial strategy that consists in periodically transacting one or multiple financial assets to offset the risk associated with a correlated liability. Deep Reinforcement Learning (DRL) algorithms have been used to find optimal solutions to dynamic hedging problems by framing them as sequential decision-making problems. However, most previous work assesses the performance of only one or two DRL algorithms, making an objective comparison across algorithms difficult. In this paper, we compare the performance of eight DRL algorithms in the context of dynamic hedging; Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), along with four variants of Deep Q-Learning (DQL) and two variants of Deep Deterministic Policy Gradient (DDPG). Two of these variants represent a novel application to the task of dynamic hedging. In our experiments, we use the Black-Scholes delta hedge as a baseline and simulate the dataset using a GJR-GARCH(1,1) model. Results show that MCPG, followed by PPO, obtain the best performance in terms of the root semi-quadratic penalty. Moreover, MCPG is the only algorithm to outperform the Black-Scholes delta hedge baseline with the allotted computational budget, possibly due to the sparsity of rewards in our environment.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal integration of chemical structures improves representations of microscopy images for morphological profiling</title>
<link>https://arxiv.org/abs/2504.09544</link>
<guid>https://arxiv.org/abs/2504.09544</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, deep learning, morphological profiling, high-throughput microscopy screens, chemical compound structure

Summary:
Recent advances in self-supervised deep learning have led to improved quantification of cellular morphological changes in high-throughput microscopy screens. The new framework, MICON (Molecular-Image Contrastive Learning), incorporates chemical compound information during pre-training to enhance learned image representations. MICON surpasses traditional feature extraction methods like CellProfiler and existing deep learning approaches, especially in identifying consistent effects of drugs across independent replicates and data centers. By modeling chemical compounds as treatments inducing counterfactual transformations of cell phenotypes, MICON outperforms methods that directly align images and compounds. This highlights the importance of considering the multimodal nature of microscopy screening data in representation learning for morphological profiling. MICON's success suggests a promising direction for future research in this field.<br /><br />Summary: <div>
arXiv:2504.09544v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised deep learning have improved our ability to quantify cellular morphological changes in high-throughput microscopy screens, a process known as morphological profiling. However, most current methods only learn from images, despite many screens being inherently multimodal, as they involve both a chemical or genetic perturbation as well as an image-based readout. We hypothesized that incorporating chemical compound structure during self-supervised pre-training could improve learned representations of images in high-throughput microscopy screens. We introduce a representation learning framework, MICON (Molecular-Image Contrastive Learning), that models chemical compounds as treatments that induce counterfactual transformations of cell phenotypes. MICON significantly outperforms classical hand-crafted features such as CellProfiler and existing deep-learning-based representation learning methods in challenging evaluation settings where models must identify reproducible effects of drugs across independent replicates and data-generating centers. We demonstrate that incorporating chemical compound information into the learning process provides consistent improvements in our evaluation setting and that modeling compounds specifically as treatments in a causal framework outperforms approaches that directly align images and compounds in a single representation space. Our findings point to a new direction for representation learning in morphological profiling, suggesting that methods should explicitly account for the multimodal nature of microscopy screening data.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A viscoplasticity model with an invariant-based non-Newtonian flow rule for unidirectional thermoplastic composites</title>
<link>https://arxiv.org/abs/2504.12069</link>
<guid>https://arxiv.org/abs/2504.12069</guid>
<content:encoded><![CDATA[
arXiv:2504.12069v1 Announce Type: new 
Abstract: A three-dimensional mesoscopic viscoplasticity model for simulating rate-dependent plasticity and creep in unidirectional thermoplastic composites is presented. The constitutive model is a transversely isotropic extension of an isotropic finite strain viscoplasticity model for neat polymers. Rate-dependent plasticity and creep are described by a non-Newtonian flow rule where the viscosity of the material depends on an equivalent stress measure through an Eyring-type relation. In the present formulation, transverse isotropy is incorporated by defining the equivalent stress measure and flow rule as functions of transversely isotropic stress invariants. In addition, the Eyring-type viscosity function is extended with anisotropic pressure dependence. As a result of the formulation, plastic flow in fiber direction is effectively excluded and pressure dependence of the polymer matrix is accounted for. The re-orientation of the transversely isotropic plane during plastic deformations is incorporated in the constitutive equations, allowing for an accurate large deformation response. The formulation is fully implicit and a consistent linearization of the algorithmic constitutive equations is performed to derive the consistent tangent modulus. The performance of the mesoscopic constitutive model is assessed through a comparison with a micromechanical model for carbon/PEEK, with the original isotropic viscoplastic version for the polymer matrix and with hyperelastic fibers. The micromodel is first used to determine the material parameters of the mesoscale model with a few stress-strain curves. It is demonstrated that the mesoscale model gives a similar response to the micromodel under various loading conditions. Finally, the mesoscale model is validated against off-axis experiments on unidirectional thermoplastic composite plies.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Material Network: Overview, applications and current directions</title>
<link>https://arxiv.org/abs/2504.12159</link>
<guid>https://arxiv.org/abs/2504.12159</guid>
<content:encoded><![CDATA[
arXiv:2504.12159v1 Announce Type: new 
Abstract: Deep Material Network (DMN) has emerged as a powerful framework for multiscale material modeling, enabling efficient and accurate predictions of material behavior across different length scales. Unlike traditional machine learning approaches, the trainable parameters in DMN have direct physical interpretations, capturing the geometric characteristics of the microstructure rather than serving as purely statistical fitting parameters. Its hierarchical tree structure effectively encodes microstructural interactions and deformation mechanisms, allowing DMN to achieve a balance between accuracy and computational efficiency. This physics-informed architecture significantly reduces computational costs compared to direct numerical simulations while preserving essential microstructural physics. Furthermore, DMN can be trained solely on a linear elastic dataset while effectively extrapolating nonlinear responses during online prediction, making it a highly efficient and scalable approach for multiscale material modeling. This article provides a comprehensive review of DMN, detailing its motivation, underlying methodology, and recent advancements. We discuss key modeling aspects, including its hierarchical structure, training process, and the role of physics-based constraints in enhancing predictive accuracy. Furthermore, we highlight its applications in component-scale multiscale analysis and inverse parameter identification, demonstrating its capability to bridge microscale material behavior with macroscale engineering predictions. Finally, we discuss challenges and future directions in improving DMN's generalization capabilities and its potential extensions for broader applications in multiscale modeling.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Analysis of Mixer Activities in the Bitcoin Network</title>
<link>https://arxiv.org/abs/2504.11924</link>
<guid>https://arxiv.org/abs/2504.11924</guid>
<content:encoded><![CDATA[
arXiv:2504.11924v1 Announce Type: cross 
Abstract: Cryptocurrency users increasingly rely on obfuscation techniques such as mixers, swappers, and decentralised or no-KYC exchanges to protect their anonymity. However, at the same time, these services are exploited by criminals to conceal and launder illicit funds. Among obfuscation services, mixers remain one of the most challenging entities to tackle. This is because their owners are often unwilling to cooperate with Law Enforcement Agencies, and technically, they operate as 'black boxes'. To better understand their functionalities, this paper proposes an approach to analyse the operations of mixers by examining their address-transaction graphs and identifying topological similarities to uncover common patterns that can define the mixer's modus operandi. The approach utilises community detection algorithms to extract dense topological structures and clustering algorithms to group similar communities. The analysis is further enriched by incorporating data from external sources related to known Exchanges, in order to understand their role in mixer operations. The approach is applied to dissect the Blender.io mixer activities within the Bitcoin blockchain, revealing: i) consistent structural patterns across address-transaction graphs; ii) that Exchanges play a key role, following a well-established pattern, which raises several concerns about their AML/KYC policies. This paper represents an initial step toward dissecting and understanding the complex nature of mixer operations in cryptocurrency networks and extracting their modus operandi.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection</title>
<link>https://arxiv.org/abs/2502.07384</link>
<guid>https://arxiv.org/abs/2502.07384</guid>
<content:encoded><![CDATA[
arXiv:2502.07384v2 Announce Type: replace 
Abstract: Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. At the inter-modality level, SAGEPhos introduces a Bio-Coupled Modal Fusion method, distilling essential kinase sequence information to refine task-oriented local substrate feature space, creating a shared semantic space that captures crucial kinase-substrate interaction patterns. Within the substrate's intra-modality domain, it focuses on Bio-Augmented Fusion, emphasizing 2D local sequence information while selectively incorporating 3D spatial information from predicted structures to complement the sequence space. Moreover, to address the lack of structural information in current datasets, we contribute a new, refined phosphorylation site prediction dataset, which incorporates crucial structural elements and will serve as a new benchmark for the field. Experimental results demonstrate that SAGEPhos significantly outperforms baseline methods. We release the SAGEPhos models and code at https://github.com/ZhangJJ26/SAGEPhos.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QPET: A Versatile and Portable Quantity-of-Interest-Preservation Framework for Error-Bounded Lossy Compression</title>
<link>https://arxiv.org/abs/2412.02799</link>
<guid>https://arxiv.org/abs/2412.02799</guid>
<content:encoded><![CDATA[
arXiv:2412.02799v3 Announce Type: replace-cross 
Abstract: Error-bounded lossy compression has been widely adopted in many scientific domains because it can address the challenges in storing, transferring, and analyzing unprecedented amounts of scientific data. Although error-bounded lossy compression offers general data distortion control by enforcing strict error bounds on raw data, it may fail to meet the quality requirements on the results of downstream analysis, a.k.a. Quantities of Interest (QoIs), derived from raw data. This may lead to uncertainties and even misinterpretations in scientific discoveries, significantly limiting the use of lossy compression in practice. In this paper, we propose QPET, a novel, versatile, and portable framework for QoI-preserving error-bounded lossy compression, which overcomes the challenges of modeling diverse QoIs by leveraging numerical strategies. QPET features (1) high portability to multiple existing lossy compressors, (2) versatile preservation to most differentiable univariate and multivariate QoIs, and (3) significant compression improvements in QoI-preservation tasks. Experiments with six real-world datasets demonstrate that integrating QPET into state-of-the-art error-bounded lossy compressors can gain 2x to 10x compression speedups of existing QoI-preserving error-bounded lossy compression solutions, up to 1000% compression ratio improvements to general-purpose compressors, and up to 133% compression ratio improvements to existing QoI-integrated scientific compressors.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>