<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CE updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CE</link>


<item>
<title>Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener</title>
<link>https://arxiv.org/abs/2507.02005</link>
<guid>https://arxiv.org/abs/2507.02005</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Explainable AI, Fatigue Strength, Welded Steel Structures, Feature Engineering<br />
Summary:<br />
This research presents a unified approach that combines Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. By incorporating expert-driven feature engineering with algorithmic feature creation, the accuracy and explainability of the models are enhanced. The study utilizes regression models such as gradient boosting, random forests, and neural networks trained using AutoML under three feature schemes. Ensemble methods, including CatBoost and LightGBM, demonstrated top performance. The domain-informed model achieved the best balance of test RMSE and $R^2 values. XAI methods identified key predictors such as stress ratio, stress range, yield strength, and post-weld treatment. Secondary geometric factors also significantly influenced fatigue life. The integration of AutoML with XAI leads to accurate, interpretable, and robust fatigue strength models for welded steel structures, facilitating AI-assisted design and assessment. Future research will focus on probabilistic fatigue life modeling and integration into digital twin environments. <br />Summary: <div>
arXiv:2507.02005v1 Announce Type: new 
Abstract: This research introduces a unified approach combining Automated Machine Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict fatigue strength in welded transverse stiffener details. It integrates expert-driven feature engineering with algorithmic feature creation to enhance accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient boosting, random forests, and neural networks - were trained using AutoML under three feature schemes: domain-informed, algorithmic, and combined. This allowed a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE $\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta \sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527% within the engineering-relevant 0 - 150 MPa domain. The denser-feature model ($\mathcal M_3$) showed minor gains during training but poorer generalization, while the simpler base-feature model ($\mathcal M_1$) performed comparably, confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG dressing vs. as-welded) as dominant predictors. Secondary geometric factors - plate width, throat thickness, stiffener height - also significantly affected fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate, interpretable, and robust fatigue strength models for welded steel structures. It bridges data-driven modeling with engineering validation, enabling AI-assisted design and assessment. Future work will explore probabilistic fatigue life modeling and integration into digital twin environments.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Resolution Independent Operator Learning</title>
<link>https://arxiv.org/abs/2507.02524</link>
<guid>https://arxiv.org/abs/2507.02524</guid>
<content:encoded><![CDATA[
<div> Keywords: time-dependent partial differential equations, recurrent DeepONet, Neural Controlled Differential Equation, transient mechanics, operator learning

Summary:
Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data is a challenging task. Existing methods like recurrent DeepONet extensions and neural-ODE surrogates have limitations in handling discrete-time and new inputs after initialization. To address these issues, NCDE-DeepONet is introduced, which combines a Neural Controlled Differential Equation (NCDE) with explicit space-time coordinates to create a continuous-time operator network. The NCDE encodes the entire load history as a solution of a controlled ODE driven by a spline-interpolated input path, allowing for input-resolution-independent representation. The trunk of the network can probe this latent path at arbitrary spatial locations and times, enabling output-resolution independence. Benchmarks on various transient mechanics problems demonstrate the robustness and accuracy of the framework, showcasing almost instant solution prediction. The approach of using controlled dynamics proves to be a principled and efficient method for high-fidelity operator learning in transient mechanics. 

<br /><br />Summary: <div>
arXiv:2507.02524v1 Announce Type: new 
Abstract: Accurately learning solution operators for time-dependent partial differential equations (PDEs) from sparse and irregular data remains a challenging task. Recurrent DeepONet extensions inherit the discrete-time limitations of sequence-to-sequence (seq2seq) RNN architectures, while neural-ODE surrogates cannot incorporate new inputs after initialization. We introduce NCDE-DeepONet, a continuous-time operator network that embeds a Neural Controlled Differential Equation (NCDE) in the branch and augments the trunk with explicit space-time coordinates. The NCDE encodes an entire load history as the solution of a controlled ODE driven by a spline-interpolated input path, making the representation input-resolution-independent: it encodes different input signal discretizations of the observed samples. The trunk then probes this latent path at arbitrary spatial locations and times, rendering the overall map output-resolution independent: predictions can be queried on meshes and time steps unseen during training without retraining or interpolation. Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems confirm the robustness and accuracy of the framework, achieving almost instant solution prediction. These findings suggest that controlled dynamics provide a principled and efficient foundation for high-fidelity operator learning in transient mechanics.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation and Heterogeneity Shape the Resilience of Community Currency Networks</title>
<link>https://arxiv.org/abs/2507.02678</link>
<guid>https://arxiv.org/abs/2507.02678</guid>
<content:encoded><![CDATA[
<div> Keywords: community currency, mutual credit systems, graph theory, behavioral connectivity, network evolution

Summary:
This paper examines community currency networks, focusing on the case study of Sardex in Sardinia, Italy. The analysis is done through a graph theoretic framework, studying strongly connected components, condensed representations, and behavioral connectivity patterns. The evolution of the network over three years is analyzed, revealing temporal contraction, flow imbalances, and structural fragmentation based on user types. The findings show deviations from degree-based models, indicating behavioral imitation among users and a preference for more active peers. The impact of heterogeneous connections between user types is also evaluated, highlighting their role in strengthening the network topology and enhancing resilience.<br /><br />Summary: <div>
arXiv:2507.02678v1 Announce Type: new 
Abstract: Community currency networks are made up of individuals and or companies that share some physical or social characteristics and engage in economic transactions using a virtual currency. This paper investigates the structural and dynamic properties of such mutual credit systems through a case study of Sardex, a community currency initiated and mainly operating in Sardinia, Italy. The transaction network is modeled as a directed weighted graph and analyzed through a graph theoretic framework focused on the analysis of strongly connected components, condensed representations, and behavioral connectivity patterns. Emphasis is placed on understanding the evolution of the network's core and peripheral structures over a three year period, with attention to temporal contraction, flow asymmetries, and structural fragmentation depending on different user types. Our findings reveal persistent deviations from degree based null models and suggest the presence of behavioral imitation, specifically, a user preference for more active peers. We further assess the impact of heterogeneous connections between different type of users, which strengthen the network topology and enhance its resilience.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery</title>
<link>https://arxiv.org/abs/2507.02730</link>
<guid>https://arxiv.org/abs/2507.02730</guid>
<content:encoded><![CDATA[
<div> Physics-Informed Automated Discovery of Kinetics, catalytic processes, kinetic models, symbolic regression, Metropolis-Hastings algorithm <br />
<br />
Summary: The article introduces the Physics-Informed Automated Discovery of Kinetics (PI-ADoK) framework for industrial catalytic processes. Traditional mechanistic models for kinetics demand expertise, while data-driven approaches lack interpretability. PI-ADoK integrates physical constraints into symbolic regression to reduce the search space and experiments needed for model convergence. It includes a Metropolis-Hastings algorithm for uncertainty quantification, providing credible prediction intervals. Benchmarking against conventional methods in catalytic case studies shows PI-ADoK enhances model fidelity and decreases the experimental burden, offering efficient and reliable kinetic model discovery for chemical reaction engineering. <div>
arXiv:2507.02730v1 Announce Type: new 
Abstract: The industrialization of catalytic processes hinges on the availability of reliable kinetic models for design, optimization, and control. Traditional mechanistic models demand extensive domain expertise, while many data-driven approaches often lack interpretability and fail to enforce physical consistency. To overcome these limitations, we propose the Physics-Informed Automated Discovery of Kinetics (PI-ADoK) framework. By integrating physical constraints directly into a symbolic regression approach, PI-ADoK narrows the search space and substantially reduces the number of experiments required for model convergence. Additionally, the framework incorporates a robust uncertainty quantification strategy via the Metropolis-Hastings algorithm, which propagates parameter uncertainty to yield credible prediction intervals. Benchmarking our method against conventional approaches across several catalytic case studies demonstrates that PI-ADoK not only enhances model fidelity but also lowers the experimental burden, highlighting its potential for efficient and reliable kinetic model discovery in chemical reaction engineering.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification</title>
<link>https://arxiv.org/abs/2507.01971</link>
<guid>https://arxiv.org/abs/2507.01971</guid>
<content:encoded><![CDATA[
<div> Keywords: support and resistance levels, deep learning, financial analysis, market microstructure, attention-based architecture

Summary:
DeepSupp is a new deep learning approach designed to detect financial support levels using multi-head attention mechanisms and advanced feature engineering. It leverages dynamic correlation matrices to capture evolving market relationships and uses an attention-based autoencoder for robust representation learning. The final support levels are identified through unsupervised clustering with DBSCAN, resulting in state-of-the-art performance across six financial metrics on S&amp;P 500 tickers. DeepSupp outperforms six baseline methods in essential support accuracy and market regime sensitivity, demonstrating consistent results across diverse market conditions. This approach fills critical gaps in support and resistance level detection, providing a scalable and reliable solution for modern financial analysis. The use of attention-based architectures in DeepSupp uncovers nuanced market patterns and enhances technical trading strategies. <br /><br />Summary: <div>
arXiv:2507.01971v1 Announce Type: cross 
Abstract: Support and resistance (SR) levels are central to technical analysis, guiding traders in entry, exit, and risk management. Despite widespread use, traditional SR identification methods often fail to adapt to the complexities of modern, volatile markets. Recent research has introduced machine learning techniques to address the following challenges, yet most focus on price prediction rather than structural level identification. This paper presents DeepSupp, a new deep learning approach for detecting financial support levels using multi-head attention mechanisms to analyze spatial correlations and market microstructure relationships. DeepSupp integrates advanced feature engineering, constructing dynamic correlation matrices that capture evolving market relationships, and employs an attention-based autoencoder for robust representation learning. The final support levels are extracted through unsupervised clustering, leveraging DBSCAN to identify significant price thresholds. Comprehensive evaluations on S&amp;P 500 tickers demonstrate that DeepSupp outperforms six baseline methods, achieving state-of-the-art performance across six financial metrics, including essential support accuracy and market regime sensitivity. With consistent results across diverse market conditions, DeepSupp addresses critical gaps in SR level detection, offering a scalable and reliable solution for modern financial analysis. Our approach highlights the potential of attention-based architectures to uncover nuanced market patterns and improve technical trading strategies.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Design of Corrugated Boards: A New FEM Modeling and Experimental Validation</title>
<link>https://arxiv.org/abs/2507.02189</link>
<guid>https://arxiv.org/abs/2507.02189</guid>
<content:encoded><![CDATA[
<div> FEM modeling, corrugated boards, homogenization method, Weibull distributions, packaging design<br />
Summary:<br />
The study presents a simplified Finite Element Method (FEM) modeling approach for large structures made of corrugated boards, focusing on customized packages. It utilizes a homogenization method to transform flute geometries into equivalent elastic models, reducing computational time. Correction factors are introduced for internal mechanisms, adjusting the effective elastic modulus and thickness in the presence of large deformations and contact. Two statistical Weibull distributions representing contact and buckling mechanisms in corrugated boards are derived experimentally and validated for computational efficiency. The statistical parameters obtained ($\beta_1 = 0.14$, $\beta_2 = 1.31) can be effectively used for simplistic representation. The research contributes to optimizing corrugated packaging design by simplifying FEM models for faster yet accurate simulations. <br /><br />Summary: <div>
arXiv:2507.02189v1 Announce Type: cross 
Abstract: This study presents a simplified FEM modeling approach suitable for large structures made of corrugated boards, such as customized packages, based on a homogenization method, which is combined with correction factors for internal mechanisms. The homogenization process reduces computational time by transforming flute geometries into equivalent elastic models. In large deformations and in the presence of contact for a given geometry, the effective elastic modulus in the thickness direction, as well as the effective thickness of the structure, are corrected by two statistical Weibull distributions representing the contact and buckling mechanisms in a corrugated board. The Weibull parameters are obtained via experimental analysis, and such a process is then validated. The results demonstrate that the statistical parameters ($\beta_1 = 0.14$, $\beta_2 = 1.31$) can be used for the simplistic representation of corrugated boards, being computationally efficient. This research contributes to the optimization of corrugated packaging design, specifically by simplifying FEM models for faster yet equally accurate simulations.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Scale Finite Element Method for Investigating Fiber Remodeling in Hypertrophic Cardiomyopathy</title>
<link>https://arxiv.org/abs/2507.02193</link>
<guid>https://arxiv.org/abs/2507.02193</guid>
<content:encoded><![CDATA[
<div> fiber disarray, hypertrophic cardiomyopathy, cellular abnormalities, cardiac pumping function, myocardium mechanics <br />
Summary: <br />
- Fiber disarray is a significant hallmark of hypertrophic cardiomyopathy (HCM) and is associated with various cardiac events such as heart failure.
- Heterogeneous distributions of hypercontractility, hypocontractility, and fibrosis contribute to the development of fiber disarray in the myocardium.
- The pattern of fiber disarray varies depending on the specific perturbation, providing insights into the progression of HCM.
- Higher fiber disarray near the epicardium compared to the endocardium suggests the role of regional myocardial mechanics in HCM development.
- Remodeled left ventricles (LVs) with fibrosis and hypocontractility exhibit declined cardiac performance, highlighting the structural and functional consequences of HCM. <br /> <div>
arXiv:2507.02193v1 Announce Type: cross 
Abstract: A significant hallmark of hypertrophic cardiomyopathy (HCM) is fiber disarray, which is associated with various cardiac events such as heart failure. Quantifying fiber disarray remains critical for understanding the disease s complex pathophysiology. This study investigates the role of heterogeneous HCM-induced cellular abnormalities in the development of fiber disarray and their subsequent impact on cardiac pumping function. Fiber disarray is predicted using a stress-based law to reorient myofibers and collagen within a multiscale finite element cardiac modeling framework, MyoFE. Specifically, the model is used to quantify the distinct impacts of heterogeneous distributions of hypercontractility, hypocontractility, and fibrosis on fiber disarray development and examines their effect on functional characteristics of the heart. Our results show that heterogenous cell level abnormalities highly disrupt the normal mechanics of myocardium and lead to significant fiber disarray. The pattern of disarray varies depending on the specific perturbation, offering valuable insights into the progression of HCM. Despite the random distribution of perturbed regions within the cardiac muscle, significantly higher fiber disarray is observed near the epicardium compared to the endocardium across all perturbed left ventricle (LV) models. This regional difference in fiber disarray, irrespective of perturbation severity, aligns with previous DT-MRI studies, highlighting the role of regional myocardial mechanics in the development of fiber disarray. Furthermore, cardiac performance declined in the remodeled LVs, particularly in those with fibrosis and hypocontractility. These findings provide important insights into the structural and functional consequences of HCM and offer a framework for future investigations into therapeutic interventions targeting cardiac remodeling.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling the Effective Elastic Modulus and Thickness of Corrugated Boards Using Gaussian Process Regression and Expected Hypervolume Improvement</title>
<link>https://arxiv.org/abs/2507.02208</link>
<guid>https://arxiv.org/abs/2507.02208</guid>
<content:encoded><![CDATA[
<div> Keywords: hypersurface modeling, effective elastic modulus, thickness, corrugated boards, Gaussian Process Regression 

Summary: 
Latin Hypercube Sampling (LHS) combined with Gaussian Process Regression (GP) and Enhanced Expected Hypervolume Improvement (EHVI) were used to model the hypersurface of the effective elastic modulus and thickness in corrugated boards. Accurate modeling of these properties is crucial for optimizing the mechanical properties of corrugated materials. LHS efficiently samples the input space, while GP adapts to the complexity of response surfaces by incorporating prediction and uncertainty. Points are generated and evaluated based on the complexity of hypersurfaces, with emphasis on points with higher variance. The performance evaluation showed that GP with EHVI had improved accuracy, indicated by lower Mean Squared Error (MSE) values for the effective elastic modulus and thickness predictions. This approach demonstrates potential for future applications in structural optimization. 

Summary: <div>
arXiv:2507.02208v1 Announce Type: cross 
Abstract: This work aims to model the hypersurface of the effective elastic modulus, \( E_{z, \text{eff}} \), and thickness, \( th_{\text{eff}} \), in corrugated boards. A Latin Hypercube Sampling (LHS) is followed by Gaussian Process Regression (GP), enhanced by EHVI as a multi-objective acquisition function. Accurate modeling of \( E_{z, \text{eff}} \) and \( th_{\text{eff}} \) is critical for optimizing the mechanical properties of corrugated materials in engineering applications. LHS provides an efficient and straightforward approach for an initial sampling of the input space; GP is expected to be able to adapt to the complexity of the response surfaces by incorporating both prediction and uncertainty. Therefore, the next points being generated and evaluated are based on the complexity of the hypersurfaces, and some points, especially those with higher variance, are more exploited and carry more importance. The performance of GP with EHVI is measured by Mean Squared Error (MSE). Prediction of GP resulted in \( \text{MSE}(E_{z, \text{eff}}) = 5.24 \, \text{kPa}^2 \) and \( \text{MSE}(th_{\text{eff}}) = 1 \, \text{mm}^2 \). GP possesses then improved accuracy and adaptability for future applications in structural optimization.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Particle Flow Filters with Taylor Expansion Series</title>
<link>https://arxiv.org/abs/2505.01597</link>
<guid>https://arxiv.org/abs/2505.01597</guid>
<content:encoded><![CDATA[
<div> Particle Flow Filters, Measurement Update, Drift Term, Diffusion Term, High-Order Polynomial Expansions<br />
<br />
Summary:<br />
Particle Flow Filters update measurements by moving particles rather than adjusting weights based on likelihood. This study introduces a new derivation method using high-order polynomial expansions, improving upon linearization techniques. The technique utilizes differential algebra to derive high-order particle flows directly onto polynomial representations of distributions. Two new particle flow filters are proposed, differing in the selection of the expansion center for Taylor polynomial evaluations. Numerical experiments demonstrate enhanced performance, particularly compared to Gromov flow and "exact" flow. <div>
arXiv:2505.01597v2 Announce Type: replace 
Abstract: Particle Flow Filters perform the measurement update by moving particles to a different location rather than modifying the particles' weight based on the likelihood. Their movement (flow) is dictated by a drift term, which continuously pushes the particle toward the posterior distribution, and a diffusion term, which guarantees the spread of particles. This work presents a novel derivation of these terms based on high-order polynomial expansions, where the common techniques based on linearization reduce to a simpler version of the new methodology. Thanks to differential algebra, the high-order particle flow is derived directly onto the polynomials representation of the distribution, embedded with differentiation and evaluation. The resulting technique proposes two new particle flow filters, whose difference relies on the selection of the expansion center for the Taylor polynomial evaluation. Numerical applications show the improvement gained by the inclusion of high-order terms, especially when comparing performance with the Gromov flow and the "exact" flow.
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPC-AI Coupling Methodology for Scientific Applications</title>
<link>https://arxiv.org/abs/2507.01025</link>
<guid>https://arxiv.org/abs/2507.01025</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, High-performance computing, Coupling, Materials science, Scientific discovery

Summary: 
This study explores the integration of high-performance computing (HPC) and artificial intelligence (AI) in scientific applications, focusing on three coupling patterns: surrogate, directive, and coordinate. Through case studies in materials science, the effectiveness of these patterns is demonstrated, highlighting technical challenges, performance improvements, and implementation details. The proposed coupling patterns offer valuable guidance for future HPC-AI ensembles in various scientific domains, not limited to materials science. The study emphasizes the transformation of numerical-based HPC applications with data-driven AI approaches to address computational intensity challenges. This research provides insight into promising perspectives for HPC-AI coupling and its potential impact on scientific discovery. <div>
arXiv:2507.01025v1 Announce Type: new 
Abstract: Artificial intelligence (AI) technologies have fundamentally transformed numerical-based high-performance computing (HPC) applications with data-driven approaches and endeavored to address existing challenges, e.g. high computational intensity, in various scientific domains. In this study, we explore the scenarios of coupling HPC and AI (HPC-AI) in the context of emerging scientific applications, presenting a novel methodology that incorporates three patterns of coupling: surrogate, directive, and coordinate. Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite, and typical HPC-AI ensembles. Through case studies in materials science, we demonstrate the application and effectiveness of these patterns. The study highlights technical challenges, performance improvements, and implementation details, providing insight into promising perspectives of HPC-AI coupling. The proposed coupling patterns are applicable not only to materials science but also to other scientific domains, offering valuable guidance for future HPC-AI ensembles in scientific discovery.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI in Product Management: A Co-Evolutionary Model</title>
<link>https://arxiv.org/abs/2507.01069</link>
<guid>https://arxiv.org/abs/2507.01069</guid>
<content:encoded><![CDATA[
<div> agentic AI, product management, co-evolutionary framework, systems theory, human-AI interaction theory

Summary: 
This study delves into the transformative impact of agentic AI on product management, proposing a conceptual framework for its integration throughout the product lifecycle. Agentic AI, known for its autonomy and goal-driven behavior, reshapes the role of product managers (PMs) as orchestrators within socio-technical ecosystems. The framework, drawing on systems theory, co-evolutionary theory, and human-AI interaction theory, outlines the capabilities of agentic AI in various stages of product development. Through an integrative review of 70+ sources, the study showcases the evolving roles of PMs in overseeing AI operations, aligning strategies, and ensuring effective integration. A key finding emphasizes the need for PMs to enhance their skills in AI literacy, governance, and systems thinking to facilitate mutual adaptation between PMs and AI. This study lays the groundwork for future research and practical implementation to promote responsible and efficient integration of agentic AI in software organizations. 

<br /><br />Summary: <div>
arXiv:2507.01069v1 Announce Type: new 
Abstract: This study explores agentic AI's transformative role in product management, proposing a conceptual co-evolutionary framework to guide its integration across the product lifecycle. Agentic AI, characterized by autonomy, goal-driven behavior, and multi-agent collaboration, redefines product managers (PMs) as orchestrators of socio-technical ecosystems. Using systems theory, co-evolutionary theory, and human-AI interaction theory, the framework maps agentic AI capabilities in discovery, scoping, business case development, development, testing, and launch. An integrative review of 70+ sources, including case studies from leading tech firms, highlights PMs' evolving roles in AI orchestration, supervision, and strategic alignment. Findings emphasize mutual adaptation between PMs and AI, requiring skills in AI literacy, governance, and systems thinking. Addressing gaps in traditional frameworks, this study provides a foundation for future research and practical implementation to ensure responsible, effective agentic AI integration in software organizations.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatially Distributed Wettability Characterization in Porous Media</title>
<link>https://arxiv.org/abs/2507.01617</link>
<guid>https://arxiv.org/abs/2507.01617</guid>
<content:encoded><![CDATA[
<div> contact angle measurement, micro-CT images, wettability heterogeneity, spatially distributed, open-source tools<br />
Summary:
An enhanced geometric algorithm for automated contact angle measurement from micro-CT images is introduced, offering improved accuracy through robust interface extrapolation. The method generates contact angle maps revealing wettability heterogeneity in mixed-wet systems. Analysis shows that averaged metrics can mask significant variability within samples; a seemingly uniformly weakly water-wet sample may exhibit substantial intermediate-wetting regions. This variation impacts pore-filling mechanisms and interface structure. The study's open-source tools enable precise spatial wettability characterization, enhancing predictions of multiphase flow behavior in porous materials. Such insights are crucial for optimizing subsurface energy processes. <div>
arXiv:2507.01617v1 Announce Type: new 
Abstract: An enhanced geometric algorithm for automated pore-by-pore contact angle measurement from micro-CT images, is presented that achieves superior accuracy compared to existing methods through robust fluid-fluid and solid-fluid interface extrapolation. Using this high resolution data, we generate spatially distributed contact angle maps that reveal previously hidden wettability heterogeneity. Our analysis of mixed-wet systems demonstrates the severe limitations of averaged metrics: a sample with a mean contact angle of 64.7 degrees, conventionally classified as uniformly weakly water-wet, exhibits 40% of its pore space in the intermediate-wetting regime (70-110 degrees). This heterogeneity explains the presence of minimal surface interfaces and fundamentally different pore-filling mechanisms operating within the same sample. By providing open-source tools for spatially-resolved wettability characterization, this work enables more accurate predictions of multiphase flow behavior in heterogeneous porous materials, essential for optimizing subsurface energy storage and recovery processes.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified Levenberg-Marquardt method for estimating the elastic material parameters of polymer waveguides using residuals between autocorrelated frequency responses</title>
<link>https://arxiv.org/abs/2507.01706</link>
<guid>https://arxiv.org/abs/2507.01706</guid>
<content:encoded><![CDATA[
<div> optimization, ultrasound, polymers, elasticity, material parameters
Summary:
This article addresses the estimation of frequency-dependent elastic parameters of polymers in the ultrasound range as an inverse problem. The approach involves fitting simulation signals to measurement signals of displacement responses in hollow cylindrical waveguides for efficiency. Two novel methods are proposed to accelerate optimization: an adapted Levenberg-Marquardt method and an improved objective function based on autocorrelated envelopes of signals. Realistic material parameter ranges are considered for reproducibility. The study focuses on isotropic materials, showing that the proposed methods reduce the total number of model evaluations, speeding up parameter identification. <div>
arXiv:2507.01706v1 Announce Type: new 
Abstract: In this contribution, we address the estimation of the frequency-dependent elastic parameters of polymers in the ultrasound range, which is formulated as an inverse problem. This inverse problem is implemented as a nonlinear regression-type optimization problem, in which the simulation signals are fitted to the measurement signals. These signals consist of displacement responses in waveguides, focusing on hollow cylindrical geometries to enhance the simulation efficiency. To accelerate the optimization and reduce the number of model evaluations and wait times, we propose two novel methods. First, we introduce an adaptation of the Levenberg-Marquardt method derived from a geometrical interpretation of the least-squares optimization problem. Second, we introduce an improved objective function based on the autocorrelated envelopes of the measurement and simulation signals. Given that this study primarily relies on simulation data to quantify optimization convergence, we aggregate the expected ranges of realistic material parameters and derive their distributions to ensure the reproducibility of optimizations with proper measurements. We demonstrate the effectiveness of our objective function modification and step adaptation for various materials with isotropic material symmetry by comparing them with a state-of-the-art optimization method. In all cases, our method reduces the total number of model evaluations, thereby shortening the time to identify the material parameters.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relevance of the Basset history term for Lagrangian particle dynamics</title>
<link>https://arxiv.org/abs/2407.01041</link>
<guid>https://arxiv.org/abs/2407.01041</guid>
<content:encoded><![CDATA[
<div> Keywords: Maxey-Riley equation, fluid dynamics, Lagrangian dynamics, clustering patterns, turbulent flow <br />
Summary: <br />
The study focuses on the impact of the integral "history term" in the Maxey-Riley equation (MRE) on the movement of finite spherical particles in fluid dynamics. Numerical computations were carried out to compare trajectories with and without the history term for a large number of particles in various flow fields. The findings reveal that neglecting the history term significantly affects clustering patterns, especially for moderate to large Stokes numbers. Additionally, the computation of finite-time Lyapunov exponents demonstrates that even for small particles, disregarding the history term leads to notable differences in the resulting scalar field, particularly in turbulent flows. This highlights the importance of considering the history term in the MRE for accurately predicting the Lagrangian dynamics of particles in fluid systems. <br /> <div>
arXiv:2407.01041v3 Announce Type: replace-cross 
Abstract: The movement of small but finite spherical particles in a fluid can be described by the Maxey-Riley equation (MRE) if they are too large to be considered passive tracers. The MRE contains an integral "history term" modeling wake effects, which causes the force acting on a particle at some given time to depend on its full past trajectory. The history term causes complications in the numerical solution of the MRE and is therefore often neglected, despite both numerical and experimental evidence that its effects are generally not negligible. By numerically computing trajectories with and without the history term of a large number of particles in different flow fields, we investigate its impact on the large-scale Lagrangian dynamics of simulated particles. We show that for moderate to large Stokes numbers, ignoring the history term leads to significant differences in clustering patterns. Furthermore, we compute finite-time Lyapunov exponents and show that, even for small particles, the differences in the resulting scalar field from ignoring the BHT can be significant, in particular if the underlying flow is turbulent.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-design of magnetic soft robots with large deformation and contacts via material point method and topology optimization</title>
<link>https://arxiv.org/abs/2503.22767</link>
<guid>https://arxiv.org/abs/2503.22767</guid>
<content:encoded><![CDATA[
<div> soft robots, magnetic particles, topology optimization, magneto-elastic dynamics, autonomous design

Summary:
The article introduces a topology optimization framework for magnetic soft robots embedded with hard magnetic particles. This framework simultaneously designs structures, material magnetization, and time-varying magnetic stimuli to achieve target behaviors such as shape morphing and locomotion. It integrates generalized topology optimization with the magneto-elastic material point method, allowing for dynamic motion and solid contacts. The framework is computationally efficient, completing all design cases within minutes. It enables the autonomous co-design of active soft materials for various tasks, including metasurfaces, drug delivery, and minimally invasive procedures. <div>
arXiv:2503.22767v2 Announce Type: replace-cross 
Abstract: Magnetic soft robots embedded with hard magnetic particles enable untethered actuation via external magnetic fields, offering remote, rapid, and precise control, which is highly promising for biomedical applications. However, designing such systems is challenging due to the complex interplay of magneto-elastic dynamics, large deformation, solid contacts, time-varying stimuli, and posture-dependent loading. As a result, most existing research relies on heuristics and trial-and-error methods or focuses on the independent design of stimuli or structures under static conditions. We propose a topology optimization framework for magnetic soft robots that simultaneously designs structures, location-specific material magnetization and time-varying magnetic stimuli, accounting for large deformations, dynamic motion, and solid contacts. This is achieved by integrating generalized topology optimization with the magneto-elastic material point method, which supports GPU-accelerated parallel simulations and auto-differentiation for sensitivity analysis. We applied this framework to design magnetic robots for various tasks, including multi-task shape morphing and locomotion, in both 2D and 3D. The method autonomously generates optimized robotic systems to achieve target behaviors without requiring human intervention. Despite the nonlinear physics and large design space, it demonstrates high computational efficiency, completing all cases within minutes. The framework provides a computational foundation for the autonomous co-design of active soft materials in applications such as metasurfaces, drug delivery, and minimally invasive procedures.
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics</title>
<link>https://arxiv.org/abs/2507.00539</link>
<guid>https://arxiv.org/abs/2507.00539</guid>
<content:encoded><![CDATA[
<div> Ensemble Kalman Filter, Reduced-Order Model, Data Assimilation, Fluid Dynamics, Computational Efficiency <br />
<br />
Summary: 
This paper introduces an innovative Reduced-Order Model (ROM) that merges experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of fluid dynamics systems. The methodology incorporates the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework to improve prediction accuracy. To address the computational demands, the ROM employs low-resolution techniques, such as downsampling datasets and utilizing low-cost Singular Value Decomposition (lcSVD) for advanced reconstruction. Results show significant reductions in computation time and RAM usage without sacrificing accuracy. The EnKF is effective in estimating and predicting fluid flow systems based on limited observations and low-fidelity data. The proposed DA method shows promise in improving computational efficiency in CFD and related fields, making it suitable for large-scale and real-time applications like environmental monitoring and aerospace. <div>
arXiv:2507.00539v1 Announce Type: new 
Abstract: This paper presents an innovative Reduced-Order Model (ROM) for merging experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of a fluid dynamics system, leading to more accurate predictions. Our methodology introduces a novel approach implementing the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework, grounded in a robust theoretical foundation and applied to fluid dynamics. To address the substantial computational demands of DA, the proposed ROM employs low-resolution (LR) techniques to drastically reduce computational costs. This approach involves downsampling datasets for DA computations, followed by an advanced reconstruction technique based on low-cost Singular Value Decomposition (lcSVD). The lcSVD method, a key innovation in this paper, has never been applied to DA before and offers a highly efficient way to enhance resolution with minimal computational resources. Our results demonstrate significant reductions in both computation time and RAM usage through the LR techniques without compromising the accuracy of the estimations. For instance, in a turbulent test case, the LR approach with a compression rate of 15.9 can achieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a low Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the high-resolution (HR) reference. Furthermore, we highlight the effectiveness of the EnKF in estimating and predicting the state of fluid flow systems based on limited observations and low-fidelity numerical data. This paper highlights the potential of the proposed DA method in fluid dynamics applications, particularly for improving computational efficiency in CFD and related fields. Its ability to balance accuracy with low computational and memory costs makes it suitable for large-scale and real-time applications, such as environmental monitoring or aerospace.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark</title>
<link>https://arxiv.org/abs/2507.00034</link>
<guid>https://arxiv.org/abs/2507.00034</guid>
<content:encoded><![CDATA[
<div> neural network, critical heat flux, non-uniform heating, spatial power profiles, transfer-learning

Summary:
- The study compiles a comprehensive dataset on critical heat flux (CHF) under both uniform and non-uniform axial heating conditions to support Phase II of the OECD/NEA AI/ML CHF benchmark.
- Classical CHF correlations show errors under uniform heating and struggle with non-uniform profiles, while modern tabular methods offer improved but imperfect predictions.
- A neural network trained on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, emphasizing the need for models that consider axial power distributions.
- The curated datasets and modeling results provided in this study set the stage for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark. <br /><br />Summary: <div>
arXiv:2507.00034v1 Announce Type: cross 
Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. To support Phase II of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power profiles, this work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating and degrade markedly when applied to non-uniform profiles, while modern tabular methods offer improved but still imperfect predictions. A neural network trained solely on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, underscoring the need for models that explicitly incorporate axial power distributions. By providing these curated datasets and baseline modeling results, this study lays the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process</title>
<link>https://arxiv.org/abs/2507.00046</link>
<guid>https://arxiv.org/abs/2507.00046</guid>
<content:encoded><![CDATA[
<div> evolutionary computing, image segmentation, Additive Friction Stir Deposition, Particle Swarm Optimization, defect detection

Summary:
Evolutionary computing-based image segmentation using Particle Swarm Optimization (PSO) was proposed for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. The methodology integrates gradient magnitude analysis with distance transforms to create attention-weighted visualizations highlighting critical interface regions. Multiple visualization techniques, including self-attention maps and multi-channel visualization, were applied to five AFSD samples to detect material transitions and potential defects. PSO algorithm determined optimal threshold values for precise segmentation. The multi-channel visualization technique combined boundary information, spatial relationships, and material density data for cohesive representations. Attention-based analysis successfully identified incomplete bonding regions and inhomogeneities in AFSD joints, offering quantitative metrics for process optimization and quality assessment of additively manufactured components. <br /><br />Summary: <div>
arXiv:2507.00046v1 Announce Type: cross 
Abstract: This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A collaborative digital twin built on FAIR data and compute infrastructure</title>
<link>https://arxiv.org/abs/2507.00048</link>
<guid>https://arxiv.org/abs/2507.00048</guid>
<content:encoded><![CDATA[
<div> machine learning, automated experimentation, self-driving laboratories, FAIR data infrastructure, optimization

Summary:
The article discusses the integration of machine learning and automated experimentation in self-driving laboratories (SDL) to accelerate discovery and optimization tasks in science and engineering. By utilizing findable, accessible, interoperable, and reusable (FAIR) data infrastructure, geographically dispersed researchers can collaborate effectively within a distributed SDL implementation on nanoHUB services. The framework allows for the sharing of raw experimental data in a central database, enabling researchers to benefit from analysis tools and machine learning models that update automatically with new data. A separate workflow on nanoHUB facilitates sequential optimization through active learning, where researchers define the optimization objective and machine learning models guide future experiment selection. The article presents the application of these concepts in an optimization task involving food dyes, showing how researchers and students can conduct experiments, share data, and explore the combination of FAIR data, predictive ML models, and sequential optimization in a cost-effective manner. <div>
arXiv:2507.00048v1 Announce Type: cross 
Abstract: The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively. This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management. In this framework, geographically dispersed collaborators conducting independent optimization tasks contribute raw experimental data to a shared central database. These researchers can then benefit from analysis tools and machine learning models that automatically update as additional data become available. New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments. Inspired by the concept of ``frugal twin", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color. With easily accessible and inexpensive materials, researchers and students can set up their own experiments, share data with collaborators, and explore the combination of FAIR data, predictive ML models, and sequential optimization. The tools introduced are generally applicable and can easily be extended to other optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The gradual transformation of inland countries -- human plowing, horse plowing and equity incentives</title>
<link>https://arxiv.org/abs/2507.00067</link>
<guid>https://arxiv.org/abs/2507.00067</guid>
<content:encoded><![CDATA[
<div> civilization, history, governance, conflict, economic development
<br />Summary:
In the article, the author emphasizes the importance of learning from history to upgrade civilization and improve its strength and survival ability. By studying the long-term stability of countries in conflict, including economic benefits and means of suppression, and utilizing mathematical methods to find optimal solutions, it is suggested that civilizations can enhance their ability to handle conflicts and reduce internal strife. The transition from human plowing to horse plowing is mentioned as a means to suppress resistance and provide resistance ability to the people. The selection of rulers should consider various institutional aspects like exams, elections, and drawing lots. Economic development, following a lognormal distribution, can be adjusted using expected value and variance to address wealth inequality. <div>
arXiv:2507.00067v1 Announce Type: cross 
Abstract: Many modern countries have not learned their lessons and often hope for the wisdom of later generations, resulting in them only possessing modern technology and difficult to iterate ancient civilizations. At present, there is no way to tell how we should learn from history and promote the gradual upgrading of civilization. Therefore, we must tell the history of civilization's progress and the means of governance, learn from experience to improve the comprehensive strength and survival ability of civilization, and achieve an optimal solution for the tempering brought by conflicts and the reduction of internal conflicts. Firstly, we must follow the footsteps of history and explore the reasons for the long-term stability of each country in conflict, including providing economic benefits to the people and means of suppressing them; then, use mathematical methods to demonstrate how we can achieve the optimal solution at the current stage. After analysis, we can conclude that the civilization transformed from human plowing to horse plowing can easily suppress the resistance of the people and provide them with the ability to resist; The selection of rulers should consider multiple institutional aspects, such as exams, elections, and drawing lots; Economic development follows a lognormal distribution and can be adjusted by expected value and variance. Using a lognormal distribution with the maximum value to divide equity can adjust the wealth gap.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPGD: Steepest Perturbed Gradient Descent Optimization</title>
<link>https://arxiv.org/abs/2411.04946</link>
<guid>https://arxiv.org/abs/2411.04946</guid>
<content:encoded><![CDATA[
<div> algorithm, optimization, gradient descent, perturbation sampling, local minima<br />
<br />
Summary:
The paper introduces the Steepest Perturbed Gradient Descent (SPGD) algorithm, which combines gradient descent with periodic uniform perturbation sampling to overcome challenges like local minima, saddle points, and plateaus in optimization problems. SPGD generates candidate solutions and selects the one with the steepest loss difference, incorporating exploration to escape sub-optimal minima. It outperforms four established methods in solving the NP-hard 3D component packing problem and shows improvement in complex non-convex optimization problems. Comparative analyses with 2D benchmark functions demonstrate SPGD's superior performance in navigating intricate optimization landscapes. The algorithm's ability to effectively search for global optima across diverse problem spaces highlights its versatility in addressing various optimization challenges.<br /> 
<br />Summary: <div>
arXiv:2411.04946v2 Announce Type: replace-cross 
Abstract: Optimization algorithms are pivotal in advancing various scientific and industrial fields but often encounter obstacles such as trapping in local minima, saddle points, and plateaus (flat regions), which makes the convergence to reasonable or near-optimal solutions particularly challenging. This paper presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that innovatively combines the principles of the gradient descent method with periodic uniform perturbation sampling to effectively circumvent these impediments and lead to better solutions whenever possible. SPGD is distinctively designed to generate a set of candidate solutions and select the one exhibiting the steepest loss difference relative to the current solution. It enhances the traditional gradient descent approach by integrating a strategic exploration mechanism that significantly increases the likelihood of escaping sub-optimal local minima and navigating complex optimization landscapes effectively. Our approach not only retains the directed efficiency of gradient descent but also leverages the exploratory benefits of stochastic perturbations, thus enabling a more comprehensive search for global optima across diverse problem spaces. We demonstrate the efficacy of SPGD in solving the 3D component packing problem, an NP-hard challenge. Preliminary results show a substantial improvement over four established methods, particularly on response surfaces with complex topographies and in multidimensional non-convex continuous optimization problems. Comparative analyses with established 2D benchmark functions highlight SPGD's superior performance, showcasing its ability to navigate complex optimization landscapes. These results emphasize SPGD's potential as a versatile tool for a wide range of optimization problems.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STONet: A neural operator for modeling solute transport in micro-cracked reservoirs</title>
<link>https://arxiv.org/abs/2412.05576</link>
<guid>https://arxiv.org/abs/2412.05576</guid>
<content:encoded><![CDATA[
<div> Neural Operator, Solute Transport Operator Network, Contaminant Transport, Micro-Cracked Porous Media, Deep Learning<br />
Summary: <br />
In this work, the Solute Transport Operator Network (STONet) is introduced as a novel neural operator to model contaminant transport in micro-cracked porous media efficiently. The model architecture of STONet combines a DeepONet structure with a transformer-based multi-head attention mechanism to enhance performance without additional computational overhead. Training data obtained from finite element simulations captures diverse scenarios, enabling accurate predictions of concentration field changes. STONet achieves high accuracy with relative errors below 1% compared to FEM simulations and reduces runtime significantly. This computational efficiency enables the development of digital twins for rapid assessment of subsurface contamination risks and optimization of environmental remediation strategies. Data and code for the paper will be available on GitHub for further research and application. <br /> <div>
arXiv:2412.05576v2 Announce Type: replace-cross 
Abstract: In this work, we introduce a novel neural operator, the Solute Transport Operator Network (STONet), to efficiently model contaminant transport in micro-cracked porous media. STONet's model architecture is specifically designed for this problem and uniquely integrates an enriched DeepONet structure with a transformer-based multi-head attention mechanism, enhancing performance without incurring additional computational overhead compared to existing neural operators. The model combines different networks to encode heterogeneous properties effectively and predict the rate of change of the concentration field to accurately model the transport process. The training data is obtained using finite element (FEM) simulations by random sampling of micro-fracture distributions and applied pressure boundary conditions, which capture diverse scenarios of fracture densities, orientations, apertures, lengths, and balance of pressure-driven to density-driven flow. Our numerical experiments demonstrate that, once trained, STONet achieves accurate predictions, with relative errors typically below 1% compared with FEM simulations while reducing runtime by approximately two orders of magnitude. This type of computational efficiency facilitates building digital twins for rapid assessment of subsurface contamination risks and optimization of environmental remediation strategies. The data and code for the paper will be published at https://github.com/ehsanhaghighat/STONet.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks</title>
<link>https://arxiv.org/abs/2503.08904</link>
<guid>https://arxiv.org/abs/2503.08904</guid>
<content:encoded><![CDATA[
<div> data-driven methods, state reconstruction, nuclear reactors, Shallow Recurrent Decoder, Generation-IV reactors

Summary:
The article discusses the application of data-driven methods for accurate state reconstruction in nuclear reactors, focusing on the challenging environment of Generation-IV reactors like the Molten Salt Fast Reactor (MSFR). By leveraging the Shallow Recurrent Decoder architecture, the study efficiently estimates the reactor's state vector, including neutron fluxes, precursors concentrations, and thermal parameters, using only three out-of-core time-series neutron flux measurements. The proposed approach extends the architecture to handle parametric time-series data, enabling robust state estimation under various operating conditions. Additionally, the methodology allows for quantifying uncertainty in state estimation with low training costs. The study's successful results highlight its potential for real-time monitoring, control, and the development of a reactor digital twin. <div>
arXiv:2503.08904v2 Announce Type: replace-cross 
Abstract: The recent developments in data-driven methods have paved the way to new methodologies to provide accurate state reconstruction of engineering systems; nuclear reactors represent particularly challenging applications for this task due to the complexity of the strongly coupled physics involved and the extremely harsh and hostile environments, especially for new technologies such as Generation-IV reactors. Data-driven techniques can combine different sources of information, including computational proxy models and local noisy measurements on the system, to robustly estimate the state. This work leverages the novel Shallow Recurrent Decoder architecture to infer the entire state vector (including neutron fluxes, precursors concentrations, temperature, pressure and velocity) of a reactor from three out-of-core time-series neutron flux measurements alone. In particular, this work extends the standard architecture to treat parametric time-series data, ensuring the possibility of investigating different accidental scenarios and showing the capabilities of this approach to provide an accurate state estimation in various operating conditions. This paper considers as a test case the Molten Salt Fast Reactor (MSFR), a Generation-IV reactor concept, characterised by strong coupling between the neutronics and the thermal hydraulics due to the liquid nature of the fuel. The promising results of this work are further strengthened by the possibility of quantifying the uncertainty associated with the state estimation, due to the considerably low training cost. The accurate reconstruction of every characteristic field in real-time makes this approach suitable for monitoring and control purposes in the framework of a reactor digital twin.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</title>
<link>https://arxiv.org/abs/2503.21248</link>
<guid>https://arxiv.org/abs/2503.21248</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, scientific research, benchmark, hypothesis generation, automated discovery

Summary:
Large language models (LLMs) are being evaluated for their potential in aiding scientific research through a new benchmark that focuses on inspiration retrieval, hypothesis composition, and ranking. An automated framework extracts key components from scientific papers across various disciplines with expert validation to ensure accuracy. By exclusively analyzing papers from 2024, the benchmark avoids data contamination. LLMs show promise in retrieving inspirations, indicating their ability to uncover novel knowledge associations. This positions LLMs as valuable tools for automated scientific discovery, capable of generating innovative hypotheses at scale with minimal human input. <div>
arXiv:2503.21248v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analogical Learning for Cross-Scenario Generalization: Framework and Application to Intelligent Localization</title>
<link>https://arxiv.org/abs/2504.08811</link>
<guid>https://arxiv.org/abs/2504.08811</guid>
<content:encoded><![CDATA[
<div> learning models, generalization, deep learning framework, analogical learning, intelligent wireless localization

Summary:
The article introduces an analogical learning (AL) framework, specifically a bipartite neural network called Mateformer, for multi-scenario learning in intelligent wireless localization. The AL approach leverages the understanding that data from different scenarios follow common underlying physical rules despite having distinct reference frames. The Mateformer network captures relativity within multiple latent feature spaces and uses this relativity to guide nonlinear analogy for accurate predictions. Experimental results show that AL outperforms existing models in accuracy in single-scenario benchmarks, exhibits stable transferability between scenarios without catastrophic forgetting, and robustly adapts to new, unseen scenarios such as dynamic weather and traffic conditions without any fine-tuning. The data and code for the framework are also made available for further research and implementation. <div>
arXiv:2504.08811v2 Announce Type: replace-cross 
Abstract: Existing learning models often exhibit poor generalization when deployed across diverse scenarios. It is primarily due to that the underlying reference frame of the data varies with the deployment environment and settings. However, despite that data of each scenario has a distinct reference frame, its generation generally follows common underlying physical rules. Based on this understanding, this article proposes a deep learning framework named analogical learning (AL), which implicitly retrieves the reference frame information associated with a scenario and then to make accurate prediction by relative analogy with other scenarios. Specifically, we design a bipartite neural network called Mateformer. Its first part captures the relativity within multiple latent feature spaces between the input data and a small amount of embedded data from the studied scenario, while its second part uses this relativity to guide the nonlinear analogy. We apply AL to the typical multi-scenario learning problem of intelligent wireless localization in cellular networks. Extensive experiments validate AL's superiority across three key dimensions. First, it achieves state-of-the-art accuracy in single-scenario benchmarks. Second, it demonstrates stable transferability between different scenarios, avoiding catastrophic forgetting. Finally, and most importantly, it robustly adapts to new, unseen scenarios--including dynamic weather and traffic conditions--without any tuning. All data and code are available at https://github.com/ziruichen-research/ALLoc.
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Integrated Electrostatic Boundary Element Simulations with Non-Conforming Higher-Order Meshes</title>
<link>https://arxiv.org/abs/2506.22676</link>
<guid>https://arxiv.org/abs/2506.22676</guid>
<content:encoded><![CDATA[
<div> CAD plugin, design analysis, virtual prototyping, electric devices, boundary element method
Summary:
The article introduces a design through analysis workflow for virtual prototyping of electric devices. A CAD plugin facilitates the interaction between design and analysis, enabling the creation of analysis models and visualization of results within the design environment. Simulations employ a fast boundary element method (BEM) capable of handling non-conforming and higher-order meshes. Numerical experiments are conducted to assess the accuracy of the approach and its sensitivity to the initial CAD representation. The workflow facilitates a close link between design and analysis, with the non-conforming higher-order BEM technique producing precise results while simplifying the interaction between the two processes. <div>
arXiv:2506.22676v1 Announce Type: new 
Abstract: We present a design through analysis workflow that enables virtual prototyping of electric devices. A CAD plugin establishes the interaction between design and analysis, allowing the preparation of analysis models and the visualization of its results within the design environment. The simulations utilize a fast boundary element method (BEM) that allows for non-conforming and higher-order meshes. Our numerical experiments investigate the accuracy of the approach and its sensitivity to the initial CAD representation. Overall, the workflow enables a close link between design and analysis, where the non-conforming higher-order BEM approach provides accurate results and significantly simplifies the interaction.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved design of an active landing gear for a passenger aircraft using multi-objective optimization technique</title>
<link>https://arxiv.org/abs/2506.22870</link>
<guid>https://arxiv.org/abs/2506.22870</guid>
<content:encoded><![CDATA[
<div> Controller coefficients, hydraulic actuator parameters, vibration absorber, landing gear system, optimization algorithm<br />
<br />
Summary:<br />
This study focuses on optimizing the landing gear system of aircraft using a bee-inspired multi-objective algorithm. The research addresses the need for better performance under varying landing and runway conditions by optimizing controller coefficients, hydraulic actuator parameters, and vibration absorber simultaneously. Sensitivity analysis for three-point landings and robustness analysis for emergency wind conditions are conducted. By applying the active shock absorber system, optimized through bee-based algorithms, improvements are seen in reducing bounce and pitch displacements, suspension travel, and impact force in both time and frequency domains. The results show enhanced passenger comfort and potentially extended structural fatigue life, showcasing practical industrial application. <div>
arXiv:2506.22870v1 Announce Type: new 
Abstract: The landing gear system is a major aircraft subsystem that must withstand extreme forces during ground maneuvers and absorb vibrations. While traditional systems perform well under normal conditions, their efficiency drops under varying landing and runway scenarios. This study addresses this issue by simultaneously optimizing controller coefficients, parameters of a nonlinear hydraulic actuator integrated into the traditional shock absorber, and a vibration absorber using a bee-inspired multi-objective algorithm. To demonstrate adaptability, the paper includes sensitivity analysis for three-point landings affected by added payload and touchdown speed, and robustness analysis for one- and two-point landings under emergency wind conditions. The dynamic flight equations of an Airbus A320-200 during landing are derived and solved numerically. Results show that the active shock absorber system, optimized via two bee-based algorithms, outperforms the passive system in reducing bounce and pitch displacements and momenta, suspension travel, and impact force in both time and frequency domains. This leads to significantly improved passenger comfort and potentially longer structural fatigue life, demonstrating industrial applicability.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feasibility of spectral-element modeling of wave propagation through the anatomy of marine mammals</title>
<link>https://arxiv.org/abs/2506.22944</link>
<guid>https://arxiv.org/abs/2506.22944</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D spectral-element method, ultrasonic wave propagation, bottlenose dolphin, marine mammal bioacoustics, environmental challenges <br />
Summary: <br />
This study presents a novel 3D spectral-element method (SEM) simulation of ultrasonic wave propagation in a bottlenose dolphin's head, addressing limitations of traditional finite-element methods (FEM). The SEM approach offers exponential convergence and efficient parallel computation, enabling high-frequency simulations with complex anatomical features accurately represented in the mesh. Utilizing Computed Tomography (CT) scan data, detailed simulations of plane and spherical waves validate the efficacy of SEM for ultrasonic time-domain modeling in marine mammal bioacoustics research. This advancement in computational modeling opens up opportunities in studying dolphin echolocation, the effects of anthropogenic marine noise pollution, and the biophysics of hearing and click generation. By overcoming FEM's challenges, SEM emerges as a powerful tool to investigate hypotheses related to dolphin bioacoustics, with implications for conservation efforts and understanding marine mammal auditory systems in the face of increasing environmental challenges. <div>
arXiv:2506.22944v1 Announce Type: new 
Abstract: This study introduces the first 3D spectral-element method (SEM) simulation of ultrasonic wave propagation in a bottlenose dolphin (Tursiops truncatus) head. Unlike traditional finite-element methods (FEM), which struggle with high-frequency simulations due to costly linear-system inversions and slower convergence, SEM offers exponential convergence and efficient parallel computation. Using Computed Tomography (CT) scan data, we developed a detailed hexahedral mesh capturing complex anatomical features, such as acoustic fats and jaws. Our simulations of plane and spherical waves confirm SEM's effectiveness for ultrasonic time-domain modeling. This approach opens new avenues for marine biology, contributing to research in echolocation, the impacts of anthropogenic marine noise pollution and the biophysics of hearing and click generation in marine mammals. By overcoming FEM's limitations, SEM provides a powerful scalable tool to test hypotheses about dolphin bioacoustics, with significant implications for conservation and understanding marine mammal auditory systems under increasing environmental challenges.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparStencil: Retargeting Sparse Tensor Cores to Scientific Stencil Computations via Structured Sparsity Transformation</title>
<link>https://arxiv.org/abs/2506.22969</link>
<guid>https://arxiv.org/abs/2506.22969</guid>
<content:encoded><![CDATA[
<div> Sparse Tensor Core, scientific stencil computations, SparStencil, structured sparsity, adaptive layout morphing.<br />
<br />
Summary: <br />
Sparse Tensor Cores (TCUs) are efficient for AI workloads with 2:4 sparsity, but not utilized for scientific stencil computations. SparStencil transforms sparse TCUs for stencil computations by restructuring patterns into sparse matrices, ensuring compatibility with 2:4 sparsity constraints. It includes Adaptive Layout Morphing to align patterns, Structured Sparsity Conversion for graph matching, and Automatic Kernel Generation for optimized kernels. Evaluated on 79 kernels, SparStencil achieves up to 7.1x speedup, reduces complexity, and matches expert-tuned performance in compute throughput and memory efficiency. <div>
arXiv:2506.22969v1 Announce Type: new 
Abstract: Sparse Tensor Cores offer exceptional performance gains for AI workloads by exploiting structured 2:4 sparsity. However, their potential remains untapped for core scientific workloads such as stencil computations, which exhibit irregular sparsity patterns.This paper presents SparStencil, the first system to retarget sparse TCUs for scientific stencil computations through structured sparsity transformation. SparStencil introduces three key techniques: (1) Adaptive Layout Morphing, which restructures stencil patterns into staircase-aligned sparse matrices via a flatten-and-crush pipeline; (2) Structured Sparsity Conversion, which formulates transformation as a graph matching problem to ensure compatibility with 2:4 sparsity constraints; (3) Automatic Kernel Generation, which compiles transformed stencils into optimized sparse MMA kernels via layout search and table-driven memory mapping. Evaluated on 79 stencil kernels spanning diverse scientific domains, SparStencil achieves up to 7.1x speedup (3.1x on average) over state-of-the-art framework while reducing code complexity and matching or exceeding expert-tuned performance in both compute throughput and memory efficiency.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a better approach to the Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2506.23028</link>
<guid>https://arxiv.org/abs/2506.23028</guid>
<content:encoded><![CDATA[
<div> Keywords: Vehicle Routing Problem, Logistics, Combinatorial Optimization, Constraints, Extensions

Summary:<br /><br />
The Vehicle Routing Problem (VRP) is a critical issue in logistics management, impacting transportation efficiency, cost reduction, and service quality. As a combinatorial optimization problem, it is widely studied in the fields of transportation, logistics, and delivery systems due to its numerous formulations and extensions. This article provides a detailed overview of VRP, exploring its theoretical foundations, limitations of the classical model, and key extensions. By reviewing various constraints, objectives, and variants in recent literature, it aims to enhance understanding of VRP and its ongoing evolution in modern optimization and decision-making processes. <div>
arXiv:2506.23028v1 Announce Type: new 
Abstract: The Vehicle Routing Problem (VRP) is a fundamental challenge in logistics management research, given its substantial influence on transportation efficiency, cost minimization, and service quality. As a combinatorial optimization problem, VRP plays a crucial role in a wide range of real world applications, particularly in transportation, logistics, and delivery systems, due to its diverse formulations and numerous extensions. Over the years, researchers have introduced various VRP variants to address specific operational constraints, emerging industry requirements and optimize specific objectives, making it one of the most extensively studied problems in operations research. This article provides a comprehensive overview of VRP by exploring its theoretical foundations, discussing the limitations of its classical model, and introducing its key extensions. By systematically reviewing the diverse constraints, objectives, and variants examined in recent literature, this study aims to contribute to a deeper understanding of VRP while highlighting its ongoing evolution and relevance in modern optimization and decision making processes.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Multiscale Topology Optimization of Spinodoid Architected Materials with Controllable Anisotropy</title>
<link>https://arxiv.org/abs/2506.23420</link>
<guid>https://arxiv.org/abs/2506.23420</guid>
<content:encoded><![CDATA[
<div> neural networks, topology optimization, spinodoid materials, data-driven design, Gaussian Process

Summary:
The article introduces a new approach to design optimization for spinodoid architected materials, which have unique properties such as stochasticity, aperiodicity, and bi-continuity. Traditional design methods face challenges due to the complexity of spinodoid design parameters. The proposed framework utilizes neural networks to automate the computation of topological gradients, making it more efficient and scalable. Additionally, a Gaussian Process surrogate is integrated to enhance the accuracy of spinodoid constitutive models. This framework provides physical insights into material distribution, showing why anisotropic spinodoids with tailored orientations are preferred in certain regions. The interpretability of the framework bridges the gap between data-driven design and mechanistic understanding, offering a clearer understanding of material behavior. <div>
arXiv:2506.23420v1 Announce Type: new 
Abstract: Spinodoid architected materials have drawn significant attention due to their unique nature in stochasticity, aperiodicity, and bi-continuity. Compared to classic periodic truss-, beam- and plate-based lattice architectures, spinodoids are insensitive to manufacturing defects, scalable for high throughput production, functionally graded by tunable local properties, and material failure resistant due to low-curvature morphology. However, the design of spinodoids is often hindered by the curse of dimensionality with extremely large design space of spinodoid types, material density, orientation, continuity, and anisotropy. From a design optimization perspective, while genetic algorithms are often beyond the reach of computing capacity, gradient-based topology optimization is challenged by the intricate mathematical derivation of gradient fields with respect to various spinodoid parameters. To address such challenges, we propose a data-driven multiscale topology optimization framework. Our framework reformulates the design variables of spinodoid materials as the parameters of neural networks, enabling automated computation of topological gradients. Additionally, it incorporates a Gaussian Process surrogate for spinodoid constitutive models, eliminating the need for repeated computational homogenization and enhancing the scalability of multiscale topology optimization. Compared to 'black-box' deep learning approaches, the proposed framework provides clear physical insights into material distribution. It explicitly reveals why anisotropic spinodoids with tailored orientations are favored in certain regions, while isotropic spinodoids are more suitable elsewhere. This interpretability helps to bridge the gap between data-driven design with mechanistic understanding.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Multiscale Topology Optimization of Soft Functionally Graded Materials with Large Deformations</title>
<link>https://arxiv.org/abs/2506.23422</link>
<guid>https://arxiv.org/abs/2506.23422</guid>
<content:encoded><![CDATA[
<div> Keywords: Functionally Graded Materials, Topology Optimization, Multiscale Architecture, Nonlinear Material Behavior, Neural Network

Summary:
Functionally Graded Materials (FGMs), particularly soft FGMs, are increasingly important in various engineering applications. Designing these complex systems poses challenges due to their multiscale nature, multiple material phases, and nonlinear behaviors. This paper presents a novel topology optimization framework for soft FGMs under large deformations. Key innovations include a microstructure reconstruction algorithm, material homogenization approach, neural network-based optimization, and a nonlinear sensitivity analysis technique. The framework generates unique topological designs with spatially varying microstructures, not achievable using linear elasticity. To ensure efficient convergence, an energy interpolation scheme and a Newton-Raphson solver with adaptive steps are employed. Numerical experiments demonstrate the effectiveness of the proposed approach in automating the design innovation of soft FGMs. 

<br /><br />Summary: <div>
arXiv:2506.23422v1 Announce Type: new 
Abstract: Functionally Graded Materials (FGMs) made of soft constituents have emerged as promising material-structure systems in potential applications across many engineering disciplines, such as soft robots, actuators, energy harvesting, and tissue engineering. Designing such systems remains challenging due to their multiscale architectures, multiple material phases, and inherent material and geometric nonlinearities. The focus of this paper is to propose a general topology optimization framework that automates the design innovation of multiscale soft FGMs exhibiting nonlinear material behaviors under large deformations. Our proposed topology optimization framework integrates several key innovations: (1) a novel microstructure reconstruction algorithm that generates composite architecture materials from a reduced design space using physically interpretable parameters; (2) a new material homogenization approach that estimates effective properties by combining the stored energy functions of multiple soft constituents; (3) a neural network-based topology optimization that incorporates data-driven material surrogates to enable bottom-up, simultaneous optimization of material and structure; and (4) a generic nonlinear sensitivity analysis technique that computes design sensitivities numerically without requiring explicit gradient derivation. To enhance the convergence of the nonlinear equilibrium equations amid topology optimization, we introduce an energy interpolation scheme and employ a Newton-Raphson solver with adaptive step sizes and convergence criteria. Numerical experiments show that the proposed framework produces distinct topological designs, different from those obtained under linear elasticity, with spatially varying microstructures.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics</title>
<link>https://arxiv.org/abs/2506.22520</link>
<guid>https://arxiv.org/abs/2506.22520</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Curiosity, Engagement, Interactive Molecular Dynamics, Team Performance 

Summary: 
The study investigates the impact of an Artificial Intelligence tutor teammate on student curiosity-driven engagement and learning effectiveness in Interactive Molecular Dynamics tasks. Through a Wizard-of-Oz paradigm, the AI's behaviors are adjusted by a human experimenter to stimulate and sustain student curiosity. Results show that high-performing teams exhibit superior task completion, deeper understanding, and increased engagement. Advanced student questions are linked to AI curiosity-triggering, indicating heightened engagement and cognitive complexity. Cross Recurrence Quantification Analysis metrics demonstrate dynamic synchronization in student-AI interactions, promoting structured yet adaptive engagement to foster curiosity. The findings suggest that the AI's dual role as a teammate and educator can provide adaptive feedback, sustaining engagement and epistemic curiosity. <div>
arXiv:2506.22520v1 Announce Type: cross 
Abstract: This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research</title>
<link>https://arxiv.org/abs/2506.23545</link>
<guid>https://arxiv.org/abs/2506.23545</guid>
<content:encoded><![CDATA[
<div> training, diagnostics, psychological research, XR technologies, human performance <br />
<br />
In this panel discussion, the potential of Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies in various domains is highlighted. These immersive systems are being used to enhance human performance in clinical psychology, space exploration, and medical education. In psychological research and training, XR provides a controlled yet realistic environment for measuring cognitive and emotional processes. For space exploration, VR-based astronaut training and diagnostic systems allow for real-time health assessments. In the field of medical education and rehabilitation, immersive environments are utilized for procedural training and patient engagement. Whether through virtual surgical simulations or gamified rehabilitation exercises, XR technologies improve learning outcomes and encourage patient adherence to treatment plans. Overall, VR/AR/XR technologies are proving to be valuable tools in enhancing performance and advancing research in high-risk and regulated environments. <br /><br />Summary: <div>
arXiv:2506.23545v1 Announce Type: cross 
Abstract: Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are increasingly recognized for their applications in training, diagnostics, and psychological research, particularly in high-risk and highly regulated environments. In this panel we discuss how immersive systems enhance human performance across multiple domains, including clinical psychology, space exploration, and medical education. In psychological research and training, XR can offer a controlled yet ecologically valid setting for measuring cognitive and affective processes. In space exploration, we discuss the development of VR-based astronaut training and diagnostic systems, allowing astronauts to perform real-time health assessments. In medical education and rehabilitation, we cover procedural training and patient engagement. From virtual surgical simulations to gamified rehabilitation exercises, immersive environments enhance both learning outcomes and treatment adherence.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment</title>
<link>https://arxiv.org/abs/2506.23739</link>
<guid>https://arxiv.org/abs/2506.23739</guid>
<content:encoded><![CDATA[
<div> Vehicle-in-the-Loop test bench, vulnerable road users, cyber-physical testing, human pose estimation, urban environments <br />
<br />Summary: <br />
This paper discusses a test environment that combines a Vehicle-in-the-Loop test bench with a motion laboratory to test vehicle interactions with pedestrians and cyclists. The study validates a human pose estimation approach using real-world and virtual representations of vulnerable road users. Results show good alignment in human pose estimation between real-world and cyber-physical test conditions for stable motion patterns but inaccuracies persist under dynamic movements and occlusions, especially for complex cyclist postures. The research aims to enhance testing methodologies for evaluating AI-based vehicle perception and improving interaction models between automated vehicles and vulnerable road users in cyber-physical environments. <div>
arXiv:2506.23739v1 Announce Type: cross 
Abstract: Ensuring safe and realistic interactions between automated driving systems and vulnerable road users (VRUs) in urban environments requires advanced testing methodologies. This paper presents a test environment that combines a Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the feasibility of cyber-physical (CP) testing of vehicle-pedestrian and vehicle-cyclist interactions. Building upon previous work focused on pedestrian localization, we further validate a human pose estimation (HPE) approach through a comparative analysis of real-world (RW) and virtual representations of VRUs. The study examines the perception of full-body motion using a commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is generated in Unreal Engine 5, where VRUs are animated in real time and projected onto a screen to stimulate the camera. The proposed stimulation technique ensures the correct perspective, enabling realistic vehicle perception. To assess the accuracy and consistency of HPE across RW and CP domains, we analyze the reliability of detections as well as variations in movement trajectories and joint estimation stability. The validation includes dynamic test scenarios where human avatars, both walking and cycling, are monitored under controlled conditions. Our results show a strong alignment in HPE between RW and CP test conditions for stable motion patterns, while notable inaccuracies persist under dynamic movements and occlusions, particularly for complex cyclist postures. These findings contribute to refining CP testing approaches for evaluating next-generation AI-based vehicle perception and to enhancing interaction models of automated vehicles and VRUs in CP environments.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A topology optimisation framework to design test specimens for one-shot identification or discovery of material models</title>
<link>https://arxiv.org/abs/2501.12756</link>
<guid>https://arxiv.org/abs/2501.12756</guid>
<content:encoded><![CDATA[
<div> Topology Optimization, Geometry Design, Material Model Calibration, Experimental Mechanics, Digital Image Correlation  
Summary:  
- The shift in material model calibration towards using complex geometry tests requires rich displacement data.  
- The paper proposes a density-based topology optimization approach to design specimen geometry for anisotropic material model calibration.  
- High-resolution specimen design aims to maximize the robustness of inverse problem solutions with noisy displacement measurements.  
- The study discusses cost function selection and topology optimization framework design for optimal specimen geometry.  
- Various optimized topologies are analyzed for identifying isotropic and anisotropic elastic responses.  

<br /><br />Summary: <div>
arXiv:2501.12756v2 Announce Type: replace 
Abstract: The increasing availability of full-field displacement data from imaging techniques in experimental mechanics is determining a gradual shift in the paradigm of material model calibration and discovery, from using several simple-geometry tests towards a few, or even one single test with complicated geometry. The feasibility of such a "one-shot" calibration or discovery heavily relies upon the richness of the measured displacement data, i.e., their ability to probe the space of the state variables and the stress space (whereby the stresses depend on the constitutive law being sought) to an extent sufficient for an accurate and robust calibration or discovery process. The richness of the displacement data is in turn directly governed by the specimen geometry. In this paper, we propose a density-based topology optimisation framework to optimally design the geometry of the target specimen for calibration of an anisotropic elastic material model. To this end, we perform automatic, high-resolution specimen design by maximising the robustness of the solution of the inverse problem, i.e., the identified material parameters, given noisy displacement measurements from digital image correlation. We discuss the choice of the cost function and the design of the topology optimisation framework, and we analyse a range of optimised topologies generated for the identification of isotropic and anisotropic elastic responses.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redefining Evaluation Standards: A Unified Framework for Evaluating the Korean Capabilities of Language Models</title>
<link>https://arxiv.org/abs/2503.22968</link>
<guid>https://arxiv.org/abs/2503.22968</guid>
<content:encoded><![CDATA[
<div> framework, Korean large language models, evaluation, benchmarks, HRET  
Summary:  
- Recent advancements in Korean large language models have led to performance variations across institutions due to inconsistent evaluation protocols.  
- HRET (Haerae Evaluation Toolkit) is introduced as an open-source framework to unify Korean LLM assessment by integrating major benchmarks, inference backends, and diverse evaluation methods.  
- The modular registry design of HRET allows for rapid incorporation of new datasets, methods, and backends to adapt to evolving research needs.  
- HRET includes morphological-aware Type-Token Ratio and systematic keyword-omission detection for diagnostic insights into language-specific behaviors, aiding in identifying shortcomings and guiding improvements in Korean LLM development.  
- The framework promotes diverse experimental approaches and language consistency enforcement to ensure genuine Korean outputs.  
<br /><br />Summary: <div>
arXiv:2503.22968v3 Announce Type: replace 
Abstract: Recent advancements in Korean large language models (LLMs) have driven numerous benchmarks and evaluation methods, yet inconsistent protocols cause up to 10 p.p performance gaps across institutions. Overcoming these reproducibility gaps does not mean enforcing a one-size-fits-all evaluation. Rather, effective benchmarking requires diverse experimental approaches and a framework robust enough to support them. To this end, we introduce HRET (Haerae Evaluation Toolkit), an open-source, registry-based framework that unifies Korean LLM assessment. HRET integrates major Korean benchmarks, multiple inference backends, and multi-method evaluation, with language consistency enforcement to ensure genuine Korean outputs. Its modular registry design also enables rapid incorporation of new datasets, methods, and backends, ensuring the toolkit adapts to evolving research needs. Beyond standard accuracy metrics, HRET incorporates Korean-focused output analyses-morphology-aware Type-Token Ratio (TTR) for evaluating lexical diversity and systematic keyword-omission detection for identifying missing concepts-to provide diagnostic insights into language-specific behaviors. These targeted analyses help researchers pinpoint morphological and semantic shortcomings in model outputs, guiding focused improvements in Korean LLM development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calculation of Photocarrier Generation from Optical Absorption for Time-domain Simulation of Optoelectronic Devices</title>
<link>https://arxiv.org/abs/2102.06702</link>
<guid>https://arxiv.org/abs/2102.06702</guid>
<content:encoded><![CDATA[
<div> optoelectronic materials, photocarrier generation rate, Poynting vector, time-domain simulations, optical absorption <br />
Summary: 
The study addresses the inaccurate calculation of photocarrier generation rates in optoelectronic materials using the Poynting vector in time-domain simulations. It proposes an optical absorption-based model that considers material dispersion near the optical frequency corresponding to the bandgap energy. By calculating instantaneous optical absorption from the polarization current density associated with the dispersion model, the proposed approach offers more accurate results compared to the Poynting vector-based method. Numerical simulations demonstrate the efficacy of the new model, particularly when dealing with strong low-frequency fields that can lead to divergent carrier densities in the Poynting vector approach. The method is further validated through simulations of a photoconductive device, highlighting its improved accuracy in determining photocarrier generation rates. <br /> <div>
arXiv:2102.06702v3 Announce Type: replace-cross 
Abstract: Photocarrier generation rate in optoelectronic materials is often calculated using the Poynting vector in the frequency domain. However, this approach is not accurate in time-domain simulations of photoconductive devices because the instantaneous Poynting vector does not distinguish between power flux densities of optical and low-frequency electromagnetic fields. The latter is generated by photocurrents and is not supposed to contribute to the photocarrier generation since the corresponding photon energy is smaller than the bandgap energy of the optoelectronic material. This work proposes an optical absorption-based model to accurately calculate the generation rate in time-domain simulations. The proposed approach considers the material dispersion near the optical frequency corresponding to the bandgap energy of the optoelectronic material and calculates the instantaneous optical absorption from the polarization current density associated with this dispersion model. Numerical examples show that the proposed method is more accurate than the Poynting vector-based approach in calculating the instantaneous optical absorption. The method is further validated against experimental results via simulations of a photoconductive device, where the Poynting vector-based approach results in divergent carrier densities when the low-frequency fields are strong.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Self-Amplifying Hypergraph Structures through Mathematical Optimization</title>
<link>https://arxiv.org/abs/2412.15776</link>
<guid>https://arxiv.org/abs/2412.15776</guid>
<content:encoded><![CDATA[
<div> amplification factor, self-amplifying structures, hypergraphs, optimization, chemical reaction networks

Summary:
The paper introduces self-amplifying structures for hypergraphs, crucial for understanding propagation and internal reinforcement in complex systems. It defines the maximal amplification factor to quantify this phenomenon and develops an optimization-based methodology to compute it efficiently. The problem of identifying the subhypergraph maximizing the amplification factor is addressed as a mixed-integer nonlinear programming (MINLP) problem, solved with an exact iterative algorithm. Extensive computational experiments on synthetic instances demonstrate the relevance and effectiveness of the proposed approach. A case study on chemical reaction networks, including the Formose reaction and E. coli core metabolism, showcases the framework's ability to identify known and novel autocatalytic subnetworks, emphasizing its practical relevance in systems chemistry and biology.<br /><br />Summary: <div>
arXiv:2412.15776v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce the concept of self-amplifying structures for hypergraphs, positioning it as a key element for understanding propagation and internal reinforcement in complex systems. To quantify this phenomenon, we define the maximal amplification factor, a metric that captures how effectively a subhypergraph contributes to its own amplification. We then develop an optimization-based methodology to compute this measure. Building on this foundation, we tackle the problem of identifying the subhypergraph maximizing the amplification factor, formulating it as a mixed-integer nonlinear programming (MINLP) problem. To solve it efficiently, we propose an exact iterative algorithm with proven convergence guarantees. In addition, we report the results of extensive computational experiments on realistic synthetic instances, demonstrating both the relevance and effectiveness of the proposed approach. Finally, we present a case study on chemical reaction networks, including the Formose reaction and E. coli core metabolism, where our framework successfully identifies known and novel autocatalytic subnetworks, highlighting its practical relevance to systems chemistry and biology.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drivetrain simulation using variational autoencoders</title>
<link>https://arxiv.org/abs/2501.17653</link>
<guid>https://arxiv.org/abs/2501.17653</guid>
<content:encoded><![CDATA[
<div> Variational Autoencoders, Vehicle Jerk Prediction, Torque Demand, Drivetrain, Data Efficiency<br />
Summary:<br />
This work explores the use of variational autoencoders (VAEs) for predicting vehicle jerk signals based on torque demand in drivetrain applications with limited real-world datasets. The study involves training VAEs on experimental data from two electric SUV variants to synthesize jerk signals that capture characteristics of different drivetrain scenarios. Comparisons with physics-based and hybrid models demonstrate the effectiveness of VAEs in generating realistic jerk signals without extensive system parametrization. Unconditional VAEs successfully produce realistic signals without prior system knowledge, while conditional VAEs can tailor signals to specific torque inputs. By reducing the reliance on costly experiments and manual modeling, VAEs offer a data-efficient approach for exploring complex operational scenarios in drivetrain simulations. This integration of generative models like VAEs shows potential for enhancing validation procedures and accelerating vehicle development processes. <br /><br /> <div>
arXiv:2501.17653v2 Announce Type: replace-cross 
Abstract: This work proposes variational autoencoders (VAEs) to predict a vehicle's jerk signals from torque demand in the context of limited real-world drivetrain datasets. We implement both unconditional and conditional VAEs, trained on experimental data from two variants of a fully electric SUV with differing torque and drivetrain configurations. The VAEs synthesize jerk signals that capture characteristics from multiple drivetrain scenarios by leveraging the learned latent space. A performance comparison with baseline physics-based and hybrid models confirms the effectiveness of the VAEs, without requiring detailed system parametrization. Unconditional VAEs generate realistic jerk signals without prior system knowledge, while conditional VAEs enable the generation of signals tailored to specific torque inputs. This approach reduces the dependence on costly and time-intensive real-world experiments and extensive manual modeling. The results support the integration of generative models such as VAEs into drivetrain simulation pipelines, both for data augmentation and for efficient exploration of complex operational scenarios, with the potential to streamline validation and accelerate vehicle development.
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Storm Surge in Color: RGB-Encoded Physics-Aware Deep Learning for Storm Surge Forecasting</title>
<link>https://arxiv.org/abs/2506.21743</link>
<guid>https://arxiv.org/abs/2506.21743</guid>
<content:encoded><![CDATA[
<div> forecasting, storm surge, machine learning, ConvLSTM networks, coastal disaster

Summary:
This study introduces a novel approach to storm surge forecasting by utilizing ConvLSTM networks on structured RGB-encoded image representations of water elevation fields. The model incorporates ground-truth wind fields as dynamic conditioning signals and topo-bathymetry as a static input to capture surge evolution drivers. Evaluation on a dataset of synthetic storms in the Gulf of Mexico shows robust 48-hour forecasting performance across multiple regions along the Texas coast and spatial extensibility to other coastal areas. By combining structured representation, physically grounded forcings, and scalable deep learning, this approach advances storm surge forecasting in usability, adaptability, and interpretability. <div>
arXiv:2506.21743v1 Announce Type: new 
Abstract: Storm surge forecasting plays a crucial role in coastal disaster preparedness, yet existing machine learning approaches often suffer from limited spatial resolution, reliance on coastal station data, and poor generalization. Moreover, many prior models operate directly on unstructured spatial data, making them incompatible with modern deep learning architectures. In this work, we introduce a novel approach that projects unstructured water elevation fields onto structured Red Green Blue (RGB)-encoded image representations, enabling the application of Convolutional Long Short Term Memory (ConvLSTM) networks for end-to-end spatiotemporal surge forecasting. Our model further integrates ground-truth wind fields as dynamic conditioning signals and topo-bathymetry as a static input, capturing physically meaningful drivers of surge evolution. Evaluated on a large-scale dataset of synthetic storms in the Gulf of Mexico, our method demonstrates robust 48-hour forecasting performance across multiple regions along the Texas coast and exhibits strong spatial extensibility to other coastal areas. By combining structured representation, physically grounded forcings, and scalable deep learning, this study advances the frontier of storm surge forecasting in usability, adaptability, and interpretability.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Laser Scan Path Design for Controlled Microstructure in Additive Manufacturing with Integrated Reduced-Order Phase-Field Modeling and Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.21815</link>
<guid>https://arxiv.org/abs/2506.21815</guid>
<content:encoded><![CDATA[
<div> machine learning, laser powder bed fusion, microstructure, phase-field method, deep reinforcement learning 

Summary:
This research focuses on optimizing laser powder bed fusion (L-PBF) processes to control microstructure outcomes like equiaxed grains. By combining physics-guided modeling with machine learning, particularly a 3D U-Net convolutional neural network, the study aimed to accelerate the prediction of crystalline grain orientations. Three scanning strategies were explored, leading to a significant speedup in computational efficiency. Deep reinforcement learning (DRL) was then employed to generate optimized scan paths for target microstructures, showcasing its effectiveness in enhancing control over microstructure outcomes. The integration of the surrogate 3D U-Net model into the DRL environment further streamlined the training process. Ultimately, the study demonstrated the potential of machine learning methods to not only improve microstructure control but also enhance computational efficiency in optimizing L-PBF processes. <div>
arXiv:2506.21815v1 Announce Type: new 
Abstract: Laser powder bed fusion (L-PBF) is a widely recognized additive manufacturing technology for producing intricate metal components with exceptional accuracy. A key challenge in L-PBF is the formation of complex microstructures affecting product quality. We propose a physics-guided, machine-learning approach to optimize scan paths for desired microstructure outcomes, such as equiaxed grains. We utilized a phase-field method (PFM) to model crystalline grain structure evolution. To reduce computational costs, we trained a surrogate machine learning model, a 3D U-Net convolutional neural network, using single-track phase-field simulations with various laser powers to predict crystalline grain orientations based on initial microstructure and thermal history. We investigated three scanning strategies across various hatch spacings within a square domain, achieving a two-orders-of-magnitude speedup using the surrogate model. To reduce trial and error in designing laser scan toolpaths, we used deep reinforcement learning (DRL) to generate optimized scan paths for target microstructure. Results from three cases demonstrate the DRL approach's effectiveness. We integrated the surrogate 3D U-Net model into our DRL environment to accelerate the reinforcement learning training process. The reward function minimizes both aspect ratio and grain volume of the predicted microstructure from the agent's scan path. The reinforcement learning algorithm was benchmarked against conventional zigzag approach for smaller and larger domains, showing machine learning methods' potential to enhance microstructure control and computational efficiency in L-PBF optimization.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-free Forecasting of Rogue Waves using Reservoir Computing</title>
<link>https://arxiv.org/abs/2506.21918</link>
<guid>https://arxiv.org/abs/2506.21918</guid>
<content:encoded><![CDATA[
<div> Reservoir Computing, Hamiltonian systems, rogue waves, nonlinear Schrdinger equation, prediction horizon <br />
Summary: <br />
This paper explores the application of Reservoir Computing in modeling rogue wave dynamics from the nonlinear Schrdinger equation, a Hamiltonian system with modulation instability. The study demonstrates the effectiveness of Reservoir Computing in capturing these dynamics from breather simulations with unstable modes. The Echo State Network successfully predicts dynamics from two distinct testing datasets, including a higher-order breather, with remarkable agreement. The reservoir is able to forecast rogue wave propagation over a long prediction horizon, even when facing unseen dynamics. Additionally, a method is introduced to enhance the Reservoir Computing prediction in autonomous mode, improving its long-term forecasting ability. These findings contribute to advancing the use of Reservoir Computing in spatio-temporal Hamiltonian systems and underline the significance of phase space coverage in training data design. <br /> <div>
arXiv:2506.21918v1 Announce Type: new 
Abstract: Recent research has demonstrated Reservoir Computing's capability to model various chaotic dynamical systems, yet its application to Hamiltonian systems remains relatively unexplored. This paper investigates the effectiveness of Reservoir Computing in capturing rogue wave dynamics from the nonlinear Schr\"{o}dinger equation, a challenging Hamiltonian system with modulation instability. The model-free approach learns from breather simulations with five unstable modes. A properly tuned parallel Echo State Network can predict dynamics from two distinct testing datasets. The first set is a continuation of the training data, whereas the second set involves a higher-order breather. An investigation of the one-step prediction capability shows remarkable agreement between the testing data and the models. Furthermore, we show that the trained reservoir can predict the propagation of rogue waves over a relatively long prediction horizon, despite facing unseen dynamics. Finally, we introduce a method to significantly improve the Reservoir Computing prediction in autonomous mode, enhancing its long-term forecasting ability. These results advance the application of Reservoir Computing to spatio-temporal Hamiltonian systems and highlight the critical importance of phase space coverage in the design of training data.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Algorithm Based on CNN-LSTM Framework for Predicting Cancer Drug Sales Volume</title>
<link>https://arxiv.org/abs/2506.21927</link>
<guid>https://arxiv.org/abs/2506.21927</guid>
<content:encoded><![CDATA[
<div> deep learning, CNN-LSTM framework, cancer drug sales, forecasting, time series data

Summary:
This study explores the application potential of a deep learning model based on the CNN-LSTM framework in forecasting cancer drug sales. The research utilizes sales records of a specific cancer drug in Egypt from 2015 to 2024 to predict sales volume. A hybrid deep learning model combining CNN and LSTM networks is employed to improve prediction accuracy. The CNN component extracts local temporal features, while the LSTM component captures long-term dependencies. Model performance is evaluated using Mean Squared Error (MSE) and Root Mean Squared Error (RMSE), showing effectiveness in handling nonlinear and volatile sales data. The results demonstrate the model's success on the test set, with an MSE of 1.150 and an RMSE of 1.072. This research provides valuable insights for data-driven decision-making in pharmaceutical marketing and healthcare resource planning. 

Summary: <div>
arXiv:2506.21927v1 Announce Type: new 
Abstract: This study explores the application potential of a deep learning model based on the CNN-LSTM framework in forecasting the sales volume of cancer drugs, with a focus on modeling complex time series data. As advancements in medical technology and cancer treatment continue, the demand for oncology medications is steadily increasing. Accurate forecasting of cancer drug sales plays a critical role in optimizing production planning, supply chain management, and healthcare policy formulation. The dataset used in this research comprises quarterly sales records of a specific cancer drug in Egypt from 2015 to 2024, including multidimensional information such as date, drug type, pharmaceutical company, price, sales volume, effectiveness, and drug classification. To improve prediction accuracy, a hybrid deep learning model combining Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks is employed. The CNN component is responsible for extracting local temporal features from the sales data, while the LSTM component captures long-term dependencies and trends. Model performance is evaluated using two widely adopted metrics: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). The results demonstrate that the CNN-LSTM model performs well on the test set, achieving an MSE of 1.150 and an RMSE of 1.072, indicating its effectiveness in handling nonlinear and volatile sales data. This research provides theoretical and technical support for data-driven decision-making in pharmaceutical marketing and healthcare resource planning.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</title>
<link>https://arxiv.org/abs/2501.06184</link>
<guid>https://arxiv.org/abs/2501.06184</guid>
<content:encoded><![CDATA[
<div> geologic map, MLLMs, GeoMap-Bench, GeoMap-Agent, AI applications <br />
Summary: 
Geologic maps play a crucial role in understanding Earth's subsurface and surface. Current Multimodal Large Language Models (MLLMs) often struggle with geologic map understanding due to cartographic generalization challenges. To address this gap, GeoMap-Bench, a benchmark for evaluating MLLMs in geologic map understanding, was established. The GeoMap-Agent, designed for geologic map understanding, consists of Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA) modules. The AI expert group acts as consultants, achieving a high overall score on GeoMap-Bench. This work, known as PEACE, empowers geologic map holistic understanding with MLLMs, paving the way for advanced AI applications in geology. <div>
arXiv:2501.06184v1 Announce Type: cross 
Abstract: Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth's subsurface and surface. These maps are indispensable in various fields, including disaster detection, resource exploration, and civil engineering. Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding. This gap is primarily due to the challenging nature of cartographic generalization, which involves handling high-resolution map, managing multiple associated components, and requiring domain-specific knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions. Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o. Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Simulations of Turbulent Flows using Lattice Boltzmann Methods on Heterogeneous High Performance Computers</title>
<link>https://arxiv.org/abs/2506.21804</link>
<guid>https://arxiv.org/abs/2506.21804</guid>
<content:encoded><![CDATA[
<div> Keywords: GPU-accelerated supercomputers, turbulent flows, Lattice Boltzmann Methods, wall-modeled LES, scalability<br />
Summary:<br />
This paper focuses on the potential of GPU-accelerated supercomputers for large-scale simulations of turbulent flows. It introduces a novel Lattice Boltzmann Method (LBM) scheme tailored for wall-modeled Large Eddy Simulations (LES) in complex geometries. The scheme is specifically designed for efficient implementation within the open source LBM framework OpenLB. The study provides detailed scalability results for different HoreKa partitions, utilizing up to 128 nodes and covering problem sizes up to 18 billion cells. The research highlights the compatibility of LBM with highly parallel execution on both CPUs and GPUs, making it a promising tool for simulating turbulent flows in various applications. <div>
arXiv:2506.21804v1 Announce Type: cross 
Abstract: Current GPU-accelerated supercomputers promise to enable large-scale simulations of turbulent flows. Lattice Boltzmann Methods (LBM) are particularly well-suited to fulfilling this promise due to their intrinsic compatibility with highly parallel execution on both SIMD CPUs and GPUs. A novel LBM scheme for wall-modeled LES in complex geometries is described with a special focus on the efficient implementation in the open source LBM framework OpenLB. Detailed scalability results are provided for all HoreKa partitions, utilizing up to 128 nodes and covering problem sizes up to 18 billion cells.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StructMG: A Fast and Scalable Structured Algebraic Multigrid</title>
<link>https://arxiv.org/abs/2506.21932</link>
<guid>https://arxiv.org/abs/2506.21932</guid>
<content:encoded><![CDATA[
<div> derive, multigrid, structured grid, algebraic multigrid, parallel efficiency
Summary:
StructMG is a novel algebraic multigrid preconditioner designed for solving large-scale sparse linear systems on structured grids. Three key principles derived from the classical 'multigrid seesaw' guide its efficient implementation. StructMG automatically constructs hierarchical grids, achieving low cost per iteration and good convergence in parallel. By using a stencil-based triple-matrix product for multi-dimensional Galerkin coarsening, grid complexity and implementation effort are reduced. A unified parallel framework for sparse triangular solvers ensures fast convergence and high parallel efficiency in smoothers. StructMG outperforms existing multigrid preconditioners like \textit{hypre}'s in radiation hydrodynamics, petroleum reservoir simulation, numerical weather prediction, and solid mechanics applications on ARM and X86 platforms, with average speedups of 15.5x, 5.5x, 6.7x, and 7.3x, respectively. Additionally, it improves strong and weak scaling efficiencies significantly.<br /><br />Summary: <div>
arXiv:2506.21932v1 Announce Type: cross 
Abstract: Parallel multigrid is widely used as preconditioners in solving large-scale sparse linear systems. However, the current multigrid library still needs more satisfactory performance for structured grid problems regarding speed and scalability. Based on the classical 'multigrid seesaw', we derive three necessary principles for an efficient structured multigrid, which instructs our design and implementation of StructMG, a fast and scalable algebraic multigrid that constructs hierarchical grids automatically. As a preconditioner, StructMG can achieve both low cost per iteration and good convergence when solving large-scale linear systems with iterative methods in parallel. A stencil-based triple-matrix product via symbolic derivation and code generation is proposed for multi-dimensional Galerkin coarsening to reduce grid complexity, operator complexity, and implementation effort. A unified parallel framework of sparse triangular solver is presented to achieve fast convergence and high parallel efficiency for smoothers, including dependence-preserving Gauss-Seidel and incomplete LU methods. Idealized and real-world problems from radiation hydrodynamics, petroleum reservoir simulation, numerical weather prediction, and solid mechanics, are evaluated on ARM and X86 platforms to show StructMG's effectiveness. In comparison to \textit{hypre}'s structured and general multigrid preconditioners, StructMG achieves the fastest time-to-solutions in all cases with average speedups of 15.5x, 5.5x, 6.7x, 7.3x over SMG, PFMG, SysPFMG, and BoomerAMG, respectively. StructMG also significantly improves strong and weak scaling efficiencies.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EUR/USD Exchange Rate Forecasting incorporating Text Mining Based on Pre-trained Language Models and Deep Learning Methods</title>
<link>https://arxiv.org/abs/2411.07560</link>
<guid>https://arxiv.org/abs/2411.07560</guid>
<content:encoded><![CDATA[
<div> Keywords: EUR/USD, exchange rate forecasting, deep learning, textual analysis, particle swarm optimization

Summary: 

This study presents a new approach for forecasting the EUR/USD exchange rate by combining deep learning, textual analysis, and particle swarm optimization. By utilizing online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior accuracy compared to traditional models. The research incorporates advanced text mining techniques such as sentiment analysis and topic modeling. Empirical results show that integrating textual data significantly improves forecasting performance, with the PSO-LSTM model outperforming benchmark models. Ablation experiments highlight the contribution of each textual data category to the overall accuracy. The study emphasizes the potential of artificial intelligence in finance and suggests future research avenues for real-time forecasting and alternative data integration. 

<br /><br />Summary: <div>
arXiv:2411.07560v2 Announce Type: replace 
Abstract: This study introduces a novel approach for EUR/USD exchange rate forecasting that integrates deep learning, textual analysis, and particle swarm optimization (PSO). By incorporating online news and analysis texts as qualitative data, the proposed PSO-LSTM model demonstrates superior performance compared to traditional econometric and machine learning models. The research employs advanced text mining techniques, including sentiment analysis using the RoBERTa-Large model and topic modeling with LDA. Empirical findings underscore the significant advantage of incorporating textual data, with the PSO-LSTM model outperforming benchmark models such as SVM, SVR, ARIMA, and GARCH. Ablation experiments reveal the contribution of each textual data category to the overall forecasting performance. The study highlights the transformative potential of artificial intelligence in finance and paves the way for future research in real-time forecasting and the integration of alternative data sources.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Data Quality for AI to AI for Data Quality: A Systematic Review of Tools for AI-Augmented Data Quality Management in Data Warehouses</title>
<link>https://arxiv.org/abs/2406.10940</link>
<guid>https://arxiv.org/abs/2406.10940</guid>
<content:encoded><![CDATA[
<div> tools, AI-augmented data quality management, data warehouses, automation capabilities, DQ rules<br />
Summary:<br />
The study evaluates the automation capabilities of 151 data quality tools for AI-augmented data quality management in data warehouse environments. Only 10 tools were found suitable, focusing more on data cleansing and preparation for AI rather than improving DQ using AI. Features like SQL-based rule specification, reconciliation logic, and explainability of AI-driven recommendations are lacking. The study provides guidance for tool selection and highlights design requirements for next-generation AI-driven DQ solutions. It advocates a shift from "data quality for AI" to "AI for data quality management".<br /> <div>
arXiv:2406.10940v3 Announce Type: replace-cross 
Abstract: While high data quality (DQ) is critical for analytics, compliance, and AI performance, data quality management (DQM) remains a complex, resource-intensive, and often manual process. This study investigates the extent to which existing tools support AI-augmented data quality management (DQM) in data warehouse environments. To this end, we conduct a systematic review of 151 DQ tools to evaluate their automation capabilities, particularly in detecting and recommending DQ rules in data warehouses -- a key component of modern data ecosystems. Using a multi-phase screening process based on functionality, trialability, regulatory compliance (e.g., GDPR), and architectural compatibility with data warehouses, only 10 tools met the criteria for AI-augmented DQM. The analysis reveals that most tools emphasize data cleansing and preparation for AI, rather than leveraging AI to improve DQ itself. Although metadata- and ML-based rule detection techniques are present, features such as SQL-based rule specification, reconciliation logic, and explainability of AI-driven recommendations remain scarce. This study offers practical guidance for tool selection and outlines critical design requirements for next-generation AI-driven DQ solutions -- advocating a paradigm shift from ``data quality for AI'' to ``AI for data quality management''.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EUR-USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods</title>
<link>https://arxiv.org/abs/2408.13214</link>
<guid>https://arxiv.org/abs/2408.13214</guid>
<content:encoded><![CDATA[
<div> Keywords: EUR/USD exchange rate, forecasting, textual data, structured data, Optuna-Bi-LSTM model 

Summary: 
The paper introduces the IUS framework, a novel approach for enhancing EUR/USD exchange rate prediction by integrating unstructured textual data with structured data. This framework utilizes large language models for sentiment analysis and exchange rate movement classification, combined with quantitative features in a Causality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then employed for forecasting. Experimental results show that the proposed method outperforms benchmark models, achieving a 10.69% reduction in MAE and a 9.56% decrease in RMSE. The fusion of unstructured and structured data proves beneficial, with the combined approach yielding higher accuracy than using structured data alone. Feature selection highlights the importance of the top 12 quantitative features combined with textual features. Overall, the IUS framework and Optuna-Bi-LSTM model offer a robust and effective method for EUR/USD exchange rate forecasting through the integration of multi-source data. 

<br /><br />Summary: <div>
arXiv:2408.13214v2 Announce Type: replace-cross 
Abstract: Accurate forecasting of the EUR/USD exchange rate is crucial for investors, businesses, and policymakers. This paper proposes a novel framework, IUS, that integrates unstructured textual data from news and analysis with structured data on exchange rates and financial indicators to enhance exchange rate prediction. The IUS framework employs large language models for sentiment polarity scoring and exchange rate movement classification of texts. These textual features are combined with quantitative features and input into a Causality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then used to forecast the EUR/USD exchange rate. Experiments demonstrate that the proposed method outperforms benchmark models, reducing MAE by 10.69% and RMSE by 9.56% compared to the best performing baseline. Results also show the benefits of data fusion, with the combination of unstructured and structured data yielding higher accuracy than structured data alone. Furthermore, feature selection using the top 12 important quantitative features combined with the textual features proves most effective. The proposed IUS framework and Optuna-Bi-LSTM model provide a powerful new approach for exchange rate forecasting through multi-source data integration.
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion</title>
<link>https://arxiv.org/abs/2506.20763</link>
<guid>https://arxiv.org/abs/2506.20763</guid>
<content:encoded><![CDATA[
<div> formulation, phase field, multi-physics, finite element, engineering

Summary: 
This article introduces a new formulation to address coupled structural integrity problems by combining phase field and multi-physics modeling techniques. The approach leverages the flexibility of the heat transfer equation, making it suitable for integration into commercial finite element packages with only integration point-level implementation. The methodology is demonstrated through the implementation of coupled phenomena using user subroutines in the Abaqus finite element package. The theoretical and computational framework is applied to four engineering and scientific problems: thermo-mechanical fracture, hydraulic fracture, hydrogen-assisted cracking, and metallic corrosion, in both 2D and 3D scenarios. The results obtained show excellent agreement with experimental, numerical, and analytical solutions. The user subroutines developed for implementation are freely accessible online, enhancing the accessibility and usability of the proposed approach. <div>
arXiv:2506.20763v1 Announce Type: new 
Abstract: We present a novel, generalised formulation to treat coupled structural integrity problems by combining phase field and multi-physics modelling. The approach exploits the versatility of the heat transfer equation and is therefore well suited to be adopted in commercial finite element packages, requiring only integration point-level implementation. This aspect is demonstrated here by implementing coupled, multi-variable phenomena through simple \texttt{UMAT} and \texttt{UMATHT} subroutines in the finite element package \texttt{Abaqus}. The generalised theoretical and computational framework presented is particularised to four problems of engineering and scientific relevance: thermo-mechanical fracture, hydraulic fracture, hydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are considered. The results reveal a very good agreement with experimental data, and existing numerical and analytical solutions.The user subroutines developed are made freely available at https://mechmat.web.ox.ac.uk/codes.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers</title>
<link>https://arxiv.org/abs/2506.20773</link>
<guid>https://arxiv.org/abs/2506.20773</guid>
<content:encoded><![CDATA[
<div> Keywords: viscoelasticity, permanent set, polymers, transient network theory, numerical framework

Summary:
An efficient numerical framework for modeling viscoelasticity and permanent set of polymers is presented. The framework is based on the hereditary integral form of transient network theory, where polymer chains belong to distinct networks with different natural equilibrium states. Chains detach from previous networks and attach to new networks in a state of zero stress. The free energy of these networks is defined in terms of the deformation gradient relative to the network's birth configuration. A decomposition of the kernel for various free energies allows for a recurrence relationship without the need for full-time history integration. The technique applies to both highly compressible and nearly incompressible materials using various material models. The framework can handle rate-dependent response and residual strains under complex loading histories, as demonstrated through multiple examples. <div>
arXiv:2506.20773v1 Announce Type: new 
Abstract: An efficient numerical framework is presented for modeling viscoelasticity and permanent set of polymers. It is based on the hereditary integral form of transient network theory, in which polymer chains belong to distinct networks each with different natural equilibrium states. Chains continually detach from previously formed networks and reattach to new networks in a state of zero stress. The free energy of these networks is given in terms of the deformation gradient relative to the configuration at which the network was born. A decomposition of the kernel for various free energies allows for a recurrence relationship to be established, bypassing the need to integrate over all time history. The technique is established for both highly compressible and nearly incompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and Ogden-Hill material models. Multiple examples are presented showing the ability to handle rate-dependent response and residual strains under complex loading histories.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation</title>
<link>https://arxiv.org/abs/2506.21362</link>
<guid>https://arxiv.org/abs/2506.21362</guid>
<content:encoded><![CDATA[
<div> Counterfactual Voting Adjustment, online platforms, information quality, helpfulness voting, content ranking  
Summary:  
The article introduces the Counterfactual Voting Adjustment (CVA) framework for fairer assessment of information quality on online platforms. CVA addresses biases in aggregated votes by considering the context of individual votes, effectively modeling position and herding biases. Preliminary experiments demonstrate that CVA accurately recovers predefined content quality. In a real experiment, reranking content based on CVA-learned quality aligns better with user sentiment and quality evaluation by GPT-4o compared to rankings based on aggregated votes. The article also offers insights into the behavioral dynamics of expert user groups across 120 StackExchange communities using CVA embeddings. <div>
arXiv:2506.21362v1 Announce Type: new 
Abstract: Efficient access to high-quality information is vital for online platforms. To promote more useful information, users not only create new content but also evaluate existing content, often through helpfulness voting. Although aggregated votes help service providers rank their user content, these votes are often biased by disparate accessibility per position and the cascaded influence of prior votes. For a fairer assessment of information quality, we propose the Counterfactual Voting Adjustment (CVA), a causal framework that accounts for the context in which individual votes are cast. Through preliminary and semi-synthetic experiments, we show that CVA effectively models the position and herding biases, accurately recovering the predefined content quality. In a real experiment, we demonstrate that reranking content based on the learned quality by CVA exhibits stronger alignment with both user sentiment and quality evaluation assessed by GPT-4o, outperforming system rankings based on aggregated votes and model-based rerankings without causal inference. Beyond the individual quality inference, our embeddings offer comparative insights into the behavioral dynamics of expert user groups across 120 major StackExchange communities.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools</title>
<link>https://arxiv.org/abs/2506.20743</link>
<guid>https://arxiv.org/abs/2506.20743</guid>
<content:encoded><![CDATA[
<div> Foundation models, materials science, AI systems, scientific discovery, multimodal. 

Summary:
Foundation models are revolutionizing materials science by enabling scalable, versatile AI systems for scientific research. These models offer cross-domain generalization and emergent capabilities, making them ideal for the diverse challenges in materials science. The survey covers various application areas, including data extraction, atomistic simulation, property prediction, materials design, process planning, and multiscale modeling. It discusses advancements in unimodal and multimodal foundation models, as well as large language model agents. Standardized datasets, open-source tools, and autonomous experimental platforms are also reviewed. While foundation models have shown early success, challenges remain in generalizability, interpretability, data imbalance, safety, and multimodal fusion. Future research directions focus on scalable pretraining, continual learning, data governance, and trustworthiness.<br /><br />Summary: <div>
arXiv:2506.20743v1 Announce Type: cross 
Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\&amp;A atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pull-off strength of mushroom-shaped fibrils adhered to rigid substrates</title>
<link>https://arxiv.org/abs/2506.20745</link>
<guid>https://arxiv.org/abs/2506.20745</guid>
<content:encoded><![CDATA[
<div> Keywords: adhesion, fibrillar structures, mushroom-shaped fibrils, computational analysis, cohesive zone model

Summary: 
- The study focuses on analyzing the detachment behavior of mushroom-shaped fibrils adhered to a rigid substrate using a computational approach based on a Dugdale cohesive zone model.
- The results indicate that the separation process during detachment is inherently unstable under load control, regardless of whether detachment initiates at the edge or center of the fibril.
- Fibrils with wide, thin mushroom caps demonstrate enhanced adhesion by reducing stress concentrations and promoting central detachment, leading to improved adhesion strength.
- While central detachment is not observed in all geometries, edge detachment can occur under specific conditions in all cases.
- Adhesion defects at the fibril center can greatly reduce pull-off strength, especially at high values of the dimensionless parameter \c{hi}.

<br /><br />Summary: <div>
arXiv:2506.20745v1 Announce Type: cross 
Abstract: The exceptional adhesion properties of biological fibrillar structures -- such as those found in geckos -- have inspired the development of synthetic adhesive surfaces. Among these, mushroom-shaped fibrils have demonstrated superior pull-off strength compared to other geometries. In this study, we employ a computational approach based on a Dugdale cohesive zone model to analyze the detachment behavior of these fibrils when adhered to a rigid substrate. The results provide complete pull-off curves, revealing that the separation process is inherently unstable under load control, regardless of whether detachment initiates at the fibril edge or center. Our findings show that fibrils with a wide, thin mushroom cap effectively reduce stress concentrations and promote central detachment, leading to enhanced adhesion. However, detachment from the center is not observed in all geometries, whereas edge detachment can occur under certain conditions in all cases. Additionally, we investigate the impact of adhesion defects at the fibril center, showing that they can significantly reduce pull-off strength, particularly at high values of the dimensionless parameter \c{hi}. These insights contribute to the optimization of bio-inspired adhesives and microstructured surfaces for various engineering applications.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering</title>
<link>https://arxiv.org/abs/2506.20821</link>
<guid>https://arxiv.org/abs/2506.20821</guid>
<content:encoded><![CDATA[
<div> Keywords: financial documents, multimodal extraction, retrieval-augmented generation, cross-modal reasoning, MultiFinRAG

Summary: 
- Financial documents, such as 10-Ks and investor presentations, are complex and require joint reasoning across modalities.
- MultiFinRAG is a framework designed for financial question answering that integrates multimodal extraction and retrieval-augmented generation.
- The framework utilizes a lightweight multimodal LLM for extracting structured outputs and textual summaries from tables and figures.
- MultiFinRAG indexes these outputs with modality-aware similarity thresholds for precise retrieval and employs a tiered fallback strategy for cross-modal reasoning.
- Despite running on basic hardware, MultiFinRAG outperforms ChatGPT-4o in accuracy by 19 percentage points on financial QA tasks involving multiple modalities. 

<br /><br />Summary: <div>
arXiv:2506.20821v1 Announce Type: cross 
Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span hundreds of pages and combine diverse modalities, including dense narrative text, structured tables, and complex figures. Answering questions over such content often requires joint reasoning across modalities, which strains traditional large language models (LLMs) and retrieval-augmented generation (RAG) pipelines due to token limitations, layout loss, and fragmented cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation framework purpose-built for financial QA. MultiFinRAG first performs multimodal extraction by grouping table and figure images into batches and sending them to a lightweight, quantized open-source multimodal LLM, which produces both structured JSON outputs and concise textual summaries. These outputs, along with narrative text, are embedded and indexed with modality-aware similarity thresholds for precise retrieval. A tiered fallback strategy then dynamically escalates from text-only to text+table+image contexts when necessary, enabling cross-modal reasoning while reducing irrelevant context. Despite running on commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy than ChatGPT-4o (free-tier) on complex financial QA tasks involving text, tables, images, and combined multimodal reasoning.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multicontinuum Homogenization for Poroelasticity Model</title>
<link>https://arxiv.org/abs/2506.20890</link>
<guid>https://arxiv.org/abs/2506.20890</guid>
<content:encoded><![CDATA[
<div> homogenization, poroelasticity, multicontinuum, porous media, computational challenges  
Summary:
In this paper, the authors derive multicontinuum poroelasticity models using the multicontinuum homogenization method. Poroelasticity models are essential in various fields to describe coupled flow and mechanics in porous media. The high contrast properties of poroelastic media create computational challenges, which standard homogenization approaches struggle to address. By employing multicontinuum methods, multiple average states called continua are defined, allowing for accurate solutions in cases of high property contrast. The authors extend previous research by deriving a generalized multicontinuum poroelasticity model using a rigorous approach. They formulate coupled constraint cell problems in oversampled regions, obtain a multicontinuum expansion of fine-scale fields, and derive the multicontinuum model while assuming the smoothness of macroscopic variables. Numerical experiments confirm the accuracy of the proposed multicontinuum models for various heterogeneous media cases. <div>
arXiv:2506.20890v1 Announce Type: cross 
Abstract: In this paper, we derive multicontinuum poroelasticity models using the multicontinuum homogenization method. Poroelasticity models are widely used in many areas of science and engineering to describe coupled flow and mechanics processes in porous media. However, in many applications, the properties of poroelastic media possess high contrast, presenting serious computational challenges. It is well known that standard homogenization approaches often fail to give an accurate solution due to the lack of macroscopic parameters. Multicontinuum approaches allow us to consider such cases by defining several average states known as continua. In the field of poroelasticity, multiple-network models arising from the multiple porous media theory are representatives of these approaches. In this work, we extend previous findings by deriving the generalized multicontinuum poroelasticity model. We apply the recently developed multicontinuum homogenization method and provide a rigorous derivation of multicontinuum equations. For this purpose, we formulate coupled constraint cell problems in oversampled regions to consider different homogenized effects. Then, we obtain a multicontinuum expansion of the fine-scale fields and derive the multicontinuum model supposing the smoothness of macroscopic variables. We present the most general version of equations and the simplified ones based on our numerical experiments. Numerical results are presented for different heterogeneous media cases and demonstrate the high accuracy of our proposed multicontinuum models.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-guided Chemical Process Optimization with a Multi-Agent Approach</title>
<link>https://arxiv.org/abs/2506.20921</link>
<guid>https://arxiv.org/abs/2506.20921</guid>
<content:encoded><![CDATA[
<div> framework, optimization, constraints, autonomous, process <br />
Summary: <br />
The article presents a novel approach to chemical process optimization using a multi-agent framework of large language model agents. This framework autonomously infers operating constraints from minimal process descriptions and collaboratively guides optimization. By employing OpenAI's o3 model, the AutoGen-based framework eliminates the need for predefined operational bounds, achieving better computational efficiency and competitive performance with conventional methods. Validated on the hydrodealkylation process, the framework demonstrated a 31-fold speedup over grid search, converging in under 20 minutes. The reasoning-guided search showcases sophisticated process understanding, correctly identifying utility trade-offs and applying domain-informed heuristics. This approach shows significant potential for optimization scenarios with poorly characterized or unavailable constraints, particularly in emerging processes and retrofit applications. <br /> <div>
arXiv:2506.20921v1 Announce Type: cross 
Abstract: Chemical process optimization is crucial to maximize production efficiency and economic performance. Traditional methods, including gradient-based solvers, evolutionary algorithms, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable, requiring engineers to rely on subjective heuristics to estimate feasible parameter ranges. To address this constraint definition bottleneck, we present a multi-agent framework of large language model (LLM) agents that autonomously infer operating constraints from minimal process descriptions, then collaboratively guide optimization using the inferred constraints. Our AutoGen-based agentic framework employs OpenAI's o3 model, with specialized agents for constraint generation, parameter validation, simulation execution, and optimization guidance. Through two phases - autonomous constraint generation using embedded domain knowledge, followed by iterative multi-agent optimization - the framework eliminates the need for predefined operational bounds. Validated on the hydrodealkylation process across cost, yield, and yield-to-cost ratio metrics, the framework demonstrated competitive performance with conventional optimization methods while achieving better computational efficiency, requiring fewer iterations to converge. Our approach converged in under 20 minutes, achieving a 31-fold speedup over grid search. Beyond computational efficiency, the framework's reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs, and applying domain-informed heuristics. This approach shows significant potential for optimization scenarios where operational constraints are poorly characterized or unavailable, particularly for emerging processes and retrofit applications.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Institutional Noise, Strategic Deviation, and Intertemporal Collapse: A Formal Model of Miner Behaviour under Protocol Uncertainty</title>
<link>https://arxiv.org/abs/2506.20992</link>
<guid>https://arxiv.org/abs/2506.20992</guid>
<content:encoded><![CDATA[
<div> blockchain, game theory, protocol mutability, institutional rules, cooperative mining

Summary:
Using a game-theoretic model, this paper examines the impact of protocol mutability on cooperative mining behavior in blockchain systems. It finds that uncertainty in institutional rules leads to higher discounting and strategic deviation among miners. Stable protocols support long-term investment and equilibrium strategies, while mutable protocols promote short-termism and collapse of cooperation. Simulation results highlight zones of instability where rational mining transitions to extractive practices. The study suggests that stable rules are crucial for productive action and sustainable cooperation in decentralized systems. Protocol design should be viewed as a fundamental economic constraint rather than a discretionary variable to foster sustainable cooperation. <div>
arXiv:2506.20992v1 Announce Type: cross 
Abstract: This paper develops a formal game-theoretic model to examine how protocol mutability disrupts cooperative mining behaviour in blockchain systems. Using a repeated game framework with stochastic rule shocks, we show that even minor uncertainty in institutional rules increases time preference and induces strategic deviation. Fixed-rule environments support long-term investment and stable equilibrium strategies; in contrast, mutable protocols lead to short-termism, higher discounting, and collapse of coordinated engagement. Simulation results identify instability zones in the parameter space where rational mining gives way to extractive or arbitrage conduct. These findings support an Austrian economic interpretation: calculability requires rule stability. Institutional noise undermines the informational basis for productive action. We conclude that protocol design must be treated as a constitutional economic constraint, not a discretionary variable, if sustainable cooperation is to emerge in decentralised systems.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Inverse Problem for Multi-armed Bandits via Convex Optimization</title>
<link>https://arxiv.org/abs/2501.18945</link>
<guid>https://arxiv.org/abs/2501.18945</guid>
<content:encoded><![CDATA[
<div> Keywords: IMAB, multi-armed bandits, convex optimization, heuristic method, neuroscience<br />
<br />
Summary: 
The article focuses on the inverse problem of multi-armed bandits (IMAB) and its application in neuroscience and psychology research. It demonstrates that the IMAB problem is not convex but can be transformed into a convex problem for easier solution. A two-step sequential heuristic method is proposed to approximately solve the IMAB problem efficiently. The method is shown to provide global solutions with a certificate under certain conditions and offers approximations to reduce computational time. Numerical experiments indicate that the proposed heuristic method outperforms traditional local optimization approaches and matches the performance of Monte Carlo methods in a shorter time frame. The method is implemented using CVXPY, making it accessible to users without expertise in convex optimization. <div>
arXiv:2501.18945v3 Announce Type: replace 
Abstract: We consider the inverse problem of multi-armed bandits (IMAB) that are widely used in neuroscience and psychology research for behavior modelling. We first show that the IMAB problem is not convex in general, but can be relaxed to a convex problem via variable transformation. Based on this result, we propose a two-step sequential heuristic for (approximately) solving the IMAB problem. We discuss a condition where our method provides global solution to the IMAB problem with certificate, as well as approximations to further save computing time. Numerical experiments indicate that our heuristic method is more robust than directly solving the IMAB problem via repeated local optimization, and can achieve the performance of Monte Carlo methods within a significantly decreased running time. We provide the implementation of our method based on CVXPY, which allows straightforward application by users not well versed in convex optimization.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Reinforcement Learning via Convex Optimization</title>
<link>https://arxiv.org/abs/2501.15957</link>
<guid>https://arxiv.org/abs/2501.15957</guid>
<content:encoded><![CDATA[
<div> Convex Optimization, Inverse Reinforcement Learning, Expert Demonstrations, Markov Decision Process, Auto-selection<br />
Summary: 
This article discusses the Inverse Reinforcement Learning (IRL) problem and introduces a convex formulation of the problem known as CIRL. The authors present a method to apply the domain-specific language CVXPY to solve the convex IRL problem, making it more accessible for users without a background in convex optimization. They also address scenarios where the expert policy is provided as state-action pairs rather than analytically, extending the CIRL problem to handle such cases. Theoretical analysis and practical implementation for hyperparameter auto-selection are discussed, allowing users to easily apply CIRL to their specific problems. This approach aims to overcome the challenges posed by traditional nonconvex IRL formulations, offering a more robust and reproducible solution for estimating unknown reward functions in Markov decision processes based on expert demonstrations. <div>
arXiv:2501.15957v2 Announce Type: replace-cross 
Abstract: We consider the inverse reinforcement learning (IRL) problem, where an unknown reward function of some Markov decision process is estimated based on observed expert demonstrations. In most existing approaches, IRL is formulated and solved as a nonconvex optimization problem, posing challenges in scenarios where robustness and reproducibility are critical. We discuss a convex formulation of the IRL problem (CIRL) initially proposed by Ng and Russel, and reformulate the problem such that the domain-specific language CVXPY can be applied directly to specify and solve the convex problem. We also extend the CIRL problem to scenarios where the expert policy is not given analytically but by trajectory as state-action pairs, which can be strongly inconsistent with optimality, by augmenting some of the constraints. Theoretical analysis and practical implementation for hyperparameter auto-selection are introduced. This note helps the users to easily apply CIRL for their problems, without background knowledge on convex optimization.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena Perspective</title>
<link>https://arxiv.org/abs/2503.18313</link>
<guid>https://arxiv.org/abs/2503.18313</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial decision-making, DeepFund, multi-agent framework, live environment

Summary:
DeepFund addresses the inadequately evaluated effectiveness of Large Language Models (LLMs) in financial decision-making by introducing a comprehensive arena platform for evaluating LLM-based trading strategies. The platform implements a multi-agent framework that simulates real-world investment decision processes, allowing LLMs to take on different key roles in managing assets and uncovering trading opportunities. DeepFund offers a web interface for visualizing LLMs' performance with fund investment metrics across various market conditions, enabling detailed comparative analysis. The platform aims to provide a more realistic and fair assessment of LLM capabilities in fund investment, addressing issues such as data leakage, navel-gazing, over-intervention, and maintenance-hardness identified in existing benchmarks. By offering diversified insights and potential applications in real-world financial markets, DeepFund contributes to advancing LLM research in financial decision-making. The code for DeepFund is publicly available on GitHub at https://github.com/HKUSTDial/DeepFund. 

<br /><br />Summary: <div>
arXiv:2503.18313v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across various domains, but their effectiveness in financial decision-making remains inadequately evaluated. Current benchmarks primarily assess LLMs' understanding on financial documents rather than the ability to manage assets or dig out trading opportunities in dynamic market conditions. Despite the release of new benchmarks for evaluating diversified tasks on the financial domain, we identified four major problems in these benchmarks, which are data leakage, navel-gazing, over-intervention, and maintenance-hard. To pave the research gap, we introduce DeepFund, a comprehensive arena platform for evaluating LLM-based trading strategies in a live environment. Our approach implements a multi-agent framework where they serve as multiple key roles that realize the real-world investment decision processes. Moreover, we provide a web interface that visualizes LLMs' performance with fund investment metrics across different market conditions, enabling detailed comparative analysis. Through DeepFund, we aim to provide a more realistic and fair assessment on LLM's capabilities in fund investment, offering diversified insights and revealing their potential applications in real-world financial markets. Our code is publicly available at https://github.com/HKUSTDial/DeepFund.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-convex Programming for Discrete Latent Factor Models Prototyping</title>
<link>https://arxiv.org/abs/2504.01431</link>
<guid>https://arxiv.org/abs/2504.01431</guid>
<content:encoded><![CDATA[
<div> CVXPY, discrete latent factor models, fitting problem, regularization terms, constraints
Summary: 
Discrete latent factor models (DLFMs) are commonly used in various fields but require custom solvers for fitting, limiting their flexibility. This paper introduces a generic framework based on CVXPY that enables users to easily specify and solve the fitting problem of a range of DLFMs through a concise script. The framework supports both regression and classification models, integrates regularization terms, and allows for constraints on DLFM parameters and latent factors. This versatility allows users to prototype DLFM structures tailored to their dataset and application needs. The open-source Python implementation of the framework is demonstrated through several examples, showcasing its utility and effectiveness in practice. <div>
arXiv:2504.01431v2 Announce Type: replace-cross 
Abstract: Discrete latent factor models (DLFMs) are widely used in various domains such as machine learning, economics, neuroscience, psychology, etc. Currently, fitting a DLFM to some dataset relies on a customized solver for individual models, which requires lots of effort to implement and is limited to the targeted specific instance of DLFMs. In this paper, we propose a generic framework based on CVXPY, which allows users to specify and solve the fitting problem of a wide range of DLFMs, including both regression and classification models, within a very short script. Our framework is flexible and inherently supports the integration of regularization terms and constraints on the DLFM parameters and latent factors, such that the users can easily prototype the DLFM structure according to their dataset and application scenario. We introduce our open-source Python implementation and illustrate the framework in several examples.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiT-SGCR: Directed Temporal Structural Representation with Global-Cluster Awareness for Ethereum Malicious Account Detection</title>
<link>https://arxiv.org/abs/2506.20123</link>
<guid>https://arxiv.org/abs/2506.20123</guid>
<content:encoded><![CDATA[
<div> Keywords: Ethereum, DeFi, malicious account detection, temporal transaction evolution, unsupervised graph encoder

Summary: 
The article proposes DiT-SGCR, an unsupervised graph encoder designed to detect malicious accounts on the Ethereum platform, focusing on the dynamic transaction behaviors of users. By incorporating directional temporal aggregation, differentiable clustering, and graph Laplacian regularization, DiT-SGCR effectively captures money movement trajectories and organized collective behavior. This approach enhances the quality and dimensionality of account embeddings while maintaining scalability advantages. Through extensive experiments on three datasets, DiT-SGCR consistently outperforms existing methods in malicious account detection, showcasing significant improvements in F1-score accuracy ranging from 3.62% to 10.83%. The research highlights the importance of considering temporal dynamics, global topology, and cluster-specific behavioral patterns in identifying malicious actors within decentralized finance ecosystems. <br /><br />Summary: <div>
arXiv:2506.20123v1 Announce Type: new 
Abstract: The detection of malicious accounts on Ethereum - the preeminent DeFi platform - is critical for protecting digital assets and maintaining trust in decentralized finance. Recent advances highlight that temporal transaction evolution reveals more attack signatures than static graphs. However, current methods either fail to model continuous transaction dynamics or incur high computational costs that limit scalability to large-scale transaction networks. Furthermore, current methods fail to consider two higher-order behavioral fingerprints: (1) direction in temporal transaction flows, which encodes money movement trajectories, and (2) account clustering, which reveals coordinated behavior of organized malicious collectives. To address these challenges, we propose DiT-SGCR, an unsupervised graph encoder for malicious account detection. Specifically, DiT-SGCR employs directional temporal aggregation to capture dynamic account interactions, then coupled with differentiable clustering and graph Laplacian regularization to generate high-quality, low-dimensional embeddings. Our approach simultaneously encodes directional temporal dynamics, global topology, and cluster-specific behavioral patterns, thereby enhancing the discriminability and robustness of account representations. Furthermore, DiT-SGCR bypasses conventional graph propagation mechanisms, yielding significant scalability advantages. Extensive experiments on three datasets demonstrate that DiT-SGCR consistently outperforms state-of-the-art methods across all benchmarks, achieving F1-score improvements ranging from 3.62% to 10.83%.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing Artificial Mechanics Intuitions from Extremely Small Data</title>
<link>https://arxiv.org/abs/2506.20148</link>
<guid>https://arxiv.org/abs/2506.20148</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, mechanics intuitions, sample-switchable training, brachistochrone problem, nonlinear deformation problem

Summary:
The article discusses the development of artificial mechanics intuitions through a sample-switchable training method that allows learning from limited examples. The method enables artificial intelligence to master complex problems like the brachistochrone problem, catenary problem, and large nonlinear deformation problem of an elastic plate using only three samples. The model's intuitive prediction capacity improves non-linearly with the number of training samples, highlighting the possibility of achieving excellent mechanics intuitions with a finite set of examples. This approach offers a new perspective on educating AI systems to intuitively understand and predict material behavior, resembling scenarios often depicted in Science-Fiction movies.
<br /><br />Summary: The article explores the development of artificial mechanics intuitions using sample-switchable training, enabling AI to master complex problems with minimal data. The model's intuitive prediction improves with the number of training samples, demonstrating the potential to achieve exceptional mechanics intuition with a limited dataset. This work presents a novel approach to educating AI systems to intuitively predict material behavior, akin to scenarios in Science-Fiction films. <div>
arXiv:2506.20148v1 Announce Type: new 
Abstract: Humans can possess good mechanics intuitions by learning from a few examples, which leads to the question of how to develop artificial mechanics intuitions that can be learned from small data, as we are eagerly entering the era of artificial intelligence. We propose in this Letter the sample-switchable training method, which successfully develops highly-accurate artificial mechanics intuitions that can master brachistochrone problem, catenary problem, and large nonlinear deformation problem of elastic plate by learning from no more than three samples. The model's intuitive prediction ability increases nonlinearly with respect to the number of training samples, suggesting that superb mechanics intuitions can be in-principle achieved based on a finite number of samples, reflecting how human brains form good mechanics intuitions just by learning a few cases. Our current work presents an alternative perspective for educating artificial intelligence capable of intuitively understand and predict how materials deform and move, a scenario that has been frequently seen in Science-Fiction movies.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opinion Dynamics with Highly Oscillating Opinions</title>
<link>https://arxiv.org/abs/2506.20472</link>
<guid>https://arxiv.org/abs/2506.20472</guid>
<content:encoded><![CDATA[
<div> Opinion Dynamics; Agent-Based Models; Evolutionary Algorithms; Immigration dataset; ATBCR <br />
Summary: <br />
Opinion Dynamics (OD) models focus on understanding how opinions evolve within a population through interactions between agents. This study addresses the limitations of existing OD models in analyzing scenarios of highly oscillating opinions, such as those found in real-world situations. The research formulates an optimization problem and applies Evolutionary Algorithms to evaluate the performance of various OD models in capturing highly oscillating dynamics. Using a real-world opinion dataset on immigration, the study finds that the ATBCR model, incorporating both rational and emotional opinion update mechanisms, is the most accurate for representing highly oscillating opinions. This research contributes to enhancing the understanding of opinion evolution in complex social contexts. <br /> <div>
arXiv:2506.20472v1 Announce Type: new 
Abstract: Opinion Dynamics (OD) models are a particular case of Agent-Based Models in which the evolution of opinions within a population is studied. In most OD models, opinions evolve as a consequence of interactions between agents, and the opinion fusion rule defines how those opinions are updated. In consequence, despite being simplistic, OD models provide an explainable and interpretable mechanism for understanding the underlying dynamics of opinion evolution. Unfortunately, existing OD models mainly focus on explaining the evolution of (usually synthetic) opinions towards consensus, fragmentation, or polarization, but they usually fail to analyze scenarios of (real-world) highly oscillating opinions. This work overcomes this limitation by studying the ability of several OD models to reproduce highly oscillating dynamics. To this end, we formulate an optimization problem which is further solved using Evolutionary Algorithms, providing both quantitative results on the performance of the optimization and qualitative interpretations on the obtained results. Our experiments on a real-world opinion dataset about immigration from the monthly barometer of the Spanish Sociological Research Center show that the ATBCR, based on both rational and emotional mechanisms of opinion update, is the most accurate OD model for capturing highly oscillating opinions.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ten simple rules for PIs to integrate Research Software Engineering into their research group</title>
<link>https://arxiv.org/abs/2506.20217</link>
<guid>https://arxiv.org/abs/2506.20217</guid>
<content:encoded><![CDATA[
<div> Research Software Engineering, high-quality research software, accessibility, PIs, reproducibility <br />
Summary:
Research Software Engineering (RSEng) plays a crucial role in producing high-quality research software, enhancing research outcomes. However, many principal investigators (PIs) and research group leaders may lack knowledge on RSEng and its benefits. The technical complexities of RSEng can also hinder accessibility for some researchers. To address these challenges, this paper presents ten simple rules to help PIs and leaders integrate RSEng into their research groups more effectively. By following these rules, researchers can improve the quality, reproducibility, and trustworthiness of their research software, ultimately leading to better, more reproducible, and trustworthy research outcomes. <div>
arXiv:2506.20217v1 Announce Type: cross 
Abstract: Research Software Engineering (RSEng) is a key success factor in producing high-quality research software, which in turn enables and improves research outcomes. However, as a principal investigator or leader of a research group you may not know what RSEng is, where to get started with it, or how to use it to maximize its benefit for your research. RSEng also often comes with technical complexity, and therefore reduced accessibility to some researchers. The ten simple rules presented in this paper aim to improve the accessibility of RSEng, and provide practical and actionable advice to PIs and leaders for integrating RSEng into their research group. By following these rules, readers can improve the quality, reproducibility, and trustworthiness of their research software, ultimately leading to better, more reproducible and more trustworthy research outcomes.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem</title>
<link>https://arxiv.org/abs/2506.20400</link>
<guid>https://arxiv.org/abs/2506.20400</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent-based simulations, electric vehicle home charging ecosystems, Python-based dashboard framework, emergent behavior, root-cause analysis

Summary:
This paper introduces a Python-based dashboard framework using Dash by Plotly for analyzing multi-agent-based simulations of electric vehicle home charging ecosystems. The framework consists of three coordinated views for exploring emergent behavior in the simulation outputs, including time-series plots, spatial heatmaps, and drill-down tools. A case study in a Danish residential network with full EV adoption and smart charging showcases the dashboard's ability to identify anomalies like transformer overloads and charging failures. The system enables quick detection and contextual explanation of system-level events that are difficult to detect through traditional post-processing methods. The dashboard provides actionable insights for researchers and distribution system operators, and its adaptable architecture can be used for analyzing other complex energy systems and distributed energy resources. 

<br /><br />Summary: <div>
arXiv:2506.20400v1 Announce Type: cross 
Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging ecosystems generate large, complex, and stochastic time-series datasets that capture interactions between households, grid infrastructure, and energy markets. These interactions can lead to unexpected system-level events, such as transformer overloads or consumer dissatisfaction, that are difficult to detect and explain through static post-processing. This paper presents a modular, Python-based dashboard framework, built using Dash by Plotly, that enables efficient, multi-level exploration and root-cause analysis of emergent behavior in MABS outputs. The system features three coordinated views (System Overview, System Analysis, and Consumer Analysis), each offering high-resolution visualizations such as time-series plots, spatial heatmaps, and agent-specific drill-down tools. A case study simulating full EV adoption with smart charging in a Danish residential network demonstrates how the dashboard supports rapid identification and contextual explanation of anomalies, including clustered transformer overloads and time-dependent charging failures. The framework facilitates actionable insight generation for researchers and distribution system operators, and its architecture is adaptable to other distributed energy resources and complex energy systems.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which Company Adjustment Matter? Insights from Uplift Modeling on Financial Health</title>
<link>https://arxiv.org/abs/2506.19049</link>
<guid>https://arxiv.org/abs/2506.19049</guid>
<content:encoded><![CDATA[
<div> Keywords: uplift modeling, company adjustment, financial status, time-dependent actions, MTDnet

Summary:
Uplift modeling, using machine learning and deep learning, analyzes the impact of company adjustments on financial status by treating adjustments as interventions. Unlike simpler scenarios, company adjustments involve a series of time-dependent actions, necessitating the consideration of both individual treatment traits and temporal order. Real-world data from Luxembourg is used for experiments, comparing two meta-learners and three existing uplift models with a new framework called MTDnet. Results highlight the importance of timing in estimating the effects of company adjustments, demonstrating the need for models that can account for the time-dependent nature of these interventions.<br /><br />Summary: <div>
arXiv:2506.19049v1 Announce Type: new 
Abstract: Uplift modeling has achieved significant success in various fields, particularly in online marketing. It is a method that primarily utilizes machine learning and deep learning to estimate individual treatment effects. This paper we apply uplift modeling to analyze the effect of company adjustment on their financial status, and we treat these adjustment as treatments or interventions in this study. Although there have been extensive studies and application regarding binary treatments, multiple treatments, and continuous treatments, company adjustment are often more complex than these scenarios, as they constitute a series of multiple time-dependent actions. The effect estimation of company adjustment needs to take into account not only individual treatment traits but also the temporal order of this series of treatments. This study collects a real-world data set about company financial statements and reported behavior in Luxembourg for the experiments. First, we use two meta-learners and three other well-known uplift models to analyze different company adjustment by simplifying the adjustment as binary treatments. Furthermore, we propose a new uplift modeling framework (MTDnet) to address the time-dependent nature of these adjustment, and the experimental result shows the necessity of considering the timing of these adjustment.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LKA: Large Kernel Adapter for Enhanced Medical Image Classification</title>
<link>https://arxiv.org/abs/2506.19118</link>
<guid>https://arxiv.org/abs/2506.19118</guid>
<content:encoded><![CDATA[
<div> Keyword: Parameter-Efficient Fine-Tuning, Medical Image Analysis, Large Kernel Adapter, Receptive Field, State-of-the-art <br />
Summary:
A new approach called the Large Kernel Adapter (LKA) is introduced to improve Parameter-Efficient Fine-Tuning (PEFT) methods for medical image analysis. The challenge in medical datasets lies in the anatomical variation and low contrast, necessitating larger receptive fields. The LKA comprises down-projection, channel-wise large kernel convolution, and up-projection to enhance adaptation. It outperforms 11 existing PEFT methods and achieves a 3.5% higher top-1 accuracy across five medical datasets, surpassing the state-of-the-art. The key factors tackled include the need for larger receptive fields in medical images and the lack of explicit enhancement in existing PEFT methods. The incorporation of a larger kernel size proves pivotal in improving pre-trained model adaptation for medical image analysis. The proposed LKA demonstrates significant advancements in parameter efficiency and performance metrics for medical image datasets. <br /><br />Summary: <div>
arXiv:2506.19118v1 Announce Type: new 
Abstract: Despite the notable success of current Parameter-Efficient Fine-Tuning (PEFT) methods across various domains, their effectiveness on medical datasets falls short of expectations. This limitation arises from two key factors: (1) medical images exhibit extensive anatomical variation and low contrast, necessitating a large receptive field to capture critical features, and (2) existing PEFT methods do not explicitly address the enhancement of receptive fields. To overcome these challenges, we propose the Large Kernel Adapter (LKA), designed to expand the receptive field while maintaining parameter efficiency. The proposed LKA consists of three key components: down-projection, channel-wise large kernel convolution, and up-projection. Through extensive experiments on various datasets and pre-trained models, we demonstrate that the incorporation of a larger kernel size is pivotal in enhancing the adaptation of pre-trained models for medical image analysis. Our proposed LKA outperforms 11 commonly used PEFT methods, surpassing the state-of-the-art by 3.5% in top-1 accuracy across five medical datasets.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Industrial Gas Turbines: Recent Trends, Advancements and Challenges</title>
<link>https://arxiv.org/abs/2506.19503</link>
<guid>https://arxiv.org/abs/2506.19503</guid>
<content:encoded><![CDATA[
<div> PINNs, Industrial Gas Turbines, aerodynamics, aeromechanical, deep learning<br />
<br />
Summary: Physics-Informed Neural Networks (PINNs) are a computational framework combining deep learning with physical constraints to solve differential equations. This survey specifically focuses on their application in Industrial Gas Turbines (IGTs). PINNs have shown promise in analyzing aerodynamic and aeromechanical phenomena in gas turbines, as well as in tasks such as flow field reconstruction, fatigue evaluation, and flutter prediction. Recent advancements in accuracy, computational efficiency, and hybrid modelling strategies have been reviewed. The survey also discusses challenges in implementation and future research directions aimed at enhancing the robustness and scalability of PINNs in the context of IGTs. <div>
arXiv:2506.19503v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising computational framework for solving differential equations by integrating deep learning with physical constraints. However, their application in gas turbines is still in its early stages, requiring further refinement and standardization for wider adoption. This survey provides a comprehensive review of PINNs in Industrial Gas Turbines (IGTs) research, highlighting their contributions to the analysis of aerodynamic and aeromechanical phenomena, as well as their applications in flow field reconstruction, fatigue evaluation, and flutter prediction, and reviews recent advancements in accuracy, computational efficiency, and hybrid modelling strategies. In addition, it explores key research efforts, implementation challenges, and future directions aimed at improving the robustness and scalability of PINNs.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spline-Based Stress Function Approach for the Principle of Minimum Complementary Energy</title>
<link>https://arxiv.org/abs/2506.19534</link>
<guid>https://arxiv.org/abs/2506.19534</guid>
<content:encoded><![CDATA[
<div> spline-based stress function, computational engineering, finite element methods, stress prediction, structural analysis <br />
Summary:<br />
This article introduces a novel numerical approach for accurate stress prediction in computational engineering applications. The current methods have limitations in predicting stress accurately and efficiently for complex geometries and boundary conditions due to the high number of unknowns required. The proposed method is based on a spline-based stress function formulation for the principle of minimum complementary energy, applied to plane, linear elastostatics. It is validated against analytical solutions and tested on challenging test cases showing promising results. The method offers improved flexibility, accuracy, and efficiency in stress prediction for various geometries and boundary conditions. It achieves comparable stress accuracy to displacement-based finite element methods while requiring fewer degrees of freedom, making it a valuable tool for structural analysis and numerical design.<br /> <div>
arXiv:2506.19534v1 Announce Type: new 
Abstract: In computational engineering, ensuring the integrity and safety of structures in fields such as aerospace and civil engineering relies on accurate stress prediction. However, analytical methods are limited to simple test cases, and displacement-based finite element methods (FEMs), while commonly used, require a large number of unknowns to achieve high accuracy; stress-based numerical methods have so far failed to provide a simple and effective alternative. This work aims to develop a novel numerical approach that overcomes these limitations by enabling accurate stress prediction with improved flexibility for complex geometries and boundary conditions and fewer degrees of freedom (DOFs). The proposed method is based on a spline-based stress function formulation for the principle of minimum complementary energy, which we apply to plane, linear elastostatics. The method is first validated against an analytical power series solution and then tested on two test cases challenging for current state-of-the-art numerical schemes, a bi-layer cantilever with anisotropic material behavior, and a cantilever with a non-prismatic, parabolic-shaped beam geometry. Results demonstrate that our approach, unlike analytical methods, can be easily applied to general geometries and boundary conditions, and achieves stress accuracy comparable to that reported in the literature for displacement-based FEMs, while requiring significantly fewer DOFs. This novel spline-based stress function approach thus provides an efficient and flexible tool for accurate stress prediction, with promising applications in structural analysis and numerical design.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis</title>
<link>https://arxiv.org/abs/2506.19610</link>
<guid>https://arxiv.org/abs/2506.19610</guid>
<content:encoded><![CDATA[
<div> localization, multimodal techniques, medical visual question answering, vision to text chain-of-thought, reasoning pathways

Summary:
From Vision to Text Chain-of-Thought (V2T-CoT) is a new method for Medical Visual Question Answering (Med-VQA) that focuses on localizing disease-specific regions in biomedical images for more accurate diagnosis. V2T-CoT automates this localization process and integrates it into the reasoning pathway, balancing answer accuracy and clinical decision-making. By fine-tuning on the R-Med 39K dataset, V2T-CoT provides precise medical reasoning paths, combining visual grounding with textual rationale generation for explainable diagnostic results. Experimental results across four Med-VQA benchmarks show that V2T-CoT achieves state-of-the-art performance, improving both performance and interpretability significantly. <div>
arXiv:2506.19610v1 Announce Type: new 
Abstract: Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), a novel approach that automates the localization of preference areas within biomedical images and incorporates this localization into region-level pixel attention as knowledge for Vision CoT. By fine-tuning the vision language model on constructed R-Med 39K dataset, V2T-CoT provides definitive medical reasoning paths. V2T-CoT integrates visual grounding with textual rationale generation to establish precise and explainable diagnostic results. Experimental results across four Med-VQA benchmarks demonstrate state-of-the-art performance, achieving substantial improvements in both performance and interpretability.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReLink: Computational Circular Design of Planar Linkage Mechanisms Using Available Standard Parts</title>
<link>https://arxiv.org/abs/2506.19657</link>
<guid>https://arxiv.org/abs/2506.19657</guid>
<content:encoded><![CDATA[
<div> Circular Economy, sustainability, planar linkage mechanisms, generative design algorithm, CO2 footprint

Summary:
ReLink is a computational framework focusing on circular design for planar linkage mechanisms, promoting sustainability by reusing standardized parts. It consists of design generation and inverse design components, aiming to minimize the need for new parts and prioritize reuse. Trade-offs between kinematic performance and CO2 footprint are analyzed when incorporating new parts. The framework addresses challenges like the combinatorial nature of the design problem and ensuring valid solutions. By combining sustainability principles with kinematic synthesis, ReLink lays the foundation for further research in computational circular design, supporting the integration of reused components into mechanical products.<br /><br />Summary: <div>
arXiv:2506.19657v1 Announce Type: new 
Abstract: The Circular Economy framework emphasizes sustainability by reducing resource consumption and waste through the reuse of components and materials. This paper presents ReLink, a computational framework for the circular design of planar linkage mechanisms using available standard parts. Unlike most mechanism design methods, which assume the ability to create custom parts and infinite part availability, ReLink prioritizes the reuse of discrete, standardized components, thus minimizing the need for new parts. The framework consists of two main components: design generation, where a generative design algorithm generates mechanisms from an inventory of available parts, and inverse design, which uses optimization methods to identify designs that match a user-defined trajectory curve. The paper also examines the trade-offs between kinematic performance and CO2 footprint when incorporating new parts. Challenges such as the combinatorial nature of the design problem and the enforcement of valid solutions are addressed. By combining sustainability principles with kinematic synthesis, ReLink lays the groundwork for further research into computational circular design to support the development of systems that integrate reused components into mechanical products.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modular and extensible library for parameterized terrain generation</title>
<link>https://arxiv.org/abs/2506.19751</link>
<guid>https://arxiv.org/abs/2506.19751</guid>
<content:encoded><![CDATA[
<div> terrain generation, procedural, Python, simulation-driven, reproducibility <br />
<br />
The article introduces a modular Python library for procedural terrain generation, focusing on controllable and well-defined artificial terrains for simulation-driven development of intelligent machines. The library allows users to create complex, parameterized terrains by combining simple modules, supporting both structured and noise-based terrain elements. It integrates with Blender for rendering and object placement, making it suitable for applications like training machine learning models or perception tasks. The system prioritizes reproducibility, variation, and easy integration with automated pipelines by offering fine-grained control over terrain features like slope, roughness, and object placement. Overall, the library's minimal yet extensible set of modules enables users to achieve high flexibility in terrain generation while maintaining simplicity in configuration and expansion.<br /><br />Summary: <div>
arXiv:2506.19751v1 Announce Type: new 
Abstract: Simulation-driven development of intelligent machines benefits from artificial terrains with controllable, well-defined characteristics. However, most existing tools for terrain generation focus on artist-driven workflows and visual realism, with limited support for parameterization, reproducibility, or scripting. We present a modular, Python-based library for procedural terrain generation that enables users to construct complex, parameterized terrains by chaining together simple modules. The system supports both structured and noise-based terrain elements, and integrates with Blender for rendering and object placement. The framework is designed to support applications such as generating synthetic terrains for training machine learning models or producing ground truth for perception tasks. By using a minimal but extensible set of modules, the system achieves high flexibility while remaining easy to configure and expand. We demonstrate that this enables fine-grained control over features such as slope, roughness, and the number of rocks, as well as extension to additional measures. This makes it well suited for workflows that demand reproducibility, variation, and integration with automated pipelines.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate identification of communication between multiple interacting neural populations</title>
<link>https://arxiv.org/abs/2506.19094</link>
<guid>https://arxiv.org/abs/2506.19094</guid>
<content:encoded><![CDATA[
<div> variational autoencoder, neural recording, brain regions, inter-regional communication, dynamical systems
Summary:
Variational autoencoder MR-LFADS is introduced for disentangling communication between brain regions, unobserved inputs, and local neural dynamics. It outperforms existing models in identifying inter-regional communication in simulations and predicts brain-wide effects of circuit perturbations in electrophysiology data. MR-LFADS shows promise in uncovering brain-wide information processing principles. <div>
arXiv:2506.19094v1 Announce Type: cross 
Abstract: Neural recording technologies now enable simultaneous recording of population activity across many brain regions, motivating the development of data-driven models of communication between brain regions. However, existing models can struggle to disentangle the sources that influence recorded neural populations, leading to inaccurate portraits of inter-regional communication. Here, we introduce Multi-Region Latent Factor Analysis via Dynamical Systems (MR-LFADS), a sequential variational autoencoder designed to disentangle inter-regional communication, inputs from unobserved regions, and local neural population dynamics. We show that MR-LFADS outperforms existing approaches at identifying communication across dozens of simulations of task-trained multi-region networks. When applied to large-scale electrophysiology, MR-LFADS predicts brain-wide effects of circuit perturbations that were held out during model fitting. These validations on synthetic and real neural data position MR-LFADS as a promising tool for discovering principles of brain-wide information processing.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research</title>
<link>https://arxiv.org/abs/2412.02065</link>
<guid>https://arxiv.org/abs/2412.02065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data access, democratization, data collection, corporate disclosures

Summary: 
Large Language Models (LLMs) have the potential to democratize access to costly datasets essential for research by automating data collection from corporate disclosures. A novel methodology using GPT-4o-mini in a Retrieval-Augmented Generation (RAG) framework successfully collected CEO pay ratios from 10,000 proxy statements and Critical Audit Matters (CAMs) from over 12,000 10-K filings with human-level accuracy. The LLM processing times were notably efficient at 9 and 40 minutes respectively, with minimal costs under $10. This approach contrasts significantly with manual collection methods requiring hundreds of hours or expensive commercial database subscriptions. By sharing their methodology and resulting datasets, the researchers aim to empower those with limited resources to engage in research and foster a more inclusive research community. 

<br /><br />Summary: <div>
arXiv:2412.02065v2 Announce Type: replace-cross 
Abstract: Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear optimals and their role in sustaining turbulence in channel flow</title>
<link>https://arxiv.org/abs/2503.08283</link>
<guid>https://arxiv.org/abs/2503.08283</guid>
<content:encoded><![CDATA[
<div> investigation, energy transfer, channel flow, optimal disturbances, turbulence

Summary:
The study focuses on the energy transfer in channel flow by analyzing nonlinear optimal disturbances that maximize energy growth over a fixed time period. Results indicate that these disturbances exhibit streak spacing and amplitude consistent with Direct Numerical Simulation (DNS) data at Re_tau = 180, suggesting they capture key mechanisms sustaining turbulence. Additionally, the time horizon for nonlinear disturbances to surpass linear optimal disturbances aligns with estimates based on eddy turnover time from previous DNS studies. This finding sheds light on the determination of turbulent time scales and highlights the potential of nonlinear disturbances in understanding turbulence dynamics. Overall, the research provides valuable insights into the energy dynamics and mechanisms underlying turbulence in channel flow. 

<br /><br />Summary: <div>
arXiv:2503.08283v2 Announce Type: replace-cross 
Abstract: We investigate the energy transfer from the mean profile to velocity fluctuations in channel flow by calculating nonlinear optimal disturbances,i.e. the initial condition of a given finite energy that achieves the highest possible energy growth during a given fixed time horizon. It is found that for a large range of time horizons and initial disturbance energies, the nonlinear optimal exhibits streak spacing and amplitude consistent with DNS at least at Re_tau = 180, which suggests that they isolate the relevant physical mechanisms that sustain turbulence. Moreover, the time horizon necessary for a nonlinear disturbance to outperform a linear optimal is consistent with previous DNS-based estimates using eddy turnover time, which offers a new perspective on how some turbulent time scales are determined.
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Equilibrium and Kinetics Prediction with a Data-Weighted Neural Network Model of Methane Steam Reforming</title>
<link>https://arxiv.org/abs/2506.17224</link>
<guid>https://arxiv.org/abs/2506.17224</guid>
<content:encoded><![CDATA[
<div> neural network, methane steam reforming, process optimization, surrogate model, energy carrier  
Summary:  
- A surrogate model was developed using an artificial neural network to simulate methane steam reforming for hydrogen production.  
- The model successfully unified kinetic and equilibrium regimes by training on a comprehensive dataset including experimental, interpolated, and theoretical data.  
- Data augmentation and weighted training improved the model's accuracy in predicting the composition of post-reaction mixtures.  
- The surrogate model demonstrated a mean squared error of 0.000498 and strong Pearson correlation coefficients of 0.927, indicating high predictive accuracy.  
- The model's capability to provide continuous derivatives of its predictions makes it useful for process modeling and optimization.  
- Overall, the surrogate model proves robust for simulating methane steam reforming, offering a valuable tool for design and process optimization.  

<br /><br />Summary: <div>
arXiv:2506.17224v1 Announce Type: new 
Abstract: Hydrogen's role is growing as an energy carrier, increasing the need for efficient production, with methane steam reforming being the most widely used technique. This process is crucial for applications like fuel cells, where hydrogen is converted into electricity, pushing for reactor miniaturization and optimized process control through numerical simulations. Existing models typically address either kinetic or equilibrium regimes, limiting their applicability. Here we show a surrogate model capable of unifying both regimes. An artificial neural network trained on a comprehensive dataset that includes experimental data from kinetic and equilibrium experiments, interpolated data, and theoretical data derived from theoretical models for each regime. Data augmentation and assigning appropriate weights to each data type enhanced training. After evaluating Bayesian Optimization and Random Sampling, the optimal model demonstrated high predictive accuracy for the composition of the post-reaction mixture under varying operating parameters, indicated by a mean squared error of 0.000498 and strong Pearson correlation coefficients of 0.927. The network's ability to provide continuous derivatives of its predictions makes it particularly useful for process modeling and optimization. The results confirm the surrogate model's robustness for simulating methane steam reforming in both kinetic and equilibrium regimes, making it a valuable tool for design and process optimization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Quantum Latent Encoding for Topology Optimization</title>
<link>https://arxiv.org/abs/2506.17487</link>
<guid>https://arxiv.org/abs/2506.17487</guid>
<content:encoded><![CDATA[
<div> neural decoding, quantum encoding, classical encoding, variational framework, topology optimization

Summary:<br />
The article introduces a variational framework for structural topology optimization that combines quantum and classical latent encoding strategies within a coordinate-based neural decoding architecture. A low-dimensional latent vector is generated through either a variational quantum circuit or Gaussian distribution and mapped to a higher-dimensional latent space via a learnable projection layer. The enriched representation is decoded into a high-resolution material distribution using a neural network that takes both the latent vector and spatial coordinates as input. Optimization is performed on the latent parameters guided by physics-based objectives such as compliance minimization and volume constraints evaluated through finite element analysis. Quantum latent vectors constructed from measured Pauli observables provide an entangled encoding. The variational formulation enables the generation of diverse and valid topologies by exploring the latent space through sampling or perturbation. Numerical experiments show that both quantum and classical encodings produce high-quality structural designs, with quantum encoding showing advantages in compliance and design diversity. This suggests the potential of quantum circuits in physics-constrained topology optimization using near-term quantum hardware.<br /> <div>
arXiv:2506.17487v1 Announce Type: new 
Abstract: A variational framework for structural topology optimization is developed, integrating quantum and classical latent encoding strategies within a coordinate-based neural decoding architecture. In this approach, a low-dimensional latent vector, generated either by a variational quantum circuit or sampled from a Gaussian distribution, is mapped to a higher-dimensional latent space via a learnable projection layer. This enriched representation is then decoded into a high-resolution material distribution using a neural network that takes both the latent vector and Fourier-mapped spatial coordinates as input. The optimization is performed directly on the latent parameters, guided solely by physics-based objectives such as compliance minimization and volume constraints evaluated through finite element analysis, without requiring any precomputed datasets or supervised training. Quantum latent vectors are constructed from the expectation values of Pauli observables measured on parameterized quantum circuits, providing a structured and entangled encoding of information. The classical baseline uses Gaussian-sampled latent vectors projected in the same manner. The proposed variational formulation enables the generation of diverse and physically valid topologies by exploring the latent space through sampling or perturbation, in contrast to traditional optimization methods that yield a single deterministic solution. Numerical experiments show that both classical and quantum encodings produce high-quality structural designs. However, quantum encodings demonstrate advantages in several benchmark cases in terms of compliance and design diversity. These results highlight the potential of quantum circuits as an effective and scalable tool for physics-constrained topology optimization and suggest promising directions for applying near-term quantum hardware in structural design.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A predictor-corrector scheme for approximating signed distances using finite element methods</title>
<link>https://arxiv.org/abs/2506.17830</link>
<guid>https://arxiv.org/abs/2506.17830</guid>
<content:encoded><![CDATA[
<div> prediction-correction approach, signed distance functions, finite element method, Eikonal equation, interfaces

Summary:
The article introduces a finite element method for computing approximate signed distance functions to arbitrary boundaries in 2D and 3D. The method utilizes a prediction-correction approach involving a linear diffusion-based prediction step and a nonlinear minimization-based correction step related to the Eikonal equation. This approach efficiently generates an initial guess for complex level set functions and facilitates convergence. The method can handle complex interfaces and level set functions with steep or flat regions, overcoming challenges faced by existing techniques. Through various examples, including classical geometries, star domains, and 3D tori, the method demonstrates accuracy, efficiency, and robustness in reinitializing diverse level set functions. <div>
arXiv:2506.17830v1 Announce Type: new 
Abstract: In this article, we introduce a finite element method designed for the robust computation of approximate signed distance functions to arbitrary boundaries in two and three dimensions. Our method employs a novel prediction-correction approach, involving first the solution of a linear diffusion-based prediction problem, followed by a nonlinear minimization-based correction problem associated with the Eikonal equation. The prediction step efficiently generates a suitable initial guess, significantly facilitating convergence of the nonlinear correction step. A key strength of our approach is its ability to handle complex interfaces and initial level set functions with arbitrary steep or flat regions, a notable challenge for existing techniques. Through several representative examples, including classical geometries and more complex shapes such as star domains and three-dimensional tori, we demonstrate the accuracy, efficiency, and robustness of the method, validating its broad applicability for reinitializing diverse level set functions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from the Storm: A Multivariate Machine Learning Approach to Predicting Hurricane-Induced Economic Losses</title>
<link>https://arxiv.org/abs/2506.17964</link>
<guid>https://arxiv.org/abs/2506.17964</guid>
<content:encoded><![CDATA[
<div> Keywords: hurricanes, economic loss, modeling framework, machine learning, insurance claims 

Summary:
Hurricanes in Florida result in substantial economic losses, prompting the need for a comprehensive framework to assess contributing factors. This study introduces a modeling framework categorizing factors into hurricane characteristics, water-related environmental factors, and socioeconomic factors at the ZIP Code Tabulation Area level. By utilizing machine learning models and insurance claims as indicators, the approach accurately predicts economic loss and evaluates the importance of each component. The findings offer valuable insights for disaster mitigation, risk assessment, and urban strategies in storm-exposed areas. The code for this research is now publicly available to facilitate further study and application in disaster management.  <br /><br />Summary: <div>
arXiv:2506.17964v1 Announce Type: new 
Abstract: Florida is particularly vulnerable to hurricanes, which frequently cause substantial economic losses. While prior studies have explored specific contributors to hurricane-induced damage, few have developed a unified framework capable of integrating a broader range of influencing factors to comprehensively assess the sources of economic loss. In this study, we propose a comprehensive modeling framework that categorizes contributing factors into three key components: (1) hurricane characteristics, (2) water-related environmental factors, and (3) socioeconomic factors of affected areas. By integrating multi-source data and aggregating all variables at the finer spatial granularity of the ZIP Code Tabulation Area (ZCTA) level, we employ machine learning models to predict economic loss, using insurance claims as indicators of incurred damage. Beyond accurate loss prediction, our approach facilitates a systematic assessment of the relative importance of each component, providing practical guidance for disaster mitigation, risk assessment, and the development of adaptive urban strategies in coastal and storm-exposed areas. Our code is now available at: https://github.com/LabRAI/Hurricane-Induced-Economic-Loss-Prediction
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A phase field model for hydraulic fracture: Drucker-Prager driving force and a hybrid coupling strategy</title>
<link>https://arxiv.org/abs/2506.18161</link>
<guid>https://arxiv.org/abs/2506.18161</guid>
<content:encoded><![CDATA[
<div> phase field approach, hydraulic fracture, coupling, Drucker-Prager, simulation

Summary: 
This study introduces a novel phase field framework for simulating hydraulic fracture, with a focus on improving the coupling between fluid flow and the phase field and incorporating a universal fracture driving force. Two key innovations are presented: a hybrid coupling approach for enhanced accuracy and flexibility in handling the fracture-fluid flow interplay, and a Drucker-Prager-based strain energy decomposition to model materials with asymmetric tension-compression fracture behavior. The framework is applied to four case studies to demonstrate its capabilities in addressing permeability coupling, cracking behavior, and multiaxial conditions in hydraulic fracturing simulations. The developed codes are freely available for download, offering valuable resources to the community for further research in this field. <div>
arXiv:2506.18161v1 Announce Type: new 
Abstract: Recent years have seen a significant interest in using phase field approaches to model hydraulic fracture, so as to optimise a process that is key to industries such as petroleum engineering, mining and geothermal energy extraction. Here, we present a novel theoretical and computational phase field framework to simulate hydraulic fracture. The framework is general and versatile, in that it allows for improved treatments of the coupling between fluid flow and the phase field, and encompasses a universal description of the fracture driving force. Among others, this allows us to bring two innovations to the phase field hydraulic fracture community: (i) a new hybrid coupling approach to handle the fracture-fluid flow interplay, offering enhanced accuracy and flexibility; and (ii) a Drucker-Prager-based strain energy decomposition, extending the simulation of hydraulic fracture to materials exhibiting asymmetric tension-compression fracture behaviour (such as shale rocks) and enabling the prediction of geomechanical phenomena such as fault reactivation and stick-slip behaviour. Four case studies are addressed to illustrate these additional modelling capabilities and bring insight into permeability coupling, cracking behaviour, and multiaxial conditions in hydraulic fracturing simulations. The codes developed are made freely available to the community and can be downloaded from {https://mechmat.web.ox.ac.uk/
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Fractal Dimension using Discrete Global Grid Systems</title>
<link>https://arxiv.org/abs/2506.18175</link>
<guid>https://arxiv.org/abs/2506.18175</guid>
<content:encoded><![CDATA[
<div> fractal dimension, Discrete Global Grid System, Minkowski-Bouligand dimension, geospatial vector data, satellite images <br />
Summary: This study explores the relationship between fractal dimension and Discrete Global Grid Systems (DGGS), used for geospatial vector data analysis. By applying the method to synthetic data and opaque cloud fields from satellite images, the results closely match theoretical fractal dimensions. The use of DGGSs addresses issues such as arbitrary grid placement, orientation, and cell size progression in geospatial data analysis. DGGSs also account for the curvature of the Earth when calculating intersections with large geographic extents. The paper validates the use of DGGSs as covering sets and highlights desirable properties for effective application in geospatial analysis. <br /> <div>
arXiv:2506.18175v1 Announce Type: new 
Abstract: This study builds a bridge between two well-studied but distant topics: fractal dimension and Discrete Global Grid System (DGGS). DGGSs are used as covering sets for geospatial vector data to calculate the Minkowski-Bouligand dimension. Using the method on synthetic data yields results within 1% of their theoretical fractal dimensions. A case study on opaque cloud fields obtained from satellite images gives fractal dimension in agreement with that available in the literature. The proposed method alleviates the problems of arbitrary grid placement and orientation, as well as the progression of cell sizes of the covering sets for geospatial data. Using DGGSs further ensure that intersections of the covering sets with the geospatial vector having large geographic extents are calculated by taking the curvature of the earth into account. This paper establishes the validity of DGGSs as covering sets theoretically and discusses desirable properties of DGGSs suitable for this purpose.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conservative data-driven finite element formulation</title>
<link>https://arxiv.org/abs/2506.18206</link>
<guid>https://arxiv.org/abs/2506.18206</guid>
<content:encoded><![CDATA[
<div> finite element framework, mixed finite element formulation, data-driven approach, conservation law, numerical simulations<br />
<br />
Summary:<br />
This paper introduces a data-driven finite element framework using a mixed finite element formulation for diffusion problems. Unlike traditional methods that rely on fitting experimental data to material models, this approach directly utilizes experimental data in the simulations without the need for parameter fitting. By satisfying conservation laws through the finite element method and enforcing continuity of flux components, a mixed formulation is introduced to handle uncertainties in datasets and predict solution non-uniqueness. Error indicators tailored for data-driven approaches allow for adaptive refinement. An example of nonlinear heat transfer in nuclear graphite demonstrates the formulation's capabilities using synthetically generated datasets to showcase its effectiveness in predicting uncertainties and providing accurate results. <div>
arXiv:2506.18206v1 Announce Type: new 
Abstract: This paper presents a new data-driven finite element framework derived with mixed finite element formulation. The standard approach to diffusion problems requires the solution of the mathematical equations that describe both the conservation law and the constitutive relations, where the latter is traditionally obtained after fitting experimental data to simplified material models. To exploit all available information and avoid bias in the material model, we follow a data-driven approach. While the conservation laws and boundary conditions are satisfied by means of the finite element method, the experimental data is used directly in the numerical simulations, avoiding the need of fitting material model parameters. In order to satisfy the conservation law a priori in the strong sense, we introduce a mixed finite element formulation. This relaxes the regularity requirements on approximation spaces while enforcing continuity of the normal flux component across all of the inner boundaries. This weaker mixed formulation provides a posteriori error indicators tailored for this data-driven approach, enabling adaptive hp-refinement. The relaxed regularity of the approximation spaces makes it easier to observe how the variation in the datasets results in the non-uniqueness of the solution, which can be quantified to predict the uncertainty of the results. The capabilities of the formulation are demonstrated in an example of the nonlinear heat transfer in nuclear graphite using synthetically generated material datasets.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Conditional Score-Guided Generative Modeling for Amortized Inference in Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2506.18227</link>
<guid>https://arxiv.org/abs/2506.18227</guid>
<content:encoded><![CDATA[
<div> conditional inference, amortized, diffusion models, generative model, neural network

Summary:
The article introduces a framework for efficient conditional inference by utilizing exact conditional score-guided diffusion models to train a non-reversible neural network as a generative model. Traditional normalizing flow methods typically require reversible architectures, limiting their expressiveness and efficiency. Leveraging a two-stage method, the authors first create a training-free conditional diffusion model with an exact score function derived under a Gaussian mixture prior. This allows for the generation of noise-labeled data efficiently. This data is then used to train a feedforward neural network that directly maps noise and observations to posterior samples, eliminating the need for reversibility or iterative sampling during inference. The resulting model offers fast, accurate, and scalable conditional sampling for high-dimensional and multi-modal posterior distributions, making it suitable for uncertainty quantification tasks such as parameter estimation for complex physical systems. The effectiveness of the approach is demonstrated through numerical experiments. 

<br /><br />Summary: <div>
arXiv:2506.18227v1 Announce Type: new 
Abstract: We propose an efficient framework for amortized conditional inference by leveraging exact conditional score-guided diffusion models to train a non-reversible neural network as a conditional generative model. Traditional normalizing flow methods require reversible architectures, which can limit their expressiveness and efficiency. Although diffusion models offer greater flexibility, they often suffer from high computational costs during inference. To combine the strengths of both approaches, we introduce a two-stage method. First, we construct a training-free conditional diffusion model by analytically deriving an exact score function under a Gaussian mixture prior formed from samples of the underlying joint distribution. This exact conditional score model allows us to efficiently generate noise-labeled data, consisting of initial diffusion Gaussian noise and posterior samples conditioned on various observation values, by solving a reverse-time ordinary differential equation. Second, we use this noise-labeled data to train a feedforward neural network that maps noise and observations directly to posterior samples, eliminating the need for reversibility or iterative sampling at inference time. The resulting model provides fast, accurate, and scalable conditional sampling for high-dimensional and multi-modal posterior distributions, making it well-suited for uncertainty quantification tasks, e.g., parameter estimation of complex physical systems. We demonstrate the effectiveness of our approach through a series of numerical experiments.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural-operator element method: Efficient and scalable finite element method enabled by reusable neural operators</title>
<link>https://arxiv.org/abs/2506.18427</link>
<guid>https://arxiv.org/abs/2506.18427</guid>
<content:encoded><![CDATA[
<div> neural-operator element method, finite element method, machine learning, PDEs, multiscale simulations 
Summary: 
The neural-operator element method (NOEM) combines the finite element method (FEM) with neural operators to efficiently solve partial differential equations (PDEs) without the need for dense meshing. By using neural operators to simulate subdomains where FEM would require many elements, NOEM reduces computational costs. Each subdomain is represented by a neural-operator element (NOE), which is integrated with standard finite elements to achieve accurate and scalable solutions. This approach addresses the challenges of high training costs and low model reusability associated with machine learning-based methods for PDEs. Extensive numerical experiments demonstrate the accuracy, efficiency, and scalability of NOEM across various scenarios, including nonlinear PDEs, multiscale problems, complex geometries, and discontinuous coefficient fields. <br /><br />Summary: <div>
arXiv:2506.18427v1 Announce Type: new 
Abstract: The finite element method (FEM) is a well-established numerical method for solving partial differential equations (PDEs). However, its mesh-based nature gives rise to substantial computational costs, especially for complex multiscale simulations. Emerging machine learning-based methods (e.g., neural operators) provide data-driven solutions to PDEs, yet they present challenges, including high training cost and low model reusability. Here, we propose the neural-operator element method (NOEM) by synergistically combining FEM with operator learning to address these challenges. NOEM leverages neural operators (NOs) to simulate subdomains where a large number of finite elements would be required if FEM was used. In each subdomain, an NO is used to build a single element, namely a neural-operator element (NOE). NOEs are then integrated with standard finite elements to represent the entire solution through the variational framework. Thereby, NOEM does not necessitate dense meshing and offers efficient simulations. We demonstrate the accuracy, efficiency, and scalability of NOEM by performing extensive and systematic numerical experiments, including nonlinear PDEs, multiscale problems, PDEs on complex geometries, and discontinuous coefficient fields.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual failure assessment diagrams for hydrogen transmission pipelines</title>
<link>https://arxiv.org/abs/2506.18554</link>
<guid>https://arxiv.org/abs/2506.18554</guid>
<content:encoded><![CDATA[
<div> Thermo-metallurgical welding, diffusion-elastic-plastic phase field fracture, hydrogen transport pipelines, residual stress, Virtual Failure Assessment Diagrams <br />
Summary: <br />
The study combines advanced welding process modeling with fracture simulations to predict failure states in hydrogen transport pipelines. By considering factors such as residual stress, material properties, and hydrogen purity, failure pressures can be efficiently quantified. Virtual Failure Assessment Diagrams are created to facilitate the assessment of asset fitness under various conditions. The model's predictions align well with industry standards but highlight the need for a more mechanistic approach to account for the heterogeneous weld microstructure. Safety factors are established to address residual stresses and brittle weld regions, ensuring a more accurate assessment of pipeline integrity and safety. <div>
arXiv:2506.18554v1 Announce Type: new 
Abstract: We combine state-of-the-art thermo-metallurgical welding process modelling with coupled diffusion-elastic-plastic phase field fracture simulations to predict the failure states of hydrogen transport pipelines. This enables quantitatively resolving residual stress states and the role of brittle, hard regions of the weld such as the heat affected zone (HAZ). Failure pressures can be efficiently quantified as a function of asset state (existing defects), materials and weld procedures adopted, and hydrogen purity. Importantly, simulations spanning numerous relevant conditions (defect size and orientations) are used to build \emph{Virtual} Failure Assessment Diagrams (FADs), enabling a straightforward uptake of this mechanistic approach in fitness-for-service assessment. Model predictions are in very good agreement with FAD approaches from the standards but show that the latter are not conservative when resolving the heterogeneous nature of the weld microstructure. Appropriate, \emph{mechanistic} FAD safety factors are established that account for the role of residual stresses and hard, brittle weld regions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Informed Neural Network Framework for Simulating Creep Buckling in Growing Viscoelastic Biological Tissues</title>
<link>https://arxiv.org/abs/2506.18565</link>
<guid>https://arxiv.org/abs/2506.18565</guid>
<content:encoded><![CDATA[
<div> framework, viscoelastic behavior, physics-informed neural network, stress relaxation, morphogenesis <br />
Summary: <br />
This study presents a novel energy-based physics-informed neural network (PINN) framework for modeling viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. The framework eliminates the need for explicit meshing or custom programs, simplifying computational complexity. By training neural networks to minimize potential energy functional, the framework ensures physics consistency, capturing creep buckling and predicting tissue growth with high accuracy. Results show the framework can predict viscoelastic instabilities, post-buckling evolution, and tissue morphological changes effectively. This versatile tool offers a promising alternative to traditional methods and can find applications in structural engineering, soft materials, and tissue development. <div>
arXiv:2506.18565v1 Announce Type: new 
Abstract: Modeling viscoelastic behavior is crucial in engineering and biomechanics, where materials undergo time-dependent deformations, including stress relaxation, creep buckling and biological tissue development. Traditional numerical methods, like the finite element method, often require explicit meshing, artificial perturbations or embedding customised programs to capture these phenomena, adding computational complexity. In this study, we develop an energy-based physics-informed neural network (PINN) framework using an incremental approach to model viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. Physics consistency is ensured by training neural networks to minimize the systems potential energy functional, implicitly satisfying equilibrium and constitutive laws. We demonstrate that this framework can naturally capture creep buckling without pre-imposed imperfections, leveraging inherent training dynamics to trigger instabilities. Furthermore, we extend our framework to biological tissue growth and morphogenesis, predicting both uniform expansion and differential growth-induced buckling in cylindrical structures. Results show that the energy-based PINN effectively predicts viscoelastic instabilities, post-buckling evolution and tissue morphological evolution, offering a promising alternative to traditional methods. This study demonstrates that PINN can be a flexible robust tool for modeling complex, time-dependent material behavior, opening possible applications in structural engineering, soft materials, and tissue development.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Communication Architecture for Autonomous Power-to-X Platforms: Enhancing Inspection and Operation With Legged Robots and 5G</title>
<link>https://arxiv.org/abs/2506.18572</link>
<guid>https://arxiv.org/abs/2506.18572</guid>
<content:encoded><![CDATA[
<div> Keywords: Power to X platforms, communication architecture, robotic system, inspection, 5G network

Summary: 
Power to X platforms are classified, and a communication architecture is proposed for monitoring, control, and teleoperation. A robotic system using a quadruped robot is integrated to autonomously perform inspection and maintenance tasks, reducing the need for human labor. The implementation considers a 5G standalone network for remote monitoring, control, and teleoperation of the robot. Evaluation includes recording and comparison of aspects such as availability and latency within this network. <div>
arXiv:2506.18572v1 Announce Type: new 
Abstract: Inspection and maintenance of offshore platforms are associated with high costs, primarily due to the significant personnel requirements and challenging operational conditions. This paper first presents a classification of Power to X platforms. Building upon this foundation, a communication architecture is proposed to enable monitoring, control, and teleoperation for a Power to X platform. To reduce the demand for human labor, a robotic system is integrated to autonomously perform inspection and maintenance tasks. The implementation utilizes a quadruped robot. Remote monitoring, control, and teleoperation of the robot are analyzed within the context of a 5G standalone network. As part of the evaluation, aspects such as availability and latency are recorded, compared, and critically assessed.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Dynamic Stock Relationship Modeling and S&amp;P500 Price Forecasting Based on Differential Graph Transformer</title>
<link>https://arxiv.org/abs/2506.18717</link>
<guid>https://arxiv.org/abs/2506.18717</guid>
<content:encoded><![CDATA[
<div> Keywords: Stock price prediction, Differential Graph Transformer, Dynamic relationship modeling, Temporal attention, Correlation metrics

Summary:
The study introduces the Differential Graph Transformer (DGT) framework for dynamic stock price prediction by capturing evolving stock relationships. DGT utilizes differential graph mechanisms and causal temporal attention to model global and local dependencies in price sequences. The framework incorporates correlation metrics (Pearson, Mutual Information, Spearman, Kendall's Tau) across different scopes as spatial-attention priors, improving prediction accuracy over traditional models. Using 10 years of S&amp;P 500 closing prices, DGT with spatial priors outperformed GRU baselines, with optimal results obtained using Kendall's Tau global matrices. Clustering analysis identified distinct stock clusters such as "high-volatility growth" and "defensive blue-chip," with each exhibiting varying prediction errors based on correlation stability. The study highlights the importance of dynamic modeling and optimal correlation metrics in enhancing financial time-series prediction and quantitative investment strategies.

Summary: <br />Keywords: Stock price prediction, Differential Graph Transformer, Dynamic relationship modeling, Temporal attention, Correlation metrics <div>
arXiv:2506.18717v1 Announce Type: new 
Abstract: Stock price prediction is vital for investment decisions and risk management, yet remains challenging due to markets' nonlinear dynamics and time-varying inter-stock correlations. Traditional static-correlation models fail to capture evolving stock relationships. To address this, we propose a Differential Graph Transformer (DGT) framework for dynamic relationship modeling and price prediction. Our DGT integrates sequential graph structure changes into multi-head self-attention via a differential graph mechanism, adaptively preserving high-value connections while suppressing noise. Causal temporal attention captures global/local dependencies in price sequences. We further evaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's Tau) across global/local/dual scopes as spatial-attention priors. Using 10 years of S&amp;P 500 closing prices (z-score normalized; 64-day sliding windows), DGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87). Kendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means clustering revealed "high-volatility growth" and "defensive blue-chip" stocks, with the latter showing lower errors (RMSE: 0.13) due to stable correlations. Kendall's Tau and Mutual Information excelled in volatile sectors. This study innovatively combines differential graph structures with Transformers, validating dynamic relationship modeling and identifying optimal correlation metrics/scopes. Clustering analysis supports tailored quantitative strategies. Our framework advances financial time-series prediction through dynamic modeling and cross-asset interaction analysis.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Real-time Structural Dynamics Simulation with Graph-based Digital Twin Modelling</title>
<link>https://arxiv.org/abs/2506.18724</link>
<guid>https://arxiv.org/abs/2506.18724</guid>
<content:encoded><![CDATA[
<div> Graph-Based Digital Twin Modelling, Structural Dynamics, Computational Efficiency, Health Monitoring, Spatial Topologies
<br />
Summary:
<br />
This study introduces a graph-based digital twin modelling (GDTM) framework for simulating structural dynamic behavior. The framework utilizes adjacency matrices to capture spatial relationships between structural elements, enhancing physical interpretability. Results from numerical and experimental validation show high accuracy in simulating dynamics across various topologies, with low Normalized Mean-Squared Error values. The framework significantly enhances computational efficiency, outperforming traditional finite element methods (FEM) by over 80-fold. By addressing challenges in data-driven methods, this research paves the way for practical applications in structural performance evaluation and health monitoring. <div>
arXiv:2506.18724v1 Announce Type: new 
Abstract: Precise and timely simulation of a structure's dynamic behavior is crucial for evaluating its performance and assessing its health status. Traditional numerical methods are often limited by high computational costs and low efficiency, while deep learning approaches offer a promising alternative. However, these data-driven methods still face challenges, such as limited physical interpretability and difficulty in adapting to diverse structural configurations. To address these issues, this study proposes a graph-based digital twin modelling (GDTM) framework to simulate structural dynamic responses across various spatial topologies. In this framework, the adjacency matrix explicitly represents the spatial relationships between structural vertices, enhancing the model's physical interpretability. The effectiveness of the proposed framework was validated through comprehensive numerical and experimental studies. The results demonstrate that the framework accurately simulated structural dynamics across different topological configurations, with Normalized Mean-Squared Error (NMSE) values consistently below 0.005 in numerical simulations and 0.0015 in experimental validations. Furthermore, the framework achieved over 80-fold improvements in computational efficiency compared to traditional finite element methods (FEM). This research promotes the practical application of graph-based structural dynamics modelling, which has the potential to significantly advance structural performance evaluation and health monitoring.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeletal Reaction Models for Gasoline Surrogate Combustion</title>
<link>https://arxiv.org/abs/2506.18853</link>
<guid>https://arxiv.org/abs/2506.18853</guid>
<content:encoded><![CDATA[
<div> Gasoline surrogate model, skeletal reaction models, sensitivity analysis, reduced-order modeling, CUR matrix decomposition<br />
<br />
Summary: <br />
Skeletal reaction models for a four-component gasoline surrogate model were developed using an implicit time-dependent basis CUR methodology. The sensitivities of species mass fractions and temperature to reaction rates were estimated using reduced-order modeling. These sensitivities were then used to create skeletal reaction models automatically. The developed models, with 679 and 494 species, accurately reproduced detailed kinetics model results, with errors of less than 1% and less than 10%, respectively. This automated approach provides a valuable tool for reducing complex detailed models while maintaining high predictive accuracy. <div>
arXiv:2506.18853v1 Announce Type: new 
Abstract: Skeletal reaction models are derived for a four-component gasoline surrogate model via an instantaneous local sensitivity analysis technique. The sensitivities of the species mass fractions and the temperature with respect to the reaction rates are estimated by a reduced-order modeling (ROM) methodology. Termed "implicit time-dependent basis CUR (implicit TDB-CUR)," this methodology is based on the CUR matrix decomposition and incorporates implicit time integration for evolving the bases. The estimated sensitivities are subsequently analyzed to develop skeletal reaction models with a fully automated procedure. The 1389-species gasoline surrogate model developed at Lawrence Livermore National Laboratory (LLNL) is selected as the detailed kinetics model. The skeletal reduction procedure is applied to this model in a zero-dimensional constant-pressure reactor over a wide range of initial conditions. The performances of the resulting skeletal models are appraised by comparison against the results via the LLNL detailed model, and also predictions via other skeletal models. Two new skeletal models are developed consisting of 679 and 494 species, respectively. The first is an alternative to an existing model with the same number of species. The predictions with this model reproduces the detailed models vital flame results with less than 1% errors. The errors via the second model are less than 10%.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems</title>
<link>https://arxiv.org/abs/2506.17236</link>
<guid>https://arxiv.org/abs/2506.17236</guid>
<content:encoded><![CDATA[
<div> blockchain, shared resources, fairness, non-commercial networks, Max-min Fair algorithms

Summary:
The dissertation focuses on fair distribution of shared resources in non-commercial blockchain networks. Blockchain networks order and timestamp records in a secure and consensual manner. In non-commercial networks, monetary solutions are not feasible, leading to challenges in fairness. The current faucet mechanism, offering fixed amounts of free tokens, is susceptible to attacks and lacks fairness. The study proposes 6 Max-min Fair algorithms as efficient blockchain faucets, addressing fairness issues. These algorithms are resistant to denial of service attacks, cost-effective in terms of computational economics, and allow for different user weighting policies. By adapting the faucet mechanism for fair distribution, the study aims to improve the efficiency and fairness of resource distribution in non-commercial blockchain networks. 

<br /><br />Summary: <div>
arXiv:2506.17236v1 Announce Type: cross 
Abstract: The present dissertation addresses the problem of fairly distributing shared resources in non-commercial blockchain networks. Blockchains are distributed systems that order and timestamp records of a given network of users, in a public, cryptographically secure, and consensual way. The records, which may in kind be events, transaction orders, sets of rules for structured transactions etc. are placed within well-defined datastructures called blocks, and they are linked to each other by the virtue of cryptographic pointers, in a total ordering which represents their temporal relations of succession. The ability to operate on the blockchain, and/or to contribute a record to the content of a block are shared resources of the blockchain systems. In commercial networks, these resources are exchanged in return for fiat money, and consequently, fairness is not a relevant problem in terms of computer engineering. In non-commercial networks, however, monetary solutions are not available, by definition. The present non-commercial blockchain networks employ trivial distribution mechanisms called faucets, which offer fixed amounts of free tokens (called cryptocurrencies) specific to the given network. This mechanism, although simple and efficient, is prone to denial of service (DoS) attacks and cannot address the fairness problem. In the present dissertation, the faucet mechanism is adapted for fair distribution, in line with Max-min Fairness scheme. In total, we contributed 6 distinct Max-min Fair algorithms as efficient blockchain faucets. The algorithms we contribute are resistant to DoS attacks, low-cost in terms of blockchain computation economics, and they also allow for different user weighting policies.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Beyond Order: A Chaos-Markov-Gaussian Framework for Short-Term Sentiment Forecasting of Any Financial OHLC timeseries Data</title>
<link>https://arxiv.org/abs/2506.17244</link>
<guid>https://arxiv.org/abs/2506.17244</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment forecasting, financial markets, CMG framework, chaos theory, deep learning

Summary:
The paper presents a novel CMG (Chaos-Markov-Gaussian) framework for short-term sentiment forecasting in financial markets. The framework integrates chaos theory, Markov property, and Gaussian processes to enhance prediction accuracy. By incorporating transformer-based deep learning models, temporal patterns can be efficiently captured. The CMG Framework is designed for fast, resource-efficient, and accurate forecasting of any financial instrument's OHLC time series. Unlike traditional models, CMG reduces overhead and generalizes well, making it valuable for analysts and financial institutions. Evaluations on market indices show that CMG consistently outperforms statistical, machine learning, and deep learning baselines in terms of accuracy and efficiency. This framework provides a promising approach for improving sentiment forecasting in financial markets. 

<br /><br />Summary: <div>
arXiv:2506.17244v1 Announce Type: cross 
Abstract: Short-term sentiment forecasting in financial markets (e.g., stocks, indices) is challenging due to volatility, non-linearity, and noise in OHLC (Open, High, Low, Close) data. This paper introduces a novel CMG (Chaos-Markov-Gaussian) framework that integrates chaos theory, Markov property, and Gaussian processes to improve prediction accuracy. Chaos theory captures nonlinear dynamics; the Markov chain models regime shifts; Gaussian processes add probabilistic reasoning. We enhance the framework with transformer-based deep learning models to capture temporal patterns efficiently. The CMG Framework is designed for fast, resource-efficient, and accurate forecasting of any financial instrument's OHLC time series. Unlike traditional models that require heavy infrastructure and instrument-specific tuning, CMG reduces overhead and generalizes well. We evaluate the framework on market indices, forecasting sentiment for the next trading day's first quarter. A comparative study against statistical, ML, and DL baselines trained on the same dataset with no feature engineering shows CMG consistently outperforms in accuracy and efficiency, making it valuable for analysts and financial institutions.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation</title>
<link>https://arxiv.org/abs/2506.17747</link>
<guid>https://arxiv.org/abs/2506.17747</guid>
<content:encoded><![CDATA[
<div> Keywords: geological modeling, reservoir characterization, Pix2Geomodel, conditional generative adversarial network, Groningen gas field

Summary:
Accurate geological modeling is essential for understanding reservoir properties, but traditional methods face challenges with complex subsurface heterogeneity and conditioning to observed data. The Pix2Geomodel, a conditional generative adversarial network (cGAN) based on Pix2Pix, was developed to predict reservoir properties in the Groningen gas field. Utilizing a large dataset, the framework achieved high accuracy in predicting facies and water saturation and moderate success with porosity and permeability. The model's performance was validated through evaluation metrics and visualizations, showcasing its ability to capture spatial variability and geological realism. Despite limitations like microstructural variability and 2D constraints, future iterations (Pix2Geomodel v2.0) could potentially integrate multi-modal data for 3D modeling. This study signifies a significant advancement in using generative AI in geoscience, which can enhance reservoir management and support open science initiatives. 

<br /><br />Summary: <div>
arXiv:2506.17747v1 Announce Type: cross 
Abstract: Accurate geological modeling is critical for reservoir characterization, yet traditional methods struggle with complex subsurface heterogeneity, and they have problems with conditioning to observed data. This study introduces Pix2Geomodel, a novel conditional generative adversarial network (cGAN) framework based on Pix2Pix, designed to predict reservoir properties (facies, porosity, permeability, and water saturation) from the Rotliegend reservoir of the Groningen gas field. Utilizing a 7.6 million-cell dataset from the Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology included data preprocessing, augmentation to generate 2,350 images per property, and training with a U-Net generator and PatchGAN discriminator over 19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection over union (mIoU), frequency weighted intersection over union (FWIoU), and visualizations assessed performance in masked property prediction and property-to-property translation tasks. Results demonstrated high accuracy for facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74, FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA 0.98, FWIoU 0.97). The framework captured spatial variability and geological realism, as validated by variogram analysis, and calculated the training loss curves for the generator and discriminator for each property. Compared to traditional methods, Pix2Geomodel offers enhanced fidelity in direct property mapping. Limitations include challenges with microstructural variability and 2D constraints, suggesting future integration of multi-modal data and 3D modeling (Pix2Geomodel v2.0). This study advances the application of generative AI in geoscience, supporting improved reservoir management and open science initiatives.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks</title>
<link>https://arxiv.org/abs/2506.18154</link>
<guid>https://arxiv.org/abs/2506.18154</guid>
<content:encoded><![CDATA[
<div> Taylor's Power Law, insect populations, universality, ecology, statistics<br />
<br />
Summary: Taylor's Power Law (TPL) correlates mean population abundances and variances across insect populations using a power function. TPL has been studied for six decades, with distinct periods and themes identified, including ecological mechanisms, skewed distributions, and mathematical/statistical explanations. Future research directions include fostering interactions between abstract and physical worlds, measuring heterogeneity, and exploring evolutionary contexts. TPL's significance lies in its practical applications in various fields such as agriculture and epidemiology, as well as its theoretical implications related to phase transitions and scale invariance. <div>
arXiv:2506.18154v1 Announce Type: cross 
Abstract: First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL) correlates the mean (M) population abundances and the corresponding variances (V) across a set of insect populations using a power function (V=aM^b). TPL has demonstrated its 'universality' across numerous fields of sciences, social sciences, and humanities. This universality has inspired two main prongs of exploration: one from mathematicians and statisticians, who might instinctively respond with a convergence theorem similar to the central limit theorem of the Gaussian distribution, and another from biologists, ecologists, physicists, etc., who are more interested in potential underlying ecological or organizational mechanisms. Over the past six decades, TPL studies have produced a punctuated landscape with three relatively distinct periods (1960s-1980s; 1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical worlds. Eight themes have been identified and reviewed on this landscape, including population spatial aggregation and ecological mechanisms, TPL and skewed statistical distributions, mathematical/statistical mechanisms of TPL, sample vs. population TPL, population stability, synchrony, and early warning signals for tipping points, TPL on complex networks, TPL in macrobiomes, and in microbiomes. Three future research directions including fostering reciprocal interactions between the two prongs, heterogeneity measuring, and exploration in the context of evolution. The significance of TPL research includes practically, population fluctuations captured by TPL are relevant for agriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor heterogeneity, earthquakes, social inequality, stock illiquidity, financial stability, tipping point events, etc.; theoretically, TPL is one form of power laws, which are related to phase transitions, universality, scale-invariance, etc.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Airalogy: AI-empowered universal data digitization for research automation</title>
<link>https://arxiv.org/abs/2506.18586</link>
<guid>https://arxiv.org/abs/2506.18586</guid>
<content:encoded><![CDATA[
<div> AI, research data, platform, standardization, multidisciplinary <br />
Summary: <br />
Research data are crucial for AI-driven science, but current applications are limited due to fragmented and inefficient data collection processes. Existing platforms lack the balance between universality and standardization needed to support diverse disciplines. Airalogy aims to bridge this gap by providing a platform that integrates scientific domain knowledge with advanced computing skills. It offers customizable, standardized data records and advanced AI tools for research assistance and automation. Already deployed in laboratories at Westlake University, Airalogy has the potential to accelerate scientific innovation across academia, industry, and the global research community. By addressing the challenges of data standardization and leveraging AI-driven capabilities, Airalogy aims to benefit humanity through enhanced research efficiency and automation. <br /> <div>
arXiv:2506.18586v1 Announce Type: cross 
Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven science, yet current AI applications remain limited to a few fields with readily available, well-structured, digitized datasets. Achieving comprehensive AI empowerment across multiple disciplines is still out of reach. Present-day research data collection is often fragmented, lacking unified standards, inefficiently managed, and difficult to share. Creating a single platform for standardized data digitization needs to overcome the inherent challenge of balancing between universality (supporting the diverse, ever-evolving needs of various disciplines) and standardization (enforcing consistent formats to fully enable AI). No existing platform accommodates both facets. Building a truly multidisciplinary platform requires integrating scientific domain knowledge with sophisticated computing skills. Researchers often lack the computational expertise to design customized and standardized data recording methods, whereas platform developers rarely grasp the intricate needs of multiple scientific domains. These gaps impede research data standardization and hamper AI-driven progress. In this study, we address these challenges by developing Airalogy (https://airalogy.com), the world's first AI- and community-driven platform that balances universality and standardization for digitizing research data across multiple disciplines. Airalogy represents entire research workflows using customizable, standardized data records and offers an advanced AI research copilot for intelligent Q&amp;A, automated data entry, analysis, and research automation. Already deployed in laboratories across all four schools of Westlake University, Airalogy has the potential to accelerate and automate scientific innovation in universities, industry, and the global research community-ultimately benefiting humanity as a whole.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-consistent integration of mechanical systems based on Livens principle</title>
<link>https://arxiv.org/abs/2312.02825</link>
<guid>https://arxiv.org/abs/2312.02825</guid>
<content:encoded><![CDATA[
<div> Hamilton-Pontryagin principle, Livens principle, structure-preserving integrator, mechanical systems, singular mass matrices  
Summary:  
Livens principle, also known as the Hamilton-Pontryagin principle, is utilized to develop a new integrator that preserves the structure of mechanical systems. Unlike traditional Hamiltonian equations, the Euler-Lagrange equations derived from Livens principle avoid the need to invert mass matrices, particularly beneficial for systems with singular mass matrices. This approach unifies Lagrangian and Hamiltonian perspectives, eliminating the requirement to define the system's Hamiltonian. The integrator conserves energy and preserves momentum maps related to system symmetries. An extension is proposed for systems with holonomic constraints, and its performance is evaluated through examples. Overall, the novel integrator based on Livens principle offers a comprehensive and efficient method for simulating mechanical systems while overcoming challenges associated with singular mass matrices. 

<br /><br />Summary: <div>
arXiv:2312.02825v3 Announce Type: replace 
Abstract: In this work we make use of Livens principle (sometimes also referred to as Hamilton-Pontryagin principle) in order to obtain a novel structure-preserving integrator for mechanical systems. In contrast to the canonical Hamiltonian equations of motion, the Euler-Lagrange equations pertaining to Livens principle circumvent the need to invert the mass matrix. This is an essential advantage with respect to singular mass matrices, which can yield severe difficulties for the modelling and simulation of multibody systems. Moreover, Livens principle unifies both Lagrangian and Hamiltonian viewpoints on mechanics. Additionally, the present framework avoids the need to set up the system's Hamiltonian. The novel scheme algorithmically conserves a general energy function and aims at the preservation of momentum maps corresponding to symmetries of the system. We present an extension to mechanical systems subject to holonomic constraints. The performance of the newly devised method is studied in representative examples.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedded Model Form Uncertainty Quantification with Measurement Noise for Bayesian Model Calibration</title>
<link>https://arxiv.org/abs/2410.12037</link>
<guid>https://arxiv.org/abs/2410.12037</guid>
<content:encoded><![CDATA[
<div> parameter calibration, Bayesian methods, model inadequacy, uncertainty quantification, heat flux estimation
Summary: 
This paper addresses the challenge of accurately calibrating simulation parameters in computer models of physical systems. It introduces a novel framework for embedding model inadequacy in Bayesian inference, allowing for more reliable propagation of uncertainties to non-observed quantities of interest (QoIs). By adapting existing likelihood models and proposing new formulations to account for noise and outliers in measurements, the approach improves predictions' representation of observed data points. The study evaluates the method's performance in the presence of discrepancies between measurements and predictions and demonstrates how uncertainty in the model inadequacy term influences QoIs. Through an application to heat flux estimation from transient thermal simulations, the proposed approach enables a more comprehensive statistical analysis of prediction reliability. <div>
arXiv:2410.12037v2 Announce Type: replace 
Abstract: A key factor in ensuring the accuracy of computer simulations that model physical systems is the proper calibration of their parameters based on real-world observations or experimental data. Inevitably, uncertainties arise, and Bayesian methods provide a robust framework for quantifying and propagating these uncertainties to model predictions. Nevertheless, Bayesian methods paired with inexact models usually produce predictions unable to represent the observed datapoints. Additionally, the quantified uncertainties of these overconfident models cannot be propagated to other Quantities of Interest (QoIs) reliably. A promising solution involves embedding a model inadequacy term in the inference parameters, allowing the quantified model form uncertainty to influence non-observed QoIs. This paper introduces a more interpretable framework for embedding the model inadequacy compared to existing methods. To overcome the limitations of current approaches, we adapt the existing likelihood models to properly account for noise in the measurements and propose two new formulations designed to address their shortcomings. Moreover, we evaluate the performance of this inadequacy-embedding approach in the presence of discrepancies between measurements and model predictions, including noise and outliers. Particular attention is given to how the uncertainty associated with the model inadequacy term propagates to the QoIs, enabling a more comprehensive statistical analysis of prediction's reliability. Finally, the proposed approach is applied to estimate the uncertainty in the predicted heat flux from a transient thermal simulation using temperature observations.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resilience-based post disaster recovery optimization for infrastructure system via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.18577</link>
<guid>https://arxiv.org/abs/2410.18577</guid>
<content:encoded><![CDATA[
<div> Keywords: infrastructure systems, post-disaster recovery, deep reinforcement learning, resilience metric, optimization

Summary:
Infrastructure systems face significant challenges during post-disaster recovery, requiring efficient repair-scheduling approaches under resource constraints. Existing methods have limitations in such contexts, prompting the proposal of a novel approach using Deep Reinforcement Learning (DRL) and a specialized resilience metric. The system's recovery process is modeled as a sequential decision-making problem on a graph-based structure, with Deep Q-learning algorithms optimizing recovery strategies. Testing on post-earthquake recovery for an electrical substation system showed Double DQN (DDQN) to be superior. Comparative analysis demonstrated the proposed method's effectiveness in optimizing resilience and rapid recovery while minimizing computational costs. This approach presents an attractive solution for enhancing infrastructure system resilience and response efficiency. 

<br /><br />Summary: <div>
arXiv:2410.18577v2 Announce Type: replace 
Abstract: Infrastructure systems are critical in modern communities but are highly susceptible to various natural and man-made disasters. Efficient post-disaster recovery requires repair-scheduling approaches under the limitation of capped resources that need to be shared across the system. Existing approaches, including component ranking methods, greedy evolutionary algorithms, and data-driven machine learning models, face various limitations when tested within such a context. To tackle these issues, we propose a novel approach to optimize post-disaster recovery of infrastructure systems by leveraging Deep Reinforcement Learning (DRL) methods and incorporating a specialized resilience metric to lead the optimization. The system topology is represented adopting a graph-based structure, where the system's recovery process is formulated as a sequential decision-making problem. Deep Q-learning algorithms are employed to learn optimal recovery strategies by mapping system states to specific actions, as for instance which component ought to be repaired next, with the goal of maximizing long-term recovery from a resilience-oriented perspective. To demonstrate the efficacy of our proposed approach, we implement this scheme on the example of post-earthquake recovery optimization for an electrical substation system. We assess different deep Q-learning algorithms to this end, namely vanilla Deep Q-Networks (DQN), Double DQN(DDQN), Duel DQN, and duel DDQN, demonstrating superiority of the DDQN for the considered problem. A further comparative analysis against baseline methods during testing reveals the superior performance of the proposed method in terms of both optimization effect and computational cost, rendering this an attractive approach in the context of resilience enhancement and rapid response and recovery.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Cancer Gene Identification through Graph Anomaly Analysis</title>
<link>https://arxiv.org/abs/2412.17240</link>
<guid>https://arxiv.org/abs/2412.17240</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, protein-protein interaction networks, cancer genes, weight heterogeneity, HIPGNN

Summary: 
This study explores the use of Graph Neural Networks (GNNs) in analyzing protein-protein interaction (PPI) networks to identify cancer genes. It identifies a unique graph anomaly in cancer genes known as weight heterogeneity, characterized by significantly higher variance in edge weights of cancer gene nodes within the graph. The study also highlights how weight heterogeneity can impact the spectral energy distribution, leading to a concentration towards the extremes of the spectrum. To address these insights, the study proposes the HIerarchical-Perspective Graph Neural Network (HIPGNN), which considers variations in spectral energy distribution and protein interaction context. Experimental results on reprocessed datasets demonstrate the superiority of HIPGNN in identifying cancer genes within PPI networks. <div>
arXiv:2412.17240v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the "flattening out" of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanics Informatics: A paradigm for efficiently learning constitutive models</title>
<link>https://arxiv.org/abs/2501.08314</link>
<guid>https://arxiv.org/abs/2501.08314</guid>
<content:encoded><![CDATA[
<div> stress state entropy, constitutive model learning, parameter identification, information content, mechanics informatics

Summary:<br />
- Mechanics informatics is introduced as a paradigm for efficient and accurate learning of constitutive laws, focusing on the interplay between experimental data and model parameters.
- The stress state entropy is proposed as a metric for quantifying the information content of experimental data, enabling the analysis of specimen geometries with varying information content for model learning.
- Specimen designs are optimized using stress state entropy in a Bayesian optimization scheme, leading to the creation of cruciform specimens with maximized entropy for accurate parameter identification.
- Tailoring specimen designs for specific experimental goals is demonstrated through minimizing entropy in Peirs shear specimens to achieve a uniform shear stress state.
- The framework addresses experimental uncertainties, explores transfer learning for replacing challenging testing protocols with simpler alternatives, and shows potential for extension to different material laws.

<br /><br />Summary: <div>
arXiv:2501.08314v2 Announce Type: replace 
Abstract: Efficient and accurate learning of constitutive laws is crucial for accurately predicting the mechanical behavior of materials under complex loading conditions. Accurate model calibration hinges on a delicate interplay between the information embedded in experimental data and the parameters that define our constitutive models.The information encoded in the parameters of the constitutive model must be complemented by the information in the data used for calibration. This interplay raises fundamental questions: How can we quantify the information content of test data? How much information does a single test convey? Also, how much information is required to accurately learn a constitutive model? To address these questions, we introduce mechanics informatics, a paradigm for efficient and accurate constitutive model learning. At its core is the stress state entropy, a metric for quantifying the information content of experimental data. Using this framework, we analyzed specimen geometries with varying information content for learning an anisotropic inelastic law. Specimens with limited information enabled accurate identification of a few parameters sensitive to the information in the data. Furthermore, we optimized specimen design by incorporating stress state entropy into a Bayesian optimization scheme. This led to the design of cruciform specimens with maximized entropy for accurate parameter identification. Conversely, minimizing entropy in Peirs shear specimens yielded a uniform shear stress state, showcasing the framework's flexibility in tailoring designs for specific experimental goals. Finally, we addressed experimental uncertainties, demonstrated the potential of transfer learning for replacing challenging testing protocols with simpler alternatives, and extension of the framework to different material laws.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Neural Networks in Supply Chain Analytics and Optimization: Concepts, Perspectives, Dataset and Benchmarks</title>
<link>https://arxiv.org/abs/2411.08550</link>
<guid>https://arxiv.org/abs/2411.08550</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, supply chain management, benchmark dataset, optimization, real-world applications

Summary: 
Graph Neural Networks (GNNs) have shown promise in various fields but have yet to be extensively studied in supply chain management. This study bridges the gap by providing a conceptual foundation for applying GNNs in supply chain optimization. By connecting supply chains with graph structures, the authors explain formulations, examples, and task guidelines for effective GNN application. They also introduce a benchmark dataset from a leading FMCG company in Bangladesh, focused on supply chain planning. Through various supply chain tasks using GNNs, the study shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models in regression, classification, detection, and anomaly detection tasks. This work lays the groundwork for leveraging GNNs in solving complex supply chain problems with methodological insights and a comprehensive dataset.
<br /><br />Summary: <div>
arXiv:2411.08550v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have recently gained traction in transportation, bioinformatics, language and image processing, but research on their application to supply chain management remains limited. Supply chains are inherently graph-like, making them ideal for GNN methodologies, which can optimize and solve complex problems. The barriers include a lack of proper conceptual foundations, familiarity with graph applications in SCM, and real-world benchmark datasets for GNN-based supply chain research. To address this, we discuss and connect supply chains with graph structures for effective GNN application, providing detailed formulations, examples, mathematical definitions, and task guidelines. Additionally, we present a multi-perspective real-world benchmark dataset from a leading FMCG company in Bangladesh, focusing on supply chain planning. We discuss various supply chain tasks using GNNs and benchmark several state-of-the-art models on homogeneous and heterogeneous graphs across six supply chain analytics tasks. Our analysis shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models by around 10-30% in regression, 10-30% in classification and detection tasks, and 15-40% in anomaly detection tasks on designated metrics. With this work, we lay the groundwork for solving supply chain problems using GNNs, supported by conceptual discussions, methodological insights, and a comprehensive dataset.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Optimization of Physics-Informed Neural Networks: Evo-PINN Frontiers and Opportunities</title>
<link>https://arxiv.org/abs/2501.06572</link>
<guid>https://arxiv.org/abs/2501.06572</guid>
<content:encoded><![CDATA[
<div> Keywords: PINNs, physics-informed neural networks, optimization, generalization, evolutionary algorithms

Summary:
PINNs, or physics-informed neural networks, integrate mathematical laws of nature into their training loss function, providing advantages over data-driven models in limited-data scenarios. However, optimizing and generalizing PINNs present challenges, including training speed, precision, and generalizability. Evolutionary algorithms (EAs) are proposed as a solution to optimize complex loss landscapes in PINNs, potentially improving training efficiency and accuracy. Synergizing gradient descent with EAs can lead to tailored neural architectures and better balancing of physics-informed learning objectives. Additionally, using evolutionary algorithms as meta-learners for generalizable PINN models shows promise. Recent literature demonstrates the early success of these approaches in tackling optimization and generalization issues in PINNs. <div>
arXiv:2501.06572v3 Announce Type: replace-cross 
Abstract: Deep learning models trained on finite data lack a complete understanding of the physical world. On the other hand, physics-informed neural networks (PINNs) are infused with such knowledge through the incorporation of mathematically expressible laws of nature into their training loss function. By complying with physical laws, PINNs provide advantages over purely data-driven models in limited-data regimes and present as a promising route towards Physical AI. This feature has propelled them to the forefront of scientific machine learning, a domain characterized by scarce and costly data. However, the vision of accurate physics-informed learning comes with significant challenges. This work examines PINNs for the first time in terms of model optimization and generalization, shedding light on the need for new algorithmic advances to overcome issues pertaining to the training speed, precision, and generalizability of today's PINN models. Of particular interest are gradient-free evolutionary algorithms (EAs) for optimizing the uniquely complex loss landscapes arising in PINN training. Methods synergizing gradient descent and EAs for discovering bespoke neural architectures and balancing multiple terms in physics-informed learning objectives are positioned as important avenues for future research. Another exciting track is to cast evolutionary as a meta-learner of generalizable PINN models. To substantiate these proposed avenues, we further highlight results from recent literature to showcase the early success of such approaches in addressing the aforementioned challenges in PINN optimization and generalization.
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fast Iterative Robust Principal Component Analysis Method</title>
<link>https://arxiv.org/abs/2506.16013</link>
<guid>https://arxiv.org/abs/2506.16013</guid>
<content:encoded><![CDATA[
<div> Keywords: Principal Component Analysis, Robust PCA, Outliers, Fast Iterative Robust PCA, Incremental PCA

Summary: 
The article introduces a new method called Fast Iterative Robust (FIR) PCA to improve the robustness of Principal Component Analysis (PCA) in the presence of outliers. Traditional PCA methods are sensitive to outliers, affecting the accuracy of the results. The FIR PCA method efficiently estimates the center location and covariance of inliers, using Incremental PCA (IPCA) to iteratively construct a subset of data points that improve location and covariance estimation while mitigating the impact of outliers. The approach demonstrates competitive accuracy and performance compared to existing robust methods, offering enhanced robustness to outlier contamination. Simulated and real-world datasets are used to evaluate the effectiveness of the method in identifying and preserving underlying data structures in the presence of outliers. <div>
arXiv:2506.16013v1 Announce Type: new 
Abstract: Principal Component Analysis (PCA) is widely used for dimensionality reduction and data analysis. However, PCA results are adversely affected by outliers often observed in real-world data. Existing robust PCA methods are often computationally expensive or exhibit limited robustness. In this work, we introduce a Fast Iterative Robust (FIR) PCA method by efficiently estimating the inliers center location and covariance. Our approach leverages Incremental PCA (IPCA) to iteratively construct a subset of data points that ensures improved location and covariance estimation that effectively mitigates the influence of outliers on PCA projection. We demonstrate that our method achieves competitive accuracy and performance compared to existing robust location and covariance methods while offering improved robustness to outlier contamination. We utilize simulated and real-world datasets to evaluate and demonstrate the efficacy of our approach in identifying and preserving underlying data structures in the presence of contamination.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Converging Single Trace Quasi-local PMCHWT Equation for the Modelling of Composite Systems</title>
<link>https://arxiv.org/abs/2506.16376</link>
<guid>https://arxiv.org/abs/2506.16376</guid>
<content:encoded><![CDATA[
<div> PMCHWT integral equation, scattering, time-harmonic fields, piecewise homogeneous, composite systems<br />
Summary:<br />
The article introduces a single trace quasi-local PMCHWT equation for modeling scattering of time-harmonic fields by composite systems with junctions. Traditional methods for solving such systems rely on Krylov iterative methods, with the number of iterations needed influenced by the system matrix's eigenvalue distribution. While Caldern preconditioning is effective for systems without junction lines, it is insufficient for those with junctions. The new approach, utilizing the global multi-trace method, offers a solution with slower iteration growth as mesh size decreases. The method maintains continuity conditions at domain interfaces and is free from interior resonances. Extensive numerical experiments confirm the method's accuracy, convergence behavior, and efficiency. This advancement represents a significant step forward in the modeling and simulation of complex systems with junctions. <br /> <div>
arXiv:2506.16376v1 Announce Type: new 
Abstract: The PMCHWT integral equation enables the modelling of scattering of time-harmonic fields by penetrable, piecewise homogeneous, systems. They have been generalised to include the modelling of composite systems that may contain junctions, i.e. lines along which three or more materials meet. Linear systems resulting upon discretisation of the PMCHWT are, because of their large dimension, typically solved by Krylov iterative methods. The number of iterations required for this solution critically depends on the eigenvalue distribution of the system matrix. For systems that do not contain junction lines, Calder\'on preconditioning, which was first applied to the electric field integral equation, has been generalised to the PMCHWT equation. When junctions are present, this approach cannot be applied. Alternative approaches, such as the global multi-trace method, conceptually remove the junction lines and as a result are amenable to Calder\'on preconditioning. This approach entails a doubling of the degrees of freedom, and the solution that is produced only approximately fulfils the continuity conditions at interfaces separating domains. In this contribution, a single trace quasi-local PMCHWT equation is introduced that requires a number of iterations for its solution that only slowly increases as the mesh size tends to zero. The method is constructed as a generalisation of the classic PMCHWT, and its discretisation is thoroughly discussed. A comprehensive suite of numerical experiments demonstrates the correctness, convergence behaviour, and efficiency of the method. The integral equation is demonstrated to be free from interior resonances.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aethorix v1.0: AI-Driven Inverse Design of Inorganic Materials for Scalable Industrial Innovation</title>
<link>https://arxiv.org/abs/2506.16609</link>
<guid>https://arxiv.org/abs/2506.16609</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, industrial manufacturing, materials development, Aethorix v1.0, R&amp;D pipelines 

Summary: 
Artificial intelligence for Science (AI4S) is revolutionizing industrial manufacturing by accelerating the discovery and optimization of advanced materials. Aethorix v1.0 is introduced as a platform that combines large language models, generative models for crystal design, and machine-learned potentials for property prediction. This platform aims to streamline the materials development cycle, from design to deployment, while adhering to manufacturing standards. The industrial value of Aethorix v1.0 is validated through a real use case, demonstrating its seamless integration into scalable R&amp;D pipelines. Its capabilities in objective mining, inorganic crystal design, and property prediction enable rapid advancement in materials science, offering a promising solution for the industry's challenges. Aethorix v1.0 showcases the potential of AI in transforming the field of industrial manufacturing and driving innovation in high-performance materials. 

<br /><br />Summary: <div>
arXiv:2506.16609v1 Announce Type: new 
Abstract: Artificial intelligence for Science (AI4S) is poised to transform industrial manufacturing by enabling the accelerated discovery and optimization of advanced (bio)materials, dramatically reducing development cycles, and unlocking novel high-performance solutions. We introduce Aethorix v1.0, a platform that integrates large language models for objective mining, diffusion-based generative models for zero-shot inorganic crystal design, and machine-learned interatomic potentials for rapid property prediction at ab initio accuracy. The platform is developed to enhance the full materials development cycle, ranging from design to deployment in use cases, while incorporating critical operational constraints to meet rigorous manufacturing standards. We validated its industrial value through a real use case, showcasing how the framework can be seamlessly embedded into scalable materials R&amp;D pipelines.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training Time Series Models with Stock Data Customization</title>
<link>https://arxiv.org/abs/2506.16746</link>
<guid>https://arxiv.org/abs/2506.16746</guid>
<content:encoded><![CDATA[
<div> stock selection, pre-training, transformer architecture, financial data, experimental results <br />
Summary:<br />
The paper introduces novel pre-training tasks tailored to stock data characteristics, including stock code classification, stock sector classification, and moving average prediction. The Stock Specialized Pre-trained Transformer (SSPT) is developed based on a two-layer transformer architecture. Experimental results demonstrate the effectiveness of the proposed pre-training methods, outperforming existing methods in terms of cumulative investment return ratio and Sharpe ratio across various stock datasets and time periods. The research also includes evaluations on simulated data to provide insights into understanding price series. The code for the proposed methods is publicly available. <div>
arXiv:2506.16746v1 Announce Type: new 
Abstract: Stock selection, which aims to predict stock prices and identify the most profitable ones, is a crucial task in finance. While existing methods primarily focus on developing model structures and building graphs for improved selection, pre-training strategies remain underexplored in this domain. Current stock series pre-training follows methods from other areas without adapting to the unique characteristics of financial data, particularly overlooking stock-specific contextual information and the non-stationary nature of stock prices. Consequently, the latent statistical features inherent in stock data are underutilized. In this paper, we propose three novel pre-training tasks tailored to stock data characteristics: stock code classification, stock sector classification, and moving average prediction. We develop the Stock Specialized Pre-trained Transformer (SSPT) based on a two-layer transformer architecture. Extensive experimental results validate the effectiveness of our pre-training methods and provide detailed guidance on their application. Evaluations on five stock datasets, including four markets and two time periods, demonstrate that SSPT consistently outperforms the market and existing methods in terms of both cumulative investment return ratio and Sharpe ratio. Additionally, our experiments on simulated data investigate the underlying mechanisms of our methods, providing insights into understanding price series. Our code is publicly available at: https://github.com/astudentuser/Pre-training-Time-Series-Models-with-Stock-Data-Customization.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Traditional Technical Analysis with AI: A Multi-Agent LLM-Based Approach to Stock Market Forecasting</title>
<link>https://arxiv.org/abs/2506.16813</link>
<guid>https://arxiv.org/abs/2506.16813</guid>
<content:encoded><![CDATA[
<div> ElliottAgents, multi-agent system, Elliott Wave Principle, AI, stock market forecasting<br />
Summary:<br />
Traditional technical analysis methods struggle to predict trends in complex financial markets. ElliottAgents integrates the Elliott Wave Principle with AI to address these challenges. The system utilizes LLMs to enhance natural language understanding and decision-making in a multi-agent framework. By employing technologies like RAG and DRL, ElliottAgents continuously analyzes market data to identify wave patterns and predict future price movements. Experimental results on historical data from U.S. companies validate its effectiveness in pattern recognition and trend forecasting across different time frames. This research demonstrates the successful combination of traditional technical analysis with modern AI approaches, providing traders with reliable and interpretable market prediction systems. <div>
arXiv:2506.16813v1 Announce Type: new 
Abstract: Traditional technical analysis methods face limitations in accurately predicting trends in today's complex financial markets. This paper introduces ElliottAgents, an multi-agent system that integrates the Elliott Wave Principle with AI for stock market forecasting. The inherent complexity of financial markets, characterized by non-linear dynamics, noise, and susceptibility to unpredictable external factors, poses significant challenges for accurate prediction. To address these challenges, the system employs LLMs to enhance natural language understanding and decision-making capabilities within a multi-agent framework. By leveraging technologies such as Retrieval-Augmented Generation (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs continuous, multi-faceted analysis of market data to identify wave patterns and predict future price movements. The research explores the system's ability to process historical stock data, recognize Elliott wave patterns, and generate actionable insights for traders. Experimental results, conducted on historical data from major U.S. companies, validate the system's effectiveness in pattern recognition and trend forecasting across various time frames. This paper contributes to the field of AI-driven financial analysis by demonstrating how traditional technical analysis methods can be effectively combined with modern AI approaches to create more reliable and interpretable market prediction systems.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Deprivation Cost Functions for Power Outages During Disasters: A Discrete Choice Modeling Approach</title>
<link>https://arxiv.org/abs/2506.16993</link>
<guid>https://arxiv.org/abs/2506.16993</guid>
<content:encoded><![CDATA[
<div> deprivation costs, electricity outages, stated preference survey, discrete choice model, Harris County<br />
<br />
Summary: 
This study addresses the lack of systematic measurement of deprivation costs related to electricity outages by developing a methodology to estimate deprivation cost functions using stated preference survey data from Harris County, Texas. The analysis compares different model architectures and utility transformations to capture the increasing and convex nature of deprivation cost functions over time. The study also identifies heterogeneity in deprivation valuation, particularly among different income groups. The results show that deprivation cost functions are strictly increasing with time and highlight the importance of flexible modeling approaches to account for systematic and random taste variation. By providing a methodological framework and empirical evidence, this research enables policymakers to better quantify service disruption costs and develop more equitable resilience strategies in infrastructure risk assessments and humanitarian logistics. <br /><br /> <div>
arXiv:2506.16993v1 Announce Type: new 
Abstract: Systems for the generation and distribution of electrical power represents critical infrastructure and, when extreme weather events disrupt such systems, this imposes substantial costs on consumers. These costs can be conceptualized as deprivation costs, an increasing function of time without service, quantifiable through individuals' willingness to pay for power restoration. Despite widespread recognition of outage impacts, a gap in the research literature exists regarding the systematic measurement of deprivation costs. This study addresses this deficiency by developing and implementing a methodology to estimate deprivation cost functions for electricity outages, using stated preference survey data collected from Harris County, Texas. This study compares multiple discrete choice model architectures, including multinomial logit and mixed logit specifications, as well as models incorporating BoxCox and exponential utility transformations for the deprivation time attribute. The analysis examines heterogeneity in deprivation valuation through sociodemographic interactions, particularly across income groups. Results confirm that power outage deprivation cost functions are convex and strictly increasing with time. Additionally, the study reveals both systematic and random taste variation in how individuals value power loss, highlighting the need for flexible modeling approaches. By providing both methodological and empirical foundations for incorporating deprivation costs into infrastructure risk assessments and humanitarian logistics, this research enables policymakers to better quantify service disruption costs and develop more equitable resilience strategies.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finance Language Model Evaluation (FLaME)</title>
<link>https://arxiv.org/abs/2506.15846</link>
<guid>https://arxiv.org/abs/2506.15846</guid>
<content:encoded><![CDATA[
<div> Language Models, Finance NLP, Evaluation Frameworks, Benchmarking, Reasoning-reinforced LMs <br />
Summary: <br />
This paper introduces a benchmarking suite, FLaME, for evaluating Language Models (LMs) on Finance NLP tasks. It addresses gaps in existing evaluation methodologies and challenges the belief in LMs' lower performance bounds in finance tasks. The study compares 23 LMs across 20 core NLP tasks in finance, including 'reasoning-reinforced' LMs. The research aims to demonstrate the potential of LMs in specialized finance tasks and provides open-source access to the FLaME framework, data, and results. <div>
arXiv:2506.15846v1 Announce Type: cross 
Abstract: Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials</title>
<link>https://arxiv.org/abs/2506.16918</link>
<guid>https://arxiv.org/abs/2506.16918</guid>
<content:encoded><![CDATA[
<div> deep learning, multiscale modeling, FE^2 approach, computational homogenization, viscoelastic material

Summary:
This study focuses on developing a hybrid model that combines data-driven deep learning techniques with physics-based approaches for multiscale simulations in time-dependent solid mechanics problems. By incorporating neural operators to predict microscale physics, the model efficiently predicts homogenized stresses with minimal error (less than 6%) while being approximately 100 times faster computationally. The approach integrates constitutive relations of the microscale into the model architecture and computes internal variables based on established physical principles. This hybrid model allows for physics-guided learning and flexibility for different materials and spatial discretizations, making it a promising method for accurately predicting the global response of materials influenced by microstructure across various time and length scales. <br /><br />Summary: <div>
arXiv:2506.16918v1 Announce Type: cross 
Abstract: The behavior of materials is influenced by a wide range of phenomena occurring across various time and length scales. To better understand the impact of microstructure on macroscopic response, multiscale modeling strategies are essential. Numerical methods, such as the $\text{FE}^2$ approach, account for micro-macro interactions to predict the global response in a concurrent manner. However, these methods are computationally intensive due to the repeated evaluations of the microscale. This challenge has led to the integration of deep learning techniques into computational homogenization frameworks to accelerate multiscale simulations. In this work, we employ neural operators to predict the microscale physics, resulting in a hybrid model that combines data-driven and physics-based approaches. This allows for physics-guided learning and provides flexibility for different materials and spatial discretizations. We apply this method to time-dependent solid mechanics problems involving viscoelastic material behavior, where the state is represented by internal variables only at the microscale. The constitutive relations of the microscale are incorporated into the model architecture and the internal variables are computed based on established physical principles. The results for homogenized stresses ($<6\%$ error) show that the approach is computationally efficient ($\sim 100 \times$ faster).
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete nonlinear elastodynamics in a port-Hamiltonian framework</title>
<link>https://arxiv.org/abs/2306.17740</link>
<guid>https://arxiv.org/abs/2306.17740</guid>
<content:encoded><![CDATA[
<div> Keywords: port-Hamiltonian, discrete elastodynamical systems, nonlinear strains, hyperelastic material behavior, midpoint discrete gradient <br />
<br />
Summary: 
The article presents a fully nonlinear port-Hamiltonian formulation for discrete elastodynamical systems. The governing equations are derived variationaly, resulting in index-1 differential algebraic equations. By performing index reduction, a port-Hamiltonian state space model is obtained, featuring nonlinear strains as independent states. The model captures hyperelastic material behavior through a nonlinear stored energy function and exhibits passivity, losslessness, and symmetry conserving angular momentum. Temporal discretization is achieved using the midpoint discrete gradient, preserving the model's beneficial properties in a discrete sense. Numerical validation in a representative example confirms the effectiveness of the proposed approach. <div>
arXiv:2306.17740v2 Announce Type: replace-cross 
Abstract: We provide a fully nonlinear port-Hamiltonian formulation for discrete elastodynamical systems as well as a structure-preserving time discretization. The governing equations are obtained in a variational manner and represent index-1 differential algebraic equations. Performing an index reduction one obtains the port-Hamiltonian state space model, which features the nonlinear strains as an independent state next to position and velocity. Moreover, hyperelastic material behavior is captured in terms of a nonlinear stored energy function. The model exhibits passivity and losslessness and has an underlying symmetry yielding the conservation of angular momentum. We perform temporal discretization using the midpoint discrete gradient, such that the beneficial properties are inherited by the developed time stepping scheme in a discrete sense. The numerical results obtained in a representative example are demonstrated to validate the findings.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid high-order methods for elasto-acoustic wave propagation in the time domain</title>
<link>https://arxiv.org/abs/2502.10870</link>
<guid>https://arxiv.org/abs/2502.10870</guid>
<content:encoded><![CDATA[
<div> Keywords: Hybrid High-Order method, Acoustic and Elastic wave equations, Time domain, Spectral analysis, Elasto-acoustic wave propagation<br />
<br />
Summary: 
The study introduces a Hybrid High-Order (HHO) method for coupling acoustic and elastic wave equations in the time domain. The method, utilizing first-order time formulation, can employ equal-order and mixed-order settings with polynomial degree k>=0 for face unknowns, along with O(1)- and O(1/h)-stabilizations. An energy-error estimate is established for the time-continuous scenario, and a numerical spectral analysis reveals the necessity of O(1)-stabilization to avoid excessive CFL limitations with explicit time discretizations. Optimal convergence rates of order (k+1) are demonstrated for general mesh analytical solutions in both equal- and mixed-order settings with O(1)-stabilization, while mixed-order setting with O(1/h)-stabilization achieves order (k+2). Test cases using a Ricker wavelet as an initial condition illustrate the method's effectiveness in simulating elasto-acoustic wave propagation in media with differing material properties. <div>
arXiv:2502.10870v2 Announce Type: replace-cross 
Abstract: We devise a Hybrid High-Order (HHO) method for the coupling between the acoustic and elastic wave equations in the time domain. A first-order formulation in time is considered. The HHO method can use equal-order and mixed-order settings with polynomial degree k>=0 for the face unknowns, together with O(1)- and O(1/h)-stabilizations. An energy-error estimate is established in the time-continuous case. A numerical spectral analysis is performed, showing that O(1)-stabilization is required to avoid excessive CFL limitations for explicit time discretizations. Moreover, the spectral radius of the stiffness matrix is fairly independent of the geometry of the mesh cells. For analytical solutions on general meshes, optimal convergence rates of order (k+1) are shown in both equal- and mixed-order settings using O(1)-stabilization, whereas order (k+2) is achieved in the mixed-order setting using O(1/h)-stabilization. Test cases with a Ricker wavelet as an initial condition showcase the relevance of the proposed method for the simulation of elasto-acoustic wave propagation across media with contrasted material properties.
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.14933</link>
<guid>https://arxiv.org/abs/2506.14933</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized finance, DeFi, cryptocurrency, financial crime, automated detection tools

Summary:
The article discusses the rapid growth of the decentralized finance (DeFi) community driven by cryptocurrency enthusiasts and the resulting increase in financial crime. The rise of cryptocurrency has created opportunities for criminal activity, and the complex and decentralized nature of the technology makes it challenging to catch and prosecute offenders. To address this issue, the implementation of automated detection tools and policies is essential. These tools can help detect and prevent financial crimes in the cryptocurrency realm, ensuring a safer environment for users and investors. By utilizing technology to monitor and enforce regulations, the DeFi community can mitigate the risks associated with criminal activity and maintain the integrity of the market. <div>
arXiv:2506.14933v1 Announce Type: new 
Abstract: The decentralized finance (DeFi) community has grown rapidly in recent years, pushed forward by cryptocurrency enthusiasts interested in the vast untapped potential of new markets. The surge in popularity of cryptocurrency has ushered in a new era of financial crime. Unfortunately, the novelty of the technology makes the task of catching and prosecuting offenders particularly challenging. Thus, it is necessary to implement automated detection tools related to policies to address the growing criminality in the cryptocurrency realm.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing Structural Vibrations via Guided Flow Matching Design Optimization</title>
<link>https://arxiv.org/abs/2506.15263</link>
<guid>https://arxiv.org/abs/2506.15263</guid>
<content:encoded><![CDATA[
<div> Flow matching, design optimization, structural vibrations, plate-like structures, generative model

Summary: 
This article introduces a novel design optimization approach for reducing structural vibrations in plate-like structures by incorporating guided flow matching. The method combines a generative flow matching model with a surrogate model trained to predict structural vibrations, aiming to create manufacturable designs with low vibrations. By leveraging the flow matching model and its training data, the approach explores a wide range of potential solutions without the need for manually-defined design parameters. Various optimization objectives, including direct optimization of specific eigenfrequencies, are considered to improve passenger comfort in engineering systems like cars, trains, and airplanes. Results show that the method outperforms random search, criterion-based design heuristics, and genetic optimization in generating diverse and effective plate designs with reduced vibrations. The code and data for this approach are publicly available for further research and application. 

<br /><br />Summary: <div>
arXiv:2506.15263v1 Announce Type: new 
Abstract: Structural vibrations are a source of unwanted noise in engineering systems like cars, trains or airplanes. Minimizing these vibrations is crucial for improving passenger comfort. This work presents a novel design optimization approach based on guided flow matching for reducing vibrations by placing beadings (indentations) in plate-like structures. Our method integrates a generative flow matching model and a surrogate model trained to predict structural vibrations. During the generation process, the flow matching model pushes towards manufacturability while the surrogate model pushes to low-vibration solutions. The flow matching model and its training data implicitly define the design space, enabling a broader exploration of potential solutions as no optimization of manually-defined design parameters is required. We apply our method to a range of differentiable optimization objectives, including direct optimization of specific eigenfrequencies through careful construction of the objective function. Results demonstrate that our method generates diverse and manufacturable plate designs with reduced structural vibrations compared to designs from random search, a criterion-based design heuristic and genetic optimization. The code and data are available from https://github.com/ecker-lab/Optimizing_Vibrating_Plates.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation of parametrized cardiac electrophysiology in three dimensions using physics-informed neural networks</title>
<link>https://arxiv.org/abs/2506.15405</link>
<guid>https://arxiv.org/abs/2506.15405</guid>
<content:encoded><![CDATA[
<div> PINNs, cardiac electrophysiology, neural networks, Aliev-Panfilov model, 3D prediction<br />
<br />
Summary:
Physics-informed neural networks (PINNs) are used in cardiac electrophysiology to predict myocardium activity in 3D based on the Aliev-Panfilov model. An FCNN architecture is employed with scaled input normalization and the strong form of partial differential equations. Training data is generated using the finite element method. Variations in spatial dimensions and parameters are studied with comparison to FE simulations. Losses in the network are weighted and controlled to optimize training and prediction accuracy. By investigating optimal hyperparameters, this study aims to improve the accuracy of predicting action potential and recovery variable fields in the myocardium. <div>
arXiv:2506.15405v1 Announce Type: new 
Abstract: Physics-informed neural networks (PINNs) are extensively used to represent various physical systems across multiple scientific domains. The same can be said for cardiac electrophysiology, wherein fully-connected neural networks (FCNNs) have been employed to predict the evolution of an action potential in a 2D space following the two-parameter phenomenological Aliev-Panfilov (AP) model. In this paper, the training behaviour of PINNs is investigated to determine optimal hyperparameters to predict the electrophysiological activity of the myocardium in 3D according to the AP model, with the inclusion of boundary and material parameters. An FCNN architecture is employed with the governing partial differential equations in their strong form, which are scaled consistently with normalization of network inputs. The finite element (FE) method is used to generate training data for the network. Numerical examples with varying spatial dimensions and parameterizations are generated using the trained models. The network predicted fields for both the action potential and the recovery variable are compared with the respective FE simulations. Network losses are weighed with individual scalar values. Their effect on training and prediction is studied to arrive at a method of controlling losses during training.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An improved point-to-surface contact algorithm with penalty method for peridynamics</title>
<link>https://arxiv.org/abs/2408.06556</link>
<guid>https://arxiv.org/abs/2408.06556</guid>
<content:encoded><![CDATA[
<div> contact forces, peridynamics, point-to-surface, accuracy, surface particles identification

Summary:
The article proposes an improved point-to-surface contact model for accurate contact force determination in peridynamics simulations, especially for complex geometries. The outer surface is identified using an eigenvalue method, and a Verlet list efficiently identifies potential contact particle pairs. A point-to-surface contact search algorithm, coupled with a penalty function method, is used to calculate precise contact locations and forces. Validation through various contact examples confirms high accuracy, aligning well with Hertz contact theory solutions. The model automatically recognizes external surface particles and accurately computes contact forces, providing valuable insights for multi-body and complex contact scenarios. <br /><br />Summary: <div>
arXiv:2408.06556v2 Announce Type: replace 
Abstract: It is significantly challenging to obtain accurate contact forces in peridynamics (PD) simulations due to the difficulty of surface particles identification, particularly for complex geometries. Here, an improved point-to-surface contact model is proposed for PD with high accuracy. First, the outer surface is identified using the eigenvalue method and then we construct a Verlet list to identify potential contact particle pairs efficiently. Subsequently, a point-to-surface contact search algorithm is utilized to determine precise contact locations with the penalty function method calculating the contact force. Finally, the accuracy of this point-to-surface contact model is validated through several representative contact examples. The results demonstrate that the point-to-surface contact model model can predict contact forces and deformations with high accuracy, aligning well with the classical Hertz contact theory solutions. This work presents a contact model for PD that automatically recognizes external surface particles and accurately calculates the contact force, which provides guidance for the study of multi-body contact as well as complex contact situations.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing oncology with federated learning: transcending boundaries in breast, lung, and prostate cancer. A systematic review</title>
<link>https://arxiv.org/abs/2408.05249</link>
<guid>https://arxiv.org/abs/2408.05249</guid>
<content:encoded><![CDATA[
<div> FL, oncology, federated learning, cancer, data privacy
<br />
Summary:
This systematic review explores the state-of-the-art of Federated Learning (FL) in oncology, focusing on breast, lung, and prostate cancer. The review highlights FL as a promising solution to overcome privacy concerns and utilize diverse, multi-center data in clinical settings. FL showed superior performance compared to centralised machine learning in 15 out of 25 studies, demonstrating its potential to enhance ML generalisability and data privacy. Despite challenges in reproducibility and methodology standardisation, FL proved effective in integrating multi-modal information for precision medicine. The review calls for future research to address these limitations and explore advanced FL methods to fully leverage data diversity in advancing cancer research. Ultimately, FL presents significant potential for transforming cancer care through its ability to harness real-world data and address clinical needs. 
<br /><br /> <div>
arXiv:2408.05249v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has emerged as a promising solution to address the limitations of centralised machine learning (ML) in oncology, particularly in overcoming privacy concerns and harnessing the power of diverse, multi-center data. This systematic review synthesises current knowledge on the state-of-the-art FL in oncology, focusing on breast, lung, and prostate cancer. Distinct from previous surveys, our comprehensive review critically evaluates the real-world implementation and impact of FL on cancer care, demonstrating its effectiveness in enhancing ML generalisability, performance and data privacy in clinical settings and data. We evaluated state-of-the-art advances in FL, demonstrating its growing adoption amid tightening data privacy regulations. FL outperformed centralised ML in 15 out of the 25 studies reviewed, spanning diverse ML models and clinical applications, and facilitating integration of multi-modal information for precision medicine. Despite the current challenges identified in reproducibility, standardisation and methodology across studies, the demonstrable benefits of FL in harnessing real-world data and addressing clinical needs highlight its significant potential for advancing cancer research. We propose that future research should focus on addressing these limitations and investigating further advanced FL methods, to fully harness data diversity and realise the transformative power of cutting-edge FL in cancer care.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy conservative and entropy stable solid wall boundary conditions for the resistive magnetohydrodynamic equations</title>
<link>https://arxiv.org/abs/2412.11132</link>
<guid>https://arxiv.org/abs/2412.11132</guid>
<content:encoded><![CDATA[
<div> diagonal-norm, summation-by-parts, entropy conservative, resistive magnetohydrodynamic equations, boundary conditions <br />
<br />
Summary: <br /> 
This article presents a novel technique for imposing non-linear entropy conservative and entropy stable wall boundary conditions for the resistive magnetohydrodynamic equations in various scenarios. The method relies on diagonal-norm, summation-by-parts, and simultaneous-approximation-term operators for ensuring entropy stability. By using a penalty flux approach and simultaneous-approximation-term technique, boundary data at the wall are weakly imposed. The proposed technique demonstrates accuracy, robustness, and efficacy in enforcing boundary conditions on high-order unstructured grids. It is shown to be compatible with various spatial operators, including finite element and finite volume schemes. Numerical simulations confirm the stability of the method in three-dimensional flows, making it a valuable tool for accurately modeling complex MHD systems. <div>
arXiv:2412.11132v2 Announce Type: replace-cross 
Abstract: We present a novel technique for imposing non-linear entropy conservative and entropy stable wall boundary conditions for the resistive magnetohydrodynamic equations in the presence of an adiabatic wall or a wall with a prescribed heat entropy flow, addressing three scenarios: electrically insulating walls, thin walls with finite conductivity, and perfectly conducting walls. The procedure relies on the formalism and mimetic properties of diagonal-norm, summation-by-parts, and simultaneous-approximation-term operators. Using the method of lines, a semi-discrete entropy estimate for the entire domain is obtained when the proposed numerical imposition of boundary conditions is coupled with an entropy-conservative or entropy-stable discrete interior operator. The resulting estimate mimics the global entropy estimate obtained at the continuous level. The boundary data at the wall are weakly imposed using a penalty flux approach and a simultaneous-approximation-term technique for both the conservative variables and the gradient of the entropy variables. Discontinuous spectral collocation operators (mass lumped nodal discontinuous Galerkin operators) on high-order unstructured grids are used to demonstrate the new procedure's accuracy, robustness, and efficacy for weakly enforcing boundary conditions. Numerical simulations confirm the non-linear stability of the proposed technique, with applications to three-dimensional flows. The procedure described is compatible with any diagonal-norm summation-by-parts spatial operator, including finite element, finite difference, finite volume, nodal and modal discontinuous Galerkin, and flux reconstruction schemes.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Chain Arbitrage: The Next Frontier of MEV in Decentralized Finance</title>
<link>https://arxiv.org/abs/2501.17335</link>
<guid>https://arxiv.org/abs/2501.17335</guid>
<content:encoded><![CDATA[
<div> arbitrage, DeFi, decentralized exchanges, cross-chain, inventory-based execution 

Summary: 
The study focuses on cross-chain arbitrage in decentralized finance (DeFi) markets, where prices are aligned through arbitrage between Layer-1 (L1) and Layer-2 (L2) blockchains. Centralized exchanges (CEXes) currently play a key role in price discovery, but as trading volume shifts on-chain, cross-chain arbitrage between decentralized exchanges (DEXes) is becoming more significant. The research includes a profit-cost model and a year-long measurement of arbitrage activity across nine blockchains. Findings show a high concentration of market activity on Ethereum-centric pairs, with a surge in volume after a blockchain upgrade. Most trades use pre-positioned inventory and settle quickly, while bridge-based arbitrages face latency issues. The study highlights the centralized nature of cross-chain arbitrage, leading to vertical integration and concentration of economic power, posing risks of censorship and centralization in the DeFi space. Decentralizing block building and reducing entry barriers are suggested as critical measures to address these challenges.<br /><br />Summary: <div>
arXiv:2501.17335v2 Announce Type: replace-cross 
Abstract: Decentralized finance (DeFi) markets spread across Layer-1 (L1) and Layer-2 (L2) blockchains rely on arbitrage to keep prices aligned. Today most price gaps are closed against centralized exchanges (CEXes), whose deep liquidity and fast execution make them the primary venue for price discovery. As trading volume migrates on-chain, cross-chain arbitrage between decentralized exchanges (DEXes) will become the canonical mechanism for price alignment. Yet, despite its importance to DeFi-and the on-chain transparency making real activity tractable in a way CEX-to-DEX arbitrage is not-existing research remains confined to conceptual overviews and hypothetical opportunity analyses.
  We study cross-chain arbitrage with a profit-cost model and a year-long measurement. The model shows that opportunity frequency, bridging time, and token depreciation determine whether inventory- or bridge-based execution is more profitable. Empirically, we analyze one year of transactions (September 2023 - August 2024) across nine blockchains and identify 242,535 executed arbitrages totaling 868.64 million USD volume. Activity clusters on Ethereum-centric L1-L2 pairs, grows 5.5x over the study period, and surges-higher volume, more trades, lower fees-after the Dencun upgrade (March 13, 2024). Most trades use pre-positioned inventory (66.96%) and settle in 9s, whereas bridge-based arbitrages take 242s, underscoring the latency cost of today's bridges. Market concentration is high: the five largest addresses execute more than half of all trades, and one alone captures almost 40% of daily volume post-Dencun. We conclude that cross-chain arbitrage fosters vertical integration, centralizing sequencing infrastructure and economic power and thereby exacerbating censorship, liveness, and finality risks; decentralizing block building and lowering entry barriers are critical to countering these threats.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Oder Splitting Schemes for Fluids with Variable Viscosity</title>
<link>https://arxiv.org/abs/2506.14424</link>
<guid>https://arxiv.org/abs/2506.14424</guid>
<content:encoded><![CDATA[
<div> Keywords: matrix-free, discontinuous Galerkin, Navier-Stokes equations, variable viscosity, nonlinear solver<br />
Summary: <br />
This article explores matrix-free higher-order discontinuous Galerkin (DG) discretizations of the Navier-Stokes equations for incompressible flows with variable viscosity. The study involves comparing linearized variants of saddle point block systems and projection-based splitting time integration schemes in terms of computational performance. The research extends the dual splitting method for non-constant viscosity, presents a higher-order DG method for incompressible flows with variable viscosity, introduces accelerated nonlinear solver variants, and evaluates monolithic and projection-based solvers in terms of their solver performance. The schemes are tested in numerical examples to verify spatial and temporal accuracy, preconditioner performance under increased viscosity contrasts, and efficiency in the backward-facing step benchmark. The study investigates conditions under which fully implicit schemes with improved temporal stability and expensive nonlinear solves outperform stable linearized variants and splitting schemes. <div>
arXiv:2506.14424v1 Announce Type: new 
Abstract: This article investigates matrix-free higher-order discontinuous Galerkin (DG) discretizations of the Navier-Stokes equations for incompressible flows with variable viscosity. The viscosity field may be prescribed analytically or governed by a rheological law, as often found in biomedical or industrial applications. We compare several linearized variants of saddle point block systems and projection-based splitting time integration schemes in terms of their computational performance. Compared to the velocity-pressure block-system for the former, the splitting scheme allows solving a sequence of simple problems such as mass, convection-diffusion and Poisson equations. We investigate under which conditions the improved temporal stability of fully implicit schemes and resulting expensive nonlinear solves outperform the splitting schemes and linearized variants that are stable under hyperbolic time step restrictions.
  The key aspects of this work are i) the extension of the dual splitting method originally proposed by G.E. Karniadakis et al. (J. Comput. Phys. 97, 414-443, 1991) towards non-constant viscosity, ii) a higher-order DG method for incompressible flows with variable viscosity, iii) accelerated nonlinear solver variants and suitable linearizations adopting a matrix-free $hp$-multigrid solver, and iv) a detailed comparison of the monolithic and projection-based solvers in terms of their (non-)linear solver performance.
  The presented schemes are evaluated in a series of numerical examples verifying their spatial and temporal accuracy, and the preconditioner performance under increasing viscosity contrasts, while their efficiency is showcased in the backward-facing step benchmark.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Digital Twins via Active Inference</title>
<link>https://arxiv.org/abs/2506.14453</link>
<guid>https://arxiv.org/abs/2506.14453</guid>
<content:encoded><![CDATA[
<div> Keywords: digital twin, active inference, Bayesian framework, predictive maintenance, railway bridge

Summary:
The paper introduces the concept of active digital twins, which leverage active inference, a neuroscience-inspired Bayesian framework, to enhance adaptability in uncertain and dynamic environments. By formulating the evolution of the active digital twin as a partially observable Markov decision process, the agent continuously refines its generative model through Bayesian updates. Decision-making is based on balancing exploitation and exploration, with actions planned to minimize expected free energy. This approach offers superior exploration capabilities, increasing autonomy and resilience in digital twins. The framework is applied to the health monitoring and predictive maintenance of a railway bridge, demonstrating its effectiveness in real-world applications. <div>
arXiv:2506.14453v1 Announce Type: new 
Abstract: Digital twins are transforming engineering and applied sciences by enabling real-time monitoring, simulation, and predictive analysis of physical systems and processes. However, conventional digital twins rely primarily on passive data assimilation, which limits their adaptability in uncertain and dynamic environments. This paper introduces the active digital twin paradigm, based on active inference. Active inference is a neuroscience-inspired, Bayesian framework for probabilistic reasoning and predictive modeling that unifies inference, decision-making, and learning under a unique, free energy minimization objective. By formulating the evolution of the active digital twin as a partially observable Markov decision process, the active inference agent continuously refines its generative model through Bayesian updates and forecasts future states and observations. Decision-making emerges from an optimization process that balances pragmatic exploitation (maximizing goal-directed utility) and epistemic exploration or information gain (actively resolving uncertainty). Actions are dynamically planned to minimize expected free energy, which quantifies both the divergence between predicted and preferred future observations, and the epistemic value of expected information gain about hidden states. This approach enables a new level of autonomy and resilience in digital twins, offering superior spontaneous exploration capabilities. The proposed framework is assessed on the health monitoring and predictive maintenance of a railway bridge.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Framework for Climate-Resilient Insurance and Real Estate Decisions</title>
<link>https://arxiv.org/abs/2506.14638</link>
<guid>https://arxiv.org/abs/2506.14638</guid>
<content:encoded><![CDATA[
<div> Keywords: Extreme weather events, Insurance viability, Building protection, Climate risks, Sustainability

Summary: 
The SSC-Insurance Model, combining SMOTE, SVM, and C-D-C algorithms, evaluates weather impacts on policies and investments, achieving high accuracy in Zhejiang and Ireland. It determines a critical threshold (43% weather increase) for insurance viability. The TOA-Preservation Model, utilizing TOPSIS-ORM and AHP, prioritizes building protection, with cultural value deemed most significant. Case studies on Nanxun Ancient Town reveal a 65.32% insurability probability and a protection score of 0.512. This research equips insurers, developers, and policymakers with practical tools to manage climate risks sustainably.<br /><br />Summary: <div>
arXiv:2506.14638v1 Announce Type: new 
Abstract: Extreme weather events increasingly threaten the insurance and real estate industries, creating conflicts between profitability and homeowner burdens. To address this, we propose the SSC-Insurance Model, which integrates SMOTE, SVM, and C-D-C algorithms to evaluate weather impacts on policies and investments. Our model achieves 88.3% accuracy in Zhejiang and 79.6% in Ireland, identifying a critical threshold (43% weather increase) for insurance viability. Additionally, we develop the TOA-Preservation Model using TOPSIS-ORM and AHP to prioritize building protection, with cultural value scoring highest (weight: 0.3383). Case studies on Nanxun Ancient Town show a 65.32% insurability probability and a protection score of 0.512. This work provides actionable tools for insurers, developers, and policymakers to manage climate risks sustainably.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimistic MEV in Ethereum Layer 2s: Why Blockspace Is Always in Demand</title>
<link>https://arxiv.org/abs/2506.14768</link>
<guid>https://arxiv.org/abs/2506.14768</guid>
<content:encoded><![CDATA[
<div> optimistic MEV, Layer 2 rollups, DeFi, on-chain arbitrage, Ethereum gas fees
<br />
Summary:
Layer 2 rollups in DeFi have absorbed over $40 billion and nearly half of Ethereum's DEX volume by Q1 2025, but their MEV dynamics are understudied. This study defines and quantifies optimistic MEV, a speculative on-chain arbitrage method prevalent on Arbitrum, Base, and Optimism. In Q1 2025, optimistic MEV accounted for over 50% of on-chain gas on Base and Optimism and 7% on Arbitrum, mainly driven by on-chain computations for arbitrage detection. Despite high gas consumption, optimistic MEV transactions pay only a fraction of total gas fees. Different success rates, code reuse patterns, and sensitivities to sequencing and block production times across networks were observed. OLS regressions linked optimistic MEV trades to ETH volatility, retail trading, and DEX aggregator usage, highlighting how Layer 2 protocols uniquely incentivize speculative MEV. 
<br /> <div>
arXiv:2506.14768v1 Announce Type: new 
Abstract: Layer 2 rollups are rapidly absorbing DeFi activity, securing over $40 billion and accounting for nearly half of Ethereum's DEX volume by Q1 2025, yet their MEV dynamics remain understudied. We address this gap by defining and quantifying optimistic MEV, a form of speculative, on-chain cyclic arbitrage whose detection and execution logic reside largely on-chain in smart contracts. As a result of their speculative nature and lack of off-chain opportunity verification, optimistic MEV transactions frequently fail to execute a profitable arbitrage.
  Applying our multi-stage identification pipeline to Arbitrum, Base, and Optimism, we find that in Q1 2025, optimistic MEV accounts for over 50% of on-chain gas on Base and Optimism and 7% on Arbitrum, driven mainly by "interaction" probes (on-chain computations searching for arbitrage). This speculative probing keeps blocks on Base and Optimism persistently full. Despite consuming over half of on-chain gas, optimistic MEV transactions pay less than one quarter of total gas fees. Cross-network comparison reveals divergent success rates, differing patterns of code reuse, and sensitivity to varying sequencer ordering and block production times. Finally, OLS regressions link optimistic MEV trade count to ETH volatility, retail trading activity, and DEX aggregator usage, showing how Layer 2 protocol parameters uniquely encourage speculative MEV.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Charging Scheduling via Balanced Bounding Box Methods</title>
<link>https://arxiv.org/abs/2506.14461</link>
<guid>https://arxiv.org/abs/2506.14461</guid>
<content:encoded><![CDATA[
<div> Keywords: Electric mobility, charging infrastructure, sustainable urban logistics, collaborative scheduling, bi-objective optimization

Summary:
The paper examines the challenges facing electric mobility and proposes shared charging as a solution. It focuses on sustainable urban logistics by facilitating collaboration between fleet operators. A bi-objective nonlinear integer programming model is formulated to address the scheduling problem for shared charging stations, balancing the cost minimization goals of each company. The Balanced Bounding Box Methods (B3Ms) are introduced to efficiently derive the efficient frontier, reducing computational time while maintaining solution diversity. Cooperative bargaining methods are applied to determine the final solution and promote balanced collaboration. Numerical case studies demonstrate the effectiveness and scalability of the developed methods, showcasing their potential for solving various bi-objective optimization problems beyond the collaborative scheduling issue presented in the study.<br /><br />Summary: <div>
arXiv:2506.14461v1 Announce Type: cross 
Abstract: Electric mobility faces several challenges, most notably the high cost of infrastructure development and the underutilization of charging stations. The concept of shared charging offers a promising solution. The paper explores sustainable urban logistics through horizontal collaboration between two fleet operators and addresses a scheduling problem for the shared use of charging stations. To tackle this, the study formulates a collaborative scheduling problem as a bi-objective nonlinear integer programming model, in which each company aims to minimize its own costs, creating inherent conflicts that require trade-offs. The Balanced Bounding Box Methods (B3Ms) are introduced in order to efficiently derive the efficient frontier, identifying a reduced set of representative solutions. These methods enhance computational efficiency by selectively disregarding closely positioned and competing solutions, preserving the diversity and representativeness of the solutions over the efficient frontier. To determine the final solution and ensure balanced collaboration, cooperative bargaining methods are applied. Numerical case studies demonstrate the viability and scalability of the developed methods, showing that the B3Ms can significantly reduce computational time while maintaining the integrity of the frontier. These methods, along with cooperative bargaining, provide an effective framework for solving various bi-objective optimization problems, extending beyond the collaborative scheduling problem presented here.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and scalable exchange-correlation with deep learning</title>
<link>https://arxiv.org/abs/2506.14665</link>
<guid>https://arxiv.org/abs/2506.14665</guid>
<content:encoded><![CDATA[
<div> machine learning, density functional theory, electronic structure, predictive modeling, atomization energies

Summary:
Skala is a new deep learning-based exchange-correlation (XC) functional designed to improve the accuracy and generality of Density Functional Theory (DFT) for predicting molecular and material properties. Unlike traditional XC functionals, Skala learns representations directly from data without relying on hand-crafted features. By training on a large dataset of high-accuracy reference data, Skala achieves chemical accuracy for atomization energies of small molecules while maintaining computational efficiency. It improves systematically with additional diverse chemistry training data, competing with hybrid functionals in general main group chemistry at the cost of semi-local DFT. Skala's performance is expected to further enhance the predictive power of first-principles simulations as the training dataset expands. 

<br /><br />Summary: <div>
arXiv:2506.14665v1 Announce Type: cross 
Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\"odinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative analysis for different finite element types in strain-gradient elasticity simulations performed on Firedrake and FEniCS</title>
<link>https://arxiv.org/abs/2411.12043</link>
<guid>https://arxiv.org/abs/2411.12043</guid>
<content:encoded><![CDATA[
<div> Keywords: additive manufacturing, metamaterials, finite element methods, strain-gradient elasticity, open-source packages<br />
<br />
Summary: 
This study focuses on the use of finite element methods to solve problems in strain-gradient elasticity in architectured materials such as metallic foams. The research compares different finite element formulations, including Lagrange, Argyris, Hermite elements, a mixed formulation, and isogeometric analysis with NURBS. Open-source packages from Firedrake and FEniCS are utilized for the study. The numerical investigation includes one- and two-dimensional problems commonly found in strain-gradient modeling literature. The developed codes are openly accessible to promote research in FEM-based computation of generalized continua. <div>
arXiv:2411.12043v2 Announce Type: replace 
Abstract: The layer-upon-layer approach in additive manufacturing, open or closed cells in polymeric or metallic foams involve an intrinsic microstructure tailored to the underlying applications. Homogenization of such architectured materials creates metamaterials modeled by higher-gradient models, specifically when the microstructure's characteristic length is comparable to the length scale of the structure. In this study, we conduct a comparative analysis of various finite elements methods for solving problems in strain-gradient elasticity. We employ open-source packages from Firedrake and FEniCS. Different finite element formulations are tested: we implement Lagrange, Argyris, Hermite elements, a Hu--Washizu type (mixed) formulation, as well as isogeometric analysis with Non-Uniform Rational B-Splines (NURBS). For the numerical study, we investigate one- and two-dimensional problems discussed in the literature of strain-gradient modeling. All developed codes are open-access to encourage research in Finite Element Method (FEM) based computation of generalized continua.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effect of Selection Format on LLM Performance</title>
<link>https://arxiv.org/abs/2503.06926</link>
<guid>https://arxiv.org/abs/2503.06926</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, classification task, prompts, formatting, performance <br />
Summary: <br />
This paper delves into the impact of different formatting options, specifically bullet points versus plain English, on the performance of large language models (LLMs) in classification tasks. The extensive experimental study conducted by the researchers revealed that presenting classification options via bullet points generally led to better model performance, although there were some exceptions to this trend. The study emphasizes the significance of carefully considering the formatting of options in prompts to enhance the overall performance of LLMs. The research further indicates the need for ongoing exploration and experimentation with various formatting techniques to continue driving improvements in model performance. The findings suggest that the presentation of classification task options can significantly influence the efficiency and effectiveness of LLMs, underscoring the importance of this aspect in the development and optimization of language models. <br /><br />Summary: <div>
arXiv:2503.06926v2 Announce Type: replace-cross 
Abstract: This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks</title>
<link>https://arxiv.org/abs/2503.16974</link>
<guid>https://arxiv.org/abs/2503.16974</guid>
<content:encoded><![CDATA[
<div> consistency, reproducibility, Large Language Model, finance, accounting

Summary:
This study evaluates the consistency and reproducibility of Large Language Models (LLMs) in finance and accounting research. Through extensive experimentation with three OpenAI models and 50 independent runs across five tasks, the study found task-dependent consistency with binary classification and sentiment analysis achieving near-perfect reproducibility. While more advanced models did not consistently demonstrate better consistency, task-specific patterns emerged. LLMs outperformed human annotators in consistency and agreement, even in cases where experts disagreed. Simple aggregation strategies across 3-5 runs improved consistency and accuracy for sentiment analysis. Despite some inconsistency in LLM outputs, downstream statistical inferences remained robust. The study addressed concerns of "G-hacking" by showing that risks of selective reporting from multiple LLM runs are relatively low for finance and accounting tasks. <div>
arXiv:2503.16974v3 Announce Type: replace-cross 
Abstract: This study provides the first comprehensive assessment of consistency and reproducibility in Large Language Model (LLM) outputs in finance and accounting research. We evaluate how consistently LLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs across five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models (GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse financial source texts and data, covering MD&amp;As, FOMC statements, finance news articles, earnings call transcripts, and financial statements. Our findings reveal substantial but task-dependent consistency, with binary classification and sentiment analysis achieving near-perfect reproducibility, while complex tasks show greater variability. More advanced models do not consistently demonstrate better consistency and reproducibility, with task-specific patterns emerging. LLMs significantly outperform expert human annotators in consistency and maintain high agreement even where human experts significantly disagree. We further find that simple aggregation strategies across 3-5 runs dramatically improve consistency. We also find that aggregation may come with an additional benefit of improved accuracy for sentiment analysis when using newer models. Simulation analysis reveals that despite measurable inconsistency in LLM outputs, downstream statistical inferences remain remarkably robust. These findings address concerns about what we term "G-hacking," the selective reporting of favorable outcomes from multiple Generative AI runs, by demonstrating that such risks are relatively low for finance and accounting tasks.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Characterization of Aggregate Flexibility via Generalized Polymatroids</title>
<link>https://arxiv.org/abs/2503.23458</link>
<guid>https://arxiv.org/abs/2503.23458</guid>
<content:encoded><![CDATA[
<div> flexibility, distributed energy resources, aggregation, Minkowski sum, optimization <br />
Summary: 
The article discusses the importance of leveraging the flexibility of distributed energy resources (DERs) to address issues related to renewable generation variability. It highlights the challenge of accurately computing the aggregate flexibility of a population, proposing the use of generalized polymatroids to efficiently represent flexibility sets. The study shows that individual flexibility sets can be categorized under this family, facilitating the exact computation of their Minkowski sum. For homogeneous DER populations, simplified representations of aggregate flexibility are derived. An optimization framework is developed to efficiently allocate aggregate flexibility among individual DERs, with a vertex-based disaggregation method proposed. The approach's optimality and computational efficiency are validated through comparisons with existing methods. <div>
arXiv:2503.23458v2 Announce Type: replace-cross 
Abstract: It is well established that the aggregate flexibility inherent in populations of distributed energy resources (DERs) can be leveraged to mitigate the intermittency and uncertainty associated with renewable generation, while also providing ancillary grid services. To enable this, aggregators must effectively represent the flexibility in the populations they control to the market or system operator. A key challenge is accurately computing the aggregate flexibility of a population, which can be formally expressed as the Minkowski sum of a collection of polytopes, a problem that is generally computationally intractable. However, the flexibility polytopes of many DERs exhibit structural symmetries that can be exploited for computational efficiency. To this end, we introduce generalized polymatroids, a family of polytopes, into the flexibility aggregation literature. We demonstrate that individual flexibility sets belong to this family, enabling efficient computation of their exact Minkowski sum. For homogeneous populations of DERs we further derive simplifications that yield more succinct representations of aggregate flexibility. Additionally, we develop an efficient optimization framework over these sets and propose a vertex-based disaggregation method, to allocate aggregate flexibility among individual DERs. Finally, we validate the optimality and computational efficiency of our approach through comparisons with existing methods.
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Classification of Levantine Ceramic Thin Sections via Neural Networks</title>
<link>https://arxiv.org/abs/2506.12250</link>
<guid>https://arxiv.org/abs/2506.12250</guid>
<content:encoded><![CDATA[
<div> deep learning, Convolutional Neural Networks, Vision Transformers, petrographic analysis, Levantine ceramics 

Summary:
Classification of ceramic thin sections is essential for understanding ancient pottery production techniques and trade networks. This study explores the use of deep learning models like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to classify Levantine ceramics based on their petrographic fabrics. A dataset of 1,424 thin section images from archaeological sites in the Levantine area was used to train these models, with transfer learning significantly improving classification accuracy. The ResNet18 model achieved 92.11% accuracy, while the ViT reached 88.34%. Explainability techniques such as Guided Grad-CAM and attention maps were used to interpret the models' decisions, showing that both CNNs and ViTs focus on key mineralogical features for classification. This study demonstrates the potential of explainable AI in archaeometric studies, providing an efficient and transparent methodology for ceramic analysis. 

<br /><br />Summary: <div>
arXiv:2506.12250v1 Announce Type: new 
Abstract: Classification of ceramic thin sections is fundamental for understanding ancient pottery production techniques, provenance, and trade networks. Although effective, traditional petrographic analysis is time-consuming. This study explores the application of deep learning models, specifically Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), as complementary tools to support the classification of Levantine ceramics based on their petrographic fabrics. A dataset of 1,424 thin section images from 178 ceramic samples belonging to several archaeological sites across the Levantine area, mostly from the Bronze Age, with few samples dating to the Iron Age, was used to train and evaluate these models. The results demonstrate that transfer learning significantly improves classification performance, with a ResNet18 model achieving 92.11% accuracy and a ViT reaching 88.34%. Explainability techniques, including Guided Grad-CAM and attention maps, were applied to interpret and visualize the models' decisions, revealing that both CNNs and ViTs successfully focus on key mineralogical features for the classification of the samples into their respective petrographic fabrics. These findings highlight the potential of explainable AI in archaeometric studies, providing a reproducible and efficient methodology for ceramic analysis while maintaining transparency in model decision-making.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A modified Newmark/Newton-Raphson method with automatic differentiation for general nonlinear dynamics analysis</title>
<link>https://arxiv.org/abs/2506.13226</link>
<guid>https://arxiv.org/abs/2506.13226</guid>
<content:encoded><![CDATA[
<div> automatic differentiation, Newmark/Newton-Raphson method, nonlinear dynamic systems, Jacobian matrix, complex nonlinear systems 

Summary: 
The study introduces the NNR-AD method, which integrates automatic differentiation into the Newmark/Newton-Raphson method to address limitations in solving complex nonlinear dynamic systems. By utilizing automatic differentiation, the NNR-AD method improves the NNR method's capability to handle complex nonlinear characteristics, simplifies the computation of Jacobian matrices, and enhances modularity for effective solutions. The NNR-AD method has been demonstrated to directly solve dynamic systems with complex nonlinear features, with rigorous validation of its accuracy and generality. This integration of automatic differentiation offers a significant advancement in solving complex nonlinear dynamic systems, providing a more efficient and effective approach compared to traditional methods. <br /><br /> <div>
arXiv:2506.13226v1 Announce Type: new 
Abstract: The Newmark/Newton-Raphson (NNR) method is widely employed for solving nonlinear dynamic systems. However, the current NNR method exhibits limited applicability in complex nonlinear dynamic systems, as the acquisition of the Jacobian matrix required for Newton iterations incurs substantial computational costs and may even prove intractable in certain cases. To address these limitations, we integrate automatic differentiation (AD) into the NNR method, proposing a modified NNR method with AD (NNR-AD) to significantly improve its capability for effectively handling complex nonlinear systems. We have demonstrated that the NNR-AD method can directly solve dynamic systems with complex nonlinear characteristics, and its accuracy and generality have been rigorously validated. Furthermore, automatic differentiation significantly simplifies the computation of Jacobian matrices for such complex nonlinear dynamic systems. This improvement endows the NNR method with enhanced modularity, thereby enabling convenient and effective solutions for complex nonlinear dynamic systems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Entropy-Stable/Double-Flux scheme for the multi-component compressible Navier-Stokes equations</title>
<link>https://arxiv.org/abs/2506.13231</link>
<guid>https://arxiv.org/abs/2506.13231</guid>
<content:encoded><![CDATA[
<div> Entropy-Stable Formulation, Double-Flux Scheme, Multi-Component Flows, Specific Heat Ratios, Compressible Flow<br />
Summary:<br />
The article introduces a novel numerical approach for improving multi-component compressible flow simulations. It combines an Entropy-Stable formulation and a Double-Flux scheme designed for multi-component flows. This approach ensures low-dissipation, oscillation-free solutions with enhanced stability compared to traditional methods. A hybrid dissipation strategy further improves robustness by blending the new approach with conventional dissipation mechanisms while maintaining consistency with the second law of thermodynamics. The method also utilizes an explicit Runge-Kutta scheme and adaptive mesh refinement for efficient time integration and capturing local flow features dynamically. Implemented in an existing compressible Navier-Stokes solver based on OpenFOAM, benchmark cases demonstrate the effectiveness of the framework in handling multi-dimensional interface and shock-interface interactions. The results confirm its favorable stability and robustness, making it a promising advancement for high-fidelity simulations of supersonic flows.<br /><br /> <div>
arXiv:2506.13231v1 Announce Type: new 
Abstract: We present a novel combination of numerical techniques to improve the efficiency, accuracy, and robustness of multi-component compressible flow simulations. At the core of our approach is an Entropy-Stable formulation that preserves kinetic energy and integrates a Double-Flux scheme tailored for multi-component flows with variable specific heat ratios. This formulation yields low-dissipation, oscillation-free solutions and enhances stability compared to standard fully conservative methods. To further improve robustness, we introduce a new hybrid dissipation strategy that blends the Entropy-Stable/Double-Flux approach with conventional dissipation mechanisms. We provide a rigorous proof that the resulting numerical flux satisfies a semi-discrete entropy inequality, ensuring consistency with the second law of thermodynamics. For time integration, we employ an explicit Runge-Kutta scheme in combination with adaptive mesh refinement to capture local flow features dynamically. The method is implemented within an existing compressible Navier-Stokes solver based on OpenFOAM. Benchmark cases, including multi-dimensional interface and shock-interface interactions, demonstrate the effectiveness of the proposed framework. The results confirm its favorable stability and robustness, validating the approach as a promising advancement for high-fidelity simulations of supersonic flows.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constitutive Manifold Neural Networks</title>
<link>https://arxiv.org/abs/2506.13648</link>
<guid>https://arxiv.org/abs/2506.13648</guid>
<content:encoded><![CDATA[
<div> neural networks, manifold learning, thermal conductivity, stochastic tensors, engineering applications  
Summary:  
Constitutive Manifold Neural Networks (CMNN) are introduced to address the challenge of accurately modeling stochastic tensor properties, such as thermal conductivity, in engineering applications. These properties, represented as symmetric positive definite tensors on a curved Riemannian manifold, require preservation of their geometric properties during neural network processing. By preprocessing the tensors to a flat vector space, CMNN ensures that the input to the neural network preserves the tensor's symmetry and spatial symmetries. A case study on stochastic anisotropic conductivity in a heat conduction problem demonstrates that CMNN outperforms traditional multi-layer perceptron architectures by preserving the geometric properties of the tensors. This highlights the importance of using manifold-aware techniques in engineering applications involving tensor-valued data. <div>
arXiv:2506.13648v1 Announce Type: new 
Abstract: Important material properties like thermal conductivity are often represented as symmetric positive definite (SPD) tensors, which exhibit variability due to inherent material heterogeneity and manufacturing uncertainties. These tensors reside on a curved Riemannian manifold, and accurately modeling their stochastic nature requires preserving both their symmetric positive definite properties and spatial symmetries. To achieve this, uncertainties are parametrized into scaling (magnitude) and rotation (orientation) components, modeled as independent random variables on a manifold structure derived from the maximum entropy principle. The propagation of such stochastic tensors through physics-based simulations necessitates computationally efficient surrogate models. However, traditional multi-layer perceptron (MLP) architectures are not well-suited for SPD tensors, as directly inputting their components fails to preserve their geometric properties, often leading to suboptimal results. To address this, we introduce Constitutive Manifold Neural Networks (CMNN). This approach introduces a preprocessing layer by mapping the SPD tensor from the curved manifold to the local tangent, a flat vector space, creating an information preserving map for input to the hidden layers of the neural networks. A case study on a steady-state heat conduction problem with stochastic anisotropic conductivity demonstrates that geometry-preserving preprocessing, such as logarithmic maps for scaling data, significantly improves learning performance over conventional MLPs. These findings underscore the importance of manifold-aware techniques when working with tensor-valued data in engineering applications.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Network for Gene Regulatory Network Inference</title>
<link>https://arxiv.org/abs/2506.13740</link>
<guid>https://arxiv.org/abs/2506.13740</guid>
<content:encoded><![CDATA[
<div> scKAN, gene regulatory networks, single-cell RNA sequencing, explainable AI, cellular dynamics<br />
Summary: <br />
The paper introduces scKAN, a novel model utilizing a Kolmogorov-Arnold network (KAN) with explainable AI to infer gene regulatory networks (GRNs) from single-cell RNA sequencing data. scKAN models gene expression as differentiable functions to accurately detect activation and inhibition regulations through explainable AI and geometric tools. It surpasses existing signed GRN inference models in AUROC and AUPRC metrics on the BEELINE benchmark, showcasing its potential in capturing biological processes in gene regulation without prior knowledge of the graph structure. The model addresses the challenges of high dimensionality and complexity in GRN inference from scRNA-seq data, offering improved scalability, explainability, and precision in detecting regulation types and capturing continuous cellular dynamics. <div>
arXiv:2506.13740v1 Announce Type: new 
Abstract: Gene regulation is central to understanding cellular processes and development, potentially leading to the discovery of new treatments for diseases and personalized medicine. Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data presents significant challenges due to its high dimensionality and complexity. Existing tree-based models, such as GENIE3 and GRNBOOST2, demonstrated scalability and explainability in GRN inference, but they cannot distinguish regulation types nor effectively capture continuous cellular dynamics. In this paper, we introduce scKAN, a novel model that employs a Kolmogorov-Arnold network (KAN) with explainable AI to infer GRNs from scRNA-seq data. By modeling gene expression as differentiable functions matching the smooth nature of cellular dynamics, scKAN can accurately and precisely detect activation and inhibition regulations through explainable AI and geometric tools. We conducted extensive experiments on the BEELINE benchmark, and scKAN surpasses and improves the leading signed GRN inference models ranging from 5.40\% to 28.37\% in AUROC and from 1.97\% to 40.45\% in AUPRC. These results highlight the potential of scKAN in capturing the underlying biological processes in gene regulation without prior knowledge of the graph structure.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Trust at Scale: Physics-Aware Neural Watermarking for Secure and Verifiable Data Pipelines</title>
<link>https://arxiv.org/abs/2506.12032</link>
<guid>https://arxiv.org/abs/2506.12032</guid>
<content:encoded><![CDATA[
<div> framework, neural watermarking, scientific data integrity, high-dimensional fields, climate modeling

Summary:
The article presents a robust neural watermarking framework designed specifically for scientific data integrity, focusing on high-dimensional fields typically found in climate modeling and fluid simulations. Utilizing a convolutional autoencoder, binary messages can be invisibly embedded into structured data like temperature, vorticity, and geopotential. The method ensures that the watermark persists even under lossy transformations such as noise injection, cropping, and compression while maintaining high fidelity with sub-1% Mean Squared Error (MSE). Compared to traditional singular value decomposition (SVD)-based watermarking, this approach achieves over 98% bit accuracy and provides visually indistinguishable reconstructions for datasets like ERA5 and Navier-Stokes. The system serves as a scalable and model-compatible tool for ensuring data provenance, auditability, and traceability in high-performance scientific workflows. It also contributes to the larger goal of securing AI systems through verifiable watermarking that is informed by physics. The evaluation was conducted on scientifically grounded datasets, demonstrating the frameworks adaptability to other structured domains like satellite imagery and autonomous vehicle perception streams. <br /><br />Summary: <div>
arXiv:2506.12032v1 Announce Type: cross 
Abstract: We present a robust neural watermarking framework for scientific data integrity, targeting high-dimensional fields common in climate modeling and fluid simulations. Using a convolutional autoencoder, binary messages are invisibly embedded into structured data such as temperature, vorticity, and geopotential. Our method ensures watermark persistence under lossy transformations - including noise injection, cropping, and compression - while maintaining near-original fidelity (sub-1\% MSE). Compared to classical singular value decomposition (SVD)-based watermarking, our approach achieves $>$98\% bit accuracy and visually indistinguishable reconstructions across ERA5 and Navier-Stokes datasets. This system offers a scalable, model-compatible tool for data provenance, auditability, and traceability in high-performance scientific workflows, and contributes to the broader goal of securing AI systems through verifiable, physics-aware watermarking. We evaluate on physically grounded scientific datasets as a representative stress-test; the framework extends naturally to other structured domains such as satellite imagery and autonomous-vehicle perception streams.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUST: Quantifying Free-Form Geometric Uncertainty of Metamaterials Using Small Data</title>
<link>https://arxiv.org/abs/2506.12051</link>
<guid>https://arxiv.org/abs/2506.12051</guid>
<content:encoded><![CDATA[
<div> Generative Uncertainty Learning, Self-supervised pretraining, Transfer learning, Geometric Uncertainties, Metamaterials <br />
Summary:
GUST framework uses deep generative models to quantify geometric uncertainties in metamaterial manufacturing. It leverages self-supervised pretraining on synthetic data to capture structure variability and a conditional distribution of fabricated geometries. Transfer learning fine-tunes the model on real-world data to adapt to specific processes. The approach requires only 960 manufactured unit cells for effective uncertainty quantification. Directly training on limited real-world data proves insufficient compared to GUST. This cost-effective method reduces data requirements while accurately modeling complex geometric uncertainties. The scalable approach benefits high-precision industries like aerospace and biomedical engineering by enabling the understanding and mitigation of manufacturing uncertainties. <br /> <div>
arXiv:2506.12051v1 Announce Type: cross 
Abstract: This paper introduces GUST (Generative Uncertainty learning via Self-supervised pretraining and Transfer learning), a framework for quantifying free-form geometric uncertainties inherent in the manufacturing of metamaterials. GUST leverages the representational power of deep generative models to learn a high-dimensional conditional distribution of as-fabricated unit cell geometries given nominal designs, thereby enabling uncertainty quantification. To address the scarcity of real-world manufacturing data, GUST employs a two-stage learning process. First, it leverages self-supervised pretraining on a large-scale synthetic dataset to capture the structure variability inherent in metamaterial geometries and an approximated distribution of as-fabricated geometries given nominal designs. Subsequently, GUST employs transfer learning by fine-tuning the pretrained model on limited real-world manufacturing data, allowing it to adapt to specific manufacturing processes and nominal designs. With only 960 unit cells additively manufactured in only two passes, GUST can capture the variability in geometry and effective material properties. In contrast, directly training a generative model on the same amount of real-world data proves insufficient, as demonstrated through both qualitative and quantitative comparisons. This scalable and cost-effective approach significantly reduces data requirements while maintaining the effectiveness in learning complex, real-world geometric uncertainties, offering an affordable method for free-form geometric uncertainty quantification in the manufacturing of metamaterials. The capabilities of GUST hold significant promise for high-precision industries such as aerospace and biomedical engineering, where understanding and mitigating manufacturing uncertainties are critical.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Green AI Architectures for Circular Economies Through Multi-Layered Sustainable Resource Optimization Framework</title>
<link>https://arxiv.org/abs/2506.12262</link>
<guid>https://arxiv.org/abs/2506.12262</guid>
<content:encoded><![CDATA[
<div> energy-efficient, Green AI, circular economies, sustainable resource consumption, optimization techniques

Summary:<br /><br />In this research paper, a new energy-efficient Green AI architecture is proposed to support circular economies and address sustainable resource consumption. The architecture integrates machine learning algorithms, energy-conscious models, and optimization techniques to facilitate decision-making for resource reuse and waste reduction. Real-world tests on lithium-ion battery recycling and urban waste management show a 25 percent reduction in energy consumption and an 18 percent improvement in resource recovery efficiency. Mathematical models and AI algorithms improve classification accuracy and reduce transportation emissions. Graphical analyses demonstrate the framework's impact on energy efficiency and sustainability, aligning with UN Sustainability Goals. This scalable solution can contribute to sustainable management strategies and technological progress, potentially safeguarding natural capital while advancing AI technologies. <br /> <div>
arXiv:2506.12262v1 Announce Type: cross 
Abstract: In this research paper, we propose a new type of energy-efficient Green AI architecture to support circular economies and address the contemporary challenge of sustainable resource consumption in modern systems. We introduce a multi-layered framework and meta-architecture that integrates state-of-the-art machine learning algorithms, energy-conscious computational models, and optimization techniques to facilitate decision-making for resource reuse, waste reduction, and sustainable production.We tested the framework on real-world datasets from lithium-ion battery recycling and urban waste management systems, demonstrating its practical applicability. Notably, the key findings of this study indicate a 25 percent reduction in energy consumption during workflows compared to traditional methods and an 18 percent improvement in resource recovery efficiency. Quantitative optimization was based on mathematical models such as mixed-integer linear programming and lifecycle assessments. Moreover, AI algorithms improved classification accuracy on urban waste by 20 percent, while optimized logistics reduced transportation emissions by 30 percent. We present graphical analyses and visualizations of the developed framework, illustrating its impact on energy efficiency and sustainability as reflected in the simulation results. This paper combines the principles of Green AI with practical insights into how such architectural models contribute to circular economies, presenting a fully scalable and scientifically rooted solution aligned with applicable UN Sustainability Goals worldwide. These results open avenues for incorporating newly developed AI technologies into sustainable management strategies, potentially safeguarding local natural capital while advancing technological progress.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Software Landscape for the Density Matrix Renormalization Group</title>
<link>https://arxiv.org/abs/2506.12629</link>
<guid>https://arxiv.org/abs/2506.12629</guid>
<content:encoded><![CDATA[
<div> Keywords: density matrix renormalization group, software landscape, modularization, standardization, collaboration

Summary: 
The article discusses the landscape of density matrix renormalization group (DMRG) software, highlighting 35 existing packages and their features. There is significant overlap in features among these packages, particularly in areas such as parallelism strategies and symmetry-adapted formulations. The lack of standard interfaces and modularity suggests a need for more collaboration, standardization, and modularization. The authors advocate for more cohesion and modularity in DMRG software to reduce duplication of efforts and improve interoperability. They believe that the challenges in the software landscape are more social than technical and aim to raise awareness among researchers and developers to encourage collaboration and optimization. Ultimately, greater cohesion and modularity in DMRG software would enhance its capabilities in tackling complex problems. 

<br /><br />Summary: <div>
arXiv:2506.12629v1 Announce Type: cross 
Abstract: The density matrix renormalization group (DMRG) algorithm is a cornerstone computational method for studying quantum many-body systems, renowned for its accuracy and adaptability. Despite DMRG's broad applicability across fields such as materials science, quantum chemistry, and quantum computing, numerous independent implementations have been developed. This survey maps the rapidly expanding DMRG software landscape, providing a comprehensive comparison of features among 35 existing packages. We found significant overlap in features among the packages when comparing key aspects, such as parallelism strategies for high-performance computing and symmetry-adapted formulations that enhance efficiency. This overlap suggests opportunities for modularization of common operations, including tensor operations, symmetry representations, and eigensolvers, as the packages are mostly independent and share few third-party library dependencies where functionality is factored out. More widespread modularization and standardization would result in reduced duplication of efforts and improved interoperability. We believe that the proliferation of packages and the current lack of standard interfaces and modularity are more social than technical. We aim to raise awareness of existing packages, guide researchers in finding a suitable package for their needs, and help developers identify opportunities for collaboration, modularity standardization, and optimization. Ultimately, this work emphasizes the value of greater cohesion and modularity, which would benefit DMRG software, allowing these powerful algorithms to tackle more complex and ambitious problems.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimLOB: Learning Representations of Limited Order Book for Financial Market Simulation</title>
<link>https://arxiv.org/abs/2406.19396</link>
<guid>https://arxiv.org/abs/2406.19396</guid>
<content:encoded><![CDATA[
<div> Transformer-based autoencoder, Limit Order Book, financial market simulation, calibration, representation learning <br />
<br />
Summary: <br />
Financial market simulation (FMS) is essential for understanding market anomalies and trading behaviors. Traditional calibration methods focused on mid-price data, resulting in biased models. This study introduces a Transformer-based autoencoder to learn vectorized representations of Limit Order Book (LOB) data, capturing market micro-structure. The learned latent representation preserves temporal auto-correlation and precedence between price levels in LOB. Experiment results demonstrate the effectiveness of the learned representation for downstream calibration tasks. This work advances FMS on LOB data, addressing the challenge of transforming LOB's tabular structure to a vectorized format for calibration purposes. <div>
arXiv:2406.19396v4 Announce Type: replace 
Abstract: Financial market simulation (FMS) serves as a promising tool for understanding market anomalies and the underlying trading behaviors. To ensure high-fidelity simulations, it is crucial to calibrate the FMS model for generating data closely resembling the observed market data. Previous efforts primarily focused on calibrating the mid-price data, leading to essential information loss of the market activities and thus biasing the calibrated model. The Limit Order Book (LOB) data is the fundamental data fully capturing the market micro-structure and is adopted by worldwide exchanges. However, LOB is not applicable to existing calibration objective functions due to its tabular structure not suitable for the vectorized input requirement. This paper proposes to explicitly learn the vectorized representations of LOB with a Transformer-based autoencoder. Then the latent vector, which captures the major information of LOB, can be applied for calibration. Extensive experiments show that the learned latent representation not only preserves the non-linear auto-correlation in the temporal axis, but the precedence between successive price levels of LOB. Besides, it is verified that the performance of the representation learning stage is consistent with the downstream calibration tasks. Thus, this work also progresses the FMS on LOB data, for the first time.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Interpretable Climate Emulators for Economics</title>
<link>https://arxiv.org/abs/2411.10768</link>
<guid>https://arxiv.org/abs/2411.10768</guid>
<content:encoded><![CDATA[
<div> framework, climate emulators, economic models, carbon-cycle, land-use change<br />
<br />
Summary: 
This paper introduces a framework for efficient and interpretable climate emulators for economic models of climate change. It proposes a generalized linear multi-reservoir model for constructing carbon-cycle emulators, customizable for specific applications. The paper evaluates three versions of the model within a representative agent economic model, finding that incorporating land-use change impacts significantly alters atmospheric carbon stocks, temperature trajectories, and optimal mitigation paths. Additionally, the study investigates pattern-scaling techniques to transform global-mean temperature projections into spatially heterogeneous warming fields and examines how regional baseline climates, non-uniform warming, and associated uncertainties influence economic damages. The findings highlight the importance of considering land-use change effects and spatial variations in climate projections for more accurate economic assessments of climate change impacts. <br /><br />Summary: <div>
arXiv:2411.10768v2 Announce Type: replace-cross 
Abstract: We introduce a framework for developing efficient and interpretable climate emulators (CEs) for economic models of climate change. The paper makes two main contributions. First, we propose a general framework for constructing carbon-cycle emulators (CCEs) for macroeconomic models. The framework is implemented as a generalized linear multi-reservoir (box) model that conserves key physical quantities and can be customized for specific applications. We consider three versions of the CCE, which we evaluate within a simple representative agent economic model: (i) a three-box setting comparable to DICE-2016, (ii) a four-box extension, and (iii) a four-box version that explicitly captures land-use change. While the three-box model reproduces benchmark results well and the fourth reservoir adds little, incorporating the impact of land-use change on the carbon storage capacity of the terrestrial biosphere substantially alters atmospheric carbon stocks, temperature trajectories, and the optimal mitigation path. Second, we investigate pattern-scaling techniques that transform global-mean temperature projections from CEs into spatially heterogeneous warming fields. We show how regional baseline climates, non-uniform warming, and the associated uncertainties propagate into economic damages.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LOB-Bench: Benchmarking Generative AI for Finance -- an Application to Limit Order Book Data</title>
<link>https://arxiv.org/abs/2502.09172</link>
<guid>https://arxiv.org/abs/2502.09172</guid>
<content:encoded><![CDATA[
<div> benchmark, financial data, generative models, limit order books, evaluation

Summary:
The article introduces LOB-Bench, a benchmarking tool developed in Python, to assess the quality of generative data for limit order books (LOB) in the finance sector. The tool aims to address the challenges posed by noisy and complex financial data by providing a quantitative evaluation framework. LOB-Bench compares generated and real LOB data using various statistical measures, including conditional and unconditional statistics, order book volumes, and order imbalance. The framework also incorporates scores from a discriminator network and market impact metrics, such as cross-correlations and price response functions. The study evaluates different generative models, including generative autoregressive state-space models and (C)GANs, and concludes that the autoregressive GenAI approach outperforms traditional model classes. This research contributes to advancing the field of financial sequence modeling by offering a standardized assessment tool for evaluating generative models in the context of limit order books. 

<br /><br />Summary: <div>
arXiv:2502.09172v2 Announce Type: replace-cross 
Abstract: While financial data presents one of the most challenging and interesting sequence modelling tasks due to high noise, heavy tails, and strategic interactions, progress in this area has been hindered by the lack of consensus on quantitative evaluation paradigms. To address this, we present LOB-Bench, a benchmark, implemented in python, designed to evaluate the quality and realism of generative message-by-order data for limit order books (LOB) in the LOBSTER format. Our framework measures distributional differences in conditional and unconditional statistics between generated and real LOB data, supporting flexible multivariate statistical evaluation. The benchmark also includes features commonly used LOB statistics such as spread, order book volumes, order imbalance, and message inter-arrival times, along with scores from a trained discriminator network. Lastly, LOB-Bench contains "market impact metrics", i.e. the cross-correlations and price response functions for specific events in the data. We benchmark generative autoregressive state-space models, a (C)GAN, as well as a parametric LOB model and find that the autoregressive GenAI approach beats traditional model classes.
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of physics-informed neural networks modeling time-harmonic wave fields</title>
<link>https://arxiv.org/abs/2506.11395</link>
<guid>https://arxiv.org/abs/2506.11395</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, Partial Differential Equations, Acoustic Wave Field, Room Acoustics <br />
Summary: <br />
- The study focuses on using physics-informed neural networks (PINNs) to model partial differential equations for solving the acoustic wave field.
- Initial results show promise for simple geometries in two-dimensional domains but face challenges in achieving convergence towards the accurate solution.
- Various factors including 3D dimensionality, realistic source modeling, Neumann boundary conditions, and complex solution quantities play a role in the optimization process.
- The research examines 3D room acoustic scenarios at low frequencies, testing different source definitions, boundary condition sets, and incorporating a complex speed of sound model.
- Convergence studies indicate the need for at least six training points per wavelength to achieve accurate training and predictions with the PINN architecture. This work contributes to the larger goal of modeling low-frequency room acoustics, including absorbers. <br /> <div>
arXiv:2506.11395v1 Announce Type: new 
Abstract: Studying physics-informed neural networks (PINNs) for modeling partial differential equations to solve the acoustic wave field has produced promising results for simple geometries in two-dimensional domains. One option is to compute the time-harmonic wave field using the Helmholtz equation. Compared to existing numerical models, the physics-informed neural networks forward problem has to overcome several topics related to the convergence of the optimization toward the "true" solution. The topics reach from considering the physical dimensionality (from 2D to 3D), the modeling of realistic sources (from a self-similar source to a realistic confined point source), the modeling of sound-hard (Neumann) boundary conditions, and the modeling of the full wave field by considering the complex solution quantities. Within this contribution, we study 3D room acoustic cases at low frequency, varying the source definition and the number of boundary condition sets and using a complex speed of sound model to account for some degree of absorption. We assess the convergence behavior by looking at the loss landscape of the PINN architecture, the $L^2$ error compared to a finite element reference simulation for each network architecture and configuration. The convergence studies showed that at least six training points per wavelength are necessary for accurate training and subsequent predictions of the PINN. The developments are part of an initiative aiming to model the low-frequency behavior of room acoustics, including absorbers.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAN-MI: A Scalable and Efficient Pipeline for Constructing High-Quality Neurodata in Motor Imagery Paradigm</title>
<link>https://arxiv.org/abs/2506.11830</link>
<guid>https://arxiv.org/abs/2506.11830</guid>
<content:encoded><![CDATA[
<div> Keyword: EEG signals, motor imagery, brain-computer interfaces, data construction, neurodata<br />
Summary:<br />
The article introduces CLEAN-MI, a data construction pipeline for motor imagery-based brain-computer interfaces. It addresses challenges such as low signal-to-noise ratio and inter-subject variability in EEG signals. CLEAN-MI combines frequency band filtering, channel template selection, subject screening, and marginal distribution alignment to enhance data quality and standardize multi-source EEG datasets. The pipeline was tested on various public MI datasets, showing consistent improvements in data quality and classification performance. This systematic approach aims to develop large-scale, efficient, and accurate neurodata for building robust and generalizable foundation models in BCI systems. <div>
arXiv:2506.11830v1 Announce Type: new 
Abstract: The construction of large-scale, high-quality datasets is a fundamental prerequisite for developing robust and generalizable foundation models in motor imagery (MI)-based brain-computer interfaces (BCIs). However, EEG signals collected from different subjects and devices are often plagued by low signal-to-noise ratio, heterogeneity in electrode configurations, and substantial inter-subject variability, posing significant challenges for effective model training. In this paper, we propose CLEAN-MI, a scalable and systematic data construction pipeline for constructing large-scale, efficient, and accurate neurodata in the MI paradigm. CLEAN-MI integrates frequency band filtering, channel template selection, subject screening, and marginal distribution alignment to systematically filter out irrelevant or low-quality data and standardize multi-source EEG datasets. We demonstrate the effectiveness of CLEAN-MI on multiple public MI datasets, achieving consistent improvements in data quality and classification performance.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effectiveness of the 'Follow-the-Sun' Strategy in Mitigating the Carbon Footprint of AI in Cloud Instances</title>
<link>https://arxiv.org/abs/2506.10990</link>
<guid>https://arxiv.org/abs/2506.10990</guid>
<content:encoded><![CDATA[
<div> carbon footprint, Follow-the-Sun, Artificial Intelligence, anomaly detection, experiment 

Summary:
- The 'Follow-the-Sun' (FtS) model aims to minimize the carbon footprint of computer workloads by dynamically relocating them to regions with cleaner energy sources.
- A study was conducted to assess the effectiveness of FtS in reducing the carbon emissions of AI training models, comparing it with two other strategies.
- Results from benchmarking four AI algorithms showed that the FtS strategy led to an average reduction of up to 14.6% in carbon emissions, with peaks of 16.3%.
- Additionally, FtS was found to help preserve the training time needed for AI models.
- Utilizing historical carbon intensity data from seven European cities in 2021, the experiment provided scientific evidence supporting the advantages of the FtS strategy in mitigating the carbon footprint of AI workloads.<br /><br />Summary: <div>
arXiv:2506.10990v1 Announce Type: cross 
Abstract: 'Follow-the-Sun' (FtS) is a theoretical computational model aimed at minimizing the carbon footprint of computer workloads. It involves dynamically moving workloads to regions with cleaner energy sources as demand increases and energy production relies more on fossil fuels. With the significant power consumption of Artificial Intelligence (AI) being a subject of extensive debate, FtS is proposed as a strategy to mitigate the carbon footprint of training AI models. However, the literature lacks scientific evidence on the advantages of FtS to mitigate the carbon footprint of AI workloads. In this paper, we present the results of an experiment conducted in a partial synthetic scenario to address this research gap. We benchmarked four AI algorithms in the anomaly detection domain and measured the differences in carbon emissions in four cases: no strategy, FtS, and two strategies previously introduced in the state of the art, namely Flexible Start and Pause and Resume. To conduct our experiment, we utilized historical carbon intensity data from the year 2021 for seven European cities. Our results demonstrate that the FtS strategy not only achieves average reductions of up to 14.6% in carbon emissions (with peaks of 16.3%) but also helps in preserving the time needed for training.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Attention-based Spatio-Temporal Neural Operator for Evolving Physics</title>
<link>https://arxiv.org/abs/2506.11328</link>
<guid>https://arxiv.org/abs/2506.11328</guid>
<content:encoded><![CDATA[
<div> machine learning, SciML, spatio-temporal, interpretability, ASNO <br />
Summary:<br /> 
The article introduces the Attention-based Spatio-Temporal Neural Operator (ASNO) to address challenges in scientific machine learning (SciML). ASNO combines separate attention mechanisms for spatial and temporal interactions to adapt to unknown physical parameters. Inspired by the backward differentiation formula, ASNO learns a transformer for temporal prediction and extrapolation and an attention-based neural operator for handling varying external loads. This enhances interpretability by isolating historical state contributions and external forces, enabling the discovery of underlying physical laws and generalizability to unseen environments. Empirical results on SciML benchmarks show that ASNO outperforms existing models, demonstrating its potential for engineering applications, physics discovery, and interpretable machine learning. <br /> <div>
arXiv:2506.11328v1 Announce Type: cross 
Abstract: In scientific machine learning (SciML), a key challenge is learning unknown, evolving physical processes and making predictions across spatio-temporal scales. For example, in real-world manufacturing problems like additive manufacturing, users adjust known machine settings while unknown environmental parameters simultaneously fluctuate. To make reliable predictions, it is desired for a model to not only capture long-range spatio-temporal interactions from data but also adapt to new and unknown environments; traditional machine learning models excel at the first task but often lack physical interpretability and struggle to generalize under varying environmental conditions. To tackle these challenges, we propose the Attention-based Spatio-Temporal Neural Operator (ASNO), a novel architecture that combines separable attention mechanisms for spatial and temporal interactions and adapts to unseen physical parameters. Inspired by the backward differentiation formula (BDF), ASNO learns a transformer for temporal prediction and extrapolation and an attention-based neural operator for handling varying external loads, enhancing interpretability by isolating historical state contributions and external forces, enabling the discovery of underlying physical laws and generalizability to unseen physical environments. Empirical results on SciML benchmarks demonstrate that ASNO outperforms over existing models, establishing its potential for engineering applications, physics discovery, and interpretable machine learning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design</title>
<link>https://arxiv.org/abs/2506.11420</link>
<guid>https://arxiv.org/abs/2506.11420</guid>
<content:encoded><![CDATA[
<div> protein-binding proteins, affinity, biomedical research, biotechnology, PPDiff<br />
<br />
Summary: 
Designing high-affinity protein-binding proteins is crucial in biomedical research and biotechnology. The PPDiff model, based on SSINC, is introduced to design binders for any protein target in a non-autoregressive manner. PPDiff integrates self-attention layers, graph layers, and causal attention layers to capture global amino acid correlations, local interactions, and simplify interdependencies within the protein sequence. PPDiff is evaluated on the PPBench dataset and achieves success rates of 50.00% for pretraining and 23.16% for target-protein mini-binder complex design. For antigen-antibody complex design, PPDiff achieves a success rate of 16.89%, outperforming baseline methods. This demonstrates the effectiveness of PPDiff in designing high-affinity binders for arbitrary protein targets without extensive wet-lab testing. <div>
arXiv:2506.11420v1 Announce Type: cross 
Abstract: Designing protein-binding proteins with high affinity is critical in biomedical research and biotechnology. Despite recent advancements targeting specific proteins, the ability to create high-affinity binders for arbitrary protein targets on demand, without extensive rounds of wet-lab testing, remains a significant challenge. Here, we introduce PPDiff, a diffusion model to jointly design the sequence and structure of binders for arbitrary protein targets in a non-autoregressive manner. PPDiffbuilds upon our developed Sequence Structure Interleaving Network with Causal attention layers (SSINC), which integrates interleaved self-attention layers to capture global amino acid correlations, k-nearest neighbor (kNN) equivariant graph layers to model local interactions in three-dimensional (3D) space, and causal attention layers to simplify the intricate interdependencies within the protein sequence. To assess PPDiff, we curate PPBench, a general protein-protein complex dataset comprising 706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on PPBenchand finetuned on two real-world applications: target-protein mini-binder complex design and antigen-antibody complex design. PPDiffconsistently surpasses baseline methods, achieving success rates of 50.00%, 23.16%, and 16.89% for the pretraining task and the two downstream applications, respectively.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the performance of multi-fidelity and reduced-dimensional neural emulators for inference of physiologic boundary conditions</title>
<link>https://arxiv.org/abs/2506.11683</link>
<guid>https://arxiv.org/abs/2506.11683</guid>
<content:encoded><![CDATA[
<div> Bayesian parameter estimation, cardiovascular modeling, surrogate modeling, high-fidelity simulations, computational cost <br />
Summary: 
This work focuses on solving inverse problems in cardiovascular modeling through Bayesian parameter estimation. It explores various methods to reduce computational costs by using low-fidelity approximations. Different approaches include constructing surrogates for high-fidelity simulations, building surrogates for the discrepancy between high and low-fidelity models, and treating the discrepancy as random noise to estimate its distribution. Five variations of these methods are validated on analytical test cases, comparing them to high-fidelity posterior distributions in terms of accuracy and computational cost. The approaches are then demonstrated on two cardiovascular examples: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy. The study showcases the effectiveness of leveraging low-fidelity approximations in reducing computational costs while maintaining accuracy in Bayesian parameter estimation for cardiovascular modeling. <br /> <div>
arXiv:2506.11683v1 Announce Type: cross 
Abstract: Solving inverse problems in cardiovascular modeling is particularly challenging due to the high computational cost of running high-fidelity simulations. In this work, we focus on Bayesian parameter estimation and explore different methods to reduce the computational cost of sampling from the posterior distribution by leveraging low-fidelity approximations. A common approach is to construct a surrogate model for the high-fidelity simulation itself. Another is to build a surrogate for the discrepancy between high- and low-fidelity models. This discrepancy, which is often easier to approximate, is modeled with either a fully connected neural network or a nonlinear dimensionality reduction technique that enables surrogate construction in a lower-dimensional space. A third possible approach is to treat the discrepancy between the high-fidelity and surrogate models as random noise and estimate its distribution using normalizing flows. This allows us to incorporate the approximation error into the Bayesian inverse problem by modifying the likelihood function. We validate five different methods which are variations of the above on analytical test cases by comparing them to posterior distributions derived solely from high-fidelity models, assessing both accuracy and computational cost. Finally, we demonstrate our approaches on two cardiovascular examples of increasing complexity: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Level set-based inverse homogenisation of three-dimensional piezoelectric materials</title>
<link>https://arxiv.org/abs/2410.03148</link>
<guid>https://arxiv.org/abs/2410.03148</guid>
<content:encoded><![CDATA[
<div> piezoelectric materials, topology optimisation, iterative solvers, metamaterials, level-set implementation <br />
Summary: 
This paper utilizes memory-distributed level set-based topology optimization to enhance the properties of three-dimensional periodic piezoelectric materials. Various iterative solvers are evaluated for their weak scalability, with the approximate Schur complement preconditioned generalized minimal residual method showing the best performance for solving piezoelectric homogenization equations. Through computational design, high-resolution piezoelectric metamaterials with improved stiffness and piezoelectric properties are created for sensor, hydrophone, and actuator applications. Two robust structures without fine-scale features are proposed, exhibiting significantly enhanced piezoelectric properties compared to the base material. The level-set approach proves effective in piezoelectricity problems by eliminating large regions of intermediate density material. An open-source memory-distributed level-set implementation is provided for the community practitioners to utilize. <br /><br />Summary: <div>
arXiv:2410.03148v3 Announce Type: replace 
Abstract: In this paper we use memory-distributed level set-based topology optimisation to design three-dimensional periodic piezoelectric materials with enhanced properties. We compare and assess several existing iterative solvers with respect to their weak scalability and find that an approximate Schur complement preconditioned generalized minimal residual method method demonstrates the best performance and scalability for solving the piezoelectric homogenisation equations. We use the developed techniques to computationally design high-resolution piezoelectric metamaterials with enhanced stiffness and piezoelectric properties that yield new insights into material design for sensor, hydrophone, and actuator applications. We suggest two robust structures with no fine-scale features that exhibit enhanced piezoelectric properties several times larger than those of the base material. We find that level set-based topology optimisation is well suited to problems involving piezoelectricity and has the advantage of avoiding large regions of intermediate density material. Our memory-distributed level-set implementation is open source and provided for practitioners in the community.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-directional Mapping of Morphology Metrics and 3D City Blocks for Enhanced Characterization and Generation of Urban Form</title>
<link>https://arxiv.org/abs/2412.15801</link>
<guid>https://arxiv.org/abs/2412.15801</guid>
<content:encoded><![CDATA[
<div> metrics, urban form, performance evaluation, morphology, sustainable urban design
Summary:
- The article discusses the importance of connecting morphology metrics with complex urban forms to enhance performance-driven computational urban design.
- It highlights the need for bi-directional mapping between metrics and urban forms to improve urban performance.
- An approach is presented to formulate metrics that can characterize urban forms and retrieve similar 3D forms.
- The methodology is demonstrated using 3D urban models of New York City, analyzing 14,248 blocks.
- Neural networks and information retrieval techniques are utilized for morphology metric encoding, urban form clustering, and evaluation.
<br /><br />Summary: <div>
arXiv:2412.15801v2 Announce Type: replace 
Abstract: Urban morphology, examining city spatial configurations, links urban design to sustainability. Morphology metrics play a fundamental role in performance-driven computational urban design (CUD) which integrates urban form generation, performance evaluation and optimization. However, a critical gap remains between performance evaluation and complex urban form generation, caused by the disconnection between morphology metrics and urban form, particularly in metric-to-form workflows. It prevents the application of optimized metrics to generate improved urban form with enhanced urban performance. Formulating morphology metrics that not only effectively characterize complex urban forms but also enable the reconstruction of diverse forms is of significant importance. This paper highlights the importance of establishing a bi-directional mapping between morphology metrics and complex urban form to enable the integration of urban form generation with performance evaluation. We present an approach that can 1) formulate morphology metrics to both characterize urban forms and in reverse, retrieve diverse similar 3D urban forms, and 2) evaluate the effectiveness of morphology metrics in representing 3D urban form characteristics of blocks by comparison. We demonstrate the methodology with 3D urban models of New York City, covering 14,248 blocks. We use neural networks and information retrieval for morphology metric encoding, urban form clustering and morphology metric evaluation. We identified an effective set of morphology metrics for characterizing block-scale urban forms through comparison. The proposed methodology tightly couples complex urban forms with morphology metrics, hence it can enable a seamless and bidirectional relationship between urban form generation and optimization in performance-driven urban design towards sustainable urban design and planning.
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring EEG Responses during Observation of Actions Performed by Human Actor and Humanoid Robot</title>
<link>https://arxiv.org/abs/2506.10170</link>
<guid>https://arxiv.org/abs/2506.10170</guid>
<content:encoded><![CDATA[
<div> action observation therapy humanoid robots EEG sensorimotor brain activity<br />
<br />
Summary: <br />
This pilot study investigated the potential of humanoid robots in supporting action observation therapy for motor and language function in neurological rehabilitation. Three healthy participants were monitored using EEG while observing actions performed by a human actor and a robot. Analysis of their brain activity revealed variability in ERSP patterns, including power suppression in sensorimotor mu and beta rhythms. One participant showed a stronger response to robot conditions compared to human conditions. Positive correlations in ERSP across all conditions implied common cognitive processes or neural networks in the mirror neuron system during action observation. Overall, the results support the feasibility of using EEG to explore differences in neural responses to observing robot- and human-induced actions. <div>
arXiv:2506.10170v1 Announce Type: new 
Abstract: Action observation (AO) therapy is a promising rehabilitative treatment for motor and language function in individuals recovering from neurological conditions, such as stroke. This pilot study aimed to investigate the potential of humanoid robots to support AO therapy in rehabilitation settings. The brain activity of three healthy right-handed participants was monitored with electroencephalography (EEG) while they observed eight different actions performed by two agents, a human actor and a robot, using their left and right arms. Their event-related spectral perturbations (ERSPs, changes in the spectral power of neural oscillations in response to an event or stimulus, compared to baseline) in sensorimotor regions were analyzed. The single-subject analysis showed variability in ERSP patterns among all participants, including power suppression in sensorimotor mu and beta rhythms. One participant showed stronger responses to "robot" AO conditions than to "human" conditions. Strong and positive correlations in ERSP across all conditions were observed for almost all participants and channels, implying common cognitive processes or neural networks at play in the mirror neuron system during AO. The results support the feasibility of using EEG to explore differences in neural responses to observation of robot- and human-induced actions.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable and flexible non-intrusive reduced-order models using reproducing kernel Hilbert spaces</title>
<link>https://arxiv.org/abs/2506.10224</link>
<guid>https://arxiv.org/abs/2506.10224</guid>
<content:encoded><![CDATA[
<div> Regularized kernel interpolation, reduced-order modeling, interpretable models, reproducing kernel Hilbert space, a posteriori error bound <br />
<br />
Summary: <br />
This paper introduces a novel non-intrusive reduced-order modeling technique using regularized kernel interpolation. Unlike traditional methods that rely on least-squares regression, this approach utilizes a reproducing kernel Hilbert space to capture the dynamics of a reduced-order model (ROM) in an interpretable manner. By incorporating carefully selected feature maps into the kernel, the resulting ROMs closely mimic the structure of the full-order model. The flexibility of the approach allows for the inclusion of both structured features and general nonlinear terms in the kernel, providing a comprehensive representation of the system dynamics. Additionally, the authors derive a computable a posteriori error bound that combines error estimates from traditional projection-based ROMs and kernel interpolants. Numerical experiments showcase the effectiveness of the approach compared to operator inference techniques using proper orthogonal decomposition and quadratic manifold dimension reduction. <div>
arXiv:2506.10224v1 Announce Type: new 
Abstract: This paper develops an interpretable, non-intrusive reduced-order modeling technique using regularized kernel interpolation. Existing non-intrusive approaches approximate the dynamics of a reduced-order model (ROM) by solving a data-driven least-squares regression problem for low-dimensional matrix operators. Our approach instead leverages regularized kernel interpolation, which yields an optimal approximation of the ROM dynamics from a user-defined reproducing kernel Hilbert space. We show that our kernel-based approach can produce interpretable ROMs whose structure mirrors full-order model structure by embedding judiciously chosen feature maps into the kernel. The approach is flexible and allows a combination of informed structure through feature maps and closure terms via more general nonlinear terms in the kernel. We also derive a computable a posteriori error bound that combines standard error estimates for intrusive projection-based ROMs and kernel interpolants. The approach is demonstrated in several numerical experiments that include comparisons to operator inference using both proper orthogonal decomposition and quadratic manifold dimension reduction.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment</title>
<link>https://arxiv.org/abs/2506.10711</link>
<guid>https://arxiv.org/abs/2506.10711</guid>
<content:encoded><![CDATA[
<div> refiner models, diffusion models, PDEs, spectral space, Kuramoto-Sivashinsky equation <br />
Summary: 
The article discusses the challenge of generating accurate and stable long rollouts for time-dependent Partial Differential Equations (PDEs). A refiner model called PDERefiner has been developed, utilizing diffusion models to refine outputs for each time step, with a focus on high-frequency accuracy. While this approach works well for some PDEs like the 1-D Kuramoto-Sivashinsky equation by degrading the amplitude of the high-frequency part, it may not be as effective for more complex cases like the Navier-Stokes equation. To address this, adjustments were made in the spectral space, leading to the development of PDE-SpectralRefiner with a v-prediction technique for Blurring diffusion models. This enhancement improved the accuracy of the outputs for both one-step Mean Squared Error (MSE) loss and rollout loss across different model backbones, such as U-Net and neural operators. <div>
arXiv:2506.10711v1 Announce Type: new 
Abstract: Generating accurate and stable long rollouts is a notorious challenge for time-dependent PDEs (Partial Differential Equations). Recently, motivated by the importance of high-frequency accuracy, a refiner model called PDERefiner utilizes diffusion models to refine outputs for every time step, since the denoising process could increase the correctness of modeling high frequency part. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the amplitude of high frequency part better than not doing refinement process. However, for some other cases, the spectrum might be more complicated. For example, for a harder PDE like Navior-Stokes equation, diffusion models could over-degrade the higher frequency part. This motivates us to release the constraint that each frequency weighs the same. We enhance our refiner model with doing adjustments on spectral space, which recovers Blurring diffusion models. We developed a new v-prediction technique for Blurring diffusion models, recovering the MSE training objective on the first refinement step. We show that in this case, for different model backbones, such as U-Net and neural operators, the outputs of PDE-SpectralRefiner are more accurate for both one-step MSE loss and rollout loss.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Analysis of Discretized Boundary Integral Operators in 3D: a High-Frequency Perspective</title>
<link>https://arxiv.org/abs/2506.10880</link>
<guid>https://arxiv.org/abs/2506.10880</guid>
<content:encoded><![CDATA[
<div> wavelength, integral equations, boundary element method, scattering phenomena, discretization<br />
Summary:<br />
The study explores the common practice of using integral equations discretized by the boundary element method to model propagation and scattering phenomena. It focuses on approximating the scatterer's boundary with mesh elements of size around a fraction of the incident wave's wavelength. By analyzing operator matrix spectra, the research reveals a discrepancy compared to continuous operators that increases as the simulation frequency rises. This challenges the conventional assumption that discretizing boundaries at a fraction of the wavelength, like /10, ensures constant solution accuracy at higher frequencies. <div>
arXiv:2506.10880v1 Announce Type: new 
Abstract: When modeling propagation and scattering phenomena using integral equations discretized by the boundary element method, it is common practice to approximate the boundary of the scatterer with a mesh comprising elements of size approximately equal to a fraction of the wavelength $\lambda$ of the incident wave, e.g., $\lambda/10$. In this work, by analyzing the spectra of the operator matrices, we show a discrepancy with respect to the continuous operators which grows with the simulation frequency, challenging the common belief that the aforementioned widely used discretization approach is sufficient to maintain the accuracy of the solution constant when increasing the frequency.
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Categorization with Relational Deep Learning in QuickBooks</title>
<link>https://arxiv.org/abs/2506.09234</link>
<guid>https://arxiv.org/abs/2506.09234</guid>
<content:encoded><![CDATA[
<div> Graph-based model, Rel-Cat, transaction categorization, relational database, link prediction <br />
<br />
Summary: 
The paper introduces a novel graph-based model, Rel-Cat, for automatic transaction categorization in QuickBooks. This model addresses challenges such as unique transaction descriptions and a wide variety of categories by formulating categorization as a link prediction task within a graph structure. By integrating natural language processing and graph machine learning techniques, Rel-Cat outperforms the existing QuickBooks model and scales effectively to a growing customer base. The model is designed to handle the cold start problem by adapting to minimal data, providing accurate categorization without compromising on accuracy. This innovative approach simplifies the architecture and enhances the customer experience by providing more accurate accounting and bookkeeping in QuickBooks. <div>
arXiv:2506.09234v1 Announce Type: new 
Abstract: Automatic transaction categorization is crucial for enhancing the customer experience in QuickBooks by providing accurate accounting and bookkeeping. The distinct challenges in this domain stem from the unique formatting of transaction descriptions, the wide variety of transaction categories, and the vast scale of the data involved. Furthermore, organizing transaction data in a relational database creates difficulties in developing a unified model that covers the entire database. In this work, we develop a novel graph-based model, named Rel-Cat, which is built directly over the relational database. We introduce a new formulation of transaction categorization as a link prediction task within this graph structure. By integrating techniques from natural language processing and graph machine learning, our model not only outperforms the existing production model in QuickBooks but also scales effectively to a growing customer base with a simpler, more effective architecture without compromising on accuracy. This design also helps tackle a key challenge of the cold start problem by adapting to minimal data.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Design Structure Matrix Optimization</title>
<link>https://arxiv.org/abs/2506.09749</link>
<guid>https://arxiv.org/abs/2506.09749</guid>
<content:encoded><![CDATA[
<div> machine learning, optimization, engineering design, design structure matrix, Large Language Models 

Summary:
Large Language Models (LLMs) show promise in solving combinatorial optimization (CO) problems in complex engineering systems by leveraging advanced reasoning and contextual understanding. This study proposes an LLM-based framework that combines network topology with domain knowledge to optimize Design Structure Matrix (DSM) element sequencing. Experiments demonstrate that this method achieves faster convergence and better solutions compared to traditional optimization methods. Incorporating contextual domain knowledge enhances optimization performance regardless of the specific LLM backbone used. The findings suggest that LLMs can effectively solve complex CO problems in engineering design by combining semantic and mathematical reasoning, paving the way for a new paradigm in LLM-based engineering design optimization. 

<br /><br />Summary: <div>
arXiv:2506.09749v1 Announce Type: new 
Abstract: In complex engineering systems, the interdependencies among components or development activities are often modeled and analyzed using Design Structure Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and enhance modularity or process efficiency constitutes a challenging combinatorial optimization (CO) problem in engineering design and operations. As problem sizes increase and dependency networks become more intricate, traditional optimization methods that solely use mathematical heuristics often fail to capture the contextual nuances and struggle to deliver effective solutions. In this study, we explore the potential of Large Language Models (LLMs) for helping solve such CO problems by leveraging their capabilities for advanced reasoning and contextual understanding. We propose a novel LLM-based framework that integrates network topology with contextual domain knowledge for iterative optimization of DSM element sequencing - a common CO problem. Experiments on various DSM cases show that our method consistently achieves faster convergence and superior solution quality compared to both stochastic and deterministic baselines. Notably, we find that incorporating contextual domain knowledge significantly enhances optimization performance regardless of the chosen LLM backbone. These findings highlight the potential of LLMs to solve complex engineering CO problems by combining semantic and mathematical reasoning. This approach paves the way towards a new paradigm in LLM-based engineering design optimization.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era</title>
<link>https://arxiv.org/abs/2506.09755</link>
<guid>https://arxiv.org/abs/2506.09755</guid>
<content:encoded><![CDATA[
<div> Keywords: Intelligent Design, Engineering innovation, Foundation Models, Multi-agent collaboration, Autonomous systems

Summary:<br />
The paper introduces Intelligent Design 4.0 (ID 4.0) as a new paradigm in engineering design, utilizing agentic AI systems. It discusses the historical evolution of Intelligent Design through different stages, leading to the emergence of multi-agent collaboration. ID 4.0 aims to automate engineering design processes through coordinated, autonomous multi-agent-based systems. The potential of ID 4.0 lies in supporting end-to-end automation, enhancing adaptivity, autonomy, and effectiveness in tackling complex design challenges. Future perspectives include addressing more complex design scenarios, practical design implementations, novel agent coordination mechanisms, and autonomous design goal-setting with improved human value alignment. The insights presented in the paper lay the groundwork for advancing Intelligent Design towards greater efficiency and innovation in engineering design.<br /><br />Summary: <div>
arXiv:2506.09755v1 Announce Type: new 
Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced engineering innovation, efficiency, quality, and productivity over recent decades, fundamentally reshaping how engineering designers think, behave, and interact with design processes. The recent emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), has demonstrated general knowledge-based reasoning capabilities, and open new paths and avenues for further transformation in engineering design. In this context, this paper introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by agentic AI systems. We review the historical evolution of ID across four distinct stages: rule-based expert systems, task-specific machine learning models, large-scale foundation AI models, and the recent emerging paradigm of multi-agent collaboration. We propose a conceptual framework for ID 4.0 and discuss its potential to support end-to-end automation of engineering design processes through coordinated, autonomous multi-agent-based systems. Furthermore, we discuss future perspectives to enhance and fully realize ID 4.0's potential, including more complex design scenarios, more practical design implementations, novel agent coordination mechanisms, and autonomous design goal-setting with better human value alignment. In sum, these insights lay a foundation for advancing Intelligent Design toward greater adaptivity, autonomy, and effectiveness in addressing increasingly complex design challenges.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superstudent intelligence in thermodynamics</title>
<link>https://arxiv.org/abs/2506.09822</link>
<guid>https://arxiv.org/abs/2506.09822</guid>
<content:encoded><![CDATA[
<div> OpenAI, o3, thermodynamics, exam, artificial intelligence <br />
<br />
Summary: OpenAI's language model o3 has surpassed university students in a thermodynamics exam, showcasing its ability to creatively apply principles. The exam is known for high failure rates and rare A-grades, requiring deep knowledge of thermodynamics. In a zero-shot mode, o3 outperformed students, achieving top scores in comparison to thousands of exams since 1985. This achievement signifies a shift in machine capabilities in complex tasks previously seen as exclusive to human intellect. The implications of this feat raise questions about the role of machines in engineering tasks and the future education of engineers. <div>
arXiv:2506.09822v1 Announce Type: new 
Abstract: In this short note, we report and analyze a striking event: OpenAI's large language model o3 has outwitted all students in a university exam on thermodynamics. The thermodynamics exam is a difficult hurdle for most students, where they must show that they have mastered the fundamentals of this important topic. Consequently, the failure rates are very high, A-grades are rare - and they are considered proof of the students' exceptional intellectual abilities. This is because pattern learning does not help in the exam. The problems can only be solved by knowledgeably and creatively combining principles of thermodynamics. We have given our latest thermodynamics exam not only to the students but also to OpenAI's most powerful reasoning model, o3, and have assessed the answers of o3 exactly the same way as those of the students. In zero-shot mode, the model o3 solved all problems correctly, better than all students who took the exam; its overall score was in the range of the best scores we have seen in more than 10,000 similar exams since 1985. This is a turning point: machines now excel in complex tasks, usually taken as proof of human intellectual capabilities. We discuss the consequences this has for the work of engineers and the education of future engineers.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Guided Ligand-Binding Protein Design</title>
<link>https://arxiv.org/abs/2506.09332</link>
<guid>https://arxiv.org/abs/2506.09332</guid>
<content:encoded><![CDATA[
<div> Keywords: AI protein models, protein design, ligand binding, natural language instructions, machine learning

Summary: In this paper, the authors introduce InstructPro, a family of AI protein generative models trained to design proteins that bind to specific ligands using human language instructions. The models, InstructPro-1B and InstructPro-3B, outperform existing baselines in generating ligand-binding proteins. InstructPro-1B achieves an impressive docking success rate of 81.52% and an average RMSD of 4.026 compared to ground truth structures. InstructPro-3B further improves the average RMSD to 2.527, demonstrating the models' capability to accurately design proteins based on textual descriptions and ligand formulas. The dataset InstructProBench, containing over 9 million triples of function descriptions, ligand formulas, and protein sequences, supports the training and evaluation of the models. These results suggest that AI protein models can effectively follow natural language instructions to design proteins with desired functions, offering a promising approach for protein engineering in various fields. 

<br /><br />Summary: <div>
arXiv:2506.09332v1 Announce Type: cross 
Abstract: Can AI protein models follow human language instructions and design proteins with desired functions (e.g. binding to a ligand)? Designing proteins that bind to a given ligand is crucial in a wide range of applications in biology and chemistry. Most prior AI models are trained on protein-ligand complex data, which is scarce due to the high cost and time requirements of laboratory experiments. In contrast, there is a substantial body of human-curated text descriptions about protein-ligand interactions and ligand formula. In this paper, we propose InstructPro, a family of protein generative models that follow natural language instructions to design ligand-binding proteins. Given a textual description of the desired function and a ligand formula in SMILES, InstructPro generates protein sequences that are functionally consistent with the specified instructions. We develop the model architecture, training strategy, and a large-scale dataset, InstructProBench, to support both training and evaluation. InstructProBench consists of 9,592,829 triples of (function description, ligand formula, protein sequence). We train two model variants: InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion parameters). Both variants consistently outperform strong baselines, including ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking success rate (81.52% at moderate confidence) and the lowest average root mean square deviation (RMSD) compared to ground truth structures (4.026{\AA}). InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating InstructPro's ability to generate ligand-binding proteins that align with the functional specifications.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Climate Emulation with Bayesian Filtering</title>
<link>https://arxiv.org/abs/2506.09891</link>
<guid>https://arxiv.org/abs/2506.09891</guid>
<content:encoded><![CDATA[
<div> climate change, machine learning, emulator, physics-informed, causal relationships

Summary: 
This study introduces a novel approach to climate modeling using machine learning techniques. Traditional climate models are computationally expensive due to their complex systems of equations. The proposed emulator leverages causal representation learning to incorporate physics-informed causal relationships, leading to more accurate climate dynamics predictions. A Bayesian filter ensures stable long-term autoregressive emulation. The emulator is shown to accurately capture climate dynamics on both synthetic and real-world climate model data. This innovative approach opens up new possibilities for quicker and more efficient simulations of climate change dynamics, allowing for improved predictions and analyses of its causes and impacts. <div>
arXiv:2506.09891v1 Announce Type: cross 
Abstract: Traditional models of climate change use complex systems of coupled equations to simulate physical processes across the Earth system. These simulations are highly computationally expensive, limiting our predictions of climate change and analyses of its causes and effects. Machine learning has the potential to quickly emulate data from climate models, but current approaches are not able to incorporate physics-informed causal relationships. Here, we develop an interpretable climate model emulator based on causal representation learning. We derive a physics-informed approach including a Bayesian filter for stable long-term autoregressive emulation. We demonstrate that our emulator learns accurate climate dynamics, and we show the importance of each one of its components on a realistic synthetic dataset and data from two widely deployed climate models.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Note on the Reliability of Goal-Oriented Error Estimates for Galerkin Finite Element Methods with Nonlinear Functionals</title>
<link>https://arxiv.org/abs/2506.09913</link>
<guid>https://arxiv.org/abs/2506.09913</guid>
<content:encoded><![CDATA[
<div> variational problem; Galerkin finite element method; discretization error; nonlinear functionals; error estimates <br />
<br />Summary: 
The study focuses on estimating discretization errors in nonlinear functionals within a variational problem solved using the Galerkin finite element method. It examines error estimates in the form of $J(u) - J(u_h) \approx \eta = L(z) - B(u_h, z)$. The research reveals instances where these error estimates are unreliable, even with an exact adjoint solution. Reliability is defined by the existence of a constant $C$ such that $|J(u) - J(u_h)| \leq C|\eta|$, and multiple examples are provided where this criterion is not met. The analysis highlights the challenges in ensuring the accuracy of error estimates in nonlinear functionals, shedding light on the complexities involved in such estimations. <div>
arXiv:2506.09913v1 Announce Type: cross 
Abstract: We consider estimating the discretization error in a nonlinear functional $J(u)$ in the setting of an abstract variational problem: find $u \in \mathcal{V}$ such that $B(u,\varphi) = L(\varphi) \; \forall \varphi \in \mathcal{V}$, as approximated by a Galerkin finite element method. Here, $\mathcal{V}$ is a Hilbert space, $B(\cdot,\cdot)$ is a bilinear form, and $L(\cdot)$ is a linear functional. We consider well-known error estimates $\eta$ of the form $J(u) - J(u_h) \approx \eta = L(z) - B(u_h, z)$, where $u_h$ denotes a finite element approximation to $u$, and $z$ denotes the solution to an auxiliary adjoint variational problem. We show that there exist nonlinear functionals for which error estimates of this form are not reliable, even in the presence of an exact adjoint solution solution $z$. An estimate $\eta$ is said to be reliable if there exists a constant $C \in \mathbb{R}_{>0}$ independent of $u_h$ such that $|J(u) - J(u_h)| \leq C|\eta|$. We present several example pairs of bilinear forms and nonlinear functionals where reliability of $\eta$ is not achieved.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks</title>
<link>https://arxiv.org/abs/2506.08563</link>
<guid>https://arxiv.org/abs/2506.08563</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Physics Informed Neural Networks, Differential Equations, Kernel Packet, RKHS norm

Summary:<br /> 
1. Differential equations play a crucial role in engineering modeling, and the Physics Informed Neural Networks (PINNs) framework has been utilized to solve complex equations efficiently.
2. The default L2 loss function in PINNs may lead to incorrect and unstable solutions for some complex equations.
3. A new framework called Kernel Packet accelerated PINNs (KP-PINNs) is introduced, which uses the reproducing kernel Hilbert space (RKHS) norm and the Kernel Packet (KP) method to enhance computation speed.
4. Theoretical analysis confirms that KP-PINNs are stable across various differential equations.
5. Numerical experiments demonstrate the effectiveness and efficiency of KP-PINNs in solving differential equations, offering a promising advancement for enhancing the stability and accuracy of PINNs-based solvers in scientific computing.<br /><br /> <div>
arXiv:2506.08563v1 Announce Type: new 
Abstract: Differential equations are involved in modeling many engineering problems. Many efforts have been devoted to solving differential equations. Due to the flexibility of neural networks, Physics Informed Neural Networks (PINNs) have recently been proposed to solve complex differential equations and have demonstrated superior performance in many applications. While the L2 loss function is usually a default choice in PINNs, it has been shown that the corresponding numerical solution is incorrect and unstable for some complex equations. In this work, we propose a new PINNs framework named Kernel Packet accelerated PINNs (KP-PINNs), which gives a new expression of the loss function using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel Packet (KP) method to accelerate the computation. Theoretical results show that KP-PINNs can be stable across various differential equations. Numerical experiments illustrate that KP-PINNs can solve differential equations effectively and efficiently. This framework provides a promising direction for improving the stability and accuracy of PINNs-based solvers in scientific computing.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid cardiac activation prediction for cardiac resynchronization therapy planning using geometric deep learning</title>
<link>https://arxiv.org/abs/2506.08987</link>
<guid>https://arxiv.org/abs/2506.08987</guid>
<content:encoded><![CDATA[
<div> deep learning, cardiac resynchronization therapy, prediction, optimization, in-silico modeling

Summary: 
- The study focuses on predicting cardiac activation time map for cardiac resynchronization therapy (CRT) using geometric deep learning models.
- Two models, GNN and GINO, were developed and trained on a synthetic dataset to predict activation time maps in real-time.
- The GINO model outperformed the GNN model, showing lower prediction errors and better robustness.
- A workflow for optimizing the pacing site in CRT using the GINO model was developed, resulting in a significant reduction in maximum activation time compared to random selection.
- An interactive web-based GUI was also developed to utilize the GINO model as a clinical decision-support tool for personalized CRT optimization. 

<br /><br />Summary: <div>
arXiv:2506.08987v1 Announce Type: new 
Abstract: Cardiac resynchronization therapy (CRT) is a common intervention for patients with dyssynchronous heart failure, yet approximately one-third of recipients fail to respond due to suboptimal lead placement. Identifying optimal pacing sites remains challenging, largely due to patient-specific anatomical variability and the limitations of current individualized planning strategies. In a step towards constructing an in-silico approach to help address this issue, we develop two geometric deep learning (DL) models, based on graph neural network (GNN) and geometry-informed neural operator (GINO), to predict cardiac activation time map in real-time for CRT planning and optimization. Both models are trained on a large synthetic dataset generated from finite-element (FE) simulations over a wide range of left ventricular (LV) geometries, pacing site configurations, and tissue conductivities. The GINO model significantly outperforms the GNN model, with lower prediction errors (1.14% vs 3.14%) and superior robustness to noise and various mesh discretization. Using the GINO model, we also develop a workflow for optimizing the pacing site in CRT from given activation time map and LV geometry. Compared to randomly selecting a pacing site, the CRT optimization workflow produces a larger reduction in maximum activation time (20% vs. 8%). In conjunction with an interactive web-based graphical user interface (GUI) available at https://dcsim.egr.msu.edu/, the GINO model shows promising potential as a clinical decision-support tool for personalized pre-procedural CRT optimization.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Proteins and Language: A Foundation Model for Protein Retrieval</title>
<link>https://arxiv.org/abs/2506.08023</link>
<guid>https://arxiv.org/abs/2506.08023</guid>
<content:encoded><![CDATA[
<div> **Keywords:** protein structures, functional interpretation, cryo-Electron Microscopy, vision-language models, contrastive learning

Summary:
This paper introduces a framework inspired by CLIP-style models to align 3D protein structures with functional annotations using contrastive learning. A dataset of 200,000 protein-caption pairs with detailed functional descriptors is created for model training. The model is evaluated on both in-domain and cross-database retrieval tasks using Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) datasets. Promising zero-shot retrieval performance is demonstrated, showcasing the potential of multimodal foundation models for enhancing our understanding of protein structure-function relationships in biology.<br /><br />Summary: <div>
arXiv:2506.08023v1 Announce Type: cross 
Abstract: This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset, facilitating the functional interpretation of protein structures derived by structural determination methods like cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of vision-language models (VLMs), we propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning. For model training, we propose a large-scale dataset of approximately 200,000 protein-caption pairs with rich functional descriptors. We evaluate our model in both in-domain and more challenging cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In both cases, our approach demonstrates promising zero-shot retrieval performance, highlighting the potential of multimodal foundation models for structure-function understanding in protein biology.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts</title>
<link>https://arxiv.org/abs/2506.08205</link>
<guid>https://arxiv.org/abs/2506.08205</guid>
<content:encoded><![CDATA[
<div> simulation, residual stress, machine learning, U-Net architecture, experimental data

Summary:
- Residual stresses can affect component performance, and accurately determining their distributions is crucial.
- The proposed Residual Stress Generator (RSG) uses machine learning to infer full-field stresses from limited measurements.
- An extensive dataset was created through process simulations with varied parameters.
- The RSG, based on U-Net architecture, showed excellent predictive accuracy and generalization in generating simulated stresses.
- The RSG successfully learned the latent structure of residual stress distribution and reduced the need for extensive experimental efforts.
- Testing on actual characterization data validated the RSG's effectiveness in predicting experimentally measured residual stresses. 

<br /><br />Summary: <div>
arXiv:2506.08205v1 Announce Type: cross 
Abstract: Residual stresses, which remain within a component after processing, can deteriorate performance. Accurately determining their full-field distributions is essential for optimizing the structural integrity and longevity. However, the experimental effort required for full-field characterization is impractical. Given these challenges, this work proposes a machine learning (ML) based Residual Stress Generator (RSG) to infer full-field stresses from limited measurements. An extensive dataset was initially constructed by performing numerous process simulations with a diverse parameter set. A ML model based on U-Net architecture was then trained to learn the underlying structure through systematic hyperparameter tuning. Then, the model's ability to generate simulated stresses was evaluated, and it was ultimately tested on actual characterization data to validate its effectiveness. The model's prediction of simulated stresses shows that it achieved excellent predictive accuracy and exhibited a significant degree of generalization, indicating that it successfully learnt the latent structure of residual stress distribution. The RSG's performance in predicting experimentally characterized data highlights the feasibility of the proposed approach in providing a comprehensive understanding of residual stress distributions from limited measurements, thereby significantly reducing experimental efforts.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems</title>
<link>https://arxiv.org/abs/2506.08475</link>
<guid>https://arxiv.org/abs/2506.08475</guid>
<content:encoded><![CDATA[
<div> autoencoders, dimensionality reduction, parametric neural networks, reduced-order modeling, thermodynamics

Summary:
The article introduces a novel framework, tLaSDI, for efficient identification of latent space dynamics in parametric nonlinear dynamical systems. By combining autoencoders for dimensionality reduction with parametric neural networks informed by thermodynamic principles, the framework can accurately learn parametric latent dynamics while maintaining important thermodynamic laws. The inclusion of a physics-informed active learning strategy further enhances model performance by adaptively sampling informative training data. Numerical experiments on various equations demonstrate the framework's superior speed and accuracy compared to traditional methods, with significant reductions in training and inference costs. The learned latent space dynamics not only provide accurate representations of physical-space dynamics but also offer valuable insights into the underlying thermodynamic behavior of the system.<br /><br />Summary: <div>
arXiv:2506.08475v1 Announce Type: cross 
Abstract: We propose an efficient thermodynamics-informed latent space dynamics identification (tLaSDI) framework for the reduced-order modeling of parametric nonlinear dynamical systems. This framework integrates autoencoders for dimensionality reduction with newly developed parametric GENERIC formalism-informed neural networks (pGFINNs), which enable efficient learning of parametric latent dynamics while preserving key thermodynamic principles such as free energy conservation and entropy generation across the parameter space. To further enhance model performance, a physics-informed active learning strategy is incorporated, leveraging a greedy, residual-based error indicator to adaptively sample informative training data, outperforming uniform sampling at equivalent computational cost. Numerical experiments on the Burgers' equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed method achieves up to 3,528x speed-up with 1-3% relative errors, and significant reduction in training (50-90%) and inference (57-61%) cost. Moreover, the learned latent space dynamics reveal the underlying thermodynamic behavior of the system, offering valuable insights into the physical-space dynamics.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation</title>
<link>https://arxiv.org/abs/2506.08604</link>
<guid>https://arxiv.org/abs/2506.08604</guid>
<content:encoded><![CDATA[
<div> Keywords: generative machine learning, physics-based flow matching, PDE problems, surrogate modeling, uncertainty quantification

Summary: 
Physics-Based Flow Matching (PBFM) is a novel generative framework that incorporates physical constraints, such as PDE residuals and algebraic relations, into the flow matching objective. This method explicitly embeds physics knowledge into the learning process, resulting in improved accuracy in noise-free sample prediction. The inclusion of temporal unrolling during training further enhances prediction accuracy. PBFM minimizes both flow matching loss and physics-based residual loss simultaneously without the need for hyperparameter tuning. By analyzing the role of minimum noise level and implementing a stochastic sampling strategy, physical residuals can be reduced. Benchmarking on three PDE problems demonstrates that PBFM outperforms existing algorithms by up to 8 times in physical residual accuracy and distributional accuracy. This approach offers a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.<br /><br />Summary: <div>
arXiv:2506.08604v1 Announce Type: cross 
Abstract: Generative machine learning methods, such as diffusion models and flow matching, have shown great potential in modeling complex system behaviors and building efficient surrogate models. However, these methods typically learn the underlying physics implicitly from data. We propose Physics-Based Flow Matching (PBFM), a novel generative framework that explicitly embeds physical constraints, both PDE residuals and algebraic relations, into the flow matching objective. We also introduce temporal unrolling at training time that improves the accuracy of the final, noise-free sample prediction. Our method jointly minimizes the flow matching loss and the physics-based residual loss without requiring hyperparameter tuning of their relative weights. Additionally, we analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of physical constraints and evaluate a stochastic sampling strategy that helps to reduce physical residuals. Through extensive benchmarks on three representative PDE problems, we show that our approach yields up to an $8\times$ more accurate physical residuals compared to FM, while clearly outperforming existing algorithms in terms of distributional accuracy. PBFM thus provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements</title>
<link>https://arxiv.org/abs/2506.08762</link>
<guid>https://arxiv.org/abs/2506.08762</guid>
<content:encoded><![CDATA[
<div> Benchmark, Japanese financial data, large language model, accounting fraud detection, earnings forecasting

Summary:
The article introduces EDINET-Bench, an open-source Japanese financial benchmark aiming to evaluate the performance of large language models (LLMs) in challenging financial tasks such as accounting fraud detection and earnings forecasting using data from Japan's EDINET. The experiments show that even state-of-the-art LLMs struggle in binary classification for fraud detection and earnings forecasting, only performing slightly better than logistic regression. This indicates significant challenges in applying LLMs to real-world financial tasks and highlights the need for domain-specific adaptation. The dataset, benchmark construction code, and evaluation code are publicly available to support further research in utilizing LLMs for financial analysis.<br /><br />Summary: <div>
arXiv:2506.08762v1 Announce Type: cross 
Abstract: Financial analysis presents complex challenges that could leverage large language model (LLM) capabilities. However, the scarcity of challenging financial datasets, particularly for Japanese financial data, impedes academic innovation in financial analytics. As LLMs advance, this lack of accessible research resources increasingly hinders their development and evaluation in this specialized domain. To address this gap, we introduce EDINET-Bench, an open-source Japanese financial benchmark designed to evaluate the performance of LLMs on challenging financial tasks including accounting fraud detection, earnings forecasting, and industry prediction. EDINET-Bench is constructed by downloading annual reports from the past 10 years from Japan's Electronic Disclosure for Investors' NETwork (EDINET) and automatically assigning labels corresponding to each evaluation task. Our experiments reveal that even state-of-the-art LLMs struggle, performing only slightly better than logistic regression in binary classification for fraud detection and earnings forecasting. These results highlight significant challenges in applying LLMs to real-world financial applications and underscore the need for domain-specific adaptation. Our dataset, benchmark construction code, and evaluation code is publicly available to facilitate future research in finance with LLMs.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)</title>
<link>https://arxiv.org/abs/2506.08844</link>
<guid>https://arxiv.org/abs/2506.08844</guid>
<content:encoded><![CDATA[
<div> dataset, missing data imputation, socioeconomic research, benchmark, IMAGIC-500

Summary:
- The study focuses on missing data imputation in socioeconomic datasets, which are often restricted due to privacy concerns.
- The authors introduce the IMAGIC-500 dataset derived from the World Bank's synthetic dataset, allowing for broad access.
- A comprehensive missing data imputation benchmark is conducted on IMAGIC-500 with various missing mechanisms and ratios.
- Evaluation criteria include imputation accuracy, computational efficiency, and impact on predictive tasks like estimating educational attainment.
- Results assess statistical, traditional machine learning, deep learning, and diffusion-based imputation methods, aiming to improve algorithm development and reproducibility in social science research. 

<br /><br />Summary: <div>
arXiv:2506.08844v1 Announce Type: cross 
Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in data science and machine learning, particularly within socioeconomic research. However, real-world socioeconomic datasets are typically subject to strict data protection protocols, which often prohibit public sharing, even for synthetic derivatives. This severely limits the reproducibility and accessibility of benchmark studies in such settings. Further, there are very few publicly available synthetic datasets. Thus, there is limited availability of benchmarks for systematic evaluation of imputation methods on socioeconomic datasets, whether real or synthetic. In this study, we utilize the World Bank's publicly available synthetic dataset, Synthetic Data for an Imaginary Country, which closely mimics a real World Bank household survey while being fully public, enabling broad access for methodological research. With this as a starting point, we derived the IMAGIC-500 dataset: we select a subset of 500k individuals across approximately 100k households with 19 socioeconomic features, designed to reflect the hierarchical structure of real-world household surveys. This paper introduces a comprehensive missing data imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR, MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation considers the imputation accuracy for continuous and categorical variables, computational efficiency, and impact on downstream predictive tasks, such as estimating educational attainment at the individual level. The results highlight the strengths and weaknesses of statistical, traditional machine learning, and deep learning imputation techniques, including recent diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate the development of robust imputation algorithms and foster reproducible social science research.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market</title>
<link>https://arxiv.org/abs/2506.06356</link>
<guid>https://arxiv.org/abs/2506.06356</guid>
<content:encoded><![CDATA[
<div> deep learning, quantitative trading, Chinese A-share market, backtesting, risk management <br />
Summary:
This paper introduces a multi-day turnover quantitative trading algorithm for the Chinese A-share market that incorporates advanced deep learning techniques for stock prediction. The algorithm consists of five interconnected modules, including initial stock selection, opening signal analysis, position sizing, profit-taking, stop-loss mechanisms, and market timing models. Trained on a decade of A-share data and backtested rigorously, the algorithm achieves impressive annualized returns of 15.2% with controlled maximum drawdown and a high Sharpe ratio. It balances capital efficiency and risk management through adaptive holding periods and optimized entry/exit timing. The strategy maintains a large number of daily positions with a short maximum holding period, utilizing dynamic profit-taking and stop-loss mechanisms to enhance turnover efficiency while preserving risk-adjusted returns. The approach shows robust performance across different market conditions and is suitable for institutional deployment due to its high capital capacity. <br /> <div>
arXiv:2506.06356v1 Announce Type: new 
Abstract: This paper presents a sophisticated multi-day turnover quantitative trading algorithm that integrates advanced deep learning techniques with comprehensive cross-sectional stock prediction for the Chinese A-share market. Our framework combines five interconnected modules: initial stock selection through deep cross-sectional prediction networks, opening signal distribution analysis using mixture models for arbitrage identification, market capitalization and liquidity-based dynamic position sizing, grid-search optimized profit-taking and stop-loss mechanisms, and multi-granularity volatility-based market timing models. The algorithm employs a novel approach to balance capital efficiency with risk management through adaptive holding periods and sophisticated entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and rigorously backtested on 2021-2024 data, our method achieves remarkable performance with 15.2\% annualized returns, maximum drawdown constrained below 5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional scalability by maintaining 50-100 daily positions with a 9-day maximum holding period, incorporating dynamic profit-taking and stop-loss mechanisms that enhance capital turnover efficiency while preserving risk-adjusted returns. Our approach exhibits robust performance across various market regimes while maintaining high capital capacity suitable for institutional deployment.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality</title>
<link>https://arxiv.org/abs/2506.06622</link>
<guid>https://arxiv.org/abs/2506.06622</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial analysis, QuantMCP, data APIs, decision-making

Summary:
QuantMCP introduces a framework to ground Large Language Models (LLMs) in financial reality by enabling them to access real-time financial data through standardized and secure tools like the Model Context Protocol (MCP). This allows LLMs to overcome issues such as data hallucination and lack of access to verified information. By leveraging Python-accessible financial data APIs, users can interact with LLMs via natural language to retrieve up-to-date and structured financial data. This enables LLMs to enhance their analytical capabilities, generate insights, and support informed financial decision-making processes. QuantMCP serves as a reliable and secure bridge between conversational AI and complex financial data, aiming to improve the reliability and analytical depth of LLM applications in finance. 

<br /><br />Summary: <div>
arXiv:2506.06622v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing financial analysis and decision-making, yet their direct application is often hampered by issues of data hallucination and lack of access to real-time, verifiable financial information. This paper introduces QuantMCP, a novel framework designed to rigorously ground LLMs in financial reality. By leveraging the Model Context Protocol (MCP) for standardized and secure tool invocation, QuantMCP enables LLMs to accurately interface with a diverse array of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can interact via natural language to precisely retrieve up-to-date financial data, thereby overcoming LLM's inherent limitations in factual data recall. More critically, once furnished with this verified, structured data, the LLM's analytical capabilities are unlocked, empowering it to perform sophisticated data interpretation, generate insights, and ultimately support more informed financial decision-making processes. QuantMCP provides a robust, extensible, and secure bridge between conversational AI and the complex world of financial data, aiming to enhance both the reliability and the analytical depth of LLM applications in finance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hype Index: an NLP-driven Measure of Market News Attention</title>
<link>https://arxiv.org/abs/2506.06329</link>
<guid>https://arxiv.org/abs/2506.06329</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, Hype Index, Media Attention, Financial News, Stock Volatility

Summary:
The paper introduces the Hype Index as a metric to quantify media attention towards large-cap equities using Natural Language Processing (NLP) techniques. Two versions of the Hype Index are constructed - News Count-Based and Capitalization Adjusted - to measure media exposure relative to stock or sector market capitalization. The Hype Index is evaluated based on its classification into hype groups, associations with returns and volatility, signaling power for market movements, and empirical properties such as correlations and trends. The findings suggest that the Hype Index family is a valuable tool for analyzing stock volatility, market signaling, and extending NLP applications in finance. <div>
arXiv:2506.06329v1 Announce Type: cross 
Abstract: This paper introduces the Hype Index as a novel metric to quantify media attention toward large-cap equities, leveraging advances in Natural Language Processing (NLP) for extracting predictive signals from financial news. Using the S&amp;P 100 as the focus universe, we first construct a News Count-Based Hype Index, which measures relative media exposure by computing the share of news articles referencing each stock or sector. We then extend it to the Capitalization Adjusted Hype Index, adjusts for economic size by taking the ratio of a stock's or sector's media weight to its market capitalization weight within its industry or sector. We compute both versions of the Hype Index at the stock and sector levels, and evaluate them through multiple lenses: (1) their classification into different hype groups, (2) their associations with returns, volatility, and VIX index at various lags, (3) their signaling power for short-term market movements, and (4) their empirical properties including correlations, samplings, and trends. Our findings suggest that the Hype Index family provides a valuable set of tools for stock volatility analysis, market signaling, and NLP extensions in Finance.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models</title>
<link>https://arxiv.org/abs/2506.06335</link>
<guid>https://arxiv.org/abs/2506.06335</guid>
<content:encoded><![CDATA[
<div> Keywords: FinBERT2, financial-specific, bidirectional encoder, LLMs, fine-tuned models

Summary:
- FinBERT2 is introduced as a specialized bidirectional encoder pretrained on a financial-specific corpus of 32b tokens, representing the largest Chinese financial pretraining corpus for models of this parameter size.
- FinBERT2 addresses the limitations of LLMs in the financial sector by outperforming other variants on discriminatory tasks by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks.
- FinBERT2 also excels in contrastive fine-tuned models, outperforming open-source and proprietary embedders across five financial retrieval tasks.
- The construction of Fin-TopicModel based on FinBERT2 variants enables superior clustering and topic representation for financial titles. 
<br /><br />Summary: FinBERT2, a specialized bidirectional encoder pretrained on a financial-specific corpus, fills the gap in financial-specific deployment of LLMs by outperforming other variants and LLMs on classification and retrieval tasks. Additionally, the Fin-TopicModel offers enhanced clustering and topic representation for financial titles, showcasing the practical applications and benefits of utilizing FinBERT in the LLMs era. <div>
arXiv:2506.06335v1 Announce Type: cross 
Abstract: In natural language processing (NLP), the focus has shifted from encoder-only tiny language models like BERT to decoder-only large language models(LLMs) such as GPT-3. However, LLMs' practical application in the financial sector has revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT on discriminative tasks despite costing much higher computational resources, such as market sentiment analysis in financial reports; (2) Application on generative tasks heavily relies on retrieval augmented generation (RAG) methods to provide current and specialized information, with general retrievers showing suboptimal performance on domain-specific retrieval tasks; (3) There are additional inadequacies in other feature-based scenarios, such as topic modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. This represents the largest known Chinese financial pretraining corpus for models of this parameter size. As a better backbone, FinBERT2 can bridge the gap in the financial-specific deployment of LLMs through the following achievements: (1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. (2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's text-embedding-3-large) embedders across five financial retrieval tasks; (3) Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables superior clustering and topic representation for financial titles. Our work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment</title>
<link>https://arxiv.org/abs/2506.06355</link>
<guid>https://arxiv.org/abs/2506.06355</guid>
<content:encoded><![CDATA[
<div> Efficient simulation, proactive preparedness, sudden-onset disasters, large language models, world models<br />
<br />
Efficient simulation is crucial for preparing for sudden-onset disasters like earthquakes. This study explores the use of large language models (LLMs) to predict earthquake impacts. By analyzing various datasets, including geospatial and socioeconomic data, the framework generates Modified Mercalli Intensity (MMI) predictions at different scales. Evaluation on past earthquakes shows a high correlation and low error rate compared to real reports. Techniques like RAG and ICL can enhance simulation performance, with visual inputs improving accuracy. These findings highlight the potential of LLMs in simulating disaster impacts, aiding in pre-event planning and response strategies.<br /><br />Summary: <div>
arXiv:2506.06355v1 Announce Type: cross 
Abstract: Efficient simulation is essential for enhancing proactive preparedness for sudden-onset disasters such as earthquakes. Recent advancements in large language models (LLMs) as world models show promise in simulating complex scenarios. This study examines multiple LLMs to proactively estimate perceived earthquake impacts. Leveraging multimodal datasets including geospatial, socioeconomic, building, and street-level imagery data, our framework generates Modified Mercalli Intensity (MMI) predictions at zip code and county scales. Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real reports at the zip code level. Techniques such as RAG and ICL can improve simulation performance, while visual inputs notably enhance accuracy compared to structured numerical data alone. These findings show the promise of LLMs in simulating disaster impacts that can help strengthen pre-event planning.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events</title>
<link>https://arxiv.org/abs/2506.06380</link>
<guid>https://arxiv.org/abs/2506.06380</guid>
<content:encoded><![CDATA[
<div> Keywords: extreme events, synthetic data generation, generative modeling techniques, evaluation framework, underexplored areas<br />
Summary:<br />
This article discusses the challenges in predicting extreme events due to the scarcity of data and introduces synthetic data generation as a solution. It reviews generative modeling techniques and large language models tailored for capturing heavy-tailed distributions in extreme event data. The authors propose an evaluation framework encompassing various metrics to assess the performance of models in extreme settings. The article categorizes application domains and highlights underexplored areas like behavioral finance and infectious outbreaks. It also offers insights into statistical theory enhancements and specialized training mechanisms for generating synthetic data. The survey outlines open challenges in synthetic rare-event research, providing a structured foundation for future advancements in this domain. <div>
arXiv:2506.06380v1 Announce Type: cross 
Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Probabilistic Framework for Learning with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.07003</link>
<guid>https://arxiv.org/abs/2506.07003</guid>
<content:encoded><![CDATA[
<div> probabilistic forecasting, operational constraints, uncertainty quantification, neural networks, probabilistic projection layer

Summary: 
ProbHardE2E is a probabilistic forecasting framework designed to incorporate operational or physical constraints as hard requirements. It enforces these constraints by utilizing variance information in a unique way, allowing for uncertainty quantification within the model. The framework utilizes a differentiable probabilistic projection layer (DPPL) that can be combined with various neural network architectures to learn the system in an end-to-end manner. ProbHardE2E can optimize a proper scoring rule without assuming a specific target distribution, enabling robust distributional estimates. It can also handle a range of non-linear constraints, increasing modeling power and flexibility. The framework is applied to learning partial differential equations with uncertainty estimates and probabilistic time-series forecasting, demonstrating its broad applicability across diverse domains. <div>
arXiv:2506.07003v1 Announce Type: cross 
Abstract: We present a general purpose probabilistic forecasting framework, ProbHardE2E, to learn systems that can incorporate operational/physical constraints as hard requirements. ProbHardE2E enforces hard constraints by exploiting variance information in a novel way; and thus it is also capable of performing uncertainty quantification (UQ) on the model. Our methodology uses a novel differentiable probabilistic projection layer (DPPL) that can be combined with a wide range of neural network architectures. This DPPL allows the model to learn the system in an end-to-end manner, compared to other approaches where the constraints are satisfied either through a post-processing step or at inference. In addition, ProbHardE2E can optimize a strictly proper scoring rule, without making any distributional assumptions on the target, which enables it to obtain robust distributional estimates (in contrast to existing approaches that generally optimize likelihood-based objectives, which are heavily biased by their distributional assumptions and model choices); and it can incorporate a range of non-linear constraints (increasing the power of modeling and flexibility). We apply ProbHardE2E to problems in learning partial differential equations with uncertainty estimates and to probabilistic time-series forecasting, showcasing it as a broadly applicable general setup that connects these seemingly disparate domains.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning</title>
<link>https://arxiv.org/abs/2506.07551</link>
<guid>https://arxiv.org/abs/2506.07551</guid>
<content:encoded><![CDATA[
<div> Chemistry, Large language models, Chemical tools, Dataset curation, Hierarchical Evolutionary Monte Carlo Tree Search

Summary: 
This article introduces a novel approach to enhance the performance of Large Language Models (LLMs) in chemistry tasks by integrating external chemical tools and dataset curation. The proposed Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework allows for independent optimization of tool planning and execution, leading to improved performance in Chemistry QA and discovery tasks. By generating the ChemToolBench dataset and leveraging self-generated data, the approach supports step-level fine-tuning of the policy model and training task-adaptive PRM and ORM. Experimental evaluations show that this approach surpasses existing models such as GPT-4o, offering a robust solution for integrating specialized tools with LLMs in advanced chemical applications. The datasets and code for this study are available on GitHub at https://github.com/AI4Chem/ChemistryAgent.

<br /><br />Summary: <div>
arXiv:2506.07551v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently demonstrated promising capabilities in chemistry tasks while still facing challenges due to outdated pretraining knowledge and the difficulty of incorporating specialized chemical expertise. To address these issues, we propose an LLM-based agent that synergistically integrates 137 external chemical tools created ranging from basic information retrieval to complex reaction predictions, and a dataset curation pipeline to generate the dataset ChemToolBench that facilitates both effective tool selection and precise parameter filling during fine-tuning and evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework, enabling independent optimization of tool planning and execution. By leveraging self-generated data, our approach supports step-level fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM that surpass GPT-4o. Experimental evaluations demonstrate that our approach significantly improves performance in Chemistry QA and discovery tasks, offering a robust solution to integrate specialized tools with LLMs for advanced chemical applications. All datasets and code are available at https://github.com/AI4Chem/ChemistryAgent .
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</title>
<link>https://arxiv.org/abs/2506.07865</link>
<guid>https://arxiv.org/abs/2506.07865</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scene geometry, appearance, physics, multi-view videos, dynamic scenes

Summary:
FreeGave is a novel approach for modeling complex dynamic 3D scenes solely from multi-view videos, without requiring object priors. The method introduces a physics code and a divergence-free module to estimate per-Gaussian velocity fields, without relying on inefficient PINN losses. Extensive experiments on various datasets demonstrate the superior performance of FreeGave in future frame extrapolation and motion segmentation tasks. The approach is able to learn meaningful 3D physical motion patterns without the need for human labels in training, showcasing its ability to capture complex physical motions at boundaries. By effectively incorporating physics simulation into neural networks, FreeGave shows promising results in learning scene geometry, appearance, and underlying physics, making it a valuable tool for understanding and analyzing dynamic scenes. 

<br /><br />Summary: <div>
arXiv:2506.07865v1 Announce Type: cross 
Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the underlying physics purely from multi-view videos. By applying various governing PDEs as PINN losses or incorporating physics simulation into neural networks, existing works often fail to learn complex physical motions at boundaries or require object priors such as masks or types. In this paper, we propose FreeGave to learn the physics of complex dynamic 3D scenes without needing any object priors. The key to our approach is to introduce a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on the inefficient PINN losses. Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of our method for future frame extrapolation and motion segmentation. Most notably, our investigation into the learned physics codes reveals that they truly learn meaningful 3D physical motion patterns in the absence of any human labels in training.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>pyCFS-data: Data Processing Framework in Python for openCFS</title>
<link>https://arxiv.org/abs/2405.03437</link>
<guid>https://arxiv.org/abs/2405.03437</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical simulation, multi-field problems, aeroacoustics, open-source framework, data processing

Summary:
openCFS is an open-source framework developed for simulating multi-field problems, focusing on aeroacoustics. It allows for the implementation of partial differential equations using the finite element method. The software, which has been continuously developed since 2000, is now known as openCFS (previously CFS++ Coupled Field Simulations written in C++). In this paper, pyCFS-data, a data processing framework written in Python, is introduced to provide a flexible and user-friendly toolbox for accessing, manipulating, pre- and postprocessing data related to openCFS simulations. This tool aims to make the handling of simulation data more efficient and convenient for users. <div>
arXiv:2405.03437v2 Announce Type: replace 
Abstract: Many numerical simulation tools have been developed and are on the market, but there is still a strong need for appropriate tools capable of simulating multi-field problems, especially in aeroacoustics. Therefore, openCFS provides an open-source framework for implementing partial differential equations using the finite element method. Since 2000, the software has been developed continuously. The result is openCFS (before 2020, known as CFS++ Coupled Field Simulations written in C++). In this paper, we present pyCFS-data, a data processing framework written in Python to provide a flexible and easy-to-use toolbox to access and manipulate, pre- and postprocess data generated by or for usage with openCFS.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting the Canonicalization for Fast and Accurate Crystal Tensor Property Prediction</title>
<link>https://arxiv.org/abs/2410.02372</link>
<guid>https://arxiv.org/abs/2410.02372</guid>
<content:encoded><![CDATA[
<div> canonicalization, crystal tensor property prediction, O(3)-equivariant framework, polar decomposition, computational efficiency

Summary: 
In the field of materials science, predicting the tensor properties of crystalline materials is crucial but computationally expensive due to the need for O(3) group tensor equivariance. The proposed framework, GoeCTP, uses polar decomposition as a form of canonicalization to ensure efficient and accurate crystal tensor property prediction. By incorporating canonicalization, GoeCTP achieves high prediction accuracy and runs significantly faster than existing methods. This novel approach eliminates the need for complex architecture designs to maintain equivariance constraints, making it a more efficient and effective solution for crystal tensor property prediction. <div>
arXiv:2410.02372v3 Announce Type: replace 
Abstract: Predicting the tensor properties of crystalline materials is a fundamental task in materials science. Unlike single-value property prediction, which is inherently invariant, tensor property prediction requires maintaining O(3) group tensor equivariance. Such equivariance constraint often requires specialized architecture designs to achieve effective predictions, inevitably introducing tremendous computational costs. Canonicalization, a classical technique for geometry, has recently been explored for efficient learning with symmetry. In this work, we revisit the problem of crystal tensor property prediction through the lens of canonicalization. Specifically, we demonstrate how polar decomposition, a simple yet efficient algebraic method, can serve as a form of canonicalization and be leveraged to ensure equivariant tensor property prediction. Building upon this insight, we propose a general O(3)-equivariant framework for fast and accurate crystal tensor property prediction, referred to as GoeCTP. By utilizing canonicalization, GoeCTP achieves high efficiency without requiring the explicit incorporation of equivariance constraints into the network architecture. Experimental results indicate that GoeCTP achieves the best prediction accuracy and runs up to 13 times faster compared to existing state-of-the-art methods in benchmarking datasets, underscoring its effectiveness and efficiency.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-PINN: A Physics-Informed Neural Network with Finite Geometric Encoding for Solid Mechanics</title>
<link>https://arxiv.org/abs/2412.09453</link>
<guid>https://arxiv.org/abs/2412.09453</guid>
<content:encoded><![CDATA[
<div> finite element method, solid mechanics, PINN, neural network, hybrid space <br />
Summary: <br />
This study addresses challenges in using Physics-Informed Neural Networks (PINN) for solid mechanics problems compared to traditional methods like the finite element method (FEM). The main challenges are the infinite domain generated by PINN conflicting with finite boundaries in solid structures and the Euclidean solution space being insufficient for complex solid geometries. The Finite-PINN model proposed in this work overcomes these challenges by incorporating finite geometric encoding into neural network inputs, creating a hybrid Euclidean-topological solution space. This model is trained using both strong-form and weak-form loss formulations, allowing it to efficiently solve forward problems with preprocessed structural geometric information and reconstruct full-field solutions from sparse observations in inverse problems by embedding physical laws and geometric information. The Finite-PINN model shows promise for addressing a wide range of solid mechanics problems effectively. <div>
arXiv:2412.09453v2 Announce Type: replace 
Abstract: PINN models have demonstrated capabilities in addressing fluid PDE problems, and their potential in solid mechanics is beginning to emerge. This study identifies two key challenges when using PINN to solve general solid mechanics problems. These challenges become evident when comparing the limitations of PINN with the well-established numerical methods commonly used in solid mechanics, such as the finite element method (FEM). Specifically: a) PINN models generate solutions over an infinite domain, which conflicts with the finite boundaries typical of most solid structures; and b) the solution space utilised by PINN is Euclidean, which is inadequate for addressing the complex geometries often present in solid structures.
  This work presents a PINN architecture for general solid mechanics problems, referred to as the Finite-PINN model. The model is designed to effectively tackle two key challenges, while retaining as much of the original PINN framework as possible. To this end, the Finite-PINN incorporates finite geometric encoding into the neural network inputs, thereby transforming the solution space from a conventional Euclidean space into a hybrid Euclidean-topological space. The model is comprehensively trained using both strong-form and weak-form loss formulations, enabling its application to a wide range of forward and inverse problems in solid mechanics. For forward problems, the Finite-PINN model efficiently approximates solutions to solid mechanics problems when the geometric information of a given structure has been preprocessed. For inverse problems, it effectively reconstructs full-field solutions from very sparse observations by embedding both physical laws and geometric information within its architecture.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Value of Battery Energy Storage in the Continuous Intraday Market: Forecast vs. Perfect Foresight Strategies</title>
<link>https://arxiv.org/abs/2501.07121</link>
<guid>https://arxiv.org/abs/2501.07121</guid>
<content:encoded><![CDATA[
<div> modeling, grid-scale battery, energy storage system, intraday market, price forecast

Summary:
Grid-scale battery energy storage systems can optimize trading in the European continuous intraday market by using a forecast-driven model. This model utilizes price forecasts to optimize trading schedules, resulting in significant earning potential. The approach outperforms key market indices and demonstrates the profitability of participating in the CID market despite its complexity. By comparing profits across various spot markets, the forecast-driven model proves to be effective in capturing market dynamics and maximizing revenue. Using real data, a 1 MW/1 MWh system earns EUR 146,237, showcasing the model's success in estimating earnings potential and optimizing trading strategies. The method surpasses key market indices by a significant margin, confirming its reliability and effectiveness in trading energy in the CID market. <div>
arXiv:2501.07121v2 Announce Type: replace 
Abstract: Grid-scale battery energy storage systems (BESSs) can provide flexibility to the power system and capture shortterm price volatility by shifting energy in time through controlled charging and discharging. The highly volatile European continuous intraday (CID) market allows trading until just a few minutes before physical delivery, offering significant earning potential. However, its high trading frequency poses substantial modeling challenges. Accurate modeling of BESSs trading in the CID market is essential to estimate revenue potential and optimize trading strategies. Additionally, comparing CID profits with other spot markets helps determine whether participating in the CID is worthwhile despite its complexity. We propose a forecast-driven model to optimize BESS trading in the CID market. Our strategy employs a rolling window modeling framework to capture market dynamics. Price forecasts for impending CID products are generated at the beginning of each window and used to optimize trading schedules for subsequent execution. We also benchmark our approach across various spot markets, offering a broad cross-market profit comparison. We evaluate our forecast-driven model across different BESS power-to-capacity ratios, comparing it to a perfect-foresight scenario and key CID market indices, such as ID1 and ID3. Using real 2023 German CID data, a 1 MW/1 MWh system adopting our method earns EUR 146 237, only 11% below perfect foresight, surpassing all other markets and indices. Our approach surpasses ID1 and ID3 by over 4% and 32%, respectively, confirming ID1 as a reliable lower-bound estimate for earnings potential in the CID market.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Deep Learning Surrogate Models for Uncertainty Propagation in Microstructure-Properties of Ceramic Aerogels</title>
<link>https://arxiv.org/abs/2501.13255</link>
<guid>https://arxiv.org/abs/2501.13255</guid>
<content:encoded><![CDATA[
<div> surrogate models, deep learning, ceramic aerogel, microstructure, mechanical response
Summary:
This study introduces a computational framework that combines physics-based simulations with deep learning surrogate models to predict the microstructural morphology and mechanical behavior of ceramic aerogel porous materials. Lattice Boltzmann simulations model microstructure formation during material synthesis, while a finite element model calculates mechanical properties. To address the computational demands of analyzing microstructural randomness, Convolutional Neural Networks (CNNs) are used to develop surrogate models for microstructure generation and mapping. CNN training is treated as a Bayesian inference problem to enable uncertainty quantification in predictions. The surrogate models produce microstructural images consistent with training data and accurately predict strain energy for in-distribution microstructures. The study investigates the generalization capability of the surrogate models and utilizes them for efficient uncertainty propagation to quantify the influence of microstructural variability on macroscopic mechanical properties.<br /><br />Summary: <div>
arXiv:2501.13255v3 Announce Type: replace 
Abstract: This study presents an integrated computational framework that, given synthesis parameters, predicts the resulting microstructural morphology and mechanical response of ceramic aerogel porous materials by combining physics-based simulations with deep learning surrogate models. Lattice Boltzmann simulations are employed to model microstructure formation during material synthesis process, while a finite element model is used to compute the corresponding mechanical properties. To overcome the prohibitive computational demands of repeated physics-based simulations required for characterizing the impact of microstructure randomness on mechanical properties, surrogate models are developed using Convolutional Neural Networks (CNNs) for both microstructure generation and microstructure-property mapping. CNN training is formulated as a Bayesian inference problem to enable uncertainty quantification and provide confidence estimates in surrogate model predictions, under limited training data furnished by physics-based simulations. Numerical results demonstrate that the microstructure surrogate model effectively generates microstructural images consistent with the morphology of training data across larger domains. The Bayesian CNN surrogate accurately predicts strain energy for in-distribution microstructures and its generalization capability to interpolated morphologies are further investigated. Finally, the surrogate models are employed for efficient uncertainty propagation, quantifying the influence of microstructural variability on macroscopic mechanical property.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay</title>
<link>https://arxiv.org/abs/2502.16789</link>
<guid>https://arxiv.org/abs/2502.16789</guid>
<content:encoded><![CDATA[
<div> alpha decay, alpha mining, Large Language Models, genetic programming, factor overfitting
<br />
Summary: 
The article introduces AlphaAgent, a framework designed to address the challenge of alpha decay in quantitative investment. By integrating Large Language Models with ad hoc regularizations, AlphaAgent aims to generate decay-resistant alpha factors. It enforces originality by comparing generated factors with existing ones, aligns hypotheses with factors for market consistency, and controls complexity to prevent overfitting. Through extensive evaluations in Chinese CSI 500 and US S&amp;P 500 markets, AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay. It demonstrates significant resistance to alpha decay and consistently delivers substantial alpha over the past four years, offering promising potential for powerful factors. <div>
arXiv:2502.16789v2 Announce Type: replace 
Abstract: Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay, where factors lose their predictive power over time, poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and US S&amp;P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications</title>
<link>https://arxiv.org/abs/2408.11878</link>
<guid>https://arxiv.org/abs/2408.11878</guid>
<content:encoded><![CDATA[
<div> Financial LLMs, Open-FinLLMs, multimodal capabilities, zero-shot, few-shot, fine-tuning.

Summary:
Open-FinLLMs are introduced as the first open-source multimodal financial Language Models (LLMs) to handle various financial tasks using text, tabular, time-series, and chart data. The suite includes FinLLaMA pre-trained on a large corpus, FinLLaMA-Instruct fine-tuned with financial instructions, and FinLLaVA enhanced with multimodal tuning pairs for cross-modal reasoning. Evaluation across 14 financial tasks and 4 multimodal tasks shows that Open-FinLLMs outperform other financial and general LLMs such as GPT-4 in tasks like financial NLP and decision-making, indicating their potential to address real-world challenges. Codes and models are released under OSI-approved licenses to encourage collaboration between academia and industry. <br /><br />Summary: Financial LLMs Open-FinLLMs introduced as open-source multimodal models, excel in diverse tasks, outperforming GPT-4, released with codes and models for collaboration. <div>
arXiv:2408.11878v3 Announce Type: replace-cross 
Abstract: Financial LLMs hold promise for advancing financial tasks and domain-specific applications. However, they are limited by scarce corpora, weak multimodal capabilities, and narrow evaluations, making them less suited for real-world application. To address this, we introduce \textit{Open-FinLLMs}, the first open-source multimodal financial LLMs designed to handle diverse tasks across text, tabular, time-series, and chart data, excelling in zero-shot, few-shot, and fine-tuning settings. The suite includes FinLLaMA, pre-trained on a comprehensive 52-billion-token corpus; FinLLaMA-Instruct, fine-tuned with 573K financial instructions; and FinLLaVA, enhanced with 1.43M multimodal tuning pairs for strong cross-modal reasoning. We comprehensively evaluate Open-FinLLMs across 14 financial tasks, 30 datasets, and 4 multimodal tasks in zero-shot, few-shot, and supervised fine-tuning settings, introducing two new multimodal evaluation datasets. Our results show that Open-FinLLMs outperforms afvanced financial and general LLMs such as GPT-4, across financial NLP, decision-making, and multi-modal tasks, highlighting their potential to tackle real-world challenges. To foster innovation and collaboration across academia and industry, we release all codes (https://anonymous.4open.science/r/PIXIU2-0D70/B1D7/LICENSE) and models under OSI-approved licenses.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Deep Learning Model for Line-integral Diagnostics Across Fusion Devices</title>
<link>https://arxiv.org/abs/2412.00087</link>
<guid>https://arxiv.org/abs/2412.00087</guid>
<content:encoded><![CDATA[
<div> physics-informed model, deep learning, plasma profiles, line-integral measurements, nuclear fusion <br />
Summary: <br />
The paper introduces a physics-informed model architecture called Onion for rapid 2D plasma profile reconstruction from line-integral measurements in nuclear fusion research. By incorporating physical information through a multiplication process and applying a physics-informed loss function based on the principle of line integration, the model shows improvements in performance. Results indicate a reduction in average relative error in reconstruction profiles compared to target profiles on both synthetic and experimental datasets. The use of Softplus activation function in the final two fully connected layers further enhances model performance. The physics-informed loss function corrects prediction errors, bringing back-projections closer to actual inputs and reducing inversion algorithm errors. Synthetic data models for generating customized diagnostic datasets and collection of soft x-ray diagnostic datasets from EAST and HL-2A contribute to the study's success in reducing reconstruction errors and accelerating the development of surrogate models in fusion research. <br /> <div>
arXiv:2412.00087v3 Announce Type: replace-cross 
Abstract: Rapid reconstruction of 2D plasma profiles from line-integral measurements is important in nuclear fusion. This paper introduces a physics-informed model architecture called Onion, that can enhance the performance of models and be adapted to various backbone networks. The model under Onion incorporates physical information by a multiplication process and applies the physics-informed loss function according to the principle of line integration. Prediction results demonstrate that the additional input of physical information improves the deep learning model's ability, leading to a reduction in the average relative error E_1 between the reconstruction profiles and the target profiles by approximately 0.84x10^(-2) on synthetic datasets and about 0.06x10^(-2) on experimental datasets. Furthermore, the implementation of the Softplus activation function in the final two fully connected layers improves model performance. This enhancement results in a reduction in the E_1 by approximately 1.06x10^(-2) on synthetic datasets and about 0.11x10^(-2) on experimental datasets. The incorporation of the physics-informed loss function has been shown to correct the model's predictions, bringing the back-projections closer to the actual inputs and reducing the errors associated with inversion algorithms. Besides, we have developed a synthetic data model to generate customized line-integral diagnostic datasets and have also collected soft x-ray diagnostic datasets from EAST and HL-2A. This study achieves reductions in reconstruction errors, and accelerates the development of surrogate models in fusion research.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-driven inventory management for new products: An adjusted Dyna-$Q$ approach with transfer learning</title>
<link>https://arxiv.org/abs/2501.08109</link>
<guid>https://arxiv.org/abs/2501.08109</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, inventory management, transfer learning, model-based approach, Dyna-Q

Summary:
The paper presents a novel reinforcement learning algorithm for inventory management of new products without historical demand data. It combines model-free and model-based approaches in a Dyna-Q structure, enhancing training efficiency and reducing model discrepancy. By incorporating transfer learning from similar existing products' demand data, the algorithm stabilizes early-stage training and improves policy estimation accuracy. Validation through a bakery inventory case study demonstrates up to a 23.7% cost reduction compared to Q-learning and up to a 77.5% faster training time than traditional Dyna-Q. Utilizing transfer learning, the adjusted Dyna-Q outperforms benchmark algorithms in terms of total cost, cost variance, and shortage percentages during a 30-day testing period. <div>
arXiv:2501.08109v4 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel reinforcement learning algorithm for inventory management of newly launched products with no historical demand information. The algorithm follows the classic Dyna-$Q$ structure, balancing the model-free and model-based approaches, while accelerating the training process of Dyna-$Q$ and mitigating the model discrepancy generated by the model-based feedback. Based on the idea of transfer learning, warm-start information from the demand data of existing similar products can be incorporated into the algorithm to further stabilize the early-stage training and reduce the variance of the estimated optimal policy. Our approach is validated through a case study of bakery inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\% reduction in average daily cost compared with $Q$-learning, and up to a 77.5\% reduction in training time within the same horizon compared with classic Dyna-$Q$. By using transfer learning, it can be found that the adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and relatively low shortage percentages among all the benchmarking algorithms under a 30-day testing.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Aspects of Strategic Trading</title>
<link>https://arxiv.org/abs/2502.07606</link>
<guid>https://arxiv.org/abs/2502.07606</guid>
<content:encoded><![CDATA[
<div> Algorithmic trading, position building, equilibrium strategies, temporary market impact, permanent market impact <br />
<br />Summary: 
This study focuses on algorithmic trading in financial markets, particularly in the context of position building with temporary and permanent market impact. The research presents an efficient algorithm for computing best responses in trading strategies, highlighting the challenges of convergence in the general setting. While the temporary impact-only scenario forms a potential game, convergence is not guaranteed in the broader context. The concept of Coarse Correlated Equilibria (CCE) is introduced as an alternative solution, computable via Follow the Perturbed Leader (FTPL) implementation. An experimental investigation demonstrates the behavior of FTPL in varying conditions of temporary and permanent market impact weighting, shedding light on strategic behaviors in complex trading environments. <div>
arXiv:2502.07606v2 Announce Type: replace-cross 
Abstract: Algorithmic trading in modern financial markets is widely acknowledged to exhibit strategic, game-theoretic behaviors whose complexity can be difficult to model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress in the setting of trading for position building. Here parties wish to buy or sell a fixed number of shares in a fixed time period in the presence of both temporary and permanent market impact, resulting in exponentially large strategy spaces. While these papers primarily consider the existence and structural properties of equilibrium strategies, in this work we focus on the algorithmic aspects of the proposed model. We give an efficient algorithm for computing best responses, and show that while the temporary impact only setting yields a potential game, best response dynamics do not generally converge for the general setting, for which no fast algorithm for (Nash) equilibrium computation is known. This leads us to consider the broader notion of Coarse Correlated Equilibria (CCE), which we show can be computed efficiently via an implementation of Follow the Perturbed Leader (FTPL). We illustrate the model and our results with an experimental investigation, where FTPL exhibits interesting behavior in different regimes of the relative weighting between temporary and permanent market impact.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-based Framework for Robust Model-Based Connector Mating in Robotic Wire Harness Installation</title>
<link>https://arxiv.org/abs/2503.09409</link>
<guid>https://arxiv.org/abs/2503.09409</guid>
<content:encoded><![CDATA[
<div> AI-based framework, automation, cable connector mating, force control, deep visuotactile learning<br />
<br />
Summary: 
An AI-based framework has been developed to automate the manual process of cable connector mating in automotive assembly. The system integrates force control with deep visuotactile learning to optimize search-and-insertion strategies using multimodal transformer architecture. A novel automated data collection and optimization pipeline reduces the need for machine learning expertise. The framework optimizes robot programs that can run on standard industrial controllers, allowing for human auditing and certification. Experimental validations on a center console assembly task show improved cycle times and robustness compared to traditional robot programming methods. Videos demonstrating the system's capabilities are available for further reference. <div>
arXiv:2503.09409v2 Announce Type: replace-cross 
Abstract: Despite the widespread adoption of industrial robots in automotive assembly, wire harness installation remains a largely manual process, as it requires precise and flexible manipulation. To address this challenge, we design a novel AI-based framework that automates cable connector mating by integrating force control with deep visuotactile learning. Our system optimizes search-and-insertion strategies using first-order optimization over a multimodal transformer architecture trained on visual, tactile, and proprioceptive data. Additionally, we design a novel automated data collection and optimization pipeline that minimizes the need for machine learning expertise. The framework optimizes robot programs that run natively on standard industrial controllers, permitting human experts to audit and certify them. Experimental validations on a center console assembly task demonstrate significant improvements in cycle times and robustness compared to conventional robot programming approaches. Videos are available under https://claudius-kienle.github.io/AppMuTT.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Informer for Option Pricing: A Transformer-Based Approach</title>
<link>https://arxiv.org/abs/2506.05565</link>
<guid>https://arxiv.org/abs/2506.05565</guid>
<content:encoded><![CDATA[
<div> neural network, option pricing, financial forecasting, long-term dependencies, market fluctuations

Summary:
This paper explores the use of the Informer neural network for accurate option pricing in financial markets. Traditional models like Black-Scholes are limited in capturing market volatility, but Informer's efficient architecture allows for better prediction accuracy by incorporating long-term dependencies and dynamically adapting to market fluctuations. The study demonstrates that Informer surpasses traditional approaches in option pricing, highlighting its potential to enhance data-driven financial forecasting in this field. The research contributes to advancing the capabilities of option pricing by providing a more adaptable and resilient framework for effective trading and risk management in financial markets.<br /><br />Summary: <div>
arXiv:2506.05565v1 Announce Type: new 
Abstract: Accurate option pricing is essential for effective trading and risk management in financial markets, yet it remains challenging due to market volatility and the limitations of traditional models like Black-Scholes. In this paper, we investigate the application of the Informer neural network for option pricing, leveraging its ability to capture long-term dependencies and dynamically adjust to market fluctuations. This research contributes to the field of financial forecasting by introducing Informer's efficient architecture to enhance prediction accuracy and provide a more adaptable and resilient framework compared to existing methods. Our results demonstrate that Informer outperforms traditional approaches in option pricing, advancing the capabilities of data-driven financial forecasting in this domain.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction</title>
<link>https://arxiv.org/abs/2506.05427</link>
<guid>https://arxiv.org/abs/2506.05427</guid>
<content:encoded><![CDATA[
<div> prediction, drug discovery, material design, computational methods, interaction details 

Summary:
The article presents the Multi-Grained Target Perception network (MTPNet) for activity cliff prediction in drug discovery and material design. MTPNet integrates Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance to optimize molecular representations based on protein interactions. This approach, utilizing receptor proteins as guiding information, outperforms previous methods by achieving an average RMSE improvement of 18.95% on various datasets. By incorporating interaction patterns through conditional deep learning, MTPNet provides accurate predictions of activity cliffs, aiding in compound optimization and design. The availability of codes for MTPNet on GitHub enhances its accessibility and applicability in research and industry. <div>
arXiv:2506.05427v1 Announce Type: cross 
Abstract: Activity cliff prediction is a critical task in drug discovery and material design. Existing computational methods are limited to handling single binding targets, which restricts the applicability of these prediction models. In this paper, we present the Multi-Grained Target Perception network (MTPNet) to incorporate the prior knowledge of interactions between the molecules and their target proteins. Specifically, MTPNet is a unified framework for activity cliff prediction, which consists of two components: Macro-level Target Semantic (MTS) guidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet dynamically optimizes molecular representations through multi-grained protein semantic conditions. To our knowledge, it is the first time to employ the receptor proteins as guiding information to effectively capture critical interaction details. Extensive experiments on 30 representative activity cliff datasets demonstrate that MTPNet significantly outperforms previous approaches, achieving an average RMSE improvement of 18.95% on top of several mainstream GNN architectures. Overall, MTPNet internalizes interaction patterns through conditional deep learning to achieve unified predictions of activity cliffs, helping to accelerate compound optimization and design. Codes are available at: https://github.com/ZishanShu/MTPNet.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Network Model of Spatial and Feature-Based Attention</title>
<link>https://arxiv.org/abs/2506.05487</link>
<guid>https://arxiv.org/abs/2506.05487</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual attention, neural network model, human cognition, spatial attention, feature-based attention

Summary: 
This article introduces a neural network model inspired by human visual attention mechanisms. The model consists of two networks, one performing a basic task and the other guiding attention based on contextual information for more complex tasks. The trained model's attention patterns closely resemble spatial and feature-based attention observed in human vision. This similarity suggests that neural network models can effectively mimic human cognition, offering valuable insights into the workings of visual attention. The study highlights the potential of using neural network models to explore and understand human cognitive processes, particularly in the realm of visual attention. <div>
arXiv:2506.05487v1 Announce Type: cross 
Abstract: Visual attention is a mechanism closely intertwined with vision and memory. Top-down information influences visual processing through attention. We designed a neural network model inspired by aspects of human visual attention. This model consists of two networks: one serves as a basic processor performing a simple task, while the other processes contextual information and guides the first network through attention to adapt to more complex tasks. After training the model and visualizing the learned attention response, we discovered that the model's emergent attention patterns corresponded to spatial and feature-based attention. This similarity between human visual attention and attention in computer vision suggests a promising direction for studying human cognition using neural network models.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Stabilization Protocol for Cross-Chain Digital Assets Using Adaptor Signatures and AI-Driven Arbitrage</title>
<link>https://arxiv.org/abs/2506.05708</link>
<guid>https://arxiv.org/abs/2506.05708</guid>
<content:encoded><![CDATA[
<div> decentralization, stability, regulatory compliance, stablecoins, hybrid stabilization protocol

Summary:
The article discusses the challenges faced by stablecoins in balancing decentralization, stability, and regulatory compliance. A hybrid stabilization protocol is proposed, combining crypto-collateralized reserves, algorithmic futures contracts, and cross-chain liquidity pools. Stabilization futures contracts (SFCs) are introduced as non-collateralized derivatives that incentivize third-party arbitrageurs to maintain price adherence. Autonomous AI agents optimize delta hedging on decentralized exchanges, while zkSNARKs ensure compliance with anti-money laundering regulations without revealing user identities. The cryptographic design reduces cross-chain liquidity concentration and ensures atomicity in transactions. The protocol's layered architecture, including SFCs, AI market making, and zero-knowledge regulatory proofs, serves as a blueprint for advanced decentralized financial infrastructure. <div>
arXiv:2506.05708v1 Announce Type: cross 
Abstract: Stablecoins face an unresolved trilemma of balancing decentralization, stability, and regulatory compliance. We present a hybrid stabilization protocol that combines crypto-collateralized reserves, algorithmic futures contracts, and cross-chain liquidity pools to achieve robust price adherence while preserving user privacy. At its core, the protocol introduces stabilization futures contracts (SFCs), non-collateralized derivatives that programmatically incentivize third-party arbitrageurs to counteract price deviations via adaptor signature atomic swaps. Autonomous AI agents optimize delta hedging across decentralized exchanges (DEXs), while zkSNARKs prove compliance with anti-money laundering (AML) regulations without exposing identities or transaction details. Our cryptographic design reduces cross-chain liquidity concentration (Herfindahl-Hirschman Index: 2,400 vs. 4,900 in single-chain systems) and ensures atomicity under standard cryptographic assumptions. The protocol's layered architecture encompassing incentive-compatible SFCs, AI-driven market making, and zero-knowledge regulatory proofs. It provides a blueprint for next-generation decentralized financial infrastructure.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts</title>
<link>https://arxiv.org/abs/2506.05755</link>
<guid>https://arxiv.org/abs/2506.05755</guid>
<content:encoded><![CDATA[
<div> flow matching models, optimal execution, financial markets, imitation learning framework, market impact costs

Summary:
flowOE is introduced as a novel imitation learning framework for optimal execution in financial markets. It learns from expert traditional strategies and selects the most suitable behavior based on market conditions, incorporating a refining loss function to improve upon learned actions. This approach outperforms both expert models and traditional benchmarks in empirical evaluations, achieving higher profits with reduced risk. FlowOE demonstrates practical applicability and potential to enhance adaptive optimal execution in dynamic financial markets. <br /><br />Summary: <div>
arXiv:2506.05755v1 Announce Type: cross 
Abstract: Optimal execution in financial markets refers to the process of strategically transacting a large volume of assets over a period to achieve the best possible outcome by balancing the trade-off between market impact costs and timing or volatility risks. Traditional optimal execution strategies, such as static Almgren-Chriss models, often prove suboptimal in dynamic financial markets. This paper propose flowOE, a novel imitation learning framework based on flow matching models, to address these limitations. FlowOE learns from a diverse set of expert traditional strategies and adaptively selects the most suitable expert behavior for prevailing market conditions. A key innovation is the incorporation of a refining loss function during the imitation process, enabling flowOE not only to mimic but also to improve upon the learned expert actions. To the best of our knowledge, this work is the first to apply flow matching models in a stochastic optimal execution problem. Empirical evaluations across various market conditions demonstrate that flowOE significantly outperforms both the specifically calibrated expert models and other traditional benchmarks, achieving higher profits with reduced risk. These results underscore the practical applicability and potential of flowOE to enhance adaptive optimal execution.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EqCollide: Equivariant and Collision-Aware Deformable Objects Neural Simulator</title>
<link>https://arxiv.org/abs/2506.05797</link>
<guid>https://arxiv.org/abs/2506.05797</guid>
<content:encoded><![CDATA[
<div> equivariant neural fields simulator, deformable objects, collisions, Graph Neural Network, scalability
Summary:
EqCollide is a novel end-to-end neural fields simulator designed for simulating collisions of deformable objects. The model incorporates an equivariant encoder to map object geometry and velocity into latent control points, followed by a Graph Neural Network-based Neural Ordinary Differential Equation to model interactions among control points via collision-aware message passing. The approach allows for accurate and stable simulations across diverse object configurations, with a significant reduction in MSE compared to baseline models. Furthermore, EqCollide demonstrates scalability, generalization to more colliding objects and extended temporal horizons, and robustness to input transformations with group action. The model also enables continuous and resolution-independent motion predictions, showcasing its potential for a wide range of applications in simulating complex interactions among deformable objects. 
<br /><br />Summary: <div>
arXiv:2506.05797v1 Announce Type: cross 
Abstract: Simulating collisions of deformable objects is a fundamental yet challenging task due to the complexity of modeling solid mechanics and multi-body interactions. Existing data-driven methods often suffer from lack of equivariance to physical symmetries, inadequate handling of collisions, and limited scalability. Here we introduce EqCollide, the first end-to-end equivariant neural fields simulator for deformable objects and their collisions. We propose an equivariant encoder to map object geometry and velocity into latent control points. A subsequent equivariant Graph Neural Network-based Neural Ordinary Differential Equation models the interactions among control points via collision-aware message passing. To reconstruct velocity fields, we query a neural field conditioned on control point features, enabling continuous and resolution-independent motion predictions. Experimental results show that EqCollide achieves accurate, stable, and scalable simulations across diverse object configurations, and our model achieves 24.34% to 35.82% lower rollout MSE even compared with the best-performing baseline model. Furthermore, our model could generalize to more colliding objects and extended temporal horizons, and stay robust to input transformed with group action.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging</title>
<link>https://arxiv.org/abs/2506.05828</link>
<guid>https://arxiv.org/abs/2506.05828</guid>
<content:encoded><![CDATA[
<div> Keywords: FinanceReasoning, benchmark, large reasoning models, financial concepts, numerical precision

Summary: 
FinanceReasoning introduces a new benchmark to evaluate the reasoning abilities of large reasoning models (LRMs) in financial numerical tasks. The benchmark includes updated questions with detailed Python solutions, covering 67.8% of financial concepts and formulas. LRMs benefit from 3,133 Python-formatted functions, enhancing their financial reasoning capabilities. The benchmark also presents 238 challenging problems requiring precise numerical reasoning. The best-performing model achieves 89.1% accuracy, highlighting the ongoing challenges faced by LRMs in numerical precision. The study demonstrates that combining Reasoner and Programmer models can improve LRMs' performance. This advancement in evaluating LRMs in specific reasoning tasks opens avenues for future research in complex domain-specific reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2506.05828v1 Announce Type: cross 
Abstract: We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs' financial reasoning capabilities through refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs' performance (e.g., 83.2% $\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Tasks and Their Complexity for the European Train Control System with Hybrid Train Detection</title>
<link>https://arxiv.org/abs/2308.02572</link>
<guid>https://arxiv.org/abs/2308.02572</guid>
<content:encoded><![CDATA[
<div> Keywords: railway networks, train control system, ETCS L2 HTD, design tasks, computational complexity

Summary:
Railway networks play a crucial role in transportation, especially for freight and public transit. Maximizing rail capacity is essential, but existing constraints must be considered. The European Train Control System (ETCS L2 HTD) introduces virtual subsections to improve train following times and increase track capacity. Design tasks for ETCS L2 HTD present new challenges that can benefit from automated methods. This paper provides a formal description of these design tasks and proves their computational complexity to be NP-complete or NP-hard. This research forms the foundation for developing methods to address these tasks and will be integrated into the Munich Train Control Toolkit for the railway industry. <div>
arXiv:2308.02572v4 Announce Type: replace-cross 
Abstract: Railway networks have become increasingly important in recent times, especially in moving freight and public transportation from road traffic and planes to more environmentally friendly trains. Since expanding the global railway network is time- and resource-consuming, maximizing the rail capacity of the existing infrastructure is desirable. However, simply running more trains is infeasible as certain constraints enforced by the train control system must be satisfied. The capacity of a network depends (amongst others) on the distance between trains allowed by this safety system. While most signaling systems rely on fixed blocks defined by costly hardware, new specifications provided by Level 2 with Hybrid Train Detection of the European Train Control System (ETCS L2 HTD), formerly known as ETCS Hybrid Level 3, allow the usage of virtual subsections. This additional degree of freedom allows for shorter train following times and, thus, more trains on existing railway tracks. On the other hand, new design tasks arise on which automated methods might be helpful for designers of modern railway networks. However, although first approaches exist that solve design problems arising within ETCS L2 HTD, neither formal descriptions nor results on the computational complexity of the corresponding design tasks exist. In this paper, we fill this gap by providing a formal description of design tasks for ETCS L2 HTD and proof that these tasks are NP-complete or NP-hard, respectively. By that, we are providing a solid basis for the future development of methods to solve those tasks, which will be integrated into the Munich Train Control Toolkit available open-source on GitHub at https://github.com/cda-tum/mtct.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and efficient predictions of keyhole dynamics in laser materials processing using machine learning-aided simulations</title>
<link>https://arxiv.org/abs/2402.16190</link>
<guid>https://arxiv.org/abs/2402.16190</guid>
<content:encoded><![CDATA[
<div> machine learning, simulation, laser materials processing, keyhole phenomenon, defects

Summary:
- The study focuses on the keyhole phenomenon in laser materials processing, which leads to the formation of pores and affects product performance.
- Pores are associated with the dynamic behavior of the keyhole, making accurate characterization and prediction challenging.
- In situ characterization using synchrotron X-ray technique is informative but complex and costly.
- The developed machine learning-aided simulation method accurately predicts keyhole dynamics, particularly keyhole depth fluctuations, for a wide range of processing parameters.
- The method achieved a mean absolute percentage error of 10%, surpassing ray-tracing simulations with a 30% error margin and reducing computational time.
- This cost-effective and efficient model can serve as an alternative to synchrotron experiments, offering potential for defect elimination or reduction in various laser materials processing techniques. 

<br /><br />Summary: <div>
arXiv:2402.16190v2 Announce Type: replace-cross 
Abstract: The keyhole phenomenon has been widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing. Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies. The formation of these pores is typically associated with the dynamic behavior of the keyhole. So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time, has been a challenging task. In situ characterization of keyhole dynamic behavior using the synchrotron X-ray technique is informative but complicated and expensive. Current simulations are generally hindered by their poor accuracy and generalization abilities in predicting keyhole depths due to the lack of accurate laser absorptance data. In this study, we develop a machine learning-aided simulation method that accurately predicts keyhole dynamics, especially in keyhole depth fluctuations, over a wide range of processing parameters. In two case studies involving titanium and aluminum alloys, we achieve keyhole depth prediction with a mean absolute percentage error of 10%, surpassing those simulated using the ray-tracing method with an error margin of 30%, while also reducing computational time. This exceptional fidelity and efficiency empower our model to serve as a cost-effective alternative to synchrotron experiments. Our machine learning-aided simulation method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Structure of Financial Equity Research Reports -- Identification of the Most Frequently Asked Questions in Financial Analyst Reports to Automate Equity Research Using Llama 3 and GPT-4</title>
<link>https://arxiv.org/abs/2407.18327</link>
<guid>https://arxiv.org/abs/2407.18327</guid>
<content:encoded><![CDATA[
<div> Keywords: financial equity research reports, automation, language models, information extraction, question archetypes

Summary:
- The research aims to categorize the content of financial equity research reports (ERRs) by analyzing 72 reports sentence-by-sentence.
- 4940 sentences from ERRs were classified into 169 unique question archetypes without predefined questions, providing an unbiased view of their content.
- 78.7% of the questions in ERRs were found to be automatable, with 48.2% as text-extractable and 30.5% as database-extractable.
- Only 21.3% of questions required human judgment, suggesting a potential for automation in the ERR writing process.
- Empirical validation using advanced language models like Llama-3-70B and GPT-4-turbo-2024-04-09 confirmed the feasibility of automating approximately 80% of ERR content.
- The study highlights the potential benefits of introducing large language models in the ERR writing process to enhance quality and efficiency. 

<br /><br />Summary: This research analyzes financial equity research reports to identify question archetypes and determine the potential for automation in answering these questions. The study reveals that a significant percentage of questions in ERRs can be automated, with text-extractable and database-extractable questions being prominent. By leveraging advanced language models, such as Llama-3-70B and GPT-4-turbo-2024-04-09, the automation of ERR writing process is feasible, providing opportunities to improve quality and efficiency. <div>
arXiv:2407.18327v2 Announce Type: replace-cross 
Abstract: This research dissects financial equity research reports (ERRs) by mapping their content into categories. There is insufficient empirical analysis of the questions answered in ERRs. In particular, it is not understood how frequently certain information appears, what information is considered essential, and what information requires human judgment to distill into an ERR. The study analyzes 72 ERRs sentence-by-sentence, classifying their 4940 sentences into 169 unique question archetypes. We did not predefine the questions but derived them solely from the statements in the ERRs. This approach provides an unbiased view of the content of the observed ERRs. Subsequently, we used public corporate reports to classify the questions' potential for automation. Answers were labeled "text-extractable" if the answers to the question were accessible in corporate reports. 78.7% of the questions in ERRs can be automated. Those automatable question consist of 48.2% text-extractable (suited to processing by large language models, LLMs) and 30.5% database-extractable questions. Only 21.3% of questions require human judgment to answer. We empirically validate using Llama-3-70B and GPT-4-turbo-2024-04-09 that recent advances in language generation and information extraction enable the automation of approximately 80% of the statements in ERRs. Surprisingly, the models complement each other's strengths and weaknesses well. The research confirms that the current writing process of ERRs can likely benefit from additional automation, improving quality and efficiency. The research thus allows us to quantify the potential impacts of introducing large language models in the ERR writing process. The full question list, including the archetypes and their frequency, will be made available online after peer review.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemReservoir -- An Open-Source Framework for Chemically-Inspired Reservoir Computing</title>
<link>https://arxiv.org/abs/2506.04249</link>
<guid>https://arxiv.org/abs/2506.04249</guid>
<content:encoded><![CDATA[
<div> Keywords: reservoir computing, cheminformatics, open-source framework, chemically-inspired reservoirs, memory capacity tasks

Summary:
ChemReservoir is introduced as an open-source framework for chemically-inspired reservoir computing, addressing the limitations of previous studies focused on DNA chemistry. Unlike previous tools, ChemReservoir is a general framework for constructing and analyzing chemically-inspired reservoirs, ensuring enhanced testing, evaluation, and reproducibility. The tool was evaluated using various cycle-based reservoir topologies and showed stable performance across different configurations in memory capacity tasks. This framework allows for the development of reservoir models not limited to DNA chemistry, making it a versatile tool for cheminformatics research. By providing a user-friendly and accessible platform, ChemReservoir contributes to the advancement of reservoir computing in the field of cheminformatics. <br /><br />Summary: <div>
arXiv:2506.04249v1 Announce Type: new 
Abstract: Reservoir computing is a type of a recurrent neural network, mapping the inputs into higher dimensional space using fixed and nonlinear dynamical systems, called reservoirs. In the literature, there are various types of reservoirs ranging from in-silico to in-vitro. In cheminformatics, previous studies contributed to the field by developing simulation-based chemically inspired in-silico reservoir models. Yahiro used a DNA-based chemical reaction network as its reservoir and Nguyen developed a DNA chemistry-inspired tool based on Gillespie algorithm. However, these software tools were designed mainly with the focus on DNA chemistry and their maintenance status has limited their current usability. Due to these limitations, there was a need for a proper open-source tool. This study introduces ChemReservoir, an open-source framework for chemically-inspired reservoir computing. In contrast to the former studies focused on DNA-chemistry, ChemReservoir is a general framework for the construction and analysis of chemically-inspired reservoirs, which also addresses the limitations in these previous studies by ensuring enhanced testing, evaluation, and reproducibility. The tool was evaluated using various cycle-based reservoir topologies and demonstrated stable performance across a range of configurations in memory capacity tasks.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive recycled plastic architecture: Vacuum-Sealed Chainmail Structures Through Computational Design</title>
<link>https://arxiv.org/abs/2506.04660</link>
<guid>https://arxiv.org/abs/2506.04660</guid>
<content:encoded><![CDATA[
<div> Keywords: recycled plastics, construction industry, modular chainmail systems, sustainability, innovative applications<br />
<br />
Summary: This paper explores the use of recycled plastics as a primary construction material in modular chainmail systems. The research demonstrates the optimization of design, testing, and fabrication of vacuum-sealed chainmail structures made of recycled plastic filaments. The study identifies the rectangular chainmail configuration as the most efficient for architectural use, with superior deformation capacity, material efficiency, and load-bearing performance. Optimization strategies for temporary structures are also proposed to balance material savings, usable area, and water drainage efficiency. The findings suggest innovative applications for extreme conditions such as disaster-prone areas, high-altitude environments, underwater platforms, and extraterrestrial habitats, leveraging the properties of recycled plastics and modular chainmail systems. This research bridging waste management and high-performance design offers solutions for challenges in harsh and resource-constrained environments. <br /><br /> <div>
arXiv:2506.04660v1 Announce Type: new 
Abstract: The construction industry is a major consumer of raw materials, accounting for nearly half of global material usage annually, while generating significant waste that poses sustainability challenges. This paper explores the untapped potential of recycled plastics as a primary construction material, leveraging their lightweight, flexible, and customizable properties for advanced applications in modular chainmail systems. Through a computational workflow, the study optimizes the design, testing, and fabrication of vacuum-sealed chainmail structures composed of recycled plastic filaments, demonstrating their adaptability and structural performance for architectural use.
  Key contributions include a novel methodology for integrating recycled plastic filaments into chainmail geometries, validated through 2D sectional testing, 3D shell structure generation, and physical modeling under vacuum constraints. The research identifies the rectangular chainmail configuration as the most efficient and adaptable, achieving superior deformation capacity, material efficiency, and load-bearing performance. Optimization strategies for temporary structures highlight practical deployment potential, balancing material savings, usable area, and water drainage efficiency.
  The findings offer a foundation for innovative applications in extreme conditions, including disaster-prone areas, high-altitude environments, underwater platforms, and extraterrestrial habitats. These applications leverage the lightweight, adaptable, and durable properties of recycled plastics and modular chainmail systems, bridging the gap between waste management and high-performance design while addressing unique challenges in harsh and resource-constrained environments.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear elastodynamic material identification of heterogeneous isogeometric Bernoulli-Euler beams</title>
<link>https://arxiv.org/abs/2506.04960</link>
<guid>https://arxiv.org/abs/2506.04960</guid>
<content:encoded><![CDATA[
<div> Finite Element Model Updating, Material Identification, Isogeometric Formulation, Bernoulli-Euler Beams, Elastic Properties<br />
<br />
Summary: This paper presents a Finite Element Model Updating framework for identifying heterogeneous material distributions in planar Bernoulli-Euler beams. The process involves identifying elastic properties from quasi-static displacements and determining density from modal data. Three independent discretizations are used, including isogeometric finite element mesh, high-resolution experimental measurement grid, and material mesh with low-order Lagrange elements. The method minimizes errors between experiments and numerical model using local optimization with trust-region method. Results from numerical examples show effectiveness in handling large displacements and noise in experimental data. B2M1 discretization is used to alleviate membrane locking. Regularization ensures stable solutions for dense material meshes. The proposed framework can be extended to shells and 3D continua. <div>
arXiv:2506.04960v1 Announce Type: new 
Abstract: This paper presents a Finite Element Model Updating framework for identifying heterogeneous material distributions in planar Bernoulli-Euler beams based on a rotation-free isogeometric formulation. The procedure follows two steps: First, the elastic properties are identified from quasi-static displacements; then, the density is determined from modal data (low frequencies and mode shapes), given the previously obtained elastic properties. The identification relies on three independent discretizations: the isogeometric finite element mesh, a high-resolution grid of experimental measurements, and a material mesh composed of low-order Lagrange elements. The material mesh approximates the unknown material distributions, with its nodal values serving as design variables. The error between experiments and numerical model is expressed in a least squares manner. The objective is minimized using local optimization with the trust-region method, providing analytical derivatives to accelerate computations. Several numerical examples exhibiting large displacements are provided to test the proposed approach. To alleviate membrane locking, the B2M1 discretization is employed when necessary. Quasi-experimental data is generated using refined finite element models with random noise applied up to 4%. The method yields satisfactory results as long as a sufficient amount of experimental data is available, even for high measurement noise. Regularization is used to ensure a stable solution for dense material meshes. The density can be accurately reconstructed based on the previously identified elastic properties. The proposed framework can be straightforwardly extended to shells and 3D continua.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMultiTime: A Four-Modal Bilingual Dataset for Financial Time-Series Analysis</title>
<link>https://arxiv.org/abs/2506.05019</link>
<guid>https://arxiv.org/abs/2506.05019</guid>
<content:encoded><![CDATA[
<div> Dataset, financial news, structured tables, K-line charts, stock prices  
Summary:  
- The paper introduces FinMultiTime, a large-scale multimodal financial time series dataset that includes financial news, structured financial tables, K-line technical charts, and stock price time series.
- The dataset encompasses 5,105 stocks from the S&amp;P 500 and HS 300 universes, covering the period from 2009 to 2025 in the U.S. and China.
- It provides data at minute-level, daily, and quarterly resolutions, enhancing the capturing of short, medium, and long-term market signals with high fidelity.
- Experiments show that the dataset's scale and data quality significantly improve prediction accuracy.
- Multimodal fusion in Transformer models results in moderate performance gains.
<br /><br />Summary: <div>
arXiv:2506.05019v1 Announce Type: new 
Abstract: Pure time series forecasting tasks typically focus exclusively on numerical features; however, real-world financial decision-making demands the comparison and analysis of heterogeneous sources of information. Recent advances in deep learning and large scale language models (LLMs) have made significant strides in capturing sentiment and other qualitative signals, thereby enhancing the accuracy of financial time series predictions. Despite these advances, most existing datasets consist solely of price series and news text, are confined to a single market, and remain limited in scale. In this paper, we introduce FinMultiTime, the first large scale, multimodal financial time series dataset. FinMultiTime temporally aligns four distinct modalities financial news, structured financial tables, K-line technical charts, and stock price time series across both the S&amp;P 500 and HS 300 universes. Covering 5,105 stocks from 2009 to 2025 in the United States and China, the dataset totals 112.6 GB and provides minute-level, daily, and quarterly resolutions, thus capturing short, medium, and long term market signals with high fidelity. Our experiments demonstrate that (1) scale and data quality markedly boost prediction accuracy; (2) multimodal fusion yields moderate gains in Transformer models; and (3) a fully reproducible pipeline enables seamless dataset updates.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmark for Antibody Binding Affinity Maturation and Design</title>
<link>https://arxiv.org/abs/2506.04235</link>
<guid>https://arxiv.org/abs/2506.04235</guid>
<content:encoded><![CDATA[
<div> Benchmarking, Antibody binding affinity maturation, Design, AbBiBench, Protein models <br />
Summary: <br />
The article introduces AbBiBench, a benchmarking framework for evaluating antibody binding affinity maturation and design. Unlike current methods that focus on the antibody alone, AbBiBench considers the antibody-antigen complex as a functional unit. The framework includes 9 datasets with 9 antigens and 155,853 mutated antibodies, allowing for systematic comparison of 14 protein models. The correlation between model likelihood and experimental affinity values is used to assess model performance. In a case study involving increasing binding affinity of antibody F045-092 to influenza H1N1, structure-conditioned inverse folding models prove to be the most effective. AbBiBench offers a biologically grounded evaluation framework to enhance the development of antibody design models that are more effective and function-aware. <br /> <div>
arXiv:2506.04235v1 Announce Type: cross 
Abstract: We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking framework for antibody binding affinity maturation and design. Unlike existing antibody evaluation strategies that rely on antibody alone and its similarity to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench considers an antibody-antigen (Ab-Ag) complex as a functional unit and evaluates the potential of an antibody design binding to given antigen by measuring protein model's likelihood on the Ab-Ag complex. We first curate, standardize, and share 9 datasets containing 9 antigens (involving influenza, anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain mutated antibodies. Using these datasets, we systematically compare 14 protein models including masked language models, autoregressive language models, inverse folding models, diffusion-based generative models, and geometric graph models. The correlation between model likelihood and experimental affinity values is used to evaluate model performance. Additionally, in a case study to increase binding affinity of antibody F045-092 to antigen influenza H1N1, we evaluate the generative power of the top-performing models by sampling a set of new antibodies binding to the antigen and ranking them based on structural integrity and biophysical properties of the Ab-Ag complex. As a result, structure-conditioned inverse folding models outperform others in both affinity correlation and generation tasks. Overall, AbBiBench provides a unified, biologically grounded evaluation framework to facilitate the development of more effective, function-aware antibody design models.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding</title>
<link>https://arxiv.org/abs/2506.04353</link>
<guid>https://arxiv.org/abs/2506.04353</guid>
<content:encoded><![CDATA[
<div> Keywords: ReXVQA, visual question answering, chest radiology, multimodal large language models, AI performance

Summary: 
ReXVQA is a new benchmark for visual question answering in chest radiology, encompassing a vast dataset of questions paired with X-ray studies. It introduces diverse and authentic tasks reflecting various radiological reasoning skills. State-of-the-art multimodal large language models were evaluated, with MedGemma achieving the highest accuracy. A human reader study revealed that MedGemma outperformed radiology residents in chest X-ray interpretation, indicating a milestone where AI surpassed expert human evaluation. The study also highlighted distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists. ReXVQA establishes a standard for evaluating radiological AI systems, offering public leaderboards and detailed evaluation metrics. The dataset will be open-sourced, laying the groundwork for AI systems capable of expert-level clinical reasoning. 

<br /><br />Summary: <div>
arXiv:2506.04353v1 Announce Type: cross 
Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual question answering (VQA) in chest radiology, comprising approximately 696,000 questions paired with 160,000 chest X-rays studies across training, validation, and test sets. Unlike prior efforts that rely heavily on template based queries, ReXVQA introduces a diverse and clinically authentic task suite reflecting five core radiological reasoning skills: presence assessment, location analysis, negation detection, differential diagnosis, and geometric reasoning. We evaluate eight state-of-the-art multimodal large language models, including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge the gap between AI performance and clinical expertise, we conducted a comprehensive human reader study involving 3 radiology residents on 200 randomly sampled cases. Our evaluation demonstrates that MedGemma achieved superior performance (83.84% accuracy) compared to human readers (best radiology resident: 77.27%), representing a significant milestone where AI performance exceeds expert human evaluation on chest X-ray interpretation. The reader study reveals distinct performance patterns between AI models and human experts, with strong inter-reader agreement among radiologists while showing more variable agreement patterns between human readers and AI models. ReXVQA establishes a new standard for evaluating generalist radiological AI systems, offering public leaderboards, fine-grained evaluation splits, structured explanations, and category-level breakdowns. This benchmark lays the foundation for next-generation AI systems capable of mimicking expert-level clinical reasoning beyond narrow pathology classification. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor-based multivariate function approximation: methods benchmarking and comparison</title>
<link>https://arxiv.org/abs/2506.04791</link>
<guid>https://arxiv.org/abs/2506.04791</guid>
<content:encoded><![CDATA[
<div> evaluation, tensor-based multivariate function construction, approximation, machine learning, benchmark 

Summary: 
This note evaluates methods for tensor-based multivariate function construction and approximation, using a collection of functions with varying complexity. The performance, features, and user experience of each method are assessed based on accuracy, computational time, and parameter tuning impact. The goal is to provide a fair comparison of available strategies to guide users in understanding the process, advantages, and limitations of each tool. The note introduces a benchmark collection of tools for tensor approximation by surrogate models and gives explicit attention to the multivariate Loewner Framework approach. The detailed comparison allows readers to grasp the capabilities and limitations of each method, with examples provided for clarity. <div>
arXiv:2506.04791v1 Announce Type: cross 
Abstract: In this note, we evaluate the performances, the features and the user-experience of some methods (and their implementations) designed for tensor- (or data-) based multivariate function construction and approximation. To this aim, a collection of multivariate functions extracted from contributive works coming from different communities, is suggested. First, these functions with varying complexity (e.g. number and degree of the variables) and nature (e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to construct tensors, each of different dimension and size on the disk. Second, grounded on this tensor, we inspect performances of each considered method (e.g. the accuracy, the computational time, the parameters tuning impact, etc.). Finally, considering the "best" parameter tuning set, we compare each method using multiple evaluation criteria. The purpose of this note is not to rank the methods but rather to evaluate as fairly as possible the different available strategies, with the idea in mind to guide users to understand the process, the possibilities, the advantages and the limits brought by each tools. The contribution claimed is to suggest a complete benchmark collection of some available tools for tensor approximation by surrogate models (e.g. rational functions, networks, etc.). In addition, as contributors of the multivariate Loewner Framework (mLF) approach (and its side implementation in MDSPACK), attention and details of the latter are more explicitly given, in order to provide readers a digest of this contributive work and some details with simple examples.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Private Smart Wallet with Probabilistic Compliance</title>
<link>https://arxiv.org/abs/2506.04853</link>
<guid>https://arxiv.org/abs/2506.04853</guid>
<content:encoded><![CDATA[
<div> smart wallet, privacy-preserving, private onboarding mechanism, compliance checks, digital payments

Summary: 
The article presents a privacy-preserving smart wallet that incorporates a novel invitation-based private onboarding mechanism. It combines proof of innocence and ancestral commitment tracking systems for compliance with an authority party. Performance analysis reveals efficient private transfers with compliance checks completing in seconds on a standard laptop, and low proof generation. On-chain costs remain minimal for affordability on a Base layer 2 network. The smart wallet enables encrypted contact list management and transaction unlinkability for enhanced privacy. The evaluation confirms the effectiveness of the approach for compliance-aware digital payments, with reduced computational and financial burdens. <div>
arXiv:2506.04853v1 Announce Type: cross 
Abstract: We propose a privacy-preserving smart wallet with a novel invitation-based private onboarding mechanism. The solution integrates two levels of compliance in concert with an authority party: a proof of innocence mechanism and an ancestral commitment tracking system using bloom filters for probabilistic UTXO chain states. Performance analysis demonstrates practical efficiency: private transfers with compliance checks complete within seconds on a consumer-grade laptop, and overall with proof generation remaining low. On-chain costs stay minimal, ensuring affordability for all operations on Base layer 2 network. The wallet facilitates private contact list management through encrypted data blobs while maintaining transaction unlinkability. Our evaluation validates the approach's viability for privacy-preserving, compliance-aware digital payments with minimized computational and financial overhead.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRC20 Pinning Attack</title>
<link>https://arxiv.org/abs/2410.11295</link>
<guid>https://arxiv.org/abs/2410.11295</guid>
<content:encoded><![CDATA[
<div> tokens, Bitcoin network, security, attack vector, Binance

Summary:
BRC20 tokens are unique assets on the Bitcoin network that allow users to embed custom content within satoshis. Despite their growing market size, the transfer mechanism of BRC20 tokens has not been thoroughly examined for security vulnerabilities. A new attack vector, known as the BRC20 pinning attack, exploits the fee levels of bundled transactions to disrupt liquidity and withdrawal requests. This attack was successfully validated in collaboration with Binance researchers, resulting in a temporary suspension of withdrawals. The impact of the attack extends to a significant portion of inscription-based tokens in the Bitcoin ecosystem, highlighting the potential risks associated with BRC20 tokens and similar assets. Security measures and further analysis are necessary to mitigate the threat posed by such attacks. 

<br /><br />Summary: <div>
arXiv:2410.11295v3 Announce Type: replace-cross 
Abstract: BRC20 tokens are a type of non-fungible asset on the Bitcoin network. They allow users to embed customised content within Bitcoin's satoshis. The token frenzy reached a market size of US\$2.811\,b (2023Q3--2025Q1). However, this intuitive design has not undergone serious security scrutiny.
  We present the first analysis of BRC20's \emph{transfer} mechanism and identify a new attack vector. A typical BRC20 transfer involves two "bundled" on-chain transactions with different fee levels: the first (i.e., \textbf{Tx1}) with a lower fee inscribes the \textsf{transfer} request, while the second (i.e., \textbf{Tx2}) with a higher fee finalizes the actual transfer. An adversary can send a manipulated fee transaction (falling between the two fee levels), which causes \textbf{Tx1} to be processed while \textbf{Tx2} is pinned in the mempool. This locks BRC20 liquidity and disrupts normal withdrawal requests from users. We term this the \emph{BRC20 pinning attack}.
  We validated the attack in real-world settings in collaboration with Binance researchers. With their knowledge and permission, we conducted a controlled test against Binance's ORDI hot wallet, resulting in a temporary suspension of ORDI withdrawals for 3.5 hours. Recovery was performed shortly after. Further analysis confirms that the attack can be applied to over \textbf{90\%} of inscription-based tokens within the Bitcoin ecosystem.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proactive Model Adaptation Against Concept Drift for Online Time Series Forecasting</title>
<link>https://arxiv.org/abs/2412.08435</link>
<guid>https://arxiv.org/abs/2412.08435</guid>
<content:encoded><![CDATA[
<div> Proactive Adaptation, Concept Drift Estimation, Forecast Model, Online Learning, Time Series<br />
<br />
Summary: Proceed is a proactive model adaptation framework designed for online time series forecasting. It addresses the challenge of concept drift by estimating and adapting to changes between training and test samples. By utilizing an adaptation generator, Proceed translates estimated drift into parameter adjustments, enhancing model performance. Trained on diverse synthetic concept drifts, Proceed demonstrates improved resilience against concept drift compared to existing online learning methods. Extensive experiments on real-world datasets confirm the effectiveness of Proceed in enhancing forecast model generalization capability and performance. The code for Proceed is available on GitHub for further exploration and application in time series forecasting tasks. <br /><br /> <div>
arXiv:2412.08435v4 Announce Type: replace-cross 
Abstract: Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Functional Materials Design with Optimal Initial Data in Surrogate-Based Active Learning</title>
<link>https://arxiv.org/abs/2506.03329</link>
<guid>https://arxiv.org/abs/2506.03329</guid>
<content:encoded><![CDATA[
<div> Keywords: functional materials, optimization, data-driven algorithms, surrogate-based active learning, quantum computing

Summary: 
This study focuses on optimizing functional materials through data-driven algorithms, which efficiently explore complex design spaces by learning relationships between material structures and performance metrics. Surrogate-based active learning, coupled with quantum computing, is highlighted as a cost-effective approach for material optimization. The use of a special surrogate model called quadratic unconstrained binary optimization is emphasized. The research investigates the impact of initial data sizes on optimization efficiency, showing that adequate initial data is crucial for achieving fast convergence and reducing computational costs. Averaged piecewise linear regression is used to identify the optimal initiation points for convergence, emphasizing the importance of proper initial data in efficient optimization of functional materials. This work contributes to improving optimization processes for functional materials by ensuring faster convergence and reduced computational expenses in surrogate-based active learning. 

<br /><br />Summary: <div>
arXiv:2506.03329v1 Announce Type: new 
Abstract: The optimization of functional materials is important to enhance their properties, but their complex geometries pose great challenges to optimization. Data-driven algorithms efficiently navigate such complex design spaces by learning relationships between material structures and performance metrics to discover high-performance functional materials. Surrogate-based active learning, continually improving its surrogate model by iteratively including high-quality data points, has emerged as a cost-effective data-driven approach. Furthermore, it can be coupled with quantum computing to enhance optimization processes, especially when paired with a special form of surrogate model ($i.e.$, quadratic unconstrained binary optimization), formulated by factorization machine. However, current practices often overlook the variability in design space sizes when determining the initial data size for optimization. In this work, we investigate the optimal initial data sizes required for efficient convergence across various design space sizes. By employing averaged piecewise linear regression, we identify initiation points where convergence begins, highlighting the crucial role of employing adequate initial data in achieving efficient optimization. These results contribute to the efficient optimization of functional materials by ensuring faster convergence and reducing computational costs in surrogate-based active learning.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the robustness of Dirichlet-Neumann coupling schemes for fluid-structure-interaction problems with nearly-closed fluid domains</title>
<link>https://arxiv.org/abs/2506.04027</link>
<guid>https://arxiv.org/abs/2506.04027</guid>
<content:encoded><![CDATA[
<div> Dirichlet-Neumann split, incompressible fluid, added-mass effect, flow resistance, nearly-closed fluid-domain<br />
<br />
Summary:<br />
Partitioned methods for fluid-structure interaction (FSI) typically use a Dirichlet-Neumann (DN) split for interface conditions. However, for nearly-closed fluid domains with incompressible fluids and Robin conditions, the DN scheme can become unstable due to increasing flow resistance. Convergence deteriorates as resistance increases, leading to instability at high resistances. This instability is linked to an added-damping effect, affecting the convergence rate of the partitioned method. Understanding this effect can improve the robustness and efficiency of FSI simulations, especially for applications like valves. The analysis also sheds light on the incompressibility dilemma for FSI problems with nearly closed fluid domains. Numerical experiments confirm these findings in more complex scenarios, highlighting the challenges and potential solutions for such FSI problems. <div>
arXiv:2506.04027v1 Announce Type: new 
Abstract: Partitioned methods for fluid-structure interaction (FSI) involve solving the structural and flow problems sequentially. These methods allow for separate settings for the fluid and solid subsystems and thus modularity, enabling reuse of advanced commercial and open-source software. Most partitioned FSI schemes apply a Dirichlet-Neumann (DN) split of the interface conditions. The DN scheme is adequate in a wide range of applications, but it is sensitive to the added-mass effect, and it is susceptible to the incompressibility dilemma, i.e. it completely fails for FSI problems with an incompressible fluid furnished with Dirichlet boundary conditions on the part of its boundary complementary to the interface. In this paper, we show that if the fluid is incompressible and the fluid domain is nearly-closed, i.e. it carries Dirichlet conditions except for a permeable part of the boundary carrying a Robin condition, then the DN partitioned approach is sensitive to the flow resistance at the permeable part, and convergence of the partitioned approach deteriorates as the flow resistance increases. The DN scheme then becomes unstable in the limit as the flow resistance passes to infinity. Based on a simple model problem, we show that in the nearly-closed case, the convergence rate of the DN partitioned method depends on a so-called added-damping effect. The analysis gives insights that can aid to improve robustness and efficiency of partitioned method for FSI problems with contact, e.g. valve applications. In addition, the results elucidate the incompressibility dilemma as a limit of the added-damping effect passing to infinity, and the corresponding challenges related to FSI problems with nearly closed fluid-domain configurations. Via numerical experiments, we consider the generalization of the results of the simple model problem to more complex nearly-closed FSI problems.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk and Reward of Transitioning from a National to a Zonal Electricity Market in Great Britain</title>
<link>https://arxiv.org/abs/2506.04107</link>
<guid>https://arxiv.org/abs/2506.04107</guid>
<content:encoded><![CDATA[
<div> consumer savings, producer surplus impacts, socioeconomic benefits, zonal market, electricity market  

Summary:  
The study evaluates the potential benefits of transitioning from a single-price national wholesale market to a zonal market design in Great Britain. Using an open-source electricity market model, the analysis shows that a six-zone market could result in significant consumer savings of around 9.4/MWh annually, totaling over 2.3 billion per year. However, generators in northern regions may experience revenue reductions of 30-40%. Policy interventions could mitigate these negative impacts, allowing for up to 97% restoration of national market revenues for affected units while still preserving around 3.1/MWh in consumer savings. The current system could achieve an annual welfare gain of 380-770 million through operational efficiency improvements alone during 2022-2024, with potential annual benefits exceeding 1-2 billion beyond 2029. These benefits outweigh potential downsides associated with increased capital costs. <div>
arXiv:2506.04107v1 Announce Type: cross 
Abstract: More spatially granular electricity wholesale markets promise more efficient operation and better asset siting in highly renewable power systems. Great Britain is considering moving from its current single-price national wholesale market to a zonal design. Existing studies reach varying and difficult-to-reconcile conclusions about the desirability of a zonal market in GB, partly because they rely on models that vary in their transparency and assumptions about future power systems. Using a novel open-source electricity market model, calibrated to match observed network behaviour, this article quantifies consumer savings, unit-level producer surplus impacts, and broader socioeconomic benefits that would have arisen had a six-zone market operated in Great Britain during 2022-2024. In the absence of mitigating policies, it is estimated that during those three years GB consumers would save approximately {\pounds}9.4/MWh (equalling an average of more than {\pounds}2.3B per year), but generators in northern regions would experience revenue reductions of 30-40\%. Policy interventions can restore these units' national market revenues to up to 97\% while still preserving around {\pounds}3.1/MWh in consumer savings (about {\pounds}750M per year). It is further estimated that the current system could achieve approximately {\pounds}380-{\pounds}770 million in annual welfare gain during 2022-2024 through improved operational efficiency alone. The drivers behind these benefits, notably wind curtailment volumes, are expected to become more pronounced towards 2030, suggesting that purely operationally achieved annual benefits of around {\pounds}1-2 billion beyond 2029 are likely. It is found that the scale of these benefits would outweigh the potential downsides related to increases in the cost of capital that have been estimated elsewhere.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints</title>
<link>https://arxiv.org/abs/2506.04171</link>
<guid>https://arxiv.org/abs/2506.04171</guid>
<content:encoded><![CDATA[
<div> Flow-Based Generative Models, Physics-Constrained Inference, Partial Differential Equations, Constraint Satisfaction, Zero-Shot Inference<br />
Summary:<br />
The article introduces Physics-Constrained Flow Matching (PCFM), a method for enforcing nonlinear constraints in pretrained flow-based generative models. Existing methods struggle to enforce physical constraints effectively, but PCFM addresses this by guiding the sampling process with physics-based corrections while maintaining alignment with learned flow. The framework outperforms both unconstrained and constrained baselines on various PDEs, including those with shocks and sharp features. PCFM ensures exact satisfaction of constraints at the final solution. This approach presents a general framework for enforcing hard constraints in scientific and general-purpose generative models, particularly valuable in applications where constraint satisfaction is critical. <br /><br />Summary: <div>
arXiv:2506.04171v1 Announce Type: cross 
Abstract: Deep generative models have recently been applied to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty-aware inference. However, enforcing physical constraints, such as conservation laws (linear and nonlinear) and physical consistencies, remains challenging. Existing methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints. In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints. Empirically, PCFM outperforms both unconstrained and constrained baselines on a range of PDEs, including those with shocks, discontinuities, and sharp features, while ensuring exact constraint satisfaction at the final solution. Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models, especially in applications where constraint satisfaction is essential.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Financial Foundation Models (MFFMs): Progress, Prospects, and Challenges</title>
<link>https://arxiv.org/abs/2506.01973</link>
<guid>https://arxiv.org/abs/2506.01973</guid>
<content:encoded><![CDATA[
<div> Keywords: FinLLMs, MFFMs, financial data, multimodal, research

Summary:
Financial Large Language Models (FinLLMs) and Multimodal Financial Foundation Models (MFFMs) are revolutionizing the analysis of financial data. While FinLLMs focus on language-centric approaches, MFFMs can process a wide range of multimodal financial data, offering a more comprehensive understanding of complex financial tasks. The progress and potential of MFFMs were discussed in a position paper presented at the MFFM Workshop at the ACM International Conference on AI in Finance 2024. Ongoing research on FinAgents at the SecureFinAI Lab at Columbia University aims to further explore the capabilities of MFFMs. By leveraging diverse data sources such as fundamental data, market data, and alternative data, MFFMs have the potential to streamline financial operations and investment processes. The Github repository for MFFMs provides a platform for collaboration and development in this emerging field. <div>
arXiv:2506.01973v1 Announce Type: new 
Abstract: Financial Large Language Models (FinLLMs), such as open FinGPT and proprietary BloombergGPT, have demonstrated great potential in select areas of financial services. Beyond this earlier language-centric approach, Multimodal Financial Foundation Models (MFFMs) can digest interleaved multimodal financial data, including fundamental data, market data, data analytics, macroeconomic, and alternative data (e.g., natural language, audio, images, and video). In this position paper, presented at the MFFM Workshop joined with ACM International Conference on AI in Finance (ICAIF) 2024, we describe the progress, prospects, and challenges of MFFMs. This paper also highlights ongoing research on FinAgents in the \textbf{SecureFinAI Lab}\footnote{\https://openfin.engineering.columbia.edu/} at Columbia University. We believe that MFFMs will enable a deeper understanding of the underlying complexity associated with numerous financial tasks and data, streamlining the operation of financial services and investment processes. Github Repo https://github.com/Open-Finance-Lab/Awesome-MFFMs/.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Polymer Property Predictions</title>
<link>https://arxiv.org/abs/2506.02129</link>
<guid>https://arxiv.org/abs/2506.02129</guid>
<content:encoded><![CDATA[
<div> Machine learning, polymer informatics, large language models, thermal properties, molecular embeddings
Summary: 
- The study explores the use of large language models (LLMs) in predicting key thermal properties in polymer science, comparing them to traditional fingerprinting-based methods.
- LLaMA-3 outperforms GPT-3.5 in predictive accuracy, likely due to its open-source architecture.
- Single-task learning proves more effective than multi-task learning, as LLMs struggle with capturing cross-property correlations.
- Analysis of molecular embeddings shows limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features.
- The findings provide guidance on selecting LLMs for polymer informatics, highlighting the interplay between molecular embeddings and natural language processing. 

<br /><br />Summary: <div>
arXiv:2506.02129v1 Announce Type: new 
Abstract: Machine learning has revolutionized polymer science by enabling rapid property prediction and generative design. Large language models (LLMs) offer further opportunities in polymer informatics by simplifying workflows that traditionally rely on large labeled datasets, handcrafted representations, and complex feature engineering. LLMs leverage natural language inputs through transfer learning, eliminating the need for explicit fingerprinting and streamlining training. In this study, we finetune general purpose LLMs -- open-source LLaMA-3-8B and commercial GPT-3.5 -- on a curated dataset of 11,740 entries to predict key thermal properties: glass transition, melting, and decomposition temperatures. Using parameter-efficient fine-tuning and hyperparameter optimization, we benchmark these models against traditional fingerprinting-based approaches -- Polymer Genome, polyGNN, and polyBERT -- under single-task (ST) and multi-task (MT) learning. We find that while LLM-based methods approach traditional models in performance, they generally underperform in predictive accuracy and efficiency. LLaMA-3 consistently outperforms GPT-3.5, likely due to its tunable open-source architecture. Additionally, ST learning proves more effective than MT, as LLMs struggle to capture cross-property correlations, a key strength of traditional methods. Analysis of molecular embeddings reveals limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features and domain-specific embeddings. These findings provide insight into the interplay between molecular embeddings and natural language processing, guiding LLM selection for polymer informatics.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singularity Blockchain Key Management via non-custodial key management</title>
<link>https://arxiv.org/abs/2506.02282</link>
<guid>https://arxiv.org/abs/2506.02282</guid>
<content:encoded><![CDATA[
<div> web3 wallets, user identity, blockchain, key management, non-custodial<br />
<br />
Summary: <br />
Web3 wallets play a crucial role in managing user identity on the blockchain by storing and providing access to private keys. Key management schemes can be either custodial or non-custodial, with the latter placing the burden of key storage and recovery on the user. Existing non-custodial schemes often require users to remember seed phrases, leading to onboarding challenges and the risk of asset loss if the key is forgotten. This paper introduces a novel non-custodial key management approach that allows users to back up and recover their private key using third-party sign-in methods such as google-oAuth. By enabling users to securely backup their keys through independent authentication methods, this technique aims to enhance user experience and reduce the likelihood of key loss. <div>
arXiv:2506.02282v1 Announce Type: new 
Abstract: web3 wallets are key to managing user identity on blockchain. The main purpose of a web3 wallet application is to manage the private key for the user and provide an interface to interact with the blockchain. The key management scheme ( KMS ) used by the wallet to store and recover the private key can be either custodial, where the keys are permissioned and in custody of the wallet provider or noncustodial where the keys are in custody of the user. The existing non-custodial key management schemes tend to offset the burden of storing and recovering the key entirely on the user by asking them to remember seed-phrases. This creates onboarding hassles for the user and introduces the risk that the user may lose their assets if they forget or lose their seedphrase/private key. In this paper, we propose a novel method of backing up user keys using a non-custodial key management technique that allows users to save and recover a backup of their private key using any independent sign-in method such as google-oAuth or other 3P oAuth.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the fracture mechanics validity of small scale tests</title>
<link>https://arxiv.org/abs/2506.02538</link>
<guid>https://arxiv.org/abs/2506.02538</guid>
<content:encoded><![CDATA[
<div> Keywords: fracture behaviour, micro-scale mechanical tests, crack growth, material properties, hydrogen embrittlement

Summary: 
This study focuses on conducting small-scale tests to understand the fracture behavior of materials. Using numerical and semi-analytical approaches, the researchers determine the conditions necessary for a valid and quantitative fracture experiment, considering factors such as sample geometry, material properties, and crack lengths. They establish the maximum value of the J-integral, known as Jmax, where fracture must occur for accurate results. Maps are generated to show the maximum valid J value as a function of yield strength, strain hardening, and sample size, providing guidance for conducting experiments. The analysis is extended to metals embrittled by hydrogen exposure, overlaying the response of these materials on the established maps to determine the conditions required for obtaining quantitative insight into such materials. <div>
arXiv:2506.02538v1 Announce Type: new 
Abstract: There is growing interest in conducting small-scale tests to gain additional insight into the fracture behaviour of components across a wide range of materials. For example, micro-scale mechanical tests inside of a microscope (\emph{in situ}) enable direct, high-resolution observation of the interplay between crack growth and microstructural phenomena (e.g., dislocation behaviour or the fracture resistance of a particular interface), and sub-size samples are increasingly used when only a limited amount of material is available. However, to obtain quantitative insight and extract relevant fracture parameters, the sample must be sufficiently large for a $J$- (HRR) or a $K$-field to exist. We conduct numerical and semi-analytical studies to map the conditions (sample geometry, material) that result in a valid, quantitative fracture experiment. Specifically, for a wide range of material properties, crack lengths and sample dimensions, we establish the maximum value of the $J$-integral where an HRR field ceases to exist (i.e., the maximum $J$ value at which fracture must occur for the test to be valid, $J_\mathrm{max}$). Maps are generated to establish the maximum valid $J$ value ($J_\mathrm{max}$) as a function of yield strength, strain hardening and minimum sample size. These maps are then used to discuss the existing experimental literature and provide guidance on how to conduct quantitative experiments. Finally, our study is particularised to the analysis of metals that have been embrittled due to hydrogen exposure. The response of relevant materials under hydrogen-containing environments are superimposed on the aforementioned maps, determining the conditions that will enable quantitative insight.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching Location Representation with Detailed Semantic Information</title>
<link>https://arxiv.org/abs/2506.02744</link>
<guid>https://arxiv.org/abs/2506.02744</guid>
<content:encoded><![CDATA[
<div> Embeddings, Urban Modeling, Contrastive Learning, Point-of-Interest, Multimodal<br />
<br />
Summary: 
The study introduces CaLLiPer+, an urban modeling approach that integrates Point-of-Interest names with categorical labels in a contrastive learning framework. The model shows improved performance in land use classification and socioeconomic status mapping compared to baseline methods, with gains of 4% to 11%. By incorporating POI names, the model enhances location retrieval and captures complex urban concepts accurately. Ablation studies demonstrate the complementary role of POI names and the benefits of using pretrained text encoders for spatial representations. The research underscores the significance of integrating fine-grained semantic attributes and multimodal learning techniques for advancing urban foundation models. <div>
arXiv:2506.02744v1 Announce Type: new 
Abstract: Spatial representations that capture both structural and semantic characteristics of urban environments are essential for urban modeling. Traditional spatial embeddings often prioritize spatial proximity while underutilizing fine-grained contextual information from places. To address this limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that systematically integrates Point-of-Interest (POI) names alongside categorical labels within a multimodal contrastive learning framework. We evaluate its effectiveness on two downstream tasks, land use classification and socioeconomic status distribution mapping, demonstrating consistent performance gains of 4% to 11% over baseline methods. Additionally, we show that incorporating POI names enhances location retrieval, enabling models to capture complex urban concepts with greater precision. Ablation studies further reveal the complementary role of POI names and the advantages of leveraging pretrained text encoders for spatial representations. Overall, our findings highlight the potential of integrating fine-grained semantic attributes and multimodal learning techniques to advance the development of urban foundation models.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Aware Density Estimation in Multiple Dimensions</title>
<link>https://arxiv.org/abs/2506.02323</link>
<guid>https://arxiv.org/abs/2506.02323</guid>
<content:encoded><![CDATA[
<div> Keywords: optimization, probability densities, multidimensional problems, splines, PET rebinning  
Summary:  
- The article presents an optimization problem to estimate probability densities in multidimensional problems with uneven sampling probability.  
- Detector sensitivity is considered as an heterogeneous density, utilizing splines on a grid for computational speed and flexible boundary conditions.  
- The method uses nuclear norm regularization on the spline's Hessian to promote sparsity, making it spatially adaptive and stable against the choice of regularization parameter.  
- The computational pipeline is tested on standard densities, with provided software for implementation.  
- A new approach to PET rebinning is showcased as an application of the framework.<br /><br />Summary: <div>
arXiv:2506.02323v1 Announce Type: cross 
Abstract: We formulate an optimization problem to estimate probability densities in the context of multidimensional problems that are sampled with uneven probability. It considers detector sensitivity as an heterogeneous density and takes advantage of the computational speed and flexible boundary conditions offered by splines on a grid. We choose to regularize the Hessian of the spline via the nuclear norm to promote sparsity. As a result, the method is spatially adaptive and stable against the choice of the regularization parameter, which plays the role of the bandwidth. We test our computational pipeline on standard densities and provide software. We also present a new approach to PET rebinning as an application of our framework.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning</title>
<link>https://arxiv.org/abs/2506.02485</link>
<guid>https://arxiv.org/abs/2506.02485</guid>
<content:encoded><![CDATA[
<div> Generative AI, Wildfire prediction, Multimodal approaches, 2D fire spread forecasting, 3D simulations 

Summary:
Generative AI models like GANs and VAEs show promise in improving wildfire prediction by integrating multimodal data and generating diverse scenarios. These models can enhance 2D fire spread forecasting and enable more realistic 3D simulations. A human-AI collaboration framework using large language models aids in automated knowledge extraction and literature synthesis. Five key visions for integrating generative AI into wildfire management include multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. The challenges of implementing these visions are also addressed, with proposed solutions to overcome them.<br /><br />Summary: <div>
arXiv:2506.02485v1 Announce Type: cross 
Abstract: Wildfires continue to inflict devastating human, environmental, and economic losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and the urgent demand for more effective response strategies. While physics-based and deep learning models have advanced wildfire simulation, they face critical limitations in predicting and visualizing multimodal fire spread in real time, particularly in both 2D and 3D spatial domains using dynamically updated GIS data. These limitations hinder timely emergency response, infrastructure protection, and community safety. Generative AI has recently emerged as a transformative approach across research and industry. Models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and diffusion-based architectures offer distinct advantages over traditional methods, including the integration of multimodal data, generation of diverse scenarios under uncertainty, and improved modeling of wildfire dynamics across spatial and temporal scales. This position paper advocates for the adoption of generative AI as a foundational framework for wildfire prediction. We explore how such models can enhance 2D fire spread forecasting and enable more realistic, scalable 3D simulations. Additionally, we employ a novel human-AI collaboration framework using large language models (LLMs) for automated knowledge extraction, literature synthesis, and bibliometric mapping. Looking ahead, we identify five key visions for integrating generative AI into wildfire management: multimodal approaches, AI foundation models, conversational AI systems, edge-computing-based scenario generation, and cognitive digital twins. We also address three major challenges accompanying these opportunities and propose potential solutions to support their implementation.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression</title>
<link>https://arxiv.org/abs/2506.02678</link>
<guid>https://arxiv.org/abs/2506.02678</guid>
<content:encoded><![CDATA[
<div> Dynamic ratio-based training, Large Language Models, efficient language reasoning, inference, System-1, System-2 <br />
<br />
Dynamic ratio-based training is proposed in this work to improve the efficiency of language reasoning in large language models. The method continuously balances the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. The approach is validated on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B models across a variety of benchmarks with varying difficulty levels. Results show a reduction of nearly 40% in the number of output tokens while maintaining reasoning accuracy. The research presents an innovative solution to the challenge of performing efficient language reasoning, particularly during inference with long outputs, without the need for sophisticated data annotations or interpolation between models. Code and data for the research will be made available soon. <br /><br />Summary: <div>
arXiv:2506.02678v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning--especially during inference with extremely long outputs--has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model's System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model's reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mesoscale phase-field model of intergranular liquid lithium corrosion of ferritic/martensitic steels</title>
<link>https://arxiv.org/abs/2506.02776</link>
<guid>https://arxiv.org/abs/2506.02776</guid>
<content:encoded><![CDATA[
<div> phase-field model, intergranular corrosion, ferritic/martensitic steels, liquid lithium, chromium concentration

Summary:
The article presents a phase-field model for simulating intergranular corrosion in ferritic/martensitic steels exposed to liquid lithium. By tracking the chromium concentration in the material, mass transport within the metal and liquid phases is analyzed. The model effectively captures intergranular corrosion by enhancing chromium diffusion along grain boundaries without the need for specific treatment. Results from simulations align closely with experimental measurements of weight loss and corrosion depth in a 9 wt% Cr steel at 600C. A sensitivity analysis reveals the influence of microstructural factors such as near-surface grain density and grain size on the corrosion process. The study also evaluates the impact of saturation on corrosion behavior. Overall, near-surface grain density is identified as a critical factor, while grain size is found to influence susceptibility to intergranular corrosion. <div>
arXiv:2506.02776v1 Announce Type: cross 
Abstract: A phase-field model is developed to simulate intergranular corrosion of ferritic/martensitic steels exposed to liquid lithium. The chromium concentration of the material is used to track the mass transport within the metal and liquid (corrosive) phase. The framework naturally captures intergranular corrosion by enhancing the diffusion of chromium along grain boundaries relative to the grain bulk with no special treatment for the corrosion front evolution. The formulation applies to arbitrary 2D and 3D polycrystalline geometries. The framework reproduces experimental measurements of weight loss and corrosion depth for a 9 wt\% Cr ferritic/martensitic steel exposed to static lithium at 600 $^\circ$C. A sensitivity analysis, varying near-surface grain density, grain size, and chromium depletion thickness, highlights the microstructural influence in the corrosion process. Moreover, the significance of saturation is considered and evaluated. Simulation results show that near-surface grain density is a deciding factor, whereas grain size dictates the susceptibility to intergranular corrosion.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.02911</link>
<guid>https://arxiv.org/abs/2506.02911</guid>
<content:encoded><![CDATA[
<div> cell type annotation, single-cell RNA sequencing data, CellPuzzles, large language models, batch-level accuracy

Summary:
Cell type annotation plays a critical role in analyzing single-cell RNA sequencing data heterogeneity. Current foundation models lack the ability to consider batch-level cellular context and provide explanatory reasoning in cell type annotation tasks. To address this, the CellPuzzles task was introduced to mimic expert annotation workflows, requiring unique cell type assignment across batches of cells. Existing large language models struggle with this task, with limited accuracy. In response, Cell-o1, a 7B large language model, was developed through supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 surpasses previous models, demonstrating superior performance and generalization across different contexts. The training dynamics and reasoning behaviors of Cell-o1 provide insights into improved batch-level annotation performance and expert-like reasoning strategies. <div>
arXiv:2506.02911v1 Announce Type: cross 
Abstract: Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to optimize convex risk measures: The cases of utility-based shortfall risk and optimized certainty equivalent risk</title>
<link>https://arxiv.org/abs/2506.01101</link>
<guid>https://arxiv.org/abs/2506.01101</guid>
<content:encoded><![CDATA[
<div> risk measures, estimation, optimization, gradient estimators, stochastic gradient algorithm

Summary: 
The article introduces the estimation and optimization of convex risk measures, specifically utility-based shortfall risk (UBSR) and Optimized Certainty Equivalent (OCE) risk, covering unbounded random variables. It extends various risk measures like entropic risk and Value-at-Risk. Non-asymptotic bounds are derived for mean absolute error and mean-squared error in estimation using sample average approximation (SAA) estimators. Expressions for UBSR and OCE gradients under smooth parameterization are provided, with gradient estimators proposed using the SAA estimator of UBSR and non-asymptotic bounds on error. A stochastic gradient algorithm is developed for optimization using these gradient estimators. Non-asymptotic convergence rate bounds are derived for the optimization of UBSR and OCE risk measures. <div>
arXiv:2506.01101v1 Announce Type: new 
Abstract: We consider the problems of estimation and optimization of two popular convex risk mea- sures: utility-based shortfall risk (UBSR) and Optimized Certainty Equivalent (OCE) risk. We extend these risk measures to cover possibly unbounded random variables. We cover prominent risk measures like the entropic risk, expectile risk, monotone mean-variance risk, Value-at-Risk, and Conditional Value-at-Risk as few special cases of either the UBSR or the OCE risk. In the context of estimation, we derive non-asymptotic bounds on the mean absolute error (MAE) and mean-squared error (MSE) of the classical sample average approximation (SAA) estimators of both, the UBSR and the OCE. Next, in the context of optimization, we derive expressions for the UBSR gradient and the OCE gradient under a smooth parameterization. Utilizing these expres- sions, we propose gradient estimators for both, the UBSR and the OCE. We use the SAA estimator of UBSR in both these gradient estimators, and derive non-asymptotic bounds on MAE and MSE for the proposed gradient estimation schemes. We incorporate the aforementioned gradient estima- tors into a stochastic gradient (SG) algorithm for optimization. Finally, we derive non-asymptotic bounds that quantify the rate of convergence of our SG algorithm for the optimization of the UBSR and the OCE risk measure
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast numerical generation of Lie closure</title>
<link>https://arxiv.org/abs/2506.01120</link>
<guid>https://arxiv.org/abs/2506.01120</guid>
<content:encoded><![CDATA[
<div> Keywords: Lie algebra, matrix, numerical construction, quantum computing, linear independence<br />
Summary:<br />
The article discusses the importance of finding the Lie-algebraic closure of matrices in quantum computing and quantum control. Analytically determining the closure is challenging for most cases, leading to a need for numerical construction. The standard algorithm for this construction relies on a subroutine to check linear independence, which can be computationally intensive. The authors present efficient methods for linear independence checks that reduce computational complexity and memory usage. One of these methods is implemented and validated against known results. These new algorithms allow for the exploration of Lie closure in larger system sizes that were previously unattainable, opening up possibilities for numerical studies in quantum computing and control applications. <div>
arXiv:2506.01120v1 Announce Type: new 
Abstract: Finding the Lie-algebraic closure of a handful of matrices has important applications in quantum computing and quantum control. For most realistic cases, the closure cannot be determined analytically, necessitating an explicit numerical construction. The standard construction algorithm makes repeated calls to a subroutine that determines whether a matrix is linearly independent from a potentially large set of matrices. Because the common implementation of this subroutine has a high complexity, the construction of Lie closure is practically limited to trivially small matrix sizes. We present efficient alternative methods of linear independence check that simultaneously reduce the computational complexity and memory footprint. An implementation of one of the methods is validated against known results. Our new algorithms enable numerical studies of Lie closure in larger system sizes than was previously possible.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emerging ML-AI Techniques for Analog and RF EDA</title>
<link>https://arxiv.org/abs/2506.00007</link>
<guid>https://arxiv.org/abs/2506.00007</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, EDA workflows, analog design, RF circuits, optimization techniques

Summary: 
This survey delves into the integration of machine learning (ML) into electronic design automation (EDA) workflows specifically tailored for analog and RF circuits. The challenges unique to analog design, such as complex constraints, nonlinear design spaces, and high computational costs, are addressed. The review encompasses state-of-the-art ML and optimization techniques for various circuit tasks, including constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey emphasizes how ML can enhance automation, elevate design quality, and reduce time-to-market while meeting desired circuit specifications. In addition, emerging trends and cross-cutting challenges, like robustness to variations and considerations of interconnect parasitics, are explored. Overall, the survey underscores the potential of ML to revolutionize analog and RF circuit design by optimizing workflow efficiency and achieving superior design outcomes. 

<br /><br />Summary: <div>
arXiv:2506.00007v1 Announce Type: cross 
Abstract: This survey explores the integration of machine learning (ML) into EDA workflows for analog and RF circuits, addressing challenges unique to analog design, which include complex constraints, nonlinear design spaces, and high computational costs. State-of-the-art learning and optimization techniques are reviewed for circuit tasks such as constraint formulation, topology generation, device modeling, sizing, placement, and routing. The survey highlights the capability of ML to enhance automation, improve design quality, and reduce time-to-market while meeting the target specifications of an analog or RF circuit. Emerging trends and cross-cutting challenges, including robustness to variations and considerations of interconnect parasitics, are also discussed.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Vessel Behavior using AIS Trajectory Data and Markovian Models in the Gulf of St. Lawrence</title>
<link>https://arxiv.org/abs/2506.00025</link>
<guid>https://arxiv.org/abs/2506.00025</guid>
<content:encoded><![CDATA[
<div> Keywords: maritime mobility, spatio-temporal analysis, vessel movement patterns, discrete-time Markov chains, COVID-19 pandemic <br />
Summary: <br />
This article presents a spatio-temporal analytical framework using discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, focusing on changes during the COVID-19 pandemic. The ocean space is divided into hexagonal cells, and mobility signatures for different vessel types are constructed based on cell transitions and dwell time. Origin-destination matrices and spatial transition probability models are developed to understand vessel dynamics at different time scales. The study reveals consistent mobility signatures for specific vessel types across different regions, suggesting underlying behavioral patterns. During the pandemic, passenger and fishing vessels show significant temporal deviations, reflecting the impact of social isolation measures and operational restrictions on non-essential maritime activities in the region. These findings contribute to a better understanding of maritime mobility patterns and highlight the influence of external factors on vessel movements. <br /> <div>
arXiv:2506.00025v1 Announce Type: cross 
Abstract: Maritime Mobility is at the center of the global economy, and analyzing and understanding such data at scale is critical for ocean conservation and governance. Accordingly, this work introduces a spatio-temporal analytical framework based on discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, emphasizing changes induced during the COVID-19 pandemic. We discretize the ocean space into hexagonal cells and construct mobility signatures for individual vessel types using the frequency of cell transitions and the dwell time within each cell. These features are used to build origin-destination matrices and spatial transition probability models that characterize vessel dynamics at different temporal resolutions. Under multiple vessel types, we contribute with a temporal evolution analysis of mobility patterns during pandemic times, highlighting significant but transient changes to recurring transportation behaviors. Our findings indicate vessel-specific mobility signatures consistent across spatially disjoint regions, suggesting that those are latent behavioral invariants. Besides, we observe significant temporal deviations among passenger and fishing vessels during the pandemic, indicating a strong influence of social isolation policies and operational limitations imposed on non-essential maritime activity in this region.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risks of AI-driven product development and strategies for their mitigation</title>
<link>https://arxiv.org/abs/2506.00047</link>
<guid>https://arxiv.org/abs/2506.00047</guid>
<content:encoded><![CDATA[
<div> progressing, automated product development, risks, mitigation strategies, AI-driven product development
<br />
Summary:
Humanity is moving towards automated product development to accelerate technological progress, but this trend poses risks that must be addressed. To mitigate these risks, principles for safer AI-driven product development are outlined, emphasizing human oversight, accountability, and explainable design. The risk assessment includes technical risks affecting product quality and safety, as well as sociotechnical risks impacting society. While AI-driven product development is still evolving, this discussion aims to balance opportunities and risks without hindering progress in understanding, norm-setting, and regulation. <div>
arXiv:2506.00047v1 Announce Type: cross 
Abstract: Humanity is progressing towards automated product development, a trend that promises faster creation of better products and thus the acceleration of technological progress. However, increasing reliance on non-human agents for this process introduces many risks. This perspective aims to initiate a discussion on these risks and appropriate mitigation strategies. To this end, we outline a set of principles for safer AI-driven product development which emphasize human oversight, accountability, and explainable design, among others. The risk assessment covers both technical risks which affect product quality and safety, and sociotechnical risks which affect society. While AI-driven product development is still in its early stages, this discussion will help balance its opportunities and risks without delaying essential progress in understanding, norm-setting, and regulation.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance</title>
<link>https://arxiv.org/abs/2506.00494</link>
<guid>https://arxiv.org/abs/2506.00494</guid>
<content:encoded><![CDATA[
<div> Keywords: Soft Fin-Ray grippers, multi-objective optimization, finite element method, multilayer perception, non-dominated sorting genetic algorithm

Summary:<br />
Soft Fin-Ray grippers are effective for delicate manipulation but face challenges in modeling grasp force and deformation for design purposes. The study uses finite element method (FEM) to estimate deflections and contact forces of the gripper when grasping cylindrical objects, creating a dataset for predicting contact force and tip displacement using a multilayer perception (MLP). The dataset includes design variables related to beam thickness and spacing, with target features of maximum contact forces and tip displacements. A multi-objective optimization problem is addressed, balancing the trade-off between force and delicate manipulation. The non-dominated sorting genetic algorithm (NSGA-II) is used to find optimized design solutions. The methodologies presented in the study can enhance the design and gripping performance of soft robotic grippers, aiding in choosing designs suitable for both delicate grasping and high-force applications.<br />Summary: <div>
arXiv:2506.00494v1 Announce Type: cross 
Abstract: Soft Fin-Ray grippers can perform delicate and careful manipulation, which has caused notable attention in different fields. These grippers can handle objects of various forms and sizes safely. The internal structure of the Fin-Ray finger plays a significant role in its adaptability and grasping performance. However, modeling the non-linear grasp force and deformation behaviors for design purposes is challenging. Moreover, when the Fin-Ray finger becomes more rigid and capable of exerting higher forces, it becomes less delicate in handling objects. The contrast between these two objectives gives rise to a multi-objective optimization problem. In this study, we employ finite element method (FEM) to estimate the deflections and contact forces of the Fin-Ray, grasping cylindrical objects. This dataset is then used to construct a multilayer perception (MLP) for prediction of the contact force and the tip displacement. The FEM dataset consists of three input and four target features. The three input features of the MLP and optimization design variables are the thickness of the front and supporting beams, the thickness of the cross beams, and the equal spacing between the cross beams. In addition, the target features are the maximum contact forces and maximum tip displacements in x- and y-directions. The magnitude of maximum contact force and magnitude of maximum tip displacement are the two objectives, showing the trade-off between force and delicate manipulation in soft Fin-Ray grippers. Furthermore, the optimized set of solutions are found using multi-objective optimal techniques. We use non-dominated sorting genetic algorithm (NSGA-II) method for this purpose. Our findings demonstrate that our methodologies can be used to improve the design and gripping performance of soft robotic grippers, helping us to choose a design not only for delicate grasping but also for high-force applications.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling the Spread of Epidemics on Networks with Differential Privacy</title>
<link>https://arxiv.org/abs/2506.00745</link>
<guid>https://arxiv.org/abs/2506.00745</guid>
<content:encoded><![CDATA[
<div> strategies, epidemic spread, vaccination, contact network, differential privacy<br />
<br />
Summary: Designing effective vaccination strategies for controlling epidemic spread on heterogeneous contact networks is crucial, especially when sensitive information is involved and privacy guarantees are needed. This study introduces $(\varepsilon,\delta)$-differentially private algorithms for reducing the maximum degree and spectral radius to design optimal vaccination strategies. A private algorithm for the multi-set multi-cover problem is developed to control network properties while preserving privacy. The tradeoff between privacy and utility of these algorithms is evaluated on various synthetic and real-world networks, demonstrating their effectiveness in controlling epidemic spread while maintaining privacy. <div>
arXiv:2506.00745v1 Announce Type: cross 
Abstract: Designing effective strategies for controlling epidemic spread by vaccination is an important question in epidemiology, especially in the early stages when vaccines are limited. This is a challenging question when the contact network is very heterogeneous, and strategies based on controlling network properties, such as the degree and spectral radius, have been shown to be effective. Implementation of such strategies requires detailed information on the contact structure, which might be sensitive in many applications. Our focus here is on choosing effective vaccination strategies when the edges are sensitive and differential privacy guarantees are needed. Our main contributions are $(\varepsilon,\delta)$-differentially private algorithms for designing vaccination strategies by reducing the maximum degree and spectral radius. Our key technique is a private algorithm for the multi-set multi-cover problem, which we use for controlling network properties. We evaluate privacy-utility tradeoffs of our algorithms on multiple synthetic and real-world networks, and show their effectiveness.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking</title>
<link>https://arxiv.org/abs/2506.01093</link>
<guid>https://arxiv.org/abs/2506.01093</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time transaction monitoring, graph-based modeling, narrative field embedding, generative explanation, financial compliance <br />
Summary: 
The paper introduces a real-time transaction monitoring framework that combines graph-based modeling, narrative field embedding, and generative explanation to automate financial compliance. The system creates dynamic transaction graphs, extracts features, and detects suspicious behavior using a graph neural network. It also generates natural language explanations aligned with regulatory clauses for flagged transactions. Experimental results on simulated financial data demonstrate high performance metrics with a 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation confirms the quality and interpretability of generated justifications. The study showcases the potential of integrating graph intelligence and generative models for explainable compliance in high-risk financial settings. <br /><br />Summary: <div>
arXiv:2506.01093v1 Announce Type: cross 
Abstract: This paper presents a real-time transaction monitoring framework that integrates graph-based modeling, narrative field embedding, and generative explanation to support automated financial compliance. The system constructs dynamic transaction graphs, extracts structural and contextual features, and classifies suspicious behavior using a graph neural network. A retrieval-augmented generation module generates natural language explanations aligned with regulatory clauses for each flagged transaction. Experiments conducted on a simulated stream of financial data show that the proposed method achieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0% recall. Expert evaluation further confirms the quality and interpretability of generated justifications. The findings demonstrate the potential of combining graph intelligence and generative models to support explainable, audit-ready compliance in high-risk financial environments.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COALESCE: Economic and Security Dynamics of Skill-Based Task Outsourcing Among Team of Autonomous LLM Agents</title>
<link>https://arxiv.org/abs/2506.01900</link>
<guid>https://arxiv.org/abs/2506.01900</guid>
<content:encoded><![CDATA[
<div> framework, autonomous LLM agents, cost optimization, task outsourcing, agent economies

Summary:
COALESCE is a framework designed to optimize resource utilization in autonomous Large Language Model (LLM) agents by enabling them to outsource specific subtasks to cost-effective third-party agents. The framework incorporates hybrid skill representation, dynamic skill discovery, task decomposition, cost comparison models, decision-making algorithms, and a communication protocol. The theoretical simulations show a 41.8% cost reduction potential, while empirical validation confirms a 20.3% cost reduction with epsilon-greedy exploration. The framework aims to leverage open standards like Google's Agent2Agent protocol to foster efficient agent interactions, reduce operational costs, enhance scalability, and create specialized agent economies. By facilitating a dynamic market for agent capabilities, COALESCE enables complex LLM agent functionalities to become more accessible and economically viable. 

<br /><br />Summary: <div>
arXiv:2506.01900v1 Announce Type: cross 
Abstract: The meteoric rise and proliferation of autonomous Large Language Model (LLM) agents promise significant capabilities across various domains. However, their deployment is increasingly constrained by substantial computational demands, specifically for Graphics Processing Unit (GPU) resources. This paper addresses the critical problem of optimizing resource utilization in LLM agent systems. We introduce COALESCE (Cost-Optimized and Secure Agent Labour Exchange via Skill-based Competence Estimation), a novel framework designed to enable autonomous LLM agents to dynamically outsource specific subtasks to specialized, cost-effective third-party LLM agents. The framework integrates mechanisms for hybrid skill representation, dynamic skill discovery, automated task decomposition, a unified cost model comparing internal execution costs against external outsourcing prices, simplified market-based decision-making algorithms, and a standardized communication protocol between LLM agents. Comprehensive validation through 239 theoretical simulations demonstrates 41.8\% cost reduction potential, while large-scale empirical validation across 240 real LLM tasks confirms 20.3\% cost reduction with proper epsilon-greedy exploration, establishing both theoretical viability and practical effectiveness. The emergence of proposed open standards like Google's Agent2Agent (A2A) protocol further underscores the need for frameworks like COALESCE that can leverage such standards for efficient agent interaction. By facilitating a dynamic market for agent capabilities, potentially utilizing protocols like A2A for communication, COALESCE aims to significantly reduce operational costs, enhance system scalability, and foster the emergence of specialized agent economies, making complex LLM agent functionalities more accessible and economically viable.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Guided Diffusion Model for Accelerating Computational Fluid Dynamics</title>
<link>https://arxiv.org/abs/2504.04375</link>
<guid>https://arxiv.org/abs/2504.04375</guid>
<content:encoded><![CDATA[
<div> diffusion models, fluid dynamics computation, machine learning, numerical solvers, turbulent flow<br />
Summary:<br />
Machine learning methods, such as diffusion models, aim to accelerate high-fidelity fluid dynamics computation by utilizing low-fidelity data produced by numerical solvers. However, existing approaches struggle to reconstruct fine-scale details when using solver-generated low-fidelity inputs. To address this issue, SG-Diff, a novel diffusion model, is proposed. It incorporates an Importance Weight strategy during training to focus on intricate fluid details and a Predictor-Corrector-Advancer SDE solver to embed physical guidance into the diffusion sampling process. Experimental results on turbulent flow datasets demonstrate SG-Diff's effectiveness in achieving more accurate reconstructions compared to state-of-the-art baselines. <br />Summary: <div>
arXiv:2504.04375v2 Announce Type: replace 
Abstract: Machine learning methods, such as diffusion models, are widely explored as a promising way to accelerate high-fidelity fluid dynamics computation via a super-resolution process from faster-to-compute low-fidelity input. However, existing approaches usually make impractical assumptions that the low-fidelity data is down-sampled from high-fidelity data. In reality, low-fidelity data is produced by numerical solvers that use a coarser resolution. Solver-generated low-fidelity data usually sacrifices fine-grained details, such as small-scale vortices compared to high-fidelity ones. Our findings show that SOTA diffusion models struggle to reconstruct fine-scale details when faced with solver-generated low-fidelity inputs. To bridge this gap, we propose SG-Diff, a novel diffusion model for reconstruction, where both low-fidelity inputs and high-fidelity targets are generated from numerical solvers. We propose an \textit{Importance Weight} strategy during training that serves as a form of self-guidance, focusing on intricate fluid details, and a \textit{Predictor-Corrector-Advancer} SDE solver that embeds physical guidance into the diffusion sampling process. Together, these techniques steer the diffusion model toward more accurate reconstructions. Experimental results on four 2D turbulent flow datasets demonstrate the efficacy of \model~against state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning in Business Analytics: A Clash of Expectations and Reality</title>
<link>https://arxiv.org/abs/2205.09337</link>
<guid>https://arxiv.org/abs/2205.09337</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, machine learning, structured datasets, gradient boosting, business analytics

Summary:
Deep learning, a popular tool in the field of artificial intelligence and machine learning, faces challenges in widespread adoption within business analytics. These challenges include computational complexity, lack of big data architecture, black-box transparency, skill shortages, and leadership commitment. Despite its benefits, deep learning may not outperform traditional machine learning models for structured datasets with fixed-length feature vectors. The study suggests that gradient boosting models are more suitable for making predictions on structured datasets in business analytics. This finding highlights the importance of viewing deep learning as a complement to existing machine learning models rather than a universal solution. The paper provides insights from empirical studies in three industry use cases, discussing practical implications and outlining future research directions. 

<br /><br />Summary: <div>
arXiv:2205.09337v2 Announce Type: replace-cross 
Abstract: Our fast-paced digital economy shaped by global competition requires increased data-driven decision-making based on artificial intelligence (AI) and machine learning (ML). The benefits of deep learning (DL) are manifold, but it comes with limitations that have, so far, interfered with widespread industry adoption. This paper explains why DL, despite its popularity, has difficulties speeding up its adoption within business analytics. It is shown that the adoption of deep learning is not only affected by computational complexity, lacking big data architecture, lack of transparency (black-box), skill shortage, and leadership commitment, but also by the fact that DL does not outperform traditional ML models in the case of structured datasets with fixed-length feature vectors. Deep learning should be regarded as a powerful addition to the existing body of ML models instead of a one size fits all solution. The results strongly suggest that gradient boosting can be seen as the go-to model for predictions on structured datasets within business analytics. In addition to the empirical study based on three industry use cases, the paper offers a comprehensive discussion of those results, practical implications, and a roadmap for future research.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated machine learning: AI-driven decision making in business analytics</title>
<link>https://arxiv.org/abs/2205.10538</link>
<guid>https://arxiv.org/abs/2205.10538</guid>
<content:encoded><![CDATA[
<div> AutoML, industrial machine learning, H2O AutoML framework, business analytics, automated decision-making <br />
Summary: <br />
The rise of AI-driven decision-making in the business world has led to a surge in interest in industrial machine learning applications. The shortage of analytics experts can be addressed by enhancing the user-friendliness of ML frameworks. Automated machine learning (AutoML) offers a solution by providing automated off-the-shelf solutions for model selection and hyperparameter tuning. In a study comparing the H2O AutoML framework with a manually tuned stacked ML model, the manual model outperformed in all three case studies, but the H2O AutoML package showed promising results. It is fast, easy to use, and delivers reliable results close to a professionally tuned model. This tool can aid in fast prototyping, shorten development cycles, and bridge the gap between demand and supply for ML experts. AutoML has the potential to empower individuals in an increasingly automated and digital world. <br /> <div>
arXiv:2205.10538v2 Announce Type: replace-cross 
Abstract: The realization that AI-driven decision-making is indispensable in today's fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards automated decisions in business analytics. Finally, AutoML has the potential to foster human empowerment in a world that is rapidly becoming more automated and digital.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETS-C: Leveraging Text Embedding for Time Series Classification</title>
<link>https://arxiv.org/abs/2407.06533</link>
<guid>https://arxiv.org/abs/2407.06533</guid>
<content:encoded><![CDATA[
<div> Keywords: language modeling, time series data, text embedding model, convolutional neural networks, lightweight model architecture <br />
Summary: 
This study introduces a novel approach for time series classification using a text embedding model in conjunction with a lightweight classification head. While previous methods focused on fine-tuning large language models for time series data, this new approach, called LETS-C, outperforms the state-of-the-art models in classification accuracy. The LETS-C model combines text embeddings with a simple classification head composed of convolutional neural networks and multilayer perceptron. By utilizing text embeddings to encode time series data, the LETS-C model achieves high performance while keeping a lightweight architecture. Through extensive experiments on a well-established time series classification benchmark, it was found that LETS-C requires only 14.5% of the trainable parameters compared to the current SOTA model. This suggests that leveraging text embedding models for time series classification tasks presents a promising direction for achieving high performance with a simpler model architecture. <br /><br />Summary: <div>
arXiv:2407.06533v2 Announce Type: replace-cross 
Abstract: Recent advancements in language modeling have shown promising results when applied to time series data. In particular, fine-tuning pre-trained large language models (LLMs) for time series classification tasks has achieved state-of-the-art (SOTA) performance on standard benchmarks. However, these LLM-based models have a significant drawback due to the large model size, with the number of trainable parameters in the millions. In this paper, we propose an alternative approach to leveraging the success of language modeling in the time series domain. Instead of fine-tuning LLMs, we utilize a text embedding model to embed time series and then pair the embeddings with a simple classification head composed of convolutional neural networks (CNN) and multilayer perceptron (MLP). We conducted extensive experiments on a well-established time series classification benchmark. We demonstrated LETS-C not only outperforms the current SOTA in classification accuracy but also offers a lightweight solution, using only 14.5% of the trainable parameters on average compared to the SOTA model. Our findings suggest that leveraging text embedding models to encode time series data, combined with a simple yet effective classification head, offers a promising direction for achieving high-performance time series classification while maintaining a lightweight model architecture.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Much is Enough? The Diminishing Returns of Tokenization Training Data</title>
<link>https://arxiv.org/abs/2502.20273</link>
<guid>https://arxiv.org/abs/2502.20273</guid>
<content:encoded><![CDATA[
<div> tokenization, hyperparameter, training data size, BPE, UnigramLM, WordPiece
<br />
Summary: 
This study examines the impact of tokenizer training data size on tokenization quality in natural language processing. By training BPE, UnigramLM, and WordPiece tokenizers on varying English training data sizes from 1GB to 900GB, it is found that improvements in tokenization quality diminish beyond 150GB due to constraints introduced by the pre-tokenization stage. The saturation effect is observed in both English and Russian data, indicating a practical limit to tokenization quality improvements achievable through increasing data size. This insight can guide the optimization of tokenization processes, reducing the computational resources needed for training on large corpora. Further research directions in tokenization algorithms are suggested based on these findings. 
<br /> <div>
arXiv:2502.20273v2 Announce Type: replace-cross 
Abstract: Tokenization, a crucial initial step in natural language processing, is governed by several key parameters, such as the tokenization algorithm, vocabulary size, pre-tokenization strategy, inference strategy, and training data corpus. This paper investigates the impact of an often-overlooked hyperparameter, tokenizer training data size. We train BPE, UnigramLM, and WordPiece tokenizers across various vocabulary sizes using English training data ranging from 1GB to 900GB. Our findings reveal diminishing returns as training data size increases beyond roughly 150GB, suggesting a practical limit to the improvements in tokenization quality achievable through additional data. We analyze this phenomenon and attribute the saturation effect to constraints introduced by the pre-tokenization stage. We then demonstrate the extent to which these findings can generalize by experimenting on data in Russian, a language typologically distant from English. While the limit appears to materialize at a later phase of pre-training, around 200GB, it is in fact observed. These results provide valuable insights for optimizing the tokenization process by reducing the compute required for training on large corpora and suggest promising directions for future research in tokenization algorithms.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Singularity Protocol for Cross Chain AMM without Intermediate Tokens or Bridges</title>
<link>https://arxiv.org/abs/2505.24337</link>
<guid>https://arxiv.org/abs/2505.24337</guid>
<content:encoded><![CDATA[
<div> AMMs, decentralized exchange, cross-chain swaps, liquidity, blockchain<br />
Summary:<br />
Automated Market Makers (AMMs) have revolutionized decentralized exchanges but face scalability challenges with cross-chain swaps. The current double-sided AMMs are inefficient and introduce risks like volatility and blockchain issues. This paper proposes a new class of AMMs that eliminate the need for intermediate tokens or bridging, enabling efficient cross-chain swaps with lower gas requirements. The new technology is based on an invariant that does not rely on bi-state dependency between assets being swapped. This innovation supports cross-chain swaps across various blockchain layers and offers a more streamlined and secure method for value transfer swaps. The proposed solution addresses the limitations of existing AMMs and provides a promising approach for improving liquidity and efficiency in cross-chain transactions. <br /><br /> <div>
arXiv:2505.24337v1 Announce Type: new 
Abstract: Automated Market Makers (AMMs) are decentralized exchange protocols that provide continuous access to token liquidity without the need for order books or traditional market makers. However, this innovation has failed to scale when it comes to cross-chain swaps. Modern cross-chain swaps employ double-sided AMMs, which are not only inefficient due to liquidity fragmentation but also require an intermediate token. This introduces inherent volatility risk as well as blockchain and bridging risk, especially in the case of wrapped tokens. This paper describes the inefficiencies of existing AMM invariants, particularly their mixed polynomial nature, and derives a new class of AMMs that do not have bi-state dependency between the assets being swapped. We propose a novel method of value transfer swaps using the described invariant that mitigates the need for bi-state dependency and eliminates the need for intermediate tokens or bridging. Furthermore, we show how this mechanism enables efficient cross-chain swaps with lower gas requirements and no bridging risks. The proposed technology is designed to support cross-chain swaps across any permutation of L1, L2, and L3 blockchains.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Black Box: Interpretability of LLMs in Finance</title>
<link>https://arxiv.org/abs/2505.24650</link>
<guid>https://arxiv.org/abs/2505.24650</guid>
<content:encoded><![CDATA[
<div> interpretability, financial services, large language models, transparency, regulatory compliance

Summary: 
This paper introduces the concept of mechanistic interpretability in the context of Large Language Models (LLMs) in the financial services sector. LLMs have shown great potential in various financial tasks but their complexity and lack of transparency raise concerns in the regulated financial industry. Mechanistic interpretability offers a transparent way to understand LLM behavior by reverse-engineering their internal workings, providing insights into how specific features influence predictions and allowing for the modification of model behavior. The paper explores the theoretical aspects of mechanistic interpretability and demonstrates its practical relevance through financial use cases such as trading strategies, sentiment analysis, bias detection, and hallucination detection. The adoption of advanced interpretability tools is expected to be crucial as LLM usage increases, ensuring that AI systems in finance are ethical, transparent, and compliant with evolving regulations. The paper emphasizes how these techniques can address interpretability requirements for regulatory and compliance purposes in the financial sector. 

<br /><br />Summary: <div>
arXiv:2505.24650v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across a spectrum of tasks in financial services, including report generation, chatbots, sentiment analysis, regulatory compliance, investment advisory, financial knowledge retrieval, and summarization. However, their intrinsic complexity and lack of transparency pose significant challenges, especially in the highly regulated financial sector, where interpretability, fairness, and accountability are critical. As far as we are aware, this paper presents the first application in the finance domain of understanding and utilizing the inner workings of LLMs through mechanistic interpretability, addressing the pressing need for transparency and control in AI systems. Mechanistic interpretability is the most intuitive and transparent way to understand LLM behavior by reverse-engineering their internal workings. By dissecting the activations and circuits within these models, it provides insights into how specific features or components influence predictions - making it possible not only to observe but also to modify model behavior. In this paper, we explore the theoretical aspects of mechanistic interpretability and demonstrate its practical relevance through a range of financial use cases and experiments, including applications in trading strategies, sentiment analysis, bias, and hallucination detection. While not yet widely adopted, mechanistic interpretability is expected to become increasingly vital as adoption of LLMs increases. Advanced interpretability tools can ensure AI systems remain ethical, transparent, and aligned with evolving financial regulations. In this paper, we have put special emphasis on how these techniques can help unlock interpretability requirements for regulatory and compliance purposes - addressing both current needs and anticipating future expectations from financial regulators globally.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Bayesian multi-fidelity inverse analysis for expensive and non-differentiable physics-based simulations in high stochastic dimensions</title>
<link>https://arxiv.org/abs/2505.24708</link>
<guid>https://arxiv.org/abs/2505.24708</guid>
<content:encoded><![CDATA[
<div> Bayesian, multi-fidelity, inverse analysis, high-dimensional, computational<br />
Summary:<br />
This article introduces a novel approach called Bayesian multi-fidelity inverse analysis (BMFIA) to address the challenges of high-dimensional Bayesian inverse analysis for computationally demanding, nonlinear physics-based high-fidelity models. The method leverages simpler lower-fidelity models designed to provide model derivatives, learning a probabilistic dependence between the lower and higher-fidelity models. This allows for statistically correcting the inaccurate lower-fidelity responses in an altered likelihood formulation. BMFIA is fully differentiable and can be applied to a wide range of scenarios, including finely-resolved spatial reconstruction problems for nonlinear and transient coupled poro-elastic media physics. The approach is demonstrated to solve Bayesian inverse problems efficiently, even with a small amount of data, making it a valuable tool for addressing complex multi-physics problems. <div>
arXiv:2505.24708v1 Announce Type: new 
Abstract: High-dimensional Bayesian inverse analysis (dim >> 100) is mostly unfeasible for computationally demanding, nonlinear physics-based high-fidelity (HF) models. Usually, the use of more efficient gradient-based inference schemes is impeded if the multi-physics models are provided by complex legacy codes. Adjoint-based derivatives are either exceedingly cumbersome to derive or non-existent for practically relevant large-scale nonlinear and coupled multi-physics problems. Similarly, holistic automated differentiation w.r.t. primary variables of multi-physics codes is usually not yet an option and requires extensive code restructuring if not considered from the outset in the software design. This absence of differentiability further exacerbates the already present computational challenges. To overcome the existing limitations, we propose a novel inference approach called Bayesian multi-fidelity inverse analysis (BMFIA), which leverages simpler and computationally cheaper lower-fidelity (LF) models that are designed to provide model derivatives. BMFIA learns a simple, probabilistic dependence of the LF and HF models, which is then employed in an altered likelihood formulation to statistically correct the inaccurate LF response. From a Bayesian viewpoint, this dependence represents a multi-fidelity conditional density (discriminative model). We demonstrate how this multi-fidelity conditional density can be learned robustly in the small data regime from only a few HF and LF simulations (50 to 300), which would not be sufficient for naive surrogate approaches. The formulation is fully differentiable and allows the flexible design of a wide range of LF models. We demonstrate that BMFIA solves Bayesian inverse problems for scenarios that used to be prohibitive, such as finely-resolved spatial reconstruction problems for nonlinear and transient coupled poro-elastic media physics.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Surprising Soupability of Documents in State Space Models</title>
<link>https://arxiv.org/abs/2505.24033</link>
<guid>https://arxiv.org/abs/2505.24033</guid>
<content:encoded><![CDATA[
<div> Keywords: Structured State Space Models, document souping, Mamba2 models, multi-hop QA, long-document reasoning

Summary:
Structured State Space Models (SSMs) are investigated to determine if their hidden states can be merged post-hoc to support downstream reasoning. A strategy called document souping is proposed, where documents are encoded independently and their representations are pooled into a single context state. This approach allows for modular encoding and reuse without the need to reprocess the full input for each query. Mamba2 models are modified to produce soupable representations, enabling support for multi-hop QA, sparse retrieval, and long-document reasoning with high accuracy. In experiments on HotpotQA, souping ten independently encoded documents achieves performance near that of a cross-encoder trained on the same inputs. Overall, this method demonstrates the effectiveness of combining independently encoded document representations for improved downstream reasoning tasks. 

Summary: <div>
arXiv:2505.24033v1 Announce Type: cross 
Abstract: We investigate whether hidden states from Structured State Space Models (SSMs) can be merged post-hoc to support downstream reasoning. Inspired by model souping, we propose a strategy where documents are encoded independently and their representations are pooled -- via simple operations like averaging -- into a single context state. This approach, which we call document souping, enables modular encoding and reuse without reprocessing the full input for each query. We finetune Mamba2 models to produce soupable representations and find that they support multi-hop QA, sparse retrieval, and long-document reasoning with strong accuracy. On HotpotQA, souping ten independently encoded documents nearly matches the performance of a cross-encoder trained on the same inputs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transaction Proximity: A Graph-Based Approach to Blockchain Fraud Prevention</title>
<link>https://arxiv.org/abs/2505.24284</link>
<guid>https://arxiv.org/abs/2505.24284</guid>
<content:encoded><![CDATA[
<div> Keywords: fraud-deterrent, public blockchains, transaction proximity, Easily Attainable Identities (EAIs), directed graph analysis

Summary: 
This paper presents a fraud-deterrent access validation system for public blockchains using Transaction Proximity and Easily Attainable Identities (EAIs) concepts. The system analyzes transaction patterns to identify wallets closely connected to centralized exchanges, aiming to prevent fraudulent activities. The analysis of the Ethereum blockchain reveals a high percentage of large USDC wallets being EAI or within one transaction hop of an EAI. Moreover, a significant number of past exploits were found to not involve EAIs, highlighting the need for such a validation system. Three implementation approaches are proposed, balancing gas cost and privacy considerations. This approach allows for programmatic compliance without restricting access or sharing personal information, maintaining blockchain openness and enabling protocols to implement customized validation systems.<br /><br />Summary: <div>
arXiv:2505.24284v1 Announce Type: cross 
Abstract: This paper introduces a fraud-deterrent access validation system for public blockchains, leveraging two complementary concepts: "Transaction Proximity", which measures the distance between wallets in the transaction graph, and "Easily Attainable Identities (EAIs)", wallets with direct transaction connections to centralized exchanges. Recognizing the limitations of traditional approaches like blocklisting (reactive, slow) and strict allow listing (privacy-invasive, adoption barriers), we propose a system that analyzes transaction patterns to identify wallets with close connections to centralized exchanges.
  Our directed graph analysis of the Ethereum blockchain reveals that 56% of large USDC wallets (with a lifetime maximum balance greater than \$10,000) are EAI and 88% are within one transaction hop of an EAI. For transactions exceeding \$2,000, 91% involve at least one EAI. Crucially, an analysis of past exploits shows that 83% of the known exploiter addresses are not EAIs, with 21% being more than five hops away from any regulated exchange. We present three implementation approaches with varying gas cost and privacy tradeoffs, demonstrating that EAI-based access control can potentially prevent most of these incidents while preserving blockchain openness. Importantly, our approach does not restrict access or share personally identifiable information, but it provides information for protocols to implement their own validation or risk scoring systems based on specific needs. This middle-ground solution enables programmatic compliance while maintaining the core values of open blockchain.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking for Attention: Randomized Attention Test Design for Validator Monitoring in Optimistic Rollups</title>
<link>https://arxiv.org/abs/2505.24393</link>
<guid>https://arxiv.org/abs/2505.24393</guid>
<content:encoded><![CDATA[
<div> scalability, blockchain, Optimistic Rollups, Randomized Attention Test, game-theoretic analysis

Summary:
Optimistic Rollups (ORUs) enhance blockchain scalability but face the verifier's dilemma due to a lack of mechanisms ensuring validator attentiveness. The Randomized Attention Test (RAT) protocol is introduced to challenge validators in ORUs, verifying their liveness and readiness. Game-theoretic analysis shows that an Ideal Security Equilibrium can be achieved with RAT, where validators are attentive and proposers honest. This equilibrium is attainable with low penalties for non-responsive validators and a low attention test frequency. RAT serves as a practical mechanism to enforce validator diligence, increasing the security and integrity of ORU systems without significant additional costs. <div>
arXiv:2505.24393v1 Announce Type: cross 
Abstract: Optimistic Rollups (ORUs) significantly enhance blockchain scalability but inherently suffer from the verifier's dilemma, particularly concerning validator attentiveness. Current systems lack mechanisms to proactively ensure validators are diligently monitoring L2 state transitions, creating a vulnerability where fraudulent states could be finalized. This paper introduces the Randomized Attention Test (RAT), a novel L1-based protocol designed to probabilistically challenge validators in ORUs, thereby verifying their liveness and computational readiness. Our game-theoretic analysis demonstrates that an Ideal Security Equilibrium, where all validators are attentive and proposers are honest, can be achieved with RAT. Notably, this equilibrium is attainable and stable with relatively low economic penalties (e.g., under $1000) for non-responsive validators and a low attention test frequency (e.g., under 1% per epoch). RAT thus provides a crucial, practical mechanism to enforce validator diligence, fortifying the overall security and integrity of ORU systems with minimizing additional costs.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV</title>
<link>https://arxiv.org/abs/2505.24781</link>
<guid>https://arxiv.org/abs/2505.24781</guid>
<content:encoded><![CDATA[
<div> Regularized Tyler's M-estimator, shrinkage coefficient, leave-one-out cross-validation, computational efficiency, high-dimensional data<br />
<br />
Summary: <br />
The study focuses on estimating a shrinkage coefficient for Regularized Tyler's M-estimator using leave-one-out cross-validation (LOOCV) log-likelihood loss. The proposed approach aims to find an optimal shrinkage coefficient by solving a selected objective function, enhancing computational efficiency by approximating the LOOCV log-likelihood loss. This approximation significantly reduces the running time complexity for the LOOCV procedure by O(n), offering a faster computation of the LOOCV estimate. The efficiency and accuracy of the method were demonstrated through synthetic high-dimensional data and real datasets for object recognition, face recognition, and handwritten digit recognition. Results indicate the proposed approach is both efficient and more precise compared to existing methods for shrinkage coefficient estimation. <div>
arXiv:2505.24781v1 Announce Type: cross 
Abstract: We consider the problem of estimating a regularization parameter, or a shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator (RTME). In particular, we propose to estimate an optimal shrinkage coefficient by setting $\alpha$ as the solution to a suitably chosen objective function; namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since LOOCV is computationally prohibitive even for moderate sample size $n$, we propose a computationally efficient approximation for the LOOCV log-likelihood loss that eliminates the need for invoking the RTME procedure $n$ times for each sample left out during the LOOCV procedure. This approximation yields an $O(n)$ reduction in the running time complexity for the LOOCV procedure, which results in a significant speedup for computing the LOOCV estimate. We demonstrate the efficiency and accuracy of the proposed approach on synthetic high-dimensional data sampled from heavy-tailed elliptical distributions, as well as on real high-dimensional datasets for object recognition, face recognition, and handwritten digit's recognition. Our experiments show that the proposed approach is efficient and consistently more accurate than other methods in the literature for shrinkage coefficient estimation.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpolating Neural Network-Tensor Decomposition (INN-TD): a scalable and interpretable approach for large-scale physics-based problems</title>
<link>https://arxiv.org/abs/2503.02041</link>
<guid>https://arxiv.org/abs/2503.02041</guid>
<content:encoded><![CDATA[
<div> Interpolating Neural Network-Tensor Decomposition, scalable, interpretable, machine learning, finite element methods, large-scale physical systems
Summary:
Interpolating Neural Network-Tensor Decomposition (INN-TD) is introduced as a framework that combines machine learning and finite element methods to model large-scale physical systems accurately and efficiently. By incorporating locally supported interpolation functions from finite element methods into the network architecture, INN-TD achieves a sparse learning structure, leading to enhanced accuracy, faster training/solving speed, and reduced memory usage. This framework is well-suited for addressing large-scale high-dimensional parametric partial differential equations in physical problems that require high precision in tasks such as training, solving, and inverse optimization. Its effectiveness lies in its ability to provide interpretable solutions for industrial problems while maintaining computational efficiency and accuracy in modeling complex physics-based systems. <br /><br />Summary: <div>
arXiv:2503.02041v3 Announce Type: replace 
Abstract: Deep learning has been extensively employed as a powerful function approximator for modeling physics-based problems described by partial differential equations (PDEs). Despite their popularity, standard deep learning models often demand prohibitively large computational resources and yield limited accuracy when scaling to large-scale, high-dimensional physical problems. Their black-box nature further hinders the application in industrial problems where interpretability and high precision are critical. To overcome these challenges, this paper introduces Interpolating Neural Network-Tensor Decomposition (INN-TD), a scalable and interpretable framework that has the merits of both machine learning and finite element methods for modeling large-scale physical systems. By integrating locally supported interpolation functions from finite element into the network architecture, INN-TD achieves a sparse learning structure with enhanced accuracy, faster training/solving speed, and reduced memory footprint. This makes it particularly effective for tackling large-scale high-dimensional parametric PDEs in training, solving, and inverse optimization tasks in physical problems where high precision is required.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data</title>
<link>https://arxiv.org/abs/2505.22670</link>
<guid>https://arxiv.org/abs/2505.22670</guid>
<content:encoded><![CDATA[
<div> semantic-spatial-topological, BIM models, network-based representation, IFC, design patterns

Summary:
This study introduces a unified network-based representation method for BIM models to capture complex spatial and topological relationships between components. By extending the IFC standard, the method incorporates local spatial relationships and topological connections, enriching the network structure. This approach enhances understanding of component interactions, dependencies, and design patterns in BIM models. The proposed representation method effectively captures semantic, topological, and spatial relationships, offering significant potential for learning design patterns in construction industry. <div>
arXiv:2505.22670v1 Announce Type: new 
Abstract: Building Information Modeling (BIM) has revolutionized the construction industry by providing a comprehensive digital representation of building structures throughout their lifecycle. However, existing research lacks effective methods for capturing the complex spatial and topological relationships between components in BIM models, which are essential for understanding design patterns and enhancing decision-making. This study proposes a unified network-based representation method that integrates the "semantic-spatial-topological" multi-dimensional design features of BIM models. By extending the IFC (Industry Foundation Classes) standard, we introduce local spatial relationships and topological connections between components to enrich the network structure. This representation method enables a more detailed understanding of component interactions, dependencies, and implicit design patterns, effectively capturing the semantic, topological, and spatial relationships in BIM, and holds significant potential for the representation and learning of design patterns.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive analysis of PINNs: Variants, Applications, and Challenges</title>
<link>https://arxiv.org/abs/2505.22761</link>
<guid>https://arxiv.org/abs/2505.22761</guid>
<content:encoded><![CDATA[
<div> Physics Informed Neural Networks (PINNs), differential equations, architecture, variants, application, challenges

Summary:
Physics Informed Neural Networks (PINNs) are a powerful computational tool for solving differential equations. This survey provides a comprehensive overview of PINNs, covering architecture, variants, applications, real-world use cases, and challenges. Existing surveys lack detail, but this one offers a thorough analysis of PINNs, including recent advancements and research. Three main contributions are discussed: an in-depth look at PINNs architecture and variants, performance analysis on various equations and applications, and a discussion of current issues and future research directions. This survey aims to standardize and popularize the use of PINNs by addressing key areas and providing valuable insights for the field moving forward.<br /><br />Summary: <div>
arXiv:2505.22761v1 Announce Type: new 
Abstract: Physics Informed Neural Networks (PINNs) have been emerging as a powerful computational tool for solving differential equations. However, the applicability of these models is still in its initial stages and requires more standardization to gain wider popularity. Through this survey, we present a comprehensive overview of PINNs approaches exploring various aspects related to their architecture, variants, areas of application, real-world use cases, challenges, and so on. Even though existing surveys can be identified, they fail to provide a comprehensive view as they primarily focus on either different application scenarios or limit their study to a superficial level. This survey attempts to bridge the gap in the existing literature by presenting a detailed analysis of all these factors combined with recent advancements and state-of-the-art research in PINNs. Additionally, we discuss prevalent challenges in PINNs implementation and present some of the future research directions as well. The overall contributions of the survey can be summarised into three sections: A detailed overview of PINNs architecture and variants, a performance analysis of PINNs on different equations and application domains highlighting their features. Finally, we present a detailed discussion of current issues and future research directions.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolution analysis of software quality metrics in an open-source java project: A case study on TestNG</title>
<link>https://arxiv.org/abs/2505.22884</link>
<guid>https://arxiv.org/abs/2505.22884</guid>
<content:encoded><![CDATA[
<div> Keywords: software quality, TestNG, Java, object-oriented metrics, static analysis<br />
Summary:<br />
This study examines the evolution of software quality metrics in five versions of the TestNG Java testing framework. Using Understand, key object-oriented metrics were analyzed, showing trends such as increased stability and maintainability. The results suggest ongoing development, refactoring, and architectural improvements have contributed to TestNG's maturity over time. This study offers insights on design evolution and recommendations for maintaining code quality in similar projects. <div>
arXiv:2505.22884v1 Announce Type: cross 
Abstract: Software quality is critical in modern software engineering, especially in large and evolving codebases. This study analyzes the evolution of software quality metrics in five successive versions of the open-source Java testing framework TestNG. Using the static analysis tool Understand, eleven key object-oriented metrics, including cyclomatic complexity, class coupling, and lines of code, were extracted for each version. Statistical and visual analyses reveal structural trends over time. The results indicate that TestNG has matured into a more stable and maintainable framework, reflecting ongoing development, refactoring, and architectural improvements. This study provides insights into design evolution and offers recommendations for maintaining code quality in similar projects.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Be.FM: Open Foundation Models for Human Behavior</title>
<link>https://arxiv.org/abs/2505.23058</link>
<guid>https://arxiv.org/abs/2505.23058</guid>
<content:encoded><![CDATA[
<div> modeling, human behavior, decision-making, benchmark tasks, behavioral science

Summary:
Be.FM is introduced as an open foundation model for human behavior modeling, using large language models and fine-tuning on diverse behavioral data. It has the potential to predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge. Through comprehensive benchmark tasks, Be.FM demonstrates its capabilities in understanding and predicting human decision-making. This opens up possibilities for leveraging foundation models in various fields to gain insights into human behavior. <div>
arXiv:2505.23058v1 Announce Type: cross 
Abstract: Despite their success in numerous fields, the potential of foundation models for modeling and understanding human behavior remains largely unexplored. We introduce Be.FM, one of the first open foundation models designed for human behavior modeling. Built upon open-source large language models and fine-tuned on a diverse range of behavioral data, Be.FM can be used to understand and predict human decision-making. We construct a comprehensive set of benchmark tasks for testing the capabilities of behavioral foundation models. Our results demonstrate that Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid subgradient and simulated annealing method for hemivariational inequalities</title>
<link>https://arxiv.org/abs/2505.23676</link>
<guid>https://arxiv.org/abs/2505.23676</guid>
<content:encoded><![CDATA[
<div> global aggregate subgradient method, hemivariational inequality problems, contact mechanics, local minimization algorithm, performance comparison

Summary:
The paper introduces a global aggregate subgradient method for solving hemivariational inequality problems in contact mechanics. It combines global search capabilities to determine starting points for a local minimization algorithm. The algorithm incorporates null steps, using aggregate and current iteration subgradients to find the search direction, as well as serious steps. The method's performance is evaluated against other solvers using a representative contact mechanics problem. The study showcases the effectiveness of the proposed approach in efficiently solving complex contact mechanics problems compared to existing methods. <div>
arXiv:2505.23676v1 Announce Type: cross 
Abstract: In this paper, we employ a global aggregate subgradient method for the numerical solution of hemivariational inequality problems arising in contact mechanics. The method integrates a global search procedure to identify starting points for a local minimization algorithm. The algorithm consists of two types of steps: null steps and serious steps. In each null step, only two subgradients are utilized: the aggregate subgradient and the subgradient computed at the current iteration point, which together determine the search direction. Furthermore, we compare the performance of the proposed method with selected solvers using a representative contact mechanics problem as a case study.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computerized Modeling of Electrophysiology and Pathoelectrophysiology of the Atria -- How Much Detail is Needed?</title>
<link>https://arxiv.org/abs/2505.23717</link>
<guid>https://arxiv.org/abs/2505.23717</guid>
<content:encoded><![CDATA[
<div> Keywords: computerized modeling, atrial electrophysiology, arrhythmias, fibrotic tissue, atrial ablation<br />
Summary:<br />
This review focuses on computerized modeling of human atrial electrophysiology, specifically addressing common arrhythmias like atrial flutter and atrial fibrillation. The key question is identifying the necessary components for accurate simulation of arrhythmogenic tissue modifications, including remodeling, cardiomyopathy, and fibrosis. It examines the balance between model complexity and computational efficiency, emphasizing the risks of oversimplification and excessive detail. Various aspects of atrial modeling, from cellular to whole atria levels, are covered, considering factors like atrial geometry, fiber direction, anisotropy, and wall thickness. The impact of different modeling approaches and the latest advances in modeling fibrotic tissue are discussed, along with verification and validation methods. The use of these models in planning atrial ablation strategies, both personalized and cohort-based, is highlighted, stressing the importance of integrating experimental data and clinical validation for improved patient outcomes.<br /> <div>
arXiv:2505.23717v1 Announce Type: cross 
Abstract: This review focuses on the computerized modeling of the electrophysiology of the human atria, emphasizing the simulation of common arrhythmias such as atrial flutter (AFlut) and atrial fibrillation (AFib). Which components of the model are necessary to accurately model arrhythmogenic tissue modifications, including remodeling, cardiomyopathy, and fibrosis, to ensure reliable simulations? The central question explored is the level of detail required for trustworthy simulations for a specific context of use. The review discusses the balance between model complexity and computational efficiency, highlighting the risks of oversimplification and excessive detail. It covers various aspects of atrial modeling, from cellular to whole atria levels, including the influence of atrial geometry, fiber direction, anisotropy, and wall thickness on simulation outcomes. The article also examines the impact of different modeling approaches, such as volumetric 3D models, bilayer models, and single surface models, on the realism of simulations. In addition, it reviews the latest advances in the modeling of fibrotic tissue and the verification and validation of atrial models. The intended use of these models in planning and optimization of atrial ablation strategies is discussed, with a focus on personalized modeling for individual patients and cohort-based approaches for broader applications. The review concludes by emphasizing the importance of integrating experimental data and clinical validation to enhance the utility of computerized atrial models to improve patient outcomes.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A blockchain-based intelligent recommender system framework for enhancing supply chain resilience</title>
<link>https://arxiv.org/abs/2404.00306</link>
<guid>https://arxiv.org/abs/2404.00306</guid>
<content:encoded><![CDATA[
<div> Intelligent recommender system technology, blockchain technology, smart contract, supply chain disruption, system dynamics simulation<br />
<br />
Summary: 
This research proposes a data-driven supply chain disruption response framework utilizing intelligent recommender system (IRS) technology integrated with blockchain (BLC) technology. A smart contract prototype demonstrates information exchange within a BLC network. An industrial case study implementation validates the BLC-IRS framework's effectiveness in responding to disruptions. A system dynamics simulation model confirms the framework's ability to mitigate disruptions in the supply chain response phase. This approach provides a practical digital solution for supply chain resilience, allowing participants to react swiftly and effectively to disruptions. By utilizing synthetic technologies, the BLC-IRS framework enhances the SCRes community's ability to access supplementary resource information in a secure and real-time manner following disruptions. <div>
arXiv:2404.00306v3 Announce Type: replace 
Abstract: This research proposed a data-driven supply chain disruption response baseline framework based on intelligent recommender system technology as an initial SCRes reactive solution. To improve the data quality and reliability of the proposed IRS as a stable, secure, and resilient decision support system, blockchain technology is integrated into the baseline architecture. The smart contract is prototyped to demonstrate the information exchange mechanism under a BLC network environment. The BLC-IRS framework is then implemented with an industrial case to demonstrate its executable function. A system dynamics (SD) simulation model is adopted to validate the BLC-IRS framework as an effective digital SCRes enhancement measure. The simulation results indicated that the proposed BLC-IRS framework can be effectively implemented as a SC disruption mitigation measure in the SCRes response phase as reactive measure, enabling SC participants to react better to SC disruptions at the physical level. Compared to previous studies that limited at the conceptual level as the proactive SCRes measure with a standalone fashion, the developed BLC-IRS contributes an executable SCRes digital solution with synthetic technologies as a reactive SCRes measure for the SCRes community, by identifying the internal and external supplementary resource information in an agile, safe, and real-time manner after SC disruption.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Perishable and Non-Perishable Product Assignment to Packaging Lines in a Sustainable Manufacturing System: An AUGMECON2VIKOR Algorithm</title>
<link>https://arxiv.org/abs/2410.21844</link>
<guid>https://arxiv.org/abs/2410.21844</guid>
<content:encoded><![CDATA[
<div> manufacturing systems, food industry, mathematical model, optimization, perishable products<br />
<br />
Summary: 
This study introduces a new mathematical model and assignment approach to optimize manufacturing systems for perishable, non-perishable, and hybrid products in the food industry. The model considers three objective functions: minimizing production costs, maximizing product quality, and reducing CO2 emissions. Comparing the proposed AUGMECON2VIKOR model to AUGMECON2, the former outperforms in generating superior Pareto solutions across all objectives. Additionally, a sensitivity analysis demonstrates the positive environmental impact, influencing cost and quality factors. By leveraging knowledge discovery and a tailored assignment model, this study offers a comprehensive approach to address the unique constraints of perishable products and enhance operational efficiency in manufacturing systems. <div>
arXiv:2410.21844v2 Announce Type: replace-cross 
Abstract: Identifying appropriate manufacturing systems for products can be considered a pivotal manufacturing task contributing to the optimization of operational and planning activities. It has gained importance in the food industry due to the distinct constraints and considerations posed by perishable and non-perishable items in this problem. Hence, this study proposes a new mathematical model according to knowledge discovery as well as an assignment model to optimize manufacturing systems for perishable, non-perishable, and hybrid products tailored to meet their unique characteristics. In the presented model, three objective functions are taken into account: (1) minimizing production costs by assigning the products to the right set of manufacturing systems, (2) maximizing the product quality by assigning the products to the systems, and (3) minimizing total CO2 emissions of the machines. A numerical example is utilized to evaluate the performance of AUGMECON2VIKOR compared to AUGMECON2. The results show that AUGMECON2VIKOR obtains superior Pareto solutions across all objective functions. Furthermore, the sensitivity analysis explores the positive green impacts, influencing both cost and quality.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Representation Learning for fMRI-based Neurological Disorder Identification</title>
<link>https://arxiv.org/abs/2412.16197</link>
<guid>https://arxiv.org/abs/2412.16197</guid>
<content:encoded><![CDATA[
<div> representation learning, meta-learning, self-supervised learning, functional Magnetic Resonance Imaging, neurological disorders<br />
Summary:<br />
The study addresses challenges in identifying neurological disorders due to data heterogeneity and scarcity. It introduces a novel representation learning approach that combines meta-learning and self-supervised learning to enhance generalization from normal to clinical features. By leveraging self-supervised learning on control data and incorporating meta-learning, the model can generalize to clinical tasks with limited training data. The approach is applied to four different clinical datasets for neurological disorder classification, demonstrating its effectiveness for diverse tasks. The public availability of the code allows for further exploration and application of the representation learning strategy. <div>
arXiv:2412.16197v2 Announce Type: replace-cross 
Abstract: Despite the impressive advances achieved using deep learning for functional brain activity analysis, the heterogeneity of functional patterns and the scarcity of imaging data still pose challenges in tasks such as identifying neurological disorders. For functional Magnetic Resonance Imaging (fMRI), while data may be abundantly available from healthy controls, clinical data is often scarce, especially for rare diseases, limiting the ability of models to identify clinically-relevant features. We overcome this limitation by introducing a novel representation learning strategy integrating meta-learning with self-supervised learning to improve the generalization from normal to clinical features. This approach enables generalization to challenging clinical tasks featuring scarce training data. We achieve this by leveraging self-supervised learning on the control dataset to focus on inherent features that are not limited to a particular supervised task and incorporating meta-learning to improve the generalization across domains. To explore the generalizability of the learned representations to unseen clinical applications, we apply the model to four distinct clinical datasets featuring scarce and heterogeneous data for neurological disorder classification. Results demonstrate the superiority of our representation learning strategy on diverse clinically-relevant tasks. Code is publicly available at https://github.com/wenhui0206/MeTSK/tree/main
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling extreme events and intermittency in turbulent diffusion with a mean gradient</title>
<link>https://arxiv.org/abs/2505.21688</link>
<guid>https://arxiv.org/abs/2505.21688</guid>
<content:encoded><![CDATA[
<div> Keywords: passive tracer transport, turbulent flows, intermittency, extreme events, stochastic dynamics<br />
<br />
Summary: 
This study examines the statistical properties of passive tracer transport in turbulent flows with a mean gradient, focusing on tracer intermittency and extreme events. An analytically tractable model is developed, combining zonal and shear velocity components with linear and nonlinear stochastic dynamics. By formulating the model in Fourier space, the researchers derive a straightforward explicit solution for the tracer invariant statistics. They identify the resonance condition responsible for non-Gaussian behavior and bursts in the tracer, pinpointing the occurrence of peak tracer variance when the zonal flow and shear flow phase speeds are equal. Through numerical experiments across various regimes, the study validates these findings and showcases how the velocity field and stochasticity influence tracer extremes. These results offer valuable insights into the mechanisms governing turbulent tracer transport, which can significantly impact uncertainty quantification and data assimilation in geophysical and environmental applications.<br /><br />Summary: <div>
arXiv:2505.21688v1 Announce Type: new 
Abstract: We study the statistical properties of passive tracer transport in turbulent flows with a mean gradient, emphasizing tracer intermittency and extreme events. An analytically tractable model is developed, coupling zonal and shear velocity components with both linear and nonlinear stochastic dynamics. Formulating the model in Fourier space, a simple explicit solution for the tracer invariant statistics is derived. Through this model we identify the resonance condition responsible for non-Gaussian behavior and bursts in the tracer. Resonant conditions, that lead to a peak in the tracer variance, occur when the zonal flow and the shear flow phase speeds are equivalent. Numerical experiments across a range of regimes, including different energy spectra and zonal flow models, are performed to validate these findings and demonstrate how the velocity field and stochasticity determines tracer extremes. These results provide additional insight into the mechanisms underlying turbulent tracer transport, with implications for uncertainty quantification and data assimilation in geophysical and environmental applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero</title>
<link>https://arxiv.org/abs/2505.21536</link>
<guid>https://arxiv.org/abs/2505.21536</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, circular economy, materials, thermodynamics, Python library
<br />
Summary: 
CiRL is a deep reinforcement learning library that focuses on the circularity of solid and fluid materials to address the challenges of achieving a net zero target. It integrates DRL into material circularity design using thermodynamical material networks. The library features CE-oriented environments in the state-space form, based on the Stable-Baselines3 Python library, and developed in Google Colaboratory for accessibility to researchers. By leveraging DRL algorithms in the context of circular economy, CiRL aims to support the transition towards a more sustainable and efficient use of finite raw materials in modern society. <div>
arXiv:2505.21536v1 Announce Type: cross 
Abstract: The demand of finite raw materials will keep increasing as they fuel modern society. Simultaneously, solutions for stopping carbon emissions in the short term are not available, thus making the net zero target extremely challenging to achieve at scale. The circular economy (CE) paradigm is gaining attention as a solution to address climate change and the uncertainties of supplies of critical materials. Hence, in this paper, we introduce CiRL, a deep reinforcement learning (DRL) library of environments focused on the circularity of both solid and fluid materials. The integration of DRL into the design of material circularity is possible thanks to the formalism of thermodynamical material networks, which is underpinned by compartmental dynamical thermodynamics. Along with the focus on circularity, this library has three more features: the new CE-oriented environments are in the state-space form, which is typically used in dynamical systems analysis and control designs; it is based on a state-of-the-art Python library of DRL algorithms, namely, Stable-Baselines3; and it is developed in Google Colaboratory to be accessible to researchers from different disciplines and backgrounds as is often the case for circular economy researchers and engineers. CiRL is publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-Capping Metric Evaluation for Improving Energy Efficiency</title>
<link>https://arxiv.org/abs/2505.21758</link>
<guid>https://arxiv.org/abs/2505.21758</guid>
<content:encoded><![CDATA[
<div> power-scaling management, resource utilization, energy-performance metrics, GPU power-capping, exascale applications

Summary:
This paper delves into the optimization of power-scaling management and resource utilization in high-performance computing systems running at exascale. By leveraging integrated CPU-GPU power management on architectures like the NVIDIA GH200 superchip, the study evaluates energy-performance metrics considering simultaneous CPU and GPU power-capping effects. Focusing on the Locally Self-Consistent Multiple Scattering (LSMS) application, the research identifies potential opportunities for energy savings in exascale applications. The results demonstrate that GPU task-specific dynamic power-cap adjustments, combined with integrated CPU-GPU power steering, can enhance energy utilization for certain GPU tasks. These findings lay the foundation for future adaptive optimization strategies in exascale computing, emphasizing the significance of even small reductions in energy consumption for overall system efficiency. <div>
arXiv:2505.21758v1 Announce Type: cross 
Abstract: With high-performance computing systems now running at exascale, optimizing power-scaling management and resource utilization has become more critical than ever. This paper explores runtime power-capping optimizations that leverage integrated CPU-GPU power management on architectures like the NVIDIA GH200 superchip. We evaluate energy-performance metrics that account for simultaneous CPU and GPU power-capping effects by using two complementary approaches: speedup-energy-delay and a Euclidean distance-based multi-objective optimization method. By targeting a mostly compute-bound exascale science application, the Locally Self-Consistent Multiple Scattering (LSMS), we explore challenging scenarios to identify potential opportunities for energy savings in exascale applications, and we recognize that even modest reductions in energy consumption can have significant overall impacts. Our results highlight how GPU task-specific dynamic power-cap adjustments combined with integrated CPU-GPU power steering can improve the energy utilization of certain GPU tasks, thereby laying the groundwork for future adaptive optimization strategies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2505.21887</link>
<guid>https://arxiv.org/abs/2505.21887</guid>
<content:encoded><![CDATA[
<div> Keywords: Robust routing, uncertainty, logistics, benchmark, stochastic dynamics 

Summary:
SVRPBench introduces an open benchmark for robust routing under uncertainty in logistics, specifically focusing on high-fidelity stochastic dynamics in vehicle routing at an urban scale. This benchmark comprises over 500 instances with realistic delivery conditions, including time-dependent congestion, delays, accidents, and time windows for customers. The dataset simulates constraint-rich scenarios such as multi-depot and multi-vehicle setups. Benchmarking results demonstrate that classical and metaheuristic methods outperform state-of-the-art RL solvers like POMO and AM under distributional shift. The findings highlight the importance of designing solvers that can generalize beyond synthetic assumptions and adapt to real-world uncertainty. The release of the dataset and evaluation suite enables reproducible research in the field of robust routing. <br /><br />Summary: SVRPBench provides an open benchmark for robust routing under uncertainty in real-world logistics, showcasing the challenges faced by state-of-the-art RL solvers and the need for adaptive solutions to dynamic stochastic conditions. <div>
arXiv:2505.21887v1 Announce Type: cross 
Abstract: Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</title>
<link>https://arxiv.org/abs/2505.21923</link>
<guid>https://arxiv.org/abs/2505.21923</guid>
<content:encoded><![CDATA[
<div> machine learning, analog circuits, topology selection, parameter inference, layout feasibility

Summary:<br />
The article introduces FALCON, a machine learning framework for automated analog circuit synthesis. FALCON guides the circuit design process by first selecting an appropriate circuit topology based on performance specifications. It then uses a graph neural network to infer circuit parameters and predict performance. The design process is constrained by layout considerations and design rules to ensure feasibility. FALCON was trained and evaluated on a large dataset of analog mm-wave circuits, demonstrating high accuracy in topology inference and performance prediction. The automated design process is efficient, completing in under 1 second per instance. Overall, FALCON shows promise as a practical and scalable tool for end-to-end analog circuit design automation.<br /> <div>
arXiv:2505.21923v1 Announce Type: cross 
Abstract: Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99\% accuracy in topology inference, <10\% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data</title>
<link>https://arxiv.org/abs/2505.22252</link>
<guid>https://arxiv.org/abs/2505.22252</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, cheminformatics, drug discovery, Explainable AI, Graph Neural Networks

Summary:
The study focuses on the importance of understanding the rationale behind predictions made by deep learning models in cheminformatics and drug discovery. Existing evaluation frameworks for Explainable AI (XAI) in this field lack real-world data and fail to capture the complexity of actual scenarios. To address this gap, the researchers introduce a new benchmark called B-XAIC, which uses real-world molecular data and diverse tasks with known ground-truth rationales. By evaluating XAI methods for Graph Neural Networks (GNNs) using B-XAIC, the study reveals the limitations of current methods in the molecular domain. This benchmark serves as a valuable resource for enhancing the faithfulness of explanations in XAI and improving the interpretability of models used in cheminformatics and drug discovery. <div>
arXiv:2505.22252v1 Announce Type: cross 
Abstract: Understanding the reasoning behind deep learning model predictions is crucial in cheminformatics and drug discovery, where molecular design determines their properties. However, current evaluation frameworks for Explainable AI (XAI) in this domain often rely on artificial datasets or simplified tasks, employing data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness. To address this, we introduce B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through a comprehensive evaluation using B-XAIC, we reveal limitations of existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain. This benchmark provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation</title>
<link>https://arxiv.org/abs/2505.22391</link>
<guid>https://arxiv.org/abs/2505.22391</guid>
<content:encoded><![CDATA[
<div> Keywords: generative modeling, diffusion models, PDE constraints, Physics-Informed Distillation, inverse problem solving<br />
Summary: 
Physics-based generative modeling offers advantages in handling partial observations and addressing both forward and inverse problems. Diffusion models have been increasingly used for modeling physical systems governed by partial differential equations (PDEs). However, a trade-off exists when enforcing PDE constraints on clean samples, leading to reduced generative accuracy. To address this, a post-hoc distillation approach called Physics-Informed Distillation of Diffusion Models (PIDDM) is proposed. This method enforces PDE constraints after the diffusion process, improving PDE satisfaction and supporting both forward and inverse problem solving. Experimental results demonstrate that PIDDM outperforms recent baselines like PIDM and DiffusionPDE, with lower computation overhead. The approach provides insights into more efficient ways of integrating physical constraints into diffusion models.<br /><br />Summary: <div>
arXiv:2505.22391v1 Announce Type: cross 
Abstract: Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2L Translation Operators for Kernel Independent Fast Multipole Methods on Modern Architectures</title>
<link>https://arxiv.org/abs/2408.07436</link>
<guid>https://arxiv.org/abs/2408.07436</guid>
<content:encoded><![CDATA[
<div> Multipole-to-Local Translation Operators, Kernel-independent Fast Multipole Method, Benchmarking, BLAS-based M2L, Randomized Low-rank Compression<br />
<br />
Summary:<br />
Hardware trends prioritize data reuse in algorithms. This study focuses on high-performance Multipole-to-Local (M2L) translation operators for the kernel-independent Fast Multipole Method (kiFMM). The traditional M2L approach is bandwidth-limited, presenting a bottleneck in the FMM. While FFT-based M2L implementations are efficient, they lack operational intensity and require specific optimizations. In contrast, BLAS-based M2L with randomized low-rank compression offers competitive performance, portability, and a simpler implementation leveraging existing BLAS infrastructure. A Rust-based implementation allows seamless strategy switching for fair benchmarking. CPU results show that FFT-based M2L excels in low-accuracy or dynamic simulations, whereas BLAS-based M2L is more effective in high-accuracy settings for static distributions, despite higher setup costs that are offset in many practical FMM applications. <br /> <div>
arXiv:2408.07436v4 Announce Type: replace 
Abstract: Hardware trends favor algorithm designs that maximize data reuse per FLOP. We develop and benchmark high-performance Multipole-to-Local (M2L) translation operators for the kernel-independent Fast Multipole Method (kiFMM), a widely adopted FMM variant that supports a broad class of kernels and has been favored by recent implementations for its simple specification. Naively implemented, M2L is bandwidth-limited and therefore a key bottleneck in the FMM. State-of-the-art FFT-based M2L implementations, though elegant and with a fast setup time, suffer from low operational intensity and require architecture-specific optimizations. We demonstrate that a BLAS-based M2L, combined with randomized low-rank compression, achieves competitive performance with greater portability and a simpler implementation leveraging existing BLAS infrastructure, at the cost of higher setup times-especially for high-accuracy settings in double precision. Our Rust-based implementation enables seamless switching between strategies for fair benchmarking. Results on CPUs show that FFT-based M2L is favorable in low-accuracy settings or dynamic particle simulations, while BLAS-based M2L is favored for high-accuracy settings for static particle distributions, where its higher setup costs are amortized in many practical applications of the FMM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving precision of A/B experiments using trigger intensity</title>
<link>https://arxiv.org/abs/2411.03530</link>
<guid>https://arxiv.org/abs/2411.03530</guid>
<content:encoded><![CDATA[
<div> randomized controlled experiment, A/B experiment, signal-to-noise ratio, sampling, bias

Summary: 
- Online randomized controlled experiments are widely used in industry to measure causal changes but often lack statistical significance due to low signal-to-noise ratios.
- Traditional methods focus on trigger observations where treatment and control models differ, leading to a costly process.
- A proposed sampling-based evaluation method reduces costs by introducing bias inversely proportional to the number of observations sampled.
- Simulation results show that bias effectively reduces to zero with a limited number of observations sampled.
- Empirical data demonstrates a 36.48% reduction in standard error with partial knowledge evaluation. 
<br /><br /> <div>
arXiv:2411.03530v2 Announce Type: replace-cross 
Abstract: In industry, online randomized controlled experiment (a.k.a. A/B experiment) is a standard approach to measure the impact of a causal change. These experiments have small treatment effect to reduce the potential blast radius. As a result, these experiments often lack statistical significance due to low signal-to-noise ratio. A standard approach for improving the precision (or reducing the standard error) focuses only on the trigger observations, where the output of the treatment and the control model are different. Although evaluation with full information about trigger observations (full knowledge) improves the precision, detecting all such trigger observations is a costly affair. In this paper, we propose a sampling based evaluation method (partial knowledge) to reduce this cost. The randomness of sampling introduces bias in the estimated outcome. We theoretically analyze this bias and show that the bias is inversely proportional to the number of observations used for sampling. We also compare the proposed evaluation methods using simulation and empirical data. In simulation, bias in evaluation with partial knowledge effectively reduces to zero when a limited number of observations (<= 0.1%) are sampled for trigger estimation. In empirical setup, evaluation with partial knowledge reduces the standard error by 36.48%.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIT-BO: High-Dimensional Bayesian Optimization with Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2505.20685</link>
<guid>https://arxiv.org/abs/2505.20685</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, high-dimensional spaces, Gradient-Informed Bayesian Optimization, Tabular Foundation Models, pre-trained model<br />
<br />
Summary: 
The paper introduces Gradient-Informed Bayesian Optimization using Tabular Foundation Models (GIT-BO) to address challenges in high-dimensional Bayesian optimization. GIT-BO leverages a pre-trained tabular foundation model to identify low-dimensional subspaces for optimization using gradient information. By creating a gradient-informed diagnostic matrix, the most sensitive directions of the model's predictions are identified for adaptive optimization without repeated model retraining. Extensive evaluation across 23 benchmarks shows GIT-BO outperforms existing Gaussian process-based methods in scalability and optimization performance, especially in high dimensions up to 500. This work showcases the effectiveness of foundation models with gradient-informed adaptive subspace identification as competitive alternatives for high-dimensional Bayesian optimization tasks. <br /> <div>
arXiv:2505.20685v1 Announce Type: new 
Abstract: Bayesian optimization (BO) effectively optimizes expensive black-box functions but faces significant challenges in high-dimensional spaces (dimensions exceeding 100) due to the curse of dimensionality. Existing high-dimensional BO methods typically leverage low-dimensional embeddings or structural assumptions to mitigate this challenge, yet these approaches frequently incur considerable computational overhead and rigidity due to iterative surrogate retraining and fixed assumptions. To address these limitations, we propose Gradient-Informed Bayesian Optimization using Tabular Foundation Models (GIT-BO), an approach that utilizes a pre-trained tabular foundation model (TFM) as a surrogate, leveraging its gradient information to adaptively identify low-dimensional subspaces for optimization. We propose a way to exploit internal gradient computations from the TFM's forward pass by creating a gradient-informed diagnostic matrix that reveals the most sensitive directions of the TFM's predictions, enabling optimization in a continuously re-estimated active subspace without the need for repeated model retraining. Extensive empirical evaluation across 23 synthetic and real-world benchmarks demonstrates that GIT-BO consistently outperforms four state-of-the-art Gaussian process-based high-dimensional BO methods, showing superior scalability and optimization performances, especially as dimensionality increases up to 500 dimensions. This work establishes foundation models, augmented with gradient-informed adaptive subspace identification, as highly competitive alternatives to traditional Gaussian process-based approaches for high-dimensional Bayesian optimization tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced and mixed precision turbulent flow simulations using explicit finite difference schemes</title>
<link>https://arxiv.org/abs/2505.20911</link>
<guid>https://arxiv.org/abs/2505.20911</guid>
<content:encoded><![CDATA[
<div> Keywords: reduced precision computing, mixed precision arithmetic, compressible turbulent flow simulations, explicit finite difference schemes, performance gains

Summary:
Reduced and mixed precision computing is being increasingly utilized in high-performance computing (HPC) for enhanced computational efficiency, especially on modern hardware like GPUs. The study focuses on applying mixed precision arithmetic in compressible turbulent flow simulations using explicit finite difference schemes. They modify the OPS and OpenSBLI frameworks to allow for customizable precision levels, enabling precise control over precision allocation for various tasks. Through numerical experiments on the Taylor-Green vortex benchmark, the researchers showcase significant performance improvements with mixed precision strategies like half-single and single-double combinations, maintaining numerical accuracy. Pure half-precision computations, however, exhibit unacceptable accuracy degradation, emphasizing the importance of careful precision selection. The study demonstrates that mixed precision configurations can decrease memory usage and communication overhead, resulting in noticeable speedups, particularly on multi-CPU and multi-GPU systems. 

<br /><br />Summary: <div>
arXiv:2505.20911v1 Announce Type: new 
Abstract: The use of reduced and mixed precision computing has gained increasing attention in high-performance computing (HPC) as a means to improve computational efficiency, particularly on modern hardware architectures like GPUs. In this work, we explore the application of mixed precision arithmetic in compressible turbulent flow simulations using explicit finite difference schemes. We extend the OPS and OpenSBLI frameworks to support customizable precision levels, enabling fine-grained control over precision allocation for different computational tasks. Through a series of numerical experiments on the Taylor-Green vortex benchmark, we demonstrate that mixed precision strategies, such as half-single and single-double combinations, can offer significant performance gains without compromising numerical accuracy. However, pure half-precision computations result in unacceptable accuracy loss, underscoring the need for careful precision selection. Our results show that mixed precision configurations can reduce memory usage and communication overhead, leading to notable speedups, particularly on multi-CPU and multi-GPU systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limitations of Nyquist Criteria in the Discretization of 2D Electromagnetic Integral Equations at High Frequency: Spectral Insights into Pollution Effects</title>
<link>https://arxiv.org/abs/2505.20942</link>
<guid>https://arxiv.org/abs/2505.20942</guid>
<content:encoded><![CDATA[
<div> Boundary Integral Equations, Boundary Element Methods, Spectral Analysis, Solution Accuracy, Electromagnetic Scattering<br />
<br />
Summary: The article discusses the use of boundary integral equations in modeling wave phenomena in various fields like elastic, acoustic, or electromagnetic. The focus is on analyzing the impact of Boundary Element Methods (BEMs) discretization on solution accuracy, particularly in electromagnetic scattering from a conducting cylinder. The study examines both ill-conditioned and well-conditioned equations, identifying a form of pollution affecting accuracy in different ways. The research proposes a solution strategy to mitigate this pollution problem. Through rigorous spectral analysis, the article provides deep insight into the root causes of numerical pollution in BEMs, highlighting the importance of understanding how discretization affects solution accuracy in boundary value problems. <div>
arXiv:2505.20942v1 Announce Type: new 
Abstract: The use of boundary integral equations in modeling boundary value problems-such as elastic, acoustic, or electromagnetic ones-is well established in the literature and widespread in practical applications. These equations are typically solved numerically using boundary element methods (BEMs), which generally provide accurate and reliable solutions. When the frequency of the wave phenomenon under study increases, the discretization of the problem is typically chosen to maintain a fixed number of unknowns per wavelength. Under these conditions, the BEM over finite-dimensional subspaces of piecewise polynomial basis functions is commonly believed to provide a bounded solution accuracy. If proven, this would constitute a significant advantage of the BEM with respect to finite element and finite difference time domain methods, which, in contrast, are affected by numerical pollution. In this work, we conduct a rigorous spectral analysis of some of the most commonly used boundary integral operators and examine the impact of the BEM discretization on the solution accuracy of widely used integral equations modeling two-dimensional electromagnetic scattering from a perfectly electrically conducting cylinder. We consider both ill-conditioned and well-conditioned equations, the latter being characterized by solution operators bounded independently of frequency. Our analysis, which is capable of tracking the effects of BEM discretization on compositions and sums of different operators, reveals a form of pollution that affects, in different measures, equations of both kinds. After elucidating the mechanism by which the BEM discretization impacts accuracy, we propose a solution strategy that can cure the pollution problem thus evidenced. The defining strength of the proposed theoretical model lies in its capacity to deliver deep insight into the root causes of the phenomenon.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of the Past: An AI-Enabled Pipeline for Traffic Simulation from Noisy, Multimodal Detector Data and Stakeholder Feedback</title>
<link>https://arxiv.org/abs/2505.21349</link>
<guid>https://arxiv.org/abs/2505.21349</guid>
<content:encoded><![CDATA[
<div> computer vision, combinatorial optimization, large language models, traffic simulation, data-driven

Summary:
The article introduces a new approach to designing traffic simulations that accurately reflect real-world traffic conditions. The proposed pipeline involves three steps: using computer vision for vehicle counting from camera footage, applying combinatorial optimization for vehicle route generation from multimodal data, and utilizing large language models for iterative simulation refinement based on natural language feedback. Through testing on a road network in Strongsville, Ohio, the framework successfully captures the city's traffic patterns in a detailed simulation. The pipeline's flexibility allows for generalization to other municipalities with varying levels of data and infrastructure availability. <div>
arXiv:2505.21349v1 Announce Type: new 
Abstract: How can a traffic simulation be designed to faithfully reflect real-world traffic conditions? Past data-driven approaches to traffic simulation in the literature have relied on unrealistic or suboptimal heuristics. They also fail to adequately account for the effects of uncertainty and multimodality in the data on simulation outcomes. In this work, we integrate advances in AI to construct a three-step, end-to-end pipeline for generating a traffic simulation from detector data: computer vision for vehicle counting from camera footage, combinatorial optimization for vehicle route generation from multimodal data, and large language models for iterative simulation refinement from natural language feedback. Using a road network from Strongsville, Ohio as a testbed, we demonstrate that our pipeline can accurately capture the city's traffic patterns in a granular simulation. Beyond Strongsville, our traffic simulation framework can be generalized to other municipalities with different levels of data and infrastructure availability.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information</title>
<link>https://arxiv.org/abs/2505.20650</link>
<guid>https://arxiv.org/abs/2505.20650</guid>
<content:encoded><![CDATA[
<div> FinTagging, XBRL benchmark, large language models, structured information extraction, semantic alignment <br />
Summary: <br />
FinTagging is introduced as a comprehensive XBRL benchmark for evaluating language models' capabilities in financial reporting. It consists of two subtasks, FinNI for entity extraction and FinCL for concept alignment, emphasizing the extraction and alignment of facts within the US-GAAP taxonomy. Large language models exhibit strong extraction abilities but struggle with fine-grained concept alignment, especially in distinguishing closely related taxonomy entries. The study underscores the importance of improved semantic reasoning and schema-aware modeling for accurate financial disclosure. The code is available on GitHub, and the data can be accessed through the Hugging Face repository. <div>
arXiv:2505.20650v1 Announce Type: cross 
Abstract: We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction</title>
<link>https://arxiv.org/abs/2505.21109</link>
<guid>https://arxiv.org/abs/2505.21109</guid>
<content:encoded><![CDATA[
<div> adaptation, language models, hallucination issues, Small Language Graph, lightweight

Summary:<br />
- The study addresses challenges in adapting large language models, focusing on reducing computational resources and minimizing hallucination issues.
- The Small Language Graph (SLG) approach, based on a graph structure with lightweight expert nodes, outperformed traditional fine-tuning methods by 3 times on the Exact Match metric.
- SLG also demonstrated a 1.7 times faster fine-tuning process compared to stand-alone models.
- The findings suggest that SLG could enable small to medium-sized engineering companies to leverage generative AI technologies without the need for expensive computational resources.
- The graph architecture and small expert nodes offer potential for distributed AI systems, potentially reducing the reliance on costly centralized compute clusters. 

<br /><br /> <div>
arXiv:2505.21109v1 Announce Type: cross 
Abstract: Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Gap Between Data-Driven And Theory-Driven Modelling - Leveraging Causal Machine Learning for Integrative Modelling of Dynamical Systems</title>
<link>https://arxiv.org/abs/2410.09516</link>
<guid>https://arxiv.org/abs/2410.09516</guid>
<content:encoded><![CDATA[
<div> causal feature selection, domain knowledge, time-series data, machine learning, predictive robustness<br />
<br />
Summary: 
This study explores the use of causal feature selection with domain knowledge in improving machine learning applications, specifically in the context of a data center system. Traditional machine learning techniques often face challenges of overfitting and unreliable predictions in novel conditions. By incorporating causality into the modeling process, predictive robustness can be enhanced. The study compares causal feature selection with traditional feature selection methods using simulated time-series data. Results show that predictions based on causal features are more robust, highlighting the potential benefits of combining causal discovery algorithms with human expertise. This approach can help address the time-consuming process of manually constructing causal graphs, particularly in complex time series with numerous variables. <div>
arXiv:2410.09516v3 Announce Type: replace 
Abstract: Classical machine learning techniques often struggle with overfitting and unreliable predictions when exposed to novel conditions. Introducing causality into the modelling process offers a promising way to mitigate these challenges by enhancing predictive robustness. However, constructing an initial causal graph manually using domain knowledge is time-consuming, particularly in complex time series with numerous variables. To address this, causal discovery algorithms can provide a preliminary causal structure that domain experts can refine. This study investigates causal feature selection with domain knowledge using a data center system as an example. We use simulated time-series data to compare different causal feature selection with traditional machine-learning feature selection methods. Our results show that predictions based on causal features are more robust compared to those derived from traditional methods. These findings underscore the potential of combining causal discovery algorithms with human expertise to improve machine learning applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fusion with Relational Learning for Molecular Property Prediction</title>
<link>https://arxiv.org/abs/2410.12128</link>
<guid>https://arxiv.org/abs/2410.12128</guid>
<content:encoded><![CDATA[
<div> Graph-based molecular representation learning, multimodal fusion, relational learning, drug discovery, materials science
<br />
Summary: 
MMFRL (Multimodal Fusion with Relational Learning for Molecular Property Prediction) is introduced as a novel framework to enhance graph-based molecular representation learning. It addresses challenges in molecular property prediction by incorporating multimodal fusion at different stages such as early, intermediate, and late. The method improves embedding initialization through multimodal pretraining using relational learning. Extensive experiments on MoleculeNet benchmarks show that MMFRL outperforms existing methods significantly, allowing for task-specific optimizations. The explainability of MMFRL provides valuable chemical insights, enhancing its potential for real-world drug discovery applications. <div>
arXiv:2410.12128v2 Announce Type: replace 
Abstract: Graph based molecular representation learning is essential for accurately predicting molecular properties in drug discovery and materials science; however, it faces significant challenges due to the intricate relationships among molecules and the limited chemical knowledge utilized during training. While contrastive learning is often employed to handle molecular relationships, its reliance on binary metrics is insufficient for capturing the complexity of these interactions. Multimodal fusion has gained attention for property reasoning, but previous work has explored only a limited range of modalities, and the optimal stages for fusing different modalities in molecular property tasks remain underexplored. In this paper, we introduce MMFRL (Multimodal Fusion with Relational Learning for Molecular Property Prediction), a novel framework designed to overcome these limitations. Our method enhances embedding initialization through multimodal pretraining using relational learning. We also conduct a systematic investigation into the impact of modality fusion at different stages such as early, intermediate, and late, highlighting their advantages and shortcomings. Extensive experiments on MoleculeNet benchmarks demonstrate that MMFRL significantly outperforms existing methods. Furthermore, MMFRL enables task-specific optimizations. Additionally, the explainability of MMFRL provides valuable chemical insights, emphasizing its potential to enhance real-world drug discovery applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A batch production scheduling problem in a reconfigurable hybrid manufacturing-remanufacturing system</title>
<link>https://arxiv.org/abs/2504.00605</link>
<guid>https://arxiv.org/abs/2504.00605</guid>
<content:encoded><![CDATA[
<div> sustainability, remanufacturing, Hybrid Manufacturing-Remanufacturing System, Reconfigurable Manufacturing System, production scheduling

Summary:
The study focuses on production scheduling in a Hybrid Manufacturing-Remanufacturing System (HMRS) with non-identical parallel reconfigurable machines and batch orders. Models using Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) are developed, with a computationally efficient Logic-based Benders Decomposition (LBBD) method for solution. The LBBD approach outperforms MILP, CP, and warm-started MILP models, achieving an average gap of about 2%. The study highlights the benefits of utilizing Reconfigurable Manufacturing System (RMS) technologies in HMRSs and provides actionable managerial insights for scheduling in such systems. It addresses the complexity of production management in HMRSs and showcases the importance of customized production capabilities for increased flexibility and efficiency in processing both new products and End-of-Life (EOL) products in a shared facility. 

<br /><br />Summary: <div>
arXiv:2504.00605v2 Announce Type: replace 
Abstract: In recent years, remanufacturing of End-of-Life (EOL) products has been adopted by manufacturing sectors as a competent practice to enhance their sustainability and market share. Due to the mass customization of products and high volatility of market, processing of new products and remanufacturing of EOLs in the same shared facility, namely Hybrid Manufacturing-Remanufacturing System (HMRS), is a mean to keep such production efficient. Accordingly, customized production capabilities are required to increase flexibility, which can be effectively provided under the Reconfigurable Manufacturing System (RMS) paradigm. Despite the advantages of utilizing RMS technologies in HMRSs, production management of such systems suffers excessive complexity. Hence, this study concentrates on the production scheduling of an HMRS consisting of non-identical parallel reconfigurable machines where the orders can be grouped into batches. In this regard, Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) models are devised to formulate the problem. Furthermore, a computationally efficient solution method is developed based on a Logic-based Benders Decomposition (LBBD) approach. The warm start technique is also implemented by providing a decent initial solution to the MILP model. Computational experiments attest to the LBBD method's superiority over the MILP, CP, and warm-started MILP models by obtaining an average gap of about 2%, besides it yields actionable managerial insights for scheduling in HMRSs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-Enhanced Monte Carlo: A Machine Learning View on Control Variate</title>
<link>https://arxiv.org/abs/2412.11257</link>
<guid>https://arxiv.org/abs/2412.11257</guid>
<content:encoded><![CDATA[
<div> ML, Monte Carlo, Prediction-Enhanced, Variance Reduction, Simulation

Summary:
Prediction-Enhanced Monte Carlo (PEMC) is introduced as a framework that uses machine learning models as predictors to reduce variance and runtime in complex simulation tasks. It overcomes the computational challenges of traditional Monte Carlo methods by leveraging modern ML surrogates for unbiased evaluation. PEMC acts as a modernized control variate approach, considering overall computation-cost-aware variance reduction. It demonstrates efficacy in various scenarios, including equity derivatives, interest rate derivatives, and healthcare decision-making processes. In equity derivatives, it reduces variance for variance swaps under stochastic local volatility models. In interest rate derivatives, it enhances swaption pricing under the HJM interest-rate model. In healthcare decision-making, it assists in ambulance dispatch and hospital load balancing by providing accurate mortality rate estimates. PEMC consistently improves performance by reducing variance while maintaining unbiasedness, highlighting its potential as a valuable enhancement to standard Monte Carlo methods. 

<br /><br />Summary: 
Keywords: ML, Monte Carlo, Prediction-Enhanced, Variance Reduction, Simulation <div>
arXiv:2412.11257v2 Announce Type: replace-cross 
Abstract: For many complex simulation tasks spanning areas such as healthcare, engineering, and finance, Monte Carlo (MC) methods are invaluable due to their unbiased estimates and precise error quantification. Nevertheless, Monte Carlo simulations often become computationally prohibitive, especially for nested, multi-level, or path-dependent evaluations lacking effective variance reduction techniques. While machine learning (ML) surrogates appear as natural alternatives, naive replacements typically introduce unquantifiable biases. We address this challenge by introducing Prediction-Enhanced Monte Carlo (PEMC), a framework that leverages modern ML models as learned predictors, using cheap and parallelizable simulation as features, to output unbiased evaluation with reduced variance and runtime. PEMC can also be viewed as a "modernized" view of control variates, where we consider the overall computation-cost-aware variance reduction instead of per-replication reduction, while bypassing the closed-form mean function requirement and maintaining the advantageous unbiasedness and uncertainty quantifiability of Monte Carlo.
  We illustrate PEMC's broader efficacy and versatility through three examples: first, equity derivatives such as variance swaps under stochastic local volatility models; second, interest rate derivatives such as swaption pricing under the Heath-Jarrow-Morton (HJM) interest-rate model. Finally, we showcase PEMC in a socially significant context - ambulance dispatch and hospital load balancing - where accurate mortality rate estimates are key for ethically sensitive decision-making. Across these diverse scenarios, PEMC consistently reduces variance while preserving unbiasedness, highlighting its potential as a powerful enhancement to standard Monte Carlo baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Beyond Words: MatVQA for Challenging Visual-Scientific Reasoning in Materials Science</title>
<link>https://arxiv.org/abs/2505.18319</link>
<guid>https://arxiv.org/abs/2505.18319</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Materials Science, MatVQA, Scientific Reasoning, Benchmarking

Summary:
MatVQA is a new benchmark designed to address the limitations of current materials science evaluation datasets by integrating visual and language modalities for scientific reasoning. The dataset features 1325 questions across four structure-property-performance reasoning tasks, encouraging MLLMs to perform fine-grained visual analysis of material imagery. By benchmarking 17 MLLMs on MatVQA, significant gaps in multimodal reasoning capabilities were revealed. The benchmark data and evaluation code are publicly available to facilitate further research in applying MLLMs to complex materials science problems.<br /><br />Summary: <div>
arXiv:2505.18319v1 Announce Type: new 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) that integrate vision and language modalities has unlocked new potentials for scientific reasoning, outperforming prior benchmarks in both natural language and coding domains. Current materials science evaluation datasets such as MaScQA and SciQA remain largely text-based and fail to capture the visual and research-level analytic complexity required in materials discovery and design. We introduce MatVQA, a scalable benchmark specifically designed to address this gap. Generated via an automated pipeline, MArxivAgent, from recent materials literature, MatVQA features 1325 questions across four critical structure-property-performance (SPP) reasoning tasks. Uniquely, MatVQA employs an iterative process to eliminate textual shortcuts, compelling MLLMs to perform fine-grained, low-level visual analysis of material imagery (e.g., microscopy, diffraction patterns) integrated with multi-step scientific reasoning. Benchmarking 17 open- and closed-source MLLMs on MatVQA reveals substantial gaps in current multimodal reasoning capabilities. MatVQA benchmark data, along with evaluation code, is publicly available in \href{https://anonymous.4open.science/r/matvqa-1E01}{https://anonymous.4open.science/r/matvqa-1E01/README.md} to catalyze further research in applying MLLMs to complex materials science problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AERO: An autonomous platform for continuous research</title>
<link>https://arxiv.org/abs/2505.18408</link>
<guid>https://arxiv.org/abs/2505.18408</guid>
<content:encoded><![CDATA[
<div> automated research, data infrastructure, collaboration, epidemiology, public health

Summary: 
The article introduces AERO, an automated research and data sharing platform developed to support cross-sector investigations during the COVID-19 pandemic. AERO allows for the automatic ingestion, validation, and transformation of monitored data for analysis, as well as the execution of analyses and data sharing among different entities. Leveraging capabilities from the Globus platform and GitHub, AERO facilitates automation, distributed execution, data sharing, and authentication. The implementation of AERO has been demonstrated with two public health surveillance applications and benchmarking with a synthetic application. Users can access these applications for testing purposes. AERO addresses the need for new data infrastructure in rapidly evolving situations, such as public health emergencies, to enable continuous, distributed, and multi-disciplinary collaboration. <div>
arXiv:2505.18408v1 Announce Type: new 
Abstract: The COVID-19 pandemic highlighted the need for new data infrastructure, as epidemiologists and public health workers raced to harness rapidly evolving data, analytics, and infrastructure in support of cross-sector investigations. To meet this need, we developed AERO, an automated research and data sharing platform for continuous, distributed, and multi-disciplinary collaboration. In this paper, we describe the AERO design and how it supports the automatic ingestion, validation, and transformation of monitored data into a form suitable for analysis; the automated execution of analyses on this data; and the sharing of data among different entities. We also describe how our AERO implementation leverages capabilities provided by the Globus platform and GitHub for automation, distributed execution, data sharing, and authentication. We present results obtained with an instance of AERO running two public health surveillance applications and demonstrate benchmarking results with a synthetic application, all of which are publicly available for testing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Equilibrium: Non-Equilibrium Foundations Should Underpin Generative Processes in Complex Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.18621</link>
<guid>https://arxiv.org/abs/2505.18621</guid>
<content:encoded><![CDATA[
<div> Generative models, non-equilibrium physics, complex dynamical systems, empirical experiments, scientific modeling <br />
<br />
Generative models inspired by non-equilibrium physics are proposed as essential for better modeling complex dynamical systems due to limitations of classical equilibrium-based models. These non-equilibrium frameworks naturally capture evolving distributions and non-stationary behavior. Empirical experiments on a dynamic system validate the effectiveness of non-equilibrium generative models in tracking temporal evolution and adapting to changing landscapes. Future directions include integrating non-equilibrium principles with generative AI to simulate rare events, infer underlying mechanisms, and represent multi-scale dynamics across scientific domains. Embracing non-equilibrium physics is deemed necessary for generative AI to serve as a scientific modeling tool, offering new capabilities for simulating, understanding, and controlling complex systems.<br /><br />Summary: <div>
arXiv:2505.18621v1 Announce Type: new 
Abstract: This position paper argues that next-generation non-equilibrium-inspired generative models will provide the essential foundation for better modeling real-world complex dynamical systems. While many classical generative algorithms draw inspiration from equilibrium physics, they are fundamentally limited in representing systems with transient, irreversible, or far-from-equilibrium behavior. We show that non-equilibrium frameworks naturally capture non-equilibrium processes and evolving distributions. Through empirical experiments on a dynamic Printz potential system, we demonstrate that non-equilibrium generative models better track temporal evolution and adapt to non-stationary landscapes. We further highlight future directions such as integrating non-equilibrium principles with generative AI to simulate rare events, inferring underlying mechanisms, and representing multi-scale dynamics across scientific domains. Our position is that embracing non-equilibrium physics is not merely beneficial--but necessary--for generative AI to serve as a scientific modeling tool, offering new capabilities for simulating, understanding, and controlling complex systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A high-order matrix-free adaptive solver for the shallow water equations with irregular bathymetry</title>
<link>https://arxiv.org/abs/2505.18743</link>
<guid>https://arxiv.org/abs/2505.18743</guid>
<content:encoded><![CDATA[
<div> AMR, coastal engineering, Discontinuous Galerkin method, deal.II library, parallelization<br />
<br />
Summary: <br />
The article introduces a new Adaptive Mesh Refinement (AMR) solver for coastal engineering applications. It is based on the Discontinuous Galerkin (DG) method and implemented in the deal.II library, offering efficient parallelization and handling of non-conforming meshes. The method is well-balanced, adaptable to realistic bathymetry data without regularity assumptions, and conservatively discretizes transported chemical species. Idealized benchmarks validate the approach, demonstrating its potential for accurate and efficient adaptive simulations of coastal flows. The solver's capabilities are further evidenced through experiments on realistic bathymetries and complex domains. <div>
arXiv:2505.18743v1 Announce Type: new 
Abstract: We present the first step in the development of an Adaptive Mesh Refinement (AMR) solver for coastal engineering applications, based on a high-order Discontinuous Galerkin (DG) method as implemented in the deal.II library. This environment provides efficient and native parallelization techniques and automatically handles non-conforming meshes to implement both static and dynamic AMR approaches. The proposed method is automatically well-balanced, allows the use of realistic bathymetry data without any regularity assumption, and includes a consistent conservative discretization for transported chemical species. Numerical experiments on idealized benchmarks validate the proposed approach, while results obtained on realistic bathymetries and complex domains show its potential for accurate and efficient adaptive simulations of coastal flows.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoofNet: A Global Multimodal Dataset for Roof Material Classification</title>
<link>https://arxiv.org/abs/2505.19358</link>
<guid>https://arxiv.org/abs/2505.19358</guid>
<content:encoded><![CDATA[
<div> Dataset, RoofNet, Earth Observation imagery, roofing types, global exposure datasets
<br />
Summary: 
RoofNet is a new dataset that combines Earth Observation imagery with curated text annotations to classify roofing materials in buildings around the world. With over 51,500 samples from 184 sites, RoofNet labels 14 key roofing types, such as asphalt shingles and clay tiles, to enhance global exposure datasets. By fine-tuning a vision-language model on a subset of annotated images, RoofNet provides accurate predictions on roofing materials across different regions. The dataset includes rich metadata like roof shape, footprint area, solar panel presence, and mixed materials indicators. RoofNet's AI-driven risk assessment capabilities make it valuable for insurance underwriting, disaster preparedness, and infrastructure policy planning. It serves as a benchmark for evaluating model generalization and offers insights for decision-making in various sectors. 
<br /> <div>
arXiv:2505.19358v1 Announce Type: new 
Abstract: Natural disasters are increasing in frequency and severity, causing hundreds of billions of dollars in damage annually and posing growing threats to infrastructure and human livelihoods. Accurate data on roofing materials is critical for modeling building vulnerability to natural hazards such as earthquakes, floods, wildfires, and hurricanes, yet such data remain unavailable. To address this gap, we introduce RoofNet, the largest and most geographically diverse novel multimodal dataset to date, comprising over 51,500 samples from 184 geographically diverse sites pairing high-resolution Earth Observation (EO) imagery with curated text annotations for global roof material classification. RoofNet includes geographically diverse satellite imagery labeled with 14 key roofing types -- such as asphalt shingles, clay tiles, and metal sheets -- and is designed to enhance the fidelity of global exposure datasets through vision-language modeling (VLM). We sample EO tiles from climatically and architecturally distinct regions to construct a representative dataset. A subset of 6,000 images was annotated in collaboration with domain experts to fine-tune a VLM. We used geographic- and material-aware prompt tuning to enhance class separability. The fine-tuned model was then applied to the remaining EO tiles, with predictions refined through rule-based and human-in-the-loop verification. In addition to material labels, RoofNet provides rich metadata including roof shape, footprint area, solar panel presence, and indicators of mixed roofing materials (e.g., HVAC systems). RoofNet supports scalable, AI-driven risk assessment and serves as a downstream benchmark for evaluating model generalization across regions -- offering actionable insights for insurance underwriting, disaster preparedness, and infrastructure policy planning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Finite Element Neural Network (IFENN) for Phase-Field Fracture with Minimal Input and Generalized Geometry-Load Handling</title>
<link>https://arxiv.org/abs/2505.19566</link>
<guid>https://arxiv.org/abs/2505.19566</guid>
<content:encoded><![CDATA[
<div> novel formulation, phase-field fracture propagation, Integrated Finite Element Neural Network (IFENN), physics-informed convolutional networks (PICNNs), unsupervised training<br />
Summary:<br />
The article presents a new approach for modeling phase-field fracture propagation using the Integrated Finite Element Neural Network (IFENN) framework. This hybrid solver scheme combines neural networks as PDE solvers within FEM, improving accuracy and speed of predictions. The novel formulation involves using physics-informed convolutional networks (PICNNs) to calculate the phase-field variable while solving equilibrium equations with FEM. By eliminating temporal features and focusing on spatial coupling between strain energy density and phase-field variable, the training process is significantly reduced to just 5 minutes. The trained PICNN embedded within IFENN can simulate crack propagation in various scenarios with high accuracy, including rectangular domains, multiple interacting cracks, and different mesh densities. This breakthrough in hybrid modeling offers a physics-consistent solution for fracture and coupled problems. <br /><br />Summary: <div>
arXiv:2505.19566v1 Announce Type: new 
Abstract: We present a novel formulation for modeling phase-field fracture propagation based on the Integrated Finite Element Neural Network (IFENN) framework. IFENN is a hybrid solver scheme that utilizes neural networks as PDE solvers within FEM, preserving accuracy via residual minimization while achieving speed-up via swift network predictions and reduction of the size of system of equations in coupled problems. In this work, we introduce a radically new formulation of IFENN in which the phase-field variable is calculated using physics-informed convolutional networks (PICNNs), while the equilibrium equation is still solved using FEM to maintain the solver robustness. Unlike conventional approaches, which rely on sequence or time-dependent models, we eliminate the need to include temporal features in the training setup and inference stage. Instead, we show that it is sufficient to learn only the spatial coupling between the strain energy density and the phase-field variable in the vicinity of the fracture process zone, and utilize this information along the advancing crack simulation. We train a single CNN in a purely physics-based, unsupervised manner on just two load increments from a single-notch tension problem, with a total training time of only 5 minutes. Following this exceptionally minimal and fast training, we show that the same PICNN can (when embedded within IFENN) model crack propagation in a very wide range of unseen scenarios, including arbitrarily rectangular domains, single and multiple interacting cracks, varying mesh densities, and arbitrary loading paths. The proposed formulation delivers breakthroughs that address many of the limitations in the existing literature of hybrid modeling, introducing a new paradigm for the development of generalizable, physics-consistent hybrid models that are applicable to fracture and other coupled problems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets</title>
<link>https://arxiv.org/abs/2505.19819</link>
<guid>https://arxiv.org/abs/2505.19819</guid>
<content:encoded><![CDATA[
<div> datasets, LoRA methods, financial tasks, performance gains, democratize financial intelligence <br />
<br />
Keywords: datasets, LoRA methods, financial tasks, performance gains, democratize financial intelligence <br />
<br />
Summary: 
The paper introduces the FinLoRA project, which explores the efficacy of low-rank adaptation (LoRA) methods in high-stakes financial domains. The project curated 19 datasets covering various financial applications, including four novel XBRL analysis datasets based on SEC filings. Five LoRA methods and five base Large Language Models (LLMs) were evaluated, with LoRA methods showing substantial performance gains averaging 36% over base models. Extensive experimental results in terms of accuracy, F1, and BERTScore were provided, along with information on computational costs during fine-tuning and inference stages. The FinLoRA project aims to democratize financial intelligence by providing an affordable and scalable approach for the general public. The datasets, LoRA adapters, code, and documentation are available on GitHub at https://github.com/Open-Finance-Lab/FinLoRA. <br /> <div>
arXiv:2505.19819v1 Announce Type: new 
Abstract: Low-rank adaptation (LoRA) methods show great potential for scaling pre-trained general-purpose Large Language Models (LLMs) to hundreds or thousands of use scenarios. However, their efficacy in high-stakes domains like finance is rarely explored, e.g., passing CFA exams and analyzing SEC filings. In this paper, we present the open-source FinLoRA project that benchmarks LoRA methods on both general and highly professional financial tasks. First, we curated 19 datasets covering diverse financial applications; in particular, we created four novel XBRL analysis datasets based on 150 SEC filings. Second, we evaluated five LoRA methods and five base LLMs. Finally, we provide extensive experimental results in terms of accuracy, F1, and BERTScore and report computational cost in terms of time and GPU memory during fine-tuning and inference stages. We find that LoRA methods achieved substantial performance gains of 36\% on average over base models. Our FinLoRA project provides an affordable and scalable approach to democratize financial intelligence to the general public. Datasets, LoRA adapters, code, and documentation are available at https://github.com/Open-Finance-Lab/FinLoRA
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision</title>
<link>https://arxiv.org/abs/2505.18181</link>
<guid>https://arxiv.org/abs/2505.18181</guid>
<content:encoded><![CDATA[
<div> dataset, machine learning, NMR spectroscopy, molecular representation, annotated

Summary:
The article introduces 2DNMRGym, a dataset for machine learning-based molecular representation learning in two-dimensional Nuclear Magnetic Resonance (NMR) spectroscopy. The dataset contains over 22,000 Heteronuclear Single Quantum Coherence (HSQC) spectra along with corresponding molecular graphs and SMILES strings. Utilizing a surrogate supervision setup, models are trained on algorithm-generated annotations and evaluated on human-annotated gold-standard labels, allowing for rigorous assessment of model generalization. Benchmark results using 2D and 3D Graph Neural Network (GNN) and GNN transformer models demonstrate promising performance. This dataset supports scalable model training and provides a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. The data and code are open-source and available on Huggingface and Github. 

<br /><br />Summary: <div>
arXiv:2505.18181v1 Announce Type: cross 
Abstract: Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy, particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays a critical role in elucidating molecular structures, interactions, and electronic properties. However, accurately interpreting 2D NMR data remains labor-intensive and error-prone, requiring highly trained domain experts, especially for complex molecules. Machine Learning (ML) holds significant potential in 2D NMR analysis by learning molecular representations and recognizing complex patterns from data. However, progress has been limited by the lack of large-scale and high-quality annotated datasets. In this work, we introduce 2DNMRGym, the first annotated experimental dataset designed for ML-based molecular representation learning in 2D NMR. It includes over 22,000 HSQC spectra, along with the corresponding molecular graphs and SMILES strings. Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained using algorithm-generated annotations derived from a previously validated method and evaluated on a held-out set of human-annotated gold-standard labels. This enables rigorous assessment of a model's ability to generalize from imperfect supervision to expert-level interpretation. We provide benchmark results using a series of 2D and 3D GNN and GNN transformer models, establishing a strong foundation for future work. 2DNMRGym supports scalable model training and introduces a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. Our data and code is open-source and available on Huggingface and Github.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Optimization Algorithms for Energy Management Systems in Microgrids: A Control Strategy Based on a PHIL System</title>
<link>https://arxiv.org/abs/2505.18210</link>
<guid>https://arxiv.org/abs/2505.18210</guid>
<content:encoded><![CDATA[
<div> optimization, microgrid, power hardware, renewable energy sources, energy management

Summary:<br />
- An adaptive multi-objective optimization approach was implemented in a real-time power hardware-in-loop configuration for a microgrid with various energy resources.
- The approach effectively balanced factors such as fuel consumption, load mismatch, power quality, battery degradation, and the use of renewable energy sources.
- Real-time experimental data was used for dynamic system state updates, with adaptive preference-based selection adjusted based on battery charging thresholds.
- The technique integrated six technical objectives and complex constraints, aiding in practical microgrid decision making and dynamic energy system optimization.
- The energy management process successfully maximized photovoltaic production, minimized power mismatch, and stabilized battery state of charge in different conditions, outperforming a baseline system without optimization techniques.<br /><br /> <div>
arXiv:2505.18210v1 Announce Type: cross 
Abstract: In this research a real time power hardware in loop configuration has been implemented for an microgrid with the combination of distribution energy resources such as photovoltaic, grid tied inverter, battery, utility grid, and a diesel generator. This paper introduces an unique adaptive multi-objective optimization approach that employs weighted optimization techniques for real-time microgrid systems. The aim is to effectively balance various factors including fuel consumption, load mismatch, power quality, battery degradation, and the utilization of renewable energy sources. A real time experimental data from power hardware in loop system has been used for dynamically updating system states. The adaptive preference-based selection method are adjusted based on state of battery charging thresholds. The technique has been integrated with six technical objectives and complex constraints. This approach helps to practical microgrid decision making and optimization of dynamic energy systems. The energy management process were also able to maximize photovoltaic production where minimizing power mismatch, stabilizing battery state of charge under different condition. The research results were also compared with the baseline system without optimization techniques, and a reliable outcome was found.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
<div> PINNs, immersed boundary method, fluid-structure interaction, neural network architectures, adaptive activation functions <br />
Summary:
Neural network architectures combining physics-informed neural networks (PINNs) with the immersed boundary method (IBM) were proposed for fluid-structure interaction (FSI) problems. Two architectures, Single-FSI and Eulerian-Lagrangian, were compared using standard Tanh and adaptive B-spline activation functions. The Eulerian-Lagrangian architecture showed superior performance, with the adaptive B-spline activation improving accuracy near boundaries. While velocity field prediction was successful, pressure recovery proved challenging without explicit force-coupling constraints. The study emphasized the importance of domain-specific architectural design and adaptive activation functions in modeling FSI within the PINN framework. <br /> <div>
arXiv:2505.18565v1 Announce Type: cross 
Abstract: We introduce neural network architectures that combine physics-informed neural networks (PINNs) with the immersed boundary method (IBM) to solve fluid-structure interaction (FSI) problems. Our approach features two distinct architectures: a Single-FSI network with a unified parameter space, and an innovative Eulerian-Lagrangian network that maintains separate parameter spaces for fluid and structure domains. We study each architecture using standard Tanh and adaptive B-spline activation functions. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that the Eulerian-Lagrangian architecture performs significantly better. The adaptive B-spline activation further enhances accuracy by providing locality-aware representation near boundaries. While our methodology shows promising results in predicting the velocity field, pressure recovery remains challenging due to the absence of explicit force-coupling constraints in the current formulation. Our findings underscore the importance of domain-specific architectural design and adaptive activation functions for modeling FSI problems within the PINN framework.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete gradient methods for port-Hamiltonian differential-algebraic equations</title>
<link>https://arxiv.org/abs/2505.18810</link>
<guid>https://arxiv.org/abs/2505.18810</guid>
<content:encoded><![CDATA[
<div> discrete gradient methods, nonlinear port-Hamiltonian differential-algebraic equations, numerical scheme, Dirac-dissipative structures, multibody system dynamics 

Summary: 
Discrete gradient methods are effective for time discretization of dynamical systems, preserving structure regardless of the total energy form. This study focuses on applying these methods to nonlinear port-Hamiltonian differential-algebraic equations, commonly used in modeling physical systems. A novel numerical scheme is introduced for semi-explicit differential-algebraic equations, with the utilization of discrete gradient pairs and Dirac-dissipative structures in general settings. The behavior under system transformations is analyzed, showing that these equations can be represented as a parametrized port-Hamiltonian semi-explicit system and an unstructured equation under specific conditions. The application to multibody system dynamics is demonstrated through numerical results, showcasing the efficiency of the approach. <div>
arXiv:2505.18810v1 Announce Type: cross 
Abstract: Discrete gradient methods are a powerful tool for the time discretization of dynamical systems, since they are structure-preserving regardless of the form of the total energy. In this work, we discuss the application of discrete gradient methods to the system class of nonlinear port-Hamiltonian differential-algebraic equations - as they emerge from the port- and energy-based modeling of physical systems in various domains. We introduce a novel numerical scheme tailored for semi-explicit differential-algebraic equations and further address more general settings using the concepts of discrete gradient pairs and Dirac-dissipative structures. Additionally, the behavior under system transformations is investigated and we demonstrate that under suitable assumptions port-Hamiltonian differential-algebraic equations admit a representation which consists of a parametrized port-Hamiltonian semi-explicit system and an unstructured equation. Finally, we present the application to multibody system dynamics and discuss numerical results to demonstrate the capabilities of our approach.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search</title>
<link>https://arxiv.org/abs/2505.19209</link>
<guid>https://arxiv.org/abs/2505.19209</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, scientific hypothesis discovery, combinatorial optimization, hierarchical search method, chemistry literature

Summary:
1. The study introduces the task of fine-grained scientific hypothesis discovery, aiming to generate detailed and experimentally actionable hypotheses from initial research directions.
2. The research explores how to utilize an LLM's internal heuristics to formulate the most promising hypothesis based on its own scoring system.
3. The alignment between LLM-generated hypotheses and ground-truth hypotheses is investigated to evaluate the quality of the generated hypotheses.
4. Comparisons are made between using an ensemble of diverse LLMs and repeated instances of the strongest LLM to shape the reward landscape for hypothesis generation.
5. The study also analyzes the effectiveness of an ensemble of identical LLMs in providing a reliable reward landscape for hypothesis discovery.
6. The proposed hierarchical search method incrementally adds details to hypotheses, leading to a smoother reward landscape and more effective optimization.
7. Empirical evaluations on a chemistry literature benchmark demonstrate that the hierarchical search method consistently outperforms strong baselines. 

<br /><br />Summary: <div>
arXiv:2505.19209v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the novel task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent chemistry literature show that our method consistently outperforms strong baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid Development of Efficient Participant-Specific Computational Models of the Wrist</title>
<link>https://arxiv.org/abs/2505.19282</link>
<guid>https://arxiv.org/abs/2505.19282</guid>
<content:encoded><![CDATA[
<div> Keywords: computational modeling, hand and wrist injuries, finite element models, participant-specific, ligament injury

Summary:
Computational modeling plays a crucial role in developing treatment options for hand and wrist injuries. However, the current models are limited, often relying on average material properties from literature. A novel automated workflow has been developed to create participant-specific finite element models using non-linear morphing techniques and algorithmic approaches. By utilizing four-dimensional computed tomography (4DCT) data, three participant-specific models were created within 2 hours, allowing for individual simulations to be performed in just 45 seconds. The models were used to investigate clinical questions such as optimizing ligament properties to participant-specific kinematics and conducting Monte Carlo analysis on the effects of ligament injury on joint contact pressure. This work paves the way for future patient-specific modeling of hand and wrist injuries, offering a more personalized approach to treatment and understanding the impacts of injuries on joint health. 

<br /><br />Summary: <div>
arXiv:2505.19282v1 Announce Type: cross 
Abstract: While computational modeling may help to develop new treatment options for hand and wrist injuries, at present, few models exist. The time and expertise required to develop and use these models is considerable. Moreover, most do not allow for variation of material properties, instead relying on literature reported averages. We have developed a novel automated workflow combining non-linear morphing techniques with various algorithmic techniques to create participant-specific finite element models. Using this workflow, three participant-specific models were created from our existing four-dimensional computed tomography (4DCT) data. These were then used to perform two analyses to demonstrate the usefulness of the models to investigate clinical questions, namely optimization of ligament properties to participant-specific kinematics, and Monte Carlo (MC) analysis of the impacts of ligament injury on joint contact pressure, as an analogue for joint injury that may lead to osteoarthritis. Participant-specific models can be created in 2 hours and individual simulations performed in 45 seconds. This work lays the groundwork for future patient-specific modeling of the hand and wrist.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs</title>
<link>https://arxiv.org/abs/2505.19457</link>
<guid>https://arxiv.org/abs/2505.19457</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, finance, evaluation, reasoning <br /> 
<br />
Summary: 
The article introduces BizFinBench, a benchmark designed to evaluate Large Language Models (LLMs) in financial applications. It includes 6,781 annotated queries in Chinese across various dimensions such as numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering. The benchmark utilizes both objective and subjective metrics and introduces IteraJudge, a method to reduce bias when LLMs serve as evaluators. The evaluation of 25 models shows no single model excels in all tasks, revealing distinct capability patterns. In Numerical Calculation, Claude-3.5-Sonnet and DeepSeek-R1 lead, while in Reasoning, proprietary models outperform open-source models. Information Extraction shows the largest performance spread, while Prediction Recognition performance variance is minimal. The study highlights that while current LLMs handle routine finance queries well, they struggle with complex scenarios that require cross-concept reasoning. BizFinBench aims to provide a rigorous benchmark for future research in the financial domain. <div>
arXiv:2505.19457v1 Announce Type: cross 
Abstract: Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients</title>
<link>https://arxiv.org/abs/2505.19538</link>
<guid>https://arxiv.org/abs/2505.19538</guid>
<content:encoded><![CDATA[
<div> Knowledge bases, experiential knowledge, DoctorRAG, retrieval precision, Med-TextGrad <br />
Summary: <br />
Existing medical reasoning systems often focus on knowledge bases, neglecting the importance of experiential knowledge from patient cases. DoctorRAG is introduced as a framework that combines clinical knowledge and case-based experience to mimic doctor-like reasoning. It enhances retrieval precision by tagging concepts and employing a hybrid retrieval mechanism from knowledge sources and patient data. The integration of the Med-TextGrad module ensures the output aligns with retrieved knowledge and patient queries. Experimental results on diverse datasets show DoctorRAG outperforms traditional RAG models, with iterative refinements further improving performance. The approach generates more accurate, relevant, and comprehensive responses, highlighting progress towards developing medical reasoning systems that more closely mimic human clinical reasoning. <div>
arXiv:2505.19538v1 Announce Type: cross 
Abstract: Existing medical RAG systems mainly leverage knowledge from medical knowledge bases, neglecting the crucial role of experiential knowledge derived from similar patient cases -- a key component of human clinical reasoning. To bridge this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like reasoning by integrating both explicit clinical knowledge and implicit case-based experience. DoctorRAG enhances retrieval precision by first allocating conceptual tags for queries and knowledge sources, together with a hybrid retrieval mechanism from both relevant knowledge and patient. In addition, a Med-TextGrad module using multi-agent textual gradients is integrated to ensure that the final output adheres to the retrieved knowledge and patient query. Comprehensive experiments on multilingual, multitask datasets demonstrate that DoctorRAG significantly outperforms strong baseline RAG models and gains improvements from iterative refinements. Our approach generates more accurate, relevant, and comprehensive responses, taking a step towards more doctor-like medical reasoning systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based Visual Pathway Delineation</title>
<link>https://arxiv.org/abs/2505.19733</link>
<guid>https://arxiv.org/abs/2505.19733</guid>
<content:encoded><![CDATA[
<div> Framework, visual pathway, MRI, feature decomposition, semi-supervised<br />
<br />
Summary: 
Accurately delineating the visual pathway (VP) using multi-parametric MR imaging data is crucial for understanding the human visual system and diagnosing related disorders. Existing methods struggle with complex cross-sequence relationships and the need for large labeled datasets. To address these challenges, a novel semi-supervised framework is proposed. It includes a correlation-constrained feature decomposition (CFD) to capture unique MRI sequence characteristics and aid in information fusion. Additionally, a consistency-based sample enhancement (CSE) module generates edge information from unlabeled data to mitigate the limited labeled data issue. The framework outperforms seven state-of-the-art approaches in VP delineation on public and in-house datasets, showcasing its effectiveness in leveraging multi-parametric MRI data for improved visual pathway delineation. <br /> <div>
arXiv:2505.19733v1 Announce Type: cross 
Abstract: Accurately delineating the visual pathway (VP) is crucial for understanding the human visual system and diagnosing related disorders. Exploring multi-parametric MR imaging data has been identified as an important way to delineate VP. However, due to the complex cross-sequence relationships, existing methods cannot effectively model the complementary information from different MRI sequences. In addition, these existing methods heavily rely on large training data with labels, which is labor-intensive and time-consuming to obtain. In this work, we propose a novel semi-supervised multi-parametric feature decomposition framework for VP delineation. Specifically, a correlation-constrained feature decomposition (CFD) is designed to handle the complex cross-sequence relationships by capturing the unique characteristics of each MRI sequence and easing the multi-parametric information fusion process. Furthermore, a consistency-based sample enhancement (CSE) module is developed to address the limited labeled data issue, by generating and promoting meaningful edge information from unlabeled data. We validate our framework using two public datasets, and one in-house Multi-Shell Diffusion MRI (MDM) dataset. Experimental results demonstrate the superiority of our approach in terms of delineation performance when compared to seven state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Generative Approach to Credit Prediction with Learnable Prompts for Multi-scale Temporal Representation Learning</title>
<link>https://arxiv.org/abs/2404.13004</link>
<guid>https://arxiv.org/abs/2404.13004</guid>
<content:encoded><![CDATA[
<div> Keywords: credit scoring, deep learning, FinLangNet, multi-scale distributions, time series classification

Summary:
FinLangNet is a novel approach for credit scoring that utilizes deep learning to generate multi-scale distributions of a user's future behavior by transforming tabular data into sequential representations. By incorporating prompt-based training inspired by Large Language Models, FinLangNet introduces prompts at different levels to capture user behavior effectively. Experimental results show that FinLangNet outperforms traditional methods like XGBoost, achieving significant improvements in performance metrics such as KS metric and relative bad debt rate. Moreover, FinLangNet demonstrates superior performance in time series classification tasks on public UEA archives, highlighting its scalability and adaptability in financial scenarios.<br /><br />Summary: <div>
arXiv:2404.13004v4 Announce Type: replace 
Abstract: Recent industrial credit scoring models remain heavily reliant on manually tuned statistical learning methods. While deep learning offers promising solutions, its effectiveness is often limited by the complexity of financial data, particularly in long-horizon scenarios. In this work, we propose FinLangNet, which addresses credit scoring by reframing it as the task of generating multi-scale distributions of a user's future behavior. Within this framework, tabular data is transformed into sequential representations, enabling the generation of user embeddings across multiple temporal scales. Inspired by the recent success of prompt-based training in Large Language Models (LLMs), FinLangNet also introduces two types of prompts to model and capture user behavior at both the feature-granularity and user-granularity levels. Experimental results demonstrate that FinLangNet outperforms the online XGBoost benchmark, achieving a 7.2\% improvement in KS metric performance and a 9.9\% reduction in the relative bad debt rate. Furthermore, FinLangNet exhibits superior performance on public UEA archives, underscoring its scalability and adaptability in time series classification tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Pathways in Reaction Networks guided by Energy Barriers using Integer Linear Programming</title>
<link>https://arxiv.org/abs/2504.10609</link>
<guid>https://arxiv.org/abs/2504.10609</guid>
<content:encoded><![CDATA[
<div> methodology, pathways, reaction networks, kinetic information, energy barriers 
Summary: 
This study introduces a computational methodology for exploring synthesis pathways in chemical reaction networks. The approach incorporates integer linear programming and directed hypergraphs to model reaction networks. Multiple pathways meeting search criteria can be ranked based on an objective function maximizing pathway probability. An automated pipeline estimates energy barriers for reactions in the network. This methodology allows for flexible and kinetically informed pathway exploration on large reaction networks, even when lacking kinetic annotations. It can be applied to networks generated via molecular space expansion approaches. <div>
arXiv:2504.10609v2 Announce Type: replace 
Abstract: Analyzing synthesis pathways for target molecules in a chemical reaction network annotated with information on the kinetics of individual reactions is an area of active study. This work presents a computational methodology for searching for pathways in reaction networks which is based on integer linear programming and the modeling of reaction networks by directed hypergraphs. Often multiple pathways fit the given search criteria. To rank them, we develop an objective function based on physical arguments maximizing the probability of the pathway. We furthermore develop an automated pipeline to estimate the energy barriers of individual reactions in reaction networks. Combined, the methodology facilitates flexible and kinetically informed pathway investigations on large reaction networks by computational means, even for networks coming without kinetic annotation, such as those created via generative approaches for expanding molecular spaces.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models</title>
<link>https://arxiv.org/abs/2411.02083</link>
<guid>https://arxiv.org/abs/2411.02083</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, quantitative reasoning, Number Token Loss, regression-like loss, token level<br />
Summary:<br />
Language models excel at text generation but struggle with quantitative reasoning due to a lack of inductive bias for numbers. The Cross Entropy loss, designed for nominal scale data, hinders number token proximity understanding. To address this, a Number Token Loss (NTL) is proposed, minimizing Lp norm or Wasserstein distance between real and predicted number tokens. NTL enhances math-related task performance without increasing runtime, even matching regression head performance in a direct comparison. Scaling to 3B parameter models shows improved performance, showcasing potential for seamless integration into Large Language Models (LLMs). This work aims to inspire LLM developers to enhance pretraining objectives. <div>
arXiv:2411.02083v2 Announce Type: replace-cross 
Abstract: While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the Cross Entropy loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the Lp norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the Cross Entropy objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope that this work can inspire LLM developers to improve their pretraining objectives. The code is available via: https://tum-ai.github.io/number-token-loss/
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemToolAgent: The Impact of Tools on Language Agents for Chemistry Problem Solving</title>
<link>https://arxiv.org/abs/2411.07228</link>
<guid>https://arxiv.org/abs/2411.07228</guid>
<content:encoded><![CDATA[
<div> ChemToolAgent, Chemistry tasks, Large language models, Evaluation, Specialized tools<br />
Summary:<br />
ChemToolAgent, an enhanced chemistry agent over ChemCrow, was developed to improve large language models (LLMs) for chemistry problem solving. Comprehensive evaluation showed that while ChemToolAgent did not consistently outperform LLMs without tools, it excelled in specialized chemistry tasks like synthesis prediction when augmented with specialized tools. However, for general chemistry questions like those in exams, agents' ability to reason correctly with chemistry knowledge proved to be more critical, and tool augmentation did not always provide added benefits. Error analysis with a chemistry expert supported these findings, highlighting the importance of considering task specificity and the role of tools in enhancing LLMs for diverse chemistry tasks. <div>
arXiv:2411.07228v3 Announce Type: replace-cross 
Abstract: To enhance large language models (LLMs) for chemistry problem solving, several LLM-based agents augmented with tools have been proposed, such as ChemCrow and Coscientist. However, their evaluations are narrow in scope, leaving a large gap in understanding the benefits of tools across diverse chemistry tasks. To bridge this gap, we develop ChemToolAgent, an enhanced chemistry agent over ChemCrow, and conduct a comprehensive evaluation of its performance on both specialized chemistry tasks and general chemistry questions. Surprisingly, ChemToolAgent does not consistently outperform its base LLMs without tools. Our error analysis with a chemistry expert suggests that: For specialized chemistry tasks, such as synthesis prediction, we should augment agents with specialized tools; however, for general chemistry questions like those in exams, agents' ability to reason correctly with chemistry knowledge matters more, and tool augmentation does not always help.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Diffusion Autoencoder for Test-time Adapting Prediction of Complex Systems</title>
<link>https://arxiv.org/abs/2505.17459</link>
<guid>https://arxiv.org/abs/2505.17459</guid>
<content:encoded><![CDATA[
<div> SparseDiff, test-time adaptation, sparse encoder, graph neural ordinary differential equation, spatial interactions <br />
Summary: SparseDiff is a novel approach for predicting the behavior of complex systems by dynamically updating the encoding scheme to capture emergent spatiotemporal structures. It utilizes a codebook-based sparse encoder to create a sparse graph topology of the spatial domain, coupled with a graph neural ordinary differential equation to model dynamics and a diffusion decoder for reconstruction. This method enables autoregressive prediction of spatiotemporal evolution while adapting the sparse topological structure to accommodate emerging spatial patterns through adaptive re-encoding. Extensive evaluations show that SparseDiff significantly reduces prediction errors compared to baselines, requiring only a fraction of the spatial resolution. <div>
arXiv:2505.17459v1 Announce Type: new 
Abstract: Predicting the behavior of complex systems is critical in many scientific and engineering domains, and hinges on the model's ability to capture their underlying dynamics. Existing methods encode the intrinsic dynamics of high-dimensional observations through latent representations and predict autoregressively. However, these latent representations lose the inherent spatial structure of spatiotemporal dynamics, leading to the predictor's inability to effectively model spatial interactions and neglect emerging dynamics during long-term prediction. In this work, we propose SparseDiff, introducing a test-time adaptation strategy to dynamically update the encoding scheme to accommodate emergent spatiotemporal structures during the long-term evolution of the system. Specifically, we first design a codebook-based sparse encoder, which coarsens the continuous spatial domain into a sparse graph topology. Then, we employ a graph neural ordinary differential equation to model the dynamics and guide a diffusion decoder for reconstruction. SparseDiff autoregressively predicts the spatiotemporal evolution and adjust the sparse topological structure to adapt to emergent spatiotemporal patterns by adaptive re-encoding. Extensive evaluations on representative systems demonstrate that SparseDiff achieves an average prediction error reduction of 49.99\% compared to baselines, requiring only 1\% of the spatial resolution.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Imputation before Prediction: A New Computational Paradigm for De Novo Peptide Sequencing</title>
<link>https://arxiv.org/abs/2505.17524</link>
<guid>https://arxiv.org/abs/2505.17524</guid>
<content:encoded><![CDATA[
<div> Keywords: de novo peptide sequencing, missing fragmentation, latent imputation, set-prediction, performance improvement

Summary:
De novo peptide sequencing is a critical technique for determining peptide sequences directly from mass spectrometry data. The new computational approach, LIPNovo, addresses the challenge of missing fragmentation in spectra by imputing missing information in the latent space using theoretical peak profiles of target peptides. By framing the imputation as a set-prediction problem and utilizing learnable peak queries, LIPNovo effectively supplements missing data during inference, leading to improved performance. Experimental results on benchmark datasets show that LIPNovo surpasses existing methods significantly. The code for LIPNovo is available on GitHub for further exploration and application. 

<br /><br />Summary: De novo peptide sequencing is vital for peptide identification from mass spectrometry data. LIPNovo introduces a novel approach to handle missing fragmentation by imputing information in the latent space, enhancing performance through set-prediction techniques. Experimentally, LIPNovo outperforms state-of-the-art methods, underscoring its effectiveness in peptide sequencing tasks. The availability of the code on GitHub enables wider adoption and exploration of LIPNovo's capabilities. <div>
arXiv:2505.17524v1 Announce Type: new 
Abstract: De novo peptide sequencing is a fundamental computational technique for ascertaining amino acid sequences of peptides directly from tandem mass spectrometry data, eliminating the need for reference databases. Cutting-edge models usually encode the observed mass spectra into latent representations from which peptides are predicted autoregressively. However, the issue of missing fragmentation, attributable to factors such as suboptimal fragmentation efficiency and instrumental constraints, presents a formidable challenge in practical applications. To tackle this obstacle, we propose a novel computational paradigm called \underline{\textbf{L}}atent \underline{\textbf{I}}mputation before \underline{\textbf{P}}rediction (LIPNovo). LIPNovo is devised to compensate for missing fragmentation information within observed spectra before executing the final peptide prediction. Rather than generating raw missing data, LIPNovo performs imputation in the latent space, guided by the theoretical peak profile of the target peptide sequence. The imputation process is conceptualized as a set-prediction problem, utilizing a set of learnable peak queries to reason about the relationships among observed peaks and directly generate the latent representations of theoretical peaks through optimal bipartite matching. In this way, LIPNovo manages to supplement missing information during inference and thus boosts performance. Despite its simplicity, experiments on three benchmark datasets demonstrate that LIPNovo outperforms state-of-the-art methods by large margins. Code is available at \href{https://github.com/usr922/LIPNovo}{https://github.com/usr922/LIPNovo}.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4D-CTA Image and geometry dataset for kinematic analysis of abdominal aortic aneurysms</title>
<link>https://arxiv.org/abs/2505.17647</link>
<guid>https://arxiv.org/abs/2505.17647</guid>
<content:encoded><![CDATA[
<div> Dataset, Abdominal Aortic Aneurysm, 4D-CTA, Biomechanics, Image Registration

Summary:<br /><br />This article presents a dataset of 4D-CTA images and patient-specific AAA geometries from ten patients for kinematic analysis of abdominal aortic aneurysms (AAA). The dataset includes images captured throughout the cardiac cycle, allowing for the study of AAA wall displacement and strain. Synthetic ground truth data from Patient 1's image is also included for method verification. The dataset facilitates non-invasive analysis of AAA kinematics using an image registration-based approach. The use of open-source file formats enhances the applicability and reusability of the dataset in AAA biomechanics studies. The research was conducted at the ISML-UWA, using images acquired at Fiona Stanley Hospital in Western Australia. This dataset provides valuable information for understanding the biomechanics of AAA and could aid in the development of improved diagnostic and treatment strategies.<br />Summary: <div>
arXiv:2505.17647v1 Announce Type: new 
Abstract: This article presents a dataset used in the article "Kinematics of Abdominal Aortic Aneurysms" [arXiv:2405.13377], published in the Journal of Biomechanics. The dataset is publicly available for download from the Zenodo data repository (https://doi.org/10.5281/zenodo.15477710). The dataset includes time-resolved 3D computed tomography angiography (4D-CTA) images of abdominal aortic aneurysm (AAA) captured throughout the cardiac cycle from ten patients diagnosed with AAA, along with ten patient-specific AAA geometries extracted from these images. Typically, the 4D-CTA dataset for each patient contains ten electrocardiogram (ECG)-gated 3D-CTA image frames acquired over a cardiac cycle, capturing both the systolic and diastolic phases of the AAA configuration. For method verification, the dataset also includes synthetic ground truth data generated from Patient 1's 3D-CTA AAA image in the diastolic phase. The ground truth data includes the patient-specific finite element (FE) biomechanical model and a synthetic systolic 3D-CTA image. The synthetic systolic image was generated by warping Patient 1's diastolic 3D-CTA image using the realistic displacement field obtained from the AAA biomechanical FE model. The images were acquired at Fiona Stanley Hospital in Western Australia and provided to the researchers at the Intelligent Systems for Medicine Laboratory at The University of Western Australia (ISML-UWA), where image-based AAA kinematic analysis was performed. Our dataset enabled the analysis of AAA wall displacement and strain throughout the cardiac cycle using a non-invasive, in vivo, image registration-based approach. The use of widely adopted, open-source file formats (NRRD for images and STL for geometries) facilitates broad applicability and reusability in AAA biomechanics studies that require patient-specific geometry and information about AAA kinematics during cardiac cycle.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A brief review of the Deep BSDE method for solving high-dimensional partial differential equations</title>
<link>https://arxiv.org/abs/2505.17032</link>
<guid>https://arxiv.org/abs/2505.17032</guid>
<content:encoded><![CDATA[
<div> Deep BSDE method, high-dimensional PDEs, numerical computation, deep learning techniques, neural networks <br />
<br />
Summary: 
The article discusses the challenges posed by high-dimensional partial differential equations (PDEs) in traditional numerical computation methods. It introduces the Deep BSDE method, which utilizes deep learning techniques to effectively solve nonlinear PDEs in high dimensions. Since its inception in 2017, the Deep BSDE method has generated widespread interest in using neural networks for tackling high-dimensional PDEs. The article briefly outlines the method, its subsequent developments, and highlights the active research being conducted in this area. The Deep BSDE method has opened up new possibilities for solving complex PDEs in very high dimensions, offering a promising approach for addressing the curse of dimensionality in numerical computations. Future directions for research in this field are also discussed, suggesting potential advancements and applications of deep learning techniques in solving high-dimensional PDEs. <div>
arXiv:2505.17032v1 Announce Type: cross 
Abstract: High-dimensional partial differential equations (PDEs) pose significant challenges for numerical computation due to the curse of dimensionality, which limits the applicability of traditional mesh-based methods. Since 2017, the Deep BSDE method has introduced deep learning techniques that enable the effective solution of nonlinear PDEs in very high dimensions. This innovation has sparked considerable interest in using neural networks for high-dimensional PDEs, making it an active area of research. In this short review, we briefly sketch the Deep BSDE method, its subsequent developments, and future directions for the field.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</title>
<link>https://arxiv.org/abs/2505.17050</link>
<guid>https://arxiv.org/abs/2505.17050</guid>
<content:encoded><![CDATA[
<div> Keywords: Project-Based Learning, Multimodal Large Language Models, PBLBench, Analytic Hierarchy Process, Education

Summary:
PBLBench is a new benchmark designed to evaluate complex reasoning tasks in Project-Based Learning using multimodal large language models. The benchmark challenges models with tasks that mirror those handled by human experts, assessing their performance using structured and weighted evaluation criteria derived from the Analytic Hierarchy Process. The study tested 15 leading models and found that even the most advanced ones achieved only 59% rank accuracy, highlighting the difficulty of the benchmark. This indicates the challenges presented by real-world educational tasks and the need for more capable AI agents to assist teachers effectively. PBLBench aims to improve teacher workload and enhance educational productivity by providing a rigorous evaluation platform for multimodal large language models in educational settings.<br /><br />Summary: <div>
arXiv:2505.17050v1 Announce Type: cross 
Abstract: Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback</title>
<link>https://arxiv.org/abs/2505.17873</link>
<guid>https://arxiv.org/abs/2505.17873</guid>
<content:encoded><![CDATA[
<div> simulator, experiment-guided ranking, hypothesis, chemistry, automated scientific discovery

Summary:
The paper introduces the concept of experiment-guided ranking in the context of hypothesis prioritization in natural sciences. Existing approaches in automated scientific discovery focus on pre-experiment ranking without considering empirical outcomes. To address this gap, the authors propose a simulator that models hypothesis performance based on similarity to a known ground truth hypothesis, incorporating noise. They validate the simulator using a dataset of chemistry hypotheses with experimentally reported outcomes. The proposed method clusters hypotheses based on functional characteristics and uses insights from simulated experimental feedback to prioritize candidates. Experimental results demonstrate that the method outperforms pre-experiment baselines and strong ablations in hypothesis ranking. <div>
arXiv:2505.17873v1 Announce Type: cross 
Abstract: Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on large language model's internal reasoning without incorporating empirical outcomes from experiments. We introduce the task of experiment-guided ranking, which aims to prioritize candidate hypotheses based on the results of previously tested ones. However, developing such strategies is challenging due to the impracticality of repeatedly conducting real experiments in natural science domains. To address this, we propose a simulator grounded in three domain-informed assumptions, modeling hypothesis performance as a function of similarity to a known ground truth hypothesis, perturbed by noise. We curate a dataset of 124 chemistry hypotheses with experimentally reported outcomes to validate the simulator. Building on this simulator, we develop a pseudo experiment-guided ranking method that clusters hypotheses by shared functional characteristics and prioritizes candidates based on insights derived from simulated experimental feedback. Experiments show that our method outperforms pre-experiment baselines and strong ablations.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SINDyG: Sparse Identification of Nonlinear Dynamical Systems from Graph-Structured Data</title>
<link>https://arxiv.org/abs/2409.04463</link>
<guid>https://arxiv.org/abs/2409.04463</guid>
<content:encoded><![CDATA[
<div> Sparse Identification of Nonlinear Dynamical Systems, Machine Learning, Sparsity-promoting techniques, Network structure, Neuronal dynamics

Summary: 
The article introduces a new method called Sparse Identification of Nonlinear Dynamical Systems from Graph-structured data (SINDyG), which incorporates network structure into sparse regression for identifying model parameters that explain underlying network dynamics. Unlike existing methods, SINDyG considers interactions between subsystems, allowing for capturing small changes in emergent system behavior. The method is applied to neuronal dynamics, modeling macroscopic oscillations of a neuron population with the extended Stuart-Landau equation. Computational experiments demonstrate improved accuracy and simplicity in identifying network dynamics compared to the original SINDy approach. This innovative approach of combining machine learning with sparsity-promoting techniques opens up new possibilities for extracting governing equations from data in various scientific fields. <br /><br />Summary: <div>
arXiv:2409.04463v3 Announce Type: replace-cross 
Abstract: The combination of machine learning (ML) and sparsity-promoting techniques is enabling direct extraction of governing equations from data, revolutionizing computational modeling in diverse fields of science and engineering. The discovered dynamical models could be used to address challenges in climate science, neuroscience, ecology, finance, epidemiology, and beyond. However, most existing sparse identification methods for discovering dynamical systems treat the whole system as one without considering the interactions between subsystems. As a result, such models are not able to capture small changes in the emergent system behavior. To address this issue, we developed a new method called Sparse Identification of Nonlinear Dynamical Systems from Graph-structured data (SINDyG), which incorporates the network structure into sparse regression to identify model parameters that explain the underlying network dynamics. We showcase the application of our proposed method using several case studies of neuronal dynamics, where we model the macroscopic oscillation of a population of neurons using the extended Stuart-Landau (SL) equation and utilize the SINDyG method to identify the underlying nonlinear dynamics. Our extensive computational experiments validate the improved accuracy and simplicity of discovered network dynamics when compared to the original SINDy approach.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A finite element solver for a thermodynamically consistent electrolyte model</title>
<link>https://arxiv.org/abs/2505.16296</link>
<guid>https://arxiv.org/abs/2505.16296</guid>
<content:encoded><![CDATA[
<div> finite element solver, electrolyte model, multicomponent ionic transport, non-equilibrium thermodynamics, FEniCSx platform
Summary:<br /><br />In this study, a finite element solver for a thermodynamically consistent electrolyte model is presented. The model accurately captures multicomponent ionic transport, incorporating steric effects, solvation, and pressure coupling. Rooted in non-equilibrium thermodynamics, the model enforces mass conservation, charge neutrality, and entropy production. It surpasses classical frameworks by using modified partial mass balances, the electrostatic Poisson equation, and a momentum balance with electrostatic potential, atomic fractions, and pressure. The solver, implemented in FEniCSx, handles one- and two-dimensional problems efficiently with various boundary conditions. It demonstrates excellent convergence behavior and robustness, validated against benchmark problems. Simulations showcase critical electrolyte phenomena such as electric double layer formation, rectification behavior, and the impacts of solvation number, Debye length, and compressibility. The modular variational formulation allows extension to complex electrochemical systems with multiple ionic species of asymmetric valences.<br /><br /> <div>
arXiv:2505.16296v1 Announce Type: new 
Abstract: In this study, we present a finite element solver for a thermodynamically consistent electrolyte model that accurately captures multicomponent ionic transport by incorporating key physical phenomena such as steric effects, solvation, and pressure coupling. The model is rooted in the principles of non-equilibrium thermodynamics and strictly enforces mass conservation, charge neutrality, and entropy production. It extends beyond classical frameworks like the Nernst-Planck system by employing modified partial mass balances, the electrostatic Poisson equation, and a momentum balance expressed in terms of electrostatic potential, atomic fractions, and pressure, thereby enhancing numerical stability and physical consistency. Implemented using the FEniCSx platform, the solver efficiently handles one- and two-dimensional problems with varied boundary conditions and demonstrates excellent convergence behavior and robustness. Validation against benchmark problems confirms its improved physical fidelity, particularly in regimes characterized by high ionic concentrations and strong electrochemical gradients. Simulation results reveal critical electrolyte phenomena, including electric double layer formation, rectification behavior, and the effects of solvation number, Debye length, and compressibility. The solver's modular variational formulation facilitates its extension to complex electrochemical systems involving multiple ionic species with asymmetric valences.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Local Patterns to Global Understanding: Cross-Stock Trend Integration for Enhanced Predictive Modeling</title>
<link>https://arxiv.org/abs/2505.16573</link>
<guid>https://arxiv.org/abs/2505.16573</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Stock Price Prediction, Cross-Stock Trend Integration, Collaborative Learning, Data Privacy<br />
<br />
Summary: 
The article introduces a novel approach, Cross-Stock Trend Integration (CSTI), for stock price prediction by merging local stock patterns into a global model. Inspired by Federated Learning (FL), this method allows collaborative learning across distributed datasets without sharing raw data, maintaining data privacy. Individual stock models are trained separately and then merged to create a unified global model, which is fine-tuned on specific stock data for local relevance. CSTI enables parallel training, optimizing computational resources and reducing training time. Extensive experiments show that CSTI outperforms benchmark models and enhances predictive capabilities of existing approaches, providing a robust alternative to single-stock learning methodologies. <div>
arXiv:2505.16573v1 Announce Type: new 
Abstract: Stock price prediction is a critical area of financial forecasting, traditionally approached by training models using the historical price data of individual stocks. While these models effectively capture single-stock patterns, they fail to leverage potential correlations among stock trends, which could improve predictive performance. Current single-stock learning methods are thus limited in their ability to provide a broader understanding of price dynamics across multiple stocks. To address this, we propose a novel method that merges local patterns into a global understanding through cross-stock pattern integration. Our strategy is inspired by Federated Learning (FL), a paradigm designed for decentralized model training. FL enables collaborative learning across distributed datasets without sharing raw data, facilitating the aggregation of global insights while preserving data privacy. In our adaptation, we train models on individual stock data and iteratively merge them to create a unified global model. This global model is subsequently fine-tuned on specific stock data to retain local relevance. The proposed strategy enables parallel training of individual stock models, facilitating efficient utilization of computational resources and reducing overall training time. We conducted extensive experiments to evaluate the proposed method, demonstrating that it outperforms benchmark models and enhances the predictive capabilities of state-of-the-art approaches. Our results highlight the efficacy of Cross-Stock Trend Integration (CSTI) in advancing stock price prediction, offering a robust alternative to traditional single-stock learning methodologies.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Integration Strategies for ESD Protection and Termination in High-Speed LVDS Systems</title>
<link>https://arxiv.org/abs/2505.16200</link>
<guid>https://arxiv.org/abs/2505.16200</guid>
<content:encoded><![CDATA[
<div> Keywords: Electrostatic Discharge (ESD), protection diodes, termination resistors, Low Voltage Differential Signaling (LVDS), signal integrity maintenance<br />
Summary:<br />
This technical article delves into the integration strategies for ESD protection diodes and termination resistors in LVDS designs. It covers critical aspects such as protection mechanisms, design considerations, impedance matching, and placement optimization techniques. The article emphasizes the significance of maintaining signal integrity and ensuring protection effectiveness in LVDS systems. It provides detailed analyses of layout considerations and advanced design strategies to address common integration challenges. The importance of balancing protection requirements with signal integrity demands is highlighted, with practical guidelines offered for implementing robust high-speed digital systems. The article discusses various methodologies for optimizing performance and validating designs, offering designers a comprehensive framework for creating reliable LVDS systems. <div>
arXiv:2505.16200v1 Announce Type: cross 
Abstract: This technical article explores comprehensive strategies for integrating Electrostatic Discharge (ESD) protection diodes and termination resistors in LowVoltage Differential Signaling (LVDS) designs. The article examines critical aspects of protection mechanisms, design considerations, impedance matching, and placement optimization techniques. Through detailed analysis of layout considerations and advanced design strategies, the article presents solutions for common integration challenges. It emphasizes the importance of signal integrity maintenance and protection effectiveness while providing practical guidelines for implementing robust LVDS systems. Various methodologies for performance optimization and validation are discussed, offering designers a thorough framework for creating reliable high-speed digital systems that balance protection requirements with signal integrity demands.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGLDBench: A Benchmark Suite for Stress-Guided Lightweight 3D Designs</title>
<link>https://arxiv.org/abs/2501.03068</link>
<guid>https://arxiv.org/abs/2501.03068</guid>
<content:encoded><![CDATA[
<div> benchmark, material layout strategies, stiff, lightweight designs, 3D domains, stress-guided<br />
<br />
Summary: 
The Stress-Guided Lightweight Design Benchmark (SGLDBench) is introduced as a benchmark suite for assessing material layout strategies in 3D domains to create stiff, lightweight designs. It includes six reference strategies and a multigrid elasticity solver for efficient execution and stiffness validation. The benchmark enables systematic comparison of design strategies based on mechanical properties, supports diverse load conditions, and offers high-resolution designs and stiffness analysis. Visual analysis is emphasized to understand the relationship between design geometry and stress distribution, providing insights into design strategy properties and behaviors. The benchmark's features are showcased through experiments comparing reference strategy results in terms of geometric and mechanical properties. <div>
arXiv:2501.03068v2 Announce Type: replace 
Abstract: We introduce the Stress-Guided Lightweight Design Benchmark (SGLDBench), a comprehensive benchmark suite for applying and evaluating material layout strategies to generate stiff, lightweight designs in 3D domains. SGLDBench provides a seamlessly integrated simulation and analysis framework, including six reference strategies and a scalable multigrid elasticity solver to efficiently execute these strategies and validate the stiffness of their results. This facilitates the systematic analysis and comparison of design strategies based on the mechanical properties they achieve. SGLDBench enables the evaluation of diverse load conditions and, through the tight integration of the solver, supports high-resolution designs and stiffness analysis. Additionally, SGLDBench emphasizes visual analysis to explore the relationship between the geometric structure of a design and the distribution of stresses, offering insights into the specific properties and behaviors of different design strategies. SGLDBench's specific features are highlighted through several experiments, comparing the results of reference strategies with respect to geometric and mechanical properties.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stochastic Dynamic Network Model of the Space Environment</title>
<link>https://arxiv.org/abs/2411.03173</link>
<guid>https://arxiv.org/abs/2411.03173</guid>
<content:encoded><![CDATA[
<div> network model, space environment, stochastic dynamics, species, collision avoidance

Summary:
The article proposes a network model to study the space environment as a dynamic system with different species of objects represented as nodes connected by stochastic links. Stochastic dynamic equations are derived to describe the evolution of the network, replicating existing results on the space environment's evolution. The analysis of the network structure identifies critical species and orbit regimes that impact the environment the most. The concept of carrying capacity in space is introduced based on the stability of network equilibria. Using current object populations and launch traffic forecasts, the model demonstrates how different policies can affect collision avoidance and post-mission disposal maneuvers. The proposed network model provides a framework for understanding and managing the space environment effectively. 

<br /><br />Summary: <div>
arXiv:2411.03173v2 Announce Type: replace-cross 
Abstract: This work proposes to model the space environment as a stochastic dynamic network where each node is a group of objects of a given class, or species, and their relationship is represented by stochastic links. A set of stochastic dynamic equations, governing the evolution of the network, are derived from the network structure and topology. It will be shown that the proposed system of stochastic dynamic equations well reproduces existing results on the evolution of the space environment. The analysis of the structure of the network and relationships among node can help to understand which species of objects and orbit regimes are more critical and affect the most the future evolution of the space environment. In analogy with ecological networks, we develop a theory of the carrying capacity of space based on the stability of equilibria of the network dynamics. Some examples are presented starting from the current population of resident objects and different launch traffic forecast models. It will be shown how the proposed network model can be used to study the effect of the adoption of different policies on the execution of collision avoidance and post mission disposal manoeuvres.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deployment of Traditional and Hybrid Machine Learning for Critical Heat Flux Prediction in the CTF Thermal Hydraulics Code</title>
<link>https://arxiv.org/abs/2505.14701</link>
<guid>https://arxiv.org/abs/2505.14701</guid>
<content:encoded><![CDATA[
<div> machine learning, critical heat flux, hybrid models, subchannel code, nuclear reactors

Summary:
- The study focuses on predicting critical heat flux (CHF) using machine learning (ML) models, crucial for safety in nuclear reactors.
- Traditional empirical correlations for CHF prediction often show discrepancies, prompting the need for more reliable methods.
- Hybrid models, combining data-driven ML with physics-based models, show improved accuracy over conventional methods.
- Integration of ML-based CHF models into subchannel codes proves effective in enhancing prediction performance.
- The study's results demonstrate that ML-based models, integrated with hybrid approaches, can reduce CHF overprediction and enhance overall accuracy, highlighting their potential in improving operational efficiency and safety in nuclear reactor systems.<br /><br />Summary: <div>
arXiv:2505.14701v1 Announce Type: new 
Abstract: Critical heat flux (CHF) marks the transition from nucleate to film boiling, where heat transfer to the working fluid can rapidly deteriorate. Accurate CHF prediction is essential for efficiency, safety, and preventing equipment damage, particularly in nuclear reactors. Although widely used, empirical correlations frequently exhibit discrepancies in comparison with experimental data, limiting their reliability in diverse operational conditions. Traditional machine learning (ML) approaches have demonstrated the potential for CHF prediction but have often suffered from limited interpretability, data scarcity, and insufficient knowledge of physical principles. Hybrid model approaches, which combine data-driven ML with physics-based models, mitigate these concerns by incorporating prior knowledge of the domain. This study integrated a purely data-driven ML model and two hybrid models (using the Biasi and Bowring CHF correlations) within the CTF subchannel code via a custom Fortran framework. Performance was evaluated using two validation cases: a subset of the Nuclear Regulatory Commission CHF database and the Bennett dryout experiments. In both cases, the hybrid models exhibited significantly lower error metrics in comparison with conventional empirical correlations. The pure ML model remained competitive with the hybrid models. Trend analysis of error parity indicates that ML-based models reduce the tendency for CHF overprediction, improving overall accuracy. These results demonstrate that ML-based CHF models can be effectively integrated into subchannel codes and can potentially increase performance in comparison with conventional methods.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local-Global Associative Frames for Symmetry-Preserving Crystal Structure Modeling</title>
<link>https://arxiv.org/abs/2505.15315</link>
<guid>https://arxiv.org/abs/2505.15315</guid>
<content:encoded><![CDATA[
<div> Keywords: crystal structures, invariance, symmetry, frames, crystal property prediction

Summary:
In the study of crystal structures, maintaining invariance to rotational transformations is essential for accurate property prediction. Traditional approaches using global or local frames have limitations in capturing both local structure heterogeneity and preserving crystal symmetry. To address this, the proposed Symmetry-Preserving Frames (SPFrame) method constructs invariant local frames while integrating global structural information to enforce invariance to SO(3) rotations. The SPFrame approach outperforms existing techniques and baselines in crystal property prediction tasks. By combining local and global frame information, SPFrame achieves superior performance in capturing the complexity of crystal structures while maintaining symmetry, demonstrating its efficacy in improving predictive accuracy for various crystal properties. 

<br /><br />Summary: <div>
arXiv:2505.15315v1 Announce Type: new 
Abstract: Crystal structures are defined by the periodic arrangement of atoms in 3D space, inherently making them equivariant to SO(3) group. A fundamental requirement for crystal property prediction is that the model's output should remain invariant to arbitrary rotational transformations of the input structure. One promising strategy to achieve this invariance is to align the given crystal structure into a canonical orientation with appropriately computed rotations, or called frames. However, existing work either only considers a global frame or solely relies on more advanced local frames based on atoms' local structure. A global frame is too coarse to capture the local structure heterogeneity of the crystal, while local frames may inadvertently disrupt crystal symmetry, limiting their expressivity. In this work, we revisit the frame design problem for crystalline materials and propose a novel approach to construct expressive Symmetry-Preserving Frames, dubbed as SPFrame, for modeling crystal structures. Specifically, this local-global associative frame constructs invariant local frames rather than equivariant ones, thereby preserving the symmetry of the crystal. In parallel, it integrates global structural information to construct an equivariant global frame to enforce SO(3) invariance. Extensive experimental results demonstrate that SPFrame consistently outperforms traditional frame construction techniques and existing crystal property prediction baselines across multiple benchmark tasks.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Open Earth Science as Fast and Accessible as Natural Language</title>
<link>https://arxiv.org/abs/2505.15690</link>
<guid>https://arxiv.org/abs/2505.15690</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural language processing, Earth observation, Large Language Models, Data analysis, Software framework

Summary: 
This study explores the feasibility of using Large Language Models (LLMs) for natural-language-driven earth observation data analysis. The research focuses on achieving high accuracy, interactive latencies, low costs, and open source software. Techniques such as model scaling, prompt optimization, and inference-time scaling optimization are employed to achieve near 100% accuracy across multiple metrics. The analysis also considers cost, latency, and maintainability of the techniques. Opportunities for further research, framework development, and ongoing work towards a comprehensive solution are identified. Collaboration and contributions are encouraged for the advancement of this field.

<br /><br />Summary: <div>
arXiv:2505.15690v1 Announce Type: new 
Abstract: Is natural-language-driven earth observation data analysis now feasible with the assistance of Large Language Models (LLMs)? For open science in service of public interest, feasibility requires reliably high accuracy, interactive latencies, low (sustainable) costs, open LLMs, and openly maintainable software -- hence, the challenge. What are the techniques and programming system requirements necessary for satisfying these constraints, and what is the corresponding development and maintenance burden in practice? This study lays the groundwork for exploring these questions, introducing an impactful earth science use-case, and providing a software framework with evaluation data and metrics, along with initial results from employing model scaling, prompt-optimization, and inference-time scaling optimization techniques. While we attain high accuracy (near 100%) across 10 of 11 metrics, the analysis further considers cost (token-spend), latency, and maintainability across this space of techniques. Finally, we enumerate opportunities for further research, general programming and evaluation framework development, and ongoing work for a comprehensive, deployable solution. This is a call for collaboration and contribution.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
<link>https://arxiv.org/abs/2505.15155</link>
<guid>https://arxiv.org/abs/2505.15155</guid>
<content:encoded><![CDATA[
<div> Framework, Quantitative Finance, Automation, Factor-model, Co-optimization
Summary: 
RD-Agent(Q) is a data-centric multi-agent framework designed to automate the research and development of quantitative strategies in financial markets. It breaks down the quant process into two stages: Research and Development, connected through a feedback loop with a multi-armed bandit scheduler for direction selection. The framework sets goal-aligned prompts, formulates hypotheses, and maps them to tasks in the Research stage, while using a code-generation agent, Co-STEER, to implement task-specific code in the Development stage. Empirical results show RD-Agent(Q) achieving higher annualized returns than classical factor libraries with fewer factors, and outperforming deep time-series models on real markets. The joint factor-model optimization provides a balance between predictive accuracy and strategy robustness. The code for RD-Agent(Q) is available on GitHub at: https://github.com/microsoft/RD-Agent.<br /><br />Summary: <div>
arXiv:2505.15155v1 Announce Type: cross 
Abstract: Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility. Despite advances in large language models and multi-agent systems, current quantitative research pipelines suffer from limited automation, weak interpretability, and fragmented coordination across key components such as factor mining and model innovation. In this paper, we propose R&amp;D-Agent for Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization. RD-Agent(Q) decomposes the quant process into two iterative stages: a Research stage that dynamically sets goal-aligned prompts, formulates hypotheses based on domain priors, and maps them to concrete tasks, and a Development stage that employs a code-generation agent, Co-STEER, to implement task-specific code, which is then executed in real-market backtests. The two stages are connected through a feedback stage that thoroughly evaluates experimental outcomes and informs subsequent iterations, with a multi-armed bandit scheduler for adaptive direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher annualized returns than classical factor libraries using 70% fewer factors, and outperforms state-of-the-art deep time-series models on real markets. Its joint factor-model optimization delivers a strong balance between predictive accuracy and strategy robustness. Our code is available at: https://github.com/microsoft/RD-Agent.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2505.15228</link>
<guid>https://arxiv.org/abs/2505.15228</guid>
<content:encoded><![CDATA[
<div> Keywords: CP-KAN, neural architecture, Chebyshev polynomial, QUBO, regression tasks <br />
Summary: <br />
- This article introduces the cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a neural architecture that combines Chebyshev polynomial basis functions and quadratic unconstrained binary optimization (QUBO).
- The primary contribution is a reformulation of the degree selection problem as a QUBO task, which significantly reduces complexity and allows for efficient degree selection across neurons.
- CP-KAN performs well in regression tasks with limited data, showing robustness to input scales and natural regularization properties from its polynomial basis.
- The architecture is theoretically linked to properties of financial time series, demonstrating its potential for analyzing financial data.
- Empirical validation across different domains shows that CP-KAN performs competitively compared to traditional architectures, particularly in scenarios where data efficiency and numerical stability are crucial. <div>
arXiv:2505.15228v1 Announce Type: cross 
Abstract: We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a neural architecture combining Chebyshev polynomial basis functions and quadratic unconstrained binary optimization (QUBO). Our primary contribution involves reformulating the degree selection problem as a QUBO task, reducing the complexity from $O(D^N)$ to a single optimization step per layer. This approach enables efficient degree selection across neurons while maintaining computational tractability. The architecture performs well in regression tasks with limited data, showing good robustness to input scales and natural regularization properties from its polynomial basis. Additionally, theoretical analysis establishes connections between CP-KAN's performance and properties of financial time series. Our empirical validation across multiple domains demonstrates competitive performance compared to several traditional architectures tested, especially in scenarios where data efficiency and numerical stability are important. Our implementation, including strategies for managing computational overhead in larger networks is available in Ref.~\citep{cpkan_implementation}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization of Probability Distributions via Divide-and-Conquer: Convergence and Error Propagation under Distributional Arithmetic Operations</title>
<link>https://arxiv.org/abs/2505.15283</link>
<guid>https://arxiv.org/abs/2505.15283</guid>
<content:encoded><![CDATA[
<div> algorithm, distribution, approximation, stability, convergence
<br />
Summary: 
This article examines a divide-and-conquer algorithm for approximating continuous one-dimensional probability distributions with finite mean. It conducts a numerical comparison with existing schemes, emphasizing the stability of discrete approximations during arithmetic operations. The study establishes an upper bound for the approximation error based on the Wasserstein-1 distance, applicable to all continuous distributions with finite mean. The algorithm often achieves optimal convergence rates, and numerical tests demonstrate its superior stability compared to other methods when subjected to arithmetic operations. <div>
arXiv:2505.15283v1 Announce Type: cross 
Abstract: This article studies a general divide-and-conquer algorithm for approximating continuous one-dimensional probability distributions with finite mean. The article presents a numerical study that compares pre-existing approximation schemes with a special focus on the stability of the discrete approximations when they undergo arithmetic operations. The main results are a simple upper bound of the approximation error in terms of the Wasserstein-1 distance that is valid for all continuous distributions with finite mean. In many use-cases, the studied method achieve optimal rate of convergence, and numerical experiments show that the algorithm is more stable than pre-existing approximation schemes in the context of arithmetic operations.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elasto-acoustic wave propagation in geophysical media using hybrid high-order methods on general meshes</title>
<link>https://arxiv.org/abs/2505.15771</link>
<guid>https://arxiv.org/abs/2505.15771</guid>
<content:encoded><![CDATA[
<div> Keywords: HHO methods, elasto-acoustic waves, explicit schemes, implicit schemes, numerical simulations
<br />
Summary:
Hybrid high-order (HHO) methods are investigated for the space semi-discretization of coupled elasto-acoustic waves using explicit and implicit Runge-Kutta schemes for time discretization. The implementation of these schemes requires static condensation of face and cell unknowns with block-diagonal matrices. CFL stability limits of explicit schemes are estimated, and a comparison between explicit and implicit schemes shows the competitiveness of implicit schemes in various scenarios. Simulations in a 2D geophysical model demonstrate the geometrical flexibility of the HHO method in handling hybrid and nonconforming meshes, yielding results comparable to spectral element software. Overall, the study highlights the effectiveness of HHO methods in accurately simulating coupled elasto-acoustic waves with efficient time discretization strategies. 
<br /> <div>
arXiv:2505.15771v1 Announce Type: cross 
Abstract: Hybrid high-order (HHO) methods are numerical methods characterized by several interesting properties such as local conservativity, geometric flexibility and high-order accuracy. Here, HHO schemes are studied for the space semi-discretization of coupled elasto-acoustic waves in the time domain using a first-order formulation. Explicit and singly diagonal implicit Runge--Kutta (ERK & SDIRK) schemes are used for the time discretization. We show that an efficient implementation of explicit (resp. implicit) time schemes calls for a static condensation of the face (resp. cell) unknowns. Crucially, both static condensation procedures only involve block-diagonal matrices. Then, we provide numerical estimates for the CFL stability limit of ERK schemes and present a comparative study on the efficiency of explicit versus implicit schemes. Our findings indicate that implicit time schemes remain competitive in many situations. Finally, simulations in a 2D realistic geophysical configuration are performed, illustrating the geometrical flexibility of the HHO method: both hybrid (triangular and quadrangular) and nonconforming (with hanging nodes) meshes are easily handled, delivering results of comparable accuracy to a reference spectral element software based on tensorized elements.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Prediction-Assisted Safe Reinforcement Learning for Electric Vehicle Charging Station Recommendation in Dynamically Coupled Transportation-Power Systems</title>
<link>https://arxiv.org/abs/2407.20679</link>
<guid>https://arxiv.org/abs/2407.20679</guid>
<content:encoded><![CDATA[
<div> Recommendation, Electric Vehicles, Transportation-Power Systems, Reinforcement Learning, Constrained Markov Decision Process

Summary:
- The study tackles the en-route charging station recommendation problem for electric vehicles in dynamically coupled transportation-power systems.
- The objective is to maximize overall traffic efficiency while ensuring power grid safety, a novel approach in existing literature.
- The problem is formulated as a constrained Markov decision process (CMDP) and addressed using an online prediction-assisted safe reinforcement learning (OP-SRL) method.
- Challenges of constrained optimization and uncertain delays are overcome through innovative approaches such as Lagrangian method and online sequence-to-sequence predictor.
- Comprehensive experimental studies demonstrate the superior performance of the proposed method in road network efficiency, power grid safety, and EV user satisfaction.
<br /><br />Summary: <div>
arXiv:2407.20679v2 Announce Type: replace 
Abstract: With the proliferation of electric vehicles (EVs), the transportation network and power grid become increasingly interdependent and coupled via charging stations. The concomitant growth in charging demand has posed challenges for both networks, highlighting the importance of charging coordination. Existing literature largely overlooks the interactions between power grid security and traffic efficiency. In view of this, we study the en-route charging station (CS) recommendation problem for EVs in dynamically coupled transportation-power systems. The system-level objective is to maximize the overall traffic efficiency while ensuring the safety of the power grid. This problem is for the first time formulated as a constrained Markov decision process (CMDP), and an online prediction-assisted safe reinforcement learning (OP-SRL) method is proposed to learn the optimal and secure policy by extending the PPO method. To be specific, we mainly address two challenges. First, the constrained optimization problem is converted into an equivalent unconstrained optimization problem by applying the Lagrangian method. Second, to account for the uncertain long-time delay between performing CS recommendation and commencing charging, we put forward an online sequence-to-sequence (Seq2Seq) predictor for state augmentation to guide the agent in making forward-thinking decisions. Finally, we conduct comprehensive experimental studies based on the Nguyen-Dupuis network and a large-scale real-world road network, coupled with IEEE 33-bus and IEEE 69-bus distribution systems, respectively. Results demonstrate that the proposed method outperforms baselines in terms of road network efficiency, power grid safety, and EV user satisfaction. The case study on the real-world network also illustrates the applicability in the practical context.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Method for Satellite Pattern-of-Life Identification</title>
<link>https://arxiv.org/abs/2412.10814</link>
<guid>https://arxiv.org/abs/2412.10814</guid>
<content:encoded><![CDATA[
<div> machine learning, satellite behavior, pattern-of-life identification, diffusion model, data sampling rates

Summary:
The article introduces a novel diffusion-based method for satellite pattern-of-life (PoL) identification, which overcomes limitations of existing approaches by eliminating the need for manual refinement or domain-specific knowledge. The method combines a multivariate time-series encoder with a diffusion model to capture hidden representations of satellite positional data and generate PoL labels. It achieves high identification quality and robustness even with varying data sampling rates, demonstrating potential for practical satellite behavior pattern identification and mission deployment. This innovative approach outperforms traditional methods and addresses challenges in analyzing satellite behaviors, offering a promising solution for space safety and satellite monitoring. The Expert-ML method, previously developed, is also discussed as a domain expertise-informed machine learning approach for PoL identification. <div>
arXiv:2412.10814v2 Announce Type: replace-cross 
Abstract: Satellite pattern-of-life (PoL) identification is crucial for space safety and satellite monitoring, involving the analysis of typical satellite behaviors such as station-keeping, drift, etc. However, existing PoL identification methods remain underdeveloped due to the complexity of aerospace systems, variability in satellite behaviors, and fluctuating observation sampling rates. In a first attempt, we developed a domain expertise-informed machine learning method (Expert-ML) to combine satellite orbital movement knowledge and machine learning models. The Expert-ML method achieved high accuracy results in simulation data and real-world data with normal sampling rate. However, this approach lacks of generality as it requires domain expertise and its performance degraded significantly when data sampling rate varied. To achieve generality, we propose a novel diffusion-based PoL identification method. Distinct from prior approaches, the proposed method leverages a diffusion model to achieve end-to-end identification without manual refinement or domain-specific knowledge. Specifically, we employ a multivariate time-series encoder to capture hidden representations of satellite positional data. The encoded features are subsequently incorporated as conditional information in the denoising process to generate PoL labels. Through experimentation across real-world satellite settings, our proposed diffusion-based method demonstrates its high identification quality and provides a robust solution even with reduced data sampling rates, indicating its great potential in practical satellite behavior pattern identification, tracking and related mission deployment.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Dynamical Systems across Environments via Diffusive Model Weight Generation</title>
<link>https://arxiv.org/abs/2505.13919</link>
<guid>https://arxiv.org/abs/2505.13919</guid>
<content:encoded><![CDATA[
<div> Data-driven methods, dynamic behaviors, cross-environment prediction, model weight generation, \texttt{EnvAd-Diff}<br />
<br />
Summary: 
The study focuses on using data-driven methods to predict physical dynamics in different environments. It addresses the issue of prediction functions failing when transferred to unseen environments by proposing a model weight generation method called \texttt{EnvAd-Diff}. The method operates in the weight space of the dynamic function to generate suitable weights based on environmental conditions for zero-shot prediction. By training expert prediction functions on dynamic trajectories from visible environments, a model zoo is created to construct sample pairs of prediction function weights and their corresponding environments. A latent space diffusion model conditioned on the environment is then trained to model the joint distribution of weights and environments. The study also introduces a physics-informed surrogate label to distinguish different environments. Generalization experiments across multiple systems show that the 1M parameter prediction function generated by \texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model. <div>
arXiv:2505.13919v1 Announce Type: new 
Abstract: Data-driven methods offer an effective equation-free solution for predicting physical dynamics. However, the same physical system can exhibit significantly different dynamic behaviors in various environments. This causes prediction functions trained for specific environments to fail when transferred to unseen environments. Therefore, cross-environment prediction requires modeling the dynamic functions of different environments. In this work, we propose a model weight generation method, \texttt{EnvAd-Diff}. \texttt{EnvAd-Diff} operates in the weight space of the dynamic function, generating suitable weights from scratch based on environmental condition for zero-shot prediction. Specifically, we first train expert prediction functions on dynamic trajectories from a limited set of visible environments to create a model zoo, thereby constructing sample pairs of prediction function weights and their corresponding environments. Subsequently, we train a latent space diffusion model conditioned on the environment to model the joint distribution of weights and environments. Considering the lack of environmental prior knowledge in real-world scenarios, we propose a physics-informed surrogate label to distinguish different environments. Generalization experiments across multiple systems demonstrate that a 1M parameter prediction function generated by \texttt{EnvAd-Diff} outperforms a pre-trained 500M parameter foundation model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLUTUS Open Source -- Breaking Barriers in Algorithmic Trading</title>
<link>https://arxiv.org/abs/2505.14050</link>
<guid>https://arxiv.org/abs/2505.14050</guid>
<content:encoded><![CDATA[
<div> Keywords: Algorithmic trading, reproducibility, standardization, collaboration, PLUTUS

Summary: 
Algorithmic trading has traditionally been a secretive and fragmented domain, lacking transparency, standardization, and collaboration. The PLUTUS Open Source initiative, sponsored by ALGOTRADE, aims to reshape this landscape by promoting openness, structure, and collaboration within the algorithmic trading ecosystem. PLUTUS introduces a reproducibility standard, a modular development framework, and a suite of community-built reference strategies, providing a systematic approach to designing, testing, and documenting trading algorithms for users regardless of their technical or financial background. The initiative invites contributions from the research and trading communities to build a transparent and inclusive future for algorithmic trading. The foundational structure of PLUTUS is outlined, along with working examples that adhere to the PLUTUS standard. By promoting reproducibility, standardization, and collaboration, PLUTUS aims to drive a positive transformation in the algorithmic trading industry. 

Summary: <br /><br />PLUTUS Open Source aims to reshape the algorithmic trading ecosystem by promoting openness, structure, and collaboration. The initiative introduces a reproducibility standard, a modular development framework, and a suite of community-built reference strategies. PLUTUS provides a systematic approach to designing, testing, and documenting trading algorithms, accessible to users with varying technical and financial backgrounds. By inviting contributions from the research and trading communities, PLUTUS aims to build a transparent and inclusive future for algorithmic trading. The initiative's foundational structure and working examples adhering to the PLUTUS standard are presented as a foundation for driving positive change in the industry. <div>
arXiv:2505.14050v1 Announce Type: new 
Abstract: Algorithmic trading has long been an opaque, fragmented domain, guarded by secrecy and built around proprietary systems. In contrast to the open, collaborative evolution in fields like machine learning or software engineering, the algorithmic trading ecosystem has been slow to adopt reproducibility, standardization, and shared infrastructure. This paper introduces PLUTUS Open Source, an initiative sponsored by ALGOTRADE to reshape this landscape through openness, structure, and collaboration. PLUTUS combines a reproducibility standard, a modular development framework, and a growing suite of community-built reference strategies. The project provides a systematic approach to designing, testing, and documenting trading algorithms, regardless of the user's technical or financial background. We outline the motivation behind the initiative, present its foundational structure, and showcase working examples that adhere to the PLUTUS standard. We also invite the broader research and trading communities to contribute, iterate, and help build a transparent and inclusive future for algorithmic trading.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-order, mixed-hybrid finite elements for Kirchhoff-Love shells</title>
<link>https://arxiv.org/abs/2505.14115</link>
<guid>https://arxiv.org/abs/2505.14115</guid>
<content:encoded><![CDATA[
<div> mixed-hybrid method, Kirchhoff-Love shells, Lagrange elements, hybridization, numerical analyses <br />
Summary: 
A novel mixed-hybrid method for Kirchhoff-Love shells is introduced, allowing the use of classical Lagrange elements in numerical analyses. Unlike displacement-based formulations, this method features displacements and moments as primary unknowns, reducing continuity requirements and enabling equal-order interpolations. Hybridization simplifies static condensation while introducing rotational degrees of freedom as Lagrange multipliers. The formulation, based on Tangential Differential Calculus, is applicable for explicit and implicit shell geometries. Mechanical boundary conditions are considered, and numerical results show optimal convergence rates for smooth solutions. New benchmark test cases are proposed to assess the method's effectiveness. <div>
arXiv:2505.14115v1 Announce Type: new 
Abstract: A novel mixed-hybrid method for Kirchhoff-Love shells is proposed that enables the use of classical, possibly higher-order Lagrange elements in numerical analyses. In contrast to purely displacement-based formulations that require higher continuity of shape functions as in IGA, the mixed formulation features displacements and moments as primary unknowns. Thereby the continuity requirements are reduced, allowing equal-order interpolations of the displacements and moments. Hybridization enables an element-wise static condensation of the degrees of freedom related to the moments, at the price of introducing (significantly less) rotational degrees of freedom acting as Lagrange multipliers to weakly enforce the continuity of tangential moments along element edges. The mixed model is formulated coordinate-free based on the Tangential Differential Calculus, making it applicable for explicitly and implicitly defined shell geometries. All mechanically relevant boundary conditions are considered. Numerical results confirm optimal higher-order convergence rates whenever the mechanical setup allows for sufficiently smooth solutions; new benchmark test cases of this type are proposed.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Maxwell Tomography Using the Volume-Surface Integral Equation for Improved Estimation of Electrical Properties</title>
<link>https://arxiv.org/abs/2505.14546</link>
<guid>https://arxiv.org/abs/2505.14546</guid>
<content:encoded><![CDATA[
<div> Tomography, Maxwell, Electrical properties, MRI, VSIE<br />
<br />
Summary:
Global Maxwell Tomography (GMT) is a method for estimating electrical properties (EP) from magnetic resonance measurements. A novel version of GMT using the volume-surface integral equation (VSIE) recalculates coil currents based on updated EP estimates, yielding more accurate reconstructions. Simulation and experimental results showed that VSIE-based GMT outperformed the traditional VIE-based method, with improvements of at least 12% in simulations and relative differences of 13-26% in experiments compared to probe-measured values. By accounting for the effect of EP on coil currents, VSIE-based GMT does not rely on an initial EP estimate, making it more suitable for experimental reconstructions. The study highlights the significance of using VSIE for enhancing GMT performance in noninvasive EP estimation. <br /><br /> <div>
arXiv:2505.14546v1 Announce Type: new 
Abstract: Objective: Global Maxwell Tomography (GMT) is a noninvasive inverse optimization method for the estimation of electrical properties (EP) from magnetic resonance (MR) measurements. GMT uses the volume integral equation (VIE) in the forward problem and assumes that the sample has negligible effect on the coil currents. Consequently, GMT calculates the coil's incident fields with an initial EP distribution and keeps them constant for all optimization iterations. This can lead to erroneous reconstructions. This work introduces a novel version of GMT that replaces VIE with the volume-surface integral equation (VSIE), which recalculates the coil currents at every iteration based on updated EP estimates before computing the associated fields. Methods: We simulated an 8-channel transceiver coil array for 7 T brain imaging and reconstructed the EP of a realistic head model using VSIE-based GMT. We built the coil, collected experimental MR measurements, and reconstructed EP of a two-compartment phantom. Results: In simulations, VSIE-based GMT outperformed VIE-based GMT by at least 12% for both EP. In experiments, the relative difference with respect to probe-measured EP values in the inner (outer) compartment was 13% (26%) and 17% (33%) for the permittivity and conductivity, respectively. Conclusion: The use of VSIE over VIE enhances GMT's performance by accounting for the effect of the EP on the coil currents. Significance: VSIE-based GMT does not rely on an initial EP estimate, rendering it more suitable for experimental reconstructions compared to the VIE-based GMT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact</title>
<link>https://arxiv.org/abs/2505.13469</link>
<guid>https://arxiv.org/abs/2505.13469</guid>
<content:encoded><![CDATA[
<div> fair lending, machine learning, algorithmic fairness, profit margins, default rates 
Summary:
- The study investigates the tradeoff between enforcing fairness in lending algorithms and maximizing profitability in financial institutions by using machine learning models.
- Simulations on synthetic data reflecting real-world lending patterns reveal that equal opportunity constraints generally have a lower impact on profit margins compared to demographic parity.
- Surprisingly, removing protected attributes from the model through fairness through unawareness results in improved fairness and profitability metrics.
- Fair lending can be profitable under specific economic conditions, and the study also examines the feature-specific drivers of unfairness in lending algorithms.
- The findings provide practical guidance for designing lending algorithms that balance ethical concerns with business objectives. 
<br /><br />Summary: <div>
arXiv:2505.13469v1 Announce Type: cross 
Abstract: As financial institutions increasingly rely on machine learning models to automate lending decisions, concerns about algorithmic fairness have risen. This paper explores the tradeoff between enforcing fairness constraints (such as demographic parity or equal opportunity) and maximizing lender profitability. Through simulations on synthetic data that reflects real-world lending patterns, we quantify how different fairness interventions impact profit margins and default rates. Our results demonstrate that equal opportunity constraints typically impose lower profit costs than demographic parity, but surprisingly, removing protected attributes from the model (fairness through unawareness) outperforms explicit fairness interventions in both fairness and profitability metrics. We further identify the specific economic conditions under which fair lending becomes profitable and analyze the feature-specific drivers of unfairness. These findings offer practical guidance for designing lending algorithms that balance ethical considerations with business objectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Methods for Model Pruning and Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.14052</link>
<guid>https://arxiv.org/abs/2505.14052</guid>
<content:encoded><![CDATA[
<div> Pruning, Language models, Model optimization, Computational complexity, Knowledge distillation
<br />
Summary:
MAMA Pruning is introduced as an enhanced method for model pruning in large language models like R1 or o3-mini. The technique aims to remove neurons and connections that are unlikely to contribute significantly during human-computer interaction. By analyzing movement and magnitude of weights and biases in the pre-training phase, as well as GRPO rewards in the post-training phase, MAMA Pruning effectively reduces model size and computational complexity. The method maintains performance comparable to unpruned models even at extreme pruning levels. Experimental results demonstrate the superiority of MAMA Pruning over existing methods across various pruning levels and computational linguistics tasks.
<br /><br />Summary: <div>
arXiv:2505.14052v1 Announce Type: cross 
Abstract: Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design and Evaluation of a Microservices Cloud Framework for Online Travel Platforms</title>
<link>https://arxiv.org/abs/2505.14508</link>
<guid>https://arxiv.org/abs/2505.14508</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Travel Agents, Microservices architecture, Cloud computing, Fault Tolerance, Cost-effective analysis<br />
Summary: <br />
The paper discusses the use of Microservices architecture in handling online travel agents globally, highlighting the benefits of flexibility and efficiency in managing large amounts of data. The Microservices Cloud Framework for Online Travel Platforms (MCF-OTP) is designed to improve performance, flexibility, and maintenance by utilizing cloud computing and microservice technologies. The framework aims to enhance fault tolerance and response time, surpassing traditional monolithic designs in scalability and uptime. Cost-effective analysis shows a net gain from startup fees and operational costs. The cloud-based environment helps reduce costs and optimize resource allocation, ultimately increasing efficiency in managing online travel platforms. <div>
arXiv:2505.14508v1 Announce Type: cross 
Abstract: Handling online travel agents globally requires efficient and flexible software solution architectures. When it needs to handle thousands of agents and billions of clients data globally. Microservices architecture is used to break down a large program into numerous, smaller services which can run individually and perform individual tasks. This paper analyses and integrates a unique Microservices Cloud Framework designed to support Online Travel Platforms (MCF-OTP). MCF-OTPs main goal is to increase the performance, flexibility, and maintenance of online travel platforms via cloud computing and microservice technologies. Large-scale travel apps, including managing numerous data sources, dealing with traffic peaks, and providing fault tolerance, can be addressed by the suggested framework. The framework increases good interpretation between flawless data synchronization, microservices, and dynamic scaling based on demand technology. An organization framework that optimizes service borders and minimizes inter-service dependencies is recommended. Thus, this can result in elevated development adaptability. In this research, the principal goal is to evaluate MCF-OTPs efficiency using the indicators of fault tolerance and response time. It is indicated by the findings that the MCF-OTP structure excels traditional monolithic designs in terms of dependability and scalability, managing traffic spikes seamlessly and decreasing downtime. The cost-effective analysis helps ascertain the net gain attained by the startup fees and the ongoing operational costs. The cloud-based environment is used to reduce the fracture cost which also helps to increase the efficiency of resource allocation, according to the research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoMesh: Adaptive Physical Simulation with Hierarchical Graph Evolutions</title>
<link>https://arxiv.org/abs/2410.03779</link>
<guid>https://arxiv.org/abs/2410.03779</guid>
<content:encoded><![CDATA[
<div> Graph neural networks, mesh-based physical simulation, EvoMesh, anisotropic message passing, adaptively guided, physical inputs<br />
<br />
Summary:<br />
EvoMesh is introduced as a fully differentiable framework for mesh-based physical simulation that learns graph hierarchies and physical dynamics together. It utilizes direction-specific aggregation of dynamic features and node selection probabilities for hierarchical levels based on physical context, enabling flexible message shortcuts and long-range dependency capture. Experimental results on various datasets demonstrate EvoMesh's superior performance over fixed-hierarchy message passing networks. The code for EvoMesh is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2410.03779v2 Announce Type: replace-cross 
Abstract: Graph neural networks have been a powerful tool for mesh-based physical simulation. To efficiently model large-scale systems, existing methods mainly employ hierarchical graph structures to capture multi-scale node relations. However, these graph hierarchies are typically manually designed and fixed, limiting their ability to adapt to the evolving dynamics of complex physical systems. We propose EvoMesh, a fully differentiable framework that jointly learns graph hierarchies and physical dynamics, adaptively guided by physical inputs. EvoMesh introduces anisotropic message passing, which enables direction-specific aggregation of dynamic features between nodes within each hierarchy, while simultaneously learning node selection probabilities for the next hierarchical level based on physical context. This design creates more flexible message shortcuts and enhances the model's capacity to capture long-range dependencies. Extensive experiments on five benchmark physical simulation datasets show that EvoMesh outperforms recent fixed-hierarchy message passing networks by large margins. Code is available at https://github.com/hbell99/EvoMesh.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries</title>
<link>https://arxiv.org/abs/2410.19236</link>
<guid>https://arxiv.org/abs/2410.19236</guid>
<content:encoded><![CDATA[
<div> algorithm, SHAP zero, Shapley values, biological sequences, interpretability

Summary:
SHAP zero is a novel algorithm that addresses the need for interpretable predictions in machine learning models for biological sequences. By amortizing the computational cost of Shapley value computation across large-scale datasets, SHAP zero enables near-zero marginal cost for future queries. It leverages the sparse Fourier transform of the model to uncover high-order feature interactions and efficiently explain predictions in models of guide RNA efficacy, DNA repair outcomes, and protein fitness. The algorithm significantly accelerates the process of extracting global biological insights and reveals rich combinatorial interactions that were previously inaccessible at scale. This work paves the way for scalable and efficient interpretability of black-box sequence models in biology.<br /><br />Summary: <div>
arXiv:2410.19236v3 Announce Type: replace-cross 
Abstract: The growing adoption of machine learning models for biological sequences has intensified the need for interpretable predictions, with Shapley values emerging as a theoretically grounded standard for model explanation. While effective for local explanations of individual input sequences, scaling Shapley-based interpretability to extract global biological insights requires evaluating thousands of sequences--incurring exponential computational cost per query. We introduce SHAP zero, a novel algorithm that amortizes the cost of Shapley value computation across large-scale biological datasets. After a one-time model sketching step, SHAP zero enables near-zero marginal cost for future queries by uncovering an underexplored connection between Shapley values, high-order feature interactions, and the sparse Fourier transform of the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and protein fitness, SHAP zero explains predictions orders of magnitude faster than existing methods, recovering rich combinatorial interactions previously inaccessible at scale. This work opens the door to principled, efficient, and scalable interpretability for black-box sequence models in biology.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced Feature Engineering for Multi-Factor Electricity Price Predictions</title>
<link>https://arxiv.org/abs/2505.11890</link>
<guid>https://arxiv.org/abs/2505.11890</guid>
<content:encoded><![CDATA[
<div> forecasting, electricity price, volatility, New South Wales, machine learning

Summary:<br /><br />
- Traditional forecasting models struggle in capturing the complex dynamics of electricity markets, leading to unreliable predictions in volatile markets like New South Wales.
- FAEP, a Feature-Augmented Electricity Price Prediction framework, addresses these challenges by leveraging Large Language Models (LLMs) and advanced feature engineering.
- By incorporating external features such as weather data and price volatility jumps, FAEP enhances prediction accuracy.
- Utilizing Retrieval-Augmented Generation (RAG) for feature extraction and a hybrid XGBoost-LSTM model, FAEP outperforms other models in the NSW electricity market.
- Experimental results demonstrate the state-of-the-art performance of FAEP in electricity price prediction, showcasing the efficiency of LLM-enhanced feature engineering and hybrid machine learning architectures. 

<br /><br />Summary: Hello, please summarize this article in en language, first extract 5 keywords, output in the same line, then line break, write a summary containing all the points in 200 words in en, output in order by points, and output in the following format '<br /><br />Summary:' , <br /> is the line break of HTML, 2 must be retained when output, and must be before the word 'Summary:' <div>
arXiv:2505.11890v1 Announce Type: new 
Abstract: Accurately forecasting electricity price volatility is crucial for effective risk management and decision-making. Traditional forecasting models often fall short in capturing the complex, non-linear dynamics of electricity markets, particularly when external factors like weather conditions and market volatility are involved. These limitations hinder their ability to provide reliable predictions in markets with high volatility, such as the New South Wales (NSW) electricity market. To address these challenges, we introduce FAEP, a Feature-Augmented Electricity Price Prediction framework. FAEP leverages Large Language Models (LLMs) combined with advanced feature engineering to enhance prediction accuracy. By incorporating external features such as weather data and price volatility jumps, and utilizing Retrieval-Augmented Generation (RAG) for effective feature extraction, FAEP overcomes the shortcomings of traditional approaches. A hybrid XGBoost-LSTM model in FAEP further refines these augmented features, resulting in a more robust prediction framework. Experimental results demonstrate that FAEP achieves state-of-art (SOTA) performance compared to other electricity price prediction models in the Australian New South Wale electricity market, showcasing the efficiency of LLM-enhanced feature engineering and hybrid machine learning architectures.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-based Discovery of Intrinsic Coordinates and Governing Equations from High-Dimensional Data</title>
<link>https://arxiv.org/abs/2505.11940</link>
<guid>https://arxiv.org/abs/2505.11940</guid>
<content:encoded><![CDATA[
<div> Keywords: governing equations, high-dimensional data, multimodal large language models, zero-shot method, spatial perception <br />
<br />
Summary: 
Discovering governing equations from high-dimensional scientific data is a challenging task due to the exponential expansion of the equation space. This paper introduces a novel zero-shot approach using multimodal large language models (MLLM) to automatically identify physical coordinates and equations from data. By enhancing MLLM's spatial perception with visual prompts and leveraging its domain knowledge, the proposed method effectively navigates the equation space. Evaluation on simulated and real data shows improved accuracy in discovering physical coordinates and equations. Long-term extrapolation accuracy is enhanced by approximately 26.96% compared to the baseline, showcasing the efficiency of the approach in understanding the evolution of systems. <div>
arXiv:2505.11940v1 Announce Type: new 
Abstract: Discovering governing equations from scientific data is crucial for understanding the evolution of systems, and is typically framed as a search problem within a candidate equation space. However, the high-dimensional nature of dynamical systems leads to an exponentially expanding equation space, making the search process extremely challenging. The visual perception and pre-trained scientific knowledge of multimodal large language models (MLLM) hold promise for providing effective navigation in high-dimensional equation spaces. In this paper, we propose a zero-shot method based on MLLM for automatically discovering physical coordinates and governing equations from high-dimensional data. Specifically, we design a series of enhanced visual prompts for MLLM to enhance its spatial perception. In addition, MLLM's domain knowledge is employed to navigate the search process within the equation space. Quantitative and qualitative evaluations on two representative types of systems demonstrate that the proposed method effectively discovers the physical coordinates and equations from both simulated and real experimental data, with long-term extrapolation accuracy improved by approximately 26.96% compared to the baseline.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Gas Well Performance with Decline Curve Analysis: A Case Study on Semutang Gas Field</title>
<link>https://arxiv.org/abs/2505.12333</link>
<guid>https://arxiv.org/abs/2505.12333</guid>
<content:encoded><![CDATA[
<div> DCA, production forecasting, gas reservoir, Semutang gas field, decline curve parameters <br />
<br />
Summary: <br />
Decline-curve analysis (DCA) is an important method for forecasting production and estimating reserves in gas reservoirs. This study applied DCA to predict future production performance and estimate ultimate recovery for a well in the Semutang gas field in Bangladesh. Different decline curve models were used, with the hyperbolic model providing the most accurate forecast. It is crucial to select the right decline model for precise production forecasting and reserve estimation, essential for effective reservoir management and resource optimization. The study emphasizes the significance of accurate decline curve analysis in the oil and gas industry. <div>
arXiv:2505.12333v1 Announce Type: new 
Abstract: Decline-curve analysis (DCA) is a widely utilized method for production forecasting and estimating remaining reserves in gas reservoir. Based on the assumptions that past production trend can be mathematically characterized and used to predict future performance. It relies on historical production data and assumes that production methods remain unchanged throughout the analysis. This method is particularly valuable due to its accuracy in forecasting and its broad acceptance within the industry. Wells in the same geographical area and producing from similar geological formations often exhibit similar decline curve parameters. This study applies DCA to forecast the future production performance and estimate the ultimate recovery for the Semutang gas field's well 5 in Bangladesh. Using historical production data, decline curves were generated based on exponential, hyperbolic, and harmonic model equations. The cumulative production estimations were 11,139.34 MMSCF for the exponential model, 11,620.26 MMSCF for the hyperbolic model, and 14,021.92 MMSCF for the harmonic model. In terms of the well's productive life, the estimates were 335.13 days, 1,152 days, and 22,611 days, respectively. Among these models, the hyperbolic decline provided the most realistic forecast, closely aligning with observed production trend. The study highlights the importance of selecting an appropriate decline model for accurate production forecasting and reserve estimation, which is essential for effective reservoir management and resource optimization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seismic analysis based on a new interval method with incomplete information</title>
<link>https://arxiv.org/abs/2505.12607</link>
<guid>https://arxiv.org/abs/2505.12607</guid>
<content:encoded><![CDATA[
<div> Keywords: seismic analysis, interval uncertainty, Monte Carlo simulation, covariance matrix adaptation evolution strategy, computational efficiency <br />
Summary: 
This study focuses on describing time-variant uncertain seismic accelerations using the minimum interval radius-based interval process (MRIP) based on the convex model for seismic analysis in engineering structures. To enhance computational efficiency in uncertainty analysis, the paper introduces the improved covariance matrix adaptation evolution strategy (CMA-ES) called DES-ES, which exhibits higher efficiency. Additionally, a computational framework named DES-ES-SS is proposed to leverage response dependency and further enhance computational efficiency while maintaining accuracy in interval uncertainty analysis of seismic structures under stationary or non-stationary seismic acceleration conditions. Numerical experiments validate the effectiveness of DES-ES-SS in improving computational efficiency without compromising accuracy. <br /><br /> <div>
arXiv:2505.12607v1 Announce Type: new 
Abstract: For seismic analysis in engineering structures, it is essential to consider the dynamic responses under seismic excitation, necessitating the description of seismic accelerations. Limit seismics samples lead to incomplete uncertainty information, which is described by the non-probabilistic method reasonable. This study employs the minimum interval radius-based interval process (MRIP) based on the convex model to describe the time-variant uncertain seismic acceleration, subsequently conducting uncertainty analysis for seismic structures. However, the Monte Carlo simulation for uncertainty analysis requires extensive deterministic computations to ensure accuracy, exhibiting poor computational efficiency. To address this issue, this paper first improves the covariance matrix adaptation evolution strategy (CMA-ES) through the dynamic evolution sequence, proposing DES-ES, whose efficiency is validated to be higher than that of CMA-ES. Furthermore, leveraging the dependency of the responses, a computational framework named DES-ES-SS is proposed. Numerical experiments demonstrate that DES-ES-SS improves computational efficiency while maintaining the accuracy of the interval uncertainty analysis of the seismic structures whether the seismic acceleration is stationary or non-stationary.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit differentiation with second-order derivatives and benchmarks in finite-element-based differentiable physics</title>
<link>https://arxiv.org/abs/2505.12646</link>
<guid>https://arxiv.org/abs/2505.12646</guid>
<content:encoded><![CDATA[
<div> differentiable programming, automatic differentiation, second-order derivatives, finite-element-based, PDE-constrained optimization

Summary:
This paper introduces a framework for computing second-order derivatives (Hessians) for implicit functions in finite-element-based differentiable physics. By utilizing primitive automatic differentiation tools, the authors develop an algorithm for efficiently calculating Hessian-vector products. Validation against finite difference approximations shows accurate results. Benchmark tests across various problem types demonstrate the advantages of using second-order information in optimization. The Newton-CG method with exact Hessians proves beneficial for nonlinear inverse problems like traction force identification and shape optimization, while the L-BFGS-B method is suitable for linear cases. These findings establish a solid foundation for incorporating second-order implicit differentiation into differentiable physics engines, enhancing optimization speed and reliability.<br /><br />Summary: <div>
arXiv:2505.12646v1 Announce Type: new 
Abstract: Differentiable programming is revolutionizing computational science by enabling automatic differentiation (AD) of numerical simulations. While first-order gradients are well-established, second-order derivatives (Hessians) for implicit functions in finite-element-based differentiable physics remain underexplored. This work bridges this gap by deriving and implementing a framework for implicit Hessian computation in PDE-constrained optimization problems. We leverage primitive AD tools (Jacobian-vector product/vector-Jacobian product) to build an algorithm for Hessian-vector products and validate the accuracy against finite difference approximations. Four benchmarks spanning linear/nonlinear, 2D/3D, and single/coupled-variable problems demonstrate the utility of second-order information. Results show that the Newton-CG method with exact Hessians accelerates convergence for nonlinear inverse problems (e.g., traction force identification, shape optimization), while the L-BFGS-B method suffices for linear cases. Our work provides a robust foundation for integrating second-order implicit differentiation into differentiable physics engines, enabling faster and more reliable optimization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grid Topology Estimation using an Information Theoretic Approach</title>
<link>https://arxiv.org/abs/2505.11517</link>
<guid>https://arxiv.org/abs/2505.11517</guid>
<content:encoded><![CDATA[
<div> mutual information, power grid, graph modeling, maximum spanning tree, IEEE networks

Summary:
The article introduces an information-theoretic approach to estimate the topology of a power grid by modeling it as a graph and analyzing voltage magnitude data of individual nodes. The mutual information between pairs of nodes is computed using different approximation methods, leading to the estimation of the power grid topology through a maximum spanning tree based on mutual information, generated with the Chow-Liu algorithm. The approach is successfully optimized and validated on IEEE networks created with MATPOWER and GridLAB-D data, as well as on networks from the European Union Joint Research Council. The experiments and results demonstrate the effectiveness of this method in accurately estimating power grid topologies. <br /><br />Summary: <div>
arXiv:2505.11517v1 Announce Type: cross 
Abstract: The topology of a power grid is estimated using an information theoretic approach. By modeling the grid as a graph and using voltage magnitude data of individual nodes in the grid, the mutual information between pairs of nodes is computed using different approximation methods. Using the well-known Chow-Liu algorithm, a maximum spanning tree based on mutual information is computed to estimate the power grid topology. Experiments and results are presented to optimize this approach with success shown for IEEE networks generated with MATPOWER and data generated using GridLAB-D. The algorithm is then cross-validated on IEEE networks generated by the European Union Joint Research Council.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASL-HJX: A Comprehensive Guide to Solving Deterministic and Stochastic Hamilton-Jacobi Equations</title>
<link>https://arxiv.org/abs/2505.11527</link>
<guid>https://arxiv.org/abs/2505.11527</guid>
<content:encoded><![CDATA[
<div> framework, Hamilton-Jacobi equations, numerical methods, operator splitting techniques, neuroscience<br />
Summary:<br />
CASL-HJX is a computational framework designed for solving deterministic and stochastic Hamilton-Jacobi equations in two spatial dimensions. It integrates numerical methods for hyperbolic PDEs with operator splitting techniques and implements implicit methods for second-order derivative terms to ensure convergence to viscosity solutions. The high-performance C++ core efficiently handles mixed-order derivative systems with time-varying dynamics, making it suitable for real-world applications. The solver's versatility is demonstrated through tutorial examples and applications in neuroscience, enabling the design of energy-efficient controllers for regulating neural populations. CASL-HJX's modular architecture allows researchers to define computational domains, configure problems, and execute simulations with high numerical accuracy. It serves as a robust tool for managing uncertainty in complex dynamical systems, bridging the gap between deterministic control methods and stochastic models. <div>
arXiv:2505.11527v1 Announce Type: cross 
Abstract: CASL-HJX is a computational framework designed for solving deterministic and stochastic Hamilton-Jacobi equations in two spatial dimensions. It provides a flexible and efficient approach to modeling front propagation problems, optimal control problems, and stochastic Hamilton-Jacobi Bellman equations. The framework integrates numerical methods for hyperbolic PDEs with operator splitting techniques and implements implicit methods for second-order derivative terms, ensuring convergence to viscosity solutions while achieving global rather than local optimization. Built with a high-performance C++ core, CASL-HJX efficiently handles mixed-order derivative systems with time-varying dynamics, making it suitable for real-world applications across multiple domains. We demonstrate the solver's versatility through tutorial examples covering various PDEs and through applications in neuroscience, where it enables the design of energy-efficient controllers for regulating neural populations to mitigate pathological synchrony. While our examples focus on these applications, the mathematical foundation of the solver makes it applicable to problems in finance, engineering, and machine learning. The modular architecture allows researchers to define computational domains, configure problems, and execute simulations with high numerical accuracy. CASL-HJX bridges the gap between deterministic control methods and stochastic models, providing a robust tool for managing uncertainty in complex dynamical systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median</title>
<link>https://arxiv.org/abs/2505.11725</link>
<guid>https://arxiv.org/abs/2505.11725</guid>
<content:encoded><![CDATA[
<div> bootstrap, quantiles, central limit theorem, moment condition, asymptotic distributions

Summary:
This paper investigates the m-out-of-n bootstrap method for estimating sample quantiles, providing parameter-free guarantees for its soundness. A central limit theorem is established for a data-driven estimator under a mild moment condition, without the need for unknown nuisance parameters. The paper presents a counter-example demonstrating the tightness of the moment assumption and derives an Edgeworth expansion for exact convergence rates, along with a Berry Esseen bound for bootstrap approximation errors. The results are illustrated through applications to practical statistics like quantiles for random walk Metropolis-Hastings and rewards of ergodic Markov decision processes, showcasing the theory's utility in modern estimation and learning tasks.<br /><br />Summary: <div>
arXiv:2505.11725v1 Announce Type: cross 
Abstract: The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet (1992), approximates the distribution of a statistic by repeatedly drawing m subsamples (with m much smaller than n) without replacement from an original sample of size n. It is now routinely used for robust inference with heavy-tailed data, bandwidth selection, and other large-sample applications. Despite its broad applicability across econometrics, biostatistics, and machine learning, rigorous parameter-free guarantees for the soundness of the m-out-of-n bootstrap when estimating sample quantiles have remained elusive.
  This paper establishes such guarantees by analyzing the estimator of sample quantiles obtained from m-out-of-n resampling of a dataset of size n. We first prove a central limit theorem for a fully data-driven version of the estimator that holds under a mild moment condition and involves no unknown nuisance parameters. We then show that the moment assumption is essentially tight by constructing a counter-example in which the CLT fails. Strengthening the assumptions slightly, we derive an Edgeworth expansion that provides exact convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap approximation error. Finally, we illustrate the scope of our results by deriving parameter-free asymptotic distributions for practical statistics, including the quantiles for random walk Metropolis-Hastings and the rewards of ergodic Markov decision processes, thereby demonstrating the usefulness of our theory in modern estimation and learning tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibility Data</title>
<link>https://arxiv.org/abs/2505.12638</link>
<guid>https://arxiv.org/abs/2505.12638</guid>
<content:encoded><![CDATA[
<div> ChromFound, scATAC-seq, single-cell, regulatory mechanisms, cell identification<br />
Summary:<br />
ChromFound is a foundation model designed for scATAC-seq data analysis, addressing challenges such as high dimensionality and lack of standardized OCR representation. It utilizes a hybrid architecture and genome-aware tokenization to capture genome-wide regulatory signals, trained on a large dataset for diverse tasks. The model demonstrates robust zero-shot performance, universal cell representation generation, and transferability in cell type annotation and cross-omics prediction. By identifying enhancer-gene links missed by other methods, ChromFound provides insights into disease risk variants in the noncoding genome. The model offers a promising framework for deciphering regulatory mechanisms and understanding complex chromatin landscapes in single-cell studies. <br /><br />Summary: <div>
arXiv:2505.12638v1 Announce Type: cross 
Abstract: The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present \textbf{ChromFound}, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Modeling of Random Fields from Limited Data via Constrained Latent Flow Matching</title>
<link>https://arxiv.org/abs/2505.13007</link>
<guid>https://arxiv.org/abs/2505.13007</guid>
<content:encoded><![CDATA[
<div> framework, generative modeling, random fields, latent flow matching, variational autoencoder <br />
Summary: 
- The article discusses a novel framework for generative modeling of random fields that incorporates domain knowledge to supplement limited data.
- The approach uses latent flow matching to operate on compressed function representations in the latent space of a pre-trained VAE.
- It includes a function decoder within the VAE and integrates physical/statistical constraints into the training process.
- The framework learns a latent function representation that generates continuous random field samples satisfying domain-specific constraints, even with sparse data.
- Two applications, wind velocity field reconstruction and material property inference, demonstrate the effectiveness of the framework in achieving improved reconstruction accuracy and inference with small training datasets. 
<br /> <div>
arXiv:2505.13007v1 Announce Type: cross 
Abstract: Deep generative models are promising tools for science and engineering, but their reliance on abundant, high-quality data limits applicability. We present a novel framework for generative modeling of random fields (probability distributions over continuous functions) that incorporates domain knowledge to supplement limited, sparse, and indirect data. The foundation of the approach is latent flow matching, where generative modeling occurs on compressed function representations in the latent space of a pre-trained variational autoencoder (VAE). Innovations include the adoption of a function decoder within the VAE and integration of physical/statistical constraints into the VAE training process. In this way, a latent function representation is learned that yields continuous random field samples satisfying domain-specific constraints when decoded, even in data-limited regimes. Efficacy is demonstrated on two challenging applications: wind velocity field reconstruction from sparse sensors and material property inference from a limited number of indirect measurements. Results show that the proposed framework achieves significant improvements in reconstruction accuracy compared to unconstrained methods and enables effective inference with relatively small training datasets that is intractable without constraints.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System reduction-based approximate reanalysis method for statically indeterminate structures with high-rank modification</title>
<link>https://arxiv.org/abs/2302.02171</link>
<guid>https://arxiv.org/abs/2302.02171</guid>
<content:encoded><![CDATA[
<div> spectral decomposition, structural reanalysis, high-rank modification, approximate static reanalysis, iterative solution <br />
Summary: <br />
The article proposes a novel approximate static reanalysis method for statically indeterminate structures with high-rank modification. The method involves dividing the structure into the basis system and additional components, rewriting equilibrium equations, and establishing a reduced equation system using spectral decomposition. The reduced system is solved using a pre-conditioned iterative solution algorithm to obtain approximate solutions for modified structures. The method shows excellent computational performance for structures with homogeneous material and functionally graded beams. The combination of system reduction and iterative solution technology proves to be an effective approach for developing high-performance reanalysis methods. <div>
arXiv:2302.02171v2 Announce Type: replace 
Abstract: Efficient structural reanalysis for high-rank modification plays an important role in engineering computations which require repeated evaluations of structural responses, such as structural optimization and probabilistic analysis. To improve the efficiency of engineering computations, a novel approximate static reanalysis method based on system reduction and iterative solution is proposed for statically indeterminate structures with high-rank modification. In this approach, a statically indeterminate structure is divided into the basis system and the additional components. Subsequently, the structural equilibrium equations are rewritten as the equation system with the stiffness matrix of the basis system and the pseudo forces derived from the additional elements. With the introduction of spectral decomposition, a reduced equation system with the element forces of the additional elements as the unknowns is established. Then, the approximate solutions of the modified structure can be obtained by solving the reduced equation system through a pre-conditioned iterative solution algorithm. The computational costs of the proposed method and the other two reanalysis methods are compared and numerical examples including static reanalysis and static nonlinear analysis are presented. The results demonstrate that the proposed method has excellent computational performance for both the structures with homogeneous material and structures composed of functionally graded beams. Meanwhile, the superiority of the proposed method indicates that the combination of system reduction and pre-conditioned iterative solution technology is an effective way to develop high-performance reanalysis methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSP-free adaptive Kriging surrogate model method for reliability analysis with small failure probability</title>
<link>https://arxiv.org/abs/2304.07010</link>
<guid>https://arxiv.org/abs/2304.07010</guid>
<content:encoded><![CDATA[
<div> Active learning reliability method, Kriging, Monte Carlo Simulation, Candidate Sample Pool, Particle Swarm Optimization

Summary:
The paper introduces a new method, CSP-free AK-MCS, to enhance reliability analysis in engineering. The method eliminates the reliance on Candidate Sample Pool (CSP) size, especially for systems with low failure probabilities. It consists of two stages: constructing a surrogate model and using Monte Carlo simulation. Surrogate model refinement is done iteratively with representative samples selected through Particle Swarm Optimization (PSO) algorithm. Adjustments like penalty intensity control and density control optimize the objective function, balancing accuracy and efficiency. Numerical examples prove the effectiveness of CSP-free AK-MCS in handling small failure probabilities, showing improved performance compared to conventional methods. This innovative approach overcomes limitations and provides exceptional results for reliability analysis. 

<br /><br />Summary: <div>
arXiv:2304.07010v5 Announce Type: replace 
Abstract: In the field of reliability engineering, the Active learning reliability method combining Kriging and Monte Carlo Simulation (AK-MCS) has been developed and demonstrated to be effective in reliability analysis. However, the performance of AK-MCS is sensitive to the size of Candidate Sample Pool (CSP), particularly for systems with small failure probabilities. To address the limitations of conventional AK-MCS that relies on CSP, this paper proposes a CSP-free AK-MCS. The proposed methodology consists of two stages: surrogate model construction and Monte Carlo simulation for estimating the failure probability. In the stage of surrogate model construction, the surrogate model is iteratively refined based on the representative samples selected by solving the optimization problem facilitated by Particle Swarm Optimization (PSO) algorithm. To achieve an optimal balance between solution accuracy and efficiency, the penalty intensity control and the density control for the experimental design points are introduced to modify the objective function in optimization. The performance of the proposed methodology is evaluated using numerical examples, and results indicate that by leveraging an optimization algorithm to select representative samples, the proposed CSP-free AK-MCS overcomes the limitations of conventional CSP-based AK-MCS and exhibits exceptional performance in addressing small failure probabilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A cable finite element formulation based on exact tension field for static nonlinear analysis of cable structures</title>
<link>https://arxiv.org/abs/2401.05609</link>
<guid>https://arxiv.org/abs/2401.05609</guid>
<content:encoded><![CDATA[
<div> beam theory, cable structures, finite element model, nonlinear analysis, numerical precision <br />
Summary: 
This paper introduces a numerically exact finite element model for analyzing the static nonlinear behavior of cable structures. The model accurately calculates the tension field by incorporating the geometrically exact beam theory and fundamental mechanical properties of cables. Unlike previous approaches, this model prioritizes numerical precision and universal applicability, deriving linearized equations with implicit expressions that include integrals. It also considers the variation in cross-sectional stiffness along the cable's length. The formulation ensures equilibrium and compatibility within cable elements, enabling accurate computation of internal forces, deformation states, and the unstrained length of the cable. The study discusses the implementation of solutions using the complete tangent matrix and internal iterations within elements. Numerical examples validate the effectiveness of the proposed cable element. <div>
arXiv:2401.05609v5 Announce Type: replace 
Abstract: This paper presents a numerically exact cable finite element model for static nonlinear analysis of cable structures. The model derives the exact expression of the tension field using the geometrically exact beam theory coupled with the fundamental mechanical characteristics of cables. The equations for the cable element are formulated by addressing the equilibrium conditions at the element boundaries and ensuring compatibility within the element. Unlike previous studies that typically provide explicit expressions for cable models, this study develops a formulation that emphasizes numerical precision and broad applicability. It achieves this by deriving linearized equations with implicit expressions incorporating integrals. The proposed model accurately computes internal forces and deformation states, and determines the unstrained length of the cable. Additionally, it accounts for the variability in cross-sectional stiffness along the cable's length. The paper discusses solution implementations using the complete tangent matrix and element internal iterations. The effectiveness of the proposed cable element is demonstrated through numerical examples.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enforced Interface Constraints for Domain Decomposition Method of Discrete Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.10925</link>
<guid>https://arxiv.org/abs/2505.10925</guid>
<content:encoded><![CDATA[
<div> Discrete Physics-Informed Neural Network, Domain Decomposition Method, Gaussian quadrature, Interface constraints, Computational efficiency<br />
<br />
Summary:
The study introduces a discrete physics-informed neural network (dPINN) framework with enforced interface constraints (EIC) for modeling physical systems using the domain decomposition method (DDM). The dPINN accurately calculates system energy through element-wise integration using Gaussian quadrature. The EIC enforces interfacial displacement constraints to ensure physical field continuity without auxiliary sampling or loss penalties, supporting independent meshing in each subdomain. By eliminating weak spatial constraints (WSC), the EIC-dPINN provides more stable and physically consistent predictions. Extensive numerical experiments in two and three dimensions confirm the framework's accuracy and showcase computational efficiency gains through parallel training. The results illustrate the framework's scalability, robustness, and potential for solving large-scale, geometrically complex problems.<br /><br />Summary: <div>
arXiv:2505.10925v1 Announce Type: new 
Abstract: This study presents a discrete physics-informed neural network (dPINN) framework, enhanced with enforced interface constraints (EIC), for modeling physical systems using the domain decomposition method (DDM). Built upon finite element-style mesh discretization, the dPINN accurately evaluates system energy through Gaussian quadrature-based element-wise integration. To ensure physical field continuity across subdomain interfaces, the EIC mechanism enforces interfacial displacement constraints without requiring auxiliary sampling or loss penalties.This formulation supports independent meshing in each subdomain, simplifying preprocessing and improving computational flexibility. Additionally, by eliminating the influence of weak spatial constraints (WSC) commonly observed in traditional PINNs, the EIC-dPINN delivers more stable and physically consistent predictions.Extensive two- and three-dimensional numerical experiments validate the proposed framework's accuracy and demonstrate the computational efficiency gains achieved through parallel training. The results highlight the framework's scalability, robustness, and potential for solving large-scale, geometrically complex problems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Travel is Cheating: Going Live with DeepFund for Real-Time Fund Investment Benchmarking</title>
<link>https://arxiv.org/abs/2505.11065</link>
<guid>https://arxiv.org/abs/2505.11065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, DeepFund, real-time market data, fund investment, evaluation

Summary:<br />
Large Language Models (LLMs) have shown promise in financial tasks but their effectiveness in fund investment remains unexplored. To address limitations in evaluating LLM-driven trading strategies, a new benchmark tool called DeepFund is introduced. DeepFund directly connects with real-time stock market data to prevent information leakage and provide fair evaluations. Test results on nine top LLMs reveal challenges in various investment dimensions, showcasing practical limitations in active fund management. Even advanced models like DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses in the real-time evaluation environment of DeepFund. This highlights the current constraints of LLMs in active fund management.<br /> <div>
arXiv:2505.11065v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated notable capabilities across financial tasks, including financial report summarization, earnings call transcript analysis, and asset classification. However, their real-world effectiveness in managing complex fund investment remains inadequately assessed. A fundamental limitation of existing benchmarks for evaluating LLM-driven trading strategies is their reliance on historical back-testing, inadvertently enabling LLMs to "time travel"-leveraging future information embedded in their training corpora, thus resulting in possible information leakage and overly optimistic performance estimates. To address this issue, we introduce DeepFund, a live fund benchmark tool designed to rigorously evaluate LLM in real-time market conditions. Utilizing a multi-agent architecture, DeepFund connects directly with real-time stock market data-specifically data published after each model pretraining cutoff-to ensure fair and leakage-free evaluations. Empirical tests on nine flagship LLMs from leading global institutions across multiple investment dimensions-including ticker-level analysis, investment decision-making, portfolio management, and risk control-reveal significant practical challenges. Notably, even cutting-edge models such as DeepSeek-V3 and Claude-3.7-Sonnet incur net trading losses within DeepFund real-time evaluation environment, underscoring the present limitations of LLMs for active fund management. Our code is available at https://github.com/HKUSTDial/DeepFund.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment</title>
<link>https://arxiv.org/abs/2505.11194</link>
<guid>https://arxiv.org/abs/2505.11194</guid>
<content:encoded><![CDATA[
<div> sequence-to-text model, protein function prediction, multimodal, contrastive alignment learning, fine-tuning

Summary:
Prot2Text-V2 is a novel multimodal sequence-to-text model that generates natural language descriptions of protein function directly from amino acid sequences. This model combines a protein language encoder with a decoder-only language model through a modality projector, improving cross-modal learning through Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE). The model is trained on curated entries from SwissProt and performs well under low-homology conditions. Prot2Text-V2 outperforms traditional and LLM-based baselines across various metrics. The combination of contrasting mean- and std-pooled protein embeddings with text representations, along with instruction-based fine-tuning using LoRA, results in accurate protein function description generation conditioned on the protein sequence.<br /><br />Summary: <div>
arXiv:2505.11194v1 Announce Type: new 
Abstract: Predicting protein function from sequence is a central challenge in computational biology. While existing methods rely heavily on structured ontologies or similarity-based techniques, they often lack the flexibility to express structure-free functional descriptions and novel biological functions. In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text model that generates free-form natural language descriptions of protein function directly from amino acid sequences. Our method combines a protein language model as a sequence encoder (ESM-3B) and a decoder-only language model (LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A key innovation is our Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE), which improves cross-modal learning by matching mean- and std-pooled protein embeddings with text representations via contrastive loss. After the alignment phase, we apply instruction-based fine-tuning using LoRA on the decoder to teach the model how to generate accurate protein function descriptions conditioned on the protein sequence. We train Prot2Text-V2 on about 250K curated entries from SwissProt and evaluate it under low-homology conditions, where test sequences have low similarity with training samples. Prot2Text-V2 consistently outperforms traditional and LLM-based baselines across various metrics.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLOVA: Global and Local Variation-Aware Analog Circuit Design with Risk-Sensitive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11208</link>
<guid>https://arxiv.org/abs/2505.11208</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, analog circuit design, variation-aware design, robustness, PVT variations

Summary:
GLOVA is introduced as an analog circuit sizing framework to address challenges in variation-aware design. It utilizes risk-sensitive reinforcement learning to improve robustness against PVT variations, with an ensemble-based critic for efficient learning. A $\mu$-$\sigma$ evaluation and simulation reordering method is proposed for design verification, reducing simulation costs. GLOVA supports industrial-level PVT variation evaluation methods, including corner simulation and global/local Monte Carlo simulations. Compared to existing frameworks, GLOVA demonstrates significant improvements in sample efficiency and time, achieving up to 80.5$\times$ improvement in sample efficiency and 76.0$\times$ reduction in time. <div>
arXiv:2505.11208v1 Announce Type: cross 
Abstract: Analog/mixed-signal circuit design encounters significant challenges due to performance degradation from process, voltage, and temperature (PVT) variations. To achieve commercial-grade reliability, iterative manual design revisions and extensive statistical simulations are required. While several studies have aimed to automate variation aware analog design to reduce time-to-market, the substantial mismatches in real-world wafers have not been thoroughly addressed. In this paper, we present GLOVA, an analog circuit sizing framework that effectively manages the impact of diverse random mismatches to improve robustness against PVT variations. In the proposed approach, risk-sensitive reinforcement learning is leveraged to account for the reliability bound affected by PVT variations, and ensemble-based critic is introduced to achieve sample-efficient learning. For design verification, we also propose $\mu$-$\sigma$ evaluation and simulation reordering method to reduce simulation costs of identifying failed designs. GLOVA supports verification through industrial-level PVT variation evaluation methods, including corner simulation as well as global and local Monte Carlo (MC) simulations. Compared to previous state-of-the-art variation-aware analog sizing frameworks, GLOVA achieves up to 80.5$\times$ improvement in sample efficiency and 76.0$\times$ reduction in time.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets</title>
<link>https://arxiv.org/abs/2502.01506</link>
<guid>https://arxiv.org/abs/2502.01506</guid>
<content:encoded><![CDATA[
<div> Keywords: social emergence, large language model agents, multi-agent framework, socio-economic systems, emergent phenomena

Summary: 
 This study introduces TwinMarket, a new framework that uses large language model agents to simulate socio-economic systems. Traditional modeling approaches struggle to capture the complexity of human behavior, but TwinMarket's use of LLMs allows for more realistic simulations. By focusing on individual behaviors and their interactions, the framework demonstrates how collective dynamics and emergent phenomena can arise. The experiments conducted in a simulated stock market environment show how individual actions can lead to group behaviors, resulting in outcomes such as financial bubbles and recessions. This approach provides valuable insights into the intricate relationship between individual decision-making and collective socio-economic patterns. Overall, TwinMarket offers a promising avenue for studying social emergence and understanding the complexities of human behavior in socio-economic systems. 

<br /><br /> <div>
arXiv:2502.01506v3 Announce Type: replace 
Abstract: The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tensor Network Structure Search with Program Synthesis</title>
<link>https://arxiv.org/abs/2502.02711</link>
<guid>https://arxiv.org/abs/2502.02711</guid>
<content:encoded><![CDATA[
<div> tensor networks, data compression, program synthesis, constraint-based assessment, search efficiency 

Summary: 
Tensor networks are effective for compressing multi-dimensional data, but finding the optimal structure is challenging. This study proposes a novel approach that views tensor network structure search as a program synthesis problem. By establishing a link between transformation programs and network structures, the method reduces the search space using output-directed splits. A synthesis algorithm identifies promising network candidates through constraint solving, minimizing the need for costly tensor decomposition. Experimental results demonstrate a significant improvement in search speed, achieving compression ratios surpassing state-of-the-art methods by 1.5 to 3 times. The approach also scales effectively to larger tensors and generalizes well to similar data, outperforming generic structures with compression ratios up to 2.4 times better. This enhanced efficiency and effectiveness make the proposed method a valuable tool for data compression tasks. 

<br /><br />Summary: <div>
arXiv:2502.02711v3 Announce Type: replace 
Abstract: Tensor networks provide a powerful framework for compressing multi-dimensional data. The optimal tensor network structure for a given data tensor depends on both data characteristics and specific optimality criteria, making tensor network structure search a difficult problem. Existing solutions typically rely on sampling and compressing numerous candidate structures; these procedures are computationally expensive and therefore limiting for practical applications. We address this challenge by viewing tensor network structure search as a program synthesis problem and introducing an efficient constraint-based assessment method that avoids costly tensor decomposition. Specifically, we establish a correspondence between transformation programs and network structures. We also design a novel operation named output-directed splits to reduce the search space without hindering expressiveness. We then propose a synthesis algorithm to identify promising network candidates through constraint solving, and avoid tensor decomposition for all but the most promising candidates. Experimental results show that our approach improves search speed by up to $10\times$ and achieves compression ratios $1.5\times$ to $3\times$ better than state-of-the-art. Notably, our approach scales to larger tensors that are unattainable by prior work. Furthermore, the discovered topologies generalize well to similar data, yielding compression ratios up to $ 2.4\times$ better than a generic structure while the runtime remains around $110$ seconds.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Market Environments for FinRL Contests</title>
<link>https://arxiv.org/abs/2504.02281</link>
<guid>https://arxiv.org/abs/2504.02281</guid>
<content:encoded><![CDATA[
<div> Contests, FinRL, financial tasks, datasets, evaluation protocols<br />
<br />
Summary: Financial reinforcement learning (FinRL) has shown promise in sequential decision-making in finance, but faces challenges in real-world trading. To address this, three FinRL Contests were organized from 2023 to 2025, covering various financial tasks with standardized definitions, datasets, environments, and baselines. Over 200 participants from 100 institutions and 22 countries joined, with open-source starter kits provided for reproducibility. The contests focused on stock trading, order execution, cryptocurrency trading, and the use of large language model-generated signals. The efforts included task formulations, data curation pipelines, environment implementations, evaluation protocols, participant performance analysis, and organizational insights. These benchmarking initiatives have enhanced the understanding and development of FinRL methods for financial applications. <br /><br /> <div>
arXiv:2504.02281v3 Announce Type: replace 
Abstract: Financial reinforcement learning (FinRL) has emerged as a promising paradigm for sequential decision-making in financial engineering. However, applying RL in real-world trading tasks remains challenging due to the non-stationarity of financial data, low signal-to-noise ratios, and various market frictions. Although numerous FinRL methods have been developed for tasks such as trading and portfolio management, the lack of standardized task definitions, datasets, environments, and baselines has hindered consistent evaluation and reproducibility. To bridge this gap, we organized three FinRL Contests from 2023 to 2025, covering a diverse range of financial tasks such as stock trading, order execution, cryptocurrency trading, and the use of large language model (LLM)-generated signals. These contests attracted 200 participants from over 100 institutions across 22 countries. To promote reproduction, we provided open-source starter kits featuring GPU-optimized parallel market environments and comprehensive documentation. In this paper, we summarize these benchmarking efforts, detailing task formulations, data curation pipelines, environment implementations, evaluation protocols, participant performance, and key organizational insights.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Promise of Data-Driven Modeling and Decision Support for Precision Oncology and Theranostics</title>
<link>https://arxiv.org/abs/2505.09899</link>
<guid>https://arxiv.org/abs/2505.09899</guid>
<content:encoded><![CDATA[
<div> Keywords: Cancer, Theranostics, Precision oncology, Reinforcement learning, Data-driven decision support 

Summary: 
This paper discusses the potential of theranostics in precision oncology by combining molecular imaging with targeted therapy for personalized cancer treatment. It explores current data-driven decision support applications with a focus on reinforcement learning to optimize patient-specific care plans for cancer treatment. The study reviews the training environments, state-space representation, performance evaluation criteria, and the measurement of risk and reward in precision oncology applications. It identifies key challenges in the field and proposes a framework that integrates data-driven modeling with reinforcement learning-based decision support to optimize radiopharmaceutical therapy dosing. The framework utilizes advanced technologies such as Neural Ordinary Differential Equations and Physics-Informed Neural Networks to enhance Physiologically Based Pharmacokinetic models. By employing reinforcement learning algorithms, it iteratively refines treatment policies based on individual patient data, aiming to improve outcomes in cancer therapy. 

<br /><br />Summary: <div>
arXiv:2505.09899v1 Announce Type: new 
Abstract: Cancer remains a leading cause of death worldwide, necessitating personalized treatment approaches to improve outcomes. Theranostics, combining molecular-level imaging with targeted therapy, offers potential for precision oncology but requires optimized, patient-specific care plans. This paper investigates state-of-the-art data-driven decision support applications with a reinforcement learning focus in precision oncology. We review current applications, training environments, state-space representation, performance evaluation criteria, and measurement of risk and reward, highlighting key challenges. We propose a framework integrating data-driven modeling with reinforcement learning-based decision support to optimize radiopharmaceutical therapy dosing, addressing identified challenges and setting directions for future research. The framework leverages Neural Ordinary Differential Equations and Physics-Informed Neural Networks to enhance Physiologically Based Pharmacokinetic models while applying reinforcement learning algorithms to iteratively refine treatment policies based on patient-specific data.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical regularized Hierarchical Generative Model for Metallic Glass Structural Generation and Energy Prediction</title>
<link>https://arxiv.org/abs/2505.09977</link>
<guid>https://arxiv.org/abs/2505.09977</guid>
<content:encoded><![CDATA[
<div> variational autoencoder, disordered materials, glass, generative AI models, physics-informed regularizers <br />
Summary: 
The article introduces GlassVAE, a hierarchical graph variational autoencoder designed to handle the complexity of disordered materials like glasses. GlassVAE uses graph representations to create rotation, translation, and permutation invariant embeddings of atomic configurations, enabling efficient generation of realistic structures and exploration of the glass energy landscape. To ensure structural realism and physical fidelity, GlassVAE is augmented with two physics-informed regularizers: a radial distribution function (RDF) loss and an energy regression loss. The regularizers play a crucial role in enhancing the accuracy of the model. By encoding high-dimensional atomistic data into a compact latent vector and decoding it to generate structures with accurate energy predictions, GlassVAE provides a rapid, physics-aware approach for modeling and designing disordered materials. <br /><br /> <div>
arXiv:2505.09977v1 Announce Type: new 
Abstract: Disordered materials such as glasses, unlike crystals, lack long range atomic order and have no periodic unit cells, yielding a high dimensional configuration space with widely varying properties. The complexity not only increases computational costs for atomistic simulations but also makes it difficult for generative AI models to deliver accurate property predictions and realistic structure generation. In this work, we introduce GlassVAE, a hierarchical graph variational autoencoder that uses graph representations to learn compact, rotation, translation, and permutation invariant embeddings of atomic configurations. The resulting structured latent space not only enables efficient generation of novel, physically plausible structures but also supports exploration of the glass energy landscape. To enforce structural realism and physical fidelity, we augment GlassVAE with two physics informed regularizers, a radial distribution function (RDF) loss that captures characteristic short and medium range ordering and an energy regression loss that reflects the broad configurational energetics. Both theoretical analysis and experimental results highlight the critical impact of these regularizers. By encoding high dimensional atomistic data into a compact latent vector and decoding it into structures with accurate energy predictions, GlassVAE provides a fast, physics aware path for modeling and designing disordered materials.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Based Aerospace Engineering -- A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2505.10142</link>
<guid>https://arxiv.org/abs/2505.10142</guid>
<content:encoded><![CDATA[
<div> Knowledge-Based Engineering, aerospace industry, knowledge management, SWARM-SLR, knowledge graph<br />
<br />
Summary: 
The study explores state-of-the-art knowledge management practices in aerospace engineering. It involves an extensive review of over 1,000 articles using SWARM-SLR methodology, qualitative analysis of 164 selected articles, and input from aerospace engineering domain experts. The research results include the creation of a knowledge graph comprising over 700 aerospace engineering processes, software, and data, formalized in the Web Ontology Language (OWL) and mapped to Wikidata entries. The knowledge graph is available on the Open Research Knowledge Graph (ORKG) and an aerospace Wikibase for further collaboration and knowledge exchange. The study highlights the importance of structured, semantic-based approaches in managing aerospace engineering knowledge, aiming to improve design processes, foster collaboration, and promote sustainable aviation practices. The intermediate and final artifacts of the study are documented in a Zenodo dataset for wider dissemination and reuse. <div>
arXiv:2505.10142v1 Announce Type: new 
Abstract: The aerospace industry operates at the frontier of technological innovation while maintaining high standards regarding safety and reliability. In this environment, with an enormous potential for re-use and adaptation of existing solutions and methods, Knowledge-Based Engineering (KBE) has been applied for decades. The objective of this study is to identify and examine state-of-the-art knowledge management practices in the field of aerospace engineering. Our contributions include: 1) A SWARM-SLR of over 1,000 articles with qualitative analysis of 164 selected articles, supported by two aerospace engineering domain expert surveys. 2) A knowledge graph of over 700 knowledge-based aerospace engineering processes, software, and data, formalized in the interoperable Web Ontology Language (OWL) and mapped to Wikidata entries where possible. The knowledge graph is represented on the Open Research Knowledge Graph (ORKG), and an aerospace Wikibase, for reuse and continuation of structuring aerospace engineering knowledge exchange. 3) Our resulting intermediate and final artifacts of the knowledge synthesis, available as a Zenodo dataset. This review sets a precedent for structured, semantic-based approaches to managing aerospace engineering knowledge. By advancing these principles, research, and industry can achieve more efficient design processes, enhanced collaboration, and a stronger commitment to sustainable aviation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Space-Time Multigrid Methods Suitable for Topology Optimisation of Transient Heat Conduction</title>
<link>https://arxiv.org/abs/2505.10168</link>
<guid>https://arxiv.org/abs/2505.10168</guid>
<content:encoded><![CDATA[
<div> topology optimization, transient heat conduction, Space-Time MultiGrid methods, diffusivity, adjoint problem <br />
Summary: <br />
This paper introduces Space-Time MultiGrid (STMG) methods for topology optimization in transient heat conduction problems. The methods employ pointwise smoothers and uniform Cartesian space-time meshes, with a coarsening strategy based on the geometric mean of minimum and maximum diffusivity for high contrast problems. Different discretization methods for coarse levels were tested, with averaging thermal resistivities on finer levels showing best results for one-dimensional cases. A proposed coarsening strategy ensuring spatial resolution on coarse grids yielded mixed results. The STMG methods successfully served as a solver for one-dimensional topology optimization, including solving the adjoint problem. The methods were robust and converged reliably during optimization cycles. They proved effective for the adjoint problem even when the prolongation operator only forwards information in time, despite the adjoint problem moving backwards in time. <div>
arXiv:2505.10168v1 Announce Type: new 
Abstract: This paper presents Space-Time MultiGrid (STMG) methods which are suitable for performing topology optimisation of transient heat conduction problems. The proposed methods use a pointwise smoother and uniform Cartesian space-time meshes. For problems with high contrast in the diffusivity, it was found that it is beneficial to define a coarsening strategy based on the geometric mean of the minimum and maximum diffusivity. However, other coarsening strategies may be better for other smoothers. Several methods of discretising the coarse levels were tested. Of these, it was best to use a method which averages the thermal resistivities on the finer levels. However, this was likely a consequence of the fact that only one spatial dimension was considered for the test problems. A second coarsening strategy was proposed which ensures spatial resolution on the coarse grids. Mixed results were found for this strategy. The proposed STMG methods were used as a solver for a one-dimensional topology optimisation problem. In this context, the adjoint problem was also solved using the STMG methods. The STMG methods were sufficiently robust for this application, since they converged during every optimisation cycle. It was found that the STMG methods also work for the adjoint problem when the prolongation operator only sends information forwards in time, even although the direction of time for the adjoint problem is backwards.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avocado Price Prediction Using a Hybrid Deep Learning Model: TCN-MLP-Attention Architecture</title>
<link>https://arxiv.org/abs/2505.09907</link>
<guid>https://arxiv.org/abs/2505.09907</guid>
<content:encoded><![CDATA[
<div> Keywords: Hass avocados, price forecasting, deep learning, TCN-MLP-Attention Architecture, agricultural markets <br />
Summary:  
- The study addresses the importance of price forecasting for high-value crops like Hass avocados due to the demand for healthy foods.
- The proposed hybrid deep learning model, TCN-MLP-Attention Architecture, combines TCN, MLP, and an Attention mechanism to handle complex price fluctuations influenced by various factors.
- The dataset consists of over 50,000 records of Hass avocado sales in the U.S. from 2015 to 2018, including variables like sales volume, average price, region, weather, and variety type.
- After systematic preprocessing, the model demonstrated excellent predictive performance, outperforming traditional methods with an RMSE of 1.23 and an MSE of 1.51.
- The research offers a scalable and effective approach for time series forecasting in agricultural markets, providing valuable insights for intelligent supply chain management and price strategy optimization. 

<br /><br />Summary: <div>
arXiv:2505.09907v1 Announce Type: cross 
Abstract: With the growing demand for healthy foods, agricultural product price forecasting has become increasingly important. Hass avocados, as a high-value crop, exhibit complex price fluctuations influenced by factors such as seasonality, region, and weather. Traditional prediction models often struggle with highly nonlinear and dynamic data. To address this, we propose a hybrid deep learning model, TCN-MLP-Attention Architecture, combining Temporal Convolutional Networks (TCN) for sequential feature extraction, Multi-Layer Perceptrons (MLP) for nonlinear interactions, and an Attention mechanism for dynamic feature weighting. The dataset used covers over 50,000 records of Hass avocado sales across the U.S. from 2015 to 2018, including variables such as sales volume, average price, time, region, weather, and variety type, collected from point-of-sale systems and the Hass Avocado Board. After systematic preprocessing, including missing value imputation and feature normalization, the proposed model was trained and evaluated. Experimental results demonstrate that the TCN-MLP-Attention model achieves excellent predictive performance, with an RMSE of 1.23 and an MSE of 1.51, outperforming traditional methods. This research provides a scalable and effective approach for time series forecasting in agricultural markets and offers valuable insights for intelligent supply chain management and price strategy optimization.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Foundation Model for Chemical Reactor Modeling: Meta-Learning with Physics-Informed Adaptation</title>
<link>https://arxiv.org/abs/2405.11752</link>
<guid>https://arxiv.org/abs/2405.11752</guid>
<content:encoded><![CDATA[
<div> neural network framework, chemical reactors, meta-learning, physics-informed, generalizable

Summary:
This work introduces a neural network framework for chemical reactor modeling that can generalize across different reactor types and quickly adapt to new chemical processes. By leveraging meta-learning, the model is pretrained on a wide range of reactor dynamics, allowing for efficient adaptation to unseen reactions with minimal data. Additionally, physics-informed fine-tuning is incorporated to ensure consistent adaptation to new reactor conditions. The framework is evaluated on three fundamental reactor types, showing superior few-shot adaptation compared to traditional approaches. By combining meta-learning with physics-informed techniques, this work paves the way for a more generalizable modeling framework in chemical engineering applications. <div>
arXiv:2405.11752v3 Announce Type: replace 
Abstract: Developing accurate models for chemical reactors is often challenging due to the complexity of reaction kinetics and process dynamics. Traditional approaches require retraining models for each new system, limiting generalizability and efficiency. In this work, we take a step toward foundation models for chemical reactor modeling by introducing a neural network framework that generalizes across diverse reactor types and rapidly adapts to new chemical processes. Our approach leverages meta-learning to pretrain the model on a broad set of reactor dynamics, enabling efficient adaptation to unseen reactions with minimal data. To further enhance generalizability, we incorporate physics-informed fine-tuning, ensuring physically consistent adaptation to new reactor conditions. Our framework is evaluated across three integer-order fundamental reactor types - continuous stirred tank reactors, batch reactors, and plug flow reactors - demonstrating superior few-shot adaptation compared to conventional data-driven, physics-informed, and transfer learning approaches. By combining meta-learning with physics-informed adaptation, this work lays the foundation for a generalizable modeling framework, advancing the development of foundation models for chemical engineering applications. Source code is available at https://github.com/killingbear999/chemical-reactor-foundation-model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2406.00367</link>
<guid>https://arxiv.org/abs/2406.00367</guid>
<content:encoded><![CDATA[
<div> RoBERTa-BiLSTM, sentiment analysis, deep learning, Transformer, latent intentions
<br />
Summary:
The article introduces a hybrid deep learning model, RoBERTa-BiLSTM, combining RoBERTa and BiLSTM for sentiment analysis. Challenges in sentiment analysis include lexical diversity, long dependencies, unknown symbols, and imbalanced datasets. Existing models like BERT and RoBERTa face issues with processing time due to sequential nature. The RoBERTa-BiLSTM model aims to address these challenges by leveraging RoBERTa for word embeddings and BiLSTM for contextual semantics. Experimental results on IMDb, Twitter US Airline, and Sentiment140 datasets show RoBERTa-BiLSTM outperforming baseline models like BERT and RoBERTa, achieving high accuracies and F1 scores. The model achieves accuracies of 80.74%, 92.36%, and 82.25% on the respective datasets, with corresponding F1 scores. This hybrid approach demonstrates improved performance in sentiment analysis tasks. 
<br /> <div>
arXiv:2406.00367v2 Announce Type: replace-cross 
Abstract: Effectively analyzing the comments to uncover latent intentions holds immense value in making strategic decisions across various domains. However, several challenges hinder the process of sentiment analysis including the lexical diversity exhibited in comments, the presence of long dependencies within the text, encountering unknown symbols and words, and dealing with imbalanced datasets. Moreover, existing sentiment analysis tasks mostly leveraged sequential models to encode the long dependent texts and it requires longer execution time as it processes the text sequentially. In contrast, the Transformer requires less execution time due to its parallel processing nature. In this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM, which combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with Bidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to generate meaningful word embedding vectors, while BiLSTM effectively captures the contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid model leverages the strengths of both sequential and Transformer models to enhance performance in sentiment analysis. We conducted experiments using datasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the proposed model against existing state-of-the-art methods. Our experimental findings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models (e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies of 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140 datasets, respectively. Additionally, the model achieves F1-scores of 80.73%, 92.35%, and 82.25% on the same datasets, respectively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Digital Twins with Quantified Uncertainty for Patient-Specific Decision Making in Oncology</title>
<link>https://arxiv.org/abs/2505.08927</link>
<guid>https://arxiv.org/abs/2505.08927</guid>
<content:encoded><![CDATA[
<div> methodology, digital twins, personalized medicine, uncertainty quantification, tumor progression

Summary: 
This study introduces an end-to-end methodology for personalized modeling in biomedicine, using digital twins to improve individual patient outcomes by combining patient data with mechanistic models of disease progression. The approach integrates longitudinal non-invasive imaging data with a reaction-diffusion model to estimate and predict spatiotemporal tumor progression while considering patient-specific anatomy. By solving a statistical inverse problem, imaging data inform spatially varying model parameters. An efficient parallel implementation of the forward model and scalable Bayesian posterior distribution approximation enable rigorous uncertainty quantification due to sparse, noisy measurements. The methodology is validated using synthetic data on a virtual patient to account for model limitations, noise level, and data frequency. Decision-making applications are demonstrated by evaluating the impact of imaging frequency and optimal experimental design. Clinical relevance is illustrated through model validation on a cohort of patients with longitudinal imaging data, showcasing the potential for risk-informed personalized medicine. 

<br /><br />Summary: <div>
arXiv:2505.08927v1 Announce Type: new 
Abstract: Quantifying the uncertainty in predictive models is critical for establishing trust and enabling risk-informed decision making for personalized medicine. In contrast to one-size-fits-all approaches that seek to mitigate risk at the population level, digital twins enable personalized modeling thereby potentially improving individual patient outcomes. Realizing digital twins in biomedicine requires scalable and efficient methods to integrate patient data with mechanistic models of disease progression. This study develops an end-to-end data-to-decisions methodology that combines longitudinal non-invasive imaging data with mechanistic models to estimate and predict spatiotemporal tumor progression accounting for patient-specific anatomy. Through the solution of a statistical inverse problem, imaging data inform the spatially varying parameters of a reaction-diffusion model of tumor progression. An efficient parallel implementation of the forward model coupled with a scalable approximation of the Bayesian posterior distribution enables rigorous, but tractable, quantification of uncertainty due to the sparse, noisy measurements. The methodology is verified on a virtual patient with synthetic data to control for model inadequacy, noise level, and the frequency of data collection. The application to decision-making is illustrated by evaluating the importance of imaging frequency and formulating an optimal experimental design question. The clinical relevance is demonstrated through a model validation study on a cohort of patients with publicly available longitudinal imaging data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of the initial post-buckling response of trusses and frames by an asymptotic approach</title>
<link>https://arxiv.org/abs/2505.09373</link>
<guid>https://arxiv.org/abs/2505.09373</guid>
<content:encoded><![CDATA[
<div> asymptotic theory, post-buckling, trusses, frames, topology optimization
<br />
Summary: 
The article explores the application of asymptotic post-buckling theory in sizing and topology optimization of trusses and frames, highlighting its potential benefits and existing computational challenges. By incorporating the lowest two asymptotic coefficients in the optimization formulation, representing initial post-buckling slope and curvature, designers can control the post-buckling response and reduce imperfection sensitivity in optimized designs. The asymptotic expansion enables the approximation of structural nonlinear response, allowing optimization for specific measures of nonlinear mechanical performance such as end-compliance or complementary work. The study demonstrates the effective use of the asymptotic method in including post-buckling constraints in structural optimization through examples of linear and nonlinear compliance minimization for trusses and frames. <div>
arXiv:2505.09373v1 Announce Type: new 
Abstract: Asymptotic post-buckling theory is applied to sizing and topology optimization of trusses and frames, exploring its potential and current computational difficulties. We show that a designs' post-buckling response can be controlled by including the lowest two asymptotic coefficients, representing the initial post-buckling slope and curvature, in the optimization formulation. This also reduces the imperfection sensitivity of the optimized design. The asymptotic expansion can further be used to approximate the structural nonlinear response, and then to optimize for a given measure of the nonlinear mechanical performance such as, for example, end-compliance or complementary work. Examples of linear and nonlinear compliance minimization of trusses and frames show the effective use of the asymptotic method for including post-buckling constraints in structural optimization.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radon Exposure Dataset</title>
<link>https://arxiv.org/abs/2505.09489</link>
<guid>https://arxiv.org/abs/2505.09489</guid>
<content:encoded><![CDATA[
<div> Keywords: radon, lung cancer, dataset, modeling, prediction

Summary: 
The study focuses on creating a comprehensive dataset for modeling and predicting household radon concentrations at small scales in Pennsylvania and Utah. It aims to identify at-risk populations in areas with high radon levels by examining geological and demographic factors. The dataset combines information on temperature, geochemistry, soil characteristics, and demographic variables such as heating fuel used and building age. This data serves as a foundational resource for future studies and can be scaled up to predict radon exposure potential at a national level. By identifying populations at risk, this research helps in mitigating the risk of lung cancer due to elevated radon levels in homes.<br /><br />Summary: <div>
arXiv:2505.09489v1 Announce Type: new 
Abstract: Exposure to elevated radon levels in the home is one of the leading causes of lung cancer in the world. The following study describes the creation of a comprehensive, state-level dataset designed to enable the modeling and prediction of household radon concentrations at Zip Code Tabulation Area (ZCTA) and sub-kilometer scales. Details include the data collection and processing involved in compiling physical and demographic factors for Pennsylvania and Utah. Attempting to mitigate this risk requires identifying the underlying geological causes and the populations that might be at risk. This work focuses on identifying at-risk populations throughout Pennsylvania and Utah, where radon levels are some of the highest in the country. The resulting dataset harmonizes geological and demographic factors from various sources and spatial resolutions, including temperature, geochemistry, and soil characteristics. Demographic variables such as the household heating fuel used, the age of building, and the housing type provide further insight into which populations could be most susceptible in areas with potentially high radon levels. This dataset also serves as a foundational resource for two other studies conducted by the authors. The resolution of the data provides a novel approach to predicting potential radon exposure, and the data processing conducted for these states can be scaled up to larger spatial resolutions (e.g., the Contiguous United States [CONUS]) and allow for a broad reclassification of radon exposure potential in the United States.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep-MacroFin: Informed Equilibrium Neural Network for Continuous Time Economic Models</title>
<link>https://arxiv.org/abs/2408.10368</link>
<guid>https://arxiv.org/abs/2408.10368</guid>
<content:encoded><![CDATA[
<div> Framework, Deep learning, Partial differential equations, Continuous time economics, Neural networks

Summary: 
The paper introduces Deep-MacroFin, a framework utilizing deep learning techniques to solve partial differential equations, specifically focusing on continuous time economic models. The framework incorporates Multi-Layer Perceptrons and Kolmogorov- Arnold Networks, optimized using Hamilton-Jacobi-Bellman equations and algebraic equations for economic information. It aims to efficiently solve high-dimensional problems with reduced computational requirements compared to traditional methods. Deep-MacroFin offers a user-friendly implementation with significant memory and computational efficiency improvements, making it adaptable to high-dimensional systems of equations. Additionally, a time-stepping scheme enhances training stability, enabling the solution of nonlinear HJB equations and 50-dimensional economic models. <div>
arXiv:2408.10368v4 Announce Type: replace-cross 
Abstract: In this paper, we present Deep-MacroFin, a comprehensive framework designed to solve partial differential equations, with a particular focus on models in continuous time economics. This framework leverages deep learning methodologies, including Multi-Layer Perceptrons and the newly developed Kolmogorov-Arnold Networks. It is optimized using economic information encapsulated by Hamilton-Jacobi-Bellman (HJB) equations and coupled algebraic equations. The application of neural networks holds the promise of accurately resolving high-dimensional problems with fewer computational demands and limitations compared to other numerical methods. This framework can be readily adapted for systems of partial differential equations in high dimensions. Importantly, it offers a more efficient (5$\times$ less CUDA memory and 40$\times$ fewer FLOPs in 100D problems) and user-friendly implementation than existing libraries. We also incorporate a time-stepping scheme to enhance training stability for nonlinear HJB equations, enabling the solution of 50D economic models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
<link>https://arxiv.org/abs/2505.08316</link>
<guid>https://arxiv.org/abs/2505.08316</guid>
<content:encoded><![CDATA[
<div> Keywords: ventral visual stream, object recognition, relative position prediction, unsupervised task-driven method, brain similarity<br />
Summary:<br />
The article introduces a new function of the ventral visual stream (VVS) called relative position (RP) prediction, in addition to its role in object recognition. It criticizes current unsupervised task-driven methods for modeling VVS through contrastive learning, arguing that this approach may not capture the capabilities of RP prediction. To address this, a new method that integrates RP learning with contrastive learning is proposed. Experimental results demonstrate that this new approach improves object recognition performance and enhances RP predictivity, ultimately leading to better model brain similarity. These findings suggest that the VVS may play a role in location perception, particularly in RP prediction, highlighting the need for a more comprehensive understanding of its functions. <br /><br />Summary: <div>
arXiv:2505.08316v1 Announce Type: new 
Abstract: Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology and geometry optimization of grid-shells under self-weight loading</title>
<link>https://arxiv.org/abs/2505.08645</link>
<guid>https://arxiv.org/abs/2505.08645</guid>
<content:encoded><![CDATA[
<div> Keywords: grid-shell structures, optimization, connectivity, elevation, second-order cone optimization

Summary:<br /><br />This manuscript presents an approach for optimizing the connectivity and elevation of grid-shell structures under pure compression or tension, considering external loading and self-weight. The method involves solving a second-order cone optimization problem to ensure convexity and globally optimal solutions. Numerical examples demonstrate the characteristics of these optimal structures, showing the importance of simultaneous topology and geometry optimization. The study reveals that as self-weight increases, optimal topology and elevation profiles change significantly. The approach provides more accurate solutions and is vastly quicker than traditional 3D layout/truss topology optimization methods. This research underscores the significance of integrating topology and geometry optimization from the initial design phase for efficient and effective structural design. <div>
arXiv:2505.08645v1 Announce Type: new 
Abstract: This manuscript presents an approach for simultaneously optimizing the connectivity and elevation of grid-shell structures acting in pure compression (or pure tension) under the combined effects of a prescribed external loading and the design-dependent self-weight of the structure itself. The method derived herein involves solving a second-order cone optimization problem, thereby ensuring convexity and obtaining globally optimal results for a given discretization of the design domain. Several numerical examples are presented, illustrating characteristics of this class of optimal structures. It is found that, as self-weight becomes more significant, both the optimal topology and the optimal elevation profile of the structure change, highlighting the importance of optimizing both topology and geometry simultaneously from the earliest stages of design. It is shown that this approach can obtain solutions with greater accuracy and several orders of magnitude more quickly than a standard 3D layout/truss topology optimization approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Value of Information-based assessment of strain-based thickness loss monitoring in ship hull structures</title>
<link>https://arxiv.org/abs/2505.07427</link>
<guid>https://arxiv.org/abs/2505.07427</guid>
<content:encoded><![CDATA[
<div> Bayesian analysis, Structural Health Monitoring, ship hulls, corrosion-induced thickness loss, decision analysis<br />
<br />
Summary: This study explores the value of information from Structural Health Monitoring (SHM) systems monitoring corrosion-induced thickness loss (CITL) in ship hulls. The research utilizes a Bayesian pre-posterior decision analysis to quantify the benefits of SHM in optimizing hull maintenance. Decision consequence cost functions based on exceedance probabilities relative to a target CITL threshold are defined, allowing for practical decision-making based on risk perception. A high-fidelity numerical model of a commercial vessel is used to compare the benefits of different CITL monitoring strategies, including strain-based SHM and traditional on-site inspections. The findings provide insights into the potential of SHM in enhancing maintenance practices for ship structures. <br /><br /> <div>
arXiv:2505.07427v1 Announce Type: cross 
Abstract: Recent advances in Structural Health Monitoring (SHM) have attracted industry interest, yet real-world applications, such as in ship structures remain scarce. Despite SHM's potential to optimise maintenance, its adoption in ships is limited due to the lack of clearly quantifiable benefits for hull maintenance. This study employs a Bayesian pre-posterior decision analysis to quantify the value of information (VoI) from SHM systems monitoring corrosion-induced thickness loss (CITL) in ship hulls, in a first-of-its-kind analysis for ship structures. We define decision-making consequence cost functions based on exceedance probabilities relative to a target CITL threshold, which can be set by the decision-maker. This introduces a practical aspect to our framework, that enables implicitly modelling the decision-maker's risk perception. We apply this framework to a large-scale, high-fidelity numerical model of a commercial vessel and examine the relative benefits of different CITL monitoring strategies, including strain-based SHM and traditional on-site inspections.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QUEST: QUantum-Enhanced Shared Transportation</title>
<link>https://arxiv.org/abs/2505.08074</link>
<guid>https://arxiv.org/abs/2505.08074</guid>
<content:encoded><![CDATA[
<div> framework, shared transportation, Quantum-Enhanced Shared Transportation, windbreaker, windsurfer
Summary:
Quantum-Enhanced Shared Transportation introduces Windbreaking-as-a-Service (WaaS) for shared transportation using larger "windbreaker" vehicles to provide aerodynamic shelter for smaller "windsurfer" vehicles. The computational framework QUEST solves the matching and assignment problems by formulating them as a mixed-integer quadratic problem and encoding them as a Quadratic Unconstrained Binary Optimization for quantum processing. Classical methods such as the Hungarian Algorithm and quantum algorithms like Quantum Approximate Optimization Algorithm (QAOA) are used to solve the assignment problem. The quantum implementation successfully identifies the optimal assignment, demonstrating the effectiveness of the QUEST pipeline for controlled prototypes. This study paves the way for addressing more complex scenarios and leveraging quantum technologies for large-scale shared-transportation instances. <br /><br />Summary: <div>
arXiv:2505.08074v1 Announce Type: cross 
Abstract: We introduce ``Windbreaking-as-a-Service'' (WaaS) as an innovative approach to shared transportation in which larger ``windbreaker'' vehicles provide aerodynamic shelter for ``windsurfer'' vehicles, thereby reducing drag and fuel consumption. As a computational framework to solve the large-scale matching and assignment problems that arise in WaaS, we present \textbf{QUEST} (Quantum-Enhanced Shared Transportation). Specifically, we formulate the pairing of windbreakers and windsurfers -- subject to timing, speed, and vehicle-class constraints -- as a mixed-integer quadratic problem (MIQP). Focusing on a single-segment prototype, we verify the solution classically via the Hungarian Algorithm, a Gurobi-based solver, and brute-force enumeration of binary vectors. We then encode the problem as a Quadratic Unconstrained Binary Optimization (QUBO) and map it to an Ising Hamiltonian, enabling the use of the Quantum Approximate Optimization Algorithm (QAOA) and other quantum and classical annealing technologies. Our quantum implementation successfully recovers the optimal assignment identified by the classical methods, confirming the soundness of the QUEST pipeline for a controlled prototype. While QAOA and other quantum heuristics do not guarantee a resolution of the fundamental complexity barriers, this study illustrates how the WaaS problem can be systematically translated into a quantum-ready model. It also lays the groundwork for addressing multi-segment scenarios and potentially leveraging quantum advantage for large-scale shared-transportation instances.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations</title>
<link>https://arxiv.org/abs/2505.08740</link>
<guid>https://arxiv.org/abs/2505.08740</guid>
<content:encoded><![CDATA[
<div> sensitivity-based regularization, Fourier Neural Operator, inverse problems, parameter inversion, high-dimensional parameter spaces <br />
Summary: <br />
The article introduces Sensitivity-Constrained Fourier Neural Operators (SC-FNO) to address limitations faced by deep learning frameworks like the Fourier Neural Operator (FNO) in solving parametric differential equations. SC-FNO includes a sensitivity-based regularization strategy that improves accuracy in predicting solution paths and outperforms standard FNO and FNO with physics-informed regularization. It is particularly effective in parameter inversion tasks and can handle high-dimensional parameter spaces with up to 82 parameters. SC-FNO reduces data and training requirements while achieving high performance across different types of differential equations and neural operators. Although there is a slight increase in training time, the benefits of SC-FNO are significant in terms of accuracy and scalability. The code and selected experiments for SC-FNO are available on GitHub for further exploration and implementation. <div>
arXiv:2505.08740v1 Announce Type: cross 
Abstract: Parametric differential equations of the form du/dt = f(u, x, t, p) are fundamental in science and engineering. While deep learning frameworks such as the Fourier Neural Operator (FNO) can efficiently approximate solutions, they struggle with inverse problems, sensitivity estimation (du/dp), and concept drift. We address these limitations by introducing a sensitivity-based regularization strategy, called Sensitivity-Constrained Fourier Neural Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths and consistently outperforms standard FNO and FNO with physics-informed regularization. It improves performance in parameter inversion tasks, scales to high-dimensional parameter spaces (tested with up to 82 parameters), and reduces both data and training requirements. These gains are achieved with a modest increase in training time (30% to 130% per epoch) and generalize across various types of differential equations and neural operators. Code and selected experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles</title>
<link>https://arxiv.org/abs/2505.08782</link>
<guid>https://arxiv.org/abs/2505.08782</guid>
<content:encoded><![CDATA[
<div> Quantum Machine Learning, NISQ devices, multi-chip ensemble VQC framework, scalability, trainability, noise resilience<br />
Summary:<br />
The article introduces a multi-chip ensemble VQC framework to address limitations of noisy intermediate-scale quantum (NISQ) devices in Quantum Machine Learning (QML). This framework partitions computations across smaller quantum chips to enhance scalability, trainability, and noise resilience. It mitigates barren plateaus, reduces quantum error bias and variance, and maintains robust generalization through controlled entanglement. The framework is designed to work with current and future quantum hardware, making it suitable for scalable QML on near-term devices. Experimental validation was done on standard benchmark datasets (MNIST, FashionMNIST, CIFAR-10) and a real-world dataset (PhysioNet EEG), demonstrating the framework's potential in enabling practical QML applications. <br /><br />Summary: <div>
arXiv:2505.08782v1 Announce Type: cross 
Abstract: Quantum Machine Learning (QML) holds significant promise for solving computational challenges across diverse domains. However, its practical deployment is constrained by the limitations of noisy intermediate-scale quantum (NISQ) devices, including noise, limited scalability, and trainability issues in variational quantum circuits (VQCs). We introduce the multi-chip ensemble VQC framework, which partitions high-dimensional computations across smaller quantum chips to enhance scalability, trainability, and noise resilience. We show that this approach mitigates barren plateaus, reduces quantum error bias and variance, and maintains robust generalization through controlled entanglement. Designed to align with current and emerging quantum hardware, the framework demonstrates strong potential for enabling scalable QML on near-term devices, as validated by experiments on standard benchmark datasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet EEG).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAAC panels can suddenly collapse before any warning of corrosion-induced surface cracking</title>
<link>https://arxiv.org/abs/2505.06294</link>
<guid>https://arxiv.org/abs/2505.06294</guid>
<content:encoded><![CDATA[
<div> Keywords: RAAC, corrosion, modeling, porosity, collapse  

<br /><br />Summary: The study addresses the issue of reinforced autoclaved aerated concrete (RAAC) panel collapses, which have gained significant attention due to safety concerns. A lack of detailed experimental data and the lengthy process of replicating natural corrosion has underscored the importance of computational modeling in this context. Researchers suspect that the high porosity of RAAC contributes to corrosion concealment; however, existing models for corrosion-induced cracking have limited capabilities in incorporating concrete porosity effects. To improve this, the authors introduce an enriched model that integrates analytical solutions of reactive transport equations with a porosity-dependent diffusivity description. This innovative approach allows for the first computational exploration of corrosion concealment in RAAC panels. The findings reveal that RAAC panels can experience sudden collapses without visible indicators of surface cracking due to corrosion. Additionally, the research identifies the specific conditions that increase the likelihood of sudden collapse, enhancing the understanding of RAAC durability under corrosive environments. This work provides crucial insights for engineers and researchers concerned with the structural integrity and safety of RAAC constructions. <div>
arXiv:2505.06294v1 Announce Type: new 
Abstract: The collapse of reinforced autoclaved aerated concrete (RAAC) panels has attracted considerable public and academic interest. As detailed experimental data are not yet available and replicating the natural corrosion process requires years or decades, computational modelling is essential to understand under which conditions corrosion remains concealed. The very high porosity of RAAC is widely suspected to be a major contributing factor. However, current corrosion-induced cracking models are known to struggle with capturing the role of concrete porosity. To remedy this critical deficiency, we propose to enrich corrosion-induced cracking modelling with the analytical solution of reactive transport equations governing the precipitation of rust and a porosity-dependent description of diffusivity. With this, the corrosion concealment in RAAC panels is studied computationally for the first time, revealing that RAAC panels can suddenly collapse before any warning of corrosion-induced surface cracking and allowing to map the conditions most likely to result in sudden collapse.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New DAPO Algorithm for Stock Trading</title>
<link>https://arxiv.org/abs/2505.06408</link>
<guid>https://arxiv.org/abs/2505.06408</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, financial trading, Dynamic Sampling Policy Optimization, Large Language Models, trading agent <br />
Summary:
The study explores the application of reinforcement learning techniques, specifically the Dynamic Sampling Policy Optimization (DAPO), in financial trading using large language models (LLMs) for risk and sentiment analysis. By combining an improved Group Relative Policy Optimization (GRPO) algorithm with LLM-based signals, a trading agent was developed and tested on the NASDAQ-100 index. Results showed a cumulative return of 230.49% and an information ratio of 0.37, surpassing the CPPO-DeepSeek baseline. Additionally, the agent's training time was reduced from 8 to 2.5 hours over 100 epochs, with decreased RAM usage. This RL-LLM framework demonstrates potential for data-efficient trading agents, offering scalability and improved performance in financial markets. <br /> <div>
arXiv:2505.06408v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning, such as Dynamic Sampling Policy Optimization (DAPO), show strong performance when paired with large language models (LLMs). Motivated by this success, we ask whether similar gains can be realized in financial trading. We design a trading agent that combines an improved Group Relative Policy Optimization (GRPO) algorithm, augmented with ideas from DAPO, with LLM-based risk and sentiment signals extracted from financial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains a cumulative return of 230.49 percent and an information ratio of 0.37, outperforming the CPPO-DeepSeek baseline. It also cuts training time from about 8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. The proposed RL-LLM framework offers a scalable path toward data-efficient trading agents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Efficient Ternary Encoding for High-Speed Data Transmission in 3D-Integrated Circuits Using Inductive Coupling Links</title>
<link>https://arxiv.org/abs/2505.06908</link>
<guid>https://arxiv.org/abs/2505.06908</guid>
<content:encoded><![CDATA[
<div> Inductive coupling links, 3D-integrated circuits, ternary signalling scheme, crosstalk reduction, electromagnetic interference<br />
<br />
Summary:<br />
This paper introduces a novel ternary signalling scheme for inductive coupling links (ICLs) in 3D-integrated circuits (3D-ICs) to mitigate crosstalk and electromagnetic interference. By utilizing three voltage levels (-V, 0V, +V) instead of the traditional binary approach, this scheme enhances signal separation, decreases crosstalk, and enhances signal integrity. The ternary system allows for increased bandwidth efficiency and reduced power consumption as compared to Non-Return to Zero (NRZ) systems due to fewer signal transitions. A modified H-Bridge transmitter is used to generate ternary symbols by regulating current flow based on binary-to-ternary mapping. Initial simulations corroborate the effectiveness of the scheme, demonstrating minimized power consumption and higher data rates in comparison with NRZ. This innovative approach presents potential benefits for high-performance computing and Internet of Things (IoT) devices in 3D-IC settings, providing enhanced noise resilience, reduced power consumption, and heightened communication efficiency. <div>
arXiv:2505.06908v1 Announce Type: new 
Abstract: This paper proposes a ternary signalling scheme for inductive coupling links (ICLs) in 3D-integrated circuits (3D-ICs) to reduce crosstalk and electromagnetic interference in multi-stacked chip communications. By converting binary data into ternary sequences with three voltage levels (-V, 0V, +V), the approach enhances signal separation, reduces crosstalk, and improves signal integrity. Unlike traditional Non-Return to Zero (NRZ) systems, the ternary scheme increases bandwidth efficiency and reduces power consumption through fewer signal transitions. A modified H-Bridge transmitter generates ternary symbols by controlling current flow based on binary-to-ternary mapping. Preliminary simulations validate the efficiency of the scheme, showing reduced power consumption and higher data rates compared to NRZ. This approach shows promise for high-performance computing and IoT devices in 3D-IC environments, offering enhanced noise resilience, lower power usage, and improved communication efficiency.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparative study of Bitcoin and Ripple cryptocurrencies trading using Deep Reinforcement Learning algorithms</title>
<link>https://arxiv.org/abs/2505.07660</link>
<guid>https://arxiv.org/abs/2505.07660</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Reinforcement Learning, Bitcoin, Ripple, Trading Strategies

<br /><br />Summary: 
This research focuses on the application of Artificial Intelligence (AI) in automated trading, specifically targeting the challenges presented by the volatile and dynamic nature of financial asset prices. The study emphasizes developing an innovative rule-based strategy to train Deep Reinforcement Learning (DRL) models for trading Bitcoin (BTC) and Ripple (XRP). Key methodologies employed include Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, and Advantage Actor-Critic algorithms, all aimed at deriving optimal trading policies. To assess the effectiveness of the proposed DRL approach, the primary performance metrics utilized are portfolio wealth and trade signals. Experimental results reveal that both Dueling and Double Deep Q-Networks exhibit superior performance when applied to XRP, as evidenced by increased portfolio wealth. The research highlights the potential of AI-driven techniques in improving trading strategies and their adaptability to fluctuating market conditions. All code related to the study is made publicly accessible through a GitHub repository, fostering transparency and collaboration within the research community. <div>
arXiv:2505.07660v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has demonstrated remarkable success across various applications. In light of this trend, the field of automated trading has developed a keen interest in leveraging AI techniques to forecast the future prices of financial assets. This interest stems from the need to address trading challenges posed by the inherent volatility and dynamic nature of asset prices. However, crafting a flawless strategy becomes a formidable task when dealing with assets characterized by intricate and ever-changing price dynamics. To surmount these formidable challenges, this research employs an innovative rule-based strategy approach to train Deep Reinforcement Learning (DRL). This application is carried out specifically in the context of trading Bitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration of Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, alongside the Advantage Actor-Critic algorithms. Each of them aims to yield an optimal policy for our application. To evaluate the effectiveness of our Deep Reinforcement Learning (DRL) approach, we rely on portfolio wealth and the trade signal as performance metrics. The experimental outcomes highlight that Duelling and Double Deep Q-Network outperformed when using XRP with the increasing of the portfolio wealth. All codes are available in this \href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\color{blue}Github link}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALFEE: Adaptive Large Foundation Model for EEG Representation</title>
<link>https://arxiv.org/abs/2505.06291</link>
<guid>https://arxiv.org/abs/2505.06291</guid>
<content:encoded><![CDATA[
<div> framework, EEG, representation, transformer, pretraining
Summary:
The article introduces the Adaptive Large Foundation model for EEG signal representation (ALFEE) framework, which aims to enhance the representation learning for EEG signals. The framework consists of a hybrid transformer architecture with two learning stages. It utilizes a hybrid attention mechanism to separate channel-wise feature aggregation and temporal dynamics modeling, allowing for robust representation of EEG signals with variable channel configurations. ALFEE includes a channel encoder, temporal encoder, and hybrid decoder to optimize task prediction, channel and temporal mask reconstruction, and temporal forecasting during pretraining. The framework also incorporates full-model adaptation during fine-tuning to improve performance across multiple tasks. Experimental results on six downstream EEG tasks demonstrate the superior performance of ALFEE over existing models. The ALFEE framework provides a scalable foundation for biological signal analysis and is available for implementation on GitHub. 
<br /><br />Summary: <div>
arXiv:2505.06291v1 Announce Type: cross 
Abstract: While foundation models excel in text, image, and video domains, the critical biological signals, particularly electroencephalography(EEG), remain underexplored. EEG benefits neurological research with its high temporal resolution, operational practicality, and safety profile. However, low signal-to-noise ratio, inter-subject variability, and cross-paradigm differences hinder the generalization of current models. Existing methods often employ simplified strategies, such as a single loss function or a channel-temporal joint representation module, and suffer from a domain gap between pretraining and evaluation tasks that compromises efficiency and adaptability. To address these limitations, we propose the Adaptive Large Foundation model for EEG signal representation(ALFEE) framework, a novel hybrid transformer architecture with two learning stages for robust EEG representation learning. ALFEE employs a hybrid attention that separates channel-wise feature aggregation from temporal dynamics modeling, enabling robust EEG representation with variable channel configurations. A channel encoder adaptively compresses variable channel information, a temporal encoder captures task-guided evolution, and a hybrid decoder reconstructs signals in both temporal and frequency domains. During pretraining, ALFEE optimizes task prediction, channel and temporal mask reconstruction, and temporal forecasting to enhance multi-scale and multi-channel representation. During fine-tuning, a full-model adaptation with a task-specific token dictionary and a cross-attention layer boosts performance across multiple tasks. After 25,000 hours of pretraining, extensive experimental results on six downstream EEG tasks demonstrate the superior performance of ALFEE over existing models. Our ALFEE framework establishes a scalable foundation for biological signal analysis with implementation at https://github.com/xw1216/ALFEE.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations</title>
<link>https://arxiv.org/abs/2505.06502</link>
<guid>https://arxiv.org/abs/2505.06502</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Learning, Generative Adversarial Networks, Super Resolution, Physical Consistency, Scientific Applications

<br /><br />Summary: 
Machine Learning, specifically Generative Adversarial Networks (GANs), has significantly transformed Super Resolution (SR) image techniques. However, existing generated images often lack physical meaningfulness, crucial for scientific applications. The proposed PC-SRGAN addresses this limitation by enhancing image resolution while ensuring physical consistency, leading to interpretable simulations. This approach demonstrates notable improvements in the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure when compared to traditional methods, even achieving effective results with limited training data (utilizing just 13% of the data required for SRGAN). In addition to SR, PC-SRGAN incorporates physically meaningful machine learning by integrating numerically justified time integrators and advanced quality metrics, aiming for reliable and causal models in scientific fields. A key benefit of PC-SRGAN over conventional SR methods is its physical consistency, positioning it as a viable surrogate model for time-dependent problems. Ultimately, this advancement supports scientific machine learning by enhancing accuracy and efficiency in image processing, improving process understanding, and offering a broader range of applications in scientific research. The source codes and data will be publicly accessible at the provided GitHub link upon paper acceptance. <div>
arXiv:2505.06502v1 Announce Type: cross 
Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. The source codes and data will be made publicly available at https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery</title>
<link>https://arxiv.org/abs/2505.06795</link>
<guid>https://arxiv.org/abs/2505.06795</guid>
<content:encoded><![CDATA[
<div> Keywords: Commodity price volatility, multi-horizon forecasting, Regularized Sparse Autoencoder, interpretable latent drivers, deep learning framework

<br /><br />Summary: This paper addresses the challenges posed by commodity price volatility, emphasizing the need for accurate multi-horizon forecasting. Predicting prices of commodities such as copper and crude oil is complicated by various factors, including macroeconomic indicators, supply and demand dynamics, and geopolitical events. Existing forecasting models often lack transparency, which limits their strategic applications. To tackle this issue, the authors introduce the Regularized Sparse Autoencoder (RSAE), a deep learning framework that facilitates simultaneous forecasting across multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series data. Notably, L1 regularization is applied to the latent vector of the model to enforce sparsity, leading to more parsimonious explanations of underlying market dynamics, including demand and supply shocks. Drawing insights from energy-based models and sparse coding, the RSAE achieves a balance between predictive accuracy and the ability to learn interpretable representations. Furthermore, evaluations on historical data for Copper and Crude Oil indicate that the RSAE not only provides competitive forecasting accuracy but also offers data-driven insights into price dynamics, distinguishing it from traditional black-box forecasting approaches. <div>
arXiv:2505.06795v1 Announce Type: cross 
Abstract: Commodity price volatility creates economic challenges, necessitating accurate multi-horizon forecasting. Predicting prices for commodities like copper and crude oil is complicated by diverse interacting factors (macroeconomic, supply/demand, geopolitical, etc.). Current models often lack transparency, limiting strategic use. This paper presents a Regularized Sparse Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon commodity price prediction and discovery of interpretable latent market drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series. Crucially, L1 regularization ($\|\mathbf{z}\|_1$) on its latent vector $\mathbf{z}$ enforces sparsity, promoting parsimonious explanations of market dynamics through learned factors representing underlying drivers (e.g., demand, supply shocks). Drawing from energy-based models and sparse coding, the RSAE optimizes predictive accuracy while learning sparse representations. Evaluated on historical Copper and Crude Oil data with numerous indicators, our findings indicate the RSAE offers competitive multi-horizon forecasting accuracy and data-driven insights into price dynamics via its interpretable latent space, a key advantage over traditional black-box approaches.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?</title>
<link>https://arxiv.org/abs/2505.07078</link>
<guid>https://arxiv.org/abs/2505.07078</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, asset pricing, stock trading, backtesting framework, market regime analysis

Summary: 
The study evaluates the effectiveness of Large Language Models (LLMs) in timing-based investment strategies across a broader range of symbols and over a longer timeframe using the FINSABER backtesting framework. Results show that previously reported advantages of LLM strategies diminish significantly when evaluated over two decades and on over 100 symbols. The analysis reveals that LLM strategies are overly conservative in bull markets, leading to underperformance against passive benchmarks, and overly aggressive in bear markets, resulting in heavy losses. The study highlights the importance of developing LLM strategies that prioritize trend detection and incorporate regime-aware risk controls rather than solely focusing on complex frameworks. 

<br /><br />Summary: <div>
arXiv:2505.07078v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating many-engine spacecraft: Exceeding 100 trillion grid points via information~geometric regularization and the MFC flow solver</title>
<link>https://arxiv.org/abs/2505.07392</link>
<guid>https://arxiv.org/abs/2505.07392</guid>
<content:encoded><![CDATA[
<div> method, exascale simulations, compressible fluid flows, rocket craft, computational cost <br />
Summary:
This work presents a method for exascale simulations of high-speed compressible fluid flows, specifically targeting multi-engine rocket craft simulations. By optimizing the implementation through information geometric regularization and unified addressing on tightly coupled CPU-GPU platforms, significant improvements in computational cost and memory footprint are achieved. The need for numerical shock capturing is eliminated, enabling the simulation of fluid flows at unprecedented scales. The use of linear stencil algorithms, despite being memory-bound, results in faster wall clock times compared to baseline numerics. This allows for CFD simulations with over 100 trillion grid points, surpassing existing state-of-the-art simulations. Ideal weak scaling is demonstrated on OLCF Frontier and CSCS Alps supercomputers, showcasing the scalability of the proposed method on large-scale platforms. <div>
arXiv:2505.07392v1 Announce Type: cross 
Abstract: This work proposes a method and optimized implementation for exascale simulations of high-speed compressible fluid flows, enabling the simulation of multi-engine rocket craft at an unprecedented scale. We significantly improve upon the state-of-the-art in terms of computational cost and memory footprint through a carefully crafted implementation of the recently proposed information geometric regularization, which eliminates the need for numerical shock capturing. Unified addressing on tightly coupled CPU--GPU platforms increases the total problem size with negligible performance hit. Despite linear stencil algorithms being memory-bound, we achieve wall clock times that are four times faster than optimized baseline numerics. This enables the execution of CFD simulations at more than 100 trillion grid points, surpassing the largest state-of-the-art publicly available simulations by an order of magnitude. Ideal weak scaling is demonstrated on OLCF Frontier and CSCS Alps using the full system, entailing 37.8K AMD MI250X GPUs (Frontier) or 9.2K NVIDIA GH200 superchips (Alps).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transient Nonlinear Electrothermal Adjoint Sensitivity Analysis for HVDC Cable Joints</title>
<link>https://arxiv.org/abs/2405.14284</link>
<guid>https://arxiv.org/abs/2405.14284</guid>
<content:encoded><![CDATA[
<div> Adjoint variable method, coupled nonlinear transient electrothermal problems, sensitivities, high voltage direct current cable joints, material sensitivities <br />
Summary: <br />
Efficient computation of sensitivities is crucial for designing and optimizing high voltage direct current cable joints. This study introduces the adjoint variable method for solving coupled nonlinear transient electrothermal problems to efficiently compute material sensitivities for a 320kV cable joint specimen. The results are compared with sensitivities obtained using the direct sensitivity method, validating the effectiveness of the proposed approach. The method can handle a large number of design parameters, providing a robust tool for optimizing cable joint designs. This research contributes to improving the efficiency and accuracy of high voltage cable joint design processes, ultimately leading to better performance and reliability of electrical transmission systems. <div>
arXiv:2405.14284v3 Announce Type: replace 
Abstract: Efficient computation of sensitivities is a promising approach for efficiently of designing and optimizing high voltage direct current cable joints. This paper presents the adjoint variable method for coupled nonlinear transient electrothermal problems as an efficient approach to compute sensitivities with respect to a large number of design parameters. The method is used to compute material sensitivities of a 320kV high voltage direct current cable joint specimen. The results are validated against sensitivities obtained via the direct sensitivity method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Martinize2 and Vermouth: Unified Framework for Topology Generation</title>
<link>https://arxiv.org/abs/2212.01191</link>
<guid>https://arxiv.org/abs/2212.01191</guid>
<content:encoded><![CDATA[
<div> Keywords: force field, molecular dynamics, Martini simulations, high-throughput, Vermouth library

Summary:
The article discusses the development of the Vermouth python library for preparing, running, and analyzing Martini simulations of complex systems. The Martinize2 program, a part of Vermouth, is highlighted for its ability to handle protonation states, post-translation modifications, and structural biases such as the elastic network. It can also convert non-protein molecules like ligands. The Vermouth library addresses the need for automation tools in high-throughput simulations and studies of complex cellular systems. Demonstrated with two high-complexity benchmarks, Martinize2 successfully converts protein structures to coarse-grained resolution from databases like I-TASSER and AlphaFold. These conversions showcase the library's capability to ensure input structure quality for safeguarding high-throughput applications.

<br /><br />Summary: <div>
arXiv:2212.01191v4 Announce Type: replace-cross 
Abstract: Ongoing advances in force field and computer hardware development enable the use of molecular dynamics (MD) to simulate increasingly complex systems with the ultimate goal of reaching cellular complexity. At the same time, rational design by high-throughput (HT) simulations is another forefront of MD. In these areas, the Martini coarse-grained force field, especially the latest version (i.e. v3), is being actively explored because it offers an enhanced spatial-temporal resolution. However, the automation tools for preparing simulations with the Martini force field, accompanying the previous version, were not designed for HT simulations or studies of complex cellular systems. Therefore, they become a major limiting factor. To address these shortcomings, we present the open-source Vermouth python library. Vermouth is designed to become the unified framework for developing programs, which prepare, run, and analyze Martini simulations of complex systems. To demonstrate the power of the Vermouth library, the Martinize2 program is showcased as a generalization of the martinize script, originally aimed to set up simulations of proteins. In contrast to the previous version, Martinize2 automatically handles protonation states in proteins and post-translation modifications, offers more options to fine-tune structural biases such as the elastic network (EN), and can convert non-protein molecules such as ligands. Finally, Martinize2 is used in two high-complexity benchmarks. The entire I-TASSER protein template database as well as a subset of 200,000 structures from the AlphaFold Protein Structure Database are converted to CG resolution and we illustrate how the checks on input structure quality can safeguard high-throughput applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships and Improved GRU</title>
<link>https://arxiv.org/abs/2409.08282</link>
<guid>https://arxiv.org/abs/2409.08282</guid>
<content:encoded><![CDATA[
<div> Keywords: stock price prediction, deep learning, GRU, long short-term relationships, algorithmic trading  

<br /><br />Summary:  
Stock price prediction is a complex challenge attracting significant attention within finance. Recent advances in deep learning and graph neural networks have emphasized exploring inter-stock relationships. However, most existing models primarily focus on short-term dynamics and tend to neglect the intricate nonlinear characteristics and higher-order interactions in the market. To address this, the paper introduces the LSR-IGRU model, aimed at improving stock price trend predictions. This model constructs a long short-term relationship matrix utilizing secondary industry information for long-term connections and overnight price data for short-term dynamics. The GRU inputs are enhanced at each step to better combine temporal and relationship data, boosting prediction accuracy. Extensive experimentation across various stock market datasets from China and the United States confirms the LSR-IGRU model's superiority over leading baseline models. Additionally, the model has been successfully implemented in a financial company's algorithmic trading system, significantly outperforming other methods in generating cumulative portfolio returns. The research's codebase is publicly available at the provided GitHub link. <div>
arXiv:2409.08282v3 Announce Type: replace-cross 
Abstract: Stock price prediction is a challenging problem in the field of finance and receives widespread attention. In recent years, with the rapid development of technologies such as deep learning and graph neural networks, more research methods have begun to focus on exploring the interrelationships between stocks. However, existing methods mostly focus on the short-term dynamic relationships of stocks and directly integrating relationship information with temporal information. They often overlook the complex nonlinear dynamic characteristics and potential higher-order interaction relationships among stocks in the stock market. Therefore, we propose a stock price trend prediction model named LSR-IGRU in this paper, which is based on long short-term stock relationships and an improved GRU input. Firstly, we construct a long short-term relationship matrix between stocks, where secondary industry information is employed for the first time to capture long-term relationships of stocks, and overnight price information is utilized to establish short-term relationships. Next, we improve the inputs of the GRU model at each step, enabling the model to more effectively integrate temporal information and long short-term relationship information, thereby significantly improving the accuracy of predicting stock trend changes. Finally, through extensive experiments on multiple datasets from stock markets in China and the United States, we validate the superiority of the proposed LSR-IGRU model over the current state-of-the-art baseline models. We also apply the proposed model to the algorithmic trading system of a financial company, achieving significantly higher cumulative portfolio returns compared to other baseline methods. Our sources are released at https://github.com/ZP1481616577/Baselines_LSR-IGRU.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unfitted finite element modelling of surface-bulk viscous flows in animal cells</title>
<link>https://arxiv.org/abs/2505.05723</link>
<guid>https://arxiv.org/abs/2505.05723</guid>
<content:encoded><![CDATA[
<div> Keywords: unfitted finite element framework, surface-bulk problems, cortex-cytoplasm interactions, cell division, computational modelling

Summary: 
This work introduces a novel unfitted finite element framework for simulating surface-bulk problems in time-dependent domains, specifically focusing on fluid-fluid interactions in animal cells. The framework addresses the challenges of modelling cortical contractions that induce surface flows and intracellular flow within large animal cells. By combining the trace finite element method for surface flows with the aggregated finite element method for bulk flows, the framework enables accurate and stable simulations on fixed Cartesian grids without remeshing. Additionally, it incorporates mechanochemical feedback through the surface transport of a molecular regulator of active tension. The method's accuracy and stability are validated through numerical experiments, showcasing its ability to capture phenomena such as self-organized pattern formation, curvature-driven relaxation, and cell cleavage. This innovative framework provides a versatile tool for studying complex morphogenetic processes in animal cells. 

<br /><br />Summary: <div>
arXiv:2505.05723v1 Announce Type: new 
Abstract: This work presents a novel unfitted finite element framework to simulate coupled surface-bulk problems in time-dependent domains, focusing on fluid-fluid interactions in animal cells between the actomyosin cortex and the cytoplasm. The cortex, a thin layer beneath the plasma membrane, provides structural integrity and drives shape changes by generating surface contractile forces akin to tension. Cortical contractions generate Marangoni-like surface flows and induce intracellular cytoplasmic flows that are essential for processes such as cell division, migration, and polarization, particularly in large animal cells. Despite its importance, the spatiotemporal regulation of cortex-cytoplasm interactions remains poorly understood and computational modelling can be very challenging because surface-bulk dynamics often lead to large cell deformations. To address these challenges, we propose a sharp-interface framework that uniquely combines the trace finite element method for surface flows with the aggregated finite element method for bulk flows. This approach enables accurate and stable simulations on fixed Cartesian grids without remeshing. The model also incorporates mechanochemical feedback through the surface transport of a molecular regulator of active tension. We solve the resulting mixed-dimensional system on a fixed Cartesian grid using a level-set-based method to track the evolving surface. Numerical experiments validate the accuracy and stability of the method, capturing phenomena such as self-organised pattern formation, curvature-driven relaxation, and cell cleavage. This novel framework offers a powerful and extendable tool for investigating increasingly complex morphogenetic processes in animal cells.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading Under Uncertainty: A Distribution-Based Strategy for Futures Markets Using FutureQuant Transformer</title>
<link>https://arxiv.org/abs/2505.05595</link>
<guid>https://arxiv.org/abs/2505.05595</guid>
<content:encoded><![CDATA[
<div> Transformer model, futures trading, price predictions, attention mechanisms, risk management 
Summary:
The FutureQuant Transformer model is introduced to predict future price ranges and volatility in futures trading. Unlike traditional models, it focuses on forecasting ranges and volatility rather than point predictions, providing richer insights for trading strategies. By leveraging attention mechanisms to analyze complex data like real-time Limit Order Books, the model significantly improves decision-making and risk management in trading. Through its ability to learn intricate market patterns, it achieved an average gain of 0.1193% per 30-minute trade, outperforming state-of-the-art models. The model uses factors such as RSI, ATR, and Bollinger Bands in a simple algorithm to make accurate predictions, marking a substantial advancement in predictive analytics for futures trading. 
<br /><br />Summary: <div>
arXiv:2505.05595v1 Announce Type: cross 
Abstract: In the complex landscape of traditional futures trading, where vast data and variables like real-time Limit Order Books (LOB) complicate price predictions, we introduce the FutureQuant Transformer model, leveraging attention mechanisms to navigate these challenges. Unlike conventional models focused on point predictions, the FutureQuant model excels in forecasting the range and volatility of future prices, thus offering richer insights for trading strategies. Its ability to parse and learn from intricate market patterns allows for enhanced decision-making, significantly improving risk management and achieving a notable average gain of 0.1193% per 30-minute trade over state-of-the-art models with a simple algorithm using factors such as RSI, ATR, and Bollinger Bands. This innovation marks a substantial leap forward in predictive analytics within the volatile domain of futures trading.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion</title>
<link>https://arxiv.org/abs/2505.05605</link>
<guid>https://arxiv.org/abs/2505.05605</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, Conversion Prediction, Embedding Tables, Multi-Task Model, Overfitting<br />
<br />
Summary: 
Deep learning models for conversion prediction in online advertising often require complex training to predict multiple objectives. The use of embedding tables encoding high cardinality categorical features can enhance model performance but poses challenges due to gradient sparsity and label sparsity. Pinterest Ads Conversion models utilize Sparse Optimizer to improve convergence speed and address multi-epoch overfitting. The severity of overfitting varies across objectives in a multi-task model based on label sparsity. A new approach using frequency-adaptive learning rates on embedding tables is proposed to mitigate multi-epoch overfitting, compared to re-initialization. Evaluation on a large-scale production dataset demonstrates the effectiveness of these techniques in enhancing model performance without sacrificing accuracy.<br /><br />Summary: <div>
arXiv:2505.05605v1 Announce Type: cross 
Abstract: Deep learning for conversion prediction has found widespread applications in online advertising. These models have become more complex as they are trained to jointly predict multiple objectives such as click, add-to-cart, checkout and other conversion types. Additionally, the capacity and performance of these models can often be increased with the use of embedding tables that encode high cardinality categorical features such as advertiser, user, campaign, and product identifiers (IDs). These embedding tables can be pre-trained, but also learned end-to-end jointly with the model to directly optimize the model objectives. Training these large tables is challenging due to: gradient sparsity, the high cardinality of the categorical features, the non-uniform distribution of IDs and the very high label sparsity. These issues make training prone to both slow convergence and overfitting after the first epoch. Previous works addressed the multi-epoch overfitting issue by using: stronger feature hashing to reduce cardinality, filtering of low frequency IDs, regularization of the embedding tables, re-initialization of the embedding tables after each epoch, etc. Some of these techniques reduce overfitting at the expense of reduced model performance if used too aggressively. In this paper, we share key learnings from the development of embedding table optimization and multi-epoch training in Pinterest Ads Conversion models. We showcase how our Sparse Optimizer speeds up convergence, and how multi-epoch overfitting varies in severity between different objectives in a multi-task model depending on label sparsity. We propose a new approach to deal with multi-epoch overfitting: the use of a frequency-adaptive learning rate on the embedding tables and compare it to embedding re-initialization. We evaluate both methods offline using an industrial large-scale production dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing GPU Energy Usage in Exascale-Ready Portable Science Applications</title>
<link>https://arxiv.org/abs/2505.05623</link>
<guid>https://arxiv.org/abs/2505.05623</guid>
<content:encoded><![CDATA[
<div> quantum Monte Carlo, adaptive mesh, GPU energy usage, exascale-ready applications, mixed-precision

Summary:<br />
The study focuses on analyzing the GPU energy usage of two exascale-ready applications, QMCPACK and AMReX-Castro, representing particle and mesh solvers, respectively. Using NVIDIA's A100 and H100 GPUs and AMD's MI250X GPUs, power, temperature, utilization, and energy traces were examined. The research highlights the energy-saving potential of mixed-precision computations, with savings ranging from 6-25% for QMCPACK and 45% for AMReX-Castro. Challenges remain in the AMD tooling for Frontier GPUs, while NVML query resolutions exhibit little variability. The study underscores the importance of application-specific metrics in informing energy-performance trade-offs and optimizing future supercomputer architectures in the post-Moore era.<br /> 

Summary: <div>
arXiv:2505.05623v1 Announce Type: cross 
Abstract: We characterize the GPU energy usage of two widely adopted exascale-ready applications representing two classes of particle and mesh solvers: (i) QMCPACK, a quantum Monte Carlo package, and (ii) AMReX-Castro, an adaptive mesh astrophysical code. We analyze power, temperature, utilization, and energy traces from double-/single (mixed)-precision benchmarks on NVIDIA's A100 and H100 and AMD's MI250X GPUs using queries in NVML and rocm smi lib, respectively. We explore application-specific metrics to provide insights on energy vs. performance trade-offs. Our results suggest that mixed-precision energy savings range between 6-25% on QMCPACK and 45% on AMReX-Castro. Also there are still gaps in the AMD tooling on Frontier GPUs that need to be understood, while query resolutions on NVML have little variability between 1 ms and 1 s. Overall, application level knowledge is crucial to define energy-cost/science-benefit opportunities for the codesign of future supercomputer architectures in the post-Moore era.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions</title>
<link>https://arxiv.org/abs/2505.05784</link>
<guid>https://arxiv.org/abs/2505.05784</guid>
<content:encoded><![CDATA[
<div> Framework, High-frequency trading, Imitation learning, Flow matching policy, Market conditions

Summary:
FlowHFT is an imitation learning framework designed for high-frequency trading (HFT) that addresses the limitations of traditional HFT approaches. It simultaneously learns strategies from various expert models, allowing for adaptive adjustment of investment decisions based on market conditions. The framework incorporates a grid-search fine-tuning mechanism to refine strategies and outperform expert strategies in complex market scenarios. Through testing in multiple market environments, FlowHFT consistently achieves superior performance compared to individual expert models, showcasing its effectiveness in dynamic and diverse market conditions. The flow matching policy utilized in FlowHFT enables the framework to learn trading strategies under different market scenarios, demonstrating its applicability in stochastic market environments. This innovative approach offers a promising solution for HFT strategies in real-world markets. 

<br /><br />Summary: <div>
arXiv:2505.05784v1 Announce Type: cross 
Abstract: High-frequency trading (HFT) is an investing strategy that continuously monitors market states and places bid and ask orders at millisecond speeds. Traditional HFT approaches fit models with historical data and assume that future market states follow similar patterns. This limits the effectiveness of any single model to the specific conditions it was trained for. Additionally, these models achieve optimal solutions only under specific market conditions, such as assumptions about stock price's stochastic process, stable order flow, and the absence of sudden volatility. Real-world markets, however, are dynamic, diverse, and frequently volatile. To address these challenges, we propose the FlowHFT, a novel imitation learning framework based on flow matching policy. FlowHFT simultaneously learns strategies from numerous expert models, each proficient in particular market scenarios. As a result, our framework can adaptively adjust investment decisions according to the prevailing market state. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism. This allows it to refine strategies and achieve superior performance even in complex or extreme market scenarios where expert strategies may be suboptimal. We test FlowHFT in multiple market environments. We first show that flow matching policy is applicable in stochastic market environments, thus enabling FlowHFT to learn trading strategies under different market conditions. Notably, our single framework consistently achieves performance superior to the best expert for each market condition.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using iterated local alignment to aggregate trajectory data into a traffic flow map</title>
<link>https://arxiv.org/abs/2406.17500</link>
<guid>https://arxiv.org/abs/2406.17500</guid>
<content:encoded><![CDATA[
<div> Keywords: vehicle trajectories, traffic flow maps, measurement noise, local alignment algorithms, spatial resolution

Summary: 
Vehicle trajectories are valuable data for generating traffic flow maps at various scales. However, the presence of measurement noise can make small-scale aggregation challenging. To address this issue, new local alignment algorithms are introduced to align road segments for accurate flow mapping. These algorithms use inferred road segments as reference points and iterate through the process to compute locally aligned flow maps. Testing on synthetic and real-world data shows that the locally aligned maps provide precise flow aggregation at multiple scales, enhancing the accuracy and spatial resolution of static and interactive maps. The innovative approach offers a solution to the noise-related challenges in small-scale flow aggregation, making it possible to generate detailed and reliable traffic flow maps for effective traffic management and planning.<br /><br />Summary: <div>
arXiv:2406.17500v4 Announce Type: replace-cross 
Abstract: Vehicle trajectories, with their detailed geolocations, are a promising data source to compute traffic flow maps at scales ranging from the city/regional level to the road level. The main obstacle is that trajectory data are prone to measurement noise. While this is negligible for city level large-scale flow aggregation, it poses substantial difficulties for road level small-scale aggregation. To overcome these difficulties, we introduce innovative local alignment algorithms, where we infer road segments to serve as local reference segments, and proceed to align nearby road segments to them. We deploy these algorithms in an iterative workflow to compute locally aligned flow maps. By applying this workflow to synthetic and empirical trajectories, we verify that our locally aligned flow maps provide high levels of accuracy and spatial resolution of flow aggregation at multiple scales for static and interactive maps.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-informed solution reconstruction in elasticity and heat transfer using the explicit constraint force method</title>
<link>https://arxiv.org/abs/2505.04875</link>
<guid>https://arxiv.org/abs/2505.04875</guid>
<content:encoded><![CDATA[
<div> Keywords: physics-informed neural networks, solution reconstruction, interpretability, robustness, explicit constraint force method

<br /><br />Summary: This article addresses the challenges faced by physics-informed neural networks (PINNs) in solution reconstruction, which estimates the complete state of a physical system from sparse measurements. A key issue identified is that the governing equations used may not align with the actual physical phenomena, leading to potential failures in fulfilling the criteria of interpretability, robustness, and data consistency. These criteria are essential for assessing reconstruction quality, ensuring that results arent overly dependent on the choice of physics loss, and enabling the unique recovery of physics parameters. The study demonstrates that conventional physics loss formulations create varying "constraint forces," which are additional source terms that influence the reconstructed solution. To mitigate these issues, the authors propose an "explicit constraint force method" (ECFM) that provides better control over the introduced source terms. By adhering to the outlined criteria, the ECFM allows for more reliable and customizable reconstructions from noisy data, even when the physics parameterization does not match the measured system accurately. This approach enhances the performance and interpretability of PINNs in practical applications. <div>
arXiv:2505.04875v1 Announce Type: new 
Abstract: One use case of ``physics-informed neural networks'' (PINNs) is solution reconstruction, which aims to estimate the full-field state of a physical system from sparse measurements. Parameterized governing equations of the system are used in tandem with the measurements to regularize the regression problem. However, in real-world solution reconstruction problems, the parameterized governing equation may be inconsistent with the physical phenomena that give rise to the measurement data. We show that due to assuming consistency between the true and parameterized physics, PINNs-based approaches may fail to satisfy three basic criteria of interpretability, robustness, and data consistency. As we argue, these criteria ensure that (i) the quality of the reconstruction can be assessed, (ii) the reconstruction does not depend strongly on the choice of physics loss, and (iii) that in certain situations, the physics parameters can be uniquely recovered. In the context of elasticity and heat transfer, we demonstrate how standard formulations of the physics loss and techniques for constraining the solution to respect the measurement data lead to different ``constraint forces" -- which we define as additional source terms arising from the constraints -- and that these constraint forces can significantly influence the reconstructed solution. To avoid the potentially substantial influence of the choice of physics loss and method of constraint enforcement on the reconstructed solution, we propose the ``explicit constraint force method'' (ECFM) to gain control of the source term introduced by the constraint. We then show that by satisfying the criteria of interpretability, robustness, and data consistency, this approach leads to more predictable and customizable reconstructions from noisy measurement data, even when the parameterization of the missing physics is inconsistent with the measured system.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermoelastic Kirchhoff Plate: A Novel Model for Shot Peen Forming Metal Panels</title>
<link>https://arxiv.org/abs/2505.05236</link>
<guid>https://arxiv.org/abs/2505.05236</guid>
<content:encoded><![CDATA[
<div> Keywords: shot peen forming, torque, bending equations, Kirchhoff plate, Monte Carlo methods  

<br /><br />Summary: The study explores shot peen forming, a technique where high-velocity steel pellets impact aluminum panels, causing localized plastic deformation. This process enhances the fatigue properties of the material and introduces a residual stress distribution that results in bending, which can be interpreted as the application of spatially varying torques. The authors develop bending equations for a thermally loaded homogeneous Kirchhoff plate to predict the effects of shot peen forming. A novel test method is introduced to extract an equivalent applied torque from the bending response of uniformly shot peened plates, simplifying the accounting for surface plasticity. This extracted torque serves as an input for a model predicting the formation of rectangular plates under varied shot peen conditions. An experimental design is created and executed to assess the models validity against actual shot peen operations. Additionally, uncertainty within the experimental results is quantified using Monte Carlo methods, providing insights into the reliability of the findings and the effectiveness of the proposed modeling approach in capturing the influence of shot peening on panel deformation. <div>
arXiv:2505.05236v1 Announce Type: new 
Abstract: A common technique used in factories to shape metal panels is shot peen forming, where the panel is sprayed with a high-velocity stream of small steel pellets called shot. The impacts between the hard steel shot and softer aluminum panel cause localized plastic deformation, both improving the fatigue properties of the material's surface and imparting a residual stress distribution that results in bending. Thus, a torque is associated with the through-thickness shot peen stress distribution. We conceptualize shot peen forming as the application of spatially varying torques, which are modeled with the input of applied temperatures. In this paper, we derive the bending equations for a thermally loaded homogeneous Kirchhoff plate in order to predict the effects of shot peen forming. A simple test is devised to extract the value of an equivalent applied torque from the bending response of uniformly shot peened plates, which circumvents the difficulty of accounting for surface plasticity. This torque can be used as an input to a model which predicts the shape of rectangular plates under more complicated shot peen conditions. An experiment is designed and carried out which investigates the agreement between the model and real shot peen operations. The effect of uncertainty in the experiment is estimated with Monte Carlo methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Stock Market Prediction Using Long Short-Term Memory Networks: A Comprehensive Deep Learning Framework</title>
<link>https://arxiv.org/abs/2505.05325</link>
<guid>https://arxiv.org/abs/2505.05325</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market, LSTM, predict, sentiment analysis, web application

<br /><br />Summary: 
This paper tackles the challenge of predicting stock market movements, highlighting the volatility and complexity of financial time series data. The study employs a deep learning framework using Long Short-Term Memory (LSTM) networks to forecast closing stock prices for leading technology companies: Apple, Google, Microsoft, and Amazon, all listed on NASDAQ. Historical stock data was sourced from Yahoo Finance and then processed through normalization and feature engineering techniques to enhance model performance. The proposed LSTM model achieved a Mean Absolute Percentage Error (MAPE) of 2.72 on unseen test data, showing a considerable improvement over traditional models like ARIMA. To further refine predictive accuracy, the researchers incorporated sentiment scores derived from real-time news articles and social media, analyzed with the VADER sentiment analysis tool. Additionally, a web application was developed to provide real-time visualizations of stock price forecasts, making the model's insights accessible to both individual and institutional investors. This research illustrates the effectiveness of LSTM networks in capturing complex financial sequences and presents a novel hybrid methodology that integrates time series forecasting with sentiment analysis for enhanced predictive capability. <div>
arXiv:2505.05325v1 Announce Type: new 
Abstract: Predicting stock market movements remains a persistent challenge due to the inherently volatile, non-linear, and stochastic nature of financial time series data. This paper introduces a deep learning-based framework employing Long Short-Term Memory (LSTM) networks to forecast the closing stock prices of major technology firms: Apple, Google, Microsoft, and Amazon, listed on NASDAQ. Historical data was sourced from Yahoo Finance and processed using normalization and feature engineering techniques. The proposed model achieves a Mean Absolute Percentage Error (MAPE) of 2.72 on unseen test data, significantly outperforming traditional models like ARIMA. To further enhance predictive accuracy, sentiment scores were integrated using real-time news articles and social media data, analyzed through the VADER sentiment analysis tool. A web application was also developed to provide real-time visualizations of stock price forecasts, offering practical utility for both individual and institutional investors. This research demonstrates the strength of LSTM networks in modeling complex financial sequences and presents a novel hybrid approach combining time series modeling with sentiment analysis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatMMFuse: Multi-Modal Fusion model for Material Property Prediction</title>
<link>https://arxiv.org/abs/2505.04634</link>
<guid>https://arxiv.org/abs/2505.04634</guid>
<content:encoded><![CDATA[
<div> Keywords: graph-based encoding, multi-modal fusion, Crystal Graph Convolution Network, SciBERT, zero-shot performance

<br /><br />Summary: The paper presents Material Multi-Modal Fusion (MatMMFuse), a novel model that combines graph-based and text-based representations for enhanced material property prediction. By integrating local feature learning from the Crystal Graph Convolution Network (CGCNN) and global information from the SciBERT model, MatMMFuse utilizes a multi-head attention mechanism for effective fusion. The model is trained end-to-end using data from the Materials Project Dataset and demonstrates significant improvements over the individual CGCNN and SciBERT models across key properties: formation energy, band gap, energy above hull, and Fermi energy. Notably, the model shows a 40% enhancement compared to the CGCNN and a 68% improvement over the SciBERT model for predicting formation energy per atom. Furthermore, MatMMFuse exhibits impressive zero-shot performance on specialized datasets, including Perovskites, Chalcogenides, and the Jarvis Dataset, outperforming the standalone CGCNN and SciBERT models. This capability allows researchers to apply the model in industrial settings where obtaining training data can be costly. <div>
arXiv:2505.04634v1 Announce Type: cross 
Abstract: The recent progress of using graph based encoding of crystal structures for high throughput material property prediction has been quite successful. However, using a single modality model prevents us from exploiting the advantages of an enhanced features space by combining different representations. Specifically, pre-trained Large language models(LLMs) can encode a large amount of knowledge which is beneficial for training of models. Moreover, the graph encoder is able to learn the local features while the text encoder is able to learn global information such as space group and crystal symmetry. In this work, we propose Material Multi-Modal Fusion(MatMMFuse), a fusion based model which uses a multi-head attention mechanism for the combination of structure aware embedding from the Crystal Graph Convolution Network (CGCNN) and text embeddings from the SciBERT model. We train our model in an end-to-end framework using data from the Materials Project Dataset. We show that our proposed model shows an improvement compared to the vanilla CGCNN and SciBERT model for all four key properties: formation energy, band gap, energy above hull and fermi energy. Specifically, we observe an improvement of 40% compared to the vanilla CGCNN model and 68% compared to the SciBERT model for predicting the formation energy per atom. Importantly, we demonstrate the zero shot performance of the trained model on small curated datasets of Perovskites, Chalcogenides and the Jarvis Dataset. The results show that the proposed model exhibits better zero shot performance than the individual plain vanilla CGCNN and SciBERT model. This enables researchers to deploy the model for specialized industrial applications where collection of training data is prohibitively expensive.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights</title>
<link>https://arxiv.org/abs/2505.04846</link>
<guid>https://arxiv.org/abs/2505.04846</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Augmented Generation, HiPerRAG, high performance computing, multimodal document parsing, scientific question answering

<br /><br />Summary: The exponential growth of scientific literature has led to challenges such as underutilized discoveries and limited interdisciplinary collaboration. Retrieval Augmented Generation (RAG) has the potential to enhance the factuality of Large Language Models (LLMs) by effectively processing vast amounts of information. However, scaling RAG to manage millions of articles presents significant hurdles, including high computational costs and complex algorithmic requirements for aligning nuanced scientific content. To tackle these issues, the authors introduce HiPerRAG, a RAG workflow leveraging high-performance computing (HPC) to index and retrieve knowledge from over 3.6 million scientific articles. HiPerRAG incorporates Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy using contrastive learning and late-interaction techniques. The system demonstrates robust performance on existing scientific question answering benchmarks, achieving 90% accuracy on SciQ and 76% on PubMedQA, surpassing both specialized models like PubMedGPT and commercial LLMs such as GPT-4. By utilizing thousands of GPUs on advanced supercomputers, HiPerRAG facilitates million document-scale RAG workflows, promoting the unity of scientific knowledge and fostering interdisciplinary innovation. <div>
arXiv:2505.04846v1 Announce Type: cross 
Abstract: The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Direct-adjoint Approach for Material Point Model Calibration with Application to Plasticity</title>
<link>https://arxiv.org/abs/2501.04584</link>
<guid>https://arxiv.org/abs/2501.04584</guid>
<content:encoded><![CDATA[
<div> Keywords: elastoplastic, calibration, optimization, Hessian, automatic differentiation  

<br /><br />Summary: This paper introduces a novel method for calibrating material parameters in local elastoplastic constitutive models by framing the calibration as a constrained optimization problem. The evolution equations of the constitutive model at a single material point serve as constraints for this optimization. The aim is to minimize the objective function that measures the discrepancy between predicted stress outcomes from the model and actual experimental data. To enhance the calibration process's efficiency, a new direct-adjoint approach is employed to compute the Hessian of the objective function, which is crucial for implementing second-order optimization techniques. Additionally, automatic differentiation is utilized for calculating both gradient and Hessian values accurately. The paper includes two numerical examples to validate the accuracy of the Hessian matrices. The results demonstrate that the Newton-Raphson algorithm significantly outperforms first-order gradient-based methods such as L-BFGS-B in terms of efficiency and effectiveness, confirming the advantages of the proposed calibration approach for elastoplastic models. <div>
arXiv:2501.04584v2 Announce Type: replace 
Abstract: This paper proposes a new approach for the calibration of material parameters in local elastoplastic constitutive models. The calibration is posed as a constrained optimization problem, where the constitutive model evolution equations for a single material point serve as constraints. The objective function quantifies the mismatch between the stress predicted by the model and corresponding experimental measurements. To improve calibration efficiency, a novel direct-adjoint approach is presented to compute the Hessian of the objective function, which enables the use of second-order optimization algorithms. Automatic differentiation is used for gradient and Hessian computations. Two numerical examples are employed to validate the Hessian matrices and to demonstrate that the Newton-Raphson algorithm consistently outperforms gradient-based algorithms such as L-BFGS-B.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact nonlinear state estimation</title>
<link>https://arxiv.org/abs/2310.10976</link>
<guid>https://arxiv.org/abs/2310.10976</guid>
<content:encoded><![CDATA[
<div> Keywords: data assimilation, generative AI, Conjugate Transform Filter, nonlinear estimation, Earth system models  

<br /><br />Summary: The article discusses the limitations of traditional data assimilation (DA) methods in geosciences, which often rely on Gaussian assumptions that can lead to analysis biases and poor forecasts. To address these challenges, the authors introduce a novel nonlinear estimation theory inspired by advancements in generative artificial intelligence (AI). They present the Conjugate Transform Filter (CTF), which generalizes the Kalman filter to accommodate non-Gaussian distributions. This new filter is designed to maintain statistical relationships in prior states and effectively converge to accurate observations. Additionally, the article introduces an ensemble approximation of the CTF, termed the Ensemble Conjugate Transform Filter (ECTF), which is validated through idealized statistical experiments involving bounded quantities with non-Gaussian distributionscommon in Earth system models. The findings indicate that ECTF performs optimally when observation errors are minor compared to forecast uncertainties and when there are strong nonlinear dependencies among state variables. Ultimately, this new filtering theory presents promising pathways for enhancing conventional DA methods by integrating them with principles from AI, paving the way for improved accuracy in data assimilation tasks in geosciences. <div>
arXiv:2310.10976v2 Announce Type: replace-cross 
Abstract: The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and validated using idealized statistical experiments that feature bounded quantities with non-Gaussian distributions, a prevalent challenge in Earth system models. Results from these experiments indicate that the greatest benefits from ECTF occur when observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Ultimately, the new filtering theory offers exciting avenues for improving conventional DA algorithms through their principled integration with AI techniques.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-device Anomaly Detection in Conveyor Belt Operations</title>
<link>https://arxiv.org/abs/2411.10729</link>
<guid>https://arxiv.org/abs/2411.10729</guid>
<content:encoded><![CDATA[
<div> Keywords: conveyor belts, anomaly detection, pattern recognition, machine learning, mining operations  

<br /><br />Summary: Conveyor belts play a vital role in mining operations by facilitating the efficient movement of bulk materials, thereby enhancing productivity. While previous studies have focused on detecting anomalies in specific conveyor components, understanding root causes such as production changes and operator errors is essential. Continuous monitoring of conveyor belt work cycles remains nascent, requiring robust solutions. This study introduces two novel methods for classifying normal and abnormal duty cycles, based on a recently proposed anomaly detection technique. The methods utilize threshold-based detection, manually extracted features, pattern-matching, and supervised tiny machine learning models, including decision tree and random forest, among others. A comprehensive evaluation demonstrates that both proposed methods outperform the existing technique on two datasets. The heuristic rule-based method excels on training dataset with performance metrics of 97.3% for normal and 80.2% for abnormal cycles, while the ML-based approach scores 91.3% and 67.9% respectively on a dataset influenced by machine aging. Implemented on low-power microcontrollers, the methods achieve real-time operation with energy consumption of just 13.3 and 20.6 J during inference, presenting a significant advancement in conveyor belt monitoring solutions. <div>
arXiv:2411.10729v2 Announce Type: replace-cross 
Abstract: Conveyor belts are crucial in mining operations by enabling the continuous and efficient movement of bulk materials over long distances, which directly impacts productivity. While detecting anomalies in specific conveyor belt components has been widely studied, identifying the root causes of these failures, such as changing production conditions and operator errors, remains critical. Continuous monitoring of mining conveyor belt work cycles is still at an early stage and requires robust solutions. Recently, an anomaly detection method for duty cycle operations of a mining conveyor belt has been proposed. Based on its limited performance and unevaluated long-term proper operation, this study proposes two novel methods for classifying normal and abnormal duty cycles. The proposed approaches are pattern recognition systems that make use of threshold-based duty-cycle detection mechanisms, manually extracted features, pattern-matching, and supervised tiny machine learning models. The explored low-computational models include decision tree, random forest, extra trees, extreme gradient boosting, Gaussian naive Bayes, and multi-layer perceptron. A comprehensive evaluation of the former and proposed approaches is carried out on two datasets. Both proposed methods outperform the former method, with the best-performing approach being dataset-dependent. The heuristic rule-based approach achieves the highest performance in the same dataset used for algorithm training, with 97.3% for normal cycles and 80.2% for abnormal cycles. The ML-based approach performs better on a dataset including the effects of machine aging, scoring 91.3% for normal cycles and 67.9% for abnormal cycles. Implemented on two low-power microcontrollers, the methods demonstrate efficient, real-time operation with energy consumption of 13.3 and 20.6 ${\mu}$J during inference. These results offer valuable insights for detecting ...
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers</title>
<link>https://arxiv.org/abs/2505.04018</link>
<guid>https://arxiv.org/abs/2505.04018</guid>
<content:encoded><![CDATA[
<div> deep learning, graph neural networks, transformers, modal identification, structural health monitoring

Summary: 
This study introduces a novel deep learning framework that combines graph neural networks, transformers, and a physics-informed loss function for modal identification in a population of structures. The transformer module breaks down multiple degrees-of-freedom measurements into single-degree-of-freedom modal responses, aiding in the identification of natural frequencies and damping ratios. Concurrently, the graph neural network captures structural configurations and identifies mode shapes corresponding to the decomposed modal responses. The model is trained in an unsupervised manner, utilizing modal decomposition theory and the independence of structural modes for guidance without labeled data. Validation through simulations and experiments shows its ability to accurately decompose dynamic responses and identify modal properties from sparse measurements, even with external load and structural variations. Comparative analyses against existing techniques highlight its superior performance, making it an attractive option for population-based structural health monitoring. 

<br /><br />Summary: <div>
arXiv:2505.04018v1 Announce Type: new 
Abstract: Modal identification is crucial for structural health monitoring and structural control, providing critical insights into structural dynamics and performance. This study presents a novel deep learning framework that integrates graph neural networks (GNNs), transformers, and a physics-informed loss function to achieve modal decomposition and identification across a population of structures. The transformer module decomposes multi-degrees-of-freedom (MDOF) structural dynamic measurements into single-degree-of-freedom (SDOF) modal responses, facilitating the identification of natural frequencies and damping ratios. Concurrently, the GNN captures the structural configurations and identifies mode shapes corresponding to the decomposed SDOF modal responses. The proposed model is trained in a purely physics-informed and unsupervised manner, leveraging modal decomposition theory and the independence of structural modes to guide learning without the need for labeled data. Validation through numerical simulations and laboratory experiments demonstrates its effectiveness in accurately decomposing dynamic responses and identifying modal properties from sparse structural dynamic measurements, regardless of variations in external loads or structural configurations. Comparative analyses against established modal identification techniques and model variations further underscore its superior performance, positioning it as a favorable approach for population-based structural health monitoring.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yield and Buckling Stress Limits in Topology Optimization of Multiscale Structures</title>
<link>https://arxiv.org/abs/2505.04353</link>
<guid>https://arxiv.org/abs/2505.04353</guid>
<content:encoded><![CDATA[
<div> Topology optimization, yield stress, local buckling, global buckling, multiscale analysis
<br />
Summary: 
This study introduces an extension of multiscale topology optimization by incorporating yield stress and local/global buckling considerations into the design process. The new framework integrates yield stress limits as constraints or objectives alongside existing buckling constraints, refining the optimization process to meet mechanical performance criteria and material yield constraints. Local density-dependent yield surfaces are established based on local yield estimates, then combined with buckling criteria to obtain topology optimized designs considering yield and buckling failure. This integration is crucial for ensuring structural integrity and durability in real-world scenarios. Numerical examples show that optimized designs are influenced by the stiffness to yield ratio of the building material. Despite the assumption of scale separation, de-homogenized structures closely match homogenized predictions even at coarse length scales. 
<br /> <div>
arXiv:2505.04353v1 Announce Type: new 
Abstract: This study presents an extension of multiscale topology optimization by integrating both yield stress and local/global buckling considerations into the design process. Building upon established multiscale methodologies, we develop a new framework incorporating yield stress limits either as constraints or objectives alongside previously established local and global buckling constraints. This approach significantly refines the optimization process, ensuring that the resulting designs meet mechanical performance criteria and adhere to critical material yield constraints. First, we establish local density-dependent von Mises yield surfaces based on local yield estimates from homogenization-based analysis to predict the local yield limits of the homogenized materials. Then, these local Yield-based Load Factors (YLFs) are combined with local and global buckling criteria to obtain topology optimized designs that consider yield and buckling failure on all levels. This integration is crucial for the practical application of optimized structures in real-world scenarios, where material yield and stability behavior critically influence structural integrity and durability. Numerical examples demonstrate how optimized designs depend on the stiffness to yield ratio of the considered building material. Despite the foundational assumption of separation of scales, the de-homogenized structures, even at relatively coarse length scales, exhibit a high degree of agreement with the corresponding homogenized predictions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery Scheme in Mobile Crowdsensing</title>
<link>https://arxiv.org/abs/2505.04361</link>
<guid>https://arxiv.org/abs/2505.04361</guid>
<content:encoded><![CDATA[
<div> Privacy-Preserving Truth Discovery Mobile Crowdsensing Reputation Data Quality<br />
<br />
Summary: The article introduces a Reputation and Data Privacy-Preserving-based Truth Discovery (RDPP-TD) scheme to enhance data quality in mobile crowdsensing (MCS). Existing truth discovery methods often result in low data quality as they only consider current-round data. The proposed RDPP-TD scheme integrates Reputation-based Truth Discovery (RTD) with a Reputation and Data Privacy-Preserving (RDPP) approach to estimate truth more accurately and ensure privacy protection for worker data and reputation values. By combining RTD with RDPP, the scheme evaluates worker reliability in a privacy-preserving manner and supports reputation-based worker recruitment and rewards. The comprehensive theoretical analysis and experiments show that RDPP-TD improves data quality by up to 33.3% while providing strong privacy protection in MCS. <div>
arXiv:2505.04361v1 Announce Type: new 
Abstract: Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS). However, existing TD methods, including privacy-preserving TD approaches, estimate the truth by weighting only the data submitted in the current round, which often results in low data quality. Moreover, there is a lack of effective TD methods that preserve both reputation and data privacy. To address these issues, a Reputation and Data Privacy-Preserving based Truth Discovery (RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD) approach, which integrates the weight of current-round data with workers' reputation values to estimate the truth, thereby achieving more accurate results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which ensures privacy preservation for sensing data and reputation values. First, the RDPP approach, when seamlessly integrated with RTD, can effectively evaluate the reliability of workers and their sensing data in a privacy-preserving manner. Second, the RDPP scheme supports reputation-based worker recruitment and rewards, ensuring high-quality data collection while incentivizing workers to provide accurate information. Comprehensive theoretical analysis and extensive experiments based on real-world datasets demonstrate that the proposed RDPP-TD scheme provides strong privacy protection and improves data quality by up to 33.3%.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Execution Welfare Across Solver-based DEXes</title>
<link>https://arxiv.org/abs/2503.00738</link>
<guid>https://arxiv.org/abs/2503.00738</guid>
<content:encoded><![CDATA[
<div> auctions, decentralized exchanges, solver-based protocols, execution welfare, liquidity profile

Summary:
Solver-based protocols in decentralized exchanges have shown to improve execution welfare for end users compared to traditional routing methods like Uniswap V2 or V3. The study analyzed data for different asset pairs (USDC-WETH and PEPE-WETH) and found that solver-based platforms such as CoWSwap, 1inchFusion, and UniswapX, offer varying levels of execution welfare. While USDC-WETH, a short-tail asset, benefited significantly from solver-based trading, the impact was less pronounced for PEPE-WETH, a long-tail asset. The study also highlighted potential inefficiencies in solver market structure, liquidity profiles, and competition dynamics among solvers. These insights underscore the advantages of solver-based protocols in improving execution outcomes but also raise concerns about market concentration and competition dynamics in decentralized exchanges. 

<br /><br />Summary: <div>
arXiv:2503.00738v3 Announce Type: replace 
Abstract: Decentralized exchanges (DEXes) have evolved dramatically since the introduction of Automated Market Makers (AMMs). In recent years, solver-based protocols have emerged as an alternative venue aiming to introduce competition for routing, access to offchain liquidity, and thereby improve end-user execution. Currently, these solver auctions are hosted on opaque backends, and the extent of price improvement they provide to end users remains unclear.
  We conduct an empirical study of the execution welfare that these protocols bring to users by analyzing data across different asset profiles (USDC-WETH and PEPE-WETH). Our results indicate that, compared to vanilla routing through Uniswap V2 or V3, solver-based protocols effectively enhance execution welfare for end users on DEXes within certain trade size ranges. This effect is most pronounced with USDC-WETH, a short-tail asset, and somewhat less significant with PEPE-WETH, a long-tail asset.
  Additionally, we identify execution welfare discrepancies across solver-based platforms (e.g., CoWSwap, 1inchFusion, UniswapX), revealing potential inefficiencies due to solver market structure, variations in liquidity profile and inventory depth among solvers. These insights highlight both the advantages and challenges of solver-based trading, underscoring its role in improving execution outcomes while raising concerns about market concentration and competition dynamics.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto.gov: Learning-based Governance for Decentralized Finance (DeFi)</title>
<link>https://arxiv.org/abs/2302.09551</link>
<guid>https://arxiv.org/abs/2302.09551</guid>
<content:encoded><![CDATA[
<div> Keywords: DeFi, governance, Auto$.$gov, deep Q-network, reinforcement learning<br />
<br />
Summary: 
Auto$.$gov is a novel governance framework for decentralized finance (DeFi) that utilizes deep Q-network reinforcement learning to automate parameter adjustments. Traditional DeFi governance methods are manual and prone to human bias and financial risks. Auto$.$gov addresses these issues by using RL to make data-driven decisions and adapt to market conditions. In simulated tests based on the Aave lending protocol, Auto$.$gov successfully prevented funds loss from price oracle attacks. Real-world tests showed that Auto$.$gov outperformed benchmark approaches by 14% and the static baseline model by tenfold in terms of protocol profitability. This innovative governance model enhances the security, profitability, and sustainability of DeFi protocols, offering a more efficient and effective alternative to traditional governance methods.<br /> 
Summary: <div>
arXiv:2302.09551v4 Announce Type: replace-cross 
Abstract: Decentralized finance (DeFi) is an integral component of the blockchain ecosystem, enabling a range of financial activities through smart-contract-based protocols. Traditional DeFi governance typically involves manual parameter adjustments by protocol teams or token holder votes, and is thus prone to human bias and financial risks, undermining the system's integrity and security. While existing efforts aim to establish more adaptive parameter adjustment schemes, there remains a need for a governance model that is both more efficient and resilient to significant market manipulations. In this paper, we introduce "Auto$.$gov", a learning-based governance framework that employs a deep Qnetwork (DQN) reinforcement learning (RL) strategy to perform semi-automated, data-driven parameter adjustments. We create a DeFi environment with an encoded action-state space akin to the Aave lending protocol for simulation and testing purposes, where Auto$.$gov has demonstrated the capability to retain funds that would have otherwise been lost to price oracle attacks. In tests with real-world data, Auto$.$gov outperforms the benchmark approaches by at least 14% and the static baseline model by tenfold, in terms of the preset performance metric--protocol profitability. Overall, the comprehensive evaluations confirm that Auto$.$gov is more efficient and effective than traditional governance methods, thereby enhancing the security, profitability, and ultimately, the sustainability of DeFi protocols.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale Parallel Simulation of Malignant Pleural Mesothelioma via Adaptive Domain Partitioning -- an Efficiency Analysis Study</title>
<link>https://arxiv.org/abs/2505.03067</link>
<guid>https://arxiv.org/abs/2505.03067</guid>
<content:encoded><![CDATA[
<div> framework, Malignant Pleural Mesothelioma, tumour growth, parallel efficiency analysis, Computational resources optimization

Summary:<br />
The article introduces a novel framework for simulating the growth of Malignant Pleural Mesothelioma (MPM) tumors using a Cellular Potts Model (CPM) coupled with partial differential equations (PDEs). A dynamic bounding box is applied to the simulation domain, based on CT scan data, to reduce memory and CPU overhead. This adaptive partitioning allows for efficient use of computational resources by reducing the 3D domain over which the PDEs are solved. The PDEs, representing oxygen, nutrients, and cytokines, are solved using the finite-volume method with a first-order implicit Euler scheme. Parallelization is achieved using mpi4py library, LinearGMRESSolver, and PETSc for efficient convergence. Parallel computation results in reduced solving time compared to serial computation, with optimizations enhancing memory usage and load balancing among cores. <div>
arXiv:2505.03067v1 Announce Type: new 
Abstract: A novel parallel efficiency analysis on a framework for simulating the growth of Malignant Pleural Mesothelioma (MPM) tumours is presented. Proliferation of MPM tumours in the pleural space is simulated using a Cellular Potts Model (CPM) coupled with partial differential equations (PDEs). Using segmented lung data from CT scans, an environment is set up with artificial tumour data in the pleural space, representing the simulation domain, onto which a dynamic bounding box is applied to restrict computations to the region of interest, dramatically reducing memory and CPU overhead. This adaptive partitioning of the domain enables efficient use of computational resources by reducing the three-dimensional (3D) domain over which the PDEs are to be solved. The PDEs, representing oxygen, nutrients, and cytokines, are solved using the finite-volume method with a first-order implicit Euler scheme. Parallelization is realized using the public Python library mpi4py in combination with LinearGMRESSolver and PETSc for efficient convergence. Performance analyses have shown that parallelization achieves a reduced solving time compared to serial computation. Also, optimizations enable efficient use of available memory and improved load balancing amongst the cores.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Applied to Short-term Solar PV Power Output Forecasting</title>
<link>https://arxiv.org/abs/2505.03188</link>
<guid>https://arxiv.org/abs/2505.03188</guid>
<content:encoded><![CDATA[
<div> Convolutional Neural Networks, Solar Photovoltaic Power Output, Transformer Architecture, Nowcast Model, Forecast Model
Summary:
The study focuses on predicting and forecasting solar PV power output using a transformer architecture and fully-connected layer. The research addresses the challenge of uncertainty in solar PV output due to weather conditions like cloud cover, which can vary over short and long timescales. By utilizing one year of image data and experimenting with different learning rates and batch sizes, the transformer architecture shows promising results in predicting PV output. However, it performs less effectively on days with clear skies compared to the baseline model. Reliable forecasts of renewable energy generation are vital for electricity market balance and supply reliability. This research contributes to improving the accuracy of solar PV power output predictions, particularly in the context of fluctuating weather conditions.<br /><br />Summary: <div>
arXiv:2505.03188v1 Announce Type: new 
Abstract: Reliable forecasts of the power output from variable renewable energy generators like solar photovoltaic systems are important to balancing load on real-time electricity markets and ensuring electricity supply reliability. However, solar PV power output is highly uncertain, with significant variations occurring over both longer (daily or seasonally) and shorter (within minutes) timescales due to weather conditions, especially cloud cover. This paper builds on existing work that uses convolutional neural networks in the computer vision task of predicting (in a Nowcast model) and forecasting (in a Forecast model) solar PV power output (Stanford EAO SUNSET Model). A pure transformer architecture followed by a fully-connected layer is applied to one year of image data with experiments run on various combinations of learning rate and batch size. We find that the transformer architecture performs almost as well as the baseline model in the PV output prediction task. However, it performs worse on sunny days.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-efficient inverse design of spinodoid metamaterials</title>
<link>https://arxiv.org/abs/2505.03415</link>
<guid>https://arxiv.org/abs/2505.03415</guid>
<content:encoded><![CDATA[
<div> surrogate model, spinodoid metamaterials, structure-property linkages, data-efficient, multi-objective inverse design
<br />
Summary:
This study introduces a data-efficient and accurate surrogate model for spinodoid metamaterials' structure-property linkages, using only 75 data points, a substantial reduction compared to previous works. By employing a neural network-based surrogate model that satisfies specific requirements such as equivariance with respect to permutations of structure parameters, the research team successfully creates a differentiable surrogate of the forward model. This model enables gradient-based optimization for inverse design problems, demonstrating reliable results in various complex tasks. The data efficiency achieved in this study opens up possibilities for inverse design applications involving nonlinear mechanical behavior, where the limitation of data availability has been a challenge. <div>
arXiv:2505.03415v1 Announce Type: new 
Abstract: We create an data-efficient and accurate surrogate model for structure-property linkages of spinodoid metamaterials with only 75 data points -- far fewer than the several thousands used in prior works -- and demonstrate its use in multi-objective inverse design. The inverse problem of finding a material microstructure that leads to given bulk properties is of great interest in mechanics and materials science. These inverse design tasks often require a large dataset, which can become unaffordable when considering material behavior that requires more expensive simulations or experiments. We generate a data-efficient surrogate for the mapping between the characteristics of the local material structure and the effective elasticity tensor and use it to inversely design structures with multiple objectives simultaneously. The presented neural network-based surrogate model achieves its data efficiency by inherently satisfying certain requirements, such as equivariance with respect to permutations of structure parameters, which avoids having to learn them from data. The resulting surrogate of the forward model is differentiable, allowing its direct use in gradient-based optimization for the inverse design problem. We demonstrate in three inverse design tasks of varying complexity that this approach yields reliable results while requiring significantly less training data than previous approaches based on neural-network surrogates. This paves the way for inverse design involving nonlinear mechanical behavior, where data efficiency is currently the limiting factor.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithm Selection in Short-Range Molecular Dynamics Simulations</title>
<link>https://arxiv.org/abs/2505.03438</link>
<guid>https://arxiv.org/abs/2505.03438</guid>
<content:encoded><![CDATA[
<div> algorithm selection, molecular dynamics simulations, performance prediction, fuzzy logic, random forest

Summary:
Three algorithm selection strategies for Molecular Dynamics simulations were investigated in this work: performance prediction using past data, fuzzy logic-based expert knowledge approach, and data-driven random forest approach. These strategies achieved speedups of up to 4.05 compared to previous methods and 1.25 compared to a static algorithm configuration selection. The study demonstrated the effectiveness of dynamic algorithm selection in improving simulation performance. The practicality of the strategies was also discussed in relation to their performance, emphasizing the feasibility of implementing such solutions in real-world scenarios. Overall, the research showcased the benefits of dynamic algorithm selection in enhancing the efficiency of particle simulations, offering significant speed improvements while maintaining practicality and ease of implementation. 

<br /><br />Summary: <div>
arXiv:2505.03438v1 Announce Type: new 
Abstract: Numerous algorithms and parallelisations have been developed for short-range particle simulations; however, none are optimally performant for all scenarios. Such a concept led to the prior development of the particle simulation library AutoPas, which implemented many of these algorithms and parallelisations and could select and tune these over the course of the simulation as the scenario changed. Prior works have, however, used only naive approaches to the algorithm selection problem, which can lead to significant overhead from trialling poorly performing algorithmic configurations.
  In this work, we investigate this problem in the case of Molecular Dynamics simulations. We present three algorithm selection strategies: an approach which makes performance predictions from past data, an expert-knowledge fuzzy logic-based approach, and a data-driven random forest-based approach. We demonstrate that these approaches can achieve speedups of up to 4.05 compared to prior approaches and 1.25 compared to a perfect configuration selection without dynamic algorithm selection. In addition, we discuss the practicality of the strategies in comparison to their performance, to highlight the tractability of such solutions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation-Based Equations for Propagation Constant in Uniform or Periodic Transmission</title>
<link>https://arxiv.org/abs/2401.06165</link>
<guid>https://arxiv.org/abs/2401.06165</guid>
<content:encoded><![CDATA[
<div> simulation-based equations, propagation constant, uniform structures, periodic structures, FPPS<br />
<br />
Summary: The article presents simulation-based equations for calculating propagation constants in uniform or periodic structures using a Field Propagation Parameter Splitter (FPPS) model. The FPPS model is based on field distributions obtained from a driven-mode solver, allowing for the separation of forward and backward waves within structures. The FPPS is tested on various structures including waveguides, closed structures, and open radiation structures, showing its ease of use and adaptability compared to other methods. The model's effectiveness is verified through comparisons with eigenmode solvers, equivalent network methods, and spectral domain integral equation methods. The FPPS can also be applied to open radiating structures and multi-dimensional periodic/uniform structures. <div>
arXiv:2401.06165v2 Announce Type: replace 
Abstract: In this work, simulation-based equations to calculate propagation constant in uniform or periodic structures (SES) are deduced and verified through simulations in various types of structures. The modeling of those structures are essentially based on field distributions from a driven-mode solver, and the field distributions are used as the input parameters of the FPPS. It allows the separation of forward and backward waves from a total wave inside such a uniform or periodic structure, and thus it can be used to calculate the propagation constants inside both uniform and periodic structures even with a strong reflection. In order to test the performance and function of the FPPS, it has been applied to a variety of typical structures, including uniform waveguides, lossfree closed structures, lossy closed structures, and open radiation structures, and compared with the results of eigenmode solvers, equivalent network methods, and spectral domain integral equation methods. The comparison shows the easy-to-use and adaptable nature of the FPPS. the FPPS. This FPPS could be also applied to open radiating structures, and even multi-dimensional periodic/uniform structures.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Precision Glass Thermoforming Assisted by Neural Networks</title>
<link>https://arxiv.org/abs/2411.06762</link>
<guid>https://arxiv.org/abs/2411.06762</guid>
<content:encoded><![CDATA[
<div> surrogate model, glass thermoforming, precision, neural network, predictive<br />
<br />
Summary: 
This study presents a surrogate model based on a dimensionless back-propagation neural network (BPNN) to predict form errors in glass thermoforming processes. Traditional trial-and-error methods can be time-consuming and costly, leading to inefficiencies in precision glass product development. The surrogate model uses geometric features and process parameters as inputs to accurately predict forming errors, allowing for adjustments in mold design. The model was tested using simulation and industrial data, showing promising results in predicting form errors with reasonable accuracy. Despite potential discrepancies in industrial training data due to perception errors and mold fabrication errors, the surrogate model demonstrated practicality for implementation in the glass-manufacturing industry. <div>
arXiv:2411.06762v2 Announce Type: replace 
Abstract: Many glass products require thermoformed geometry with high precision. However, the traditional approach of developing a thermoforming process through trials and errors can cause large waste of time and resources and often end up with unsuccessfulness. Hence, there is a need to develop an efficient predictive model, replacing the costly simulations or experiments, to assist the design of precision glass thermoforming. In this work, we report a surrogate model, based on a dimensionless back-propagation neural network (BPNN), that can adequately predict the form errors and thus compensate for these errors in mold design using geometric features and process parameters as inputs. Our trials with simulation and industrial data indicate that the surrogate model can predict forming errors with adequate accuracy. Although perception errors (mold designers' decisions) and mold fabrication errors make the industrial training data less reliable than simulation data, our preliminary training and testing results still achieved a reasonable consistency with industrial data, suggesting that the surrogate models are directly implementable in the glass-manufacturing industry.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolution of Reinforcement Learning in Quantitative Finance: A Survey</title>
<link>https://arxiv.org/abs/2408.10932</link>
<guid>https://arxiv.org/abs/2408.10932</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, finance, Quantitative Finance, machine learning, financial markets

Summary: 
This survey reviews 167 publications on the application of Reinforcement Learning (RL) in finance. RL has shown significant progress in the past decade and is gaining traction in the financial sector due to its dynamic approach and integration with machine learning techniques. Financial markets, known for their complexity and randomness, provide a challenging environment for RL applications. The survey examines various RL frameworks and applications in Quantitative Finance, highlighting the strengths and weaknesses of existing methods. Transfer learning, meta-learning, and multi-agent solutions are identified as key components advancing RL in finance. The survey also explores emerging themes in RL applications and proposes future research directions in this field. <div>
arXiv:2408.10932v3 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Asset Pricing: Integrating FinBERT-Based Sentiment Quantification with the Fama--French Five-Factor Model</title>
<link>https://arxiv.org/abs/2505.01432</link>
<guid>https://arxiv.org/abs/2505.01432</guid>
<content:encoded><![CDATA[
<div> FinBERT, sentiment factors, multi-factor asset pricing models, Fama French five-factor regression, market volatility <br />
<br />
Summary:This paper investigates the impact of text-derived, time-varying sentiment factors on stock returns using FinBERT. Through a comprehensive study covering 2020-2022, a dynamic sentiment index and its volatility are constructed from financial news and social media data. Results show that sentiment positively influences returns in normal market conditions but its effect varies under extreme volatility. The study reveals the time-varying nature of sentiment sensitivity and demonstrates improved abnormal returns prediction during the Federal Reserve rate hike event in June 15, 2022, using a sentiment-augmented five-factor model. These findings advocate for the integration of high-frequency sentiment analysis in traditional asset pricing models, offering valuable insights for investors and regulators.<br /><br /> <div>
arXiv:2505.01432v1 Announce Type: new 
Abstract: This paper presents a comprehensive study on the integration of text-derived, time-varying sentiment factors into traditional multi-factor asset pricing models. Leveraging FinBERT, a domain-specific deep learning language model, we construct a dynamic sentiment index and its volatility from large-scale financial news and social media data covering 2020 to 2022. By embedding these sentiment measures into the Fama French five-factor regression, we rigorously examine whether sentiment significantly explains variations in daily stock returns and how its impact evolves across different market volatility regimes. Empirical results demonstrate that sentiment has a consistently positive impact on returns during normal periods, while its effect is amplified or even reversed under extreme market conditions. Rolling regressions reveal the time-varying nature of sentiment sensitivity, and an event study around the June 15, 2022 Federal Reserve 75 basis point rate hike shows that a sentiment-augmented five-factor model better explains abnormal returns relative to the baseline model. Our findings support the incorporation of high-frequency, NLP-derived sentiment into classical asset pricing frameworks and suggest implications for investors and regulators.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimising Kernel-based Multivariate Statistical Process Control</title>
<link>https://arxiv.org/abs/2505.01556</link>
<guid>https://arxiv.org/abs/2505.01556</guid>
<content:encoded><![CDATA[
<div> kernel MSPC, multivariate statistical process control, kernel functions, kernel flows, Gaussian Process Regression

Summary:
Kernel MSPC is a framework for monitoring complex processes by analyzing multiple process variables simultaneously. Kernel MSPC enhances process monitoring capabilities by capturing non-linear relationships using kernel functions. This study proposes optimizing kernel MSPC parameters using Kernel Flows, a kernel learning methodology. The methodology also utilizes kernel combinations to learn the optimal kernel type and individual kernel parameters for each variable. The proposed approach is evaluated using cases from the Tennessee Eastman Process benchmark and successfully detects faults that were not identified in the original study. The study demonstrates the effectiveness of the proposed optimization technique and the incorporation of kernel combinations for improved process monitoring in complex systems. <br /><br />Summary: <div>
arXiv:2505.01556v1 Announce Type: new 
Abstract: Multivariate Statistical Process Control (MSPC) is a framework for monitoring and diagnosing complex processes by analysing the relationships between multiple process variables simultaneously. Kernel MSPC extends the methodology by leveraging kernel functions to capture non-linear relationships between the data, enhancing the process monitoring capabilities. However, optimising the kernel MSPC parameters, such as the kernel type and kernel parameters, is often done in literature in time-consuming and non-procedural manners such as cross-validation or grid search. In the present paper, we propose optimising the kernel MSPC parameters with Kernel Flows (KF), a recent kernel learning methodology introduced for Gaussian Process Regression (GPR). Apart from the optimisation technique, the novelty of the study resides also in the utilisation of kernel combinations for learning the optimal kernel type, and introduces individual kernel parameters for each variable. The proposed methodology is evaluated with multiple cases from the benchmark Tennessee Eastman Process. The faults are detected for all evaluated cases, including the ones not detected in the original study.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction</title>
<link>https://arxiv.org/abs/2505.01781</link>
<guid>https://arxiv.org/abs/2505.01781</guid>
<content:encoded><![CDATA[
<div> Keywords: Mean-Variance model, Black-Litterman model, Singular Spectrum analysis, Multivariate Aligned Empirical Mode Decomposition, Temporal Convolutional Networks

Summary:
The traditional Mean-Variance model is limited by sensitivity to input parameters and lack of flexibility. In contrast, the Black-Litterman model combines market equilibrium returns with investors' subjective views, showing promise in asset price prediction. A novel hybrid deep learning model incorporating Singular Spectrum analysis, Multivariate Aligned Empirical Mode Decomposition, and Temporal Convolutional Networks is proposed to enhance prediction accuracy. Experimental results demonstrate noise reduction preprocessing improves model performance significantly. The hybrid model outperforms three benchmark models in multivariate decomposition. An investment portfolio constructed using 20 NASDAQ 100 index stocks shows that when combined with the Black-Litterman model, the hybrid forecasting model produces better returns and risk control capabilities than Mean-Variance, Equal-Weighted, and Market-Weighted models over a short holding period. 

<br /><br />Summary: <div>
arXiv:2505.01781v1 Announce Type: new 
Abstract: The sensitivity to input parameters and lack of flexibility limits the traditional Mean-Variance model. In contrast, the Black-Litterman model has attracted widespread attention by integrating market equilibrium returns with investors' subjective views. This paper proposes a novel hybrid deep learning model combining Singular Spectrum analysis (SSA), Multivariate Aligned Empirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks (TCNs), aiming to improve the prediction accuracy of asset prices and thus enhance the ability of the Black-Litterman model to generate subjective views. Experimental results show that noise reduction pre-processing can improve the model's accuracy, and the prediction performance of the proposed model is significantly better than that of three multivariate decomposition benchmark models. We construct an investment portfolio by using 20 representative stocks from the NASDAQ 100 index. By combining the hybrid forecasting model with the Black-Litterman model, the generated investment portfolio exhibits better returns and risk control capabilities than the Mean-Variance, Equal-Weighted, and Market-Weighted models in the short holding period.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computational framework for predicting the effect of surface roughness in fatigue</title>
<link>https://arxiv.org/abs/2505.01871</link>
<guid>https://arxiv.org/abs/2505.01871</guid>
<content:encoded><![CDATA[
<div> Keywords: surface roughness, fatigue life, phase field model, stochastic nature, failure strength <br />
Summary: Surface roughness significantly affects the fatigue life of structural components and can be quantified using the surface factor. A numerical framework based on the phase field method has been developed to estimate the surface factor considering the stochastic nature of roughness. The model's validity is confirmed through experimental data validation. The study explores the impact of key parameters on the fatigue life of rough surfaces, including surface topology and failure strength. Notably, an increase in average surface roughness coupled with a decrease in the correlation length of the surface profile results in a pronounced effect on fatigue life. This effect is more prominent at higher failure strengths. <br /><br /> <div>
arXiv:2505.01871v1 Announce Type: new 
Abstract: Surface roughness is a critical factor influencing the fatigue life of structural components. Its effect is commonly quantified using a correction coefficient known as the surface factor. In this paper, a phase field based numerical framework is proposed to estimate the surface factor while accounting for the stochastic nature of surface roughness. The model is validated against existing experimental data. Furthermore, we investigate the influence of key parameters on the fatigue life of rough surfaces, such as surface topology and failure strength. An important effect of surface roughness is observed when the average surface roughness increases and the correlation length of the surface profile decreases. This effect becomes more pronounced with higher failure strengths.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Scheme of Electromagnetic Scattering From Scatterers With Incomplete Profiles</title>
<link>https://arxiv.org/abs/2505.02086</link>
<guid>https://arxiv.org/abs/2505.02086</guid>
<content:encoded><![CDATA[
<div> deep learning, electromagnetic scattering, incomplete profile, limited scattering data, forward and inverse problems

Summary:
A new deep learning scheme is proposed to solve electromagnetic scattering problems where the profile of the dielectric scatterer is incomplete. The scheme utilizes a limited amount of scattering data to compensate for the missing profile information. Existing solvers struggle to handle this situation effectively. The proposed scheme addresses this challenge by simultaneously solving the forward and inverse scattering problems. By using deep learning, the EM forward scattering from an incompletely known dielectric scatterer is derived, and numerical experiments are conducted to demonstrate the scheme's performance for both 2-D and 3-D EM scattering problems. The results showcase the effectiveness of the proposed deep learning-based approach in recovering the unknown parts of the scatterer profile accurately. 

<br /><br />Summary: <div>
arXiv:2505.02086v1 Announce Type: new 
Abstract: A deep learning scheme is proposed to solve the electromagnetic (EM) scattering problems where the profile of the dielectric scatterer of interest is incomplete. As a compensation, a limited amount of scattering data is provided, which is in principle containing sufficient information associated with the missing part of the profile. The existing solvers can hardly realize the compensation if the known part of the profile and the scattering data are combined straightforwardly. On one hand, the well-developed forward solvers have no mechanism to accept the scattering data, which can recover the unknown part of the profile if properly used. On the other hand, the existing solvers for inverse problems cannot retrieve the complete profile with an acceptable accuracy from the limited amount of scattering data, even when the available part of the profile can be fed into the solvers. This work aims to handle the difficulty. To this end, the EM forward scattering from an incompletely known dielectric scatterer is derived. A scheme based on DL is then proposed where the forward and inverse scattering problems are solved simultaneously. Numerical experiments are conducted to demonstrate the performance of the proposed DL-based scheme for both two-dimensional (2-D) and three-dimensional (3-D) EM scattering problems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning-Aided Approach for Estimating Field Permeability Map by Fusing Well Logs, Well Tests, and Seismic Data</title>
<link>https://arxiv.org/abs/2505.02093</link>
<guid>https://arxiv.org/abs/2505.02093</guid>
<content:encoded><![CDATA[
<div> Data fusion, permeability maps, reservoir simulation, convolutional neural network, Western Siberia<br />
<br />
Summary: 
Obtaining reliable permeability maps for oil reservoirs is essential for accurate reservoir simulation models and recovery strategies. Existing methods face challenges due to the integration of various data sources and lack of direct inter-well space information. This study proposes a novel data-fusion approach to predict two-dimensional permeability maps across the entire reservoir area. Utilizing non-parametric regression with a customized kernel shape, incorporating well logs, tests, and seismic data. A convolutional neural network processes seismic data and integrates it with other sources using a multi-stage fusion procedure to enhance the training dataset and construct the permeability map. Testing on a real oil reservoir in Western Siberia shows the developed map aligns with well permeability estimations and significantly improves inter-well space permeability predictions through seismic data integration. <br /><br /> <div>
arXiv:2505.02093v1 Announce Type: new 
Abstract: Obtaining reliable permeability maps of oil reservoirs is crucial for building a robust and accurate reservoir simulation model and, therefore, designing effective recovery strategies. This problem, however, remains challenging, as it requires the integration of various data sources by experts from different disciplines. Moreover, there are no sources to provide direct information about the inter-well space. In this work, a new method based on the data-fusion approach is proposed for predicting two-dimensional permeability maps on the whole reservoir area. This method utilizes non-parametric regression with a custom kernel shape accounting for different data sources: well logs, well tests, and seismics. A convolutional neural network is developed to process seismic data and then incorporate it with other sources. A multi-stage data fusion procedure helps to artificially increase the training dataset for the seismic interpretation model and finally to construct the adequate permeability map. The proposed methodology of permeability map construction from different sources was tested on a real oil reservoir located in Western Siberia. The results demonstrate that the developed map perfectly corresponds to the permeability estimations in the wells, and the inter-well space permeability predictions are considerably improved through the incorporation of the seismic data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking</title>
<link>https://arxiv.org/abs/2505.02139</link>
<guid>https://arxiv.org/abs/2505.02139</guid>
<content:encoded><![CDATA[
<div> representation learning, limit order book, financial market, LOBench, China A-share market<br />
Summary:<br />
The paper introduces a systematic comparative study of limit order book (LOB) representation learning to extract transferable, compact features capturing essential LOB properties. A standardized benchmark called LOBench is presented with curated datasets, unified preprocessing, and strong baselines using real China A-share market data. The study shows the sufficiency and necessity of LOB representations for various downstream tasks, highlighting their advantages over traditional task-specific end-to-end models and advanced representation learning models for general time series. This work establishes a reproducible framework and provides clear guidelines for future research. The datasets and code are publicly available at the provided link, allowing for further exploration and validation of the proposed methods.<br /> <div>
arXiv:2505.02139v1 Announce Type: new 
Abstract: The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constrains, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at https://github.com/financial-simulation-lab/LOBench.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach</title>
<link>https://arxiv.org/abs/2505.02170</link>
<guid>https://arxiv.org/abs/2505.02170</guid>
<content:encoded><![CDATA[
<div> fantasy football, integer programming, hybrid scoring metric, artificial intelligence, Premier League

Summary:
The paper introduces novel deterministic and robust integer programming models for selecting the optimal starting eleven and captain in fantasy football, a billion-dollar industry. A hybrid scoring metric is proposed using an interpretable artificial intelligence framework, leading to the highest scores while maintaining consistent performance. The models' performance is evaluated using data from the 2023/24 Premier League season, showing effectiveness during out-of-sample periods. Strategic averaging techniques for estimating cost vectors and the proposed hybrid approach are found to be successful. The paper provides insights into optimal formations and player selections, offering valuable strategies for fantasy football enthusiasts. <div>
arXiv:2505.02170v1 Announce Type: new 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Constrained by a fixed budget, decision-makers draft a squad whose players are expected to perform well in the upcoming weeks to maximize total points. This paper proposes novel deterministic and robust integer programming models that select the optimal starting eleven and the captain. A new hybrid scoring metric is constructed using an interpretable artificial intelligence framework and underlying match performance data. Several objective functions and estimation techniques are introduced for the programming model. To the best of my knowledge, this is the first study to approach fantasy football through this lens. The models' performance is evaluated using data from the 2023/24 Premier League season. Results indicate that the proposed hybrid method achieved the highest score while maintaining consistent performance. Utilizing the Monte Carlo simulation, the strategic choice of averaging techniques for estimating cost vectors, and the proposed hybrid approach are shown to be effective during the out-of-sample period. This paper also provides a thorough analysis of the optimal formations and players selected by the models, offering valuable insights into effective fantasy football strategies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Method for Optimizing Submarine Search and Rescue Strategy Under Environmental Uncertainty</title>
<link>https://arxiv.org/abs/2505.02186</link>
<guid>https://arxiv.org/abs/2505.02186</guid>
<content:encoded><![CDATA[
<div> framework, dynamic analysis, Monte Carlo method, Bayesian method, economic optimization
Summary:
The article presents a hybrid algorithm framework for locating and rescuing deep-sea submersibles in uncertain ocean environments. By combining dynamic analysis, Monte Carlo, and Bayesian methods, a probabilistic prediction approach is used to improve search efficiency. The Monte Carlo method is employed to account for environmental variability, enhancing location prediction accuracy. Bayesian grid research and probabilistic updating are integrated based on trajectory predictions, with Bayesian filtering for complex scenarios. Economic optimization is conducted through cost-benefit analysis using the entropy weight method, and the CER is applied for evaluation. This comprehensive approach aims to maximize the rate of successful rescues while minimizing costs, addressing the challenges of deep-sea submersible rescue operations effectively. <br /><br /> <div>
arXiv:2505.02186v1 Announce Type: new 
Abstract: When coping with the urgent challenge of locating and rescuing a deep-sea submersible in the event of communication or power failure, environmental uncertainty in the ocean can not be ignored. However, classic physical models are limited to deterministic scenarios. Therefore, we present a hybrid algorithm framework combined with dynamic analysis for target submarine, Monte Carlo and Bayesian method for conducting a probabilistic prediction to improve the search efficiency. Herein, the Monte Carlo is performed to overcome the environmental variability to improve the accuracy in location prediction. According to the trajectory prediction, we integrated the Bayesian based grid research and probabilistic updating. For more complex situations, we introduced the Bayesian filtering. Aiming to maximize the rate of successful rescue and costs, the economic optimization is performed utilizing the cost-benefit analysis based on entropy weight method and the CER is applied for evaluation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting the Dynamics of Complex System via Multiscale Diffusion Autoencoder</title>
<link>https://arxiv.org/abs/2505.02450</link>
<guid>https://arxiv.org/abs/2505.02450</guid>
<content:encoded><![CDATA[
<div> Keywords: Multiscale Diffusion Prediction Network, complex systems, latent space, spatiotemporal evolution, graph neural network

Summary:
The article introduces a new method, the Multiscale Diffusion Prediction Network (MDPNet), for predicting the dynamics of complex systems. Existing methods often overlook the multiscale structure of complex systems, leading to inaccuracies in predictions of spatiotemporal evolution. MDPNet aims to address this by leveraging the multiscale structure to discover the latent space of intrinsic dynamics. It utilizes a multiscale diffusion autoencoder to encode multiscale features and guide the diffusion model for reliable reconstruction. Additionally, an attention-based graph neural ordinary differential equation is introduced to model the co-evolution across different scales. The proposed method shows promising results in extensive evaluations on representative systems, achieving a significant average prediction error reduction of 53.23% compared to baseline methods. It also demonstrates superior robustness and generalization, highlighting its potential for various scientific and engineering applications. 

<br /><br />Summary: <div>
arXiv:2505.02450v1 Announce Type: new 
Abstract: Predicting the dynamics of complex systems is crucial for various scientific and engineering applications. The accuracy of predictions depends on the model's ability to capture the intrinsic dynamics. While existing methods capture key dynamics by encoding a low-dimensional latent space, they overlook the inherent multiscale structure of complex systems, making it difficult to accurately predict complex spatiotemporal evolution. Therefore, we propose a Multiscale Diffusion Prediction Network (MDPNet) that leverages the multiscale structure of complex systems to discover the latent space of intrinsic dynamics. First, we encode multiscale features through a multiscale diffusion autoencoder to guide the diffusion model for reliable reconstruction. Then, we introduce an attention-based graph neural ordinary differential equation to model the co-evolution across different scales. Extensive evaluations on representative systems demonstrate that the proposed method achieves an average prediction error reduction of 53.23% compared to baselines, while also exhibiting superior robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Compression for Time Series Modelling: A Case Study of Smart Grid Demand Forecasting</title>
<link>https://arxiv.org/abs/2505.02606</link>
<guid>https://arxiv.org/abs/2505.02606</guid>
<content:encoded><![CDATA[
<div> wavelet-based compression, time series forecasting, smart energy systems, data management, high-frequency data <br />
<br />
Summary: <br />
Efficient time series forecasting is crucial for smart energy systems, but the increasing amount of high-frequency data poses storage and transmission challenges. This study investigates the use of Discrete Wavelet Transform (DWT)-based data compression to address these challenges while maintaining forecasting accuracy. By applying biorthogonal wavelets at different compression rates, the study evaluates the impact on three forecasting models: Ordinary Least Squares (OLS), XGBoost, and Time Series Dense Encoder (TiDE). Results show that XGBoost is robust to compression artifacts, while OLS is sensitive to smooth wavelets and high compression rates. TiDE demonstrates some variability but remains competitive. The study suggests that wavelet-based compression can efficiently manage data in smart energy systems without compromising forecasting accuracy, with potential applications in climate modeling, water supply systems, and industrial operations. <div>
arXiv:2505.02606v1 Announce Type: new 
Abstract: Efficient time series forecasting is essential for smart energy systems, enabling accurate predictions of energy demand, renewable resource availability, and grid stability. However, the growing volume of high-frequency data from sensors and IoT devices poses challenges for storage and transmission. This study explores Discrete Wavelet Transform (DWT)-based data compression as a solution to these challenges while ensuring forecasting accuracy. A case study of a seawater supply system in Hirtshals, Denmark, operating under dynamic weather, operational schedules, and seasonal trends, is used for evaluation.
  Biorthogonal wavelets of varying orders were applied to compress data at different rates. Three forecasting models - Ordinary Least Squares (OLS), XGBoost, and the Time Series Dense Encoder (TiDE) - were tested to assess the impact of compression on forecasting performance. Lossy compression rates up to $r_{\mathrm{lossy}} = 0.999$ were analyzed, with the Normalized Mutual Information (NMI) metric quantifying the relationship between compression and information retention. Results indicate that wavelet-based compression can retain essential features for accurate forecasting when applied carefully.
  XGBoost proved highly robust to compression artifacts, maintaining stable performance across diverse compression rates. In contrast, OLS demonstrated sensitivity to smooth wavelets and high compression rates, while TiDE showed some variability but remained competitive. This study highlights the potential of wavelet-based compression for scalable, efficient data management in smart energy systems without sacrificing forecasting accuracy. The findings are relevant to other fields requiring high-frequency time series forecasting, including climate modeling, water supply systems, and industrial operations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components</title>
<link>https://arxiv.org/abs/2505.01627</link>
<guid>https://arxiv.org/abs/2505.01627</guid>
<content:encoded><![CDATA[
<div> Automated Classification, Large Language Models, Function-Based Design, Mechanical Assembly Parts, Domain Adaptation <br />
Summary: <br />
The study introduces a novel framework utilizing Large Language Models (LLMs) for automated classification of mechanical assembly parts' functions in the conceptual design phase of product development. Functional modeling, a critical aspect of early-phase engineering, is often hindered by the lack of structured functional data. Large Language Models (LLMs) show promise in addressing this gap by automating function annotation through domain adaptation (DA) using fine-tuning. The study showcases the effectiveness of fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR) to enhance the semantic representation of mechanical parts. Evaluation on the A Big CAD (ABC) dataset demonstrates that domain-adapted LLMs can generate high-quality functional data, improving early design decision-making and supporting more effective design exploration. <br /> <div>
arXiv:2505.01627v1 Announce Type: cross 
Abstract: The conceptual design phase represents a critical early stage in the product development process, where designers generate potential solutions that meet predefined design specifications based on functional requirements. Functional modeling, a foundational aspect of this phase, enables designers to reason about product functions before specific structural details are determined. A widely adopted approach to functional modeling is the Function-Behavior-Structure (FBS) framework, which supports the transformation of functional intent into behavioral and structural descriptions. However, the effectiveness of function-based design is often hindered by the lack of well-structured and comprehensive functional data. This scarcity can negatively impact early design decision-making and hinder the development of accurate behavioral models. Recent advances in Large Language Models (LLMs), such as those based on GPT architectures, offer a promising avenue to address this gap. LLMs have demonstrated significant capabilities in language understanding and natural language processing (NLP), making them suitable for automated classification tasks. This study proposes a novel LLM-based domain adaptation (DA) framework using fine-tuning for the automated classification of mechanical assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the traditionally manual and subjective process of function annotation can be improved in both accuracy and consistency. A case study demonstrates fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the domain-adapted LLM can generate high-quality functional data, enhancing the semantic representation of mechanical parts and supporting more effective design exploration in early-phase engineering.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strengthening Infrastructure Resilience to Hurricanes by Modeling Transportation and Electric Power Network Interdependencies</title>
<link>https://arxiv.org/abs/2404.12978</link>
<guid>https://arxiv.org/abs/2404.12978</guid>
<content:encoded><![CDATA[
<div> agent-based model, resilience, hurricane, infrastructure disruptions, interdependencies

Summary:
The study introduces an agent-based model (ABM) to analyze community resilience to hurricane-induced infrastructure disruptions, specifically focusing on the interconnections between electric power and transportation networks. Agents in the ABM represent different system components, such as electric power network, transportation network, hazards, and households. By considering interactions within and among systems, the model simulates household resilience during a hurricane in Miami-Dade County, Florida. The model incorporates two key interdependencies: the role of transportation in fuel delivery to power plants and restoration teams' access, as well as the impact of power outages on transportation network components. Validated against Hurricane Irma data, the ABM demonstrates the effectiveness of a traffic lights-based restoration strategy, prioritizing signal recovery to minimize traffic disruptions and accelerate household power restoration. The study emphasizes the importance of timely traffic signal restoration, road accessibility for restoration teams, and uninterrupted fuel transportation for efficient power restoration in hurricane scenarios. 

<br /><br />Summary: <div>
arXiv:2404.12978v2 Announce Type: replace 
Abstract: This study presents an agent-based model (ABM) developed to simulate the resilience of a community to hurricane-induced infrastructure disruptions, focusing on the interdependencies between electric power and transportation networks. In this ABM approach, agents represent the components of a system, where interactions within a system shape intra-dependency of a system and interactions among systems shape interdependencies. To study household resilience subject to a hurricane, a library of agents has been created including electric power network, transportation network, wind/flooding hazards, and household agents. The ABM is applied over the household and infrastructure data from a community (Zip code 33147) in Miami-Dade County, Florida. Interdependencies between the two networks are modeled in two ways, (i) representing the role of transportation in fuel delivery to power plants and restoration teams' access, (ii) impact of power outage on transportation network components. Restoring traffic signals quickly is crucial as their outage can slow down traffic and increase the chance of crashes. We simulate three restoration strategies: component based, distance based, and traffic lights based restoration. The model is validated against Hurricane Irma data, showing consistent behavior with varying hazard intensities. Scenario analyses explore the impact of restoration strategies, road accessibility, and wind speed intensities on power restoration. Results demonstrate that a traffic lights based restoration strategy efficiently prioritizes signal recovery without delaying household power restoration time. Restoration of power services will be faster if restoration teams do not need to wait due to inaccessible roads and fuel transportation to power plants is not delayed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A finite strain model for fiber angle plasticity of textile fabrics based on isogeometric shell finite elements</title>
<link>https://arxiv.org/abs/2412.20131</link>
<guid>https://arxiv.org/abs/2412.20131</guid>
<content:encoded><![CDATA[
<div> shear elastoplasticity model, textile fabrics, anisotropic Kirchhoff-Love shells, plasticity, frictional sliding <br />
Summary: <br />
This work introduces a shear elastoplasticity model for textile fabrics based on anisotropic Kirchhoff-Love shells with embedded fiber bending. The model accounts for rotational inter-ply frictional sliding between fiber families in textile composites experiencing large deformation, emphasizing dry fabrics like woven and non-crimp fabrics. Utilizing relative angles between fiber families as strain measures, the model is formulated using surface invariants without thickness integration. A yield function with isotropic hardening and simple evolution equation is proposed, calibrated using the picture frame test, and validated with experimental data like the bias extension test. The elastoplastic model's accuracy is confirmed through good agreement with experimental results, and its application to 3D shell problems is demonstrated. <div>
arXiv:2412.20131v2 Announce Type: replace 
Abstract: This work presents a shear elastoplasticity model for textile fabrics within the theoretical framework of anisotropic Kirchhoff-Love shells with bending of embedded fibers proposed by Duong et al. (2023). The plasticity model aims at capturing the rotational inter-ply frictional sliding between fiber families in textile composites undergoing large deformation. Such effects are usually dominant in dry textile fabrics such as woven and non-crimp fabrics. The model explicitly uses relative angles between fiber families as strain measures for the kinematics. The plasticity model is formulated directly with surface invariants without resorting to thickness integration. Motivated by experimental observations from the picture frame test, a yield function is proposed with isotropic hardening and a simple evolution equation. A classical return mapping algorithm is employed to solve the elastoplastic problem within the isogeometric finite shell element formulation of Duong et al. (2022). The verification of the implementation is facilitated by the analytical solution for the picture frame test. The proposed plasticity model is calibrated from the picture frame test and is then validated by the bias extension test, considering available experimental data for different samples from the literature. Good agreement between model prediction and experimental data is obtained. Finally, the applicability of the elastoplasticity model to 3D shell problems is demonstrated.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiskLabs: Predicting Financial Risk Using Large Language Model based on Multimodal and Multi-Sources Data</title>
<link>https://arxiv.org/abs/2404.07452</link>
<guid>https://arxiv.org/abs/2404.07452</guid>
<content:encoded><![CDATA[
<div> framework, LLMs, financial risk prediction, RiskLabs, multimodal data <br />
Summary: <br />
This paper introduces RiskLabs, a framework that utilizes large language models (LLMs) to predict financial risks by integrating multimodal financial data sources such as Earnings Conference Calls (ECCs), market-related time series data, and contextual news data. The study demonstrates the effectiveness of RiskLabs in forecasting market volatility and variance. It examines the contributions of different data sources to financial risk assessment and emphasizes the importance of LLMs in this process. The paper also discusses the challenges associated with using LLMs for financial risk prediction and explores the potential benefits of combining LLMs with multimodal data for enhanced risk analysis. <div>
arXiv:2404.07452v2 Announce Type: replace-cross 
Abstract: The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering, and stock movement prediction (binary classification), the application of LLMs to financial risk prediction remains underexplored. Addressing this gap, in this paper, we introduce RiskLabs, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely integrates multimodal financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data to improve financial risk prediction. Empirical results demonstrate RiskLabs' effectiveness in forecasting both market volatility and variance. Through comparative experiments, we examine the contributions of different data sources to financial risk assessment and highlight the crucial role of LLMs in this process. We also discuss the challenges associated with using LLMs for financial risk prediction and explore the potential of combining them with multimodal data for this purpose.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Spectral-based Physics-informed Finite Operator Learning for Prediction of Mechanical Behavior of Microstructures</title>
<link>https://arxiv.org/abs/2410.19027</link>
<guid>https://arxiv.org/abs/2410.19027</guid>
<content:encoded><![CDATA[
<div> operator learning, spectral methods, heterogeneous materials, physics-informed, microstructures<br />
<br />
Summary: A novel physics-informed operator learning technique based on spectral methods introduces the Lippmann-Schwinger operator in Fourier space to model heterogeneous materials. The method accelerates training by enabling gradient construction on a fixed discretization in Fourier space and maps microstructure shapes to mechanical responses without labeled data. Training minimizes equilibrium in Fourier space under loading conditions, ensuring periodicity. Physically constrained and diverse training data enhance accuracy, although performance may degrade for out-of-distribution microstructures. Integration of a Fourier Neural Operator improves accuracy in predicting stress fields and offers zero-shot super-resolution capabilities in heterogeneous domains. Extension to handle 3D problems and adaptation to finite elasticity demonstrate robustness in handling nonlinear mechanical behavior. This framework shows promise for efficient and scalable prediction of mechanical responses in complex material systems while reducing training time for physics-informed neural operators. <br /><br /> <div>
arXiv:2410.19027v3 Announce Type: replace-cross 
Abstract: A novel physics-informed operator learning technique based on spectral methods is introduced to model the complex behavior of heterogeneous materials. The Lippmann-Schwinger operator in Fourier space is employed to construct physical constraints with minimal computational overhead, effectively eliminating the need for automatic differentiation. The introduced methodology accelerates the training process by enabling gradient construction on a fixed, finite discretization in Fourier space. Later, the spectral physics-informed finite operator learning (SPiFOL) framework is built based on this discretization and trained to map the arbitrary shape of microstructures to their mechanical responses (strain fields) without relying on labeled data. The training is done by minimizing equilibrium in Fourier space concerning the macroscopic loading condition, which also guarantees the periodicity. SPiFOL, as a physics-informed operator learning method, enables rapid predictions through forward inference after training. To ensure accuracy, we incorporate physical constraints and diversify the training data. However, performance may still degrade for out-of-distribution microstructures. SPiFOL is further enhanced by integrating a Fourier Neural Operator (FNO). Compared to the standard data-driven FNO, SPiFOL shows higher accuracy in predicting stress fields and provides nearly resolution-independent results. Additionally, its zero-shot super-resolution capabilities are explored in heterogeneous domains. Finally, SPiFOL is extended to handle 3D problems and further adapted to finite elasticity, demonstrating the robustness of the framework in handling nonlinear mechanical behavior. The framework shows great potential for efficient and scalable prediction of mechanical responses in complex material systems while also reducing the training time required for training physics-informed neural operators.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction</title>
<link>https://arxiv.org/abs/2501.01010</link>
<guid>https://arxiv.org/abs/2501.01010</guid>
<content:encoded><![CDATA[
<div> State Space Model, cryptocurrency, Bitcoin, forecasting, financial data
<br />
Summary:
<br />
- The study addresses the challenge of predicting Bitcoin prices due to market volatility and dynamics.
- Traditional models like ARIMA and neural networks struggle with regime shifts and dependencies in data.
- The proposed CryptoMamba, a Mamba-based State Space Model, effectively captures long-range dependencies.
- CryptoMamba outperforms previous models in accuracy and generalizability across market conditions.
- The model's practical utility is demonstrated through accurate forecasts translating into financial gains, making it advantageous for stock and cryptocurrency price forecasting tasks. 
<br /> <div>
arXiv:2501.01010v2 Announce Type: replace-cross 
Abstract: Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming physics-informed machine learning to convex optimization</title>
<link>https://arxiv.org/abs/2505.01047</link>
<guid>https://arxiv.org/abs/2505.01047</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Physics-Informed, Convex Optimization, B-splines, Adaptive Knot Optimization

Summary:
Convex-PIML is a framework proposed to address optimization challenges in Physics-Informed Machine Learning (PIML). By transforming PIML into convex optimization, the framework overcomes limitations and enables effective integration of data with physical laws. The use of linear combination of B-splines promotes the convexity of the loss function, allowing well-established convex optimization algorithms to be utilized for efficient solutions. An adaptive knot optimization method tackles the spectral bias issue of PIML, enhancing performance. The framework is theoretically guaranteed and tested across scenarios with various physical priors, demonstrating effective solution of optimization problems. Convex-PIML shows promise for diverse applications by offering a comprehensive approach to combining data and physics for scientific problem-solving. <br /><br />Summary: <div>
arXiv:2505.01047v1 Announce Type: new 
Abstract: Physics-Informed Machine Learning (PIML) offers a powerful paradigm of integrating data with physical laws to address important scientific problems, such as parameter estimation, inferring hidden physics, equation discovery, and state prediction, etc. However, PIML still faces many serious optimization challenges that significantly restrict its applications. In this study, we propose a comprehensive framework that transforms PIML to convex optimization to overcome all these limitations, referred to as Convex-PIML. The linear combination of B-splines is utilized to approximate the data, promoting the convexity of the loss function. By replacing the non-convex components of the loss function with convex approximations, the problem is further converted into a sequence of successively refined approximated convex optimization problems. This conversion allows the use of well-established convex optimization algorithms, obtaining solutions effectively and efficiently. Furthermore, an adaptive knot optimization method based on error estimate is introduced to mitigate the spectral bias issue of PIML, further improving the performance. The proposed theoretically guaranteed framework is tested in scenarios with distinct types of physical prior. The results indicate that optimization problems are effectively solved in these scenarios, highlighting the potential of the framework for broad applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduced-order structure-property linkages for stochastic metamaterials</title>
<link>https://arxiv.org/abs/2505.01283</link>
<guid>https://arxiv.org/abs/2505.01283</guid>
<content:encoded><![CDATA[
<div> additive manufacturing, mechanical metamaterials, unit cell geometries, materials informatics framework, Gaussian process regression

Summary:<br />
The article discusses the efficient design and evaluation of mechanical metamaterials using additive manufacturing. It emphasizes the need to establish connections between unit cell designs and their mechanical properties, which can be computationally intensive. The study employs principal component analysis to identify key features from a large dataset of 2D metamaterials. Fast Fourier transform-based simulations are utilized to calculate the effective elastic stiffness of different unit cell designs. Gaussian process regression is then applied to create reduced-order models mapping designs to their elastic constants. The research demonstrates the creation of robust structure-property maps through a low-dimensional representation of the dataset. Additionally, an active learning approach is used to train a surrogate model with minimal data points, showcasing the ability to generate accurate maps with a small fraction of the original dataset. <div>
arXiv:2505.01283v1 Announce Type: new 
Abstract: The capabilities of additive manufacturing have facilitated the design and production of mechanical metamaterials with diverse unit cell geometries. Establishing linkages between the vast design space of unit cells and their effective mechanical properties is critical for the efficient design and performance evaluation of such metamaterials. However, physics-based simulations of metamaterial unit cells across the entire design space are computationally expensive, necessitating a materials informatics framework to efficiently capture complex structure-property relationships. In this work, principal component analysis of 2-point correlation functions is performed to extract the salient features from a large dataset of randomly generated 2D metamaterials. Physics-based simulations are performed using a fast Fourier transform (FFT)-based homogenization approach to efficiently compute the homogenized effective elastic stiffness across the extensive unit cell designs. Subsequently, Gaussian process regression is used to generate reduced-order surrogates, mapping unit cell designs to their homogenized effective elastic constant. It is demonstrated that the adopted workflow enables a high-value low-dimensional representation of the voluminous stochastic metamaterial dataset, facilitating the construction of robust structure-property maps. Finally, an uncertainty-based active learning framework is utilized to train a surrogate model with a significantly smaller number of data points compared to the original full dataset. It is shown that a dataset as small as $0.61\%$ of the entire dataset is sufficient to generate accurate and robust structure-property maps.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimICD: A Closed-Loop Simulation Framework For ICD Therapy</title>
<link>https://arxiv.org/abs/2505.01371</link>
<guid>https://arxiv.org/abs/2505.01371</guid>
<content:encoded><![CDATA[
<div> simulation, ICD behavior, cardiac electrophysiology, arrhythmic episodes, therapy progression

Summary:
SimICD is a new simulation tool that integrates virtual ICD logic algorithms with cardiac electrophysiology simulations to simulate therapy progression decisions during arrhythmic episodes. This tool fills a gap in available models by allowing for the testing of ICD functionality in a controlled environment before clinical use. The simulations conducted with SimICD demonstrate the realistic simulation of cardiac signals and ICD responses that align with real-world devices' logic. This enables the reprogramming of ICD parameters to adapt to specific tachy-arrhythmia episodes, improving treatment customization and efficacy. Overall, SimICD facilitates virtual studies of ICD behavior, enhancing the understanding and testing of device functionality for better clinical outcomes. 

<br /><br />Summary: <div>
arXiv:2505.01371v1 Announce Type: new 
Abstract: Virtual studies of ICD behaviour are crucial for testing device functionality in a controlled environment prior to clinical application. Although previous works have shown the viability of using in silico testing for diagnosis, there is a notable gap in available models that can simulate therapy progression decisions during arrhythmic episodes. This work introduces SimICD, a simulation tool which combines virtual ICD logic algorithms with cardiac electrophysiology simulations in a feedback loop, allowing the progression of ICD therapy protocols to be simulated for a range of tachy-arrhythmia episodes. Using a cohort of virtual patients, we demonstrate the ability of SimICD to simulate realistic cardiac signals and ICD responses that align with the logic of real-world devices, facilitating the reprogramming of ICD parameters to adapt to specific episodes.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design for a Digital Twin in Clinical Patient Care</title>
<link>https://arxiv.org/abs/2505.01206</link>
<guid>https://arxiv.org/abs/2505.01206</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Twins, clinical patient care, knowledge graphs, ensemble learning, decision-making <br />
Summary:<br />
Digital Twins have the potential to revolutionize personalized clinical patient care by integrating knowledge graphs and ensemble learning to create a comprehensive reflection of a patient's clinical journey. These Digital Twins are predictive, modular, evolving, informed, interpretable, and explainable, providing valuable insights for clinicians to make informed decisions. By incorporating these elements, Digital Twins can cater to a variety of medical fields, from oncology to epidemiology, offering a versatile tool for healthcare professionals. This innovative approach ensures that Digital Twins are not only accurate and predictive but also adaptable to changing patient needs. Overall, this new design for Digital Twins has the capacity to significantly enhance clinical decision-making processes and improve patient outcomes.<br />Summary: <div>
arXiv:2505.01206v1 Announce Type: cross 
Abstract: Digital Twins hold great potential to personalize clinical patient care, provided the concept is translated to meet specific requirements dictated by established clinical workflows. We present a generalizable Digital Twin design combining knowledge graphs and ensemble learning to reflect the entire patient's clinical journey and assist clinicians in their decision-making. Such Digital Twins can be predictive, modular, evolving, informed, interpretable and explainable with applications ranging from oncology to epidemiology.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks</title>
<link>https://arxiv.org/abs/2402.08978</link>
<guid>https://arxiv.org/abs/2402.08978</guid>
<content:encoded><![CDATA[
<div> visualization, financial cluster analysis, quantitative analysis, qualitative analysis, multi-view clustering

Summary: 
Prismatic is a visual analytics system designed to assist investors in financial cluster analysis by integrating quantitative and qualitative analysis. It addresses challenges such as numerous pairwise comparisons and dynamic correlations across different time periods. Prismatic features dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation processes. By using a multi-view clustering approach, it combines data-driven clusters with knowledge-driven similarity, providing a more nuanced understanding of business correlations. The system offers well-coordinated visual views to facilitate a comprehensive interpretation of intertwined quantitative and qualitative features. Case studies on formulating concept stocks and interviews with domain experts demonstrate the effectiveness and usefulness of Prismatic in discovering investment alternatives and managing risks in the financial market. <div>
arXiv:2402.08978v2 Announce Type: replace-cross 
Abstract: Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural Networks</title>
<link>https://arxiv.org/abs/2409.07769</link>
<guid>https://arxiv.org/abs/2409.07769</guid>
<content:encoded><![CDATA[
<div> Graph neural network, mesh-based super-resolution, fluid flows, multiscale model, Reynolds number
<br />
Summary:
<br />
This study presents a novel approach using a graph neural network (GNN) for mesh-based three-dimensional super-resolution of fluid flows. The GNN operates on localized meshes of elements to improve accuracy. The architecture includes coarse-scale and fine-scale processors separated by a graph unpooling layer for multiscale modeling. Results from simulations of Taylor-Green Vortex and backward-facing step flow demonstrate the GNN's ability to produce accurate super-resolved fields compared to coarse-scale and multiscale models. Reconstruction errors increase with higher Reynolds numbers. Additionally, the GNN shows promising capabilities for geometry extrapolation in cross-mesh scenarios. <div>
arXiv:2409.07769v4 Announce Type: replace-cross 
Abstract: A graph neural network (GNN) approach is introduced in this work which enables mesh-based three-dimensional super-resolution of fluid flows. In this framework, the GNN is designed to operate not on the full mesh-based field at once, but on localized meshes of elements (or cells) directly. To facilitate mesh-based GNN representations in a manner similar to spectral (or finite) element discretizations, a baseline GNN layer (termed a message passing layer, which updates local node properties) is modified to account for synchronization of coincident graph nodes, rendering compatibility with commonly used element-based mesh connectivities. The architecture is multiscale in nature, and is comprised of a combination of coarse-scale and fine-scale message passing layer sequences (termed processors) separated by a graph unpooling layer. The coarse-scale processor embeds a query element (alongside a set number of neighboring coarse elements) into a single latent graph representation using coarse-scale synchronized message passing over the element neighborhood, and the fine-scale processor leverages additional message passing operations on this latent graph to correct for interpolation errors. Demonstration studies are performed using hexahedral mesh-based data from Taylor-Green Vortex and backward-facing step flow simulations at Reynolds numbers of 1600 and 3200. Through analysis of both global and local errors, the results ultimately show how the GNN is able to produce accurate super-resolved fields compared to targets in both coarse-scale and multiscale model configurations. Reconstruction errors for fixed architectures were found to increase in proportion to the Reynolds number. Geometry extrapolation studies on a separate cavity flow configuration show promising cross-mesh capabilities of the super-resolution strategy.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Multimodal Multiscale Data Fusion for Digital Twins in Aerosol Jet Electronics Printing</title>
<link>https://arxiv.org/abs/2505.00176</link>
<guid>https://arxiv.org/abs/2505.00176</guid>
<content:encoded><![CDATA[
<div> Aerosol Jet Printing, Additive Manufacturing, Machine Learning, Process-Structure-Property Modeling, Diffusion Models<br />
<br />
Summary: This study introduces a novel generative modeling methodology using diffusion models to fuse multimodal and multiscale Process-Structure-Property (PSP) data in Aerosol Jet Printing (AJP). The method aims to enhance manufacturing by quantitatively connecting process parameters, structural features, and material properties. Current machine learning approaches for AJP face limitations in handling complex data, highlighting the need for comprehensive analysis through fusion methods. The proposed approach registers and fuses optical microscopy and confocal profilometry data from AJP, capturing intricate PSP relationships and providing insights into dynamic manufacturing systems' digital twins. The results demonstrate effective fusion and fine-tuning steps, offering a deeper understanding of complex AJP processes.<br /><br />Summary: <div>
arXiv:2505.00176v1 Announce Type: new 
Abstract: The rising demand for high-value electronics necessitates advanced manufacturing techniques capable of meeting stringent specifications for precise, complex, and compact devices, driving the shift toward innovative additive manufacturing (AM) solutions. Aerosol Jet Printing (AJP) is a versatile AM technique that utilizes aerosolized functional materials to accurately print intricate patterns onto diverse substrates. Machine learning (ML)- based Process-Structure-Property (PSP) modeling is essential for enhancing AJP manufacturing, as it quantitatively connects process parameters, structural features, and resulting material properties. However, current ML approaches for modeling PSP relationships in AJP face significant limitations in handling multimodal and multiscale data, underscoring a critical need for generative methods capable of comprehensive analysis through multimodal and multiscale fusion. To address this challenge, this study introduces a novel generative modeling methodology leveraging diffusion models for PSP data fusion in AJP. The proposed method integrates multimodal, multiscale PSP features in two phases: (1) registering the features, and (2) fusing them to generate causal relationships between PSP attributes. A case study demonstrates the registration and fusion of optical microscopy (OM) images and confocal profilometry (CP) data from AJP, along with the fine-tuning of the fusion step. The results effectively capture complex PSP relationships, offering deeper insights into digital twins of dynamic manufacturing systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Thermal Control Based on Spatial Thermal Comfort with Reconstructed Environmental Data</title>
<link>https://arxiv.org/abs/2505.00468</link>
<guid>https://arxiv.org/abs/2505.00468</guid>
<content:encoded><![CDATA[
<div> Reconstruction, Thermal comfort, Predicted Mean Vote (PMV), Gappy Proper Orthogonal Decomposition (Gappy POD), Multi-occupant living lab environment

Summary:
The study introduces a novel method for estimating thermal comfort by incorporating spatial environmental data reconstructed using the Gappy Proper Orthogonal Decomposition (Gappy POD) algorithm. This addresses limitations of fixed-location sensors by enabling accurate reconstruction of indoor temperature fields. A group PMV-based control framework is developed to consider the thermal comfort of multiple occupants, allowing for individual and group-level thermal condition calculations. Experimental results demonstrate the effectiveness of the Gappy POD algorithm in temperature reconstruction with a low average relative error. The spatial variability in PMV values based on occupant location highlights the importance of adaptive thermal control strategies. The study underscores the significance of adaptive thermal control strategies that consider both spatial and individual variability for enhanced occupant-centric building operations. <br /><br />Summary: <div>
arXiv:2505.00468v1 Announce Type: new 
Abstract: Achieving thermal comfort while maintaining energy efficiency is a critical objective in building system control. Conventional thermal comfort models, such as the Predicted Mean Vote (PMV), rely on both environmental and personal variables. However, the use of fixed-location sensors limits the ability to capture spatial variability, which reduces the accuracy of occupant-specific comfort estimation. To address this limitation, this study proposes a new PMV estimation method that incorporates spatial environmental data reconstructed using the Gappy Proper Orthogonal Decomposition (Gappy POD) algorithm. In addition, a group PMV-based control framework is developed to account for the thermal comfort of multiple occupants. The Gappy POD method enables fast and accurate reconstruction of indoor temperature fields from sparse sensor measurements. Using these reconstructed fields and occupant location data, spatially resolved PMV values are calculated. Group-level thermal conditions are then derived through statistical aggregation methods and used to control indoor temperature in a multi-occupant living lab environment. Experimental results show that the Gappy POD algorithm achieves an average relative error below 3\% in temperature reconstruction. PMV distributions varied by up to 1.26 scale units depending on occupant location. Moreover, thermal satisfaction outcomes varied depending on the group PMV method employed. These findings underscore the importance for adaptive thermal control strategies that incorporate both spatial and individual variability, offering valuable insights for future occupant-centric building operations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review</title>
<link>https://arxiv.org/abs/2505.00210</link>
<guid>https://arxiv.org/abs/2505.00210</guid>
<content:encoded><![CDATA[
<div> machine learning, manufacturing, control systems, generative models, process monitoring<br />
<br />
Summary: This review explores the integration of generative machine learning (ML) in dynamic manufacturing processes to enhance in-situ monitoring and control systems. The complex characteristics of manufacturing systems, including time-varying parameters and uncertainties, necessitate advanced control techniques that can respond in real-time while maintaining product quality. The review categorizes approaches into Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated methods, highlighting the potential of generative ML in decision-making, process guidance, simulation, and digital twins. However, challenges such as the separation between generation and control functions, lack of physical understanding of manufacturing phenomena, and model adaptation from other domains need to be addressed. Future research directions focus on developing integrated frameworks that combine generative ML and control technologies to effectively address the dynamic complexities of modern manufacturing systems. <div>
arXiv:2505.00210v1 Announce Type: cross 
Abstract: Dynamic manufacturing processes exhibit complex characteristics defined by time-varying parameters, nonlinear behaviors, and uncertainties. These characteristics require sophisticated in-situ monitoring techniques utilizing multimodal sensor data and adaptive control systems that can respond to real-time feedback while maintaining product quality. Recently, generative machine learning (ML) has emerged as a powerful tool for modeling complex distributions and generating synthetic data while handling these manufacturing uncertainties. However, adopting these generative technologies in dynamic manufacturing systems lacks a functional control-oriented perspective to translate their probabilistic understanding into actionable process controls while respecting constraints. This review presents a functional classification of Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated approaches, offering a perspective for understanding existing ML-enhanced control systems and incorporating generative ML. The analysis of generative ML architectures within this framework demonstrates control-relevant properties and potential to extend current ML-enhanced approaches where conventional methods prove insufficient. We show generative ML's potential for manufacturing control through decision-making applications, process guidance, simulation, and digital twins, while identifying critical research gaps: separation between generation and control functions, insufficient physical understanding of manufacturing phenomena, and challenges adapting models from other domains. To address these challenges, we propose future research directions aimed at developing integrated frameworks that combine generative ML and control technologies to address the dynamic complexities of modern manufacturing systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subspace-Distance-Enabled Active Learning for Efficient Data-Driven Model Reduction of Parametric Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.00460</link>
<guid>https://arxiv.org/abs/2505.00460</guid>
<content:encoded><![CDATA[
<div> active learning, data-driven model reduction, parametric data-driven reduced-order model, proper orthogonal decomposition, subspace-distance-enabled active learning

Summary:<br />
In scenarios where high-fidelity dynamical systems require repetitive evaluation across a wide range of parameter configurations without access to governing equations, data-driven model reduction techniques are preferred. This study introduces an active learning method for constructing a parametric data-driven reduced-order model by selecting crucial parameter samples from the parameter domain. The approach involves representing high-fidelity solution snapshots in parameter-specific linear subspaces using proper orthogonal decomposition, with the relative distance between these subspaces guiding the active learning process. A distance metric is provided for comparing similarity between linear subspaces of different dimensions. The proposed subspace-distance-enabled active learning (SDE-AL) framework is successfully applied to enhance existing reduced-order modeling methods through active-learning-driven extensions. Positive results are demonstrated for two parametric physical models, showcasing the effectiveness of the SDE-AL approach.  <div>
arXiv:2505.00460v1 Announce Type: cross 
Abstract: In situations where the solution of a high-fidelity dynamical system needs to be evaluated repeatedly, over a vast pool of parametric configurations and in absence of access to the underlying governing equations, data-driven model reduction techniques are preferable. We propose a novel active learning approach to build a parametric data-driven reduced-order model (ROM) by greedily picking the most important parameter samples from the parameter domain. As a result, during the ROM construction phase, the number of high-fidelity solutions dynamically grow in a principled fashion. The high-fidelity solution snapshots are expressed in several parameter-specific linear subspaces, with the help of proper orthogonal decomposition (POD), and the relative distance between these subspaces is used as a guiding mechanism to perform active learning. For successfully achieving this, we provide a distance measure to evaluate the similarity between pairs of linear subspaces with different dimensions, and also show that this distance measure is a metric. The usability of the proposed subspace-distance-enabled active learning (SDE-AL) framework is demonstrated by augmenting two existing non-intrusive reduced-order modeling approaches, and providing their active-learning-driven (ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN. Furthermore, we report positive results for two parametric physical models, highlighting the efficiency of the proposed SDE-AL approach.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks</title>
<link>https://arxiv.org/abs/2505.00530</link>
<guid>https://arxiv.org/abs/2505.00530</guid>
<content:encoded><![CDATA[
<div> Keywords: SMILES-based molecule generation, deep reinforcement learning, catastrophic forgetting, molecule validity, exploration mechanisms

Summary:<br />
The article introduces a novel RL algorithm, Partial SMILES Validation-PPO (PSV-PPO), designed to address catastrophic forgetting during molecule generation in drug discovery. By incorporating real-time partial SMILES validation at each step of the sequence generation process, PSV-PPO prevents the deterioration of molecule validity while promoting exploration. Unlike traditional approaches that validate molecules only after completion, PSV-PPO evaluates potential branches at each step, ensuring high validity rates even during aggressive exploration. Experimental results on benchmark datasets show that PSV-PPO reduces invalid structures while maintaining competitive performance. The framework of PSV-PPO can be extended to incorporate additional domain knowledge, enhancing RL applications in drug discovery. <br /><br />Summary: <div>
arXiv:2505.00530v1 Announce Type: cross 
Abstract: SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMM-based DEX on the XRP Ledger</title>
<link>https://arxiv.org/abs/2312.13749</link>
<guid>https://arxiv.org/abs/2312.13749</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Market Maker, Decentralized Exchanges, XRP Ledger, Agent-based simulations, Continuous Auction Mechanism

Summary:
The study compares an Automated Market Maker (AMM)-based Decentralized Exchange (DEX) implementation on the XRP Ledger (XRPL) against a Generic AMM-based DEX on Ethereum. Through agent-based simulations using real market data, the XRPL-AMM-DEX demonstrates superior price synchronization, reduced slippage, and improved returns due to lower fees and shorter block times on the XRPL. The study also highlights the benefits of the integrated Continuous Auction Mechanism (CAM) in mitigating impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). This comparative analysis is the first to explore protocol-level and smart contract AMM-based DEX implementations and validate theoretical auction mechanisms through simulations. <div>
arXiv:2312.13749v4 Announce Type: replace 
Abstract: Automated Market Maker (AMM)-based Decentralized Exchanges (DEXs) are crucial in Decentralized Finance (DeFi), but Ethereum implementations suffer from high transaction costs and price synchronization challenges. To address these limitations, we compare the XRP Ledger (XRPL)-AMM-Decentralized Exchange (DEX), a protocol-level implementation, against a Generic AMM-based DEX (G-AMM-DEX) on Ethereum, akin to Uniswap's V2 AMM implementation, through agent-based simulations using real market data and multiple volatility scenarios generated via Geometric Brownian Motion (GBM). Results demonstrate that the XRPL-AMM-DEX achieves superior price synchronization, reduced slippage, and improved returns due to XRPL's lower fees and shorter block times, with benefits amplifying during market volatility. The integrated Continuous Auction Mechanism (CAM) further mitigates impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). To the best of our knowledge, this study represents the first comparative analysis between protocol-level and smart contract AMM-based DEX implementations and the first agent-based simulation validating theoretical auction mechanisms for AMM-based DEXs.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing</title>
<link>https://arxiv.org/abs/2504.21317</link>
<guid>https://arxiv.org/abs/2504.21317</guid>
<content:encoded><![CDATA[
<div> redundancy, machine learning, additive manufacturing, process monitoring, mitigation

Summary:<br />
- Redundancy in machine learning-based additive manufacturing process monitoring systems can lead to increased costs, compromised performance, and high computational requirements.
- This paper defines redundancy at sample-level, feature-level, and model-level and proposes a multi-level redundancy mitigation framework.
- The framework includes methods such as data registration, downscaling, cross-modality knowledge transfer, and model pruning to reduce redundancy and improve model performance.
- In a case study for in-situ defect detection in directed energy deposition, the proposed approach showed a 91% reduction in latency, a 47% decrease in error rate, and a 99.4% reduction in storage requirements.
- The framework also allows for lower sensor costs and energy consumption, resulting in a lightweight, cost-effective, and scalable monitoring system.

<br /><br />Summary: <div>
arXiv:2504.21317v1 Announce Type: new 
Abstract: The deployment of machine learning (ML)-based process monitoring systems has significantly advanced additive manufacturing (AM) by enabling real-time defect detection, quality assessment, and process optimization. However, redundancy is a critical yet often overlooked challenge in the deployment and operation of ML-based AM process monitoring systems. Excessive redundancy leads to increased equipment costs, compromised model performance, and high computational requirements, posing barriers to industrial adoption. However, existing research lacks a unified definition of redundancy and a systematic framework for its evaluation and mitigation. This paper defines redundancy in ML-based AM process monitoring and categorizes it into sample-level, feature-level, and model-level redundancy. A comprehensive multi-level redundancy mitigation (MLRM) framework is proposed, incorporating advanced methods such as data registration, downscaling, cross-modality knowledge transfer, and model pruning to systematically reduce redundancy while improving model performance. The framework is validated through an ML-based in-situ defect detection case study for directed energy deposition (DED), demonstrating a 91% reduction in latency, a 47% decrease in error rate, and a 99.4% reduction in storage requirements. Additionally, the proposed approach lowers sensor costs and energy consumption, enabling a lightweight, cost-effective, and scalable monitoring system. By defining redundancy and introducing a structured mitigation framework, this study establishes redundancy analysis and mitigation as a key enabler of efficient ML-based process monitoring in production environments.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Security Analysis and Implementation of Cryptocurrency Systems on Blockchain 2.0</title>
<link>https://arxiv.org/abs/2504.21367</link>
<guid>https://arxiv.org/abs/2504.21367</guid>
<content:encoded><![CDATA[
<div> blockchain, decentralization, smart contracts, security, cryptocurrency
<br />
Summary: 
Blockchain technology has revolutionized decentralization by providing a trust system through cryptography and computing power. Smart contracts have further expanded blockchain application possibilities by enabling automatic execution based on predefined triggers. However, the programmability of smart contracts introduces security vulnerabilities. This article delves into the technical details of blockchain 2.0, focusing on Ethereum, and explains the operation principles of contract virtual machines. It discusses how cryptocurrencies are constructed and operated on blockchain 2.0, highlighting common security issues and solutions. Drawing on research and on-chain practices, this comprehensive perspective aims to enhance the understanding of cryptocurrency technology on blockchain 2.0 and offers insights for creating more secure cryptocurrency contracts. <div>
arXiv:2504.21367v1 Announce Type: new 
Abstract: Blockchain technology has set off a wave of decentralization in the world since its birth. The trust system constructed by blockchain technology based on cryptography algorithm and computing power provides a practical and powerful solution to solve the trust problem in human society. In order to make more convenient use of the characteristics of blockchain and build applications on it, smart contracts appear. By defining some trigger automatic execution contracts, the application space of blockchain is expanded and the foundation for the rapid development of blockchain is laid. This is blockchain 2.0. However, the programmability of smart contracts also introduces vulnerabilities. In order to cope with the insufficient security guarantee of high-value application networks running on blockchain 2.0 and smart contracts, this article will be represented by Ethereum to introduce the technical details of understanding blockchain 2.0 and the operation principle of contract virtual machines, and explain how cryptocurrencies based on blockchain 2.0 are constructed and operated. The common security problems and solutions are also discussed. Based on relevant research and on-chain practice, this paper provides a complete and comprehensive perspective to understanding cryptocurrency technology based on blockchain 2.0 and provides a reference for building more secure cryptocurrency contracts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level datasets training method in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2504.21328</link>
<guid>https://arxiv.org/abs/2504.21328</guid>
<content:encoded><![CDATA[
<div> Neural Networks, Physics-Informed, PDEs, Multi-grid method, High-frequency components <br />
Summary: Physics-Informed Neural Networks (PINNs) have shown promise in solving PDEs but struggle with stiff and high-frequency problems, leading to accuracy issues. An alternative approach inspired by the multi-grid method is proposed to address these challenges. By training with different levels of samples, errors of varying frequencies can be efficiently removed, improving accuracy without complex tuning. The method is tested on 1D and 2D equations with high-frequency components and Lid-driven cavity flows at different Reynolds numbers. Results show a 30% to 60% accuracy improvement and success in solving complex high-frequency PDEs up to Re=5000. Synergies with transfer learning are also explored for more challenging problems. <div>
arXiv:2504.21328v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks have emerged as a promising methodology for solving PDEs, gaining significant attention in computer science and various physics-related fields. Despite being demonstrated the ability to incorporate the physics of laws for versatile applications, PINNs still struggle with the challenging problems which are stiff to be solved and/or have high-frequency components in the solutions, resulting in accuracy and convergence issues. It may not only increase computational costs, but also lead to accuracy loss or solution divergence. In this study, an alternative approach is proposed to mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD community, the underlying idea of the current approach is to efficiently remove different frequency errors via training with different levels of training samples, resulting in a simpler way to improve the training accuracy without spending time in fine-tuning of neural network structures, loss weights as well as hyperparameters. To demonstrate the efficacy of current approach, we first investigate canonical 1D ODE with high-frequency component and 2D convection-diffusion equation with V-cycle training strategy. Finally, the current method is employed for the classical benchmark problem of steady Lid-driven cavity flows at different Reynolds numbers, to investigate the applicability and efficacy for the problem involved multiple modes of high and low frequency. By virtue of various training sequence modes, improvement through predictions lead to 30% to 60% accuracy improvement. We also investigate the synergies between current method and transfer learning techniques for more challenging problems (i.e., higher Re). From the present results, it also revealed that the current framework can produce good predictions even for the case of Re=5000, demonstrating the ability to solve complex high-frequency PDEs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation</title>
<link>https://arxiv.org/abs/2504.21574</link>
<guid>https://arxiv.org/abs/2504.21574</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Artificial Intelligence, financial ecosystem, cybersecurity, ethical risks, regulatory landscape

Summary: 
Generative Artificial Intelligence (GenAI) is revolutionizing the financial sector by enhancing customer engagement, automating workflows, and extracting insights from vast data. This survey explores how banks, insurers, asset managers, and fintech firms globally are adopting GenAI tools like large language models. While driving innovation, GenAI poses cybersecurity threats like AI-generated phishing and ethical concerns around bias and data misuse. The evolving regulatory landscape is analyzed, including initiatives for risk-based AI governance by major financial regulators. Best practices for secure and responsible GenAI adoption are proposed, such as explainability techniques and human oversight. By drawing on academic research, industry cases, and policy frameworks, this chapter offers insights on leveraging GenAI's potential while managing its complex risks.<br /><br />Summary: <div>
arXiv:2504.21574v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence (GenAI) is rapidly reshaping the global financial landscape, offering unprecedented opportunities to enhance customer engagement, automate complex workflows, and extract actionable insights from vast financial data. This survey provides an overview of GenAI adoption across the financial ecosystem, examining how banks, insurers, asset managers, and fintech startups worldwide are integrating large language models and other generative tools into their operations. From AI-powered virtual assistants and personalized financial advisory to fraud detection and compliance automation, GenAI is driving innovation across functions. However, this transformation comes with significant cybersecurity and ethical risks. We discuss emerging threats such as AI-generated phishing, deepfake-enabled fraud, and adversarial attacks on AI systems, as well as concerns around bias, opacity, and data misuse. The evolving global regulatory landscape is explored in depth, including initiatives by major financial regulators and international efforts to develop risk-based AI governance. Finally, we propose best practices for secure and responsible adoption - including explainability techniques, adversarial testing, auditability, and human oversight. Drawing from academic literature, industry case studies, and policy frameworks, this chapter offers a perspective on how the financial sector can harness GenAI's transformative potential while navigating the complex risks it introduces.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of HPC-Accelerated CFD in National Security and Defense</title>
<link>https://arxiv.org/abs/2504.07837</link>
<guid>https://arxiv.org/abs/2504.07837</guid>
<content:encoded><![CDATA[
<div> HPC, Computational Fluid Dynamics, defense applications, open-source frameworks, MPI domain decomposition, GPU acceleration, hybrid parallelism, research voids, exascale readiness, machine learning surrogate models<br />
Summary: <br />
This review discusses the use of High-Performance Computing (HPC) and Computational Fluid Dynamics (CFD) in defense-related national security applications. It examines the utilization of open-source CFD frameworks such as OpenFOAM, SU2, and ADflow in security-sensitive simulations. The review also explores how HPC techniques like MPI domain decomposition and GPU acceleration, along with hybrid parallelism, enhance open-source frameworks for managing large defense CFD simulations. Additionally, it addresses the technological advancements and research voids driving the field's development, categorizing scientific contributions into air, maritime, and space domains. The review highlights modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers, showcasing how custom open-source solutions support workflows for rapid completion of multi-million cell simulations. Finally, it discusses new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to improve CFD use in defense research and development. <div>
arXiv:2504.07837v2 Announce Type: replace 
Abstract: Using High-Performance Computing (HPC), Computational Fluid Dynamics (CFD) now serves as an essential component in defense-related national security applications including missile interception and hypersonic propulsion as well as naval stealth optimization and urban hazard dispersion. This review combines two decades of open-source and public-domain research on HPC-accelerated CFD in defense, addressing three key questions: Which security-sensitive simulations have utilized open-source CFD frameworks such as OpenFOAM, SU2 and ADflow? Which HPC techniques, such as MPI domain decomposition and GPU acceleration together with hybrid parallelism best enhance open-source frameworks to manage large defense CFD simulations? Which technological advancements and research voids currently drive the directional development of the field? Examining several research studies sourced from NASA, DoD HPC centers, and academic institutions, scientific contributions have been classified into air, maritime, and space domains. Modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers show how custom open-source solutions support workflows with rapid completion of multi-million cell simulations. The conclusion highlights new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to deliver practical insights for improving CFD use in defense research and development.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews</title>
<link>https://arxiv.org/abs/2502.05439</link>
<guid>https://arxiv.org/abs/2502.05439</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic systems, financial services industry, modeling crew, model risk management, autonomous decision-making<br />
Summary: 
This paper delves into the utilization of agentic systems in the financial services sector, employing teams of artificial intelligence agents to carry out intricate modeling and model risk management tasks with human oversight. The modeling crew, comprising a judge agent and specialized agents, executes a spectrum of activities from data analysis to model evaluation. Likewise, the model risk management crew, under the watchful eye of a judge agent, ensures compliance, replicability, and soundness of models. Numerical examples applied in credit card fraud detection, credit card approval, and portfolio credit risk modeling demonstrate the efficacy and reliability of these agentic crews. The integration of human judgment with machine efficiency showcases the potential of agentic systems in driving innovation and automation in financial decision-making processes.<br /><br />Summary: <div>
arXiv:2502.05439v2 Announce Type: replace-cross 
Abstract: The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews with human-in-the-loop module that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a judge agent and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection/hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a judge agent along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLiB: High-fidelity differentiable modeling of lithium-ion batteries and efficient gradient-based parameter identification</title>
<link>https://arxiv.org/abs/2504.20674</link>
<guid>https://arxiv.org/abs/2504.20674</guid>
<content:encoded><![CDATA[
<div> DiffLiB, LIB simulation, gradient-based inverse parameter identification, automatic differentiation, advanced computational framework<br />
Summary:<br />
DiffLiB is introduced as a high-fidelity LIB simulation framework that utilizes advanced differentiable programming techniques for efficient gradient-based inverse parameter identification in the complex DFN model. Customized automatic differentiation rules are defined to enable efficient gradient-based optimization, improving computational performance significantly compared to gradient-free methods. The framework shows excellent agreement in forward predictions, maintaining low terminal voltage discrepancies. In parameter identification tasks using measured voltage data, DiffLiB demonstrates superior computational performance with significantly fewer forward predictions and less computational time required. These results highlight DiffLiB as a versatile and powerful computational tool for advanced LIB development. <br /> <div>
arXiv:2504.20674v1 Announce Type: new 
Abstract: The physics-based Doyle-Fuller-Newman (DFN) model, widely adopted for its precise electrochemical modeling, stands out among various simulation models of lithium-ion batteries (LIBs). Although the DFN model is powerful in forward predictive analysis, the inverse identification of its model parameters has remained a long-standing challenge. The numerous unknown parameters associated with the nonlinear, time-dependent, and multi-scale DFN model are extremely difficult to be determined accurately and efficiently, hindering the practical use of such battery simulation models in industrial applications. To tackle this challenge, we introduce DiffLiB, a high-fidelity finite-element-based LIB simulation framework, equipped with advanced differentiable programming techniques so that efficient gradient-based inverse parameter identification is enabled. Customized automatic differentiation rules are defined by identifying the VJP (vector-Jacobian product) structure in the chain rule and implemented using adjoint-based implicit differentiation methods. Four numerical examples, including both 2D and 3D forward predictions and inverse parameter identification, are presented to validate the accuracy and computational efficiency of DiffLiB. Benchmarking against COMSOL demonstrates excellent agreement in forward predictions, with terminal voltage discrepancies maintaining a root-mean-square error (RMSE) below 2 mV across all test conditions. In parameter identification tasks using experimentally measured voltage data, the proposed gradient-based optimization scheme achieves superior computational performance, with 96% fewer forward predictions and 72% less computational time compared with gradient-free approaches. These results demonstrate that DiffLiB is a versatile and powerful computational framework for the development of advanced LIBs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Heterogeneity within Elastic and Inelastic Discrete Mechanical Models</title>
<link>https://arxiv.org/abs/2504.20861</link>
<guid>https://arxiv.org/abs/2504.20861</guid>
<content:encoded><![CDATA[
<div> Keywords: heterogeneous media, elastic behavior, fracture behavior, homogenization, randomization <br />
Summary: 
The study examines the elastic and fracture behaviors of discrete models of heterogeneous media with different forms of randomization. These models achieve homogeneity through volumetric-deviatoric decomposition or stress homogenization methods. It is observed that stress oscillations in heterogeneous geometric structures cannot be accurately replicated by randomizing elastic parameters in homogeneous models. Additionally, the macroscopic response to uniaxial tension shows differences between homogenized and standard materials, with the homogenized material exhibiting higher peak stress and steeper softening. However, randomizing elastic parameters and adjusting inelastic parameters can bring the macroscopic response closer to the standard material, despite differing damage distributions. This research sheds light on the potential for controlled random assignment of heterogeneity in homogeneous models and provides valuable insights into the behavior of these materials. <br /><br /> <div>
arXiv:2504.20861v1 Announce Type: new 
Abstract: The study investigates the elastic and fracture behaviors of discrete, elastically homogeneous models of heterogeneous media. The homogeneity is accomplished either by volumetric-deviatoric decomposition of constitutive function or by an auxiliary stress homogenization method. The elastic parameters of the homogenized material models are randomly varied in space to introduce heterogeneity independently of the geometric properties of the discrete model. Several forms of randomization are investigated using statistical properties of nodal stress oscillations in periodic representative volume elements (RVEs). It is found that the stress oscillations present in discrete models built on heterogeneous geometric structures with standard constitutive models cannot be replicated by randomization of the elastically homogeneous discrete system. The marginal distributions as well as dependencies between stress tensor components cannot be adequately matched.
  With respect to quasi-brittle fracture behavior, the macroscopic response of the different models is studied for the load case of uniaxial tension. The elastically homogenized material provides higher peak stress occurring at lower strain levels and a steeper softening phase, compared to the standard material. Randomization of the elastic material parameters, as well as adjustment of inelastic material parameters, brings the macroscopic response of the homogenized material close to that of the standard material, although the damage distribution prior to the strain localization differs. These findings provide insight into the potential for controlled, random assignment of heterogeneity in homogeneous models, using physically-based discretizations of material structure with standard constitutive models for comparison.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiscale modelling of thermally stressed superelastic polyimide</title>
<link>https://arxiv.org/abs/2504.20123</link>
<guid>https://arxiv.org/abs/2504.20123</guid>
<content:encoded><![CDATA[
<div> Keywords: thermo-mechanical processes, superelastic polyimide, multiscale approach, smoothed particle hydrodynamics, molecular dynamics

Summary: 
Thermo-mechanical processes at the atomistic scale in superelastic polyimide are studied using a sequential multiscale approach. The continuum-scale smoothed particle hydrodynamics (SPH) model is coupled with atomistic molecular dynamics (MD) to investigate thermal expansion and stress relaxation. Constitutive modelling integrates thermo-mechanical properties derived from MD simulations. Benchmark tests on heat transfer validate the results. Simulation of the insulation capabilities of superelastic polyimide on an aluminium plate shows significant reduction in thermal stress, strain, and temperature development. The multiscale method effectively captures thermo-mechanical interactions in superelastic polyimide, demonstrating its potential for exploring material behavior under thermal stress. <div>
arXiv:2504.20123v1 Announce Type: cross 
Abstract: Many thermo-mechanical processes, such as thermal expansion and stress relaxation, originate at the atomistic scale. We develop a sequential multiscale approach to study thermally stressed superelastic polyimide to explore these effects. The continuum-scale smoothed particle hydrodynamics (SPH) model is coupled with atomistic molecular dynamics (MD) through constitutive modelling, where thermo-mechanical properties and equations of state are derived from MD simulations. The results are verified through benchmark problems of heat transfer. Finally, we analyse the insulating capabilities of superelastic polyimide by simulating the thermal response of an aluminium plate. The result shows a considerable reduction in the thermal stress, strain and temperature field development in the aluminium plate when superelastic polyimide is used as an insulator. The present work demonstrates the effectiveness of the multi-scale method in capturing thermo-mechanical interactions in superelastic polyimide.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling</title>
<link>https://arxiv.org/abs/2504.20586</link>
<guid>https://arxiv.org/abs/2504.20586</guid>
<content:encoded><![CDATA[
<div> Monte Carlo method, capacitance extraction, variance reduction, layout-dependent effects, floating random walk <br />
<br />
Summary: 
The article introduces a new variance reduction method for floating random walk-based capacitance extraction, a method commonly used in integrated circuit analysis. This new method, designed to address challenges in ever-denser process technologies and layout-dependent effects, offers a significant improvement in extraction efficiency. It complements existing mathematical formulations for variance reduction and has been shown to reduce variance in all extractions, particularly those affected by layout-dependent effects. Numerical experiments demonstrate that the new method can decrease the number of walks required by up to 30% and reduce overall extraction times even further compared to previously proposed variance reduction techniques for floating random walks. This advancement in capacitance extraction techniques could have a significant impact on the efficiency and accuracy of integrated circuit analysis for complex designs. <br /> <div>
arXiv:2504.20586v1 Announce Type: cross 
Abstract: Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 30% in number of walks necessary and even more in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open FinLLM Leaderboard: Towards Financial AI Readiness</title>
<link>https://arxiv.org/abs/2501.10963</link>
<guid>https://arxiv.org/abs/2501.10963</guid>
<content:encoded><![CDATA[
<div> Keywords: FinLLMs, multimodal capabilities, open leaderboard, financial tasks, AI models performance <br />
Summary: Financial large language models (FinLLMs), equipped with multimodal capabilities, are poised to transform various applications in business, finance, accounting, and auditing. To drive real-world adoption, robust benchmarks for assessing FinLLMs' and FinAgents' performance are essential. An open FinLLM leaderboard, established in collaboration with Linux Foundation and Hugging Face, offers a platform for evaluating and comparing AI models across a wide range of financial tasks. By democratizing access to financial knowledge and intelligence, these advancements can empower chatbots or agents to elevate individuals' analytical proficiency to a professional level in a matter of months. The open leaderboard is inclusive of contributions from academia, the open-source community, industry, and stakeholders, with a focus on continually updating with new datasets, tasks, and models. By fostering a collaborative and transparent ecosystem, the aim is to advance readiness for financial AI solutions. <br /><br />Summary: <div>
arXiv:2501.10963v2 Announce Type: replace 
Abstract: Financial large language models (FinLLMs) with multimodal capabilities are envisioned to revolutionize applications across business, finance, accounting, and auditing. However, real-world adoption requires robust benchmarks of FinLLMs' and FinAgents' performance. Maintaining an open leaderboard is crucial for encouraging innovative adoption and improving model effectiveness. In collaboration with Linux Foundation and Hugging Face, we create an open FinLLM leaderboard, which serves as an open platform for assessing and comparing AI models' performance on a wide spectrum of financial tasks. By demoncratizing access to advances of financial knowledge and intelligence, a chatbot or agent may enhance the analytical capabilities of the general public to a professional level within a few months of usage. This open leaderboard welcomes contributions from academia, open-source community, industry, and stakeholders. In particular, we encourage contributions of new datasets, tasks, and models for continual update. Through fostering a collaborative and open ecosystem, we seek to promote financial AI readiness.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression</title>
<link>https://arxiv.org/abs/2504.18925</link>
<guid>https://arxiv.org/abs/2504.18925</guid>
<content:encoded><![CDATA[
<div> Neural Voxel Contextual Coding, Vector Quantization Contextual Coding, 4DGS data compression, multi-rate compression, storage reduction<br />
Summary:<br />
This study introduces 4DGS-CC, a contextual coding framework for compressing 4D Gaussian Splatting (4DGS) data to address storage challenges. The framework decomposes the 4DGS data into 4D neural voxels and 3DGS components for efficient compression. It leverages Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC) to compress the data, achieving a storage reduction of approximately 12 times while maintaining rendering fidelity. The approach separates temporal and spatial dimensions in the data decomposition process and utilizes prior information for contextual coding using NVCC. Additionally, a codebook is employed to store spherical harmonics information from canonical 3DGS, which is compressed using VQCC with auxiliary hyperpriors. The integrated NVCC and VQCC enable tailored multi-rate compression of 4DGS data, making it suitable for specific storage requirements. Extensive experiments validate the effectiveness of the proposed method in achieving significant storage savings without compromising data quality. <br /><br /> <div>
arXiv:2504.18925v1 Announce Type: new 
Abstract: Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints.Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively.Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding.Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook.By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error</title>
<link>https://arxiv.org/abs/2504.19963</link>
<guid>https://arxiv.org/abs/2504.19963</guid>
<content:encoded><![CDATA[
<div> probabilistic model, subspaces, principal component analysis, model reduction, computational mechanics
<br />
Summary:<br />
This paper introduces a probabilistic model of subspaces based on probabilistic principal component analysis (PCA). The method uses quantities derived from probabilistic PCA to construct distributions of the sample matrix and the principal subspaces in an embedding space. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition. The constructed stochastic subspace can help characterize model-form uncertainty in computational mechanics. The method is justified by probabilistic PCA, satisfying linear constraints like boundary conditions, and has only one hyperparameter, simplifying training. The algorithm is easy to implement. Comparisons with existing approaches show promising results in low-dimensional visualization, parametric static problems, and dynamics modeling of space structures. <div>
arXiv:2504.19963v1 Announce Type: new 
Abstract: This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We compare the proposed method with existing approaches in a low-dimensional visualization example and a parametric static problem, and demonstrate its performance in a dynamics model of a space structure.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantBench: Benchmarking AI Methods for Quantitative Investment</title>
<link>https://arxiv.org/abs/2504.18600</link>
<guid>https://arxiv.org/abs/2504.18600</guid>
<content:encoded><![CDATA[
<div> benchmark, artificial intelligence, quantitative investment, industry practices, QuantBench  
Summary:  
QuantBench is introduced as a benchmark platform for AI in quantitative investment, aiming to bridge the gap between academic research and industry practices. It offers standardization aligned with industry standards, flexibility for integrating AI algorithms, and full coverage of the quantitative investment process. Using QuantBench, empirical studies highlight the importance of continual learning to address distribution shifts, improved methods for modeling relational financial data, and robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common platform for evaluation, QuantBench fosters collaboration between researchers and practitioners, accelerating progress in the field of AI for quantitative investment, similar to the impact of benchmark platforms in other domains such as computer vision and natural language processing. <div>
arXiv:2504.18600v1 Announce Type: cross 
Abstract: The field of artificial intelligence (AI) in quantitative investment has seen significant advancements, yet it lacks a standardized benchmark aligned with industry practices. This gap hinders research progress and limits the practical application of academic innovations. We present QuantBench, an industrial-grade benchmark platform designed to address this critical need. QuantBench offers three key strengths: (1) standardization that aligns with quantitative investment industry practices, (2) flexibility to integrate various AI algorithms, and (3) full-pipeline coverage of the entire quantitative investment process. Our empirical studies using QuantBench reveal some critical research directions, including the need for continual learning to address distribution shifts, improved methods for modeling relational financial data, and more robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common ground for evaluation and fostering collaboration between researchers and practitioners, QuantBench aims to accelerate progress in AI for quantitative investment, similar to the impact of benchmark platforms in computer vision and natural language processing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Open-Source Software Is Less Likely to Become Abandoned Than One Might Think! Lessons from Curating a Catalog of Maintained Scientific Software</title>
<link>https://arxiv.org/abs/2504.18971</link>
<guid>https://arxiv.org/abs/2504.18971</guid>
<content:encoded><![CDATA[
<div> classification, scientific software, longevity, survival models, open-source

Summary:
Using large language models, the study classifies over 18,000 scientific software projects, analyzing their attributes to understand factors affecting longevity. Infrastructural layers, downstream dependencies, publication mentions, and government participants are linked to longer lifespans, while newer projects with academic participants have shorter lifespans. Despite common perceptions, scientific projects have a longer lifespan than non-scientific open-source projects. The curated dataset provides a valuable resource for future research on scientific software, offering insights that could help prolong the lifespan of both scientific and non-scientific software projects. <div>
arXiv:2504.18971v1 Announce Type: cross 
Abstract: Scientific software is essential to scientific innovation and in many ways it is distinct from other types of software. Abandoned (or unmaintained), buggy, and hard to use software, a perception often associated with scientific software can hinder scientific progress, yet, in contrast to other types of software, its longevity is poorly understood. Existing data curation efforts are fragmented by science domain and/or are small in scale and lack key attributes. We use large language models to classify public software repositories in World of Code into distinct scientific domains and layers of the software stack, curating a large and diverse collection of over 18,000 scientific software projects. Using this data, we estimate survival models to understand how the domain, infrastructural layer, and other attributes of scientific software affect its longevity. We further obtain a matched sample of non-scientific software repositories and investigate the differences. We find that infrastructural layers, downstream dependencies, mentions of publications, and participants from government are associated with a longer lifespan, while newer projects with participants from academia had shorter lifespan. Against common expectations, scientific projects have a longer lifetime than matched non-scientific open-source software projects. We expect our curated attribute-rich collection to support future research on scientific software and provide insights that may help extend longevity of both scientific and other projects.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spreading of highly cohesive metal powders with transverse oscillation kinematics</title>
<link>https://arxiv.org/abs/2504.18981</link>
<guid>https://arxiv.org/abs/2504.18981</guid>
<content:encoded><![CDATA[
<div> powder bed additive manufacturing, laser powder bed fusion, binder jetting, transverse oscillation kinematic, dense powder layers<br />
<br />
Summary: 
The study examines the challenges of spreading fine powders in powder bed additive manufacturing processes and proposes a transverse oscillation kinematic for powder spreading. Computational simulations using a DEM-FEM framework show that transverse oscillation of a non-rotating roller can facilitate the spreading of dense powder layers with high packing fractions. Experimental validation confirms the computational results, with high packing fractions achieved for transverse oscillation frequencies above 200 Hz. Statistical analysis demonstrates that increasing transverse surface velocity improves layer uniformity and reduces cracking defects. The proposed transverse oscillation kinematic has the potential to produce thin and consistently uniform powder layers in additive manufacturing processes, offering a promising solution for handling highly cohesive powders. <br /><br /> <div>
arXiv:2504.18981v1 Announce Type: cross 
Abstract: Powder bed additive manufacturing processes such as laser powder bed fusion (LPBF) or binder jetting (BJ) benefit from using fine (D50 $\leq20~\mu m$) powders. However, the increasing level of cohesion with decreasing particle size makes spreading a uniform and continuous layer challenging. As a result, LPBF typically employs a coarser size distribution, and rotating roller mechanisms are used in BJ machines, that can create wave-like surface profiles due to roller run-out.
  In this work, a transverse oscillation kinematic for powder spreading is proposed, explored computationally, and validated experimentally. Simulations are performed using an integrated discrete element-finite element (DEM-FEM) framework and predict that transverse oscillation of a non-rotating roller facilitates the spreading of dense powder layers (beyond 50% packing fraction) with a high level of robustness to kinematic parameters. The experimental study utilizes a custom-built mechanized powder spreading testbed and X-ray transmission imaging for the analysis of spread powder layers. Experimental results generally validate the computational results, however, also exhibit parasitic layer cracking. For transverse oscillation frequencies above 200 Hz, powder layers of high packing fraction (between 50-60%) were formed, and for increased layer thicknesses, highly uniform and continuous layers were deposited. Statistical analysis of the experimental powder layer morphology as a function of kinematic spreading parameters revealed that an increasing transverse surface velocity improves layer uniformity and reduces cracking defects. This suggests that with minor improvements to the machine design, the proposed transverse oscillation kinematic has the potential to result in thin and consistently uniform powder layers of highly cohesive powder.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Bayesian Optimal Experimental Design with Normalizing Flows</title>
<link>https://arxiv.org/abs/2404.13056</link>
<guid>https://arxiv.org/abs/2404.13056</guid>
<content:encoded><![CDATA[
<div> Bayesian optimal experimental design, variational OED, normalizing flows, Monte Carlo estimators, gradient-based optimization<br />
<br />
Summary: The study introduces a novel approach, vOED-NFs, which utilizes normalizing flows (NFs) to enhance variational optimal experimental design (vOED) by approximating posterior distributions. The method employs NFs with a conditional invertible neural network architecture and includes a summary network for data dimension reduction. Monte Carlo estimators and gradient expressions enable simultaneous optimization of variational parameters and design variables. The algorithm is validated on benchmark problems and applied to scenarios involving cathodic electrophoretic deposition and stochastic modeling of aphid population. Results demonstrate that a composition of 4-5 coupling layers reduces EIG estimation bias, while NFs provide accurate approximations of posterior distributions, effectively capturing non-Gaussian and multi-modal features. The vOED-NFs approach offers a computationally efficient and accurate method for Bayesian optimal experimental design without the need for explicit likelihood evaluations. <br /><br /> <div>
arXiv:2404.13056v2 Announce Type: replace-cross 
Abstract: Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Modeling of Lipid Nanoparticle Formation for the Delivery of Nucleic Acid Therapeutics</title>
<link>https://arxiv.org/abs/2408.08577</link>
<guid>https://arxiv.org/abs/2408.08577</guid>
<content:encoded><![CDATA[
<div> Keywords: nucleic acids, lipid nanoparticles, mechanistic modeling, process development, process control<br />
Summary: 
Nucleic acids, including mRNA, are a promising therapeutic modality for treating various diseases. Lipid nanoparticles (LNPs) have been used as a delivery system for nucleic acids in COVID-19 vaccines. However, understanding the formation and structure of LNPs is challenging, especially during scale-up of manufacturing processes. Mathematical and computational methods offer a way to improve understanding of LNP formation and aid in process development and control. This article discusses strategies for mechanistic modeling of LNP formation, starting with predicting important physicochemical properties of the species involved. It outlines a framework for constructing models of reactor- and particle-scale processes, linking insights from the models to product quality attributes and process understanding. Finally, the article explores using these models to guide advanced process control and optimization strategies.<br /><br />Summary: <div>
arXiv:2408.08577v2 Announce Type: replace-cross 
Abstract: Nucleic acids such as mRNA have emerged as a promising therapeutic modality with the capability of addressing a wide range of diseases. Lipid nanoparticles (LNPs) as a delivery platform for nucleic acids were used in the COVID-19 vaccines and have received much attention. While modern manufacturing processes which involve rapidly mixing an organic stream containing the lipids with an aqueous stream containing the nucleic acids are conceptually straightforward, detailed understanding of LNP formation and structure is still limited and scale-up can be challenging. Mathematical and computational methods are a promising avenue for deepening scientific understanding of the LNP formation process and facilitating improved process development and control. This article describes strategies for the mechanistic modeling of LNP formation, starting with strategies to estimate and predict important physicochemical properties of the various species such as diffusivities and solubilities. Subsequently, a framework is outlined for constructing mechanistic models of reactor- and particle-scale processes. Insights gained from the various models are mapped back to product quality attributes and process insights. Lastly, the use of the models to guide development of advanced process control and optimization strategies is discussed.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatternPaint: Practical Layout Pattern Generation Using Diffusion-Based Inpainting</title>
<link>https://arxiv.org/abs/2409.01348</link>
<guid>https://arxiv.org/abs/2409.01348</guid>
<content:encoded><![CDATA[
<div> diffusion-based framework, VLSI layout patterns, design for manufacturing, few-shot finetuning, technology nodes 
Summary:
PatternPaint is a diffusion-based framework designed for generating diverse VLSI layout patterns essential for design for manufacturing. It addresses the challenge of limited design-rule-compliant training samples and simplifies complex layout pattern generation through inpainting processes. The framework incorporates a template-based denoising scheme and utilizes few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results demonstrate the effectiveness of PatternPaint in generating legal patterns in complex 2D metal interconnect design rule settings for sub-3nm technology nodes. It achieves high diversity scores and improves legality rates significantly through few-shot finetuning. This approach offers a production-ready solution for layout pattern generation in the development of new technology nodes. 
<br /><br />Summary: <div>
arXiv:2409.01348v4 Announce Type: replace-cross 
Abstract: Generating diverse VLSI layout patterns is essential for various downstream tasks in design for manufacturing, as design rules continually evolve during the development of new technology nodes. However, existing training-based methods for layout pattern generation rely on large datasets. In practical scenarios, especially when developing a new technology node, obtaining such extensive layout data is challenging. Consequently, training models with large datasets becomes impractical, limiting the scalability and adaptability of prior approaches. To this end, we propose PatternPaint, a diffusion-based framework capable of generating legal patterns with limited design-rule-compliant training samples. PatternPaint simplifies complex layout pattern generation into a series of inpainting processes with a template-based denoising scheme. Furthermore, we perform few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results show that using a sub-3nm technology node (Intel 18A), our model is the only one that can generate legal patterns in complex 2D metal interconnect design rule settings among all previous works and achieves a high diversity score. Additionally, our few-shot finetuning can boost the legality rate with 1.87X improvement compared to the original pretrained model. As a result, we demonstrate a production-ready approach for layout pattern generation in developing new technology nodes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets</title>
<link>https://arxiv.org/abs/2410.13105</link>
<guid>https://arxiv.org/abs/2410.13105</guid>
<content:encoded><![CDATA[
<div> Decentralized Finance, DeFi, lending, algorithm-driven, liquidity pools <br />
<br />
Summary: 
This work proposes a dynamic model for the DeFi lending market, incorporating evolving demand and supply curves and an adaptive interest rate controller to respond to market changes in real-time. The Recursive Least Squares algorithm is used to track external market conditions, ensuring stable utilization and managing default and liquidation risks. The algorithm provides theoretical guarantees on interest rate convergence and utilization stability while also addressing vulnerability to adversarial manipulation. Two approaches are proposed to mitigate manipulation: detecting extreme fluctuations and enhancing elasticity through interest rate derivative markets. The dynamic model shows low error rates on real data and the interest rate controller outperforms static curve protocols in optimizing utilization and reducing liquidations. <div>
arXiv:2410.13105v4 Announce Type: replace-cross 
Abstract: Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller tracks the external market and achieves stable utilization, while also controlling default and liquidation risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. We propose two complementary approaches to mitigating adversarial manipulation: an algorithmic method that detects extreme demand and supply fluctuations and a market-based strategy that enhances elasticity, potentially via interest rate derivative markets. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Evaluation of Variational Quantum Eigensolver and Quantum Dynamics Algorithms on the Advection-Diffusion Equation</title>
<link>https://arxiv.org/abs/2503.24045</link>
<guid>https://arxiv.org/abs/2503.24045</guid>
<content:encoded><![CDATA[
<div> Variational Quantum Eigensolver, linear advection-diffusion equation, quantum algorithms, partial differential equations, noiseless simulation<br />
<br />
Summary: 
The study investigates the performance of quantum algorithms in solving a linear one-dimensional advection-diffusion equation. It compares Variational Quantum Eigensolver (VQE) with Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS) on small quantum hardware. While VQE on a noiseless simulator achieves high accuracy with low infidelities, the dynamics algorithms suffer from errors due to noise and limited shot statistics on hardware. VQE outperforms the dynamics methods in terms of accuracy as it reaches low infidelities with moderate circuit depths. The comparison provides insights into the accuracy and resource demands of different algorithms for solving partial differential equations. The study concludes with a discussion on potential extensions to higher-dimensional and nonlinear PDEs relevant to engineering and finance. <br /><br /> <div>
arXiv:2503.24045v2 Announce Type: replace-cross 
Abstract: We investigate the potential of near-term quantum algorithms for solving partial differential equations (PDEs), focusing on a linear one-dimensional advection-diffusion equation as a test case. This study benchmarks a ground-state algorithm, Variational Quantum Eigensolver (VQE), against three leading quantum dynamics algorithms, Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS), applied to the same PDE on small quantum hardware. While Trotterization is fully quantum, VarQTE and AVQDS are variational algorithms that reduce circuit depth for noisy intermediate-scale quantum (NISQ) devices. However, hardware results from these dynamics methods show sizable errors due to noise and limited shot statistics. To establish a noise-free performance baseline, we implement the VQE-based solver on a noiseless statevector simulator. Our results show VQE can reach final-time infidelities as low as ${O}(10^{-9})$ with $N=4$ qubits and moderate circuit depths, outperforming hardware-deployed dynamics methods that show infidelities $\gtrsim 10^{-1}$. By comparing noiseless VQE to shot-based and hardware-run algorithms, we assess their accuracy and resource demands, providing a baseline for future quantum PDE solvers. We conclude with a discussion of limitations and potential extensions to higher-dimensional, nonlinear PDEs relevant to engineering and finance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTFinRAG: Interactive Modularized Financial RAG Benchmark</title>
<link>https://arxiv.org/abs/2504.18024</link>
<guid>https://arxiv.org/abs/2504.18024</guid>
<content:encoded><![CDATA[
<div> Keywords: financial sectors, language model technologies, SMARTFinRAG, evaluation paradigm, open-source architecture<br />
Summary: SMARTFinRAG is a new platform designed to address gaps in assessing specialized RAG systems in the financial sector. It introduces a modular architecture that allows components to be interchanged during runtime, a document-centric evaluation paradigm that creates domain-specific QA pairs from financial documents, and an intuitive interface to facilitate research-implementation integration. The evaluation of SMARTFinRAG shows variations in retrieval efficacy and response quality across different configurations. The platform's open-source architecture promotes transparent and reproducible research, while also helping financial institutions overcome deployment challenges when implementing RAG systems. This innovative platform aims to enhance the adoption and evaluation of language model technologies in the financial industry. <br /><br /> <div>
arXiv:2504.18024v1 Announce Type: new 
Abstract: Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Governing Equations of Geomagnetic Storm Dynamics with Symbolic Regression</title>
<link>https://arxiv.org/abs/2504.18461</link>
<guid>https://arxiv.org/abs/2504.18461</guid>
<content:encoded><![CDATA[
<div> solar wind, geomagnetic storms, Disturbance Storm Time index, symbolic regression, PySR framework

Summary:
The study focuses on using symbolic regression to create data-driven equations that describe the behavior of the Disturbance Storm Time (Dst) index during geomagnetic storms. By analyzing historical data from the NASA OMNIweb database, including various solar wind parameters, the study aims to develop models that accurately predict the evolution of the Dst index. The models generated by the PySR framework showcase a hierarchy of complexity levels and outperform traditional empirical models in terms of accuracy and interpretability. The evaluation of the models on historical storm events, such as the 2003 Halloween Storm and the 2015 St. Patrick's Day Storm, demonstrates their effectiveness in capturing nonlinear dependencies and thresholding effects in Dst evolution. Overall, the study provides valuable insights into the mechanisms driving geomagnetic storms and offers interpretable mathematical expressions for predicting their intensity. 

<br /><br />Summary: <div>
arXiv:2504.18461v1 Announce Type: new 
Abstract: Geomagnetic storms are large-scale disturbances of the Earth's magnetosphere driven by solar wind interactions, posing significant risks to space-based and ground-based infrastructure. The Disturbance Storm Time (Dst) index quantifies geomagnetic storm intensity by measuring global magnetic field variations. This study applies symbolic regression to derive data-driven equations describing the temporal evolution of the Dst index. We use historical data from the NASA OMNIweb database, including solar wind density, bulk velocity, convective electric field, dynamic pressure, and magnetic pressure. The PySR framework, an evolutionary algorithm-based symbolic regression library, is used to identify mathematical expressions linking dDst/dt to key solar wind. The resulting models include a hierarchy of complexity levels and enable a comparison with well-established empirical models such as the Burton-McPherron-Russell and O'Brien-McPherron models. The best-performing symbolic regression models demonstrate superior accuracy in most cases, particularly during moderate geomagnetic storms, while maintaining physical interpretability. Performance evaluation on historical storm events includes the 2003 Halloween Storm, the 2015 St. Patrick's Day Storm, and a 2017 moderate storm. The results provide interpretable, closed-form expressions that capture nonlinear dependencies and thresholding effects in Dst evolution.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAQGA: A Quantum-Enhanced Genetic Algorithm with Novel Entanglement-Aware Crossovers</title>
<link>https://arxiv.org/abs/2504.17923</link>
<guid>https://arxiv.org/abs/2504.17923</guid>
<content:encoded><![CDATA[
<div> genetic algorithms, combinatorial optimization, portfolio optimization, quantum computing, quantum circuits

Summary:
- Genetic algorithms are effective in complex optimization problems, such as portfolio optimization.
- Quantum computing can address challenging tasks, and quantum genetic algorithms combine the benefits of both approaches.
- The proposed quantum genetic algorithm introduces a novel crossover strategy generating quantum circuits from binary solutions.
- It encodes entanglement patterns from parent solutions to enhance performance without significantly increasing circuit depth.
- Testing on a portfolio optimization problem using IBM's 127 qubits Eagle processor and simulators shows a significant improvement in fitness values compared to classical and quantum-inspired genetic algorithms, highlighting the potential of quantum computers in solving real-world combinatorial optimization problems.<br /><br /> <div>
arXiv:2504.17923v1 Announce Type: cross 
Abstract: Genetic algorithms are highly effective optimization techniques for many computationally challenging problems, including combinatorial optimization tasks like portfolio optimization. Quantum computing has also shown potential in addressing these complex challenges. Combining these approaches, quantum genetic algorithms leverage the principles of superposition and entanglement to enhance the performance of classical genetic algorithms. In this work, we propose a novel quantum genetic algorithm introducing an innovative crossover strategy to generate quantum circuits from a binary solution. We incorporate a heuristic method to encode entanglement patterns from parent solutions into circuits for the next generation. Our algorithm advances quantum genetic algorithms by utilizing a limited number of entanglements, enabling efficient exploration of optimal solutions without significantly increasing circuit depth, making it suitable for near-term applications. We test this approach on a portfolio optimization problem using an IBM 127 qubits Eagle processor (ibm_quebec) and simulators. Compared to state-of-the-art algorithms, our results show that the proposed method improves fitness values by 33.6% over classical genetic algorithm and 37.2% over quantum-inspired genetic algorithm, using the same iteration counts and population sizes with real quantum hardware employing 100 qubits. These findings highlight the potential of current quantum computers to address real-world utility-scale combinatorial optimization problems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach For Bitcoin Forecasting</title>
<link>https://arxiv.org/abs/2504.18206</link>
<guid>https://arxiv.org/abs/2504.18206</guid>
<content:encoded><![CDATA[
<div> Keywords: Bitcoin, cryptocurrency, time series, machine learning, directional accuracy

Summary:
In this study, the researchers focus on Bitcoin, a popular cryptocurrency, and investigate the use of different time series data along with machine learning algorithms to forecast its price movements. The analysis reveals that incorporating the Open, High, and Low prices of Bitcoin significantly improves directional accuracy. The Low price, in particular, plays a crucial role in enhancing the forecast accuracy when used in combination with a Gated Recurrent Unit network and a baseline forecast. The study also finds that other Bitcoin-related features, apart from price data, have minimal impact on prediction accuracy. Overall, the proposed method displays comparable performance to existing approaches in terms of directional accuracy, emphasizing the importance of considering specific time series data and machine learning techniques for forecasting cryptocurrency prices.<br /><br />Summary: <div>
arXiv:2504.18206v1 Announce Type: cross 
Abstract: Bitcoin is one of the cryptocurrencies that is gaining more popularity in recent years. Previous studies have shown that closing price alone is not enough to forecast stock market series. We introduce a new set of time series and demonstrate that a subset is necessary to improve directional accuracy based on a machine learning ensemble. In our experiments, we study which time series and machine learning algorithms deliver the best results. We found that the most relevant time series that contribute to improving directional accuracy are Open, High and Low, with the largest contribution of Low in combination with an ensemble of Gated Recurrent Unit network and a baseline forecast. The relevance of other Bitcoin-related features that are not price-related is negligible. The proposed method delivers similar performance to the state-of-the-art when observing directional accuracy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid modelling of reactive transport in porous media using machine learning: limitations and solutions</title>
<link>https://arxiv.org/abs/2405.14548</link>
<guid>https://arxiv.org/abs/2405.14548</guid>
<content:encoded><![CDATA[
<div> machine learning, reactive transport, porous media, geochemical reactions, cation exchange problem

Summary:
Machine learning models are explored as replacements for a geochemical module in simulating reactive transport in porous media. Testing on a cation exchange problem reveals that while the surrogate models perform well in isolated predictions, they struggle with rollout predictions over successive time steps. By incorporating physics-based constraints and tailored dataset generation strategies, accurate rollout predictions are achieved. The study highlights the limitation of machine learning surrogates in predicting over multiple time steps, even for a simple sorption equilibrium reaction like the cation exchange problem. However, with the addition of physics-based modifications, these limitations can be overcome. The research provides a detailed analysis of these limitations and potential mitigation strategies.<br /><br /> <div>
arXiv:2405.14548v2 Announce Type: replace 
Abstract: Reactive transport in porous media plays a pivotal role in subsurface reservoir processes, influencing fluid properties and geochemical characteristics. However, coupling fluid flow and transport with geochemical reactions is computationally intensive, requiring geochemical calculations at each grid cell and each time step within a discretized simulation domain. Although recent advancements have integrated machine learning techniques as surrogates for geochemical simulations, ensuring computational efficiency and accuracy remains a challenge. This work investigates machine learning models as replacements for a geochemical module in a simulation of reactive transport in porous media. As a proof of concept, we test this approach on a well-documented cation exchange problem. While the surrogate models excel in isolated predictions, they fall short in rollout predictions over successive time steps. By introducing modifications, including physics-based constraints and tailored dataset generation strategies, we show that machine learning surrogates can achieve accurate rollout predictions. Our findings emphasize that even for a simple sorption equilibrium reaction (cation exchange problem), machine learning surrogates alone fail in predicting over successive time-steps. Incorporating simple physics-based modifications enables us to overcome this limitation. A detailed analysis of the limitations and potential mitigation strategies is presented in this work.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Function-coherent gambles</title>
<link>https://arxiv.org/abs/2503.01855</link>
<guid>https://arxiv.org/abs/2503.01855</guid>
<content:encoded><![CDATA[
<div> function-coherent gambles, non-linear utility, intertemporal choice, discounting, desirability framework
Summary:
The paper introduces function-coherent gambles, a generalization of the desirable gambles framework that allows for non-linear utility functions. Core axioms for function-coherence are established, and a representation theorem is proven, linking acceptable gambles to continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, such as hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. The integration of these alternative discounting models within the function-coherent framework provides a unified treatment for modeling complex patterns of time preference. This approach bridges the gap between normative theory and real-world behavior in intertemporal decision-making under uncertainty, offering theoretical foundations for understanding sophisticated time preferences within the desirability paradigm. <br /><br />Summary: <div>
arXiv:2503.01855v2 Announce Type: replace-cross 
Abstract: The desirable gambles framework provides a foundational approach to imprecise probability theory but relies heavily on linear utility assumptions. This paper introduces function-coherent gambles, a generalization that accommodates non-linear utility while preserving essential rationality properties. We establish core axioms for function-coherence and prove a representation theorem that characterizes acceptable gambles through continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, including hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. We demonstrate how these alternatives to constant-rate exponential discounting can be integrated within the function-coherent framework. This unified treatment provides theoretical foundations for modeling sophisticated patterns of time preference within the desirability paradigm, bridging a gap between normative theory and observed behavior in intertemporal decision-making under genuine uncertainty.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Entropy Stable Formulation of Two-equation Turbulence Models with Particular Reference to the k-epsilon Model</title>
<link>https://arxiv.org/abs/2504.17110</link>
<guid>https://arxiv.org/abs/2504.17110</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical algorithms, partial differential equations, entropy production inequality, turbulence models, k-epsilon model

Summary:
This article discusses the importance of incorporating nonlinear physical stability principles, such as the entropy production inequality, in the design of numerical algorithms for partial differential equations. By introducing space-time averaging and defining entropy variables, a symmetric system of advective-diffusive equations can be derived for turbulence models, including the k-epsilon model. Positivity and symmetry constraints are necessary for the turbulence diffusivity coefficients and source terms to ensure the design of entropy producing two-equation turbulence models. This approach emphasizes the use of physical principles over artificial viscosity for designing robust algorithms. <div>
arXiv:2504.17110v1 Announce Type: new 
Abstract: Consistency and stability are two essential ingredients in the design of numerical algorithms for partial differential equations. Robust algorithms can be developed by incorporating nonlinear physical stability principles in their design, such as the entropy production inequality (i.e., the Clausius-Duhem inequality or second law of thermodynamics), rather than by simply adding artificial viscosity (a common approach). This idea is applied to the k-epsilon and two-equation turbulence models by introducing space-time averaging. Then, a set of entropy variables can be defined which leads to a symmetric system of advective-diffusive equations. Positivity and symmetry of the equations require certain constraints on the turbulence diffusivity coefficients and the turbulence source terms. With these, we are able to design entropy producing two-equation turbulence models and, in particular, the k-epsilon model.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Stock Prices for Enhanced Multi-Step Forecast and Prediction</title>
<link>https://arxiv.org/abs/2504.17313</link>
<guid>https://arxiv.org/abs/2504.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: stock price forecasting, stock price prediction, PCIE model, tokenization method, multi-step prediction<br />
Summary: <br />
Effective stock price forecasting and prediction are essential for investors and policymakers but are challenging due to the dynamic nature of stock price data. Forecasting and prediction targets have distinct statistical characteristics and multi-step approaches provide richer information but are more difficult. The Patched Channel Integration Encoder (PCIE) model is introduced to address these challenges by utilizing multiple stock channels and a novel tokenization method. The tokenization process involves univariate patching and temporal learning to reduce cumulative errors. Experimental results demonstrate that PCIE outperforms current state-of-the-art models in both forecast and prediction tasks. <div>
arXiv:2504.17313v1 Announce Type: new 
Abstract: Effective stock price forecasting (estimating future prices) and prediction (estimating future price changes) are pivotal for investors, regulatory agencies, and policymakers. These tasks enable informed decision-making, risk management, strategic planning, and superior portfolio returns. Despite their importance, forecasting and prediction are challenging due to the dynamic nature of stock price data, which exhibit significant temporal variations in distribution and statistical properties. Additionally, while both forecasting and prediction targets are derived from the same dataset, their statistical characteristics differ significantly. Forecasting targets typically follow a log-normal distribution, characterized by significant shifts in mean and variance over time, whereas prediction targets adhere to a normal distribution. Furthermore, although multi-step forecasting and prediction offer a broader perspective and richer information compared to single-step approaches, it is much more challenging due to factors such as cumulative errors and long-term temporal variance. As a result, many previous works have tackled either single-step stock price forecasting or prediction instead. To address these issues, we introduce a novel model, termed Patched Channel Integration Encoder (PCIE), to tackle both stock price forecasting and prediction. In this model, we utilize multiple stock channels that cover both historical prices and price changes, and design a novel tokenization method to effectively embed these channels in a cross-channel and temporally efficient manner. Specifically, the tokenization process involves univariate patching and temporal learning with a channel-mixing encoder to reduce cumulative errors. Comprehensive experiments validate that PCIE outperforms current state-of-the-art models in forecast and prediction tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems</title>
<link>https://arxiv.org/abs/2504.17354</link>
<guid>https://arxiv.org/abs/2504.17354</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, effective contact area, rough surfaces, machine learning algorithms, multi-query contexts
<br />
Summary: 
This study presents a surrogate modeling framework for predicting the effective contact area in rough surface contact using fast-to-evaluate machine learning algorithms. The models are trained on a precomputed dataset of imposed load and roughness parameters to predict the effective contact area efficiently. The Kernel Ridge Regressor is identified as the best trade-off between accuracy and efficiency, making it suitable for general-purpose surrogate modeling. The Gaussian Process Regressor is also effective for uncertainty quantification tasks. The models' generalization capability is validated on unseen simulation scenarios, demonstrating their transferability to new configurations. While database generation is a significant cost in the process, the overall approach is practical and efficient for multi-query tasks, even after accounting for initial expenses. <br /><br />Summary: <div>
arXiv:2504.17354v1 Announce Type: new 
Abstract: The effective contact area in rough surface contact plays a critical role in multi-physics phenomena such as wear, sealing, and thermal or electrical conduction. Although accurate numerical methods, like the Boundary Element Method (BEM), are available to compute this quantity, their high computational cost limits their applicability in multi-query contexts, such as uncertainty quantification, parameter identification, and multi-scale algorithms, where many repeated evaluations are required. This study proposes a surrogate modeling framework for predicting the effective contact area using fast-to-evaluate data-driven techniques. Various machine learning algorithms are trained on a precomputed dataset, where the inputs are the imposed load and statistical roughness parameters, and the output is the corresponding effective contact area. All models undergo hyperparameter optimization to enable fair comparisons in terms of predictive accuracy and computational efficiency, evaluated using established quantitative metrics. Among the models, the Kernel Ridge Regressor demonstrates the best trade-off between accuracy and efficiency, achieving high predictive accuracy, low prediction time, and minimal training overhead-making it a strong candidate for general-purpose surrogate modeling. The Gaussian Process Regressor provides an attractive alternative when uncertainty quantification is required, although it incurs additional computational cost due to variance estimation. The generalization capability of the Kernel Ridge model is validated on an unseen simulation scenario, confirming its ability to transfer to new configurations. Database generation constitutes the dominant cost in the surrogate modeling process. Nevertheless, the approach proves practical and efficient for multi-query tasks, even when accounting for this initial expense.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>polyGen: A Learning Framework for Atomic-level Polymer Structure Generation</title>
<link>https://arxiv.org/abs/2504.17656</link>
<guid>https://arxiv.org/abs/2504.17656</guid>
<content:encoded><![CDATA[
<div> Latent diffusion model, polymer structures, generative algorithms, molecular encoding, structure matching criteria 

Summary: 

polyGen is introduced as a novel approach to generating realistic polymer structures using a latent diffusion model. Leveraging a molecular encoding that captures polymer connectivity, polyGen can generate diverse conformations of both linear chains and complex branched structures. The model shows improvement in joint learning between similar chemical structures through training augmentation with DFT-optimized molecular structures. However, its performance decreases when handling repeat units with a high atom count. polyGen represents a paradigm shift in atomic-level structure generation for polymer science, providing the first proof-of-concept for predicting realistic atomic-level polymer conformations while considering their intrinsic structural flexibility. <div>
arXiv:2504.17656v1 Announce Type: new 
Abstract: Synthetic polymeric materials underpin fundamental technologies in the energy, electronics, consumer goods, and medical sectors, yet their development still suffers from prolonged design timelines. Although polymer informatics tools have supported speedup, polymer simulation protocols continue to face significant challenges: on-demand generation of realistic 3D atomic structures that respect the conformational diversity of polymer structures. Generative algorithms for 3D structures of inorganic crystals, bio-polymers, and small molecules exist, but have not addressed synthetic polymers. In this work, we introduce polyGen, the first latent diffusion model designed specifically to generate realistic polymer structures from minimal inputs such as the repeat unit chemistry alone, leveraging a molecular encoding that captures polymer connectivity throughout the architecture. Due to a scarce dataset of only 3855 DFT-optimized polymer structures, we augment our training with DFT-optimized molecular structures, showing improvement in joint learning between similar chemical structures. We also establish structure matching criteria to benchmark our approach on this novel problem. polyGen effectively generates diverse conformations of both linear chains and complex branched structures, though its performance decreases when handling repeat units with a high atom count. Given these initial results, polyGen represents a paradigm shift in atomic-level structure generation for polymer science-the first proof-of-concept for predicting realistic atomic-level polymer conformations while accounting for their intrinsic structural flexibility.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy</title>
<link>https://arxiv.org/abs/2504.17124</link>
<guid>https://arxiv.org/abs/2504.17124</guid>
<content:encoded><![CDATA[
<div> X-ray absorption near edge structure (XANES) spectroscopy, adaptive sampling methods, Bayesian optimization, absorption edge, pre-edge peaks <br />
<br />Summary: <br />
X-ray absorption near edge structure (XANES) spectroscopy is a valuable technique for analyzing chemical states in materials, but traditional data collection methods can be time-consuming. This study introduces a knowledge-injected Bayesian optimization approach for adaptive XANES data collection, taking into account spectral features like absorption edges and pre-edge peaks. The method efficiently reconstructs absorption edges with high accuracy using only 15-20% of the measurement points needed in conventional sampling. It can determine the x-ray energy of sharp peaks with minimal errors, achieving overall root-mean-square errors below 0.005 compared to traditional methods. The experiments on battery materials and catalysts demonstrate its effectiveness for both static and dynamic XANES measurements, enhancing data collection efficiency and enabling better time resolution for tracking chemical changes. This automated approach reduces common errors in XANES experiments and facilitates dynamic studies requiring high temporal resolution. <div>
arXiv:2504.17124v1 Announce Type: cross 
Abstract: X-ray absorption near edge structure (XANES) spectroscopy is a powerful technique for characterizing the chemical state and symmetry of individual elements within materials, but requires collecting data at many energy points which can be time-consuming. While adaptive sampling methods exist for efficiently collecting spectroscopic data, they often lack domain-specific knowledge about XANES spectra structure. Here we demonstrate a knowledge-injected Bayesian optimization approach for adaptive XANES data collection that incorporates understanding of spectral features like absorption edges and pre-edge peaks. We show this method accurately reconstructs the absorption edge of XANES spectra using only 15-20% of the measurement points typically needed for conventional sampling, while maintaining the ability to determine the x-ray energy of the sharp peak after absorption edge with errors less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and overall root-mean-square errors less than 0.005 compared to compared to traditionally sampled spectra. Our experiments on battery materials and catalysts demonstrate the method's effectiveness for both static and dynamic XANES measurements, improving data collection efficiency and enabling better time resolution for tracking chemical changes. This approach advances the degree of automation in XANES experiments reducing the common errors of under- or over-sampling points in near the absorption edge and enabling dynamic experiments that require high temporal resolution or limited measurement time.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints</title>
<link>https://arxiv.org/abs/2504.17142</link>
<guid>https://arxiv.org/abs/2504.17142</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, microelectronic components, multiphysics constraints, design optimization, high-dimensional solution space
Summary:
This study focuses on using reinforcement learning techniques for designing microelectronic components under multiphysics constraints. Traditional global optimization approaches are limited when dealing with intricate solution spaces and constraints. The complexity of microelectronic component design, such as ASICs and HI interposers, poses challenges for conventional methods. The study explores optimizing interconnect geometry for ASIC chips and component placement on a HI interposer while meeting thermoelastic and design constraints. The placement problem involves a high-dimensional solution space, highlighting the need for advanced techniques like reinforcement learning. By developing and testing an RL-based framework, the study aims to enhance the design and optimization processes for microelectronic components with multiphysics constraints. <div>
arXiv:2504.17142v1 Announce Type: cross 
Abstract: This study focuses on the development of reinforcement learning based techniques for the design of microelectronic components under multiphysics constraints. While traditional design approaches based on global optimization approaches are effective when dealing with a small number of design parameters, as the complexity of the solution space and of the constraints increases different techniques are needed. This is an important reason that makes the design and optimization of microelectronic components (characterized by large solution space and multiphysics constraints) very challenging for traditional methods. By taking as prototypical elements an application-specific integrated circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and numerically test an optimization framework based on reinforcement learning (RL). More specifically, we consider the optimization of the bonded interconnect geometry for an ASIC chip as well as the placement of components on a HI interposer while satisfying thermoelastic and design constraints. This placement problem is particularly interesting because it features a high-dimensional solution space.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection, Classification and Prevalence of Self-Admitted Aging Debt</title>
<link>https://arxiv.org/abs/2504.17428</link>
<guid>https://arxiv.org/abs/2504.17428</guid>
<content:encoded><![CDATA[
<div> Aging Debt, Self-Admitted Aging Debt, software aging, source code comments, OSS repositories <br />
Summary:<br />
The study introduces Aging Debt (AD) as a concept to represent increased maintenance efforts in software. It focuses on Self-Admitted Aging Debt (SAAD) observed in source code comments to detect and measure AD in software. A taxonomy is developed to categorize temporal software aging into Active and Dormant types. Analysis of over 9,000+ OSS repositories shows that more than 21% exhibit SAAD, with Dormant AD being prevalent. The study highlights the critical aspect of software maintenance and the importance of addressing evolutionary indicators like source code comments in software aging research. The proposed taxonomy can assist researchers in detailed software aging studies and help practitioners in developing proactive maintenance strategies. <br /> <div>
arXiv:2504.17428v1 Announce Type: cross 
Abstract: Context: Previous research on software aging is limited with focus on dynamic runtime indicators like memory and performance, often neglecting evolutionary indicators like source code comments and narrowly examining legacy issues within the TD context. Objective: We introduce the concept of Aging Debt (AD), representing the increased maintenance efforts and costs needed to keep software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed in source code comments left by software developers. Method: We employ a mixed-methods approach, combining qualitative and quantitative analyses to detect and measure AD in software. This includes framing SAAD patterns from the source code comments after analysing the source code context, then utilizing the SAAD patterns to detect SAAD comments. In the process, we develop a taxonomy for SAAD that reflects the temporal aging of software and its associated debt. Then we utilize the taxonomy to quantify the different types of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes temporal software aging into Active and Dormant types. Our extensive analysis of over 9,000+ Open Source Software (OSS) repositories reveals that more than 21% repositories exhibit signs of SAAD as observed from our gold standard SAAD dataset. Notably, Dormant AD emerges as the predominant category, highlighting a critical but often overlooked aspect of software maintenance. Conclusion: As software volume grows annually, so do evolutionary aging and maintenance challenges; our proposed taxonomy can aid researchers in detailed software aging studies and help practitioners develop improved and proactive maintenance strategies.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An approach based on metaheuristic algorithms to the timetabling problem in deregulated railway markets</title>
<link>https://arxiv.org/abs/2504.17455</link>
<guid>https://arxiv.org/abs/2504.17455</guid>
<content:encoded><![CDATA[
<div> Genetic Algorithm, train timetabling, liberalized railway markets, optimization, scheduling<br />
<br />
Summary: <br />
The paper addresses the train timetabling problem in liberalized railway markets, focusing on maximizing infrastructure capacity and revenue optimization for infrastructure managers and railway undertakings. A modular simulation framework is introduced to simulate deregulated railway systems, evaluating ten metaheuristic algorithms using the MEALPY Python library. Results show that the Genetic Algorithm outperforms other algorithms in revenue optimization, convergence speed, and schedule adherence. Alternative algorithms, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, exhibit slower convergence and higher variability. The study highlights the trade-off between scheduling more trains and meeting requested times, offering insights into solving complex scheduling challenges in deregulated railway systems. <div>
arXiv:2504.17455v1 Announce Type: cross 
Abstract: The train timetabling problem in liberalized railway markets represents a challenge to the coordination between infrastructure managers and railway undertakings. Efficient scheduling is critical in maximizing infrastructure capacity and utilization while adhering as closely as possible to the requests of railway undertakings. These objectives ultimately contribute to maximizing the infrastructure manager's revenues. This paper sets out a modular simulation framework to reproduce the dynamics of deregulated railway systems. Ten metaheuristic algorithms using the MEALPY Python library are then evaluated in order to optimize train schedules in the liberalized Spanish railway market. The results show that the Genetic Algorithm outperforms others in revenue optimization, convergence speed, and schedule adherence. Alternatives, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, show slower convergence and higher variability. The results emphasize the trade-off between scheduling more trains and adhering to requested times, providing insights into solving complex scheduling problems in deregulated railway systems.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Equitable Rail Service Allocation Through Fairness-Oriented Timetabling in Liberalized Markets</title>
<link>https://arxiv.org/abs/2504.17489</link>
<guid>https://arxiv.org/abs/2504.17489</guid>
<content:encoded><![CDATA[
<div> Keywords: European rail transport, liberalization, infrastructure capacity, railway market, equity metrics 

Summary: 
In the changing landscape of European rail transport due to liberalization, railway undertakings now compete for limited infrastructure capacity to offer their services. The equitable allocation of infrastructure by the infrastructure manager is crucial for the efficiency and sustainability of this competitive environment. A methodology utilizing Jain, Gini, and Atkinson equity metrics is proposed to address the rail service allocation problem in a liberalized railway market. The results from computational tests demonstrate that this methodology promotes equitable planning across various competitiveness scenarios. This stands in contrast to approaches solely focused on maximizing the infrastructure manager's profit without considering fair allocation. The study supports the use of the proposed methodology and equity metrics as effective tools for planning and decision-making within a liberalized railway market. 

<br /><br />Summary: <div>
arXiv:2504.17489v1 Announce Type: cross 
Abstract: Over the last few decades, European rail transport has undergone major changes as part of the process of liberalization set out in European regulations. In this context of liberalization, railway undertakings compete with each other for the limited infrastructure capacity available to offer their rail services. The infrastructure manager is responsible for the equitable allocation of infrastructure between all companies in the market, which is essential to ensure the efficiency and sustainability of this competitive ecosystem. In this paper, a methodology based on Jain, Gini and Atkinson equity metrics is used to solve the rail service allocation problem in a liberalized railway market, analyzing the solutions obtained. The results show that the proposed methodology and the equity metrics used allow for equitable planning in different competitiveness scenarios. These results contrast with solutions where the objective of the infrastructure manager is to maximize its own profit, without regard for the equitable allocation of infrastructure. Therefore, the computational tests support the methodology and metrics used as a planning and decision support tool in a liberalized railway market.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and generalizable protein-ligand binding affinity prediction with geometric deep learning</title>
<link>https://arxiv.org/abs/2504.16261</link>
<guid>https://arxiv.org/abs/2504.16261</guid>
<content:encoded><![CDATA[
<div> deep learning, protein-ligand binding, affinity prediction, interatomic potential, machine learning<br />
Summary:<br />
The article introduces IPBind, a computational method based on geometric deep learning, for predicting protein-ligand binding affinity. While existing algorithms struggle with novel protein-ligand complexes, IPBind leverages interatomic potential to make robust predictions. Experimental results on binding affinity prediction benchmarks show the effectiveness and universality of IPBind, providing atom-level insights. This work underscores the benefits of using machine learning interatomic potential in protein-ligand binding affinity prediction. <div>
arXiv:2504.16261v1 Announce Type: new 
Abstract: Protein-ligand binding complexes are ubiquitous and essential to life. Protein-ligand binding affinity prediction (PLA) quantifies the binding strength between ligands and proteins, providing crucial insights for discovering and designing potential candidate ligands. While recent advances have been made in predicting protein-ligand complex structures, existing algorithms for interaction and affinity prediction suffer from a sharp decline in performance when handling ligands bound with novel unseen proteins. We propose IPBind, a geometric deep learning-based computational method, enabling robust predictions by leveraging interatomic potential between complex's bound and unbound status. Experimental results on widely used binding affinity prediction benchmarks demonstrate the effectiveness and universality of IPBind. Meanwhile, it provides atom-level insights into prediction. This work highlights the advantage of leveraging machine learning interatomic potential for predicting protein-ligand binding affinity.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Different Transformer Model Structures for Stock Prediction</title>
<link>https://arxiv.org/abs/2504.16361</link>
<guid>https://arxiv.org/abs/2504.16361</guid>
<content:encoded><![CDATA[
<div> Transformer, stock index prediction, model architectures, forecasting, ProbSparse attention

Summary:
- The paper compares different Transformer model architectures for stock index prediction, highlighting the importance of architectural choices in predictive accuracy. 
- Existing studies often treat Transformers as black boxes, overlooking the impact of specific structural designs on performance.
- Five Transformer structures were evaluated: encoder-only, decoder-only, Vanilla Transformer, Vanilla Transformer without embedding layers, and Vanilla Transformer with ProbSparse attention.
- Results indicate that Transformer models generally outperform traditional approaches, with the decoder-only structure being the most effective in all scenarios.
- The Transformer with ProbSparse attention showed the poorest performance in most cases. 

<br /><br />Summary: <div>
arXiv:2504.16361v1 Announce Type: new 
Abstract: This paper compares different Transformer model architectures for stock index prediction. While many studies have shown that Transformers perform well in stock price forecasting, few have explored how different structural designs impact performance. Most existing works treat the Transformer as a black box, overlooking how specific architectural choices may affect predictive accuracy. However, understanding these differences is critical for developing more effective forecasting models. This study aims to identify which Transformer variant is most suitable for stock forecasting. This study evaluates five Transformer structures: (1) encoder-only Transformer, (2) decoder-only Transformer, (3) Vanilla Transformer (encoder + decoder), (4) Vanilla Transformer without embedding layers, and (5) Vanilla Transformer with ProbSparse attention. Results show that Transformer-based models generally outperform traditional approaches. Transformer with decoder only structure outperforms all other models in all scenarios. Transformer with ProbSparse attention has the worst performance in almost all cases.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preconditioning Natural and Second Order Gradient Descent in Quantum Optimization: A Performance Benchmark</title>
<link>https://arxiv.org/abs/2504.16518</link>
<guid>https://arxiv.org/abs/2504.16518</guid>
<content:encoded><![CDATA[
<div> optimization, parametric quantum circuits, noisy gradient evaluations, curvature information, BFGS update rule <br />
Summary: <br />
The optimization of parametric quantum circuits faces challenges due to the non-convex nature of the objective function, noisy gradient evaluations, and barren plateaus. Selecting the right classical optimizer is crucial in quantum-classical applications. Incorporating curvature information in the parameter update can aid in faster convergence. Quasi-Newton and quantum natural gradient methods show promise in this regard but have drawbacks. A study evaluates the performance of optimizers on MaxCut problems using a shallow QAOA algorithm. To address noise sensitivity and iteration cost, a novel approach called secant-penalization in the BFGS update rule (SP-BFGS) is introduced, leading to improved outcomes for QAOA optimization problems. This method stabilizes BFGS updates against gradient noise, showcasing potential for optimizing parametric quantum circuits in noisy environments. <br /> <div>
arXiv:2504.16518v1 Announce Type: new 
Abstract: The optimization of parametric quantum circuits is technically hindered by three major obstacles: the non-convex nature of the objective function, noisy gradient evaluations, and the presence of barren plateaus. As a result, the selection of classical optimizer becomes a critical factor in assessing and exploiting quantum-classical applications. One promising approach to tackle these challenges involves incorporating curvature information into the parameter update. The most prominent methods in this field are quasi-Newton and quantum natural gradient methods, which can facilitate faster convergence compared to first-order approaches. Second order methods however exhibit a significant trade-off between computational cost and accuracy, as well as heightened sensitivity to noise. This study evaluates the performance of three families of optimizers on synthetically generated MaxCut problems on a shallow QAOA algorithm. To address noise sensitivity and iteration cost, we demonstrate that incorporating secant-penalization in the BFGS update rule (SP-BFGS) yields improved outcomes for QAOA optimization problems, introducing a novel approach to stabilizing BFGS updates against gradient noise.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields</title>
<link>https://arxiv.org/abs/2504.16600</link>
<guid>https://arxiv.org/abs/2504.16600</guid>
<content:encoded><![CDATA[
<div> Keywords: passive implants, magnetic fields, safety assessment, numerical approach, thermal diffusion <br />
Summary: <br />
- Safety assessment of patients with one-dimensional structured passive implants exposed to low or medium frequency magnetic fields poses challenges due to different length scales. <br />
- A novel numerical approach is proposed, solving three-dimensional and one-dimensional coupled problems, considering thermal diffusion through metallic implants for improved accuracy. <br />
- Results from measurements on a cranial plate exposed to a magnetic field show a 25% improvement in accuracy compared to the method based on thermal seeds. <br />
- Application of the proposed method in a magnetic hyperthermia case study predicts a 10% lower temperature increase near the implant compared to the overestimation by relying on thermal seeds. <br /> <div>
arXiv:2504.16600v1 Announce Type: new 
Abstract: Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Software Engineering Research on Jupyter Notebook</title>
<link>https://arxiv.org/abs/2504.16180</link>
<guid>https://arxiv.org/abs/2504.16180</guid>
<content:encoded><![CDATA[
<div> Trends, gaps, methodologies, human-computer interaction, reusability <br />
Summary:<br />
The study analyzed software engineering research on Jupyter notebooks, finding that most publications are in human-computer interaction venues rather than traditional software engineering ones. Various software engineering topics were addressed, but solutions for testing, refactoring, and documentation specific to notebooks are lacking. Only a small percentage of studies provide reusable code, and many replication packages are not stored in permanent repositories. Future research opportunities include developing automatic testing frameworks, refactoring clones between notebooks, and generating group documentation for coherent code cells. <div>
arXiv:2504.16180v1 Announce Type: cross 
Abstract: Context: Jupyter Notebook has emerged as a versatile tool that transforms how researchers, developers, and data scientists conduct and communicate their work. As the adoption of Jupyter notebooks continues to rise, so does the interest from the software engineering research community in improving the software engineering practices for Jupyter notebooks.
  Objective: The purpose of this study is to analyze trends, gaps, and methodologies used in software engineering research on Jupyter notebooks.
  Method: We selected 146 relevant publications from the DBLP Computer Science Bibliography up to the end of 2024, following established systematic literature review guidelines. We explored publication trends, categorized them based on software engineering topics, and reported findings based on those topics.
  Results: The most popular venues for publishing software engineering research on Jupyter notebooks are related to human-computer interaction instead of traditional software engineering venues. Researchers have addressed a wide range of software engineering topics on notebooks, such as code reuse, readability, and execution environment. Although reusability is one of the research topics for Jupyter notebooks, only 64 of the 146 studies can be reused based on their provided URLs. Additionally, most replication packages are not hosted on permanent repositories for long-term availability and adherence to open science principles.
  Conclusion: Solutions specific to notebooks for software engineering issues, including testing, refactoring, and documentation, are underexplored. Future research opportunities exist in automatic testing frameworks, refactoring clones between notebooks, and generating group documentation for coherent code cells.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Doubly Stochastic Transformers</title>
<link>https://arxiv.org/abs/2504.16275</link>
<guid>https://arxiv.org/abs/2504.16275</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, Softmax, Sinkhorn algorithm, doubly stochastic matrix, variational quantum circuit <br />
Summary: 
The study introduces a hybrid classical-quantum Doubly Stochastic Transformer (QDSFormer) that incorporates a variational quantum circuit in place of the Softmax in the attention layer of the Transformer model. The parametric quantum circuit used in the QDSFormer enhances the expressive power of the model, resulting in more diverse Doubly Stochastic Matrices (DSMs) that preserve information better than classical operators. Through experiments on small-scale object recognition tasks, the QDSFormer outperforms standard Vision Transformers and other doubly stochastic Transformers. Additionally, a quantum-inspired doubly stochastic Transformer based on QR decomposition is also compared. The QDSFormer demonstrates improved training stability and lower performance variation, suggesting that it could mitigate instability during training on small-scale data. These findings highlight the potential of quantum-inspired approaches in enhancing the performance and stability of Transformer models.
<br /><br />Summary: <div>
arXiv:2504.16275v1 Announce Type: cross 
Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often destabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard Vision Transformer and other doubly stochastic Transformers. Beyond the established Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. The QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixing Data-Driven and Physics-Based Constitutive Models using Uncertainty-Driven Phase Fields</title>
<link>https://arxiv.org/abs/2504.16713</link>
<guid>https://arxiv.org/abs/2504.16713</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, multiscale models, data-driven, phase-field model, Gaussian Process surrogate <br />
Summary:
This article presents an adaptive mixture approach for accelerating multiscale models using data-driven surrogate modeling techniques. The approach involves using a fast probabilistic surrogate model for most of the computational domain, switching to the true high-fidelity model only when necessary. By incorporating phases in the computational domain and utilizing a phase-field model driven by surrogate uncertainty, the transition between models is smooth and accurate. The method reduces the time required to collect a large training dataset, while still maintaining high accuracy in simulations. The study compares this approach to a purely local model and demonstrates its effectiveness using a Gaussian Process surrogate for an elasto-plastic material. The adaptive mixture of models shows great potential for speeding up multiscale simulations without compromising accuracy or stability. <br /> <div>
arXiv:2504.16713v1 Announce Type: cross 
Abstract: There is a high interest in accelerating multiscale models using data-driven surrogate modeling techniques. Creating a large training dataset encompassing all relevant load scenarios is essential for a good surrogate, yet the computational cost of producing this data quickly becomes a limiting factor. Commonly, a pre-trained surrogate is used throughout the computational domain. Here, we introduce an alternative adaptive mixture approach that uses a fast probabilistic surrogate model as constitutive model when possible, but resorts back to the true high-fidelity model when necessary. The surrogate is thus not required to be accurate for every possible load condition, enabling a significant reduction in the data collection time. We achieve this by creating phases in the computational domain corresponding to the different models. These phases evolve using a phase-field model driven by the surrogate uncertainty. When the surrogate uncertainty becomes large, the phase-field model causes a local transition from the surrogate to the high-fidelity model, maintaining a highly accurate simulation. We discuss the requirements of this approach to achieve accurate and numerically stable results and compare the phase-field model to a purely local approach that does not enforce spatial smoothness for the phase mixing. Using a Gaussian Process surrogate for an elasto-plastic material, we demonstrate the potential of this mixture of models to accelerate multiscale simulations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements in Constitutive Model Calibration: Leveraging the Power of Full-Field DIC Measurements and In-Situ Load Path Selection for Reliable Parameter Inference</title>
<link>https://arxiv.org/abs/2411.07310</link>
<guid>https://arxiv.org/abs/2411.07310</guid>
<content:encoded><![CDATA[
<div> calibration, material characterization, computational engineering, Bayesian inference, uncertainty quantification
<br />
Summary:
Interlaced Characterization and Calibration (ICC) presents an improved workflow for material model calibration, addressing current limitations. It efficiently uses full-field data for calibration, aligns collected data with optimal experimental design, quantifies parameter uncertainty through Bayesian inference, and incorporates a real-time feedback loop. Demonstrated on an aluminum cruciform specimen, ICC utilizes Bayesian optimal experimental design for selecting load steps, principal component analysis for reducing data dimensions, and fast surrogate models for computational efficiency. This framework allows for reliable calibration of high-fidelity constitutive models with quantified uncertainty. By advancing the state-of-the-art in material characterization and model calibration, ICC supports credible decision-making in solid mechanics modeling, potentially increasing modeling agility. 
<br /> <div>
arXiv:2411.07310v3 Announce Type: replace 
Abstract: Accurate material characterization and model calibration are essential for computationally-supported engineering decisions. Current characterization and calibration methods (1) use simplified test specimen geometries and global data, (2) cannot guarantee that sufficient characterization data is collected for a specific model of interest, (3) use deterministic methods that provide best-fit parameter values with no uncertainty quantification, and (4) are sequential, inflexible, and time-consuming. This work brings together several recent advancements into an improved workflow called Interlaced Characterization and Calibration that advances the state-of-the-art in constitutive model calibration. The ICC paradigm (1) efficiently uses full-field data to calibrate a high-fidelity material model, (2) aligns the data needed with the data collected with an optimal experimental design protocol, (3) quantifies parameter uncertainty through Bayesian inference, and (4) incorporates these advances into a quasi real-time feedback loop. The ICC framework is demonstrated on the calibration of a material model using simulated full-field data for an aluminum cruciform specimen being deformed bi-axially. The cruciform is actively driven through the myopically optimal load path using Bayesian optimal experimental design, which selects load steps that yield the maximum expected information gain. To aid in numerical stability and preserve computational resources, the full-field data is dimensionally reduced via principal component analysis, and fast surrogate models which approximate the input-output relationships of the expensive finite element model are used. The tools demonstrated here show that high-fidelity constitutive models can be efficiently and reliably calibrated with quantified uncertainty, thus supporting credible decision-making and potentially increasing the agility of solid mechanics modeling.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Optimization of Physics-Informed Neural Networks: Advancing Generalizability by the Baldwin Effect</title>
<link>https://arxiv.org/abs/2312.03243</link>
<guid>https://arxiv.org/abs/2312.03243</guid>
<content:encoded><![CDATA[
<div> evolutionary selection, physics-informed neural networks, meta-learning, prediction accuracy, computational efficiency
<br />
Summary:
This paper introduces a novel approach to enhance the generalizability of Physics-Informed Neural Networks (PINNs) by incorporating principles of Baldwinian evolution. The proposed Baldwinian-PINNs are pre-wired with connection strengths to enable efficient learning of physics tasks. By utilizing a two-stage stochastic programming formulation that combines evolutionary selection pressure and lifetime learning, these evolved PINNs exhibit fast and accurate predictions across various physics problems. The Baldwinian-PINNs outperform state-of-the-art gradient-based meta-learning methods by achieving over a 70x improvement in accuracy while requiring 700x less computational time when solving the diffusion-reaction equation. This research marks a significant advancement in meta-learning for PINNs, providing a more generalizable and efficient approach to solving complex physics simulations. Sample codes for implementation are available on GitHub for further exploration. 
<br /> <div>
arXiv:2312.03243v3 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) are at the forefront of scientific machine learning, making possible the creation of machine intelligence that is cognizant of physical laws and able to accurately simulate them. However, today's PINNs are often trained for a single physics task and require computationally expensive re-training for each new task, even for tasks from similar physics domains. To address this limitation, this paper proposes a pioneering approach to advance the generalizability of PINNs through the framework of Baldwinian evolution. Drawing inspiration from the neurodevelopment of precocial species that have evolved to learn, predict and react quickly to their environment, we envision PINNs that are pre-wired with connection strengths inducing strong biases towards efficient learning of physics. A novel two-stage stochastic programming formulation coupling evolutionary selection pressure (based on proficiency over a distribution of physics tasks) with lifetime learning (to specialize on a sampled subset of those tasks) is proposed to instantiate the Baldwin effect. The evolved Baldwinian-PINNs demonstrate fast and physics-compliant prediction capabilities across a range of empirically challenging problem instances with more than an order of magnitude improvement in prediction accuracy at a fraction of the computation cost compared to state-of-the-art gradient-based meta-learning methods. For example, when solving the diffusion-reaction equation, a 70x improvement in accuracy was obtained while taking 700x less computational time. This paper thus marks a leap forward in the meta-learning of PINNs as generalizable physics solvers. Sample codes are available at https://github.com/chiuph/Baldwinian-PINN.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dual-stage constitutive modeling framework based on finite strain data-driven identification and physics-augmented neural networks</title>
<link>https://arxiv.org/abs/2504.15492</link>
<guid>https://arxiv.org/abs/2504.15492</guid>
<content:encoded><![CDATA[
<div> approach; hyperelastic constitutive models; Data-Driven Identification; Physics-Augmented Neural Network; Finite Element simulations <br />
Summary: 
This study introduces a novel dual-stage approach for generating hyperelastic constitutive models using only experimentally measurable data. The first step involves applying a Data-Driven Identification (DDI) formulation to identify stress-strain tuples based on measured displacement fields. In the second step, a Physics-Augmented Neural Network (PANN) is calibrated using the data set, ensuring compliance with hyperelasticity conditions while maintaining flexibility. The approach is validated through synthetic 2D data generated in virtual experiments and applied in 3D Finite Element simulations. Additionally, a real experiment with noisy data is successfully mimicked, showcasing the effectiveness of the proposed method for automated hyperelastic model generation. <br /> <div>
arXiv:2504.15492v1 Announce Type: new 
Abstract: In this contribution, we present a novel consistent dual-stage approach for the automated generation of hyperelastic constitutive models which only requires experimentally measurable data. To generate input data for our approach, an experiment with full-field measurement has to be conducted to gather testing force and corresponding displacement field of the sample. Then, in the first step of the dual-stage framework, a new finite strain Data-Driven Identification (DDI) formulation is applied. This method enables to identify tuples consisting of stresses and strains by only prescribing the applied boundary conditions and the measured displacement field. In the second step, the data set is used to calibrate a Physics-Augmented Neural Network (PANN), which fulfills all common conditions of hyperelasticity by construction and is very flexible at the same time. We demonstrate the applicability of our approach by several descriptive examples. Two-dimensional synthetic data are exemplarily generated in virtual experiments by using a reference constitutive model. The calibrated PANN is then applied in 3D Finite Element simulations. In addition, a real experiment including noisy data is mimicked.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Higher-Order Interpolation and Restriction in ExaHyPE Avoiding Non-physical Reflections</title>
<link>https://arxiv.org/abs/2504.15814</link>
<guid>https://arxiv.org/abs/2504.15814</guid>
<content:encoded><![CDATA[
<div> Wave equations, adaptive mesh refinement, ExaHyPE, ExaGRyPE, interpolation schemes<br />
Summary:<br />
Wave equations play an essential role in understanding various phenomena, but tracking them over regular meshes is computationally challenging given their large scales. Adaptive mesh refinement (AMR) helps by creating higher-resolution meshes near regions of interest. ExaHyPE and ExaGRyPE are software engines developed to solve wave problems using AMR. To advance the mesh in time, a set of higher-order interpolation schemes was introduced to address errors near AMR boundaries and improve computational efficiency. By calculating derivatives at each coarse grid cell to approximate fine cells, these new methods outperform the traditional tensor-product approach in terms of speed and accuracy. This improvement was demonstrated in a benchmark simulation of a stationary black hole, where the errors near AMR boundaries were eliminated. <div>
arXiv:2504.15814v1 Announce Type: new 
Abstract: Wave equations help us to understand phenomena ranging from earthquakes to tsunamis. These phenomena materialise over very large scales. It would be computationally infeasible to track them over a regular mesh. Yet, since the phenomena are localised, adaptive mesh refinement (AMR) can be used to construct meshes with a higher resolution close to the regions of interest. ExaHyPE is a software engine created to solve wave problems using AMR, and we use it as baseline to construct our numerical relativity application called ExaGRyPE. To advance the mesh in time, we have to interpolate and restrict along resolution transitions in each and every time step. ExaHyPE's vanilla code version uses a d-linear tensor-product approach. In benchmarks of a stationary black hole this performs slowly and leads to errors in conserved quantities near AMR boundaries. We therefore introduce a set of higher-order interpolation schemes where the derivatives are calculated at each coarse grid cell to approximate the enclosed fine cells. The resulting methods run faster than the tensor-product approach. Most importantly, when running the stationary black hole simulation using the higher order methods the errors near the AMR boundaries are removed.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AneuPy: An open source Python tool for creating simulation-ready geometries of abdominal aortic aneurysms</title>
<link>https://arxiv.org/abs/2504.15285</link>
<guid>https://arxiv.org/abs/2504.15285</guid>
<content:encoded><![CDATA[
<div> AAA, abdominal aortic aneurysm, geometrical models, open-source software, AneuPy <br />
Summary:
Abdominal aortic aneurysms (AAAs) are a serious health concern, particularly among older individuals, due to their potential for life-threatening rupture. The geometric characteristics of AAAs, such as maximum diameter and wall thickness, are vital for assessing rupture risk. However, there is a lack of open-source software for generating simulation-ready AAA geometries. AneuPy, a Python-based tool, addresses this gap by automating the generation of idealized and patient-specific AAA geometrical models with minimal input data. By facilitating the creation of simulation-ready geometries for biomechanical and hemodynamic analyses, AneuPy aims to enhance research in AAA and improve patient-specific risk assessment. Its efficiency and flexibility make it a valuable tool for modeling AAA using finite element analysis (FEA), computational fluid dynamics (CFD), or fluid-structure interaction (FSI) methods. <br /><br />Summary: <div>
arXiv:2504.15285v1 Announce Type: cross 
Abstract: Abdominal aortic aneurysms (AAAs) are localized dilations of the abdominal aorta that can lead to life-threatening rupture if left untreated. AAAs predominantly affect older individuals, with a high mortality rate upon rupture, making early diagnosis and risk assessment critical. The geometric characteristics of an AAA, such as its maximum diameter, asymmetry, and wall thickness, play a crucial role in biomechanical models used to assess rupture risk. Despite the growing use of computational modeling to study AAAs, there is a lack of open source software that facilitates the generation of simulation-ready geometries tailored for biomechanical and hemodynamic analyses. To address this need, we introduce AneuPy, an open-source Python-based tool designed to generate idealized and patient-specific AAA geometrical models. AneuPy provides an efficient and automated approach to aneurysm geometry generation, requiring minimal input data while allowing for flexible parameterization. By streamlining the creation of simulation-ready geometries for finite element analysis (FEA), computational fluid dynamics (CFD), or fluid-structure interaction (FSI) models, AneuPy aims to facilitate research in AAAs and enhance patient-specific risk assessment.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.17908</link>
<guid>https://arxiv.org/abs/2412.17908</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, data poisoning, backdoor attack, detection methods, reinforcement learning

Summary:
The article explores the potential impact of large language models utilizing reinforcement learning in various fields, such as finance, physics, and artificial intelligence ecosystems. Specifically, the focus is on a backdoor attack named FinanceLLMsBackRL, which targets data poisoning without prior triggers. This attack poses a threat to financial institutions using reinforcement learning to simulate scenarios for models before and after regular operations. The study proposes a method for detecting such attacks through dynamic systems and statistical analysis of data distribution. Through this research, the authors aim to shed light on the vulnerabilities that may arise from the integration of large language models with reinforcement learning in critical sectors. This work underscores the importance of developing robust security measures to safeguard against potential threats in the evolving landscape of artificial intelligence systems. 

<br /><br />Summary: <div>
arXiv:2412.17908v3 Announce Type: replace-cross 
Abstract: With the rapid development of generative artificial intelligence, particularly large language models a number of sub-fields of deep learning have made significant progress and are now very useful in everyday applications. For example,financial institutions simulate a wide range of scenarios for various models created by their research teams using reinforcement learning, both before production and after regular operations. In this work, we propose a backdoor attack that focuses solely on data poisoning and a method of detection by dynamic systems and statistical analysis of the distribution of data. This particular backdoor attack is classified as an attack without prior consideration or trigger, and we name it FinanceLLMsBackRL. Our aim is to examine the potential effects of large language models that use reinforcement learning systems for text production or speech recognition, finance, physics, or the ecosystem of contemporary artificial intelligence models.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Aware Compression of Plasma Distribution Functions with GPU-Accelerated Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2504.14897</link>
<guid>https://arxiv.org/abs/2504.14897</guid>
<content:encoded><![CDATA[
<div> compression, plasma simulations, Gaussian Mixture Models, in-situ, real-time <br />
<br />
Summary: 
This article introduces a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) for large-scale plasma simulations. By approximating electron and ion velocity distribution functions with GMMs, the method captures plasma features like mean velocity and temperature, enabling identification of heating processes and beam generation. The approach involves constructing a histogram to reduce computational overhead and implementing GPU-accelerated, in-situ GMM fitting within the iPIC3D simulator for real-time compression. The compressed representation is stored using the ADIOS 2 library to optimize the I/O process. Compared to other algorithms like SZ, MGARD, and BLOSC2, the GMM-based method retains a physics-based approach, preserving the physical interpretation of plasma phenomena while achieving compression ratios of up to $10^4 and processing times comparable to standard compression engines. <div>
arXiv:2504.14897v1 Announce Type: new 
Abstract: Data compression is a critical technology for large-scale plasma simulations. Storing complete particle information requires Terabyte-scale data storage, and analysis requires ad-hoc scalable post-processing tools. We propose a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) to approximate electron and ion velocity distribution functions with a number of Gaussian components. This GMM-based method allows us to capture plasma features such as mean velocity and temperature, and it enables us to identify heating processes and generate beams. We first construct a histogram to reduce computational overhead and apply GPU-accelerated, in-situ GMM fitting within \texttt{iPIC3D}, a large-scale implicit Particle-in-Cell simulator, ensuring real-time compression. The compressed representation is stored using the \texttt{ADIOS 2} library, thus optimizing the I/O process. The GPU and histogramming implementation provides a significant speed-up with respect to GMM on particles (both in time and required memory at run-time), enabling real-time compression. Compared to algorithms like SZ, MGARD, and BLOSC2, our GMM-based method has a physics-based approach, retaining the physical interpretation of plasma phenomena such as beam formation, acceleration, and heating mechanisms. Our GMM algorithm achieves a compression ratio of up to $10^4$, requiring a processing time comparable to, or even lower than, standard compression engines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A parallel implementation of reduced-order modeling of large-scale systems</title>
<link>https://arxiv.org/abs/2504.14338</link>
<guid>https://arxiv.org/abs/2504.14338</guid>
<content:encoded><![CDATA[
<div> algorithm, distributed, reduced-order models, aerospace engineering, parallel

Summary:<br />
This paper introduces distributed Operator Inference (dOpInf), a parallel algorithm designed for constructing physics-based reduced-order models (ROMs) in large-scale aerospace engineering simulations. The algorithm efficiently processes high-dimensional datasets using distributed computing, enabling the learning of structured ROMs that approximate underlying dynamical systems. dOpInf is scalable, allowing for fully parallelized reduced modeling on thousands of processors. The resulting ROMs are computationally inexpensive, suitable for tasks such as design exploration, risk assessment, and uncertainty quantification. A tutorial using a 2D Navier-Stokes flow case study is provided to guide users through implementation, making dOpInf accessible for integration into complex aerospace simulations.<br /> <div>
arXiv:2504.14338v1 Announce Type: cross 
Abstract: Motivated by the large-scale nature of modern aerospace engineering simulations, this paper presents a detailed description of distributed Operator Inference (dOpInf), a recently developed parallel algorithm designed to efficiently construct physics-based reduced-order models (ROMs) for problems with large state dimensions. One such example is the simulation of rotating detonation rocket engines, where snapshot data generated by high-fidelity large-eddy simulations have many millions of degrees of freedom. dOpInf enables, via distributed computing, the efficient processing of datasets with state dimensions that are too large to process on a single computer, and the learning of structured physics-based ROMs that approximate the dynamical systems underlying those datasets. All elements of dOpInf are scalable, leading to a fully parallelized reduced modeling approach that can scale to the thousands of processors available on leadership high-performance computing platforms. The resulting ROMs are computationally cheap, making them ideal for key engineering tasks such as design space exploration, risk assessment, and uncertainty quantification. To illustrate the practical application of dOpInf, we provide a step-by-step tutorial using a 2D Navier-Stokes flow over a step scenario as a case study. This tutorial guides users through the implementation process, making dOpInf accessible for integration into complex aerospace engineering simulations.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
<div> Dialogue framework, teaching capabilities, evaluation, language models, pedagogical effectiveness<br />
Summary:<br />
EducationQ is a multi-agent dialogue framework designed to assess the teaching capabilities of large language models (LLMs). Evaluating 14 LLMs from major AI organizations, the study found that teaching effectiveness does not solely rely on model scale or general reasoning abilities. Smaller open-source models sometimes outperformed larger commercial ones in educational contexts. The research highlighted a gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Through a mixed-methods approach combining quantitative metrics, qualitative analysis, and expert case studies, distinct pedagogical strengths of top-performing models were identified. Human expert evaluations supported the automated qualitative analysis of effective teaching behaviors. The study suggests that LLMs-as-teachers require specialized optimization beyond simple scaling, indicating a need for targeted enhancement of specific pedagogical effectiveness in future educational AI development.<br /> 
Summary: <div>
arXiv:2504.14928v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues</title>
<link>https://arxiv.org/abs/2504.14963</link>
<guid>https://arxiv.org/abs/2504.14963</guid>
<content:encoded><![CDATA[
<div> acoustic features, speaker identification, textual data, fuzzy fingerprints, pre-trained models
Summary:
This study explores the use of fuzzy fingerprints from large pre-trained models to enhance text-based speaker identification. By incorporating speaker-specific tokens and context-aware modeling, the accuracy of speaker identification from text reaches up to 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. The approach of using fuzzy fingerprints enables approximation of full fine-tuning performance with fewer hidden units, enhancing interpretability. The analysis includes a mechanism to identify speaker-agnostic lines and handle ambiguous utterances. The study highlights the challenges in text-based speaker identification and offers insights for future improvements. 
<br /><br />Summary: <div>
arXiv:2504.14963v1 Announce Type: cross 
Abstract: Speaker identification using voice recordings leverages unique acoustic features, but this approach fails when only textual data is available. Few approaches have attempted to tackle the problem of identifying speakers solely from text, and the existing ones have primarily relied on traditional methods. In this work, we explore the use of fuzzy fingerprints from large pre-trained models to improve text-based speaker identification. We integrate speaker-specific tokens and context-aware modeling, demonstrating that conversational context significantly boosts accuracy, reaching 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering improved interpretability. Finally, we analyze ambiguous utterances and propose a mechanism to detect speaker-agnostic lines. Our findings highlight key challenges and provide insights for future improvements in text-based speaker identification.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends</title>
<link>https://arxiv.org/abs/2405.08542</link>
<guid>https://arxiv.org/abs/2405.08542</guid>
<content:encoded><![CDATA[
<div> Metaverse, Industrial Production, XR, Blockchain, AI

Summary:
The article explores the potential of the Industrial Metaverse in enhancing industrial production through technologies such as XR, blockchain, AI, digital twin, and 6G. It discusses the advantages of using the Metaverse in industrial settings and the key enabling technologies for various aspects of production operations. Challenges such as security concerns, resource limitations, and interoperability issues are identified, along with existing solutions to address them. The article also outlines future research directions and open issues in the Industrial Metaverse as it continues to evolve in the industrial production field. <div>
arXiv:2405.08542v2 Announce Type: replace 
Abstract: As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks. This deployment, called the Industrial Metaverse, is used for product design, production operations, industrial quality inspection, and product testing. However, there lacks of in-depth understanding of the enabling technologies associated with the Industrial Metaverse. This encompasses both the precise industrial scenarios targeted by each technology and the potential migration of technologies developed in other domains to the industrial sector. Driven by this issue, in this article, we conduct a comprehensive survey of the state-of-the-art literature on the Industrial Metaverse. Specifically, we first analyze the advantages of the Metaverse for industrial production. Then, we review a collection of key enabling technologies of the Industrial Metaverse, including blockchain (BC), digital twin (DT), 6G, XR, and artificial intelligence (AI), and analyze how these technologies can support different aspects of industrial production. Subsequently, we present numerous formidable challenges encountered within the Industrial Metaverse, including confidentiality and security concerns, resource limitations, and interoperability constraints. Furthermore, we investigate the extant solutions devised to address them. Finally, we briefly outline several open issues and future research directions of the Industrial Metaverse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An optimization-based coupling of reduced order models with efficient reduced adjoint basis generation approach</title>
<link>https://arxiv.org/abs/2408.14450</link>
<guid>https://arxiv.org/abs/2408.14450</guid>
<content:encoded><![CDATA[
<div> Optimization-based coupling, Lagrange multiplier, multiple modeling, simulation, reduced order models<br />
<br />
Summary: Optimization-based coupling (OBC) offers a promising approach in various modeling and simulation scenarios but faces challenges in time-dependent problems due to computational costs. This paper introduces an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission issue, utilizing reduced order models to alleviate computational burdens. The "optimize-then-reduce" strategy is employed to solve the minimization problem at each time step efficiently. A key innovation is the development of a technique for effective adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. Numerical experiments validate the accuracy of the proposed approach while comparing different methods for selecting reduced order bases for adjoint systems. Criteria such as decay of snapshot energy, average iteration counts, and timings are assessed to demonstrate the effectiveness of the optimization-based ROM-ROM coupling. <div>
arXiv:2408.14450v2 Announce Type: replace 
Abstract: Optimization-based coupling (OBC) is an attractive alternative to traditional Lagrange multiplier approaches in multiple modeling and simulation contexts. However, application of OBC to time-dependent problems has been hindered by the computational cost of finding the stationary points of the associated Lagrangian, which requires primal and adjoint solves. This issue can be mitigated by using OBC in conjunction with computationally efficient reduced order models (ROM). To demonstrate the potential of this combination, in this paper we develop an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission problem. We pursue the ``optimize-then-reduce'' path towards solving the minimization problem at each timestep and solve reduced-space adjoint system of equations, where the main challenge in this formulation is the generation of adjoint snapshots and reduced bases for the adjoint systems required by the optimizer. One of the main contributions of the paper is a new technique for efficient adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. We present numerical studies demonstrating the accuracy of the approach along with comparison between various approaches for selecting a reduced order basis for the adjoint systems, including decay of snapshot energy, average iteration counts, and timings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioFit: A WebGL-Based Tool for Fast and Efficient Parameterization of Cardiac Action Potential Models to Fit User-Provided Data</title>
<link>https://arxiv.org/abs/2504.13274</link>
<guid>https://arxiv.org/abs/2504.13274</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiac action potential, Particle swarm optimization, Interactive tool, Model parameter fitting, Physiological dynamics

Summary:
Interactive browser-based tool presented in this study utilizes particle swarm optimization (PSO) algorithm to find model parameter sets that accurately reproduce cardiac dynamics from user-provided data. The tool offers rapid customization and can achieve low-error fittings in a few iterations, requiring only a few seconds of runtime on typical machines. Users can select parameters to fit, define their value ranges, and adjust PSO algorithm hyperparameters through a user-friendly webpage interface. Various models were successfully fitted to different datasets, demonstrating the tool's versatility and efficiency. Convergence of fitting is influenced by the choice of model, dataset characteristics, and PSO algorithm settings. Through these fittings, new insights were gained regarding the physiological and dynamical implications of the model parameters.  <br /><br />Summary: <div>
arXiv:2504.13274v1 Announce Type: new 
Abstract: Cardiac action potential models allow examination of a variety of cardiac dynamics, including how behavior may change under specific interventions. To study a specific scenario, including patient-specific cases, model parameter sets must be found that accurately reproduce the dynamics of interest. To facilitate this complex and time-consuming process, we present an interactive browser-based tool that uses the particle swarm optimization (PSO) algorithm implemented in JavaScript and taking advantage of the WebGL API for hardware acceleration. Our tool allows rapid customization and can find low-error fittings to user-provided voltage time series or action potential duration data from multiple cycle lengths in a few iterations (10-32), corresponding to a runtime of a few seconds on most machines. Additionally, our tool focuses on ease of use and flexibility, providing a webpage interface that allows users to select a subset of parameters to fit, set the range of values each parameter is allowed to assume, and control the PSO algorithm hyperparameters. We demonstrate our tool's utility by fitting a variety of models to different datasets, showing how convergence is affected by model choice, dataset properties, and PSO algorithmic settings, and explaining new insights gained about the physiological and dynamical roles of the model parameters.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ascribe New Dimensions to Scientific Data Visualization with VR</title>
<link>https://arxiv.org/abs/2504.13448</link>
<guid>https://arxiv.org/abs/2504.13448</guid>
<content:encoded><![CDATA[
<div> Keywords: computer mouse, scientific images, Virtual Reality, AI-driven algorithms, multimodal analysis<br />
<br />
Summary: <br />
The article discusses the limitations of the traditional 2D visualization methods in exploring complex, multi-scale scientific images using a computer mouse. It introduces ASCRIBE-VR, a Virtual Reality platform that integrates AI-driven algorithms with scientific images. ASCRIBE-VR allows for immersive and interactive visualization, supporting the analysis of advanced datasets such as X-ray CT and Magnetic Resonance. The VR tools are compatible with Meta Quest and can seamlessly explore large-scale 3D images by merging AI-generated results with VR visualization. This integration enhances scientific discovery by bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins. <div>
arXiv:2504.13448v1 Announce Type: cross 
Abstract: For over half a century, the computer mouse has been the primary tool for interacting with digital data, yet it remains a limiting factor in exploring complex, multi-scale scientific images. Traditional 2D visualization methods hinder intuitive analysis of inherently 3D structures. Virtual Reality (VR) offers a transformative alternative, providing immersive, interactive environments that enhance data comprehension. This article introduces ASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research with Immersive Browsing \& Exploration, which integrates AI-driven algorithms with scientific images. ASCRIBE-VR enables multimodal analysis, structural assessments, and immersive visualization, supporting scientific visualization of advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D imaging. Our VR tools, compatible with Meta Quest, can consume the output of our AI-based segmentation and iterative feedback processes to enable seamless exploration of large-scale 3D images. By merging AI-generated results with VR visualization, ASCRIBE-VR enhances scientific discovery, bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Models Meet Financial Data Modalities</title>
<link>https://arxiv.org/abs/2504.13521</link>
<guid>https://arxiv.org/abs/2504.13521</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithmic trading, deep learning, limit order book, high-frequency trading, predictive performance

Summary: 
This study explores the integration of deep learning models with various financial data sources to improve algorithmic trading strategies and portfolio optimization. By developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation, the researchers achieved state-of-the-art performance in high-frequency trading algorithms. This approach underscores the effectiveness of deep learning in handling structured financial data and enhancing predictive performance in trading strategies. The study highlights the potential of incorporating limit order book analysis into algorithmic trading using deep learning models. By leveraging deep learning techniques, the researchers were able to extract meaningful signals from diverse financial data sources, including candlestick charts, order statistics, traded volume data, and news flow. This research contributes to bridging the gap between deep learning and structured financial data analysis, showcasing the promise of deep learning in financial applications. 

Summary: <div>
arXiv:2504.13521v1 Announce Type: cross 
Abstract: Algorithmic trading relies on extracting meaningful signals from diverse financial data sources, including candlestick charts, order statistics on put and canceled orders, traded volume data, limit order books, and news flow. While deep learning has demonstrated remarkable success in processing unstructured data and has significantly advanced natural language processing, its application to structured financial data remains an ongoing challenge. This study investigates the integration of deep learning models with financial data modalities, aiming to enhance predictive performance in trading strategies and portfolio optimization. We present a novel approach to incorporating limit order book analysis into algorithmic trading by developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation. Our methodology for processing limit order book data achieves state-of-the-art performance in high-frequency trading algorithms, underscoring the effectiveness of deep learning in financial applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bitcoin's Edge: Embedded Sentiment in Blockchain Transactional Data</title>
<link>https://arxiv.org/abs/2504.13598</link>
<guid>https://arxiv.org/abs/2504.13598</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency, Blockchain, Natural Language Processing, Sentiment Analysis, Financial Predictions

Summary:
Blockchain technology is not only used for financial transactions but also for storing and sharing non-financial content. This hidden content can impact cryptocurrency price movements by conveying private information and shaping public sentiment. Current methods of analyzing blockchain data are limited, prompting the use of Natural Language Processing techniques to extract sentiment from transactional data. The study demonstrates the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency prices on Bitcoin and Ethereum blockchains. It uncovers an informational advantage for Bitcoin over Ethereum, showing that sentiment analysis can effectively predict price movements. This research highlights the value of blockchain sentiment analysis in enhancing financial predictions within cryptocurrency markets, providing a novel framework for leveraging freely available, transparent, and immutable data for informed decision-making in the digital asset space. 

<br /><br />Summary: <div>
arXiv:2504.13598v1 Announce Type: cross 
Abstract: Cryptocurrency blockchains, beyond their primary role as distributed payment systems, are increasingly used to store and share arbitrary content, such as text messages and files. Although often non-financial, this hidden content can impact price movements by conveying private information, shaping sentiment, and influencing public opinion. However, current analyses of such data are limited in scope and scalability, primarily relying on manual classification or hand-crafted heuristics. In this work, we address these limitations by employing Natural Language Processing techniques to analyze, detect patterns, and extract public sentiment encoded within blockchain transactional data. Using a variety of Machine Learning techniques, we showcase for the first time the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency price movements on the Bitcoin and Ethereum blockchains. Our findings shed light on a previously underexplored source of freely available, transparent, and immutable data and introduce blockchain sentiment analysis as a novel and robust framework for enhancing financial predictions in cryptocurrency markets. Incidentally, we discover an asymmetry between cryptocurrencies; Bitcoin has an informational advantage over Ethereum in that the sentiment embedded into transactional data is sufficient to predict its price movement.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems</title>
<link>https://arxiv.org/abs/2504.13768</link>
<guid>https://arxiv.org/abs/2504.13768</guid>
<content:encoded><![CDATA[
<div> Graph neural network, multi-body dynamical systems, internal loads, predictive maintenance, digital twin <br />
Summary: Equi-Euler GraphNet is proposed for accurate real-time modeling of multi-body dynamical systems. It simultaneously predicts internal forces and global trajectories, crucial for fault detection and predictive maintenance in digital twin applications. It introduces equivariant message-passing and temporal-aware iterative node update mechanisms, tailored for cylindrical roller bearings. Trained on high-fidelity simulations, it generalizes well and outperforms state-of-the-art models in trajectory prediction. Equi-Euler GraphNet achieves efficient and stable rollouts over thousands of time steps with minimal error accumulation, providing a 200x speedup over conventional solvers. This makes it a valuable tool for digital twins, design, and maintenance applications. <br /> <div>
arXiv:2504.13768v1 Announce Type: cross 
Abstract: Accurate real-time modeling of multi-body dynamical systems is essential for enabling digital twin applications across industries. While many data-driven approaches aim to learn system dynamics, jointly predicting internal loads and system trajectories remains a key challenge. This dual prediction is especially important for fault detection and predictive maintenance, where internal loads-such as contact forces-act as early indicators of faults, reflecting wear or misalignment before affecting motion. These forces also serve as inputs to degradation models (e.g., crack growth), enabling damage prediction and remaining useful life estimation. We propose Equi-Euler GraphNet, a physics-informed graph neural network (GNN) that simultaneously predicts internal forces and global trajectories in multi-body systems. In this mesh-free framework, nodes represent system components and edges encode interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an equivariant message-passing scheme, interpreting edge messages as interaction forces consistent under Euclidean transformations; and (2) a temporal-aware iterative node update mechanism, based on Euler integration, to capture influence of distant interactions over time. Tailored for cylindrical roller bearings, it decouples ring dynamics from constrained motion of rolling elements. Trained on high-fidelity multiphysics simulations, Equi-Euler GraphNet generalizes beyond the training distribution, accurately predicting loads and trajectories under unseen speeds, loads, and configurations. It outperforms state-of-the-art GNNs focused on trajectory prediction, delivering stable rollouts over thousands of time steps with minimal error accumulation. Achieving up to a 200x speedup over conventional solvers while maintaining comparable accuracy, it serves as an efficient reduced-order model for digital twins, design, and maintenance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer Encoder and Multi-features Time2Vec for Financial Prediction</title>
<link>https://arxiv.org/abs/2504.13801</link>
<guid>https://arxiv.org/abs/2504.13801</guid>
<content:encoded><![CDATA[
<div> Transformers, financial prediction, time series analysis, attention mechanism, correlation feature selection<br />
<br />
Summary:<br />
This paper introduces a novel neural network architecture that combines Time2Vec with the Encoder of the Transformer model for financial prediction. By studying different markets, a correlation feature selection method is proposed to improve the accuracy of predicting multiple stock prices. Through fine-tuning hyperparameters, the method outperforms benchmark models and traditional encoding methods like positional encoding. The integration of Time2Vec with the Transformer model allows for the capture of both short and long-range dependencies, aiding in understanding broader market trends. Selecting correlation features further enhances prediction accuracy, particularly in industries where companies exhibit correlated stock price movements. <div>
arXiv:2504.13801v1 Announce Type: cross 
Abstract: Financial prediction is a complex and challenging task of time series analysis and signal processing, expected to model both short-term fluctuations and long-term temporal dependencies. Transformers have remarkable success mostly in natural language processing using attention mechanism, which also influenced the time series community. The ability to capture both short and long-range dependencies helps to understand the financial market and to recognize price patterns, leading to successful applications of Transformers in stock prediction. Although, the previous research predominantly focuses on individual features and singular predictions, that limits the model's ability to understand broader market trends. In reality, within sectors such as finance and technology, companies belonging to the same industry often exhibit correlated stock price movements.
  In this paper, we develop a novel neural network architecture by integrating Time2Vec with the Encoder of the Transformer model. Based on the study of different markets, we propose a novel correlation feature selection method. Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a comparative analysis of our results against benchmark models. We conclude that our method outperforms other state-of-the-art encoding methods such as positional encoding, and we also conclude that selecting correlation features enhance the accuracy of predicting multiple stock prices.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding</title>
<link>https://arxiv.org/abs/2408.11363</link>
<guid>https://arxiv.org/abs/2408.11363</guid>
<content:encoded><![CDATA[
<div> Keywords: ProteinGPT, large language model, protein sequences, protein structures, protein analysis <br />
Summary: ProteinGPT is a cutting-edge multimodal large language model designed for analyzing protein sequences and structures efficiently. By integrating protein sequence and structure encoders with a large language model, ProteinGPT can provide accurate and contextually relevant responses to protein-related queries. The model was trained on a dataset of 132,092 proteins, each annotated with property tags and QA pairs, using GPT-4o for instruction tuning. Experimental results show that ProteinGPT outperforms baseline models and general-purpose LLMs in understanding and responding to protein-related questions, achieving high performance on both semantic and lexical metrics. The code and data for ProteinGPT are available on GitHub, making this powerful tool accessible to researchers in the field. <br /><br /> <div>
arXiv:2408.11363v2 Announce Type: replace-cross 
Abstract: Understanding biological processes, drug development, and biotechnological advancements requires a detailed analysis of protein structures and functions, a task that is inherently complex and time-consuming in traditional protein research. To streamline this process, we introduce ProteinGPT, a state-of-the-art multimodal large language model for proteins that enables users to upload protein sequences and/or structures for comprehensive analysis and responsive inquiries. ProteinGPT integrates protein sequence and structure encoders with linear projection layers to ensure precise representation adaptation and leverages a large language model (LLM) to generate accurate, contextually relevant responses. To train ProteinGPT, we constructed a large-scale dataset of 132,092 proteins, each annotated with 20-30 property tags and 5-10 QA pairs per protein, and optimized the instruction-tuning process using GPT-4o. Experiments demonstrate that ProteinGPT effectively generates informative responses to protein-related questions, achieving high performance on both semantic and lexical metrics and significantly outperforming baseline models and general-purpose LLMs in understanding and responding to protein-related queries. Our code and data are available at https://github.com/ProteinGPT/ProteinGPT.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Accurate Prediction of Antenna Reflection Coefficients in Planar Layered Media Environment via Generalized Scattering Matrix</title>
<link>https://arxiv.org/abs/2504.12613</link>
<guid>https://arxiv.org/abs/2504.12613</guid>
<content:encoded><![CDATA[
<div> Keywords: reflection coefficient, antenna, planar layered medium, generalized scattering matrix, numerical algorithm

Summary: 
The article presents a new numerical algorithm for evaluating the reflection coefficient of an antenna in the presence of a planar layered medium. This algorithm utilizes the antenna's generalized scattering matrix (GSM) to model the interaction between the antenna and the medium through spherical-to-planar vector wave transformations. By avoiding approximations that could compromise accuracy, the algorithm reduces algebraic complexity and significantly speeds up antenna performance evaluation. While a one-time preprocessing cost is required to obtain the antenna's GSM in free space, the numerical evaluation speed of this method surpasses that of commercial software FEKO by several orders of magnitude, maintaining high accuracy. This advancement in computational efficiency offers a promising approach for accurately evaluating antenna performance in complex scenarios involving layered media. 

<br /><br />Summary: <div>
arXiv:2504.12613v1 Announce Type: new 
Abstract: The numerical algorithm for evaluating the reflection coefficient of an antenna in the presence of the planar layered medium is reformulated using the antenna's generalized scattering matrix (GSM). The interaction between the antenna and the layered medium is modeled through spherical-to-planar vector wave transformations, ensuring no approximations that could compromise computational accuracy. This theoretical framework significantly reduces algebraic complexity, resulting in a marked increase in the speed of antenna performance evaluation. Excluding the one-time preprocessing cost of obtaining the antenna's GSM in free space, the numerical evaluation speed of this method exceeds that of the commercial software FEKO by several orders of magnitude, while maintaining nearly identical accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study</title>
<link>https://arxiv.org/abs/2504.12503</link>
<guid>https://arxiv.org/abs/2504.12503</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, engineering design, continual learning, regression tasks, catastrophic forgetting

Summary:
Continual learning (CL) is introduced to engineering design to address the challenge of incorporating new knowledge into models without retraining from scratch, which is computationally expensive. This study benchmarks various CL methods on regression tasks using five engineering datasets and establishes nine new engineering CL benchmarks. The results show that existing CL methods can improve performance compared to naive baselines, with the Replay strategy achieving comparable performance to retraining while reducing training time significantly. This points to the potential of CL in enhancing real-world engineering workflows. The code and datasets used in the study will be made available for further research and applications. 

<br /><br />Summary: <div>
arXiv:2504.12503v1 Announce Type: cross 
Abstract: Engineering problems that apply machine learning often involve computationally intensive methods but rely on limited datasets. As engineering data evolves with new designs and constraints, models must incorporate new knowledge over time. However, high computational costs make retraining models from scratch infeasible. Continual learning (CL) offers a promising solution by enabling models to learn from sequential data while mitigating catastrophic forgetting, where a model forgets previously learned mappings. This work introduces CL to engineering design by benchmarking several CL methods on representative regression tasks. We apply these strategies to five engineering datasets and construct nine new engineering CL benchmarks to evaluate their ability to address forgetting and improve generalization. Preliminary results show that applying existing CL methods to these tasks improves performance over naive baselines. In particular, the Replay strategy achieved performance comparable to retraining in several benchmarks while reducing training time by nearly half, demonstrating its potential for real-world engineering workflows. The code and datasets used in this work will be available at: https://github.com/kmsamuel/cl-for-engineering-release.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational quantum and neural quantum states algorithms for the linear complementarity problem</title>
<link>https://arxiv.org/abs/2504.08141</link>
<guid>https://arxiv.org/abs/2504.08141</guid>
<content:encoded><![CDATA[
<div> variational quantum algorithms, VQAs, hybrid quantum-classical methods, variational quantum linear solver, VQLS

Summary:
Variational quantum algorithms (VQAs) are being explored as a way to harness quantum computing while addressing current hardware limitations. This study introduces the variational quantum linear solver (VQLS) and its classical counterpart, the variational neural linear solver (VNLS), within a minimum map Newton solver for a rigid body contact model. The VNLS accurately simulates the dynamics of rigid spherical bodies during collisions, suggesting that quantum and quantum-inspired linear algebra algorithms can be effective for modeling certain physical systems. This research opens up the possibility of using VQAs and quantum-inspired classical algorithms to solve real-world problems, highlighting their potential utility in various applications. <br /><br />Summary: <div>
arXiv:2504.08141v2 Announce Type: replace 
Abstract: Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Editing for Offline Model-based Optimization</title>
<link>https://arxiv.org/abs/2405.13964</link>
<guid>https://arxiv.org/abs/2405.13964</guid>
<content:encoded><![CDATA[
<div> Offline model-based optimization, surrogate model, out-of-distribution issue, diffusion prior, Design Editing for Offline Model-based Optimization (DEMO)<br />
Summary:<br />
Offline model-based optimization (MBO) involves maximizing a black-box objective function using a dataset of designs and scores. Traditional methods use surrogate models but are prone to errors when predicting scores for new designs. To address this, DEMO introduces a diffusion prior to calibrate overly optimized designs. It generates pseudo design candidates using gradient ascent, then refines them through an editing process involving noise and denoising with the diffusion prior. Empirical evaluations across seven tasks show that DEMO, with proper tuning, achieves competitive scores compared to previous literature. <div>
arXiv:2405.13964v4 Announce Type: replace-cross 
Abstract: Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. While these pseudo design candidates contain information beyond the offline dataset, they might be invalid or have erroneously high predicted scores. Therefore, to address this challenge while utilizing the information provided by pseudo design candidates, we propose an editing process to refine these pseudo design candidates. We introduce noise to the pseudo design candidates and subsequently denoise them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. Empirical evaluations on seven offline MBO tasks show that, with properly tuned hyperparameters, DEMOs score is competitive with the best previously reported scores in the literature.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Contextual Market Equilibrium Computation through Deep Learning</title>
<link>https://arxiv.org/abs/2406.15459</link>
<guid>https://arxiv.org/abs/2406.15459</guid>
<content:encoded><![CDATA[
<div> Market equilibrium, computational economics, large-scale buyer population, contextual market model, deep learning-based method<br />
<br />
Summary: 
The paper explores the computation of market equilibrium in scenarios with a large-scale buyer population using a deep learning-based method called MarketFCNet. The approach introduces a contextual market model where buyers and goods are represented by their contexts. MarketFCNet uses a neural network to parameterize the allocation of goods to buyers based on their contexts. An efficient method is proposed to estimate the loss function for training the network, enabling optimization through gradient descent. The Nash Gap metric is introduced to measure the deviation of the allocation and prices from the market equilibrium. Experimental results show that MarketFCNet outperforms existing methods in terms of performance and running times as the market scale increases, showcasing the potential of deep learning in approximating large-scale contextual market equilibrium. <br /><br /> <div>
arXiv:2406.15459v2 Announce Type: replace-cross 
Abstract: Market equilibrium is one of the most fundamental solution concepts in economics and social optimization analysis. Existing works on market equilibrium computation primarily focus on settings with relatively few buyers. Motivated by this, our paper investigates the computation of market equilibrium in scenarios with a large-scale buyer population, where buyers and goods are represented by their contexts. Building on this realistic and generalized contextual market model, we introduce MarketFCNet, a deep learning-based method for approximating market equilibrium. We start by parameterizing the allocation of each good to each buyer using a neural network, which depends solely on the context of the buyer and the good. Next, we propose an efficient method to unbiasedly estimate the loss function of the training algorithm, enabling us to optimize the network parameters through gradient. To evaluate the approximated solution, we propose a metric called Nash Gap, which quantifies the deviation of the given allocation and price pair from the market equilibrium. Experimental results indicate that MarketFCNet delivers competitive performance and significantly lower running times compared to existing methods as the market scale expands, demonstrating the potential of deep learning-based methods to accelerate the approximation of large-scale contextual market equilibrium.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrivAerML: High-Fidelity Computational Fluid Dynamics Dataset for Road-Car External Aerodynamics</title>
<link>https://arxiv.org/abs/2408.11969</link>
<guid>https://arxiv.org/abs/2408.11969</guid>
<content:encoded><![CDATA[
<div> Machine Learning, automotive aerodynamics, open-source dataset, high-fidelity CFD, DrivAer notchback<br />
Summary:<br />
Machine Learning has the potential to transform automotive aerodynamics by providing quick flow predictions during the design phase. However, a lack of open-source training data for realistic road cars using high-fidelity CFD methods is impeding progress. To overcome this hurdle, a high-fidelity open-source dataset for automotive aerodynamics has been created. The dataset comprises 500 variants of the DrivAer notchback vehicle, generated through parametric morphing. Mesh generation and scale-resolving CFD were conducted using state-of-the-art automated workflows. The dataset, published under the CC-BY-SA license, includes geometries and detailed aerodynamic information. This initiative marks the first large public-domain dataset for complex automotive configurations derived from high-fidelity CFD simulations. <div>
arXiv:2408.11969v2 Announce Type: replace-cross 
Abstract: Machine Learning (ML) has the potential to revolutionise the field of automotive aerodynamics, enabling split-second flow predictions early in the design process. However, the lack of open-source training data for realistic road cars, using high-fidelity CFD methods, represents a barrier to their development. To address this, a high-fidelity open-source (CC-BY-SA) public dataset for automotive aerodynamics has been generated, based on 500 parametrically morphed variants of the widely-used DrivAer notchback generic vehicle. Mesh generation and scale-resolving CFD was executed using consistent and validated automatic workflows representative of the industrial state-of-the-art. Geometries and rich aerodynamic data are published in open-source formats. To our knowledge, this is the first large, public-domain dataset for complex automotive configurations generated using high-fidelity CFD.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Algorithms for Option Hedging</title>
<link>https://arxiv.org/abs/2504.05521</link>
<guid>https://arxiv.org/abs/2504.05521</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, dynamic hedging, financial assets, risk, DRL algorithms<br />
<br />
Summary: 
Dynamic hedging involves offsetting financial risk through periodic transactions. This study compares the performance of eight DRL algorithms in dynamic hedging scenarios. The algorithms include Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), various Deep Q-Learning (DQL) and Deep Deterministic Policy Gradient (DDPG) variants. The experiments use a GJR-GARCH(1,1) model to simulate the dataset and evaluate against the Black-Scholes delta hedge baseline. Results show that MCPG and PPO perform best in terms of the root semi-quadratic penalty. MCPG outperforms the baseline within the allotted computational budget due to sparse rewards in the environment. This comparison provides insights into the effectiveness of different DRL algorithms in dynamic hedging strategies, with potential implications for financial risk management. 
<br /><br />Summary: <div>
arXiv:2504.05521v2 Announce Type: replace-cross 
Abstract: Dynamic hedging is a financial strategy that consists in periodically transacting one or multiple financial assets to offset the risk associated with a correlated liability. Deep Reinforcement Learning (DRL) algorithms have been used to find optimal solutions to dynamic hedging problems by framing them as sequential decision-making problems. However, most previous work assesses the performance of only one or two DRL algorithms, making an objective comparison across algorithms difficult. In this paper, we compare the performance of eight DRL algorithms in the context of dynamic hedging; Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), along with four variants of Deep Q-Learning (DQL) and two variants of Deep Deterministic Policy Gradient (DDPG). Two of these variants represent a novel application to the task of dynamic hedging. In our experiments, we use the Black-Scholes delta hedge as a baseline and simulate the dataset using a GJR-GARCH(1,1) model. Results show that MCPG, followed by PPO, obtain the best performance in terms of the root semi-quadratic penalty. Moreover, MCPG is the only algorithm to outperform the Black-Scholes delta hedge baseline with the allotted computational budget, possibly due to the sparsity of rewards in our environment.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal integration of chemical structures improves representations of microscopy images for morphological profiling</title>
<link>https://arxiv.org/abs/2504.09544</link>
<guid>https://arxiv.org/abs/2504.09544</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, deep learning, morphological profiling, high-throughput microscopy screens, chemical compound structure

Summary:
Recent advances in self-supervised deep learning have led to improved quantification of cellular morphological changes in high-throughput microscopy screens. The new framework, MICON (Molecular-Image Contrastive Learning), incorporates chemical compound information during pre-training to enhance learned image representations. MICON surpasses traditional feature extraction methods like CellProfiler and existing deep learning approaches, especially in identifying consistent effects of drugs across independent replicates and data centers. By modeling chemical compounds as treatments inducing counterfactual transformations of cell phenotypes, MICON outperforms methods that directly align images and compounds. This highlights the importance of considering the multimodal nature of microscopy screening data in representation learning for morphological profiling. MICON's success suggests a promising direction for future research in this field.<br /><br />Summary: <div>
arXiv:2504.09544v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised deep learning have improved our ability to quantify cellular morphological changes in high-throughput microscopy screens, a process known as morphological profiling. However, most current methods only learn from images, despite many screens being inherently multimodal, as they involve both a chemical or genetic perturbation as well as an image-based readout. We hypothesized that incorporating chemical compound structure during self-supervised pre-training could improve learned representations of images in high-throughput microscopy screens. We introduce a representation learning framework, MICON (Molecular-Image Contrastive Learning), that models chemical compounds as treatments that induce counterfactual transformations of cell phenotypes. MICON significantly outperforms classical hand-crafted features such as CellProfiler and existing deep-learning-based representation learning methods in challenging evaluation settings where models must identify reproducible effects of drugs across independent replicates and data-generating centers. We demonstrate that incorporating chemical compound information into the learning process provides consistent improvements in our evaluation setting and that modeling compounds specifically as treatments in a causal framework outperforms approaches that directly align images and compounds in a single representation space. Our findings point to a new direction for representation learning in morphological profiling, suggesting that methods should explicitly account for the multimodal nature of microscopy screening data.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A viscoplasticity model with an invariant-based non-Newtonian flow rule for unidirectional thermoplastic composites</title>
<link>https://arxiv.org/abs/2504.12069</link>
<guid>https://arxiv.org/abs/2504.12069</guid>
<content:encoded><![CDATA[
arXiv:2504.12069v1 Announce Type: new 
Abstract: A three-dimensional mesoscopic viscoplasticity model for simulating rate-dependent plasticity and creep in unidirectional thermoplastic composites is presented. The constitutive model is a transversely isotropic extension of an isotropic finite strain viscoplasticity model for neat polymers. Rate-dependent plasticity and creep are described by a non-Newtonian flow rule where the viscosity of the material depends on an equivalent stress measure through an Eyring-type relation. In the present formulation, transverse isotropy is incorporated by defining the equivalent stress measure and flow rule as functions of transversely isotropic stress invariants. In addition, the Eyring-type viscosity function is extended with anisotropic pressure dependence. As a result of the formulation, plastic flow in fiber direction is effectively excluded and pressure dependence of the polymer matrix is accounted for. The re-orientation of the transversely isotropic plane during plastic deformations is incorporated in the constitutive equations, allowing for an accurate large deformation response. The formulation is fully implicit and a consistent linearization of the algorithmic constitutive equations is performed to derive the consistent tangent modulus. The performance of the mesoscopic constitutive model is assessed through a comparison with a micromechanical model for carbon/PEEK, with the original isotropic viscoplastic version for the polymer matrix and with hyperelastic fibers. The micromodel is first used to determine the material parameters of the mesoscale model with a few stress-strain curves. It is demonstrated that the mesoscale model gives a similar response to the micromodel under various loading conditions. Finally, the mesoscale model is validated against off-axis experiments on unidirectional thermoplastic composite plies.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Material Network: Overview, applications and current directions</title>
<link>https://arxiv.org/abs/2504.12159</link>
<guid>https://arxiv.org/abs/2504.12159</guid>
<content:encoded><![CDATA[
arXiv:2504.12159v1 Announce Type: new 
Abstract: Deep Material Network (DMN) has emerged as a powerful framework for multiscale material modeling, enabling efficient and accurate predictions of material behavior across different length scales. Unlike traditional machine learning approaches, the trainable parameters in DMN have direct physical interpretations, capturing the geometric characteristics of the microstructure rather than serving as purely statistical fitting parameters. Its hierarchical tree structure effectively encodes microstructural interactions and deformation mechanisms, allowing DMN to achieve a balance between accuracy and computational efficiency. This physics-informed architecture significantly reduces computational costs compared to direct numerical simulations while preserving essential microstructural physics. Furthermore, DMN can be trained solely on a linear elastic dataset while effectively extrapolating nonlinear responses during online prediction, making it a highly efficient and scalable approach for multiscale material modeling. This article provides a comprehensive review of DMN, detailing its motivation, underlying methodology, and recent advancements. We discuss key modeling aspects, including its hierarchical structure, training process, and the role of physics-based constraints in enhancing predictive accuracy. Furthermore, we highlight its applications in component-scale multiscale analysis and inverse parameter identification, demonstrating its capability to bridge microscale material behavior with macroscale engineering predictions. Finally, we discuss challenges and future directions in improving DMN's generalization capabilities and its potential extensions for broader applications in multiscale modeling.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Analysis of Mixer Activities in the Bitcoin Network</title>
<link>https://arxiv.org/abs/2504.11924</link>
<guid>https://arxiv.org/abs/2504.11924</guid>
<content:encoded><![CDATA[
arXiv:2504.11924v1 Announce Type: cross 
Abstract: Cryptocurrency users increasingly rely on obfuscation techniques such as mixers, swappers, and decentralised or no-KYC exchanges to protect their anonymity. However, at the same time, these services are exploited by criminals to conceal and launder illicit funds. Among obfuscation services, mixers remain one of the most challenging entities to tackle. This is because their owners are often unwilling to cooperate with Law Enforcement Agencies, and technically, they operate as 'black boxes'. To better understand their functionalities, this paper proposes an approach to analyse the operations of mixers by examining their address-transaction graphs and identifying topological similarities to uncover common patterns that can define the mixer's modus operandi. The approach utilises community detection algorithms to extract dense topological structures and clustering algorithms to group similar communities. The analysis is further enriched by incorporating data from external sources related to known Exchanges, in order to understand their role in mixer operations. The approach is applied to dissect the Blender.io mixer activities within the Bitcoin blockchain, revealing: i) consistent structural patterns across address-transaction graphs; ii) that Exchanges play a key role, following a well-established pattern, which raises several concerns about their AML/KYC policies. This paper represents an initial step toward dissecting and understanding the complex nature of mixer operations in cryptocurrency networks and extracting their modus operandi.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection</title>
<link>https://arxiv.org/abs/2502.07384</link>
<guid>https://arxiv.org/abs/2502.07384</guid>
<content:encoded><![CDATA[
arXiv:2502.07384v2 Announce Type: replace 
Abstract: Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. At the inter-modality level, SAGEPhos introduces a Bio-Coupled Modal Fusion method, distilling essential kinase sequence information to refine task-oriented local substrate feature space, creating a shared semantic space that captures crucial kinase-substrate interaction patterns. Within the substrate's intra-modality domain, it focuses on Bio-Augmented Fusion, emphasizing 2D local sequence information while selectively incorporating 3D spatial information from predicted structures to complement the sequence space. Moreover, to address the lack of structural information in current datasets, we contribute a new, refined phosphorylation site prediction dataset, which incorporates crucial structural elements and will serve as a new benchmark for the field. Experimental results demonstrate that SAGEPhos significantly outperforms baseline methods. We release the SAGEPhos models and code at https://github.com/ZhangJJ26/SAGEPhos.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QPET: A Versatile and Portable Quantity-of-Interest-Preservation Framework for Error-Bounded Lossy Compression</title>
<link>https://arxiv.org/abs/2412.02799</link>
<guid>https://arxiv.org/abs/2412.02799</guid>
<content:encoded><![CDATA[
arXiv:2412.02799v3 Announce Type: replace-cross 
Abstract: Error-bounded lossy compression has been widely adopted in many scientific domains because it can address the challenges in storing, transferring, and analyzing unprecedented amounts of scientific data. Although error-bounded lossy compression offers general data distortion control by enforcing strict error bounds on raw data, it may fail to meet the quality requirements on the results of downstream analysis, a.k.a. Quantities of Interest (QoIs), derived from raw data. This may lead to uncertainties and even misinterpretations in scientific discoveries, significantly limiting the use of lossy compression in practice. In this paper, we propose QPET, a novel, versatile, and portable framework for QoI-preserving error-bounded lossy compression, which overcomes the challenges of modeling diverse QoIs by leveraging numerical strategies. QPET features (1) high portability to multiple existing lossy compressors, (2) versatile preservation to most differentiable univariate and multivariate QoIs, and (3) significant compression improvements in QoI-preservation tasks. Experiments with six real-world datasets demonstrate that integrating QPET into state-of-the-art error-bounded lossy compressors can gain 2x to 10x compression speedups of existing QoI-preserving error-bounded lossy compression solutions, up to 1000% compression ratio improvements to general-purpose compressors, and up to 133% compression ratio improvements to existing QoI-integrated scientific compressors.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>