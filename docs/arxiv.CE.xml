<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CE updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CE</link>

<item>
<title>Physics-Aware Compression of Plasma Distribution Functions with GPU-Accelerated Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2504.14897</link>
<guid>https://arxiv.org/abs/2504.14897</guid>
<content:encoded><![CDATA[
<div> compression, plasma simulations, Gaussian Mixture Models, in-situ, real-time <br>
<br>
Summary: 
This article introduces a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) for large-scale plasma simulations. By approximating electron and ion velocity distribution functions with GMMs, the method captures plasma features like mean velocity and temperature, enabling identification of heating processes and beam generation. The approach involves constructing a histogram to reduce computational overhead and implementing GPU-accelerated, in-situ GMM fitting within the iPIC3D simulator for real-time compression. The compressed representation is stored using the ADIOS 2 library to optimize the I/O process. Compared to other algorithms like SZ, MGARD, and BLOSC2, the GMM-based method retains a physics-based approach, preserving the physical interpretation of plasma phenomena while achieving compression ratios of up to $10^4 and processing times comparable to standard compression engines. <div>
arXiv:2504.14897v1 Announce Type: new 
Abstract: Data compression is a critical technology for large-scale plasma simulations. Storing complete particle information requires Terabyte-scale data storage, and analysis requires ad-hoc scalable post-processing tools. We propose a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) to approximate electron and ion velocity distribution functions with a number of Gaussian components. This GMM-based method allows us to capture plasma features such as mean velocity and temperature, and it enables us to identify heating processes and generate beams. We first construct a histogram to reduce computational overhead and apply GPU-accelerated, in-situ GMM fitting within \texttt{iPIC3D}, a large-scale implicit Particle-in-Cell simulator, ensuring real-time compression. The compressed representation is stored using the \texttt{ADIOS 2} library, thus optimizing the I/O process. The GPU and histogramming implementation provides a significant speed-up with respect to GMM on particles (both in time and required memory at run-time), enabling real-time compression. Compared to algorithms like SZ, MGARD, and BLOSC2, our GMM-based method has a physics-based approach, retaining the physical interpretation of plasma phenomena such as beam formation, acceleration, and heating mechanisms. Our GMM algorithm achieves a compression ratio of up to $10^4$, requiring a processing time comparable to, or even lower than, standard compression engines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A parallel implementation of reduced-order modeling of large-scale systems</title>
<link>https://arxiv.org/abs/2504.14338</link>
<guid>https://arxiv.org/abs/2504.14338</guid>
<content:encoded><![CDATA[
<div> algorithm, distributed, reduced-order models, aerospace engineering, parallel

Summary:<br>
This paper introduces distributed Operator Inference (dOpInf), a parallel algorithm designed for constructing physics-based reduced-order models (ROMs) in large-scale aerospace engineering simulations. The algorithm efficiently processes high-dimensional datasets using distributed computing, enabling the learning of structured ROMs that approximate underlying dynamical systems. dOpInf is scalable, allowing for fully parallelized reduced modeling on thousands of processors. The resulting ROMs are computationally inexpensive, suitable for tasks such as design exploration, risk assessment, and uncertainty quantification. A tutorial using a 2D Navier-Stokes flow case study is provided to guide users through implementation, making dOpInf accessible for integration into complex aerospace simulations.<br> <div>
arXiv:2504.14338v1 Announce Type: cross 
Abstract: Motivated by the large-scale nature of modern aerospace engineering simulations, this paper presents a detailed description of distributed Operator Inference (dOpInf), a recently developed parallel algorithm designed to efficiently construct physics-based reduced-order models (ROMs) for problems with large state dimensions. One such example is the simulation of rotating detonation rocket engines, where snapshot data generated by high-fidelity large-eddy simulations have many millions of degrees of freedom. dOpInf enables, via distributed computing, the efficient processing of datasets with state dimensions that are too large to process on a single computer, and the learning of structured physics-based ROMs that approximate the dynamical systems underlying those datasets. All elements of dOpInf are scalable, leading to a fully parallelized reduced modeling approach that can scale to the thousands of processors available on leadership high-performance computing platforms. The resulting ROMs are computationally cheap, making them ideal for key engineering tasks such as design space exploration, risk assessment, and uncertainty quantification. To illustrate the practical application of dOpInf, we provide a step-by-step tutorial using a 2D Navier-Stokes flow over a step scenario as a case study. This tutorial guides users through the implementation process, making dOpInf accessible for integration into complex aerospace engineering simulations.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
<div> Dialogue framework, teaching capabilities, evaluation, language models, pedagogical effectiveness<br>
Summary:<br>
EducationQ is a multi-agent dialogue framework designed to assess the teaching capabilities of large language models (LLMs). Evaluating 14 LLMs from major AI organizations, the study found that teaching effectiveness does not solely rely on model scale or general reasoning abilities. Smaller open-source models sometimes outperformed larger commercial ones in educational contexts. The research highlighted a gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Through a mixed-methods approach combining quantitative metrics, qualitative analysis, and expert case studies, distinct pedagogical strengths of top-performing models were identified. Human expert evaluations supported the automated qualitative analysis of effective teaching behaviors. The study suggests that LLMs-as-teachers require specialized optimization beyond simple scaling, indicating a need for targeted enhancement of specific pedagogical effectiveness in future educational AI development.<br> 
Summary: <div>
arXiv:2504.14928v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues</title>
<link>https://arxiv.org/abs/2504.14963</link>
<guid>https://arxiv.org/abs/2504.14963</guid>
<content:encoded><![CDATA[
<div> acoustic features, speaker identification, textual data, fuzzy fingerprints, pre-trained models
Summary:
This study explores the use of fuzzy fingerprints from large pre-trained models to enhance text-based speaker identification. By incorporating speaker-specific tokens and context-aware modeling, the accuracy of speaker identification from text reaches up to 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. The approach of using fuzzy fingerprints enables approximation of full fine-tuning performance with fewer hidden units, enhancing interpretability. The analysis includes a mechanism to identify speaker-agnostic lines and handle ambiguous utterances. The study highlights the challenges in text-based speaker identification and offers insights for future improvements. 
<br><br>Summary: <div>
arXiv:2504.14963v1 Announce Type: cross 
Abstract: Speaker identification using voice recordings leverages unique acoustic features, but this approach fails when only textual data is available. Few approaches have attempted to tackle the problem of identifying speakers solely from text, and the existing ones have primarily relied on traditional methods. In this work, we explore the use of fuzzy fingerprints from large pre-trained models to improve text-based speaker identification. We integrate speaker-specific tokens and context-aware modeling, demonstrating that conversational context significantly boosts accuracy, reaching 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering improved interpretability. Finally, we analyze ambiguous utterances and propose a mechanism to detect speaker-agnostic lines. Our findings highlight key challenges and provide insights for future improvements in text-based speaker identification.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends</title>
<link>https://arxiv.org/abs/2405.08542</link>
<guid>https://arxiv.org/abs/2405.08542</guid>
<content:encoded><![CDATA[
<div> Metaverse, Industrial Production, XR, Blockchain, AI

Summary:
The article explores the potential of the Industrial Metaverse in enhancing industrial production through technologies such as XR, blockchain, AI, digital twin, and 6G. It discusses the advantages of using the Metaverse in industrial settings and the key enabling technologies for various aspects of production operations. Challenges such as security concerns, resource limitations, and interoperability issues are identified, along with existing solutions to address them. The article also outlines future research directions and open issues in the Industrial Metaverse as it continues to evolve in the industrial production field. <div>
arXiv:2405.08542v2 Announce Type: replace 
Abstract: As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks. This deployment, called the Industrial Metaverse, is used for product design, production operations, industrial quality inspection, and product testing. However, there lacks of in-depth understanding of the enabling technologies associated with the Industrial Metaverse. This encompasses both the precise industrial scenarios targeted by each technology and the potential migration of technologies developed in other domains to the industrial sector. Driven by this issue, in this article, we conduct a comprehensive survey of the state-of-the-art literature on the Industrial Metaverse. Specifically, we first analyze the advantages of the Metaverse for industrial production. Then, we review a collection of key enabling technologies of the Industrial Metaverse, including blockchain (BC), digital twin (DT), 6G, XR, and artificial intelligence (AI), and analyze how these technologies can support different aspects of industrial production. Subsequently, we present numerous formidable challenges encountered within the Industrial Metaverse, including confidentiality and security concerns, resource limitations, and interoperability constraints. Furthermore, we investigate the extant solutions devised to address them. Finally, we briefly outline several open issues and future research directions of the Industrial Metaverse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An optimization-based coupling of reduced order models with efficient reduced adjoint basis generation approach</title>
<link>https://arxiv.org/abs/2408.14450</link>
<guid>https://arxiv.org/abs/2408.14450</guid>
<content:encoded><![CDATA[
<div> Optimization-based coupling, Lagrange multiplier, multiple modeling, simulation, reduced order models<br>
<br>
Summary: Optimization-based coupling (OBC) offers a promising approach in various modeling and simulation scenarios but faces challenges in time-dependent problems due to computational costs. This paper introduces an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission issue, utilizing reduced order models to alleviate computational burdens. The "optimize-then-reduce" strategy is employed to solve the minimization problem at each time step efficiently. A key innovation is the development of a technique for effective adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. Numerical experiments validate the accuracy of the proposed approach while comparing different methods for selecting reduced order bases for adjoint systems. Criteria such as decay of snapshot energy, average iteration counts, and timings are assessed to demonstrate the effectiveness of the optimization-based ROM-ROM coupling. <div>
arXiv:2408.14450v2 Announce Type: replace 
Abstract: Optimization-based coupling (OBC) is an attractive alternative to traditional Lagrange multiplier approaches in multiple modeling and simulation contexts. However, application of OBC to time-dependent problems has been hindered by the computational cost of finding the stationary points of the associated Lagrangian, which requires primal and adjoint solves. This issue can be mitigated by using OBC in conjunction with computationally efficient reduced order models (ROM). To demonstrate the potential of this combination, in this paper we develop an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission problem. We pursue the ``optimize-then-reduce'' path towards solving the minimization problem at each timestep and solve reduced-space adjoint system of equations, where the main challenge in this formulation is the generation of adjoint snapshots and reduced bases for the adjoint systems required by the optimizer. One of the main contributions of the paper is a new technique for efficient adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. We present numerical studies demonstrating the accuracy of the approach along with comparison between various approaches for selecting a reduced order basis for the adjoint systems, including decay of snapshot energy, average iteration counts, and timings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>CardioFit: A WebGL-Based Tool for Fast and Efficient Parameterization of Cardiac Action Potential Models to Fit User-Provided Data</title>
<link>https://arxiv.org/abs/2504.13274</link>
<guid>https://arxiv.org/abs/2504.13274</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiac action potential, Particle swarm optimization, Interactive tool, Model parameter fitting, Physiological dynamics

Summary:
Interactive browser-based tool presented in this study utilizes particle swarm optimization (PSO) algorithm to find model parameter sets that accurately reproduce cardiac dynamics from user-provided data. The tool offers rapid customization and can achieve low-error fittings in a few iterations, requiring only a few seconds of runtime on typical machines. Users can select parameters to fit, define their value ranges, and adjust PSO algorithm hyperparameters through a user-friendly webpage interface. Various models were successfully fitted to different datasets, demonstrating the tool's versatility and efficiency. Convergence of fitting is influenced by the choice of model, dataset characteristics, and PSO algorithm settings. Through these fittings, new insights were gained regarding the physiological and dynamical implications of the model parameters.  <br /><br />Summary: <div>
arXiv:2504.13274v1 Announce Type: new 
Abstract: Cardiac action potential models allow examination of a variety of cardiac dynamics, including how behavior may change under specific interventions. To study a specific scenario, including patient-specific cases, model parameter sets must be found that accurately reproduce the dynamics of interest. To facilitate this complex and time-consuming process, we present an interactive browser-based tool that uses the particle swarm optimization (PSO) algorithm implemented in JavaScript and taking advantage of the WebGL API for hardware acceleration. Our tool allows rapid customization and can find low-error fittings to user-provided voltage time series or action potential duration data from multiple cycle lengths in a few iterations (10-32), corresponding to a runtime of a few seconds on most machines. Additionally, our tool focuses on ease of use and flexibility, providing a webpage interface that allows users to select a subset of parameters to fit, set the range of values each parameter is allowed to assume, and control the PSO algorithm hyperparameters. We demonstrate our tool's utility by fitting a variety of models to different datasets, showing how convergence is affected by model choice, dataset properties, and PSO algorithmic settings, and explaining new insights gained about the physiological and dynamical roles of the model parameters.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ascribe New Dimensions to Scientific Data Visualization with VR</title>
<link>https://arxiv.org/abs/2504.13448</link>
<guid>https://arxiv.org/abs/2504.13448</guid>
<content:encoded><![CDATA[
<div> Keywords: computer mouse, scientific images, Virtual Reality, AI-driven algorithms, multimodal analysis<br />
<br />
Summary: <br />
The article discusses the limitations of the traditional 2D visualization methods in exploring complex, multi-scale scientific images using a computer mouse. It introduces ASCRIBE-VR, a Virtual Reality platform that integrates AI-driven algorithms with scientific images. ASCRIBE-VR allows for immersive and interactive visualization, supporting the analysis of advanced datasets such as X-ray CT and Magnetic Resonance. The VR tools are compatible with Meta Quest and can seamlessly explore large-scale 3D images by merging AI-generated results with VR visualization. This integration enhances scientific discovery by bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins. <div>
arXiv:2504.13448v1 Announce Type: cross 
Abstract: For over half a century, the computer mouse has been the primary tool for interacting with digital data, yet it remains a limiting factor in exploring complex, multi-scale scientific images. Traditional 2D visualization methods hinder intuitive analysis of inherently 3D structures. Virtual Reality (VR) offers a transformative alternative, providing immersive, interactive environments that enhance data comprehension. This article introduces ASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research with Immersive Browsing \& Exploration, which integrates AI-driven algorithms with scientific images. ASCRIBE-VR enables multimodal analysis, structural assessments, and immersive visualization, supporting scientific visualization of advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D imaging. Our VR tools, compatible with Meta Quest, can consume the output of our AI-based segmentation and iterative feedback processes to enable seamless exploration of large-scale 3D images. By merging AI-generated results with VR visualization, ASCRIBE-VR enhances scientific discovery, bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Models Meet Financial Data Modalities</title>
<link>https://arxiv.org/abs/2504.13521</link>
<guid>https://arxiv.org/abs/2504.13521</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithmic trading, deep learning, limit order book, high-frequency trading, predictive performance

Summary: 
This study explores the integration of deep learning models with various financial data sources to improve algorithmic trading strategies and portfolio optimization. By developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation, the researchers achieved state-of-the-art performance in high-frequency trading algorithms. This approach underscores the effectiveness of deep learning in handling structured financial data and enhancing predictive performance in trading strategies. The study highlights the potential of incorporating limit order book analysis into algorithmic trading using deep learning models. By leveraging deep learning techniques, the researchers were able to extract meaningful signals from diverse financial data sources, including candlestick charts, order statistics, traded volume data, and news flow. This research contributes to bridging the gap between deep learning and structured financial data analysis, showcasing the promise of deep learning in financial applications. 

Summary: <div>
arXiv:2504.13521v1 Announce Type: cross 
Abstract: Algorithmic trading relies on extracting meaningful signals from diverse financial data sources, including candlestick charts, order statistics on put and canceled orders, traded volume data, limit order books, and news flow. While deep learning has demonstrated remarkable success in processing unstructured data and has significantly advanced natural language processing, its application to structured financial data remains an ongoing challenge. This study investigates the integration of deep learning models with financial data modalities, aiming to enhance predictive performance in trading strategies and portfolio optimization. We present a novel approach to incorporating limit order book analysis into algorithmic trading by developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation. Our methodology for processing limit order book data achieves state-of-the-art performance in high-frequency trading algorithms, underscoring the effectiveness of deep learning in financial applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bitcoin's Edge: Embedded Sentiment in Blockchain Transactional Data</title>
<link>https://arxiv.org/abs/2504.13598</link>
<guid>https://arxiv.org/abs/2504.13598</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency, Blockchain, Natural Language Processing, Sentiment Analysis, Financial Predictions

Summary:
Blockchain technology is not only used for financial transactions but also for storing and sharing non-financial content. This hidden content can impact cryptocurrency price movements by conveying private information and shaping public sentiment. Current methods of analyzing blockchain data are limited, prompting the use of Natural Language Processing techniques to extract sentiment from transactional data. The study demonstrates the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency prices on Bitcoin and Ethereum blockchains. It uncovers an informational advantage for Bitcoin over Ethereum, showing that sentiment analysis can effectively predict price movements. This research highlights the value of blockchain sentiment analysis in enhancing financial predictions within cryptocurrency markets, providing a novel framework for leveraging freely available, transparent, and immutable data for informed decision-making in the digital asset space. 

<br /><br />Summary: <div>
arXiv:2504.13598v1 Announce Type: cross 
Abstract: Cryptocurrency blockchains, beyond their primary role as distributed payment systems, are increasingly used to store and share arbitrary content, such as text messages and files. Although often non-financial, this hidden content can impact price movements by conveying private information, shaping sentiment, and influencing public opinion. However, current analyses of such data are limited in scope and scalability, primarily relying on manual classification or hand-crafted heuristics. In this work, we address these limitations by employing Natural Language Processing techniques to analyze, detect patterns, and extract public sentiment encoded within blockchain transactional data. Using a variety of Machine Learning techniques, we showcase for the first time the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency price movements on the Bitcoin and Ethereum blockchains. Our findings shed light on a previously underexplored source of freely available, transparent, and immutable data and introduce blockchain sentiment analysis as a novel and robust framework for enhancing financial predictions in cryptocurrency markets. Incidentally, we discover an asymmetry between cryptocurrencies; Bitcoin has an informational advantage over Ethereum in that the sentiment embedded into transactional data is sufficient to predict its price movement.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems</title>
<link>https://arxiv.org/abs/2504.13768</link>
<guid>https://arxiv.org/abs/2504.13768</guid>
<content:encoded><![CDATA[
<div> Graph neural network, multi-body dynamical systems, internal loads, predictive maintenance, digital twin <br />
Summary: Equi-Euler GraphNet is proposed for accurate real-time modeling of multi-body dynamical systems. It simultaneously predicts internal forces and global trajectories, crucial for fault detection and predictive maintenance in digital twin applications. It introduces equivariant message-passing and temporal-aware iterative node update mechanisms, tailored for cylindrical roller bearings. Trained on high-fidelity simulations, it generalizes well and outperforms state-of-the-art models in trajectory prediction. Equi-Euler GraphNet achieves efficient and stable rollouts over thousands of time steps with minimal error accumulation, providing a 200x speedup over conventional solvers. This makes it a valuable tool for digital twins, design, and maintenance applications. <br /> <div>
arXiv:2504.13768v1 Announce Type: cross 
Abstract: Accurate real-time modeling of multi-body dynamical systems is essential for enabling digital twin applications across industries. While many data-driven approaches aim to learn system dynamics, jointly predicting internal loads and system trajectories remains a key challenge. This dual prediction is especially important for fault detection and predictive maintenance, where internal loads-such as contact forces-act as early indicators of faults, reflecting wear or misalignment before affecting motion. These forces also serve as inputs to degradation models (e.g., crack growth), enabling damage prediction and remaining useful life estimation. We propose Equi-Euler GraphNet, a physics-informed graph neural network (GNN) that simultaneously predicts internal forces and global trajectories in multi-body systems. In this mesh-free framework, nodes represent system components and edges encode interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an equivariant message-passing scheme, interpreting edge messages as interaction forces consistent under Euclidean transformations; and (2) a temporal-aware iterative node update mechanism, based on Euler integration, to capture influence of distant interactions over time. Tailored for cylindrical roller bearings, it decouples ring dynamics from constrained motion of rolling elements. Trained on high-fidelity multiphysics simulations, Equi-Euler GraphNet generalizes beyond the training distribution, accurately predicting loads and trajectories under unseen speeds, loads, and configurations. It outperforms state-of-the-art GNNs focused on trajectory prediction, delivering stable rollouts over thousands of time steps with minimal error accumulation. Achieving up to a 200x speedup over conventional solvers while maintaining comparable accuracy, it serves as an efficient reduced-order model for digital twins, design, and maintenance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer Encoder and Multi-features Time2Vec for Financial Prediction</title>
<link>https://arxiv.org/abs/2504.13801</link>
<guid>https://arxiv.org/abs/2504.13801</guid>
<content:encoded><![CDATA[
<div> Transformers, financial prediction, time series analysis, attention mechanism, correlation feature selection<br />
<br />
Summary:<br />
This paper introduces a novel neural network architecture that combines Time2Vec with the Encoder of the Transformer model for financial prediction. By studying different markets, a correlation feature selection method is proposed to improve the accuracy of predicting multiple stock prices. Through fine-tuning hyperparameters, the method outperforms benchmark models and traditional encoding methods like positional encoding. The integration of Time2Vec with the Transformer model allows for the capture of both short and long-range dependencies, aiding in understanding broader market trends. Selecting correlation features further enhances prediction accuracy, particularly in industries where companies exhibit correlated stock price movements. <div>
arXiv:2504.13801v1 Announce Type: cross 
Abstract: Financial prediction is a complex and challenging task of time series analysis and signal processing, expected to model both short-term fluctuations and long-term temporal dependencies. Transformers have remarkable success mostly in natural language processing using attention mechanism, which also influenced the time series community. The ability to capture both short and long-range dependencies helps to understand the financial market and to recognize price patterns, leading to successful applications of Transformers in stock prediction. Although, the previous research predominantly focuses on individual features and singular predictions, that limits the model's ability to understand broader market trends. In reality, within sectors such as finance and technology, companies belonging to the same industry often exhibit correlated stock price movements.
  In this paper, we develop a novel neural network architecture by integrating Time2Vec with the Encoder of the Transformer model. Based on the study of different markets, we propose a novel correlation feature selection method. Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a comparative analysis of our results against benchmark models. We conclude that our method outperforms other state-of-the-art encoding methods such as positional encoding, and we also conclude that selecting correlation features enhance the accuracy of predicting multiple stock prices.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding</title>
<link>https://arxiv.org/abs/2408.11363</link>
<guid>https://arxiv.org/abs/2408.11363</guid>
<content:encoded><![CDATA[
<div> Keywords: ProteinGPT, large language model, protein sequences, protein structures, protein analysis <br />
Summary: ProteinGPT is a cutting-edge multimodal large language model designed for analyzing protein sequences and structures efficiently. By integrating protein sequence and structure encoders with a large language model, ProteinGPT can provide accurate and contextually relevant responses to protein-related queries. The model was trained on a dataset of 132,092 proteins, each annotated with property tags and QA pairs, using GPT-4o for instruction tuning. Experimental results show that ProteinGPT outperforms baseline models and general-purpose LLMs in understanding and responding to protein-related questions, achieving high performance on both semantic and lexical metrics. The code and data for ProteinGPT are available on GitHub, making this powerful tool accessible to researchers in the field. <br /><br /> <div>
arXiv:2408.11363v2 Announce Type: replace-cross 
Abstract: Understanding biological processes, drug development, and biotechnological advancements requires a detailed analysis of protein structures and functions, a task that is inherently complex and time-consuming in traditional protein research. To streamline this process, we introduce ProteinGPT, a state-of-the-art multimodal large language model for proteins that enables users to upload protein sequences and/or structures for comprehensive analysis and responsive inquiries. ProteinGPT integrates protein sequence and structure encoders with linear projection layers to ensure precise representation adaptation and leverages a large language model (LLM) to generate accurate, contextually relevant responses. To train ProteinGPT, we constructed a large-scale dataset of 132,092 proteins, each annotated with 20-30 property tags and 5-10 QA pairs per protein, and optimized the instruction-tuning process using GPT-4o. Experiments demonstrate that ProteinGPT effectively generates informative responses to protein-related questions, achieving high performance on both semantic and lexical metrics and significantly outperforming baseline models and general-purpose LLMs in understanding and responding to protein-related queries. Our code and data are available at https://github.com/ProteinGPT/ProteinGPT.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Accurate Prediction of Antenna Reflection Coefficients in Planar Layered Media Environment via Generalized Scattering Matrix</title>
<link>https://arxiv.org/abs/2504.12613</link>
<guid>https://arxiv.org/abs/2504.12613</guid>
<content:encoded><![CDATA[
<div> Keywords: reflection coefficient, antenna, planar layered medium, generalized scattering matrix, numerical algorithm

Summary: 
The article presents a new numerical algorithm for evaluating the reflection coefficient of an antenna in the presence of a planar layered medium. This algorithm utilizes the antenna's generalized scattering matrix (GSM) to model the interaction between the antenna and the medium through spherical-to-planar vector wave transformations. By avoiding approximations that could compromise accuracy, the algorithm reduces algebraic complexity and significantly speeds up antenna performance evaluation. While a one-time preprocessing cost is required to obtain the antenna's GSM in free space, the numerical evaluation speed of this method surpasses that of commercial software FEKO by several orders of magnitude, maintaining high accuracy. This advancement in computational efficiency offers a promising approach for accurately evaluating antenna performance in complex scenarios involving layered media. 

<br /><br />Summary: <div>
arXiv:2504.12613v1 Announce Type: new 
Abstract: The numerical algorithm for evaluating the reflection coefficient of an antenna in the presence of the planar layered medium is reformulated using the antenna's generalized scattering matrix (GSM). The interaction between the antenna and the layered medium is modeled through spherical-to-planar vector wave transformations, ensuring no approximations that could compromise computational accuracy. This theoretical framework significantly reduces algebraic complexity, resulting in a marked increase in the speed of antenna performance evaluation. Excluding the one-time preprocessing cost of obtaining the antenna's GSM in free space, the numerical evaluation speed of this method exceeds that of the commercial software FEKO by several orders of magnitude, while maintaining nearly identical accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study</title>
<link>https://arxiv.org/abs/2504.12503</link>
<guid>https://arxiv.org/abs/2504.12503</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, engineering design, continual learning, regression tasks, catastrophic forgetting

Summary:
Continual learning (CL) is introduced to engineering design to address the challenge of incorporating new knowledge into models without retraining from scratch, which is computationally expensive. This study benchmarks various CL methods on regression tasks using five engineering datasets and establishes nine new engineering CL benchmarks. The results show that existing CL methods can improve performance compared to naive baselines, with the Replay strategy achieving comparable performance to retraining while reducing training time significantly. This points to the potential of CL in enhancing real-world engineering workflows. The code and datasets used in the study will be made available for further research and applications. 

<br /><br />Summary: <div>
arXiv:2504.12503v1 Announce Type: cross 
Abstract: Engineering problems that apply machine learning often involve computationally intensive methods but rely on limited datasets. As engineering data evolves with new designs and constraints, models must incorporate new knowledge over time. However, high computational costs make retraining models from scratch infeasible. Continual learning (CL) offers a promising solution by enabling models to learn from sequential data while mitigating catastrophic forgetting, where a model forgets previously learned mappings. This work introduces CL to engineering design by benchmarking several CL methods on representative regression tasks. We apply these strategies to five engineering datasets and construct nine new engineering CL benchmarks to evaluate their ability to address forgetting and improve generalization. Preliminary results show that applying existing CL methods to these tasks improves performance over naive baselines. In particular, the Replay strategy achieved performance comparable to retraining in several benchmarks while reducing training time by nearly half, demonstrating its potential for real-world engineering workflows. The code and datasets used in this work will be available at: https://github.com/kmsamuel/cl-for-engineering-release.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational quantum and neural quantum states algorithms for the linear complementarity problem</title>
<link>https://arxiv.org/abs/2504.08141</link>
<guid>https://arxiv.org/abs/2504.08141</guid>
<content:encoded><![CDATA[
<div> variational quantum algorithms, VQAs, hybrid quantum-classical methods, variational quantum linear solver, VQLS

Summary:
Variational quantum algorithms (VQAs) are being explored as a way to harness quantum computing while addressing current hardware limitations. This study introduces the variational quantum linear solver (VQLS) and its classical counterpart, the variational neural linear solver (VNLS), within a minimum map Newton solver for a rigid body contact model. The VNLS accurately simulates the dynamics of rigid spherical bodies during collisions, suggesting that quantum and quantum-inspired linear algebra algorithms can be effective for modeling certain physical systems. This research opens up the possibility of using VQAs and quantum-inspired classical algorithms to solve real-world problems, highlighting their potential utility in various applications. <br /><br />Summary: <div>
arXiv:2504.08141v2 Announce Type: replace 
Abstract: Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Editing for Offline Model-based Optimization</title>
<link>https://arxiv.org/abs/2405.13964</link>
<guid>https://arxiv.org/abs/2405.13964</guid>
<content:encoded><![CDATA[
<div> Offline model-based optimization, surrogate model, out-of-distribution issue, diffusion prior, Design Editing for Offline Model-based Optimization (DEMO)<br />
Summary:<br />
Offline model-based optimization (MBO) involves maximizing a black-box objective function using a dataset of designs and scores. Traditional methods use surrogate models but are prone to errors when predicting scores for new designs. To address this, DEMO introduces a diffusion prior to calibrate overly optimized designs. It generates pseudo design candidates using gradient ascent, then refines them through an editing process involving noise and denoising with the diffusion prior. Empirical evaluations across seven tasks show that DEMO, with proper tuning, achieves competitive scores compared to previous literature. <div>
arXiv:2405.13964v4 Announce Type: replace-cross 
Abstract: Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. While these pseudo design candidates contain information beyond the offline dataset, they might be invalid or have erroneously high predicted scores. Therefore, to address this challenge while utilizing the information provided by pseudo design candidates, we propose an editing process to refine these pseudo design candidates. We introduce noise to the pseudo design candidates and subsequently denoise them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. Empirical evaluations on seven offline MBO tasks show that, with properly tuned hyperparameters, DEMOs score is competitive with the best previously reported scores in the literature.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Contextual Market Equilibrium Computation through Deep Learning</title>
<link>https://arxiv.org/abs/2406.15459</link>
<guid>https://arxiv.org/abs/2406.15459</guid>
<content:encoded><![CDATA[
<div> Market equilibrium, computational economics, large-scale buyer population, contextual market model, deep learning-based method<br />
<br />
Summary: 
The paper explores the computation of market equilibrium in scenarios with a large-scale buyer population using a deep learning-based method called MarketFCNet. The approach introduces a contextual market model where buyers and goods are represented by their contexts. MarketFCNet uses a neural network to parameterize the allocation of goods to buyers based on their contexts. An efficient method is proposed to estimate the loss function for training the network, enabling optimization through gradient descent. The Nash Gap metric is introduced to measure the deviation of the allocation and prices from the market equilibrium. Experimental results show that MarketFCNet outperforms existing methods in terms of performance and running times as the market scale increases, showcasing the potential of deep learning in approximating large-scale contextual market equilibrium. <br /><br /> <div>
arXiv:2406.15459v2 Announce Type: replace-cross 
Abstract: Market equilibrium is one of the most fundamental solution concepts in economics and social optimization analysis. Existing works on market equilibrium computation primarily focus on settings with relatively few buyers. Motivated by this, our paper investigates the computation of market equilibrium in scenarios with a large-scale buyer population, where buyers and goods are represented by their contexts. Building on this realistic and generalized contextual market model, we introduce MarketFCNet, a deep learning-based method for approximating market equilibrium. We start by parameterizing the allocation of each good to each buyer using a neural network, which depends solely on the context of the buyer and the good. Next, we propose an efficient method to unbiasedly estimate the loss function of the training algorithm, enabling us to optimize the network parameters through gradient. To evaluate the approximated solution, we propose a metric called Nash Gap, which quantifies the deviation of the given allocation and price pair from the market equilibrium. Experimental results indicate that MarketFCNet delivers competitive performance and significantly lower running times compared to existing methods as the market scale expands, demonstrating the potential of deep learning-based methods to accelerate the approximation of large-scale contextual market equilibrium.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrivAerML: High-Fidelity Computational Fluid Dynamics Dataset for Road-Car External Aerodynamics</title>
<link>https://arxiv.org/abs/2408.11969</link>
<guid>https://arxiv.org/abs/2408.11969</guid>
<content:encoded><![CDATA[
<div> Machine Learning, automotive aerodynamics, open-source dataset, high-fidelity CFD, DrivAer notchback<br />
Summary:<br />
Machine Learning has the potential to transform automotive aerodynamics by providing quick flow predictions during the design phase. However, a lack of open-source training data for realistic road cars using high-fidelity CFD methods is impeding progress. To overcome this hurdle, a high-fidelity open-source dataset for automotive aerodynamics has been created. The dataset comprises 500 variants of the DrivAer notchback vehicle, generated through parametric morphing. Mesh generation and scale-resolving CFD were conducted using state-of-the-art automated workflows. The dataset, published under the CC-BY-SA license, includes geometries and detailed aerodynamic information. This initiative marks the first large public-domain dataset for complex automotive configurations derived from high-fidelity CFD simulations. <div>
arXiv:2408.11969v2 Announce Type: replace-cross 
Abstract: Machine Learning (ML) has the potential to revolutionise the field of automotive aerodynamics, enabling split-second flow predictions early in the design process. However, the lack of open-source training data for realistic road cars, using high-fidelity CFD methods, represents a barrier to their development. To address this, a high-fidelity open-source (CC-BY-SA) public dataset for automotive aerodynamics has been generated, based on 500 parametrically morphed variants of the widely-used DrivAer notchback generic vehicle. Mesh generation and scale-resolving CFD was executed using consistent and validated automatic workflows representative of the industrial state-of-the-art. Geometries and rich aerodynamic data are published in open-source formats. To our knowledge, this is the first large, public-domain dataset for complex automotive configurations generated using high-fidelity CFD.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Algorithms for Option Hedging</title>
<link>https://arxiv.org/abs/2504.05521</link>
<guid>https://arxiv.org/abs/2504.05521</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, dynamic hedging, financial assets, risk, DRL algorithms<br />
<br />
Summary: 
Dynamic hedging involves offsetting financial risk through periodic transactions. This study compares the performance of eight DRL algorithms in dynamic hedging scenarios. The algorithms include Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), various Deep Q-Learning (DQL) and Deep Deterministic Policy Gradient (DDPG) variants. The experiments use a GJR-GARCH(1,1) model to simulate the dataset and evaluate against the Black-Scholes delta hedge baseline. Results show that MCPG and PPO perform best in terms of the root semi-quadratic penalty. MCPG outperforms the baseline within the allotted computational budget due to sparse rewards in the environment. This comparison provides insights into the effectiveness of different DRL algorithms in dynamic hedging strategies, with potential implications for financial risk management. 
<br /><br />Summary: <div>
arXiv:2504.05521v2 Announce Type: replace-cross 
Abstract: Dynamic hedging is a financial strategy that consists in periodically transacting one or multiple financial assets to offset the risk associated with a correlated liability. Deep Reinforcement Learning (DRL) algorithms have been used to find optimal solutions to dynamic hedging problems by framing them as sequential decision-making problems. However, most previous work assesses the performance of only one or two DRL algorithms, making an objective comparison across algorithms difficult. In this paper, we compare the performance of eight DRL algorithms in the context of dynamic hedging; Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), along with four variants of Deep Q-Learning (DQL) and two variants of Deep Deterministic Policy Gradient (DDPG). Two of these variants represent a novel application to the task of dynamic hedging. In our experiments, we use the Black-Scholes delta hedge as a baseline and simulate the dataset using a GJR-GARCH(1,1) model. Results show that MCPG, followed by PPO, obtain the best performance in terms of the root semi-quadratic penalty. Moreover, MCPG is the only algorithm to outperform the Black-Scholes delta hedge baseline with the allotted computational budget, possibly due to the sparsity of rewards in our environment.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal integration of chemical structures improves representations of microscopy images for morphological profiling</title>
<link>https://arxiv.org/abs/2504.09544</link>
<guid>https://arxiv.org/abs/2504.09544</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, deep learning, morphological profiling, high-throughput microscopy screens, chemical compound structure

Summary:
Recent advances in self-supervised deep learning have led to improved quantification of cellular morphological changes in high-throughput microscopy screens. The new framework, MICON (Molecular-Image Contrastive Learning), incorporates chemical compound information during pre-training to enhance learned image representations. MICON surpasses traditional feature extraction methods like CellProfiler and existing deep learning approaches, especially in identifying consistent effects of drugs across independent replicates and data centers. By modeling chemical compounds as treatments inducing counterfactual transformations of cell phenotypes, MICON outperforms methods that directly align images and compounds. This highlights the importance of considering the multimodal nature of microscopy screening data in representation learning for morphological profiling. MICON's success suggests a promising direction for future research in this field.<br /><br />Summary: <div>
arXiv:2504.09544v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised deep learning have improved our ability to quantify cellular morphological changes in high-throughput microscopy screens, a process known as morphological profiling. However, most current methods only learn from images, despite many screens being inherently multimodal, as they involve both a chemical or genetic perturbation as well as an image-based readout. We hypothesized that incorporating chemical compound structure during self-supervised pre-training could improve learned representations of images in high-throughput microscopy screens. We introduce a representation learning framework, MICON (Molecular-Image Contrastive Learning), that models chemical compounds as treatments that induce counterfactual transformations of cell phenotypes. MICON significantly outperforms classical hand-crafted features such as CellProfiler and existing deep-learning-based representation learning methods in challenging evaluation settings where models must identify reproducible effects of drugs across independent replicates and data-generating centers. We demonstrate that incorporating chemical compound information into the learning process provides consistent improvements in our evaluation setting and that modeling compounds specifically as treatments in a causal framework outperforms approaches that directly align images and compounds in a single representation space. Our findings point to a new direction for representation learning in morphological profiling, suggesting that methods should explicitly account for the multimodal nature of microscopy screening data.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A viscoplasticity model with an invariant-based non-Newtonian flow rule for unidirectional thermoplastic composites</title>
<link>https://arxiv.org/abs/2504.12069</link>
<guid>https://arxiv.org/abs/2504.12069</guid>
<content:encoded><![CDATA[
arXiv:2504.12069v1 Announce Type: new 
Abstract: A three-dimensional mesoscopic viscoplasticity model for simulating rate-dependent plasticity and creep in unidirectional thermoplastic composites is presented. The constitutive model is a transversely isotropic extension of an isotropic finite strain viscoplasticity model for neat polymers. Rate-dependent plasticity and creep are described by a non-Newtonian flow rule where the viscosity of the material depends on an equivalent stress measure through an Eyring-type relation. In the present formulation, transverse isotropy is incorporated by defining the equivalent stress measure and flow rule as functions of transversely isotropic stress invariants. In addition, the Eyring-type viscosity function is extended with anisotropic pressure dependence. As a result of the formulation, plastic flow in fiber direction is effectively excluded and pressure dependence of the polymer matrix is accounted for. The re-orientation of the transversely isotropic plane during plastic deformations is incorporated in the constitutive equations, allowing for an accurate large deformation response. The formulation is fully implicit and a consistent linearization of the algorithmic constitutive equations is performed to derive the consistent tangent modulus. The performance of the mesoscopic constitutive model is assessed through a comparison with a micromechanical model for carbon/PEEK, with the original isotropic viscoplastic version for the polymer matrix and with hyperelastic fibers. The micromodel is first used to determine the material parameters of the mesoscale model with a few stress-strain curves. It is demonstrated that the mesoscale model gives a similar response to the micromodel under various loading conditions. Finally, the mesoscale model is validated against off-axis experiments on unidirectional thermoplastic composite plies.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Material Network: Overview, applications and current directions</title>
<link>https://arxiv.org/abs/2504.12159</link>
<guid>https://arxiv.org/abs/2504.12159</guid>
<content:encoded><![CDATA[
arXiv:2504.12159v1 Announce Type: new 
Abstract: Deep Material Network (DMN) has emerged as a powerful framework for multiscale material modeling, enabling efficient and accurate predictions of material behavior across different length scales. Unlike traditional machine learning approaches, the trainable parameters in DMN have direct physical interpretations, capturing the geometric characteristics of the microstructure rather than serving as purely statistical fitting parameters. Its hierarchical tree structure effectively encodes microstructural interactions and deformation mechanisms, allowing DMN to achieve a balance between accuracy and computational efficiency. This physics-informed architecture significantly reduces computational costs compared to direct numerical simulations while preserving essential microstructural physics. Furthermore, DMN can be trained solely on a linear elastic dataset while effectively extrapolating nonlinear responses during online prediction, making it a highly efficient and scalable approach for multiscale material modeling. This article provides a comprehensive review of DMN, detailing its motivation, underlying methodology, and recent advancements. We discuss key modeling aspects, including its hierarchical structure, training process, and the role of physics-based constraints in enhancing predictive accuracy. Furthermore, we highlight its applications in component-scale multiscale analysis and inverse parameter identification, demonstrating its capability to bridge microscale material behavior with macroscale engineering predictions. Finally, we discuss challenges and future directions in improving DMN's generalization capabilities and its potential extensions for broader applications in multiscale modeling.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Analysis of Mixer Activities in the Bitcoin Network</title>
<link>https://arxiv.org/abs/2504.11924</link>
<guid>https://arxiv.org/abs/2504.11924</guid>
<content:encoded><![CDATA[
arXiv:2504.11924v1 Announce Type: cross 
Abstract: Cryptocurrency users increasingly rely on obfuscation techniques such as mixers, swappers, and decentralised or no-KYC exchanges to protect their anonymity. However, at the same time, these services are exploited by criminals to conceal and launder illicit funds. Among obfuscation services, mixers remain one of the most challenging entities to tackle. This is because their owners are often unwilling to cooperate with Law Enforcement Agencies, and technically, they operate as 'black boxes'. To better understand their functionalities, this paper proposes an approach to analyse the operations of mixers by examining their address-transaction graphs and identifying topological similarities to uncover common patterns that can define the mixer's modus operandi. The approach utilises community detection algorithms to extract dense topological structures and clustering algorithms to group similar communities. The analysis is further enriched by incorporating data from external sources related to known Exchanges, in order to understand their role in mixer operations. The approach is applied to dissect the Blender.io mixer activities within the Bitcoin blockchain, revealing: i) consistent structural patterns across address-transaction graphs; ii) that Exchanges play a key role, following a well-established pattern, which raises several concerns about their AML/KYC policies. This paper represents an initial step toward dissecting and understanding the complex nature of mixer operations in cryptocurrency networks and extracting their modus operandi.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection</title>
<link>https://arxiv.org/abs/2502.07384</link>
<guid>https://arxiv.org/abs/2502.07384</guid>
<content:encoded><![CDATA[
arXiv:2502.07384v2 Announce Type: replace 
Abstract: Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. At the inter-modality level, SAGEPhos introduces a Bio-Coupled Modal Fusion method, distilling essential kinase sequence information to refine task-oriented local substrate feature space, creating a shared semantic space that captures crucial kinase-substrate interaction patterns. Within the substrate's intra-modality domain, it focuses on Bio-Augmented Fusion, emphasizing 2D local sequence information while selectively incorporating 3D spatial information from predicted structures to complement the sequence space. Moreover, to address the lack of structural information in current datasets, we contribute a new, refined phosphorylation site prediction dataset, which incorporates crucial structural elements and will serve as a new benchmark for the field. Experimental results demonstrate that SAGEPhos significantly outperforms baseline methods. We release the SAGEPhos models and code at https://github.com/ZhangJJ26/SAGEPhos.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QPET: A Versatile and Portable Quantity-of-Interest-Preservation Framework for Error-Bounded Lossy Compression</title>
<link>https://arxiv.org/abs/2412.02799</link>
<guid>https://arxiv.org/abs/2412.02799</guid>
<content:encoded><![CDATA[
arXiv:2412.02799v3 Announce Type: replace-cross 
Abstract: Error-bounded lossy compression has been widely adopted in many scientific domains because it can address the challenges in storing, transferring, and analyzing unprecedented amounts of scientific data. Although error-bounded lossy compression offers general data distortion control by enforcing strict error bounds on raw data, it may fail to meet the quality requirements on the results of downstream analysis, a.k.a. Quantities of Interest (QoIs), derived from raw data. This may lead to uncertainties and even misinterpretations in scientific discoveries, significantly limiting the use of lossy compression in practice. In this paper, we propose QPET, a novel, versatile, and portable framework for QoI-preserving error-bounded lossy compression, which overcomes the challenges of modeling diverse QoIs by leveraging numerical strategies. QPET features (1) high portability to multiple existing lossy compressors, (2) versatile preservation to most differentiable univariate and multivariate QoIs, and (3) significant compression improvements in QoI-preservation tasks. Experiments with six real-world datasets demonstrate that integrating QPET into state-of-the-art error-bounded lossy compressors can gain 2x to 10x compression speedups of existing QoI-preserving error-bounded lossy compression solutions, up to 1000% compression ratio improvements to general-purpose compressors, and up to 133% compression ratio improvements to existing QoI-integrated scientific compressors.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>