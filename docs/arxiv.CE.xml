<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CE updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CE</link>


<item>
<title>4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression</title>
<link>https://arxiv.org/abs/2504.18925</link>
<guid>https://arxiv.org/abs/2504.18925</guid>
<content:encoded><![CDATA[
<div> Neural Voxel Contextual Coding, Vector Quantization Contextual Coding, 4DGS data compression, multi-rate compression, storage reduction<br />
Summary:<br />
This study introduces 4DGS-CC, a contextual coding framework for compressing 4D Gaussian Splatting (4DGS) data to address storage challenges. The framework decomposes the 4DGS data into 4D neural voxels and 3DGS components for efficient compression. It leverages Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC) to compress the data, achieving a storage reduction of approximately 12 times while maintaining rendering fidelity. The approach separates temporal and spatial dimensions in the data decomposition process and utilizes prior information for contextual coding using NVCC. Additionally, a codebook is employed to store spherical harmonics information from canonical 3DGS, which is compressed using VQCC with auxiliary hyperpriors. The integrated NVCC and VQCC enable tailored multi-rate compression of 4DGS data, making it suitable for specific storage requirements. Extensive experiments validate the effectiveness of the proposed method in achieving significant storage savings without compromising data quality. <br /><br /> <div>
arXiv:2504.18925v1 Announce Type: new 
Abstract: Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints.Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively.Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding.Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook.By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error</title>
<link>https://arxiv.org/abs/2504.19963</link>
<guid>https://arxiv.org/abs/2504.19963</guid>
<content:encoded><![CDATA[
<div> probabilistic model, subspaces, principal component analysis, model reduction, computational mechanics
<br />
Summary:<br />
This paper introduces a probabilistic model of subspaces based on probabilistic principal component analysis (PCA). The method uses quantities derived from probabilistic PCA to construct distributions of the sample matrix and the principal subspaces in an embedding space. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition. The constructed stochastic subspace can help characterize model-form uncertainty in computational mechanics. The method is justified by probabilistic PCA, satisfying linear constraints like boundary conditions, and has only one hyperparameter, simplifying training. The algorithm is easy to implement. Comparisons with existing approaches show promising results in low-dimensional visualization, parametric static problems, and dynamics modeling of space structures. <div>
arXiv:2504.19963v1 Announce Type: new 
Abstract: This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We compare the proposed method with existing approaches in a low-dimensional visualization example and a parametric static problem, and demonstrate its performance in a dynamics model of a space structure.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuantBench: Benchmarking AI Methods for Quantitative Investment</title>
<link>https://arxiv.org/abs/2504.18600</link>
<guid>https://arxiv.org/abs/2504.18600</guid>
<content:encoded><![CDATA[
<div> benchmark, artificial intelligence, quantitative investment, industry practices, QuantBench  
Summary:  
QuantBench is introduced as a benchmark platform for AI in quantitative investment, aiming to bridge the gap between academic research and industry practices. It offers standardization aligned with industry standards, flexibility for integrating AI algorithms, and full coverage of the quantitative investment process. Using QuantBench, empirical studies highlight the importance of continual learning to address distribution shifts, improved methods for modeling relational financial data, and robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common platform for evaluation, QuantBench fosters collaboration between researchers and practitioners, accelerating progress in the field of AI for quantitative investment, similar to the impact of benchmark platforms in other domains such as computer vision and natural language processing. <div>
arXiv:2504.18600v1 Announce Type: cross 
Abstract: The field of artificial intelligence (AI) in quantitative investment has seen significant advancements, yet it lacks a standardized benchmark aligned with industry practices. This gap hinders research progress and limits the practical application of academic innovations. We present QuantBench, an industrial-grade benchmark platform designed to address this critical need. QuantBench offers three key strengths: (1) standardization that aligns with quantitative investment industry practices, (2) flexibility to integrate various AI algorithms, and (3) full-pipeline coverage of the entire quantitative investment process. Our empirical studies using QuantBench reveal some critical research directions, including the need for continual learning to address distribution shifts, improved methods for modeling relational financial data, and more robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common ground for evaluation and fostering collaboration between researchers and practitioners, QuantBench aims to accelerate progress in AI for quantitative investment, similar to the impact of benchmark platforms in computer vision and natural language processing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scientific Open-Source Software Is Less Likely to Become Abandoned Than One Might Think! Lessons from Curating a Catalog of Maintained Scientific Software</title>
<link>https://arxiv.org/abs/2504.18971</link>
<guid>https://arxiv.org/abs/2504.18971</guid>
<content:encoded><![CDATA[
<div> classification, scientific software, longevity, survival models, open-source

Summary:
Using large language models, the study classifies over 18,000 scientific software projects, analyzing their attributes to understand factors affecting longevity. Infrastructural layers, downstream dependencies, publication mentions, and government participants are linked to longer lifespans, while newer projects with academic participants have shorter lifespans. Despite common perceptions, scientific projects have a longer lifespan than non-scientific open-source projects. The curated dataset provides a valuable resource for future research on scientific software, offering insights that could help prolong the lifespan of both scientific and non-scientific software projects. <div>
arXiv:2504.18971v1 Announce Type: cross 
Abstract: Scientific software is essential to scientific innovation and in many ways it is distinct from other types of software. Abandoned (or unmaintained), buggy, and hard to use software, a perception often associated with scientific software can hinder scientific progress, yet, in contrast to other types of software, its longevity is poorly understood. Existing data curation efforts are fragmented by science domain and/or are small in scale and lack key attributes. We use large language models to classify public software repositories in World of Code into distinct scientific domains and layers of the software stack, curating a large and diverse collection of over 18,000 scientific software projects. Using this data, we estimate survival models to understand how the domain, infrastructural layer, and other attributes of scientific software affect its longevity. We further obtain a matched sample of non-scientific software repositories and investigate the differences. We find that infrastructural layers, downstream dependencies, mentions of publications, and participants from government are associated with a longer lifespan, while newer projects with participants from academia had shorter lifespan. Against common expectations, scientific projects have a longer lifetime than matched non-scientific open-source software projects. We expect our curated attribute-rich collection to support future research on scientific software and provide insights that may help extend longevity of both scientific and other projects.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spreading of highly cohesive metal powders with transverse oscillation kinematics</title>
<link>https://arxiv.org/abs/2504.18981</link>
<guid>https://arxiv.org/abs/2504.18981</guid>
<content:encoded><![CDATA[
<div> powder bed additive manufacturing, laser powder bed fusion, binder jetting, transverse oscillation kinematic, dense powder layers<br />
<br />
Summary: 
The study examines the challenges of spreading fine powders in powder bed additive manufacturing processes and proposes a transverse oscillation kinematic for powder spreading. Computational simulations using a DEM-FEM framework show that transverse oscillation of a non-rotating roller can facilitate the spreading of dense powder layers with high packing fractions. Experimental validation confirms the computational results, with high packing fractions achieved for transverse oscillation frequencies above 200 Hz. Statistical analysis demonstrates that increasing transverse surface velocity improves layer uniformity and reduces cracking defects. The proposed transverse oscillation kinematic has the potential to produce thin and consistently uniform powder layers in additive manufacturing processes, offering a promising solution for handling highly cohesive powders. <br /><br /> <div>
arXiv:2504.18981v1 Announce Type: cross 
Abstract: Powder bed additive manufacturing processes such as laser powder bed fusion (LPBF) or binder jetting (BJ) benefit from using fine (D50 $\leq20~\mu m$) powders. However, the increasing level of cohesion with decreasing particle size makes spreading a uniform and continuous layer challenging. As a result, LPBF typically employs a coarser size distribution, and rotating roller mechanisms are used in BJ machines, that can create wave-like surface profiles due to roller run-out.
  In this work, a transverse oscillation kinematic for powder spreading is proposed, explored computationally, and validated experimentally. Simulations are performed using an integrated discrete element-finite element (DEM-FEM) framework and predict that transverse oscillation of a non-rotating roller facilitates the spreading of dense powder layers (beyond 50% packing fraction) with a high level of robustness to kinematic parameters. The experimental study utilizes a custom-built mechanized powder spreading testbed and X-ray transmission imaging for the analysis of spread powder layers. Experimental results generally validate the computational results, however, also exhibit parasitic layer cracking. For transverse oscillation frequencies above 200 Hz, powder layers of high packing fraction (between 50-60%) were formed, and for increased layer thicknesses, highly uniform and continuous layers were deposited. Statistical analysis of the experimental powder layer morphology as a function of kinematic spreading parameters revealed that an increasing transverse surface velocity improves layer uniformity and reduces cracking defects. This suggests that with minor improvements to the machine design, the proposed transverse oscillation kinematic has the potential to result in thin and consistently uniform powder layers of highly cohesive powder.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Bayesian Optimal Experimental Design with Normalizing Flows</title>
<link>https://arxiv.org/abs/2404.13056</link>
<guid>https://arxiv.org/abs/2404.13056</guid>
<content:encoded><![CDATA[
<div> Bayesian optimal experimental design, variational OED, normalizing flows, Monte Carlo estimators, gradient-based optimization<br />
<br />
Summary: The study introduces a novel approach, vOED-NFs, which utilizes normalizing flows (NFs) to enhance variational optimal experimental design (vOED) by approximating posterior distributions. The method employs NFs with a conditional invertible neural network architecture and includes a summary network for data dimension reduction. Monte Carlo estimators and gradient expressions enable simultaneous optimization of variational parameters and design variables. The algorithm is validated on benchmark problems and applied to scenarios involving cathodic electrophoretic deposition and stochastic modeling of aphid population. Results demonstrate that a composition of 4-5 coupling layers reduces EIG estimation bias, while NFs provide accurate approximations of posterior distributions, effectively capturing non-Gaussian and multi-modal features. The vOED-NFs approach offers a computationally efficient and accurate method for Bayesian optimal experimental design without the need for explicit likelihood evaluations. <br /><br /> <div>
arXiv:2404.13056v2 Announce Type: replace-cross 
Abstract: Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Modeling of Lipid Nanoparticle Formation for the Delivery of Nucleic Acid Therapeutics</title>
<link>https://arxiv.org/abs/2408.08577</link>
<guid>https://arxiv.org/abs/2408.08577</guid>
<content:encoded><![CDATA[
<div> Keywords: nucleic acids, lipid nanoparticles, mechanistic modeling, process development, process control<br />
Summary: 
Nucleic acids, including mRNA, are a promising therapeutic modality for treating various diseases. Lipid nanoparticles (LNPs) have been used as a delivery system for nucleic acids in COVID-19 vaccines. However, understanding the formation and structure of LNPs is challenging, especially during scale-up of manufacturing processes. Mathematical and computational methods offer a way to improve understanding of LNP formation and aid in process development and control. This article discusses strategies for mechanistic modeling of LNP formation, starting with predicting important physicochemical properties of the species involved. It outlines a framework for constructing models of reactor- and particle-scale processes, linking insights from the models to product quality attributes and process understanding. Finally, the article explores using these models to guide advanced process control and optimization strategies.<br /><br />Summary: <div>
arXiv:2408.08577v2 Announce Type: replace-cross 
Abstract: Nucleic acids such as mRNA have emerged as a promising therapeutic modality with the capability of addressing a wide range of diseases. Lipid nanoparticles (LNPs) as a delivery platform for nucleic acids were used in the COVID-19 vaccines and have received much attention. While modern manufacturing processes which involve rapidly mixing an organic stream containing the lipids with an aqueous stream containing the nucleic acids are conceptually straightforward, detailed understanding of LNP formation and structure is still limited and scale-up can be challenging. Mathematical and computational methods are a promising avenue for deepening scientific understanding of the LNP formation process and facilitating improved process development and control. This article describes strategies for the mechanistic modeling of LNP formation, starting with strategies to estimate and predict important physicochemical properties of the various species such as diffusivities and solubilities. Subsequently, a framework is outlined for constructing mechanistic models of reactor- and particle-scale processes. Insights gained from the various models are mapped back to product quality attributes and process insights. Lastly, the use of the models to guide development of advanced process control and optimization strategies is discussed.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatternPaint: Practical Layout Pattern Generation Using Diffusion-Based Inpainting</title>
<link>https://arxiv.org/abs/2409.01348</link>
<guid>https://arxiv.org/abs/2409.01348</guid>
<content:encoded><![CDATA[
<div> diffusion-based framework, VLSI layout patterns, design for manufacturing, few-shot finetuning, technology nodes 
Summary:
PatternPaint is a diffusion-based framework designed for generating diverse VLSI layout patterns essential for design for manufacturing. It addresses the challenge of limited design-rule-compliant training samples and simplifies complex layout pattern generation through inpainting processes. The framework incorporates a template-based denoising scheme and utilizes few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results demonstrate the effectiveness of PatternPaint in generating legal patterns in complex 2D metal interconnect design rule settings for sub-3nm technology nodes. It achieves high diversity scores and improves legality rates significantly through few-shot finetuning. This approach offers a production-ready solution for layout pattern generation in the development of new technology nodes. 
<br /><br />Summary: <div>
arXiv:2409.01348v4 Announce Type: replace-cross 
Abstract: Generating diverse VLSI layout patterns is essential for various downstream tasks in design for manufacturing, as design rules continually evolve during the development of new technology nodes. However, existing training-based methods for layout pattern generation rely on large datasets. In practical scenarios, especially when developing a new technology node, obtaining such extensive layout data is challenging. Consequently, training models with large datasets becomes impractical, limiting the scalability and adaptability of prior approaches. To this end, we propose PatternPaint, a diffusion-based framework capable of generating legal patterns with limited design-rule-compliant training samples. PatternPaint simplifies complex layout pattern generation into a series of inpainting processes with a template-based denoising scheme. Furthermore, we perform few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results show that using a sub-3nm technology node (Intel 18A), our model is the only one that can generate legal patterns in complex 2D metal interconnect design rule settings among all previous works and achieves a high diversity score. Additionally, our few-shot finetuning can boost the legality rate with 1.87X improvement compared to the original pretrained model. As a result, we demonstrate a production-ready approach for layout pattern generation in developing new technology nodes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets</title>
<link>https://arxiv.org/abs/2410.13105</link>
<guid>https://arxiv.org/abs/2410.13105</guid>
<content:encoded><![CDATA[
<div> Decentralized Finance, DeFi, lending, algorithm-driven, liquidity pools <br />
<br />
Summary: 
This work proposes a dynamic model for the DeFi lending market, incorporating evolving demand and supply curves and an adaptive interest rate controller to respond to market changes in real-time. The Recursive Least Squares algorithm is used to track external market conditions, ensuring stable utilization and managing default and liquidation risks. The algorithm provides theoretical guarantees on interest rate convergence and utilization stability while also addressing vulnerability to adversarial manipulation. Two approaches are proposed to mitigate manipulation: detecting extreme fluctuations and enhancing elasticity through interest rate derivative markets. The dynamic model shows low error rates on real data and the interest rate controller outperforms static curve protocols in optimizing utilization and reducing liquidations. <div>
arXiv:2410.13105v4 Announce Type: replace-cross 
Abstract: Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller tracks the external market and achieves stable utilization, while also controlling default and liquidation risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. We propose two complementary approaches to mitigating adversarial manipulation: an algorithmic method that detects extreme demand and supply fluctuations and a market-based strategy that enhances elasticity, potentially via interest rate derivative markets. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Evaluation of Variational Quantum Eigensolver and Quantum Dynamics Algorithms on the Advection-Diffusion Equation</title>
<link>https://arxiv.org/abs/2503.24045</link>
<guid>https://arxiv.org/abs/2503.24045</guid>
<content:encoded><![CDATA[
<div> Variational Quantum Eigensolver, linear advection-diffusion equation, quantum algorithms, partial differential equations, noiseless simulation<br />
<br />
Summary: 
The study investigates the performance of quantum algorithms in solving a linear one-dimensional advection-diffusion equation. It compares Variational Quantum Eigensolver (VQE) with Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS) on small quantum hardware. While VQE on a noiseless simulator achieves high accuracy with low infidelities, the dynamics algorithms suffer from errors due to noise and limited shot statistics on hardware. VQE outperforms the dynamics methods in terms of accuracy as it reaches low infidelities with moderate circuit depths. The comparison provides insights into the accuracy and resource demands of different algorithms for solving partial differential equations. The study concludes with a discussion on potential extensions to higher-dimensional and nonlinear PDEs relevant to engineering and finance. <br /><br /> <div>
arXiv:2503.24045v2 Announce Type: replace-cross 
Abstract: We investigate the potential of near-term quantum algorithms for solving partial differential equations (PDEs), focusing on a linear one-dimensional advection-diffusion equation as a test case. This study benchmarks a ground-state algorithm, Variational Quantum Eigensolver (VQE), against three leading quantum dynamics algorithms, Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS), applied to the same PDE on small quantum hardware. While Trotterization is fully quantum, VarQTE and AVQDS are variational algorithms that reduce circuit depth for noisy intermediate-scale quantum (NISQ) devices. However, hardware results from these dynamics methods show sizable errors due to noise and limited shot statistics. To establish a noise-free performance baseline, we implement the VQE-based solver on a noiseless statevector simulator. Our results show VQE can reach final-time infidelities as low as ${O}(10^{-9})$ with $N=4$ qubits and moderate circuit depths, outperforming hardware-deployed dynamics methods that show infidelities $\gtrsim 10^{-1}$. By comparing noiseless VQE to shot-based and hardware-run algorithms, we assess their accuracy and resource demands, providing a baseline for future quantum PDE solvers. We conclude with a discussion of limitations and potential extensions to higher-dimensional, nonlinear PDEs relevant to engineering and finance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTFinRAG: Interactive Modularized Financial RAG Benchmark</title>
<link>https://arxiv.org/abs/2504.18024</link>
<guid>https://arxiv.org/abs/2504.18024</guid>
<content:encoded><![CDATA[
<div> Keywords: financial sectors, language model technologies, SMARTFinRAG, evaluation paradigm, open-source architecture<br />
Summary: SMARTFinRAG is a new platform designed to address gaps in assessing specialized RAG systems in the financial sector. It introduces a modular architecture that allows components to be interchanged during runtime, a document-centric evaluation paradigm that creates domain-specific QA pairs from financial documents, and an intuitive interface to facilitate research-implementation integration. The evaluation of SMARTFinRAG shows variations in retrieval efficacy and response quality across different configurations. The platform's open-source architecture promotes transparent and reproducible research, while also helping financial institutions overcome deployment challenges when implementing RAG systems. This innovative platform aims to enhance the adoption and evaluation of language model technologies in the financial industry. <br /><br /> <div>
arXiv:2504.18024v1 Announce Type: new 
Abstract: Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Governing Equations of Geomagnetic Storm Dynamics with Symbolic Regression</title>
<link>https://arxiv.org/abs/2504.18461</link>
<guid>https://arxiv.org/abs/2504.18461</guid>
<content:encoded><![CDATA[
<div> solar wind, geomagnetic storms, Disturbance Storm Time index, symbolic regression, PySR framework

Summary:
The study focuses on using symbolic regression to create data-driven equations that describe the behavior of the Disturbance Storm Time (Dst) index during geomagnetic storms. By analyzing historical data from the NASA OMNIweb database, including various solar wind parameters, the study aims to develop models that accurately predict the evolution of the Dst index. The models generated by the PySR framework showcase a hierarchy of complexity levels and outperform traditional empirical models in terms of accuracy and interpretability. The evaluation of the models on historical storm events, such as the 2003 Halloween Storm and the 2015 St. Patrick's Day Storm, demonstrates their effectiveness in capturing nonlinear dependencies and thresholding effects in Dst evolution. Overall, the study provides valuable insights into the mechanisms driving geomagnetic storms and offers interpretable mathematical expressions for predicting their intensity. 

<br /><br />Summary: <div>
arXiv:2504.18461v1 Announce Type: new 
Abstract: Geomagnetic storms are large-scale disturbances of the Earth's magnetosphere driven by solar wind interactions, posing significant risks to space-based and ground-based infrastructure. The Disturbance Storm Time (Dst) index quantifies geomagnetic storm intensity by measuring global magnetic field variations. This study applies symbolic regression to derive data-driven equations describing the temporal evolution of the Dst index. We use historical data from the NASA OMNIweb database, including solar wind density, bulk velocity, convective electric field, dynamic pressure, and magnetic pressure. The PySR framework, an evolutionary algorithm-based symbolic regression library, is used to identify mathematical expressions linking dDst/dt to key solar wind. The resulting models include a hierarchy of complexity levels and enable a comparison with well-established empirical models such as the Burton-McPherron-Russell and O'Brien-McPherron models. The best-performing symbolic regression models demonstrate superior accuracy in most cases, particularly during moderate geomagnetic storms, while maintaining physical interpretability. Performance evaluation on historical storm events includes the 2003 Halloween Storm, the 2015 St. Patrick's Day Storm, and a 2017 moderate storm. The results provide interpretable, closed-form expressions that capture nonlinear dependencies and thresholding effects in Dst evolution.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAQGA: A Quantum-Enhanced Genetic Algorithm with Novel Entanglement-Aware Crossovers</title>
<link>https://arxiv.org/abs/2504.17923</link>
<guid>https://arxiv.org/abs/2504.17923</guid>
<content:encoded><![CDATA[
<div> genetic algorithms, combinatorial optimization, portfolio optimization, quantum computing, quantum circuits

Summary:
- Genetic algorithms are effective in complex optimization problems, such as portfolio optimization.
- Quantum computing can address challenging tasks, and quantum genetic algorithms combine the benefits of both approaches.
- The proposed quantum genetic algorithm introduces a novel crossover strategy generating quantum circuits from binary solutions.
- It encodes entanglement patterns from parent solutions to enhance performance without significantly increasing circuit depth.
- Testing on a portfolio optimization problem using IBM's 127 qubits Eagle processor and simulators shows a significant improvement in fitness values compared to classical and quantum-inspired genetic algorithms, highlighting the potential of quantum computers in solving real-world combinatorial optimization problems.<br /><br /> <div>
arXiv:2504.17923v1 Announce Type: cross 
Abstract: Genetic algorithms are highly effective optimization techniques for many computationally challenging problems, including combinatorial optimization tasks like portfolio optimization. Quantum computing has also shown potential in addressing these complex challenges. Combining these approaches, quantum genetic algorithms leverage the principles of superposition and entanglement to enhance the performance of classical genetic algorithms. In this work, we propose a novel quantum genetic algorithm introducing an innovative crossover strategy to generate quantum circuits from a binary solution. We incorporate a heuristic method to encode entanglement patterns from parent solutions into circuits for the next generation. Our algorithm advances quantum genetic algorithms by utilizing a limited number of entanglements, enabling efficient exploration of optimal solutions without significantly increasing circuit depth, making it suitable for near-term applications. We test this approach on a portfolio optimization problem using an IBM 127 qubits Eagle processor (ibm_quebec) and simulators. Compared to state-of-the-art algorithms, our results show that the proposed method improves fitness values by 33.6% over classical genetic algorithm and 37.2% over quantum-inspired genetic algorithm, using the same iteration counts and population sizes with real quantum hardware employing 100 qubits. These findings highlight the potential of current quantum computers to address real-world utility-scale combinatorial optimization problems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Machine Learning Approach For Bitcoin Forecasting</title>
<link>https://arxiv.org/abs/2504.18206</link>
<guid>https://arxiv.org/abs/2504.18206</guid>
<content:encoded><![CDATA[
<div> Keywords: Bitcoin, cryptocurrency, time series, machine learning, directional accuracy

Summary:
In this study, the researchers focus on Bitcoin, a popular cryptocurrency, and investigate the use of different time series data along with machine learning algorithms to forecast its price movements. The analysis reveals that incorporating the Open, High, and Low prices of Bitcoin significantly improves directional accuracy. The Low price, in particular, plays a crucial role in enhancing the forecast accuracy when used in combination with a Gated Recurrent Unit network and a baseline forecast. The study also finds that other Bitcoin-related features, apart from price data, have minimal impact on prediction accuracy. Overall, the proposed method displays comparable performance to existing approaches in terms of directional accuracy, emphasizing the importance of considering specific time series data and machine learning techniques for forecasting cryptocurrency prices.<br /><br />Summary: <div>
arXiv:2504.18206v1 Announce Type: cross 
Abstract: Bitcoin is one of the cryptocurrencies that is gaining more popularity in recent years. Previous studies have shown that closing price alone is not enough to forecast stock market series. We introduce a new set of time series and demonstrate that a subset is necessary to improve directional accuracy based on a machine learning ensemble. In our experiments, we study which time series and machine learning algorithms deliver the best results. We found that the most relevant time series that contribute to improving directional accuracy are Open, High and Low, with the largest contribution of Low in combination with an ensemble of Gated Recurrent Unit network and a baseline forecast. The relevance of other Bitcoin-related features that are not price-related is negligible. The proposed method delivers similar performance to the state-of-the-art when observing directional accuracy.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rapid modelling of reactive transport in porous media using machine learning: limitations and solutions</title>
<link>https://arxiv.org/abs/2405.14548</link>
<guid>https://arxiv.org/abs/2405.14548</guid>
<content:encoded><![CDATA[
<div> machine learning, reactive transport, porous media, geochemical reactions, cation exchange problem

Summary:
Machine learning models are explored as replacements for a geochemical module in simulating reactive transport in porous media. Testing on a cation exchange problem reveals that while the surrogate models perform well in isolated predictions, they struggle with rollout predictions over successive time steps. By incorporating physics-based constraints and tailored dataset generation strategies, accurate rollout predictions are achieved. The study highlights the limitation of machine learning surrogates in predicting over multiple time steps, even for a simple sorption equilibrium reaction like the cation exchange problem. However, with the addition of physics-based modifications, these limitations can be overcome. The research provides a detailed analysis of these limitations and potential mitigation strategies.<br /><br /> <div>
arXiv:2405.14548v2 Announce Type: replace 
Abstract: Reactive transport in porous media plays a pivotal role in subsurface reservoir processes, influencing fluid properties and geochemical characteristics. However, coupling fluid flow and transport with geochemical reactions is computationally intensive, requiring geochemical calculations at each grid cell and each time step within a discretized simulation domain. Although recent advancements have integrated machine learning techniques as surrogates for geochemical simulations, ensuring computational efficiency and accuracy remains a challenge. This work investigates machine learning models as replacements for a geochemical module in a simulation of reactive transport in porous media. As a proof of concept, we test this approach on a well-documented cation exchange problem. While the surrogate models excel in isolated predictions, they fall short in rollout predictions over successive time steps. By introducing modifications, including physics-based constraints and tailored dataset generation strategies, we show that machine learning surrogates can achieve accurate rollout predictions. Our findings emphasize that even for a simple sorption equilibrium reaction (cation exchange problem), machine learning surrogates alone fail in predicting over successive time-steps. Incorporating simple physics-based modifications enables us to overcome this limitation. A detailed analysis of the limitations and potential mitigation strategies is presented in this work.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Function-coherent gambles</title>
<link>https://arxiv.org/abs/2503.01855</link>
<guid>https://arxiv.org/abs/2503.01855</guid>
<content:encoded><![CDATA[
<div> function-coherent gambles, non-linear utility, intertemporal choice, discounting, desirability framework
Summary:
The paper introduces function-coherent gambles, a generalization of the desirable gambles framework that allows for non-linear utility functions. Core axioms for function-coherence are established, and a representation theorem is proven, linking acceptable gambles to continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, such as hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. The integration of these alternative discounting models within the function-coherent framework provides a unified treatment for modeling complex patterns of time preference. This approach bridges the gap between normative theory and real-world behavior in intertemporal decision-making under uncertainty, offering theoretical foundations for understanding sophisticated time preferences within the desirability paradigm. <br /><br />Summary: <div>
arXiv:2503.01855v2 Announce Type: replace-cross 
Abstract: The desirable gambles framework provides a foundational approach to imprecise probability theory but relies heavily on linear utility assumptions. This paper introduces function-coherent gambles, a generalization that accommodates non-linear utility while preserving essential rationality properties. We establish core axioms for function-coherence and prove a representation theorem that characterizes acceptable gambles through continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, including hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. We demonstrate how these alternatives to constant-rate exponential discounting can be integrated within the function-coherent framework. This unified treatment provides theoretical foundations for modeling sophisticated patterns of time preference within the desirability paradigm, bridging a gap between normative theory and observed behavior in intertemporal decision-making under genuine uncertainty.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Entropy Stable Formulation of Two-equation Turbulence Models with Particular Reference to the k-epsilon Model</title>
<link>https://arxiv.org/abs/2504.17110</link>
<guid>https://arxiv.org/abs/2504.17110</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical algorithms, partial differential equations, entropy production inequality, turbulence models, k-epsilon model

Summary:
This article discusses the importance of incorporating nonlinear physical stability principles, such as the entropy production inequality, in the design of numerical algorithms for partial differential equations. By introducing space-time averaging and defining entropy variables, a symmetric system of advective-diffusive equations can be derived for turbulence models, including the k-epsilon model. Positivity and symmetry constraints are necessary for the turbulence diffusivity coefficients and source terms to ensure the design of entropy producing two-equation turbulence models. This approach emphasizes the use of physical principles over artificial viscosity for designing robust algorithms. <div>
arXiv:2504.17110v1 Announce Type: new 
Abstract: Consistency and stability are two essential ingredients in the design of numerical algorithms for partial differential equations. Robust algorithms can be developed by incorporating nonlinear physical stability principles in their design, such as the entropy production inequality (i.e., the Clausius-Duhem inequality or second law of thermodynamics), rather than by simply adding artificial viscosity (a common approach). This idea is applied to the k-epsilon and two-equation turbulence models by introducing space-time averaging. Then, a set of entropy variables can be defined which leads to a symmetric system of advective-diffusive equations. Positivity and symmetry of the equations require certain constraints on the turbulence diffusivity coefficients and the turbulence source terms. With these, we are able to design entropy producing two-equation turbulence models and, in particular, the k-epsilon model.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Stock Prices for Enhanced Multi-Step Forecast and Prediction</title>
<link>https://arxiv.org/abs/2504.17313</link>
<guid>https://arxiv.org/abs/2504.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: stock price forecasting, stock price prediction, PCIE model, tokenization method, multi-step prediction<br />
Summary: <br />
Effective stock price forecasting and prediction are essential for investors and policymakers but are challenging due to the dynamic nature of stock price data. Forecasting and prediction targets have distinct statistical characteristics and multi-step approaches provide richer information but are more difficult. The Patched Channel Integration Encoder (PCIE) model is introduced to address these challenges by utilizing multiple stock channels and a novel tokenization method. The tokenization process involves univariate patching and temporal learning to reduce cumulative errors. Experimental results demonstrate that PCIE outperforms current state-of-the-art models in both forecast and prediction tasks. <div>
arXiv:2504.17313v1 Announce Type: new 
Abstract: Effective stock price forecasting (estimating future prices) and prediction (estimating future price changes) are pivotal for investors, regulatory agencies, and policymakers. These tasks enable informed decision-making, risk management, strategic planning, and superior portfolio returns. Despite their importance, forecasting and prediction are challenging due to the dynamic nature of stock price data, which exhibit significant temporal variations in distribution and statistical properties. Additionally, while both forecasting and prediction targets are derived from the same dataset, their statistical characteristics differ significantly. Forecasting targets typically follow a log-normal distribution, characterized by significant shifts in mean and variance over time, whereas prediction targets adhere to a normal distribution. Furthermore, although multi-step forecasting and prediction offer a broader perspective and richer information compared to single-step approaches, it is much more challenging due to factors such as cumulative errors and long-term temporal variance. As a result, many previous works have tackled either single-step stock price forecasting or prediction instead. To address these issues, we introduce a novel model, termed Patched Channel Integration Encoder (PCIE), to tackle both stock price forecasting and prediction. In this model, we utilize multiple stock channels that cover both historical prices and price changes, and design a novel tokenization method to effectively embed these channels in a cross-channel and temporally efficient manner. Specifically, the tokenization process involves univariate patching and temporal learning with a channel-mixing encoder to reduce cumulative errors. Comprehensive experiments validate that PCIE outperforms current state-of-the-art models in forecast and prediction tasks.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems</title>
<link>https://arxiv.org/abs/2504.17354</link>
<guid>https://arxiv.org/abs/2504.17354</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, effective contact area, rough surfaces, machine learning algorithms, multi-query contexts
<br />
Summary: 
This study presents a surrogate modeling framework for predicting the effective contact area in rough surface contact using fast-to-evaluate machine learning algorithms. The models are trained on a precomputed dataset of imposed load and roughness parameters to predict the effective contact area efficiently. The Kernel Ridge Regressor is identified as the best trade-off between accuracy and efficiency, making it suitable for general-purpose surrogate modeling. The Gaussian Process Regressor is also effective for uncertainty quantification tasks. The models' generalization capability is validated on unseen simulation scenarios, demonstrating their transferability to new configurations. While database generation is a significant cost in the process, the overall approach is practical and efficient for multi-query tasks, even after accounting for initial expenses. <br /><br />Summary: <div>
arXiv:2504.17354v1 Announce Type: new 
Abstract: The effective contact area in rough surface contact plays a critical role in multi-physics phenomena such as wear, sealing, and thermal or electrical conduction. Although accurate numerical methods, like the Boundary Element Method (BEM), are available to compute this quantity, their high computational cost limits their applicability in multi-query contexts, such as uncertainty quantification, parameter identification, and multi-scale algorithms, where many repeated evaluations are required. This study proposes a surrogate modeling framework for predicting the effective contact area using fast-to-evaluate data-driven techniques. Various machine learning algorithms are trained on a precomputed dataset, where the inputs are the imposed load and statistical roughness parameters, and the output is the corresponding effective contact area. All models undergo hyperparameter optimization to enable fair comparisons in terms of predictive accuracy and computational efficiency, evaluated using established quantitative metrics. Among the models, the Kernel Ridge Regressor demonstrates the best trade-off between accuracy and efficiency, achieving high predictive accuracy, low prediction time, and minimal training overhead-making it a strong candidate for general-purpose surrogate modeling. The Gaussian Process Regressor provides an attractive alternative when uncertainty quantification is required, although it incurs additional computational cost due to variance estimation. The generalization capability of the Kernel Ridge model is validated on an unseen simulation scenario, confirming its ability to transfer to new configurations. Database generation constitutes the dominant cost in the surrogate modeling process. Nevertheless, the approach proves practical and efficient for multi-query tasks, even when accounting for this initial expense.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>polyGen: A Learning Framework for Atomic-level Polymer Structure Generation</title>
<link>https://arxiv.org/abs/2504.17656</link>
<guid>https://arxiv.org/abs/2504.17656</guid>
<content:encoded><![CDATA[
<div> Latent diffusion model, polymer structures, generative algorithms, molecular encoding, structure matching criteria 

Summary: 

polyGen is introduced as a novel approach to generating realistic polymer structures using a latent diffusion model. Leveraging a molecular encoding that captures polymer connectivity, polyGen can generate diverse conformations of both linear chains and complex branched structures. The model shows improvement in joint learning between similar chemical structures through training augmentation with DFT-optimized molecular structures. However, its performance decreases when handling repeat units with a high atom count. polyGen represents a paradigm shift in atomic-level structure generation for polymer science, providing the first proof-of-concept for predicting realistic atomic-level polymer conformations while considering their intrinsic structural flexibility. <div>
arXiv:2504.17656v1 Announce Type: new 
Abstract: Synthetic polymeric materials underpin fundamental technologies in the energy, electronics, consumer goods, and medical sectors, yet their development still suffers from prolonged design timelines. Although polymer informatics tools have supported speedup, polymer simulation protocols continue to face significant challenges: on-demand generation of realistic 3D atomic structures that respect the conformational diversity of polymer structures. Generative algorithms for 3D structures of inorganic crystals, bio-polymers, and small molecules exist, but have not addressed synthetic polymers. In this work, we introduce polyGen, the first latent diffusion model designed specifically to generate realistic polymer structures from minimal inputs such as the repeat unit chemistry alone, leveraging a molecular encoding that captures polymer connectivity throughout the architecture. Due to a scarce dataset of only 3855 DFT-optimized polymer structures, we augment our training with DFT-optimized molecular structures, showing improvement in joint learning between similar chemical structures. We also establish structure matching criteria to benchmark our approach on this novel problem. polyGen effectively generates diverse conformations of both linear chains and complex branched structures, though its performance decreases when handling repeat units with a high atom count. Given these initial results, polyGen represents a paradigm shift in atomic-level structure generation for polymer science-the first proof-of-concept for predicting realistic atomic-level polymer conformations while accounting for their intrinsic structural flexibility.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy</title>
<link>https://arxiv.org/abs/2504.17124</link>
<guid>https://arxiv.org/abs/2504.17124</guid>
<content:encoded><![CDATA[
<div> X-ray absorption near edge structure (XANES) spectroscopy, adaptive sampling methods, Bayesian optimization, absorption edge, pre-edge peaks <br />
<br />Summary: <br />
X-ray absorption near edge structure (XANES) spectroscopy is a valuable technique for analyzing chemical states in materials, but traditional data collection methods can be time-consuming. This study introduces a knowledge-injected Bayesian optimization approach for adaptive XANES data collection, taking into account spectral features like absorption edges and pre-edge peaks. The method efficiently reconstructs absorption edges with high accuracy using only 15-20% of the measurement points needed in conventional sampling. It can determine the x-ray energy of sharp peaks with minimal errors, achieving overall root-mean-square errors below 0.005 compared to traditional methods. The experiments on battery materials and catalysts demonstrate its effectiveness for both static and dynamic XANES measurements, enhancing data collection efficiency and enabling better time resolution for tracking chemical changes. This automated approach reduces common errors in XANES experiments and facilitates dynamic studies requiring high temporal resolution. <div>
arXiv:2504.17124v1 Announce Type: cross 
Abstract: X-ray absorption near edge structure (XANES) spectroscopy is a powerful technique for characterizing the chemical state and symmetry of individual elements within materials, but requires collecting data at many energy points which can be time-consuming. While adaptive sampling methods exist for efficiently collecting spectroscopic data, they often lack domain-specific knowledge about XANES spectra structure. Here we demonstrate a knowledge-injected Bayesian optimization approach for adaptive XANES data collection that incorporates understanding of spectral features like absorption edges and pre-edge peaks. We show this method accurately reconstructs the absorption edge of XANES spectra using only 15-20% of the measurement points typically needed for conventional sampling, while maintaining the ability to determine the x-ray energy of the sharp peak after absorption edge with errors less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and overall root-mean-square errors less than 0.005 compared to compared to traditionally sampled spectra. Our experiments on battery materials and catalysts demonstrate the method's effectiveness for both static and dynamic XANES measurements, improving data collection efficiency and enabling better time resolution for tracking chemical changes. This approach advances the degree of automation in XANES experiments reducing the common errors of under- or over-sampling points in near the absorption edge and enabling dynamic experiments that require high temporal resolution or limited measurement time.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints</title>
<link>https://arxiv.org/abs/2504.17142</link>
<guid>https://arxiv.org/abs/2504.17142</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, microelectronic components, multiphysics constraints, design optimization, high-dimensional solution space
Summary:
This study focuses on using reinforcement learning techniques for designing microelectronic components under multiphysics constraints. Traditional global optimization approaches are limited when dealing with intricate solution spaces and constraints. The complexity of microelectronic component design, such as ASICs and HI interposers, poses challenges for conventional methods. The study explores optimizing interconnect geometry for ASIC chips and component placement on a HI interposer while meeting thermoelastic and design constraints. The placement problem involves a high-dimensional solution space, highlighting the need for advanced techniques like reinforcement learning. By developing and testing an RL-based framework, the study aims to enhance the design and optimization processes for microelectronic components with multiphysics constraints. <div>
arXiv:2504.17142v1 Announce Type: cross 
Abstract: This study focuses on the development of reinforcement learning based techniques for the design of microelectronic components under multiphysics constraints. While traditional design approaches based on global optimization approaches are effective when dealing with a small number of design parameters, as the complexity of the solution space and of the constraints increases different techniques are needed. This is an important reason that makes the design and optimization of microelectronic components (characterized by large solution space and multiphysics constraints) very challenging for traditional methods. By taking as prototypical elements an application-specific integrated circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and numerically test an optimization framework based on reinforcement learning (RL). More specifically, we consider the optimization of the bonded interconnect geometry for an ASIC chip as well as the placement of components on a HI interposer while satisfying thermoelastic and design constraints. This placement problem is particularly interesting because it features a high-dimensional solution space.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection, Classification and Prevalence of Self-Admitted Aging Debt</title>
<link>https://arxiv.org/abs/2504.17428</link>
<guid>https://arxiv.org/abs/2504.17428</guid>
<content:encoded><![CDATA[
<div> Aging Debt, Self-Admitted Aging Debt, software aging, source code comments, OSS repositories <br />
Summary:<br />
The study introduces Aging Debt (AD) as a concept to represent increased maintenance efforts in software. It focuses on Self-Admitted Aging Debt (SAAD) observed in source code comments to detect and measure AD in software. A taxonomy is developed to categorize temporal software aging into Active and Dormant types. Analysis of over 9,000+ OSS repositories shows that more than 21% exhibit SAAD, with Dormant AD being prevalent. The study highlights the critical aspect of software maintenance and the importance of addressing evolutionary indicators like source code comments in software aging research. The proposed taxonomy can assist researchers in detailed software aging studies and help practitioners in developing proactive maintenance strategies. <br /> <div>
arXiv:2504.17428v1 Announce Type: cross 
Abstract: Context: Previous research on software aging is limited with focus on dynamic runtime indicators like memory and performance, often neglecting evolutionary indicators like source code comments and narrowly examining legacy issues within the TD context. Objective: We introduce the concept of Aging Debt (AD), representing the increased maintenance efforts and costs needed to keep software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed in source code comments left by software developers. Method: We employ a mixed-methods approach, combining qualitative and quantitative analyses to detect and measure AD in software. This includes framing SAAD patterns from the source code comments after analysing the source code context, then utilizing the SAAD patterns to detect SAAD comments. In the process, we develop a taxonomy for SAAD that reflects the temporal aging of software and its associated debt. Then we utilize the taxonomy to quantify the different types of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes temporal software aging into Active and Dormant types. Our extensive analysis of over 9,000+ Open Source Software (OSS) repositories reveals that more than 21% repositories exhibit signs of SAAD as observed from our gold standard SAAD dataset. Notably, Dormant AD emerges as the predominant category, highlighting a critical but often overlooked aspect of software maintenance. Conclusion: As software volume grows annually, so do evolutionary aging and maintenance challenges; our proposed taxonomy can aid researchers in detailed software aging studies and help practitioners develop improved and proactive maintenance strategies.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An approach based on metaheuristic algorithms to the timetabling problem in deregulated railway markets</title>
<link>https://arxiv.org/abs/2504.17455</link>
<guid>https://arxiv.org/abs/2504.17455</guid>
<content:encoded><![CDATA[
<div> Genetic Algorithm, train timetabling, liberalized railway markets, optimization, scheduling<br />
<br />
Summary: <br />
The paper addresses the train timetabling problem in liberalized railway markets, focusing on maximizing infrastructure capacity and revenue optimization for infrastructure managers and railway undertakings. A modular simulation framework is introduced to simulate deregulated railway systems, evaluating ten metaheuristic algorithms using the MEALPY Python library. Results show that the Genetic Algorithm outperforms other algorithms in revenue optimization, convergence speed, and schedule adherence. Alternative algorithms, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, exhibit slower convergence and higher variability. The study highlights the trade-off between scheduling more trains and meeting requested times, offering insights into solving complex scheduling challenges in deregulated railway systems. <div>
arXiv:2504.17455v1 Announce Type: cross 
Abstract: The train timetabling problem in liberalized railway markets represents a challenge to the coordination between infrastructure managers and railway undertakings. Efficient scheduling is critical in maximizing infrastructure capacity and utilization while adhering as closely as possible to the requests of railway undertakings. These objectives ultimately contribute to maximizing the infrastructure manager's revenues. This paper sets out a modular simulation framework to reproduce the dynamics of deregulated railway systems. Ten metaheuristic algorithms using the MEALPY Python library are then evaluated in order to optimize train schedules in the liberalized Spanish railway market. The results show that the Genetic Algorithm outperforms others in revenue optimization, convergence speed, and schedule adherence. Alternatives, such as Particle Swarm Optimization and Ant Colony Optimization Continuous, show slower convergence and higher variability. The results emphasize the trade-off between scheduling more trains and adhering to requested times, providing insights into solving complex scheduling problems in deregulated railway systems.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Equitable Rail Service Allocation Through Fairness-Oriented Timetabling in Liberalized Markets</title>
<link>https://arxiv.org/abs/2504.17489</link>
<guid>https://arxiv.org/abs/2504.17489</guid>
<content:encoded><![CDATA[
<div> Keywords: European rail transport, liberalization, infrastructure capacity, railway market, equity metrics 

Summary: 
In the changing landscape of European rail transport due to liberalization, railway undertakings now compete for limited infrastructure capacity to offer their services. The equitable allocation of infrastructure by the infrastructure manager is crucial for the efficiency and sustainability of this competitive environment. A methodology utilizing Jain, Gini, and Atkinson equity metrics is proposed to address the rail service allocation problem in a liberalized railway market. The results from computational tests demonstrate that this methodology promotes equitable planning across various competitiveness scenarios. This stands in contrast to approaches solely focused on maximizing the infrastructure manager's profit without considering fair allocation. The study supports the use of the proposed methodology and equity metrics as effective tools for planning and decision-making within a liberalized railway market. 

<br /><br />Summary: <div>
arXiv:2504.17489v1 Announce Type: cross 
Abstract: Over the last few decades, European rail transport has undergone major changes as part of the process of liberalization set out in European regulations. In this context of liberalization, railway undertakings compete with each other for the limited infrastructure capacity available to offer their rail services. The infrastructure manager is responsible for the equitable allocation of infrastructure between all companies in the market, which is essential to ensure the efficiency and sustainability of this competitive ecosystem. In this paper, a methodology based on Jain, Gini and Atkinson equity metrics is used to solve the rail service allocation problem in a liberalized railway market, analyzing the solutions obtained. The results show that the proposed methodology and the equity metrics used allow for equitable planning in different competitiveness scenarios. These results contrast with solutions where the objective of the infrastructure manager is to maximize its own profit, without regard for the equitable allocation of infrastructure. Therefore, the computational tests support the methodology and metrics used as a planning and decision support tool in a liberalized railway market.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate and generalizable protein-ligand binding affinity prediction with geometric deep learning</title>
<link>https://arxiv.org/abs/2504.16261</link>
<guid>https://arxiv.org/abs/2504.16261</guid>
<content:encoded><![CDATA[
<div> deep learning, protein-ligand binding, affinity prediction, interatomic potential, machine learning<br />
Summary:<br />
The article introduces IPBind, a computational method based on geometric deep learning, for predicting protein-ligand binding affinity. While existing algorithms struggle with novel protein-ligand complexes, IPBind leverages interatomic potential to make robust predictions. Experimental results on binding affinity prediction benchmarks show the effectiveness and universality of IPBind, providing atom-level insights. This work underscores the benefits of using machine learning interatomic potential in protein-ligand binding affinity prediction. <div>
arXiv:2504.16261v1 Announce Type: new 
Abstract: Protein-ligand binding complexes are ubiquitous and essential to life. Protein-ligand binding affinity prediction (PLA) quantifies the binding strength between ligands and proteins, providing crucial insights for discovering and designing potential candidate ligands. While recent advances have been made in predicting protein-ligand complex structures, existing algorithms for interaction and affinity prediction suffer from a sharp decline in performance when handling ligands bound with novel unseen proteins. We propose IPBind, a geometric deep learning-based computational method, enabling robust predictions by leveraging interatomic potential between complex's bound and unbound status. Experimental results on widely used binding affinity prediction benchmarks demonstrate the effectiveness and universality of IPBind. Meanwhile, it provides atom-level insights into prediction. This work highlights the advantage of leveraging machine learning interatomic potential for predicting protein-ligand binding affinity.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Different Transformer Model Structures for Stock Prediction</title>
<link>https://arxiv.org/abs/2504.16361</link>
<guid>https://arxiv.org/abs/2504.16361</guid>
<content:encoded><![CDATA[
<div> Transformer, stock index prediction, model architectures, forecasting, ProbSparse attention

Summary:
- The paper compares different Transformer model architectures for stock index prediction, highlighting the importance of architectural choices in predictive accuracy. 
- Existing studies often treat Transformers as black boxes, overlooking the impact of specific structural designs on performance.
- Five Transformer structures were evaluated: encoder-only, decoder-only, Vanilla Transformer, Vanilla Transformer without embedding layers, and Vanilla Transformer with ProbSparse attention.
- Results indicate that Transformer models generally outperform traditional approaches, with the decoder-only structure being the most effective in all scenarios.
- The Transformer with ProbSparse attention showed the poorest performance in most cases. 

<br /><br />Summary: <div>
arXiv:2504.16361v1 Announce Type: new 
Abstract: This paper compares different Transformer model architectures for stock index prediction. While many studies have shown that Transformers perform well in stock price forecasting, few have explored how different structural designs impact performance. Most existing works treat the Transformer as a black box, overlooking how specific architectural choices may affect predictive accuracy. However, understanding these differences is critical for developing more effective forecasting models. This study aims to identify which Transformer variant is most suitable for stock forecasting. This study evaluates five Transformer structures: (1) encoder-only Transformer, (2) decoder-only Transformer, (3) Vanilla Transformer (encoder + decoder), (4) Vanilla Transformer without embedding layers, and (5) Vanilla Transformer with ProbSparse attention. Results show that Transformer-based models generally outperform traditional approaches. Transformer with decoder only structure outperforms all other models in all scenarios. Transformer with ProbSparse attention has the worst performance in almost all cases.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preconditioning Natural and Second Order Gradient Descent in Quantum Optimization: A Performance Benchmark</title>
<link>https://arxiv.org/abs/2504.16518</link>
<guid>https://arxiv.org/abs/2504.16518</guid>
<content:encoded><![CDATA[
<div> optimization, parametric quantum circuits, noisy gradient evaluations, curvature information, BFGS update rule <br />
Summary: <br />
The optimization of parametric quantum circuits faces challenges due to the non-convex nature of the objective function, noisy gradient evaluations, and barren plateaus. Selecting the right classical optimizer is crucial in quantum-classical applications. Incorporating curvature information in the parameter update can aid in faster convergence. Quasi-Newton and quantum natural gradient methods show promise in this regard but have drawbacks. A study evaluates the performance of optimizers on MaxCut problems using a shallow QAOA algorithm. To address noise sensitivity and iteration cost, a novel approach called secant-penalization in the BFGS update rule (SP-BFGS) is introduced, leading to improved outcomes for QAOA optimization problems. This method stabilizes BFGS updates against gradient noise, showcasing potential for optimizing parametric quantum circuits in noisy environments. <br /> <div>
arXiv:2504.16518v1 Announce Type: new 
Abstract: The optimization of parametric quantum circuits is technically hindered by three major obstacles: the non-convex nature of the objective function, noisy gradient evaluations, and the presence of barren plateaus. As a result, the selection of classical optimizer becomes a critical factor in assessing and exploiting quantum-classical applications. One promising approach to tackle these challenges involves incorporating curvature information into the parameter update. The most prominent methods in this field are quasi-Newton and quantum natural gradient methods, which can facilitate faster convergence compared to first-order approaches. Second order methods however exhibit a significant trade-off between computational cost and accuracy, as well as heightened sensitivity to noise. This study evaluates the performance of three families of optimizers on synthetically generated MaxCut problems on a shallow QAOA algorithm. To address noise sensitivity and iteration cost, we demonstrate that incorporating secant-penalization in the BFGS update rule (SP-BFGS) yields improved outcomes for QAOA optimization problems, introducing a novel approach to stabilizing BFGS updates against gradient noise.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields</title>
<link>https://arxiv.org/abs/2504.16600</link>
<guid>https://arxiv.org/abs/2504.16600</guid>
<content:encoded><![CDATA[
<div> Keywords: passive implants, magnetic fields, safety assessment, numerical approach, thermal diffusion <br />
Summary: <br />
- Safety assessment of patients with one-dimensional structured passive implants exposed to low or medium frequency magnetic fields poses challenges due to different length scales. <br />
- A novel numerical approach is proposed, solving three-dimensional and one-dimensional coupled problems, considering thermal diffusion through metallic implants for improved accuracy. <br />
- Results from measurements on a cranial plate exposed to a magnetic field show a 25% improvement in accuracy compared to the method based on thermal seeds. <br />
- Application of the proposed method in a magnetic hyperthermia case study predicts a 10% lower temperature increase near the implant compared to the overestimation by relying on thermal seeds. <br /> <div>
arXiv:2504.16600v1 Announce Type: new 
Abstract: Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Literature Review of Software Engineering Research on Jupyter Notebook</title>
<link>https://arxiv.org/abs/2504.16180</link>
<guid>https://arxiv.org/abs/2504.16180</guid>
<content:encoded><![CDATA[
<div> Trends, gaps, methodologies, human-computer interaction, reusability <br />
Summary:<br />
The study analyzed software engineering research on Jupyter notebooks, finding that most publications are in human-computer interaction venues rather than traditional software engineering ones. Various software engineering topics were addressed, but solutions for testing, refactoring, and documentation specific to notebooks are lacking. Only a small percentage of studies provide reusable code, and many replication packages are not stored in permanent repositories. Future research opportunities include developing automatic testing frameworks, refactoring clones between notebooks, and generating group documentation for coherent code cells. <div>
arXiv:2504.16180v1 Announce Type: cross 
Abstract: Context: Jupyter Notebook has emerged as a versatile tool that transforms how researchers, developers, and data scientists conduct and communicate their work. As the adoption of Jupyter notebooks continues to rise, so does the interest from the software engineering research community in improving the software engineering practices for Jupyter notebooks.
  Objective: The purpose of this study is to analyze trends, gaps, and methodologies used in software engineering research on Jupyter notebooks.
  Method: We selected 146 relevant publications from the DBLP Computer Science Bibliography up to the end of 2024, following established systematic literature review guidelines. We explored publication trends, categorized them based on software engineering topics, and reported findings based on those topics.
  Results: The most popular venues for publishing software engineering research on Jupyter notebooks are related to human-computer interaction instead of traditional software engineering venues. Researchers have addressed a wide range of software engineering topics on notebooks, such as code reuse, readability, and execution environment. Although reusability is one of the research topics for Jupyter notebooks, only 64 of the 146 studies can be reused based on their provided URLs. Additionally, most replication packages are not hosted on permanent repositories for long-term availability and adherence to open science principles.
  Conclusion: Solutions specific to notebooks for software engineering issues, including testing, refactoring, and documentation, are underexplored. Future research opportunities exist in automatic testing frameworks, refactoring clones between notebooks, and generating group documentation for coherent code cells.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Doubly Stochastic Transformers</title>
<link>https://arxiv.org/abs/2504.16275</link>
<guid>https://arxiv.org/abs/2504.16275</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer, Softmax, Sinkhorn algorithm, doubly stochastic matrix, variational quantum circuit <br />
Summary: 
The study introduces a hybrid classical-quantum Doubly Stochastic Transformer (QDSFormer) that incorporates a variational quantum circuit in place of the Softmax in the attention layer of the Transformer model. The parametric quantum circuit used in the QDSFormer enhances the expressive power of the model, resulting in more diverse Doubly Stochastic Matrices (DSMs) that preserve information better than classical operators. Through experiments on small-scale object recognition tasks, the QDSFormer outperforms standard Vision Transformers and other doubly stochastic Transformers. Additionally, a quantum-inspired doubly stochastic Transformer based on QR decomposition is also compared. The QDSFormer demonstrates improved training stability and lower performance variation, suggesting that it could mitigate instability during training on small-scale data. These findings highlight the potential of quantum-inspired approaches in enhancing the performance and stability of Transformer models.
<br /><br />Summary: <div>
arXiv:2504.16275v1 Announce Type: cross 
Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often destabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard Vision Transformer and other doubly stochastic Transformers. Beyond the established Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. The QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixing Data-Driven and Physics-Based Constitutive Models using Uncertainty-Driven Phase Fields</title>
<link>https://arxiv.org/abs/2504.16713</link>
<guid>https://arxiv.org/abs/2504.16713</guid>
<content:encoded><![CDATA[
<div> surrogate modeling, multiscale models, data-driven, phase-field model, Gaussian Process surrogate <br />
Summary:
This article presents an adaptive mixture approach for accelerating multiscale models using data-driven surrogate modeling techniques. The approach involves using a fast probabilistic surrogate model for most of the computational domain, switching to the true high-fidelity model only when necessary. By incorporating phases in the computational domain and utilizing a phase-field model driven by surrogate uncertainty, the transition between models is smooth and accurate. The method reduces the time required to collect a large training dataset, while still maintaining high accuracy in simulations. The study compares this approach to a purely local model and demonstrates its effectiveness using a Gaussian Process surrogate for an elasto-plastic material. The adaptive mixture of models shows great potential for speeding up multiscale simulations without compromising accuracy or stability. <br /> <div>
arXiv:2504.16713v1 Announce Type: cross 
Abstract: There is a high interest in accelerating multiscale models using data-driven surrogate modeling techniques. Creating a large training dataset encompassing all relevant load scenarios is essential for a good surrogate, yet the computational cost of producing this data quickly becomes a limiting factor. Commonly, a pre-trained surrogate is used throughout the computational domain. Here, we introduce an alternative adaptive mixture approach that uses a fast probabilistic surrogate model as constitutive model when possible, but resorts back to the true high-fidelity model when necessary. The surrogate is thus not required to be accurate for every possible load condition, enabling a significant reduction in the data collection time. We achieve this by creating phases in the computational domain corresponding to the different models. These phases evolve using a phase-field model driven by the surrogate uncertainty. When the surrogate uncertainty becomes large, the phase-field model causes a local transition from the surrogate to the high-fidelity model, maintaining a highly accurate simulation. We discuss the requirements of this approach to achieve accurate and numerically stable results and compare the phase-field model to a purely local approach that does not enforce spatial smoothness for the phase mixing. Using a Gaussian Process surrogate for an elasto-plastic material, we demonstrate the potential of this mixture of models to accelerate multiscale simulations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements in Constitutive Model Calibration: Leveraging the Power of Full-Field DIC Measurements and In-Situ Load Path Selection for Reliable Parameter Inference</title>
<link>https://arxiv.org/abs/2411.07310</link>
<guid>https://arxiv.org/abs/2411.07310</guid>
<content:encoded><![CDATA[
<div> calibration, material characterization, computational engineering, Bayesian inference, uncertainty quantification
<br />
Summary:
Interlaced Characterization and Calibration (ICC) presents an improved workflow for material model calibration, addressing current limitations. It efficiently uses full-field data for calibration, aligns collected data with optimal experimental design, quantifies parameter uncertainty through Bayesian inference, and incorporates a real-time feedback loop. Demonstrated on an aluminum cruciform specimen, ICC utilizes Bayesian optimal experimental design for selecting load steps, principal component analysis for reducing data dimensions, and fast surrogate models for computational efficiency. This framework allows for reliable calibration of high-fidelity constitutive models with quantified uncertainty. By advancing the state-of-the-art in material characterization and model calibration, ICC supports credible decision-making in solid mechanics modeling, potentially increasing modeling agility. 
<br /> <div>
arXiv:2411.07310v3 Announce Type: replace 
Abstract: Accurate material characterization and model calibration are essential for computationally-supported engineering decisions. Current characterization and calibration methods (1) use simplified test specimen geometries and global data, (2) cannot guarantee that sufficient characterization data is collected for a specific model of interest, (3) use deterministic methods that provide best-fit parameter values with no uncertainty quantification, and (4) are sequential, inflexible, and time-consuming. This work brings together several recent advancements into an improved workflow called Interlaced Characterization and Calibration that advances the state-of-the-art in constitutive model calibration. The ICC paradigm (1) efficiently uses full-field data to calibrate a high-fidelity material model, (2) aligns the data needed with the data collected with an optimal experimental design protocol, (3) quantifies parameter uncertainty through Bayesian inference, and (4) incorporates these advances into a quasi real-time feedback loop. The ICC framework is demonstrated on the calibration of a material model using simulated full-field data for an aluminum cruciform specimen being deformed bi-axially. The cruciform is actively driven through the myopically optimal load path using Bayesian optimal experimental design, which selects load steps that yield the maximum expected information gain. To aid in numerical stability and preserve computational resources, the full-field data is dimensionally reduced via principal component analysis, and fast surrogate models which approximate the input-output relationships of the expensive finite element model are used. The tools demonstrated here show that high-fidelity constitutive models can be efficiently and reliably calibrated with quantified uncertainty, thus supporting credible decision-making and potentially increasing the agility of solid mechanics modeling.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Optimization of Physics-Informed Neural Networks: Advancing Generalizability by the Baldwin Effect</title>
<link>https://arxiv.org/abs/2312.03243</link>
<guid>https://arxiv.org/abs/2312.03243</guid>
<content:encoded><![CDATA[
<div> evolutionary selection, physics-informed neural networks, meta-learning, prediction accuracy, computational efficiency
<br />
Summary:
This paper introduces a novel approach to enhance the generalizability of Physics-Informed Neural Networks (PINNs) by incorporating principles of Baldwinian evolution. The proposed Baldwinian-PINNs are pre-wired with connection strengths to enable efficient learning of physics tasks. By utilizing a two-stage stochastic programming formulation that combines evolutionary selection pressure and lifetime learning, these evolved PINNs exhibit fast and accurate predictions across various physics problems. The Baldwinian-PINNs outperform state-of-the-art gradient-based meta-learning methods by achieving over a 70x improvement in accuracy while requiring 700x less computational time when solving the diffusion-reaction equation. This research marks a significant advancement in meta-learning for PINNs, providing a more generalizable and efficient approach to solving complex physics simulations. Sample codes for implementation are available on GitHub for further exploration. 
<br /> <div>
arXiv:2312.03243v3 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) are at the forefront of scientific machine learning, making possible the creation of machine intelligence that is cognizant of physical laws and able to accurately simulate them. However, today's PINNs are often trained for a single physics task and require computationally expensive re-training for each new task, even for tasks from similar physics domains. To address this limitation, this paper proposes a pioneering approach to advance the generalizability of PINNs through the framework of Baldwinian evolution. Drawing inspiration from the neurodevelopment of precocial species that have evolved to learn, predict and react quickly to their environment, we envision PINNs that are pre-wired with connection strengths inducing strong biases towards efficient learning of physics. A novel two-stage stochastic programming formulation coupling evolutionary selection pressure (based on proficiency over a distribution of physics tasks) with lifetime learning (to specialize on a sampled subset of those tasks) is proposed to instantiate the Baldwin effect. The evolved Baldwinian-PINNs demonstrate fast and physics-compliant prediction capabilities across a range of empirically challenging problem instances with more than an order of magnitude improvement in prediction accuracy at a fraction of the computation cost compared to state-of-the-art gradient-based meta-learning methods. For example, when solving the diffusion-reaction equation, a 70x improvement in accuracy was obtained while taking 700x less computational time. This paper thus marks a leap forward in the meta-learning of PINNs as generalizable physics solvers. Sample codes are available at https://github.com/chiuph/Baldwinian-PINN.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dual-stage constitutive modeling framework based on finite strain data-driven identification and physics-augmented neural networks</title>
<link>https://arxiv.org/abs/2504.15492</link>
<guid>https://arxiv.org/abs/2504.15492</guid>
<content:encoded><![CDATA[
<div> approach; hyperelastic constitutive models; Data-Driven Identification; Physics-Augmented Neural Network; Finite Element simulations <br />
Summary: 
This study introduces a novel dual-stage approach for generating hyperelastic constitutive models using only experimentally measurable data. The first step involves applying a Data-Driven Identification (DDI) formulation to identify stress-strain tuples based on measured displacement fields. In the second step, a Physics-Augmented Neural Network (PANN) is calibrated using the data set, ensuring compliance with hyperelasticity conditions while maintaining flexibility. The approach is validated through synthetic 2D data generated in virtual experiments and applied in 3D Finite Element simulations. Additionally, a real experiment with noisy data is successfully mimicked, showcasing the effectiveness of the proposed method for automated hyperelastic model generation. <br /> <div>
arXiv:2504.15492v1 Announce Type: new 
Abstract: In this contribution, we present a novel consistent dual-stage approach for the automated generation of hyperelastic constitutive models which only requires experimentally measurable data. To generate input data for our approach, an experiment with full-field measurement has to be conducted to gather testing force and corresponding displacement field of the sample. Then, in the first step of the dual-stage framework, a new finite strain Data-Driven Identification (DDI) formulation is applied. This method enables to identify tuples consisting of stresses and strains by only prescribing the applied boundary conditions and the measured displacement field. In the second step, the data set is used to calibrate a Physics-Augmented Neural Network (PANN), which fulfills all common conditions of hyperelasticity by construction and is very flexible at the same time. We demonstrate the applicability of our approach by several descriptive examples. Two-dimensional synthetic data are exemplarily generated in virtual experiments by using a reference constitutive model. The calibrated PANN is then applied in 3D Finite Element simulations. In addition, a real experiment including noisy data is mimicked.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Higher-Order Interpolation and Restriction in ExaHyPE Avoiding Non-physical Reflections</title>
<link>https://arxiv.org/abs/2504.15814</link>
<guid>https://arxiv.org/abs/2504.15814</guid>
<content:encoded><![CDATA[
<div> Wave equations, adaptive mesh refinement, ExaHyPE, ExaGRyPE, interpolation schemes<br />
Summary:<br />
Wave equations play an essential role in understanding various phenomena, but tracking them over regular meshes is computationally challenging given their large scales. Adaptive mesh refinement (AMR) helps by creating higher-resolution meshes near regions of interest. ExaHyPE and ExaGRyPE are software engines developed to solve wave problems using AMR. To advance the mesh in time, a set of higher-order interpolation schemes was introduced to address errors near AMR boundaries and improve computational efficiency. By calculating derivatives at each coarse grid cell to approximate fine cells, these new methods outperform the traditional tensor-product approach in terms of speed and accuracy. This improvement was demonstrated in a benchmark simulation of a stationary black hole, where the errors near AMR boundaries were eliminated. <div>
arXiv:2504.15814v1 Announce Type: new 
Abstract: Wave equations help us to understand phenomena ranging from earthquakes to tsunamis. These phenomena materialise over very large scales. It would be computationally infeasible to track them over a regular mesh. Yet, since the phenomena are localised, adaptive mesh refinement (AMR) can be used to construct meshes with a higher resolution close to the regions of interest. ExaHyPE is a software engine created to solve wave problems using AMR, and we use it as baseline to construct our numerical relativity application called ExaGRyPE. To advance the mesh in time, we have to interpolate and restrict along resolution transitions in each and every time step. ExaHyPE's vanilla code version uses a d-linear tensor-product approach. In benchmarks of a stationary black hole this performs slowly and leads to errors in conserved quantities near AMR boundaries. We therefore introduce a set of higher-order interpolation schemes where the derivatives are calculated at each coarse grid cell to approximate the enclosed fine cells. The resulting methods run faster than the tensor-product approach. Most importantly, when running the stationary black hole simulation using the higher order methods the errors near the AMR boundaries are removed.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AneuPy: An open source Python tool for creating simulation-ready geometries of abdominal aortic aneurysms</title>
<link>https://arxiv.org/abs/2504.15285</link>
<guid>https://arxiv.org/abs/2504.15285</guid>
<content:encoded><![CDATA[
<div> AAA, abdominal aortic aneurysm, geometrical models, open-source software, AneuPy <br />
Summary:
Abdominal aortic aneurysms (AAAs) are a serious health concern, particularly among older individuals, due to their potential for life-threatening rupture. The geometric characteristics of AAAs, such as maximum diameter and wall thickness, are vital for assessing rupture risk. However, there is a lack of open-source software for generating simulation-ready AAA geometries. AneuPy, a Python-based tool, addresses this gap by automating the generation of idealized and patient-specific AAA geometrical models with minimal input data. By facilitating the creation of simulation-ready geometries for biomechanical and hemodynamic analyses, AneuPy aims to enhance research in AAA and improve patient-specific risk assessment. Its efficiency and flexibility make it a valuable tool for modeling AAA using finite element analysis (FEA), computational fluid dynamics (CFD), or fluid-structure interaction (FSI) methods. <br /><br />Summary: <div>
arXiv:2504.15285v1 Announce Type: cross 
Abstract: Abdominal aortic aneurysms (AAAs) are localized dilations of the abdominal aorta that can lead to life-threatening rupture if left untreated. AAAs predominantly affect older individuals, with a high mortality rate upon rupture, making early diagnosis and risk assessment critical. The geometric characteristics of an AAA, such as its maximum diameter, asymmetry, and wall thickness, play a crucial role in biomechanical models used to assess rupture risk. Despite the growing use of computational modeling to study AAAs, there is a lack of open source software that facilitates the generation of simulation-ready geometries tailored for biomechanical and hemodynamic analyses. To address this need, we introduce AneuPy, an open-source Python-based tool designed to generate idealized and patient-specific AAA geometrical models. AneuPy provides an efficient and automated approach to aneurysm geometry generation, requiring minimal input data while allowing for flexible parameterization. By streamlining the creation of simulation-ready geometries for finite element analysis (FEA), computational fluid dynamics (CFD), or fluid-structure interaction (FSI) models, AneuPy aims to facilitate research in AAAs and enhance patient-specific risk assessment.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trading Devil RL: Backdoor attack via Stock market, Bayesian Optimization and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.17908</link>
<guid>https://arxiv.org/abs/2412.17908</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, data poisoning, backdoor attack, detection methods, reinforcement learning

Summary:
The article explores the potential impact of large language models utilizing reinforcement learning in various fields, such as finance, physics, and artificial intelligence ecosystems. Specifically, the focus is on a backdoor attack named FinanceLLMsBackRL, which targets data poisoning without prior triggers. This attack poses a threat to financial institutions using reinforcement learning to simulate scenarios for models before and after regular operations. The study proposes a method for detecting such attacks through dynamic systems and statistical analysis of data distribution. Through this research, the authors aim to shed light on the vulnerabilities that may arise from the integration of large language models with reinforcement learning in critical sectors. This work underscores the importance of developing robust security measures to safeguard against potential threats in the evolving landscape of artificial intelligence systems. 

<br /><br />Summary: <div>
arXiv:2412.17908v3 Announce Type: replace-cross 
Abstract: With the rapid development of generative artificial intelligence, particularly large language models a number of sub-fields of deep learning have made significant progress and are now very useful in everyday applications. For example,financial institutions simulate a wide range of scenarios for various models created by their research teams using reinforcement learning, both before production and after regular operations. In this work, we propose a backdoor attack that focuses solely on data poisoning and a method of detection by dynamic systems and statistical analysis of the distribution of data. This particular backdoor attack is classified as an attack without prior consideration or trigger, and we name it FinanceLLMsBackRL. Our aim is to examine the potential effects of large language models that use reinforcement learning systems for text production or speech recognition, finance, physics, or the ecosystem of contemporary artificial intelligence models.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Aware Compression of Plasma Distribution Functions with GPU-Accelerated Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2504.14897</link>
<guid>https://arxiv.org/abs/2504.14897</guid>
<content:encoded><![CDATA[
<div> compression, plasma simulations, Gaussian Mixture Models, in-situ, real-time <br />
<br />
Summary: 
This article introduces a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) for large-scale plasma simulations. By approximating electron and ion velocity distribution functions with GMMs, the method captures plasma features like mean velocity and temperature, enabling identification of heating processes and beam generation. The approach involves constructing a histogram to reduce computational overhead and implementing GPU-accelerated, in-situ GMM fitting within the iPIC3D simulator for real-time compression. The compressed representation is stored using the ADIOS 2 library to optimize the I/O process. Compared to other algorithms like SZ, MGARD, and BLOSC2, the GMM-based method retains a physics-based approach, preserving the physical interpretation of plasma phenomena while achieving compression ratios of up to $10^4 and processing times comparable to standard compression engines. <div>
arXiv:2504.14897v1 Announce Type: new 
Abstract: Data compression is a critical technology for large-scale plasma simulations. Storing complete particle information requires Terabyte-scale data storage, and analysis requires ad-hoc scalable post-processing tools. We propose a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) to approximate electron and ion velocity distribution functions with a number of Gaussian components. This GMM-based method allows us to capture plasma features such as mean velocity and temperature, and it enables us to identify heating processes and generate beams. We first construct a histogram to reduce computational overhead and apply GPU-accelerated, in-situ GMM fitting within \texttt{iPIC3D}, a large-scale implicit Particle-in-Cell simulator, ensuring real-time compression. The compressed representation is stored using the \texttt{ADIOS 2} library, thus optimizing the I/O process. The GPU and histogramming implementation provides a significant speed-up with respect to GMM on particles (both in time and required memory at run-time), enabling real-time compression. Compared to algorithms like SZ, MGARD, and BLOSC2, our GMM-based method has a physics-based approach, retaining the physical interpretation of plasma phenomena such as beam formation, acceleration, and heating mechanisms. Our GMM algorithm achieves a compression ratio of up to $10^4$, requiring a processing time comparable to, or even lower than, standard compression engines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A parallel implementation of reduced-order modeling of large-scale systems</title>
<link>https://arxiv.org/abs/2504.14338</link>
<guid>https://arxiv.org/abs/2504.14338</guid>
<content:encoded><![CDATA[
<div> algorithm, distributed, reduced-order models, aerospace engineering, parallel

Summary:<br />
This paper introduces distributed Operator Inference (dOpInf), a parallel algorithm designed for constructing physics-based reduced-order models (ROMs) in large-scale aerospace engineering simulations. The algorithm efficiently processes high-dimensional datasets using distributed computing, enabling the learning of structured ROMs that approximate underlying dynamical systems. dOpInf is scalable, allowing for fully parallelized reduced modeling on thousands of processors. The resulting ROMs are computationally inexpensive, suitable for tasks such as design exploration, risk assessment, and uncertainty quantification. A tutorial using a 2D Navier-Stokes flow case study is provided to guide users through implementation, making dOpInf accessible for integration into complex aerospace simulations.<br /> <div>
arXiv:2504.14338v1 Announce Type: cross 
Abstract: Motivated by the large-scale nature of modern aerospace engineering simulations, this paper presents a detailed description of distributed Operator Inference (dOpInf), a recently developed parallel algorithm designed to efficiently construct physics-based reduced-order models (ROMs) for problems with large state dimensions. One such example is the simulation of rotating detonation rocket engines, where snapshot data generated by high-fidelity large-eddy simulations have many millions of degrees of freedom. dOpInf enables, via distributed computing, the efficient processing of datasets with state dimensions that are too large to process on a single computer, and the learning of structured physics-based ROMs that approximate the dynamical systems underlying those datasets. All elements of dOpInf are scalable, leading to a fully parallelized reduced modeling approach that can scale to the thousands of processors available on leadership high-performance computing platforms. The resulting ROMs are computationally cheap, making them ideal for key engineering tasks such as design space exploration, risk assessment, and uncertainty quantification. To illustrate the practical application of dOpInf, we provide a step-by-step tutorial using a 2D Navier-Stokes flow over a step scenario as a case study. This tutorial guides users through the implementation process, making dOpInf accessible for integration into complex aerospace engineering simulations.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
<div> Dialogue framework, teaching capabilities, evaluation, language models, pedagogical effectiveness<br />
Summary:<br />
EducationQ is a multi-agent dialogue framework designed to assess the teaching capabilities of large language models (LLMs). Evaluating 14 LLMs from major AI organizations, the study found that teaching effectiveness does not solely rely on model scale or general reasoning abilities. Smaller open-source models sometimes outperformed larger commercial ones in educational contexts. The research highlighted a gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Through a mixed-methods approach combining quantitative metrics, qualitative analysis, and expert case studies, distinct pedagogical strengths of top-performing models were identified. Human expert evaluations supported the automated qualitative analysis of effective teaching behaviors. The study suggests that LLMs-as-teachers require specialized optimization beyond simple scaling, indicating a need for targeted enhancement of specific pedagogical effectiveness in future educational AI development.<br /> 
Summary: <div>
arXiv:2504.14928v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues</title>
<link>https://arxiv.org/abs/2504.14963</link>
<guid>https://arxiv.org/abs/2504.14963</guid>
<content:encoded><![CDATA[
<div> acoustic features, speaker identification, textual data, fuzzy fingerprints, pre-trained models
Summary:
This study explores the use of fuzzy fingerprints from large pre-trained models to enhance text-based speaker identification. By incorporating speaker-specific tokens and context-aware modeling, the accuracy of speaker identification from text reaches up to 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. The approach of using fuzzy fingerprints enables approximation of full fine-tuning performance with fewer hidden units, enhancing interpretability. The analysis includes a mechanism to identify speaker-agnostic lines and handle ambiguous utterances. The study highlights the challenges in text-based speaker identification and offers insights for future improvements. 
<br /><br />Summary: <div>
arXiv:2504.14963v1 Announce Type: cross 
Abstract: Speaker identification using voice recordings leverages unique acoustic features, but this approach fails when only textual data is available. Few approaches have attempted to tackle the problem of identifying speakers solely from text, and the existing ones have primarily relied on traditional methods. In this work, we explore the use of fuzzy fingerprints from large pre-trained models to improve text-based speaker identification. We integrate speaker-specific tokens and context-aware modeling, demonstrating that conversational context significantly boosts accuracy, reaching 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering improved interpretability. Finally, we analyze ambiguous utterances and propose a mechanism to detect speaker-agnostic lines. Our findings highlight key challenges and provide insights for future improvements in text-based speaker identification.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends</title>
<link>https://arxiv.org/abs/2405.08542</link>
<guid>https://arxiv.org/abs/2405.08542</guid>
<content:encoded><![CDATA[
<div> Metaverse, Industrial Production, XR, Blockchain, AI

Summary:
The article explores the potential of the Industrial Metaverse in enhancing industrial production through technologies such as XR, blockchain, AI, digital twin, and 6G. It discusses the advantages of using the Metaverse in industrial settings and the key enabling technologies for various aspects of production operations. Challenges such as security concerns, resource limitations, and interoperability issues are identified, along with existing solutions to address them. The article also outlines future research directions and open issues in the Industrial Metaverse as it continues to evolve in the industrial production field. <div>
arXiv:2405.08542v2 Announce Type: replace 
Abstract: As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks. This deployment, called the Industrial Metaverse, is used for product design, production operations, industrial quality inspection, and product testing. However, there lacks of in-depth understanding of the enabling technologies associated with the Industrial Metaverse. This encompasses both the precise industrial scenarios targeted by each technology and the potential migration of technologies developed in other domains to the industrial sector. Driven by this issue, in this article, we conduct a comprehensive survey of the state-of-the-art literature on the Industrial Metaverse. Specifically, we first analyze the advantages of the Metaverse for industrial production. Then, we review a collection of key enabling technologies of the Industrial Metaverse, including blockchain (BC), digital twin (DT), 6G, XR, and artificial intelligence (AI), and analyze how these technologies can support different aspects of industrial production. Subsequently, we present numerous formidable challenges encountered within the Industrial Metaverse, including confidentiality and security concerns, resource limitations, and interoperability constraints. Furthermore, we investigate the extant solutions devised to address them. Finally, we briefly outline several open issues and future research directions of the Industrial Metaverse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An optimization-based coupling of reduced order models with efficient reduced adjoint basis generation approach</title>
<link>https://arxiv.org/abs/2408.14450</link>
<guid>https://arxiv.org/abs/2408.14450</guid>
<content:encoded><![CDATA[
<div> Optimization-based coupling, Lagrange multiplier, multiple modeling, simulation, reduced order models<br />
<br />
Summary: Optimization-based coupling (OBC) offers a promising approach in various modeling and simulation scenarios but faces challenges in time-dependent problems due to computational costs. This paper introduces an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission issue, utilizing reduced order models to alleviate computational burdens. The "optimize-then-reduce" strategy is employed to solve the minimization problem at each time step efficiently. A key innovation is the development of a technique for effective adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. Numerical experiments validate the accuracy of the proposed approach while comparing different methods for selecting reduced order bases for adjoint systems. Criteria such as decay of snapshot energy, average iteration counts, and timings are assessed to demonstrate the effectiveness of the optimization-based ROM-ROM coupling. <div>
arXiv:2408.14450v2 Announce Type: replace 
Abstract: Optimization-based coupling (OBC) is an attractive alternative to traditional Lagrange multiplier approaches in multiple modeling and simulation contexts. However, application of OBC to time-dependent problems has been hindered by the computational cost of finding the stationary points of the associated Lagrangian, which requires primal and adjoint solves. This issue can be mitigated by using OBC in conjunction with computationally efficient reduced order models (ROM). To demonstrate the potential of this combination, in this paper we develop an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission problem. We pursue the ``optimize-then-reduce'' path towards solving the minimization problem at each timestep and solve reduced-space adjoint system of equations, where the main challenge in this formulation is the generation of adjoint snapshots and reduced bases for the adjoint systems required by the optimizer. One of the main contributions of the paper is a new technique for efficient adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. We present numerical studies demonstrating the accuracy of the approach along with comparison between various approaches for selecting a reduced order basis for the adjoint systems, including decay of snapshot energy, average iteration counts, and timings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioFit: A WebGL-Based Tool for Fast and Efficient Parameterization of Cardiac Action Potential Models to Fit User-Provided Data</title>
<link>https://arxiv.org/abs/2504.13274</link>
<guid>https://arxiv.org/abs/2504.13274</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiac action potential, Particle swarm optimization, Interactive tool, Model parameter fitting, Physiological dynamics

Summary:
Interactive browser-based tool presented in this study utilizes particle swarm optimization (PSO) algorithm to find model parameter sets that accurately reproduce cardiac dynamics from user-provided data. The tool offers rapid customization and can achieve low-error fittings in a few iterations, requiring only a few seconds of runtime on typical machines. Users can select parameters to fit, define their value ranges, and adjust PSO algorithm hyperparameters through a user-friendly webpage interface. Various models were successfully fitted to different datasets, demonstrating the tool's versatility and efficiency. Convergence of fitting is influenced by the choice of model, dataset characteristics, and PSO algorithm settings. Through these fittings, new insights were gained regarding the physiological and dynamical implications of the model parameters.  <br /><br />Summary: <div>
arXiv:2504.13274v1 Announce Type: new 
Abstract: Cardiac action potential models allow examination of a variety of cardiac dynamics, including how behavior may change under specific interventions. To study a specific scenario, including patient-specific cases, model parameter sets must be found that accurately reproduce the dynamics of interest. To facilitate this complex and time-consuming process, we present an interactive browser-based tool that uses the particle swarm optimization (PSO) algorithm implemented in JavaScript and taking advantage of the WebGL API for hardware acceleration. Our tool allows rapid customization and can find low-error fittings to user-provided voltage time series or action potential duration data from multiple cycle lengths in a few iterations (10-32), corresponding to a runtime of a few seconds on most machines. Additionally, our tool focuses on ease of use and flexibility, providing a webpage interface that allows users to select a subset of parameters to fit, set the range of values each parameter is allowed to assume, and control the PSO algorithm hyperparameters. We demonstrate our tool's utility by fitting a variety of models to different datasets, showing how convergence is affected by model choice, dataset properties, and PSO algorithmic settings, and explaining new insights gained about the physiological and dynamical roles of the model parameters.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ascribe New Dimensions to Scientific Data Visualization with VR</title>
<link>https://arxiv.org/abs/2504.13448</link>
<guid>https://arxiv.org/abs/2504.13448</guid>
<content:encoded><![CDATA[
<div> Keywords: computer mouse, scientific images, Virtual Reality, AI-driven algorithms, multimodal analysis<br />
<br />
Summary: <br />
The article discusses the limitations of the traditional 2D visualization methods in exploring complex, multi-scale scientific images using a computer mouse. It introduces ASCRIBE-VR, a Virtual Reality platform that integrates AI-driven algorithms with scientific images. ASCRIBE-VR allows for immersive and interactive visualization, supporting the analysis of advanced datasets such as X-ray CT and Magnetic Resonance. The VR tools are compatible with Meta Quest and can seamlessly explore large-scale 3D images by merging AI-generated results with VR visualization. This integration enhances scientific discovery by bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins. <div>
arXiv:2504.13448v1 Announce Type: cross 
Abstract: For over half a century, the computer mouse has been the primary tool for interacting with digital data, yet it remains a limiting factor in exploring complex, multi-scale scientific images. Traditional 2D visualization methods hinder intuitive analysis of inherently 3D structures. Virtual Reality (VR) offers a transformative alternative, providing immersive, interactive environments that enhance data comprehension. This article introduces ASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research with Immersive Browsing \& Exploration, which integrates AI-driven algorithms with scientific images. ASCRIBE-VR enables multimodal analysis, structural assessments, and immersive visualization, supporting scientific visualization of advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D imaging. Our VR tools, compatible with Meta Quest, can consume the output of our AI-based segmentation and iterative feedback processes to enable seamless exploration of large-scale 3D images. By merging AI-generated results with VR visualization, ASCRIBE-VR enhances scientific discovery, bridging the gap between computational analysis and human intuition in materials research, connecting human-in-the-loop with digital twins.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Models Meet Financial Data Modalities</title>
<link>https://arxiv.org/abs/2504.13521</link>
<guid>https://arxiv.org/abs/2504.13521</guid>
<content:encoded><![CDATA[
<div> Keywords: algorithmic trading, deep learning, limit order book, high-frequency trading, predictive performance

Summary: 
This study explores the integration of deep learning models with various financial data sources to improve algorithmic trading strategies and portfolio optimization. By developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation, the researchers achieved state-of-the-art performance in high-frequency trading algorithms. This approach underscores the effectiveness of deep learning in handling structured financial data and enhancing predictive performance in trading strategies. The study highlights the potential of incorporating limit order book analysis into algorithmic trading using deep learning models. By leveraging deep learning techniques, the researchers were able to extract meaningful signals from diverse financial data sources, including candlestick charts, order statistics, traded volume data, and news flow. This research contributes to bridging the gap between deep learning and structured financial data analysis, showcasing the promise of deep learning in financial applications. 

Summary: <div>
arXiv:2504.13521v1 Announce Type: cross 
Abstract: Algorithmic trading relies on extracting meaningful signals from diverse financial data sources, including candlestick charts, order statistics on put and canceled orders, traded volume data, limit order books, and news flow. While deep learning has demonstrated remarkable success in processing unstructured data and has significantly advanced natural language processing, its application to structured financial data remains an ongoing challenge. This study investigates the integration of deep learning models with financial data modalities, aiming to enhance predictive performance in trading strategies and portfolio optimization. We present a novel approach to incorporating limit order book analysis into algorithmic trading by developing embedding techniques and treating sequential limit order book snapshots as distinct input channels in an image-based representation. Our methodology for processing limit order book data achieves state-of-the-art performance in high-frequency trading algorithms, underscoring the effectiveness of deep learning in financial applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bitcoin's Edge: Embedded Sentiment in Blockchain Transactional Data</title>
<link>https://arxiv.org/abs/2504.13598</link>
<guid>https://arxiv.org/abs/2504.13598</guid>
<content:encoded><![CDATA[
<div> Keywords: Cryptocurrency, Blockchain, Natural Language Processing, Sentiment Analysis, Financial Predictions

Summary:
Blockchain technology is not only used for financial transactions but also for storing and sharing non-financial content. This hidden content can impact cryptocurrency price movements by conveying private information and shaping public sentiment. Current methods of analyzing blockchain data are limited, prompting the use of Natural Language Processing techniques to extract sentiment from transactional data. The study demonstrates the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency prices on Bitcoin and Ethereum blockchains. It uncovers an informational advantage for Bitcoin over Ethereum, showing that sentiment analysis can effectively predict price movements. This research highlights the value of blockchain sentiment analysis in enhancing financial predictions within cryptocurrency markets, providing a novel framework for leveraging freely available, transparent, and immutable data for informed decision-making in the digital asset space. 

<br /><br />Summary: <div>
arXiv:2504.13598v1 Announce Type: cross 
Abstract: Cryptocurrency blockchains, beyond their primary role as distributed payment systems, are increasingly used to store and share arbitrary content, such as text messages and files. Although often non-financial, this hidden content can impact price movements by conveying private information, shaping sentiment, and influencing public opinion. However, current analyses of such data are limited in scope and scalability, primarily relying on manual classification or hand-crafted heuristics. In this work, we address these limitations by employing Natural Language Processing techniques to analyze, detect patterns, and extract public sentiment encoded within blockchain transactional data. Using a variety of Machine Learning techniques, we showcase for the first time the predictive power of blockchain-embedded sentiment in forecasting cryptocurrency price movements on the Bitcoin and Ethereum blockchains. Our findings shed light on a previously underexplored source of freely available, transparent, and immutable data and introduce blockchain sentiment analysis as a novel and robust framework for enhancing financial predictions in cryptocurrency markets. Incidentally, we discover an asymmetry between cryptocurrencies; Bitcoin has an informational advantage over Ethereum in that the sentiment embedded into transactional data is sufficient to predict its price movement.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems</title>
<link>https://arxiv.org/abs/2504.13768</link>
<guid>https://arxiv.org/abs/2504.13768</guid>
<content:encoded><![CDATA[
<div> Graph neural network, multi-body dynamical systems, internal loads, predictive maintenance, digital twin <br />
Summary: Equi-Euler GraphNet is proposed for accurate real-time modeling of multi-body dynamical systems. It simultaneously predicts internal forces and global trajectories, crucial for fault detection and predictive maintenance in digital twin applications. It introduces equivariant message-passing and temporal-aware iterative node update mechanisms, tailored for cylindrical roller bearings. Trained on high-fidelity simulations, it generalizes well and outperforms state-of-the-art models in trajectory prediction. Equi-Euler GraphNet achieves efficient and stable rollouts over thousands of time steps with minimal error accumulation, providing a 200x speedup over conventional solvers. This makes it a valuable tool for digital twins, design, and maintenance applications. <br /> <div>
arXiv:2504.13768v1 Announce Type: cross 
Abstract: Accurate real-time modeling of multi-body dynamical systems is essential for enabling digital twin applications across industries. While many data-driven approaches aim to learn system dynamics, jointly predicting internal loads and system trajectories remains a key challenge. This dual prediction is especially important for fault detection and predictive maintenance, where internal loads-such as contact forces-act as early indicators of faults, reflecting wear or misalignment before affecting motion. These forces also serve as inputs to degradation models (e.g., crack growth), enabling damage prediction and remaining useful life estimation. We propose Equi-Euler GraphNet, a physics-informed graph neural network (GNN) that simultaneously predicts internal forces and global trajectories in multi-body systems. In this mesh-free framework, nodes represent system components and edges encode interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an equivariant message-passing scheme, interpreting edge messages as interaction forces consistent under Euclidean transformations; and (2) a temporal-aware iterative node update mechanism, based on Euler integration, to capture influence of distant interactions over time. Tailored for cylindrical roller bearings, it decouples ring dynamics from constrained motion of rolling elements. Trained on high-fidelity multiphysics simulations, Equi-Euler GraphNet generalizes beyond the training distribution, accurately predicting loads and trajectories under unseen speeds, loads, and configurations. It outperforms state-of-the-art GNNs focused on trajectory prediction, delivering stable rollouts over thousands of time steps with minimal error accumulation. Achieving up to a 200x speedup over conventional solvers while maintaining comparable accuracy, it serves as an efficient reduced-order model for digital twins, design, and maintenance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer Encoder and Multi-features Time2Vec for Financial Prediction</title>
<link>https://arxiv.org/abs/2504.13801</link>
<guid>https://arxiv.org/abs/2504.13801</guid>
<content:encoded><![CDATA[
<div> Transformers, financial prediction, time series analysis, attention mechanism, correlation feature selection<br />
<br />
Summary:<br />
This paper introduces a novel neural network architecture that combines Time2Vec with the Encoder of the Transformer model for financial prediction. By studying different markets, a correlation feature selection method is proposed to improve the accuracy of predicting multiple stock prices. Through fine-tuning hyperparameters, the method outperforms benchmark models and traditional encoding methods like positional encoding. The integration of Time2Vec with the Transformer model allows for the capture of both short and long-range dependencies, aiding in understanding broader market trends. Selecting correlation features further enhances prediction accuracy, particularly in industries where companies exhibit correlated stock price movements. <div>
arXiv:2504.13801v1 Announce Type: cross 
Abstract: Financial prediction is a complex and challenging task of time series analysis and signal processing, expected to model both short-term fluctuations and long-term temporal dependencies. Transformers have remarkable success mostly in natural language processing using attention mechanism, which also influenced the time series community. The ability to capture both short and long-range dependencies helps to understand the financial market and to recognize price patterns, leading to successful applications of Transformers in stock prediction. Although, the previous research predominantly focuses on individual features and singular predictions, that limits the model's ability to understand broader market trends. In reality, within sectors such as finance and technology, companies belonging to the same industry often exhibit correlated stock price movements.
  In this paper, we develop a novel neural network architecture by integrating Time2Vec with the Encoder of the Transformer model. Based on the study of different markets, we propose a novel correlation feature selection method. Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a comparative analysis of our results against benchmark models. We conclude that our method outperforms other state-of-the-art encoding methods such as positional encoding, and we also conclude that selecting correlation features enhance the accuracy of predicting multiple stock prices.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProteinGPT: Multimodal LLM for Protein Property Prediction and Structure Understanding</title>
<link>https://arxiv.org/abs/2408.11363</link>
<guid>https://arxiv.org/abs/2408.11363</guid>
<content:encoded><![CDATA[
<div> Keywords: ProteinGPT, large language model, protein sequences, protein structures, protein analysis <br />
Summary: ProteinGPT is a cutting-edge multimodal large language model designed for analyzing protein sequences and structures efficiently. By integrating protein sequence and structure encoders with a large language model, ProteinGPT can provide accurate and contextually relevant responses to protein-related queries. The model was trained on a dataset of 132,092 proteins, each annotated with property tags and QA pairs, using GPT-4o for instruction tuning. Experimental results show that ProteinGPT outperforms baseline models and general-purpose LLMs in understanding and responding to protein-related questions, achieving high performance on both semantic and lexical metrics. The code and data for ProteinGPT are available on GitHub, making this powerful tool accessible to researchers in the field. <br /><br /> <div>
arXiv:2408.11363v2 Announce Type: replace-cross 
Abstract: Understanding biological processes, drug development, and biotechnological advancements requires a detailed analysis of protein structures and functions, a task that is inherently complex and time-consuming in traditional protein research. To streamline this process, we introduce ProteinGPT, a state-of-the-art multimodal large language model for proteins that enables users to upload protein sequences and/or structures for comprehensive analysis and responsive inquiries. ProteinGPT integrates protein sequence and structure encoders with linear projection layers to ensure precise representation adaptation and leverages a large language model (LLM) to generate accurate, contextually relevant responses. To train ProteinGPT, we constructed a large-scale dataset of 132,092 proteins, each annotated with 20-30 property tags and 5-10 QA pairs per protein, and optimized the instruction-tuning process using GPT-4o. Experiments demonstrate that ProteinGPT effectively generates informative responses to protein-related questions, achieving high performance on both semantic and lexical metrics and significantly outperforming baseline models and general-purpose LLMs in understanding and responding to protein-related queries. Our code and data are available at https://github.com/ProteinGPT/ProteinGPT.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Accurate Prediction of Antenna Reflection Coefficients in Planar Layered Media Environment via Generalized Scattering Matrix</title>
<link>https://arxiv.org/abs/2504.12613</link>
<guid>https://arxiv.org/abs/2504.12613</guid>
<content:encoded><![CDATA[
<div> Keywords: reflection coefficient, antenna, planar layered medium, generalized scattering matrix, numerical algorithm

Summary: 
The article presents a new numerical algorithm for evaluating the reflection coefficient of an antenna in the presence of a planar layered medium. This algorithm utilizes the antenna's generalized scattering matrix (GSM) to model the interaction between the antenna and the medium through spherical-to-planar vector wave transformations. By avoiding approximations that could compromise accuracy, the algorithm reduces algebraic complexity and significantly speeds up antenna performance evaluation. While a one-time preprocessing cost is required to obtain the antenna's GSM in free space, the numerical evaluation speed of this method surpasses that of commercial software FEKO by several orders of magnitude, maintaining high accuracy. This advancement in computational efficiency offers a promising approach for accurately evaluating antenna performance in complex scenarios involving layered media. 

<br /><br />Summary: <div>
arXiv:2504.12613v1 Announce Type: new 
Abstract: The numerical algorithm for evaluating the reflection coefficient of an antenna in the presence of the planar layered medium is reformulated using the antenna's generalized scattering matrix (GSM). The interaction between the antenna and the layered medium is modeled through spherical-to-planar vector wave transformations, ensuring no approximations that could compromise computational accuracy. This theoretical framework significantly reduces algebraic complexity, resulting in a marked increase in the speed of antenna performance evaluation. Excluding the one-time preprocessing cost of obtaining the antenna's GSM in free space, the numerical evaluation speed of this method exceeds that of the commercial software FEKO by several orders of magnitude, while maintaining nearly identical accuracy.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning Strategies for 3D Engineering Regression Problems: A Benchmarking Study</title>
<link>https://arxiv.org/abs/2504.12503</link>
<guid>https://arxiv.org/abs/2504.12503</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, engineering design, continual learning, regression tasks, catastrophic forgetting

Summary:
Continual learning (CL) is introduced to engineering design to address the challenge of incorporating new knowledge into models without retraining from scratch, which is computationally expensive. This study benchmarks various CL methods on regression tasks using five engineering datasets and establishes nine new engineering CL benchmarks. The results show that existing CL methods can improve performance compared to naive baselines, with the Replay strategy achieving comparable performance to retraining while reducing training time significantly. This points to the potential of CL in enhancing real-world engineering workflows. The code and datasets used in the study will be made available for further research and applications. 

<br /><br />Summary: <div>
arXiv:2504.12503v1 Announce Type: cross 
Abstract: Engineering problems that apply machine learning often involve computationally intensive methods but rely on limited datasets. As engineering data evolves with new designs and constraints, models must incorporate new knowledge over time. However, high computational costs make retraining models from scratch infeasible. Continual learning (CL) offers a promising solution by enabling models to learn from sequential data while mitigating catastrophic forgetting, where a model forgets previously learned mappings. This work introduces CL to engineering design by benchmarking several CL methods on representative regression tasks. We apply these strategies to five engineering datasets and construct nine new engineering CL benchmarks to evaluate their ability to address forgetting and improve generalization. Preliminary results show that applying existing CL methods to these tasks improves performance over naive baselines. In particular, the Replay strategy achieved performance comparable to retraining in several benchmarks while reducing training time by nearly half, demonstrating its potential for real-world engineering workflows. The code and datasets used in this work will be available at: https://github.com/kmsamuel/cl-for-engineering-release.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational quantum and neural quantum states algorithms for the linear complementarity problem</title>
<link>https://arxiv.org/abs/2504.08141</link>
<guid>https://arxiv.org/abs/2504.08141</guid>
<content:encoded><![CDATA[
<div> variational quantum algorithms, VQAs, hybrid quantum-classical methods, variational quantum linear solver, VQLS

Summary:
Variational quantum algorithms (VQAs) are being explored as a way to harness quantum computing while addressing current hardware limitations. This study introduces the variational quantum linear solver (VQLS) and its classical counterpart, the variational neural linear solver (VNLS), within a minimum map Newton solver for a rigid body contact model. The VNLS accurately simulates the dynamics of rigid spherical bodies during collisions, suggesting that quantum and quantum-inspired linear algebra algorithms can be effective for modeling certain physical systems. This research opens up the possibility of using VQAs and quantum-inspired classical algorithms to solve real-world problems, highlighting their potential utility in various applications. <br /><br />Summary: <div>
arXiv:2504.08141v2 Announce Type: replace 
Abstract: Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design Editing for Offline Model-based Optimization</title>
<link>https://arxiv.org/abs/2405.13964</link>
<guid>https://arxiv.org/abs/2405.13964</guid>
<content:encoded><![CDATA[
<div> Offline model-based optimization, surrogate model, out-of-distribution issue, diffusion prior, Design Editing for Offline Model-based Optimization (DEMO)<br />
Summary:<br />
Offline model-based optimization (MBO) involves maximizing a black-box objective function using a dataset of designs and scores. Traditional methods use surrogate models but are prone to errors when predicting scores for new designs. To address this, DEMO introduces a diffusion prior to calibrate overly optimized designs. It generates pseudo design candidates using gradient ascent, then refines them through an editing process involving noise and denoising with the diffusion prior. Empirical evaluations across seven tasks show that DEMO, with proper tuning, achieves competitive scores compared to previous literature. <div>
arXiv:2405.13964v4 Announce Type: replace-cross 
Abstract: Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. While these pseudo design candidates contain information beyond the offline dataset, they might be invalid or have erroneously high predicted scores. Therefore, to address this challenge while utilizing the information provided by pseudo design candidates, we propose an editing process to refine these pseudo design candidates. We introduce noise to the pseudo design candidates and subsequently denoise them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. Empirical evaluations on seven offline MBO tasks show that, with properly tuned hyperparameters, DEMOs score is competitive with the best previously reported scores in the literature.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Contextual Market Equilibrium Computation through Deep Learning</title>
<link>https://arxiv.org/abs/2406.15459</link>
<guid>https://arxiv.org/abs/2406.15459</guid>
<content:encoded><![CDATA[
<div> Market equilibrium, computational economics, large-scale buyer population, contextual market model, deep learning-based method<br />
<br />
Summary: 
The paper explores the computation of market equilibrium in scenarios with a large-scale buyer population using a deep learning-based method called MarketFCNet. The approach introduces a contextual market model where buyers and goods are represented by their contexts. MarketFCNet uses a neural network to parameterize the allocation of goods to buyers based on their contexts. An efficient method is proposed to estimate the loss function for training the network, enabling optimization through gradient descent. The Nash Gap metric is introduced to measure the deviation of the allocation and prices from the market equilibrium. Experimental results show that MarketFCNet outperforms existing methods in terms of performance and running times as the market scale increases, showcasing the potential of deep learning in approximating large-scale contextual market equilibrium. <br /><br /> <div>
arXiv:2406.15459v2 Announce Type: replace-cross 
Abstract: Market equilibrium is one of the most fundamental solution concepts in economics and social optimization analysis. Existing works on market equilibrium computation primarily focus on settings with relatively few buyers. Motivated by this, our paper investigates the computation of market equilibrium in scenarios with a large-scale buyer population, where buyers and goods are represented by their contexts. Building on this realistic and generalized contextual market model, we introduce MarketFCNet, a deep learning-based method for approximating market equilibrium. We start by parameterizing the allocation of each good to each buyer using a neural network, which depends solely on the context of the buyer and the good. Next, we propose an efficient method to unbiasedly estimate the loss function of the training algorithm, enabling us to optimize the network parameters through gradient. To evaluate the approximated solution, we propose a metric called Nash Gap, which quantifies the deviation of the given allocation and price pair from the market equilibrium. Experimental results indicate that MarketFCNet delivers competitive performance and significantly lower running times compared to existing methods as the market scale expands, demonstrating the potential of deep learning-based methods to accelerate the approximation of large-scale contextual market equilibrium.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DrivAerML: High-Fidelity Computational Fluid Dynamics Dataset for Road-Car External Aerodynamics</title>
<link>https://arxiv.org/abs/2408.11969</link>
<guid>https://arxiv.org/abs/2408.11969</guid>
<content:encoded><![CDATA[
<div> Machine Learning, automotive aerodynamics, open-source dataset, high-fidelity CFD, DrivAer notchback<br />
Summary:<br />
Machine Learning has the potential to transform automotive aerodynamics by providing quick flow predictions during the design phase. However, a lack of open-source training data for realistic road cars using high-fidelity CFD methods is impeding progress. To overcome this hurdle, a high-fidelity open-source dataset for automotive aerodynamics has been created. The dataset comprises 500 variants of the DrivAer notchback vehicle, generated through parametric morphing. Mesh generation and scale-resolving CFD were conducted using state-of-the-art automated workflows. The dataset, published under the CC-BY-SA license, includes geometries and detailed aerodynamic information. This initiative marks the first large public-domain dataset for complex automotive configurations derived from high-fidelity CFD simulations. <div>
arXiv:2408.11969v2 Announce Type: replace-cross 
Abstract: Machine Learning (ML) has the potential to revolutionise the field of automotive aerodynamics, enabling split-second flow predictions early in the design process. However, the lack of open-source training data for realistic road cars, using high-fidelity CFD methods, represents a barrier to their development. To address this, a high-fidelity open-source (CC-BY-SA) public dataset for automotive aerodynamics has been generated, based on 500 parametrically morphed variants of the widely-used DrivAer notchback generic vehicle. Mesh generation and scale-resolving CFD was executed using consistent and validated automatic workflows representative of the industrial state-of-the-art. Geometries and rich aerodynamic data are published in open-source formats. To our knowledge, this is the first large, public-domain dataset for complex automotive configurations generated using high-fidelity CFD.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Algorithms for Option Hedging</title>
<link>https://arxiv.org/abs/2504.05521</link>
<guid>https://arxiv.org/abs/2504.05521</guid>
<content:encoded><![CDATA[
<div> Deep Reinforcement Learning, dynamic hedging, financial assets, risk, DRL algorithms<br />
<br />
Summary: 
Dynamic hedging involves offsetting financial risk through periodic transactions. This study compares the performance of eight DRL algorithms in dynamic hedging scenarios. The algorithms include Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), various Deep Q-Learning (DQL) and Deep Deterministic Policy Gradient (DDPG) variants. The experiments use a GJR-GARCH(1,1) model to simulate the dataset and evaluate against the Black-Scholes delta hedge baseline. Results show that MCPG and PPO perform best in terms of the root semi-quadratic penalty. MCPG outperforms the baseline within the allotted computational budget due to sparse rewards in the environment. This comparison provides insights into the effectiveness of different DRL algorithms in dynamic hedging strategies, with potential implications for financial risk management. 
<br /><br />Summary: <div>
arXiv:2504.05521v2 Announce Type: replace-cross 
Abstract: Dynamic hedging is a financial strategy that consists in periodically transacting one or multiple financial assets to offset the risk associated with a correlated liability. Deep Reinforcement Learning (DRL) algorithms have been used to find optimal solutions to dynamic hedging problems by framing them as sequential decision-making problems. However, most previous work assesses the performance of only one or two DRL algorithms, making an objective comparison across algorithms difficult. In this paper, we compare the performance of eight DRL algorithms in the context of dynamic hedging; Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), along with four variants of Deep Q-Learning (DQL) and two variants of Deep Deterministic Policy Gradient (DDPG). Two of these variants represent a novel application to the task of dynamic hedging. In our experiments, we use the Black-Scholes delta hedge as a baseline and simulate the dataset using a GJR-GARCH(1,1) model. Results show that MCPG, followed by PPO, obtain the best performance in terms of the root semi-quadratic penalty. Moreover, MCPG is the only algorithm to outperform the Black-Scholes delta hedge baseline with the allotted computational budget, possibly due to the sparsity of rewards in our environment.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal integration of chemical structures improves representations of microscopy images for morphological profiling</title>
<link>https://arxiv.org/abs/2504.09544</link>
<guid>https://arxiv.org/abs/2504.09544</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, deep learning, morphological profiling, high-throughput microscopy screens, chemical compound structure

Summary:
Recent advances in self-supervised deep learning have led to improved quantification of cellular morphological changes in high-throughput microscopy screens. The new framework, MICON (Molecular-Image Contrastive Learning), incorporates chemical compound information during pre-training to enhance learned image representations. MICON surpasses traditional feature extraction methods like CellProfiler and existing deep learning approaches, especially in identifying consistent effects of drugs across independent replicates and data centers. By modeling chemical compounds as treatments inducing counterfactual transformations of cell phenotypes, MICON outperforms methods that directly align images and compounds. This highlights the importance of considering the multimodal nature of microscopy screening data in representation learning for morphological profiling. MICON's success suggests a promising direction for future research in this field.<br /><br />Summary: <div>
arXiv:2504.09544v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised deep learning have improved our ability to quantify cellular morphological changes in high-throughput microscopy screens, a process known as morphological profiling. However, most current methods only learn from images, despite many screens being inherently multimodal, as they involve both a chemical or genetic perturbation as well as an image-based readout. We hypothesized that incorporating chemical compound structure during self-supervised pre-training could improve learned representations of images in high-throughput microscopy screens. We introduce a representation learning framework, MICON (Molecular-Image Contrastive Learning), that models chemical compounds as treatments that induce counterfactual transformations of cell phenotypes. MICON significantly outperforms classical hand-crafted features such as CellProfiler and existing deep-learning-based representation learning methods in challenging evaluation settings where models must identify reproducible effects of drugs across independent replicates and data-generating centers. We demonstrate that incorporating chemical compound information into the learning process provides consistent improvements in our evaluation setting and that modeling compounds specifically as treatments in a causal framework outperforms approaches that directly align images and compounds in a single representation space. Our findings point to a new direction for representation learning in morphological profiling, suggesting that methods should explicitly account for the multimodal nature of microscopy screening data.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A viscoplasticity model with an invariant-based non-Newtonian flow rule for unidirectional thermoplastic composites</title>
<link>https://arxiv.org/abs/2504.12069</link>
<guid>https://arxiv.org/abs/2504.12069</guid>
<content:encoded><![CDATA[
arXiv:2504.12069v1 Announce Type: new 
Abstract: A three-dimensional mesoscopic viscoplasticity model for simulating rate-dependent plasticity and creep in unidirectional thermoplastic composites is presented. The constitutive model is a transversely isotropic extension of an isotropic finite strain viscoplasticity model for neat polymers. Rate-dependent plasticity and creep are described by a non-Newtonian flow rule where the viscosity of the material depends on an equivalent stress measure through an Eyring-type relation. In the present formulation, transverse isotropy is incorporated by defining the equivalent stress measure and flow rule as functions of transversely isotropic stress invariants. In addition, the Eyring-type viscosity function is extended with anisotropic pressure dependence. As a result of the formulation, plastic flow in fiber direction is effectively excluded and pressure dependence of the polymer matrix is accounted for. The re-orientation of the transversely isotropic plane during plastic deformations is incorporated in the constitutive equations, allowing for an accurate large deformation response. The formulation is fully implicit and a consistent linearization of the algorithmic constitutive equations is performed to derive the consistent tangent modulus. The performance of the mesoscopic constitutive model is assessed through a comparison with a micromechanical model for carbon/PEEK, with the original isotropic viscoplastic version for the polymer matrix and with hyperelastic fibers. The micromodel is first used to determine the material parameters of the mesoscale model with a few stress-strain curves. It is demonstrated that the mesoscale model gives a similar response to the micromodel under various loading conditions. Finally, the mesoscale model is validated against off-axis experiments on unidirectional thermoplastic composite plies.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Material Network: Overview, applications and current directions</title>
<link>https://arxiv.org/abs/2504.12159</link>
<guid>https://arxiv.org/abs/2504.12159</guid>
<content:encoded><![CDATA[
arXiv:2504.12159v1 Announce Type: new 
Abstract: Deep Material Network (DMN) has emerged as a powerful framework for multiscale material modeling, enabling efficient and accurate predictions of material behavior across different length scales. Unlike traditional machine learning approaches, the trainable parameters in DMN have direct physical interpretations, capturing the geometric characteristics of the microstructure rather than serving as purely statistical fitting parameters. Its hierarchical tree structure effectively encodes microstructural interactions and deformation mechanisms, allowing DMN to achieve a balance between accuracy and computational efficiency. This physics-informed architecture significantly reduces computational costs compared to direct numerical simulations while preserving essential microstructural physics. Furthermore, DMN can be trained solely on a linear elastic dataset while effectively extrapolating nonlinear responses during online prediction, making it a highly efficient and scalable approach for multiscale material modeling. This article provides a comprehensive review of DMN, detailing its motivation, underlying methodology, and recent advancements. We discuss key modeling aspects, including its hierarchical structure, training process, and the role of physics-based constraints in enhancing predictive accuracy. Furthermore, we highlight its applications in component-scale multiscale analysis and inverse parameter identification, demonstrating its capability to bridge microscale material behavior with macroscale engineering predictions. Finally, we discuss challenges and future directions in improving DMN's generalization capabilities and its potential extensions for broader applications in multiscale modeling.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Analysis of Mixer Activities in the Bitcoin Network</title>
<link>https://arxiv.org/abs/2504.11924</link>
<guid>https://arxiv.org/abs/2504.11924</guid>
<content:encoded><![CDATA[
arXiv:2504.11924v1 Announce Type: cross 
Abstract: Cryptocurrency users increasingly rely on obfuscation techniques such as mixers, swappers, and decentralised or no-KYC exchanges to protect their anonymity. However, at the same time, these services are exploited by criminals to conceal and launder illicit funds. Among obfuscation services, mixers remain one of the most challenging entities to tackle. This is because their owners are often unwilling to cooperate with Law Enforcement Agencies, and technically, they operate as 'black boxes'. To better understand their functionalities, this paper proposes an approach to analyse the operations of mixers by examining their address-transaction graphs and identifying topological similarities to uncover common patterns that can define the mixer's modus operandi. The approach utilises community detection algorithms to extract dense topological structures and clustering algorithms to group similar communities. The analysis is further enriched by incorporating data from external sources related to known Exchanges, in order to understand their role in mixer operations. The approach is applied to dissect the Blender.io mixer activities within the Bitcoin blockchain, revealing: i) consistent structural patterns across address-transaction graphs; ii) that Exchanges play a key role, following a well-established pattern, which raises several concerns about their AML/KYC policies. This paper represents an initial step toward dissecting and understanding the complex nature of mixer operations in cryptocurrency networks and extracting their modus operandi.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection</title>
<link>https://arxiv.org/abs/2502.07384</link>
<guid>https://arxiv.org/abs/2502.07384</guid>
<content:encoded><![CDATA[
arXiv:2502.07384v2 Announce Type: replace 
Abstract: Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. At the inter-modality level, SAGEPhos introduces a Bio-Coupled Modal Fusion method, distilling essential kinase sequence information to refine task-oriented local substrate feature space, creating a shared semantic space that captures crucial kinase-substrate interaction patterns. Within the substrate's intra-modality domain, it focuses on Bio-Augmented Fusion, emphasizing 2D local sequence information while selectively incorporating 3D spatial information from predicted structures to complement the sequence space. Moreover, to address the lack of structural information in current datasets, we contribute a new, refined phosphorylation site prediction dataset, which incorporates crucial structural elements and will serve as a new benchmark for the field. Experimental results demonstrate that SAGEPhos significantly outperforms baseline methods. We release the SAGEPhos models and code at https://github.com/ZhangJJ26/SAGEPhos.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QPET: A Versatile and Portable Quantity-of-Interest-Preservation Framework for Error-Bounded Lossy Compression</title>
<link>https://arxiv.org/abs/2412.02799</link>
<guid>https://arxiv.org/abs/2412.02799</guid>
<content:encoded><![CDATA[
arXiv:2412.02799v3 Announce Type: replace-cross 
Abstract: Error-bounded lossy compression has been widely adopted in many scientific domains because it can address the challenges in storing, transferring, and analyzing unprecedented amounts of scientific data. Although error-bounded lossy compression offers general data distortion control by enforcing strict error bounds on raw data, it may fail to meet the quality requirements on the results of downstream analysis, a.k.a. Quantities of Interest (QoIs), derived from raw data. This may lead to uncertainties and even misinterpretations in scientific discoveries, significantly limiting the use of lossy compression in practice. In this paper, we propose QPET, a novel, versatile, and portable framework for QoI-preserving error-bounded lossy compression, which overcomes the challenges of modeling diverse QoIs by leveraging numerical strategies. QPET features (1) high portability to multiple existing lossy compressors, (2) versatile preservation to most differentiable univariate and multivariate QoIs, and (3) significant compression improvements in QoI-preservation tasks. Experiments with six real-world datasets demonstrate that integrating QPET into state-of-the-art error-bounded lossy compressors can gain 2x to 10x compression speedups of existing QoI-preserving error-bounded lossy compression solutions, up to 1000% compression ratio improvements to general-purpose compressors, and up to 133% compression ratio improvements to existing QoI-integrated scientific compressors.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>